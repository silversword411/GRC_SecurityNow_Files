GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#932

DATE:		July 25, 2023

TITLE:		Satellite Insecurity, Part 2

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-931.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  What did Apple recently say to the UK?  What's Google's "Web Environment Integrity" and why is it so controversial?  Who's the latest to express unhappiness over Google Analytics?  What happy news did the UK deliver about IoT security that the U.S. has not done so far?  Might you be qualified to join the U.S.'s forthcoming Expeditionary Cyber Force?  What's the latest on ransomware attack payouts and also on the massive MOVEit maelstrom?  And who's the most recent major player to announce the adoption of Passkeys?  Once we all have the answers to those questions, we're going to spend some time with our faithful listeners, then wrap up this Part 2 of our look at the current and quite distressing state of satellite insecurity.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  Get ready.  We're going to talk about a lot of things, a farewell to a hacker we know and love, no longer with us, sad to say.  We'll also talk about Apple.  They're saying, you keep this up, we're leaving the U.K.  And a proposal that Google says might eliminate the need for adblockers.  That and Satellite Insecurity, Part 2, all coming up next on Security Now!.	



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 932, recorded Tuesday, July 25th, 2023:  Satellite Insecurity, Part 2.



It's time for Security Now!, the show where we cover your safety, your security, your privacy, and everything else online with this guy right here, Mr. Steve Gibson.  Hi, Steve.



STEVE GIBSON:  And basically show you that you have none of the above.



LEO:  Yes.	



STEVE:  Despite all of the efforts.



LEO:  Which you probably knew, so...



STEVE:  Which, yeah.  And that makes it much more fun and the reason that we're never going to run out of things to talk about.



LEO:  That's true.		



STEVE:  We're going to finish our two-part episode today on the topic of satellite insecurity.  In our listener feedback section it turns out we've got - I think, when I was first putting this together yesterday, one of our listeners identified himself as in the satellite security industry.  And since then I ran across another.  So we've got some listeners who are saying, hey, this is great.



But first we're going to talk about what Apple recently had to say to the U.K.; answer the question of what's Google's Web Enforcement Integrity, I'm sorry, Web Environment Integrity, and why it's become so controversial; who's the latest to express unhappiness over Google Analytics; what happy news did the U.K. deliver to IoT security community; and what has the U.S. along those lines not done so far.



Might you, our listener, be qualified - listeners, we have more than one - be qualified to join the U.S.'s forthcoming Expeditionary Cyber Force?  What's the latest on ransomware attack payouts, and also the latest on the massive MOVEit maelstrom?  And who's the most recent major player to announce the adoption of Passkeys?



Once we have all the answers to those questions laid out, we're going to spend some time with our faithful listeners, then wrap up, as I said, the second part of our two-part look at the current and unfortunately quite distressing state of satellite insecurity.  And it's going to be fun because it follows the model of the development of security that we've been tracking now for the 18-plus years of the podcast.  And we do have a great Picture of the Week, thanks to another one of our listeners.



LEO:  And I have an update from Allyn Malventano, my SSD guy. 



STEVE:  Great.



LEO:  Remember we were talking last week about whether, A, you should turn off the swap file on Windows; and, B, if you do have it on, whether you should ever have it on an SSD.  And, you know, for years our recommendation was put it on the fastest drive you've got.  In fact, you put it on the inner circle of the fastest drive you've got so you get the best performance on your SSD or your swap file.  Allyn says, "Hey, Leo, you're both right.  Yes, it does add wear.  No, it's not enough to worry about," asterisk, "assuming you have sufficient DRAM to handle most tasks that are not constantly heavily swapping to disk."



STEVE:  You aren't thrashing; right.



LEO:  Yeah.  "If you are on a very memory-constrained system, swap can quickly become most of the SSD writes.  In extreme cases it could wear a drive faster than its rating."  He says:  "You want me to come on this show with you two and referee?"  Thank you, Al.



STEVE:  Actually, I did have one of our listeners compliment us on the fact that we had a discussion, we obviously had different positions and had a disagreement, but there was no puffery and no one got upset.  We just sort of, you know, you said this is what you think, I said this is what - anyway, he said it was really refreshing.



LEO:  You never hear that anymore, do you.



STEVE:  These days, no.  It's all pretty polarized.



LEO:  So, you know, personally, can you even still turn off the swap file on Windows?  I mean, I'm surprised that they still let you do that.



STEVE:  None of mine are on.  They're all turned off.  They work great.



LEO:  Yeah, okay, you can do that, okay.



STEVE:  Yup.



LEO:  Now I am going to tell you about our sponsor, and then we're going to get into the meat of the show, especially the most important part, the Picture of the Week.



STEVE:  The Picture of the Week.



LEO:  Absolutely.  Now, I think, Steve, you have a Picture of the Week for us.



STEVE:  So this is a great one.  I gave this one the caption "Why Reading the Manual Is Always a Good Idea."  But it could also have the caption "There's More Than One Way to Skin the Cat."  Imagine that you have a sort of an old-school coffee pot, but sort of reminiscent of a teapot, where it's got the main pot and then sort of a, what, a pouring spout spigot, you know, sort of like up and pointing out.  Well, the traditional way of pouring coffee from that pot would be to pick it up by its handle and move it over to the cup and tilt it until the coffee runs out of the spout; right?



Well, this picture demonstrates the alternative means of pouring yourself a cup of coffee, or what happens if you try to figure this out and you haven't read the manual, which is we have a guy blowing, he's got his whole mouth over the open top of the coffee pot.  He's blowing really hard down into the coffee, which of course forces the coffee up the spout and through a parabolic arc in the air, landing in the coffee cup.  And, you know, in this era, this day and age of photoshopping and fake pictures and things, you wonder did this really happen.  The coffee landing in the cup looks kind of real.  He does have his eyes focused where they should be. 



LEO:  He's aiming.  He's aiming.



STEVE:  Yeah.  Yeah, he's like having to - because you have to, yes, he's got to meter his blow in order to get the velocity correct, or he's going to overshoot or undershoot.  I'm sure this was not the first take of this particular operation.  Anyway, if this is real, I salute him.  Congratulations.  And of course he's going to have a mess because as he stops blowing, then all the coffee that's in flight is going to end up being a mess.  But if this was an actual photo, stop-action, caught mid-stream, congratulations.  Definitely a great candidate for our Picture of the Week.  Well, and actually it made it into the Picture of the Week.  And, boy, Leo, I've got some other ones, some good ones coming.



I had to share a bit of sad news with our listeners.  You already know.  The wider world received the news at the end of last week that the famous and long-since reformed hacker Kevin Mitnick had quietly passed away the previous Sunday, on July 16th, which was just three weeks shy of Kevin's 60th birthday.  He had been fighting pancreatic cancer for more than a year, and he left unfortunately behind his wife and unborn baby.  So I know that, Leo, you were good friends with Kevin.  He was on, back in the TechTV days, The Screen Savers a number of times, often with Wozniak, who was also a friend of Kevin's.



LEO:  Yeah, we have - I played, on this Sunday on TWiT, and I guess that's why you know that I know, I played a little clip from The Screen Savers where we had Kevin come on after eight years he'd been banned from using the Internet because of his conviction and his jail time.  And his probation had ended, and he came on The Screen Savers to use the Internet for the first time.  So we brought in Emmanuel Goldstein from the 2600 Magazine, famed hacker.  He was the devil on Kevin's left shoulder, and Steve Wozniak the angel on his right shoulder.  And Steve, by the way, brought him a brand new MacBook to use for...



STEVE:  They had a PowerBook.  



LEO:  Yeah, very nice of him.  And so you can see that.  It's on YouTube, if you search for Kevin Mitnick and The Screen Savers.  



STEVE:  And it had a great cartoon.  Woz had had one of the artists at Apple draw a neat cartoon where it showed the PowerBook on a table just out of reach from Kevin, who was behind bars, trying to, like, poke at it and reach it with a stick or a cane or something from inside his cell.



LEO:  It was pretty funny, yeah.  He wasn't allowed to use anything, you know, not just a computer but a smartphone of any kind.



STEVE:  Well, you know, Leo, he could have taken over the world if he'd had a smartphone, from his cell.



LEO:  I don't know if he was joking, but he said he couldn't even use an electronic toilet.  I don't know if that was a joke or serious.



STEVE:  Wow.



LEO:  Yeah.  Well, you know, it was federal crime.  I think, you know, there was some agreement that he was perhaps over punished and over prosecuted for a relatively moderate crime.  But anyway, he was freed.  And it's sad to see his, you know, finally having a family after all that time, to miss out on that is very tragic.  He was a really sweet guy.  I really liked Kevin.



STEVE:  Yeah.  So last Thursday BBC News carried a story under the headline "Apple slams UK surveillance bill proposals," but the first line of their piece was a showstopper.  It read:  "Apple says it will remove services such as FaceTime and iMessage from the UK rather than weaken security if new proposals are made law and enacted."  So, okay.  I mean, we've sort of been waiting to hear from Apple; right?  We've heard from Signal, and we've heard from WhatsApp.



So as we know, since we've been tracking this super-engaging struggle between the commercial forces who want to enforce absolute privacy, and those in the governments who are wishing to make privacy conditional, the UK is seeking to update their Investigatory Powers Act (IPA), which was originally created in 2016.  So now, you know, seven years later they want to update it.  It wants to require messaging services to clear their security features with the UK's Home Office before releasing them to customers.  The Act also lets the Home Office demand that security features are disabled, without telling the public.  And under this forthcoming update, this would have to be immediate upon the Home Office's demand.



So WhatsApp, Signal, and all the others have previously expressed their strongest possible opposition to this, with Signal making what has been, you know, up to now the strongest public statement, stating that they will simply "walk," as they put it, from the UK.  Now, you know, Apple has clearly been in opposition to this, too, but until now it hasn't drawn any such sharp line in the sand.  But that's what just happened.



The UK government has just opened an eight-week-long what they called a "consultation" on the proposed amendments to the IPA.  The government's claiming that they are "not seeking to create new powers," but only to make the Act more relevant to the current technology.  Uh-huh.  Apple has submitted its formal nine-page response to this now-open consultation period.  Apple formally opposes three things:  Having to tell the Home Office of any changes to product security features before they're released; the requirement for non-UK-based companies to comply with changes that would affect their product globally, such as providing a backdoor to end-to-end encryption; and having to take action immediately if a notice to disable or block a feature is received from the Home Office, rather than waiting until after the demand has been reviewed or appealed.  Which is the way things are today.



Apple says three things:  It would not make changes to security features specifically for one country that would weaken a product for all of its users; second, some changes would require reissuing a software update so could not be made secretly; and, third, the proposals "constitute a serious and direct threat to data security and information privacy" that would affect people outside the UK.



And, you know, remember that what the governments, the various governments here, are asking for is not simply the ability for these various encrypted services to respond to targeted court-ordered surveillance.  That's an entirely different ask.  What the governments are seeking now is universal surveillance of all communications of all kinds for all of their citizens.  And it's hard to argue that that's not new.  That's not an update to anything that exists today.



The BBC in their report quoted a cybersecurity expert, Professor Alan Woodward, from Surrey University, who said that technology companies are "unlikely to accept the proposals."  In an understatement.  He said:  "There is a degree of arrogance and ignorance from the government if they believe some of the larger tech companies will comply with the new requirements without a major fight."



And I think that Signal and Apple have been quite clear that they have no interest in, or need, to fight. In order to avoid breaking any newly enacted legislation, they'll simply pull their services from those regions which enact laws that seek to violate the privacy of their users.  Period.  You know, fight over.  Nothing to fight about.  Then we'll see what the voters in those areas think of the fact that their government has essentially denied them these services which they had been having and enjoying with no problem, and now apparently they can't any longer.  And we'll also see how the bureaucrats, law enforcement, and intelligence services like not having any secure messaging services available for them in support of their own needs for privacy.  You know, what's good for the goose.



So the Home Office told the BBC that the Investigatory Powers Act was designed to "protect the public from criminals, child sex abusers, and terrorists."  That's obviously an honorable goal, but the price for doing so is just too high, at least using this technology.  So anyway, we've been following this fascinating evolution.  And it is interesting that here this professor says, oh, you know, the government's ignorant, they think they can do this.  Well, governments create laws; right?  And so they can create any law that they want to, but no one's forcing Apple to do something it doesn't want to do.  So it'll be interesting to see if, you know, does the UK back down when they realize that these companies are serious?  Or is it going to take a period of not having these services available and then, what?  Anyway, really, really interesting.



Okay.  Four Google engineers have put forth a proposal, unofficially, that immediately generated a huge backlash across the web developer community.  Despite the fact, and in some cases perhaps due to the fact, that this proposal was dropped on GitHub as one of the engineer's personal projects, not from Google officially, many Google skeptics see this as Google's sort of backdoored means of sliding this quietly into the stream.  But if that's what it was, it didn't work because it quickly hit everyone's radar.



The developers termed this proposal, basically a web standards proposal, Web Environment Integrity.  The industry, however, quickly slapped it with the term "Web DRM" and noted that it would instantly provide a means for websites to refuse to offer their content to any browser running an adblocker or to disable adblockers remotely.  And given that Google's revenue stream is largely advertising, the fact that this new web standard was proposed sort of "off the books" by four web developers who all just happen to be employed by Google, well, one could be forgiven for questioning or at least wondering about the true motives behind this.



And essentially it does, indeed, amount to Web DRM, a means for enforcing the display of exactly what any website wishes to be displayed by empowering websites to selectively remove all user freedom at their web client end to alter the website's display in any way the website chooses.  Now, okay.  This is not to say that there could not also be true significant upside user benefits.  For example, allowing a banking website to rigorously control what, if any, third-party extensions are enabled when a user visits their site, essentially locking the web browser client in order to enhance the visit's security, well, you could see that could be a good thing.  But it's equally obvious that taking this control away from users could be abused by allowing any website to decide, on behalf of their visitors, what browser environments are acceptable.



Okay.  The engineer authors start off their description of Web Environment Integrity by explaining.  They said:  "Users often depend on websites trusting the client environment they run in.  This trust may assume that the client environment is honest about certain aspects of itself, keeps user data and intellectual property secure, and is transparent about whether or not a human is using it.  This trust is the backbone of the open Internet, critical for the safety of user data and for the sustainability of" - uh-huh - "the website's business.



"Some examples of scenarios where users depend on client trust include," and they give us four.  "Users," they say, "like visiting websites that are expensive to create and maintain, but they often want or need to do it without paying directly.  These websites fund themselves with ads, but the advertisers can only afford to pay for humans to see the ads, rather than bots.  This creates a need for human users to prove to websites that they're human, sometimes through tasks like challenges or logins."



Okay.  Second:  "Users want to know they're interacting with real people on social websites, but bad actors often want to promote posts with fake engagement, for example, to promote products, or make a news story seem more important.  Websites can only show users what content is popular with real people if websites are able to know the difference between a trusted and untrusted environment."



Third:  "Users playing a game on a website want to know whether other players are using software that enforces the game's rules."  And finally:  "Users sometimes get tricked into installing malicious software that imitates software like their banking apps to steal from those users.  The bank's Internet interface could protect those users, if it could establish that the requests it's getting actually come from the bank's or other trustworthy software."



So, yes, there are undoubtedly some valid use cases.  But this is a problem, too.  Whether or not this proposal ever advances past the controversy created by its appearance, it points to a tension that appears to be developing.  Should websites be able to reach across the Internet and exert full control over the experiences of their visitors?  When we run a native app on our local computer, we have very limited control over what it does and how it works.  We can launch it and terminate it, but that's about it.  It's not difficult to imagine that many websites would like to enforce that same level of control.



Anyway, I put a link in the show notes for anyone who might be interested in digging deeper into this specific proposal.  Because this thing may just be, you know, immediately shot down like a Chinese weather balloon, it's probably not worth going any further.  If it ends up taking hold, we'll certainly be giving it a much deeper look.  I mean, I looked at the spec.  It uses protocols related to WebAuthn.  So it's reusing some of that.  It uses public key crypto and this notion of something attesting to the state of the client at the user's end in order to essentially provide Web DRM.



And it was interesting to me that Google said, yeah, you know, websites that have ads are going to - they need the advertisers to know that real people are looking at them.  And I'm thinking, uh-huh.  And those websites also need visitors not to be able to blind themselves willingly by using an adblocker.  So both sides to that argument.



We've noted a number of times that various EU countries have been complaining, and have even now taken to suing organizations within their own borders who are continuing to use Google Analytics, which, they state, potentially transfers private identifiable data outside of their borders.  But now this concern has come home to roost with a letter that the Federal Trade Commission, our FTC, and the U.S. Department of Health and Human Services (HHS) have sent to 130 hospital systems and telehealth providers warning them about their obligations to protect their clients' personal health information.



So listen to this.  They wrote:  "The Office of Civil Rights at the U.S. Department of Health and Human Services and the Federal Trade Commission are writing to draw your attention" - and this was sent to 130 hospital systems - "to draw your attention to serious privacy and security risks related to the use of online tracking technologies that may be present on your website or mobile application, and impermissibly disclosing consumers' sensitive personal health information to third parties.



"Recent research, news reports, FTC enforcement actions, and an OCR bulletin have highlighted risks and concerns about the use of technologies, such as the Meta/Facebook pixel and Google Analytics, that can track a user's online activities.  These tracking technologies gather identifiable information about users as they interact with a website or mobile app, often in ways which are not avoidable by and largely unknown to users.



"Impermissible disclosures of an individual's personal health information to third parties may result in a wide range of harms to an individual or others.  Such disclosures can reveal sensitive information including health conditions, diagnoses, medications, medical treatments, frequency of visits to healthcare professionals, where an individual seeks medical treatment, and more.  In addition, impermissible disclosures of personal health information may result in identity theft, financial loss, discrimination, stigma, mental anguish, or other serious negative consequences to the reputation, health, or physical safety of the individual or to others.  Health Insurance Portability and Accountability Act of 1996, reminding us of HIPAA, H-I-P-A-A.



"If you are a covered entity or business associate under HIPAA, you must comply with the HIPAA Privacy, Security, and Breach Notification Rules, with regard to protected health information which is transmitted or maintained in electronic or any other form or medium.  The HIPAA Rules apply when the information that a regulated entity collects through tracking technologies or discloses to third parties, for example, tracking technology vendors, includes PHI, personal health information.



"HIPAA regulated entities are not permitted to use tracking technologies in a manner that would result in impermissible disclosures of PHI to third parties or other violations of the HIPAA Rules.  OCR's December 2022 bulletin about the use of online tracking technologies by HIPAA regulated entities provides a general overview of how the HIPAA Rules apply.  This bulletin discusses what tracking technologies are and reminds regulated entities of their obligations to comply with the HIPAA Rules when using tracking technologies.



"To the extent you are using the tracking technologies described in this letter" - meaning, you know, Meta/Facebook pixel and Google Analytics - "on your website or app, we strongly encourage you to review the laws cited in this letter and take actions to protect the privacy and security of individuals' health information."  So, yeah.  While this is not the same as the "thou shall not use" commandment that EU countries are issuing to their own local entities, Google has been Analyticizing for the past 17 years now, since 2005, and only now does it appear that people are beginning to say, "Hey, hold on here a second," and just looking at what happens under the hood of these tracking technologies.



Leo, I know that you've covered this.  I thought this was interesting, at least the U.S. side of this.  The European Union has just approved a draft version of what they are calling their "Cyber Resilience Act."  It's a set of new cybersecurity-related rules for IoT devices.  The Act passed the EU's Industry, Research, and Energy Committee with 61 votes in favor, one against, and 10 abstentions.  Under the new regulations, vendors must - get this.  Vendors must ensure their products meet a certain set of criteria before being sold in the Eurozone.  Products will have to come with automatic security updates as the default option - yay - must ensure data confidentiality using encryption, and vendors must inform authorities of any attacks.  And the new rules are expected to enter into effect by next year.



This is great news for the consumer overall, since any products sold globally, which include the Eurozone, would need to be in compliance.  So, for example, U.S. consumers would reap the benefits, as well.  And in this case the EU is ahead of the U.S. since all we have managed to get done here so far is to design an attractive shield emblem that will be placed on any devices that are compliant with a set of standards that don't yet exist.  But hey, at least we have a pretty-looking emblem shield.



LEO:  Yes.  I liken that to shipping the T-shirt before you have the product.



STEVE:  Yeah, look how pretty this is going to be.  We don't know what it means yet.  We don't know what you're going to have to do to get one.  But don't you want it?   Yeah.  Leo, let's take a break.



LEO:  All right.  We did talk about it on TWiG because Stacey, as you know, is an IoT guru.



STEVE:   Yup.



LEO:  And the NIST guidelines, you know, are good.  And if they follow those NIST guidelines, I guess, you know, including the thing you and I both care about probably the most, which is over-the-air updates, firmware updates of your IoT devices.



STEVE:  Yes.  In fact, I don't think I talked about it on the podcast, but the Zyxel routers had a problem in April, and they're now all being commandeered into a botnet.



LEO:  Yeah.



STEVE:  Because, sorry about that.



LEO:  Can't update them.



STEVE:  Nope.



LEO:  Yeah.  So that's a big problem.  Not just for you as a user, but for the Internet as a community.  Steve, let's go.  More to do here.



STEVE:  So we know you listen to the podcast, so you already have some qualifications.  Do you like to travel?  See faraway places and wonder what the people there are saying?  Enjoy wearing ridiculous camo when sitting in front of a computer?  Well, you may be just what the U.S. is looking for!



LEO:  Woohoo.



STEVE:  Lieutenant General Timothy Haugh, the nominee to become the next head of the NSA and CYBERCOM, has pledged to create "expeditionary cyber forces."



LEO:  Oh, dear.



STEVE:  That can be deployed into far off lands to reach important tactical targets in forward locations.  So get ready to pack up your laptop and head out.



LEO:  Wow.  Wow.



STEVE:  Yup.



LEO:  See the world.



STEVE:  We're not sitting in some bunker in Colorado anymore.  No, no, no, no.  We're going to update our TSA passport and, that's right, see the world and wonder what they're saying when you get there.



LEO:  Wow.  That's hysterical.



STEVE:  Yeah.  I mean, it's true.  Expeditionary cyber forces.  So, fun.



Okay.  I have in the show notes a chart showing from the beginning that this started being measured in 2019 to now, the percentage of ransomware payouts that have been made based on the number of attacks.  It depicts, happily, a more or less steady drop in the percentage of ransomware attacks which actually result in cash being paid.  When Coveware began tracking ransom payment rates at the start of 2019, 85% of ransomware attacks resulted in payments.  Today, that number has hit an all-time low of just 34%.



Coveware's report, which was just published on Friday, was titled "Ransomware Monetization Rates Fall to Record Low Despite Jump in Average Ransom Payment."  So the news is not all good, but it's great that today only one out of every three attacks results in payment.  That sure beats 85% from four years ago. The first three sentences of their report reads:  "In the second quarter of 2023, the percentage of ransomware attacks that resulted in the victim paying fell to a record low of 34%.  The trend represents the compounding effects that we've noted previously of companies continuing to invest in security, continuity assets" - meaning you're not put out of business completely when your machines are encrypted, continuity assets - "and incident response training.  Despite these encouraging statistics, ransomware threat actors and the entire cyber extortion economy continue to evolve their attack and extortion tactics."



Still, good news, down to one out of three.  So hopefully that cools things down a little bit because 84% guarantee of payment, that would have been much harder to resist than one third.  And there's also, you know, a non-zero chance of the good guys catching the bad guys, which we've seen a number of times.



In an update on the MOVEit mess, Emsisoft reports that the total number of confirmed victims of the Progress Software MOVEit Transfer SQL injection attacks has now passed 380.  And Coveware expects that the Russian Cl0p gang behind the attacks will receive somewhere between 75 and $100 million in total.  So unfortunately it was a worthwhile attack for those guys to launch.  Yeah, and there on the screen is the gradual, you know, pretty much, it's not a straight line, it's a wavy line, but from 2019 it's been heading downward.



LEO:  That's good.



STEVE:  Basically I think four years ago everybody got caught with their servers down.



LEO:  It was 85% payouts.



STEVE:  Yes.



LEO:  In 2019.  Wow.



STEVE:  Almost a guaranteed payout.  If your company got zapped with ransomware, it was like, oh, crap.  Send them some money, we need our data back.



LEO:  Yeah.



STEVE:  And now down to one out of three.  So that's great progress.  The latest major player to be adding support for Passkeys, would you believe TikTok?



LEO:  All right.  Yay.



STEVE:  They said:  "We'll begin rolling out passkeys for iOS in certain regions."  And I thought it was interesting that they didn't list the U.S.  They said:  "Starting with Asia, Africa, Australia, and South America beginning this month."  And they said:  "And anticipate expanding to other geographies and operating systems over time."  And they also noted they'd become a member of the FIDO Alliance.  And Leo, I was watching something the other day, some talking head show that was - oh, it was in regards to the strike, the Hollywood actors and screenwriters strikes.  TikTok has equal revenue to like the big streaming services.  It's massive.



LEO:  Oh, yeah.  Oh, yeah.



STEVE:  I had no - this is like, you know, Henry doing his cooking videos.



LEO:  Yeah.  Yeah.



STEVE:  It's astonishing.



LEO:  Don't knock it.  He's got pretty good income himself.  I'm happy he's paying his own way.  Yeah, he's got, like, two - and I asked him the other day because I saw him, he was making a chicken cordon bleu sandwich.



STEVE:  As one does.



LEO:  As one does.  I said, "Where do you get the ideas for these?"  He says, "I just make them up."  And I asked him, I said, TikTok, you know, he has more than two million, I think it's two point something million, 2.2 million followers on TikTok.  He said, "Yeah, but it's not my primary platform anymore.  Instagram's kind of taken over."  And he really wants YouTube Shorts and YouTube to be his place because the payouts are better.  That's the difference.  That's the difference.



STEVE:  Yup.



LEO:  But TikTok's money, but they keep it.



STEVE:  And it's like, what is it, like $800 billion or something?  I mean, it was like - and it was in a chart showing Netflix and Hulu and...



LEO:  Yeah.  But you know what?  The content on TikTok is much more compelling, and people spend more time watching it - not you and me, we're old.



STEVE:  I don't even - yeah.



LEO:  Yeah.  Younger people spend a lot of time scrolling.



STEVE:  Wow.



LEO:  And you've got the eyeballs, it's a great place.  Plus you can put more ads in.  You can't put ads in Netflix.  Or you can, but we can't...



STEVE:  Remember having - remember when we had that much time, Leo?



LEO:  Ah, those were the days.



STEVE:  Okay.  So we got some feedback from our listeners.  Allan E. said:  "I agree that I would never save two-factor authentication seeds in my password manager.  But it may be the least bad option for protecting shared business accounts on social media accounts in some cases."  And so I just wanted to say I didn't intend to suggest that there was no justifiable use case for having a password manager store time-based token secrets.  The question was just a perfect opportunity to highlight and talk about a beautiful example of the inherent tradeoff which exists between user convenience and security.



And to that end, Steven Haver, he said:  "Re Bitwarden TOTP."  He said:  "It's actually a huge increase in security for people who otherwise can't be bothered to turn on two-factor authentication," which you can't argue against that.  He says:  "It's also extremely useful with shared logins that are shared with multiple people via a Bitwarden Organization mode account."  He said:  "But for more tech-savvy people, I understand why you would want the greater compartmentalization."



He said:  "I run a hybrid approach where my 'less important'" - he has in air quotes - "TOTPs are in Bitwarden, the more important ones are in OTP Auth" - as are all of mine - "and the most important ones are WebAuthn" - meaning Passkeys - "on my security keys."  He said:  "In a way, TOTP seems like a dying format for those who already use a security key, as FIDO/U2F/WebAuthn become available on more and more sites."  And, you know, now soon TikTok.  "Whereas," he said, "once there were 40 secrets in my OTP Auth, now I'm down to just a handful."  He says:  "Thanks for a great podcast, and super excited to take 6.1 out for a spin soon."  Signed, Steven.



So thank you, Steven.  Of course I agree with everything Steven has just said, and with his hybrid approach, which makes sense.  I suspect that, for most people, just using Bitwarden will be the way to go.  But, again, my point was to use this more as an example of the nature of always the tradeoff that exists between convenience and security.  Super long password, way more secure, way less convenient.  So, you know, you need some way to manage that.



Sakis Kasampalis.  I'm sorry if I butchered your name.  I tried.  He wrote:  "Hey, Steve.  How exactly is Threads blocking Europeans using VPNs?"  He said:  "I thought that the idea of a VPN was that they cannot tell where you're located.  Are they blacklisting IPs of the popular third-party VPNs?  What about self-hosted ones?"  Okay.  So that question has multiple parts.  The first part is that there are two ends to every connection, and every end inherently knows the address and therefore the rough location of the other end of a connection.  So when someone in the EU connects to Meta directly, Meta gets their IP address and can choose to refuse it.



The clearest way to visualize what a VPN does is to see it as two connections, the user's connection to the VPN service and the VPN service's connection to the destination.  So when a customer is connecting through a VPN, Meta doesn't see the customer's IP and their rough location.  Meta is being connected to by the VPN, so that's the only IP and location that Meta sees.  This bring us to the second part of Sakis's question, which is, "Are they blacklisting IPs of popular third-party VPNs?"  And the answer to that is probably yes, that's certainly one way to do what they are doing.



It might also be that in the interest of preserving their users' privacy, VPNs might be deliberately stripping out some user tagging information that a user's web browser would normally provide.  So Meta might either and/or be detecting the presence of a middleman in the connection through the means of the metadata in their requests.  But either way, Meta can simply decide not to honor "indirect connections" through VPNs specifically because they can be used to mask the user's true location.



And finally, as for self-hosting VPNs, the question would be where the VPN's traffic would emerge onto the Internet.  Self-hosting sort of suggests that the endpoint is still located local to the user.  But then its IP would be geolocated and blocked.  So it would be necessary to self-host a VPN in such a way that the VPN's traffic emerged onto the Internet from a non-blocked region.  That might be doable, for example, by spinning up an AWS or Azure cloud instance, but that seems like a lot of trouble to go through just to obtain foreign access to Threads, whose popularity, by the way, appears to have collapsed overnight.



And Leo, on Sunday's TWiT show you and your two guests talked about the collapse of Threads traffic.  One of the guests noted how easy Meta had made it for Instagram users to join Threads.  Even I joined Threads because I have a stagnant Instagram account, and I wanted to grab my handle just in case Threads might amount to something someday.



LEO:  Are you @SGgrc on Threads?



STEVE:  That's exactly me.



LEO:  Okay.  All right.



STEVE:  But, you know, I wanted to note that Threads' apparent overnight success was always entirely illusory, because when it's made that easy to join, joining doesn't actually mean anything.



LEO:  Right, that's true, yeah.



STEVE:  You know, it's reminiscent of the news website paywall model.  Remember that originally all sites were free and ad supported.  Then some of them thought, "Hey, look at all the traffic we have.  Let's charge a little bit of money for people coming."  And mostly people said, "Wait, what?  You want actual money?  I think I'll find the same news elsewhere, thanks very much."  So it's going to be very interesting, I think, to see how, over the long term, how Meta's Threads does.  And that's really the only metric that matters.



LEO:  Well, of course Elon gave it a nice big boost over the weekend by changing the name of Twitter to X.



STEVE:  Oh.  And you know who has the trademark on X?



LEO:  Microsoft has one.  There are many.



STEVE:  Yes.  And maybe it will be dilution, but Meta also owns a trademark which is very, I mean, it looks exactly like, well, close enough.  And you know, as we know, for a trademark the test is whether a user might reasonably be confused by someone's conflicting use of a registered trademark.



LEO:  And, by the way, this is why I'm very glad that Twitter is no longer Twitter.



STEVE:  Yeah, that's a good point.



LEO:  And Elon is no longer Chief Twit.  We had words.  We had a little - we had some words with them, back in the day.



STEVE:  I remember back in the early days, yeah.



LEO:  And I'm very pleased that they're now X, and I will not start a podcast network called X.  It's not a great name, if you ask me.



STEVE:  And he can't have or get X.com, can he?



LEO:  He has X.com, yes.



STEVE:  He does?



LEO:  He's had that since day one.  So the story is hysterical.  He tried to rename - so PayPal, before it was PayPal, was X.com.  He's had it since then.



STEVE:  Ah.



LEO:  And the story is Peter Thiel and Sam Levchin, his cofounders, fired Elon because he wanted to use X as the name for PayPal, and they said no, we're going to call it PayPal.  And so that was when he left PayPal, and he took his money with him, and of course started a few other things since then.  



STEVE:  And I'll note that PayPal has done just fine since Elon left.



LEO:  Oh, yeah, yeah.  He was apparently difficult.  But one of his kids is named X, you know.  X is part of the name.  But he likes that letter for unknown - you know why?  Because he's nuts.



STEVE:  Well, I would argue that Twitter does need some competition.  I mean, like some real competition.



LEO:  There's a lot.  There's Threads.



STEVE:  I can't think of any better...



LEO:  There's Bluesky.  There's of course Mastodon.  There's a lot of good choices.



STEVE:  Yup.



LEO:  The problem is they're fragmenting the overall space.



STEVE:  Right.



LEO:  I have to say, because it's Meta, a lot of brands, a lot of politicians, a lot of newsmakers are all on Threads.  So that may be just how they win is just that's where everybody went; right?



STEVE:  And if Meta could actually deliver on some of the challenges that this kind of platform inherently has, you know, which Twitter was admittedly struggling with, but honestly apparently working to fix or, like, at least mitigate.



LEO:  Right.



STEVE:  Having the same platform, I mean, I've listened to so many people who are disappointed in what Twitter has become because it used to be a place they could quickly go to get news.  And it's just not that anymore.



LEO:  Yeah, yeah.



STEVE:  I don't need it for that.



LEO:  I think it's good to have something like that in the world, I think.



STEVE:  Yes.  It's a real need.  It is an absolute need.  Matthew N. Dudek, he said:  "Hi, Steve.  I'm looking into getting some wireless keyboards for the office, and I was concerned about the security" - I'm glad - "of the connection between the keyboard and the dongle," he says, "not Bluetooth, one like the Logitech K400+."  He said:  "Have you found any info on this, and if man-in-the-middle attacks are a problem for these kinds of devices?  What about the security of Bluetooth keyboards?  Are they any better?"



LEO:  That's a great question, since I just bought a Bluetooth keyboard.  Tell me.



STEVE:  I'm glad you did.  And that's what my wife is using, and I'm going to explain why you are both using those.



LEO:  Oh, good.  Oh, good.



STEVE:  Many years ago we talked about the very early widely available wireless keyboards which claimed to be offering "encryption."  But we had some fun at the time because the encryption turned out to amount to nothing more than XORing the byte that the keyboard sent with a static value.  You know, literally it was an XOR mask which would always flip the same bits in the byte, regardless of what was being sent.  So at best we would call that obfuscation, since passively recording the use of the keyboard and performing a frequency analysis of the characters seen would quickly reveal the exact fixed XOR mask.  And once you have that, everything typed could be unscrambled, and anything desired could be injected.



Okay, now, the keyboard in question uses Logitech's own "Unifying Receiver" technology.  It's not horrible security inasmuch as it uses AES encryption in counter mode.  Unfortunately, they tried to do it on the cheap, and a security review of the technology four years ago resulted in CVE-2019-13053.  And that CVE was the result of an incomplete fix for CVE-2016-10761 three years before that.  Logitech has publicly stated that they feel it's good enough, and that they will not be changing anything.  And of course at this late date changing anything would be quite "disunifying."



LEO:  Oh.



STEVE:  So from a quick look at the current state of Logitech's technology, it appears that allowing an attacker to press a few keys on the keyboard - this is with the Logitech unifying receiver technology as it is today...



LEO:  That little thing that ships with a dongle, if you want, you can put in your computer.



STEVE:  Right, right.  And I've got, you know, my mouse has one because I like Logitech mice.



LEO:  Yeah, me, too.



STEVE:  Right, right.  So allowing an attacker to press a few keys on the keyboard while sniffing its transmission is all that's needed.  Also, the protocol leaks metadata for things like turning the NUM LOCK and CAPS LOCK lights on and off and for other functions.  This allows for entirely passive attacks.  For AES in counter mode to be used securely, the counter's values can never be reused under the same initialization vector.  But enforcing that guarantee is difficult for any bare bones protocol, which is what Logitech created for their mice, keyboards, pointers, and other peripherals.



So the solution is simple:  Where true security is important, just use the full Bluetooth protocol, though such a keyboard may be more expensive, and they probably are, than Logitech's K400+.  All of my own keyboards are wired.  But as I said, my wife uses a Logitech MX Keys keyboard.



LEO:  I think those are nice, yeah.



STEVE:  Oh, it is a lovely, lovely low-profile keyboard that uses a full Bluetooth low-energy link.  Once it was paired to her Windows 10 machine, she has never had a problem with it.  So I can vouch for that and the protocol.



LEO:  I wanted a Bluetooth keyboard that would - I have two computers, one monitor, and I wanted a Bluetooth keyboard that would allow me to switch back and forth.  And that was Bluetooth.  And I also wanted clicky keys.  I know you're a clicky key fan.



STEVE:  Oh, boy, yeah.



LEO:  I really, I'm going to recommend - it's 200 bucks, not cheap.  But the Keychron Q1 Pro wireless custom mechanical keyboard, and I happen to like the brown switches, the Keychron Browns.



STEVE:  Uh-huh.



LEO:  This is a really wonderful - first keyboard I've really loved in a long time.



STEVE:  And do you actively switch it between computers?  Or does it just pair?



LEO:  Yeah.  No, no, in this one, and I like it this way, Function Key 1 is the first computer; Function Key 2 is a second.  I think you can do 4.



STEVE:  Oh, so you can have both machines on and listening.



LEO:  Oh, yeah, yeah, yeah.  That's right.



STEVE:  In sort of a KVM style.



LEO:  Yeah, except it's a little more manual because my mouse is the same thing.  I have a Logitech mouse that has three Bluetooth pairings.  So I switch the mouse to two, Function 2 on the keyboard, and then my HDMI port I switch to, you know, Port 2 on the monitor.



STEVE:  Leo.  Leo.  You qualify for the Expeditionary Force Cyber Team.



LEO:  I do wear my BDUs while I'm playing [crosstalk].



STEVE:  If you've got jammies that look like camo, you're good to go.



LEO:  It is a little bit manual, but I have had such bad experiences with KVM switches over the years that just I thought, you know what...



STEVE:  No, that's very cool.



LEO:  It works.  It works perfectly every time.  It's really a good way to do it, yeah.



STEVE:  And you may well want to have the other machine's screen still visible while you're over talking to...



LEO:  And I could do that.  Yes, I could do that.  In fact, one of the computers I do keep on because it's a server, so it's always running.  So I don't want to - I want to be able to switch back and forth while they're live.  And it works.



STEVE:  Nice.



LEO:  Works great, yeah, nice.



STEVE:  Glenn Lau asked:  "Is it possible to SpinRite a phone, iOS or Android, to speed up the phone?"  And unfortunately, I'm pretty certain that would not work.  While it would be possible to plug the phone into a PC to view it as a drive, only the user-facing storage portion would be seen, not the underlying hidden protected kernel and apps, which is really what you'd want to be rewriting.  So, you know, users don't get any access to that from the outside.



LEO:  Yeah.



STEVE:  And we don't want them to, by the way.



LEO:  Right.



STEVE:  Jorge Moran, he said:  "Hi, Steve.  I'm a big fan.  I've been listening to Security Now! for years.  I was wondering.  A couple of weeks ago when you talked about your Syncthing setup, you said you don't like containers. Is it just because of the added complexity, or do you have more reasons?"



LEO:  Ah, good question.



STEVE:  Okay, so great question.  Only personal preference.  I totally get it that there's a place for containers like Docker.  I agree that they are a terrific solution for many applications.  But just for myself, I have often seen how quickly things can get out of control when the approach which I would characterize as "just throw some more code at it" is taken.  So if I need to run Syncthing on Synology, and the only way to do that was to be containerized, then that's what I'd do.  But Leo, thanks to you, I don't need to do that.



LEO:  Yeah.



STEVE:  It just feels much better to be running Syncthing as a native Synology build.



LEO:  Docker's very lightweight.  The idea is you're using the same operating system on multiple containers.  They're somewhat isolated from one another, so they're pretty lightweight.



STEVE:  And it brings all of the dependent libraries and stuff.



LEO:  Exactly.  Exactly.



STEVE:  Right.



LEO:  But Docker is by default not particularly secure, so that's something that made me nervous about running it on my Synology, which must be secure; right?



STEVE:  Well, and there's a perfect example of why my KISS approach works for me.  And his question reminded me of another aspect of a story I've shared before, of how when I attended that DigiCert customer advisory meeting in Utah nearly six years ago, I casually mentioned, like during some coffee time, the rack of equipment that I had at the Level 3 data center.  And all of the guys around the table turned and looked at me like I had two heads.  So I said, "What?"  And one of them said, and he was clearly speaking for all of them, since the rest of them were like nodding their heads, he said:  "Steve, no one does hardware anymore."



And I took that to mean that they'd all moved all of their infrastructure to the cloud and were now paying Amazon or Microsoft or whomever for virtually hosting their entire infrastructures.  But I also noted that everyone but I worked for a major corporation, and that none of them but I were paying the bills for their infrastructures.



LEO:  Ah, yes, that's true.



STEVE:  And, you know, and it is true that I do occasionally need to drive over to Level 3 to exchange a dead SSD or a spinning drive which has died in a RAID.  You know, it's not an emergency, but it's like, okay, I received email saying we've lost a drive, come give us a little TLC.



But in return for that - and of course I do also enjoy getting to touch actual hardware, which always feels good - my infrastructure costs are fixed and very low.  I own all the hardware.  So I'm renting space, cooling, bandwidth and power.  And these days that doesn't cost very much because Level 3 actively wants to keep me from virtualizing my infrastructure with AWS or Azure.  I don't tell them that, but they have nothing to worry about.  They're not going to be losing me.



So when Jorge ended his tweet asking:  "Is it just because of the added complexity, or do you have more reasons?" actually my first thought was, "Hey, I even avoid compilers wherever possible."



LEO:  Wow.



STEVE:  Uh-huh.



LEO:  He hand assembles his code with a pencil and a piece of graph paper.



STEVE:  That's right, baby.  One one zero, one zero one, one zero zero.  People wonder, why is SpinRite taking so long?  One one zero.  Zero zero one.  One one zero.  Brian Weeden, he said:  "Steve, loved the show this week on satellites.  I work in the space sector on this issue."



LEO:  Wow.



STEVE:  "As you're prepping next week, I can offer up an open source report that my org puts out which includes an entire chapter on cyberattacks on satellites."  I have a link in the show notes.  He said:  "Looking forward to next week's Part 2."



So I followed the link that Brian provided.  And since it's exactly on point for today, I'll share the report's introductory paragraph, which introduces the term "counterspace."  It reads:  "Space security has become an increasingly salient policy issue.  Over the past several years, there has been growing concern from multiple governments over the reliance on vulnerable space capabilities for national security, and the corresponding proliferation of offensive counterspace capabilities that could be used to disrupt, deny, degrade, or destroy space systems.  This in turn has led to increased rhetoric from some countries about the need to prepare for future conflicts on Earth to extend into space, and calls for some corners to increase the development of offensive counterspace capabilities and put in place more aggressive policies and postures.



"We feel strongly," writes his org, "that a more open and public debate on these issues is urgently needed.  Space is not the sole domain of militaries and intelligence agencies.  Our global society and economy is increasingly dependent on space capabilities, and a future conflict in space could have massive long-term negative repercussions that are felt here on Earth.  Even testing of these capabilities could have long-lasting negative repercussions for the space environment, and all who operate there.  The public should be as aware of the developing threats and risks of different policy options as would be the case for other national security issues in the air, land, and sea domains.



"The 2023 edition of the report assesses the current and near-term future capabilities for each country, along with the potential military utility.  The countries covered in this report are divided up into those who have conducted debris-causing anti-satellite tests - the United States, Russia, China, and India."  Let me say that again.  "Countries covered in this report are divided up into those who have conductive debris-causing anti-satellite tests - the U.S., Russia, China, India - and those who are developing counterspace technologies - Australia, France, Japan, Iran, North Korea [wonderful], South Korea, and the UK.  It covers events and activities through February 2023."



And I have to say, when you scroll down and just look at some of the charts, wow.  I appreciated the idea that just testing, this report noted that just testing some of these things like debris-causing events, meaning you deliberately blast some out-of-service, no-longer-used satellite to see if you can, and unfortunately it explodes, and a lot more debris now to be tracking.  Wow.



And in fact I was watching, TWiT was replaying I guess a recent episode of This Week in Space in the live feed before MacBreak Weekly, and you had a guy, neat guy, on who was talking about exactly this, about like the problems with the number, the individual pieces of crap that now have to all be individually tracked, and it actually is causing a problem when you want to launch something new up there because it's got to - you have to find a clear path.  And so you need to time your launch window so that your whatever it is rocket will be moving through a place where it's not going to hit any of this crap on its way up.  Oh, my god.



LEO:  Not to mention the Kessler effect; right?  I mean, at some point...



STEVE:  Well, that is, that is the Kessler effect is that something hits something else, and then that hits something else, and you end up with this domino explosion of junk.  Oh, Leo.  We are not so clever.



LEO:  Well, here's the good news.  It will shield us from the sun.  So climate change is no longer an issue. 



STEVE:  It may shield us from departure.



LEO:  It may change the climate in the wrong direction, but okay.  At least we'll cool off a little.  We don't have those hot summers.



STEVE:  Someday parents will tell their children, you know, eclipses used to be infrequent events.  Now, it's like, "Mommy, what are these shadows passing along the ground?"  "Well, yes, Earwig, that's now..."



LEO:  Really, is that the name of the future, Earwig?  Is that what we're going to...



STEVE:  Yeah.  We're going to start, we're going to call our kids - I figured that was safe.  That's not a name that anyone's using today.



LEO:  It's safe.  No one's using Earwig.  I can't wait to read your first sci-fi novel, Steve.  That'll be fun.  Earwig.



STEVE:  I have no big SpinRite news this week.  I am at the start of the work to update SpinRite.  Oh, actually I'm well into it.  Remember that when I began three years ago I created that new USB drive setup capability since I knew that was going to be needed.  So I'm in the process now of amalgamating that, that InitDisk technology, into the Windows SpinRite component.  I'll get that done.  I'll release it for testing to our group.  I'll come back and give the DOS SpinRite another rev because a few pieces of debris have accumulated there in its orbit.  And then I will end up merging it all together, and we will have SpinRite 6.1.  So on that note, Leo, let's take our final break, and then we're going to look at more about what could go wrong in space.



LEO:  Oh, boy.  I can't wait.  I love it that we have somebody from the Secure World Foundation listening to the show and keeping us honest.  That's promoting cooperative solutions for space sustainability.  Didn't know such a thing existed.  Okay, Steve.  Let's talk about Satellite Insecurity, Part Deux.



STEVE:  So of course last week we began our coverage of this important topic.  Now, I'm going to confess that I rolled my eyes when our previous U.S. president, Donald Trump, announced the creation of Space Force, a new branch of the military intended to focus upon what happens above our heads.  My eye-rolling was mostly due to a lack of appreciation, which I now have, of what is an obvious need.  Satellites are uniquely vulnerable to many forms of attack.  Both physical and cyberattacks are actually happening.  Last week we learned that ground-based missiles are capable of destroying satellites from the ground, and that space-borne robot satellites capable of both repairing friendly satellites and deliberately damaging hostile satellites are not science fiction.  They exist, too.  I was thinking, I don't remember what that James Bond movie was where the opening scene showed some spaceship big maw opened...



LEO:  "Moonraker."  It was "Moonraker."  They took the satellites in.  He was stealing the satellites, yes.



STEVE:  Right, right.  Anyway, so that's not - it was fiction then, not so much now.  



LEO:  Not so much, yeah.



STEVE:  So it was against this backdrop that all of this was triggered by the recent publication of a research paper which demonstrated that those satellites orbiting above are also disturbingly vulnerable to ground-based cyberattack, which is our focus today.



The short news blurb about this which initially caught my eye said:  "Satellite security decades behind."  And boy, by the time we're finished with this today, you're going to understand exactly how true that is.  "A team of academics from Germany has analyzed the firmware of three Low Earth Orbit satellite models and found satellite security practices lagging by decades compared to modern laptops and mobile devices.  Researchers found the firmware to be prone to several types of vulnerabilities, lacking basic protection features such as encryption [wow] and authentication.  The researchers claim they devised attacks that could hijack satellite systems, cut satellites off from their ground stations, move satellites to new areas, and even crash them into the ground or into other space objects."



LEO:  Oh, no, a message to Q.



STEVE:  As I mentioned last week, the researchers assembled their research into a paper titled "Space Odyssey:  An Experimental Software Security Analysis of Satellites."  The research was delivered during the recent 44th IEEE Symposium on Security and Privacy held two months ago in May, and it was awarded a Distinguished Paper Award for the conference.



So here's what the team described of their finding in their paper's Abstract.  They said:  "Satellites are an essential aspect of our modern society and have contributed significantly to the way we live today, most notable through modern telecommunications, global positioning, and Earth observation. In recent years, and especially in the wake of the New Space Era, the number of satellite deployments has seen explosive growth."



LEO:  It was "You Only Live Twice."  I got the wrong movie.  "Moonraker" would be the obvious one; right?



STEVE:  Of course.



LEO:  "You Only Live Twice."  Yeah, yeah, they captured the satellites.



STEVE:  That's perfect.



LEO:  There's James Bond in his spacesuit because 007 is good anywhere.  And here they come.



STEVE:  Uh-oh.



LEO:  Uh-oh. Oh, no.  Oh, no.  Anyway, we can do it.



STEVE:  Boy, is that a hokey-looking satellite.



LEO:  Before CGI, I have to say, we really put up with a lot of crappy-looking stuff, didn't we.  We didn't know any better.



STEVE:  Yeah.  Don't watch any old episodes of "Lost in Space," Leo.  It really does...



LEO:  I know, they don't age well, do they.



STEVE:  Danger, Will Robinson.



LEO:  He was about to fall over every time he waved his arms.



STEVE:  Wow.  So they said:  "In this paper we provide a taxonomy of threats against satellite firmware.  We then conduct an experimental security analysis of three real-world firmware images.  We base our analysis on a set of real-world attacker models and find several security-critical vulnerabilities in all analyzed firmware images."  Actually 13 critical problems spread among three actual satellites.



They said:  "The results of our experimental security assessment show that modern in-orbit satellites suffer from different software security vulnerabilities and often a lack of proper access protection mechanisms.  They also underline the need to overcome prevailing but obsolete assumptions.  To substantiate our observations, we also performed a survey of 19 professional satellite developers to obtain a comprehensive picture of the satellite security landscape."



Okay.  So in other words, after this team of six researchers had uncovered what they thought they had uncovered, they were like, what?  Really?  So they did the survey just like as a sanity check, like to confirm that what they thought they saw was like - and the guys were like, uh, yup, that's the way we do it.



So they begin by explaining a bit of the history of the industry, which I want to share since it will be so entirely believable and even understandable, though also so obviously wrong, to our podcast audience.  So these guys explain, they said:  "Satellites are sophisticated technical devices that are placed in outer space for research purposes or to provide terrestrial applications with services that leverage the coverage of the Earth's surface from a distance.  While the first satellite, Sputnik, dates back to 1957, we're in the midst of a renaissance of spaceflight referred to as the New Space Era.



Especially in recent years, we have observed an enormous growth in the number of earth-orbiting satellites.  According to the United Nations Office for Outer Space Affairs (UNOOSA), the number of satellites has nearly doubled from 4,867 in 2019 to 9,350 last year in 2022.  The majority of these satellites form mega-constellations like Starlink, which plans to launch more than 40,000 satellites in coming years."  So to put that in perspective, we don't quite yet have 10,000.  We have 9,350 last year.  Starlink wants to put up an additional 40,000.  



They said:  "Small satellites are at the heart of this New Space Era as their size and the widespread use of commercial off-the-shelf (COTS), commercial off-the-shelf components makes them affordable even for small institutions.  Furthermore, they cover a broad spectrum of use cases ranging from commercial applications like Earth observation, machine-to-machine communication, and Internet services, to research applications such as technology testing, weather and earthquake forecasting, and even interplanetary missions.



"Although their applications vary widely, small satellites commonly consist of radio equipment and microcontroller boards. Hence in the broadest sense they are computer systems connected to a ground station on Earth, and sometimes even to other satellites.  Because they rely on wireless connections for command and control, and use microcontrollers, they are potentially as vulnerable to attacks as any other connected IT platform on Earth."  Can you say IoT?  Except not "I," you know.  So it's not Internet of Things, it's Space of Things.



"This issue," they say, "has not been very relevant in the past  since access to ground stations was expensive and limited to large satellite operators.  However, the situation changed fundamentally in recent years."  Get a load of this.  I didn't know this.  "Nowadays, ground stations are even affordable for private individuals.  And with the emergence of..."



LEO:  Wait, what?



STEVE:  "...Ground Station as a Service..."



LEO:  What?



STEVE:  "...(GSaaS) models, such as those offered by Amazon Web Services and Microsoft Azure, the entry barrier becomes even lower."  They said:  "We've seen in the mobile network security domain how the providers' assumption that the radio equipment required for attacks would be too costly and out of reach for attackers was ultimately disproved by technological advances."  Right, like the Pineapple and, Leo, that thing you have in your pocket.



LEO:  Oh, the Flipper Zero, yes.



STEVE:  That's right.  So affordable ground stations create a new novel attack surface where adversaries can communicate with satellites and take advantage of software vulnerabilities.  If they successfully compromise the satellite's firmware, they can access the satellite and potentially take over complete control of the system.  And in fact these guys did that.  They said:  "Despite warnings being made early, little has been done to address this problem for several reasons."  Once again, our favorite anti-security thing, inertia.



LEO:  Yeah.



STEVE:  Well, and some lack of understanding.  They said:  "While the lack of security standards for satellites and the complex supply chain complicate the situation, the main reason is the inaccessibility of satellite firmware."  Right?  It's like, it's up there.  You can't get it.



So they said:  "Historically, satellite developers have relied on [oh, yes] security by obscurity.  The developers of the Iridium network even mentioned that their system would be too complex for attackers."  Yeah, how did that work out?  "Attackers have nevertheless successfully decrypted the communication of the network.  The inaccessibility of satellites in orbit makes dumping of the firmware by researchers very challenging, if not impossible, impeding progress in this area.  Hence, the developers of satellite firmware act as gatekeepers and do not provide researchers with research subjects."



And just I'll pause here for a second and think about almost every instance that we talk about here of a security researcher finding serious problems in some widget wasn't supported in any way by the widget's widget maker.  It was them taking the widget apart and sticking some probes into its brains and sucking its firmware out through a JTAG interface, and then...



LEO:  Sounds painful.



STEVE:  Oh, the widget is never the same, Leo.



LEO:  Oh, yes.  No, that's pretty bad news, yeah.



STEVE:  It's not good for the widgets, no.  But some have to be sacrificed for the greater good.  So here's the problem.  When your widgets are flying around, you know, miles above you, you can't get them.



So they said:  "Previous commentators have acknowledged that the topic is still understudied and conclude that collaboration between satellite development and the security field is required.  Additionally, well-known topics like the security of satellite communication, the security of satellite-based Internet services, and threat scenarios for satellites have recently gained increasing attention."  Thank god, and it's about time.  "However, discussions around individual satellites typically lack technical details of satellite and real-world foundations due to the inaccessibility of satellite software."



Okay.  So we have a situation where the physical isolation that's inherent in anything launched into orbit has supported a laxity of security rigor.  And it also really sounds as though the developers of these systems have not been following along with the startling advances being made in the capabilities of the underground hacking community here on the ground.  As we've seen time and time again, if money can be made through some hack or attack, it's going to happen, and those attacks are only going to be improving over time.  It is a very good thing that Bitcoin was not a satellite-based cryptocurrency, or there wouldn't be any satellites left in orbit today.



But in all seriousness, the U.S., China, and Russia don't care about the price of Bitcoin.  What they want is the ability to instantly cripple each other's above-Earth command-and-control infrastructure if the you-know-what suddenly hits the fan.



These researchers felt that they were able to significantly contribute to an understanding of satellite-based insecurity in three ways.  They said:  "First, we present a taxonomy of threats against onboard satellite firmware.  Such a systematic review of the attack surfaces allows us to better represent the complex nature of satellites and categorize security-relevant findings throughout the paper.



"Second, we conduct an experimental and comprehensive security analysis of three real-world, in-orbit satellites to better understand the attack surface and the current state of software security in this particular domain.  We focus on Low Earth Orbit (LEO) satellites, as this orbit is the main focus of the New Space Era."  Meaning these are the ones that are going to be going up a lot, and we need to get them secured.  And we've become dependent on these little puppies; and, boy, are they little.  Get a load of this.  They said...



LEO:  They should call them "Little LEOs," then.



STEVE:  Little LEOs, that's right.



LEO:  Awww.



STEVE:   "The most prevalent satellite class is the nanosatellite" - Nano LEOs - "more specifically, the CubeSat, which is a standard form factor of 10-centimeter cubes called 'Units' or 'U's.'"  Okay, that's four inches on a side.



LEO:  Wow.



STEVE:  I know.  These satellites, you know, I wonder if they're going to start calling them "cluster satellites."  That would be bad.  Anyway, "These satellites typically weigh less than 1.33 kilograms per U and are used in many different projects.  After a long period of persuasion, trust building, discussions, and contracts" - you know, they had to sign - "we obtained access to several [three] satellite firmware images that we were able to analyze."  In other words, they couldn't get them from the air.  So they said, look, we're Germans.  You can trust us.  We're going to sign contracts.  We'll tell you what we find.  You haven't ever bothered to look at your own code.  Please let us look at it.  We're going to help you.



LEO:  We're Germans.  We know how to find this stuff.



STEVE:  That's right.



LEO:  Yes.



STEVE:  "All vulnerabilities," they said, "have been responsibly disclosed to the vendors."  They said:  "Note that the entry barrier to identify these vulnerabilities was complex, given the sensitive nature of these systems.  To the best of our knowledge, our work is the first to demonstrate exploitation of satellite firmware vulnerabilities allowing attackers to gain persistent control over the satellite."



Third, and this is where they said we conducted the survey of 19 professionals to ask are you serious about this.  And there were 17 satellites that they had technical information about, and those participants had worked on an aggregate of 132 different satellites.  So this was the right group of people to ask.



So thankfully, satellite communications is not entirely a standards-free, roll-your-own environment, although it is nothing like the Internet.  There is a standards body known as the CCSDS for Consultative Committee for Space Data Systems (CCSDS).  It's a consortium of numerous space agencies that agree together on the standards that'll be used for a satellite's communications.  So the CCSDS provides the protocol standards for communicating with all components and parties involved in spacecraft operations.  These standards cover all the layers of the OSI networking model, usually offering a couple of options per layer.



Two protocols stand out and were examined by these researchers.  There's the higher level protocol which is like our TCP on the Internet called the SDLS, which is the Space Data Link Security protocol.  And as I said, it's the data link layer like TLS.  And then there's the lower level protocol, which we would call IP in the Internet, and that's called the SPP, the Space Packet Protocol.



So their paper then delves into the detailed intercommunications among the various satellite components.  The attacker's goals are no different in the sky than they are on the ground.  They would love to take over the entire package if they could.  But failing that, being able to tap into the communications flow might be all that's available.  And if so, they'll take that.  But if even that is out of reach, then denying the services provided by the satellite to its rightful users is the final fallback.  That should all sound familiar because it's exactly what we have down here on the ground.



The researchers explain that the information containment that has historically existed until recently has been crumbling with the many recent changes taking place within the satellite industry.  You know, and that makes sense; right?  If there's only, like, three companies making and launching satellites, then it's easy to keep your secrets secret.  But as we know, the more people who know a secret, the less secret it is.



They said:  "For decades, the satellite community and developers have acted as gatekeepers for the topic of satellite security.  By keeping the software and components of satellites under lock, they created a barrier of obscurity that prevented any meaningful research on this subject.  Hence, external communities had no way to study satellite internals and potential security issues.



"In recent years this changed, as the developments in the space domain have moved towards the use of common off-the-shelf components" - in other words, not some bizarro one-off processor, but a cortex or a standard chip that IDA Pro or Ghidra would be able to reverse the code for.  Also we have open satellite designs, and open-source libraries.



They said:  "These factors have been multiplied by the explosive growth in the number of satellites and the inherent increase in the size of the community.  Hence, the number of people holding knowledge about satellites has been steadily increasing. Overall,  we argue that a transformation is slowly happening concerning the effectiveness of security by obscurity in space-borne assets."  In other words, it's not going to hold any longer, folks.  And so you can't be relying on that the way you have in the past.



And they conclude with this:  "As a result, we must assume that attackers have detailed knowledge of the target satellite, including detailed documentation and access to firmware images.  Further, several open-source satellites already enable attackers to study satellites.  We therefore assume attackers have detailed knowledge of satellites, including their firmware, except for their cryptographic secrets."



So in other words, this is the modern security model which is being brought to an industry that never had it before, at least from the standpoint of these researchers.  The satellite industry may not have caught up yet, but the only way for researchers to test current satellite security is with an honest set of assumptions of the threat model.  As we know, it's always necessary to assume that one's adversary knows everything about the design of their target because too often that's exactly the case.



Another area that they needed to address they termed the "Myth of Inaccessibility."  They wrote:  "Until recently, it was generally assumed that satellites always communicate with prohibitively expensive" - they use the abbreviation "GSes," meaning Ground Stations.  "As a result, only a few actors could attack a satellite, similar to the assumption for mobile cell phone networks many years ago.  Unfortunately, this assumption had a major impact on the adaptation of security features for satellites, meaning the lack of them.



However, ground station prices have dropped significantly in the past few years.  Today it's possible to create a fully functional ground station" - of your own, in your backyard - "for less than $10,000, and there are open-source communities around developing ground stations.  In addition, GSaaS" - as we said, Ground Station as a Service - "providers such as Amazon Web Services or Microsoft Azure rent a ground station to the user, or allow ground station owners to monetize unused ground station capacity by temporarily renting it to end users."  Right.  What could possibly go wrong?



"As a result, one does not even need to own ground station equipment to interact with satellites.  Additionally, transceivers for specific satellite services have become so compact and cheap" that Leo even has one in his pocket.  No.



LEO:  I do, I do, yes.



STEVE:  So cheap that - just stand outside and point to the heavens, Leo.  "Furthermore, there are now many LEO satellite constellations in space with satellite-to-satellite communication capability."  So they're able to talk to each other.  "At the same time, there is an increasing number of smaller research LEO satellites.  There are already a number of satellites with significant communication capabilities in space that are even intended to be used by third parties.  Therefore, we believe that there is a paradigm shift in the assumption that satellites are inaccessible, which is particularly pronounced for Low Earth Orbit satellites."



Okay.  So the researchers examined a trio of satellites with widely varying architectures.  Actually, that was one thing that sort of impeded their research.  There was one that was based on a LEON - or no, it was the AVR32 that was just announced very recently, and its instruction set was not yet well-supported by the various disassemblers.  But so there are three satellites.  One used an ARM Cortex-M3; another used, as I said, that much more recent AVR32 instruction set; and the third used a LEON3 - I wonder if LEON is like for LEO, you know.



LEO:  With an N.  Leo Nano.



STEVE:  Yeah, a LEON3 SPARC V8 processor.  In all three cases, upon reverse engineering the satellite's current firmware using IDA Pro and Ghidra, both which we've covered in the past, in each case they uncovered multiple remotely exploitable vulnerabilities that led to remote code execution.  Meaning these things are vulnerable.



In return for receiving access to the firmware images, they responsibly disclosed their discoveries of a total of 13 of these "all of them were bad" vulnerabilities across the three satellites they examined.  The good news is that sky-bound firmware can be uploaded.  The bad news is that, for example, in the case of that ARM Cortex-M3 processor contained in a satellite which was launched in 2013, the firmware update process they were told takes anywhere from several days to a week, depending upon the ground station and link quality.  This is due to the low-bandwidth UHF/VHF components which run at - wait for it - 9600 baud, and the sharing of bandwidth.



So to share a sense for the sorts of things they found in these 13 items, they wrote:  "Insecure-by-design TeleCommands."  TCs is an abbreviation for TeleCommands, which is the process of, obviously, sending a command up to something in orbit.  So they said:  "Even with no access protection, a satellite should be designed so that TeleCommands do not compromise the satellite's stability without further validation.  Two deliberately present TeleCommands" - this is in one particular satellite - "allow arbitrary reading and writing of memory.  On the technical level," they said, "the attacker controls all parameters passed to memcpy through command arguments, such that these" - I know, Leo, I hear you in the background.  Yes, I know.  It's unbelievable.



LEO:  I didn't even have my mic on and you heard me.



STEVE:  "Such that these two TeleCommands are dangerous TCs.  Anyone with a custom ground station could utilize them to gain remote code execution and seize control of the satellite."  They said:  "Noteworthy, the ability to execute arbitrary code, which these provide, would allow an attacker to write firmware updates to the flash memory persistently, making the takeover irreversible.



"Modern operating systems such as Linux or Windows deploy defenses to prevent trivial exploitation of such vulnerabilities, but the RTOS in this ARM Cortex-M3 based satellite does not feature any such protections.  In particular, neither ASLR" - of course we know that's Address Space Layout Randomization - "nor stack cookies" - which prevents trivial buffer overruns - "are used.  To prove the impact of this vulnerability, we built an exploit, sent our payload over the COM interface to our rebuilt satellite in the lab, and executed arbitrary code.  In our case, we play sound over the connected speaker."



Okay.  So just to be clear, this satellite that they're referring to actually had deliberate commands which were received over its communications link which allowed any of the machine's memory to be read back, written to, or moved around.  I mean, again, this is like Microsoft that built that command into the early Windows Metafile; right?  Where if the Metafile interpreter didn't do what you want, you could just put some native code in the Metafile and tell the machine to execute it.  What could possibly go wrong with running your own native code from a media file that the machine could be sent?  Anyway, what could possibly go wrong with allowing firmware to be rewritten in an in-orbit satellite using some commands that are not authenticated?



Anyway, so of course coming from a security-aware state, the security-aware state that we have all been living in for many years now, it's almost difficult to appreciate what they mean when they say that the security of many of these satellites relies upon a lack of access to satellite communicating ground stations.  In other words, they were not kidding at all.  Some of these satellites, as I said, will actually obey by deliberate design remote commands to read, write, and move memory around with no concept of protection, just because they thought, well, you know, who can talk to these things?  Very few people.  And we're not going to give them our firmware, so they're never going to know what's up there anyway.  



Here's another example.  They call this one "Trusted ICP Size Field.  Upon receiving an ICP packet, the packet is passed through a FreeRTOS data queue to the command scheduler, which executes the associated command using the included arguments.  We observed that a function parsing the command structure does not validate the 'length of arguments' field against the total length of the ICP packet."  I mean, this is Security 101; right?  "Or its payload.  Thus, any external attacker can specify a malicious field length, which indicates that the arguments would be longer than they actually are.  This causes a command handler function to use more bytes from the memory heap than intended, leading to a buffer overread.  Hence, an attacker can include other data in the attacker-TC (TeleCommand) which leads to a control data leak.



"Again, we verify that this works on the real satellite by testing it on our recreated hardware and manage to successfully exploit the vulnerability.  The leak itself is reliable and is not impacted by environmental conditions, but extracting specific secrets depends on the heap layout.  This vulnerability is reminiscent of the well-known OpenSSL Heartbleed vulnerability."



Or how about this one, which describes something they found in a different satellite:  "OPS-SAT uses a flash file system to store files."  And I don't know which OS that uses.  Maybe they say.  Oh, I think it's another FreeRTOS.  "OPS-SAT uses a flash file system to store files, including the firmware image.  Existing TeleCommands allow to create new files and write to them, providing the capability to upload a malicious firmware image onto the satellite.  To change the filesystem path pointing to the current image, critical commands must be enabled, which is a global Boolean value in the satellite's settings.  Crucially, changing this flag can be done via a TeleCommand that does not require verification.  Hence, external attackers can conduct arbitrary firmware updates, which allows them to seize control over the satellite.



"Interestingly, similar critical functionalities are also hidden behind the same flag, indicating that engineers were aware of its critical importance, but decided not to implement further protection."  Okay, so anybody can flip the flag which is protecting this.  And once you do, it's not protected, as are other important functions.  And once it's not protected, then you're able to upload your own firmware, name it what you want, and then change the path to the current image, causing the satellite to switch to it.



And here's the last one I'll share, a problem in a widely used library.  "A widely used," they write, "a widely used space" - we have the space SDK.  That would be the SSDK, I guess.  "A widely used space SDK utilizes the UFFS library, which implements a low-cost flash file system."  I'm sure that's what FFS, you know, Flash File System.  "The library is used on roughly 75 spacecraft.  And according to the library's author" - who I guess is proud - "is also used by NASA."  They wrote:  "We identified a stack-based" - I guess that would be a space stack based - "buffer overflow vulnerability in the file renaming procedure, where the name of the new file is copied to a buffer of static size..."



LEO:  Oh, oh, oh.



STEVE:  I know, "without any size check."



LEO:  Oh, lord.



STEVE:  "Resulting" - now, again, this library is used in roughly 75 spacecraft, and NASA is using it - "Resulting in arbitrary code execution.  We experimentally verified that this vulnerability can be exploited to gain arbitrary code execution.  In OPS-SAT this function is only exposed in an inaccessible UART debug-port, posing no security threat to OPS-SAT in its current state.  Still, moving files is a reasonable file system interaction to be exposed via TeleCommands to semi-privileged attackers.  Hence, any of the other roughly 75 spacecraft implementing such functionality are also likely to be vulnerable."



Okay.  By this point, everyone should have an idea by now of, like, what's been going on.  These guys were not kidding when they characterized the satellite industry's security as lagging behind by several decades.  Thanks to an attitude of, well, "We are not the PC industry, we are not connected to the Internet, and you can't talk to our birds without special equipment," the security concerns that all of us on the ground here have been fighting for the past several decades and has created endless fodder for this podcast, doesn't appear to have sunk in at all.



Sure, there are instances of mistakes that have not been caught, like these guys.  But the most glaring insanity are deliberately designed commands which are insanely powerful and lacking in any authentication, assuming that those commands will never be issued by anybody because they're not connected to the Internet.  They require a ground station.   That assumption may have been useful 10 years ago, but it holds today.  They implicitly assume that no bad guy will ever be able to get their hands on a radio, even now that Amazon and Microsoft will happily lend you one of theirs.



So I sincerely hope that this work, and others similar to it, have or will come to the attention of all of the relevant parties.  The good news is that down here on the ground, where we have the Internet, and it's been connecting everyone to everyone else since its beginning, we have had to develop highly, insanely well, you know, peak security awareness.  And so hopefully that will rub off on all of the space-bound guys.



LEO:  You know, I just always assumed that NASA put a lot of effort into secure code and testing and all of that stuff.  Maybe NASA does, but obviously there's a lot of commercial space going on.



STEVE:  Right, right.  You know.  And Leo, come on, would Elon delay the launch of a Starlink?



LEO:  No comment.



STEVE:  Just launch it now.  We'll fix it in orbit.



LEO:  We'll fix it in orbit.



STEVE:  That's right.



LEO:  That should be the name of this show, "We'll Fix It in Orbit."



STEVE:  We'll fix it in orbit.  



LEO:  Steve Gibson, you're the best.  We look forward to Tuesday all week long so we can all listen and hear your words of wisdom.



Copyright (c) 2023 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.






GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#933

DATE:		August 1, 2023

TITLE:		TETRA:BURST

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-933.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  It turns out that Advanced Persistent Threats have been leveraging satellite communications for many years.  We start by looking at that.  Then we'll find out what the next iOS release will be doing to further thwart device tracking.  What new feature is Android 6+ releasing?  What's the latest on the forthcoming seventh branch of the U.S. military?  Why has Russia suddenly criminalized open source contribution?  And what do we learn from VirusTotal's 2023 "malware-we've-seen" update?  Then, after we share some of the terrific podcast-relevant feedback received from our amazing listeners following last week's second satellite insecurity podcast, we're going to examine one of the revelations to be detailed during next week's Black Hat hacking conference in Las Vegas.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  We're going to talk more about satellite communications.  We've got some really expert listeners, some fascinating insights into that.  We'll also talk about Russia.  They've actually criminalized open source contribution.  And then VirusTotal's 2023 malware-we've-seen update.  Plus a look at a radio solution used by law enforcement all over the world that is woefully insecure.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 933, recorded Tuesday, August 1st, 2023:  TETRA:BURST.



It's time for Security Now!, the show where we talk about your security, your privacy, your health and welfare online with this guy right here, Mr. Steve Gibson of the Gibson Research Corporation.  Hi, Steve.



STEVE GIBSON:  Hello, Leo.  Great to be with you for this first day of August.	



LEO:  Yeah.	



STEVE:  We're in the low 80s here, so we're in, like, paradise compared to the rest of the country.  It's either thunderstorms and tornadoes, and my sister posted something on Facebook with like some huge lightning storm she was in in Colorado yesterday or last evening.  And of course Arizona's breaking records at 110 degrees or higher for more days in a row than they've ever had.  And here we are, it's a little humid, but otherwise it's great.



LEO:  Really.  That's nice.  You don't get a lot of humidity, do you?  You're in an arid clime.



STEVE:  No, it's odd.  It's odd for us, yeah.  Some El Nino thing happening.



LEO:  Oh, yeah.	



STEVE:  And I'll just note that we're two weeks away from finishing our 18th year of this podcast.



LEO:  OMG.  18 years.  Wow.



STEVE:  Yeah, I think it's August 18.  Or, yeah, I think it's - I think it is August 18 is our 18th, is the end of year 18.  We'll begin into 19.  So wow.  Very cool.  So it turns out that Advanced Persistent Threats have been leveraging satellite communications for many years.  So before we wrap up all of our staring-at-the-heavens discussion, we're going to look at that.



LEO:  We should, and I hope you will, and I know you will, talk about what - it's a terrible name, Advanced Persistent Threat, for what it really is.



STEVE:  Yeah.



LEO:  But I'm sure you'll explain that.



STEVE:  Yeah, yeah.  Also we're going to find out what the next iOS release will be doing to further thwart device tracking.  And I know you touched on that in your previous podcast on MacBreak Weekly, and also what new feature Android 6 is releasing.  But you also cast some doubt on whether that was happening on MacBreak Weekly.



LEO:  Yeah.



STEVE:  So I'm kind of curious to see whether we're talking about the same thing.  Also we've got some news on the latest forthcoming seventh branch of the U.S. military.  And we're going to wonder why Russia suddenly criminalized contributions to open source software, which is bizarre.



LEO:  Can you believe that?  Ugh.  Because they don't control it; right?



STEVE:  Yeah, huh?  And what do we learn from VirusTotal's 2023 "malware-we've-seen" update?  Then we're going to share - we got an amazing amount of terrific podcast-related feedback from our astonishingly varied listeners.  We've got more people who know about satellite security, it turns out, who had some interesting stuff to add to our discussion last week, which I'm going to share.



And then we're going to examine one of the revelations to be detailed during next week's upcoming Black Hat hacking conference in Las Vegas, thus the title of today's podcast, TETRA:BURST.  It turns out that when Europeans design a secure radio protocol that has four different encryption algorithms, which they allocate to different countries, you've got to say, "What?  Why?"  So everybody gets to use TEA1 except the European military gets to use TEA2.  Don't you kind of want to wonder why?



LEO:  You get an algorithm, and you get an algorithm, and you and you and you.  Wow.



STEVE:  That's right.  And they're all secret and unpublished.



LEO:  Oh, that's not good.



STEVE:  And it turned out that only by leveraging some zero-days in a Motorola implementation of this encrypted handset were a bunch of guys in the Netherlands, researchers in the Netherlands, able to crack the Secure Enclave to for the first time ever get access to these proprietary encryption algorithms.  And, oh my god, would you believe that they're not secure?  So anyway, we're going to have fun today.



LEO:  Why is it that people roll their own?  I mean, it's not like the Enigma machine that it's security through obscurity; right?



STEVE:  Well, unfortunately, it's an attempt at that.



LEO:  It is, yeah.



STEVE:  The only way I can give them, well, I would give them an out is to say that it's decades old.  So this exists from the '90s.  But, I mean, I'm giving away a lot of the podcast coming up.



LEO:  All right, save it, save it.



STEVE:  But believe it or not, they've replaced them, having been caught, with new proprietary secret algorithms.  



LEO:  Now there's no excuse.  If there's one thing we know about crypto, it's got to be open, which is exactly why Russia doesn't like open.



STEVE:  Yeah.



LEO:  Now let's build the Picture of the Week, shall we, Steve?



STEVE:  Okay.  This is just - it struck me as funny.  I've got a killer one coming for next week, but this was repurposing an old photo with a wonderful caption that has got nothing to do with security.  But I just got a kick out of it.



LEO:  Okay.  It's going to take me a second because I did not have it on this computer.  I have two different computers, and I've got to make sure I get the one with the Picture of the Week.  The good news is I haven't seen it yet.



STEVE:  That's correct.



LEO:  So you will see my genuine reaction.



STEVE:  First reaction.  It's just cute.



LEO:  We'll do it together [laughter].  Okay.  Okay, Mr. Spock.  Never play with super glue.



STEVE:  Never play with super glue.  This just shows Spock doing his Vulcan hand sign with the two fingers stuck...



LEO:  It does look like they're glued.  They might be stuck, yeah.  Like he was trying to wave.



STEVE:  It's funny because my best friend at the time, a guy named Gary Rawlings, was my best man in my first wedding.  And I said, "Rawlings, do not embarrass me.  You're going to do the best man speech.  Whatever you do, you know."  Because, I mean, he knew where the bodies were buried, times, you know, to the power of infinity.  And so he was very dangerous to have up there onstage.  And he had like kind of a dry sense of humor where he could really go too far.  So, I mean, I put the fear of god into him.  And so he got up, he received the microphone, and he held his hand up.  And he said, "Gibson told me I was forbidden for saying anything really that would embarrass him.  So I'm just going to say 'Live Long and Prosper.'"  Now, Gary could not do the Vulcan hand sign.  So he had rubber bands around his...



LEO:  Oh, no. 



STEVE:  Around his fingers.



LEO:  That's hysterical.



STEVE:  In order to make them do that.  So anyway, I thanked him.



LEO:  I could do it with one hand but not the other.  I could do it with my non-dominant hand.  



STEVE:  I'm clearly a double - I'm a double Vulcan.  I can,  yeah, I can...



LEO:  I guess it takes practice.



STEVE:  ...animate them and do whatever they need to do, yeah.



LEO:  Okay.



STEVE:  Okay.  So before we wander away, as I said, at least for the time being, from the topic of satellite security - which turns out it's a rich field, I mean, there's been generated a huge amount of interest among our listeners.  So I'm glad that we spend some time talking about this last couple weeks.  I want to talk about another aspect of the use of satellites by bad guys, which again I wasn't aware of, but makes sense when you think about it, the deliberate routing of Internet connections through space.  This is done as a means of thwarting the persistent efforts by law enforcement to track down, shut down, and sometimes take over the command-and-control servers and infrastructure which is being used by the major advanced persistent threat groups.  Since it's another thing that we've never explicitly covered, I thought that now, while we're still looking skyward, would be a good time to add this to the growing list of things that we have covered.



So way back in September of 2015 - this is not news, this is eight years ago - Kaspersky published an informative research piece titled "Satellite Turla," T-U-R-L-A.  Turla is the name of an APT group.  And so their title was "Satellite Turla:  APT Command and Control in the Sky."



LEO:  What is an APT?  Can you, I mean, I know it's resident, basically, a resident infection.  Yeah?



STEVE:  I think the first time we encountered it on the podcast was when one was discovered at Sony Entertainment.



LEO:  Right.  They were wandering around for months inside the Sony systems.



STEVE:  Like a long time, yes.



LEO:  Yeah, yeah.



STEVE:  Yes.  And so that may have been where this notion of, you know, so Advanced Persistent Threat.  Advanced, obviously, means it's not some script kiddie doing, you know, up to nonsense.  This is a serious, a serious organizational intrusion.  Persistent meaning that, you know, again, it wasn't something that was executed and then died.  It established a foothold in some sort of corporate asset, and from there it then was used for surveillance over some long period of time.  We've seen printers for example, you know, no one would think of a printer as being a computer, but of course they are.  And their firmware is no more secure than anything else, unfortunately, these days.  And so we've seen APTs that set up shop in printers where, as I said, no one thinks to look.



LEO:  Right.



STEVE:  And then from there, you know, they're on the network, so they're able to go out and see what's going on.  So all of these things need some means of phoning home in order to report the things that they have found and also, as we'll see, to create a means for allowing the bad guys back in over time.  So Kaspersky in their write-up stopped short of explaining the detailed network packet flow.  But they did provide enough for us to fill in the rest of the technology.  So first, I've skipped over some of their warm-up introduction, which would be redundant for our audience, but I want to sort of create the background that they did create, and then we'll figure out how the packet flow works.



So Kaspersky said:  "When you are an APT group, you need to deal with many different problems.  One of them, and perhaps the biggest, is the constant seizure and takedown of domains and servers used for command-and-control.  These servers are constantly appropriated by law enforcement or shut down by ISPs.  Sometimes they can be used to trace the attackers back to their physical locations.  Some of the most advanced threat actors or users of commercial hacking tools have found a solution to the takedown problem, the use of satellite-based Internet links."  And again, this is in 2015.  So this has only matured since then.  "In the past," Kaspersky wrote, "we've seen three different actors using such links to mask their operations.  The most interesting and unusual of them is the Turla group.



"Also known as Snake or Uroboros, names which come from its top class rootkit, the Turla cyberespionage group has been active for more than eight years."  And that was more than eight years in 2015, and they're still a name that's around, so they've been at this for a while.  Kaspersky said:  "Several papers have been published about the group's operations, but until recently little information was available about the more unusual aspects of their operations, such as the first stages of infection through watering-hole attacks.



"What makes the Turla group special is not just the complexity of its tools, which include the Uroboros rootkit, aka 'Snake,' as well as mechanisms designed to bypass air gaps through multi-stage proxy networks inside LANs, but the exquisite satellite-based command-and-control mechanism used in the latter stages of the attack.



"In this blog, we hope to shed more light on the satellite-based command-and-control mechanisms that APT groups, including the Turla/Snake group, use to control their most important victims.  As the use of these mechanisms becomes more popular, it's important for system admins to deploy the correct defense strategies to mitigate such attacks.  For IOCs" - remember, indications of compromise - "see the appendix.



"Although relatively rare, since 2007 several elite APT groups have been using  and abusing  satellite links to manage their operations, most often their command-and-control infrastructure.  Turla is one of them.  Using this approach offers some advantages, such as making it hard to identify the operators behind the attack, but it also poses some risks to the attackers.  On the one hand, it's valuable because the true location and hardware of the command-and-control server cannot be easily determined or physically seized.  Satellite-based Internet receivers can be located anywhere within the area covered by a satellite, and this is generally quite large.  The method used by the Turla group to hijack the downstream links is highly anonymous and does not require a valid satellite Internet subscription.  On the other hand, the disadvantage comes from the fact that satellite-based Internet is slow and can be unstable.



"In the beginning, it was unclear to us and other researchers whether some of the links observed were commercial Internet connections via satellite, purchased by the attackers, or if the attackers had breached the ISPs and performed man-in-the-middle attacks at the router level to hijack the stream.  We have analyzed these mechanisms and come to the astonishing conclusion that the method used by the Turla group is incredibly simple and straightforward, as well as highly anonymous and very cheap to operate and manage.



"Purchasing satellite-based Internet links is one of the options APT groups can choose to secure their command-and-control traffic.  However, full duplex satellite links can be very expensive."  Now, this is in 2015.  "A simple, duplex, 1Mb up/down satellite link may cost up to $7,000 per week. For longer term contracts this cost may decrease considerably, but the bandwidth still remains very expensive."  And again, this is back in 2015, so things may have changed since.



"Another way of getting a command-and-control server into a satellite's IP range is to hijack the network traffic between the victim and the satellite operator, and to inject packets along the way.  This requires either exploitation of the satellite provider itself, or of another ISP on the way," you know, in line.  "These kinds of hijacking attacks have been observed in the past and were documented by Renesys, now part of Dyn, in a blog post dated in November of 2013, so two years before this one was written in September of 2015.



"According to Renesys:  'Various providers' BGP routes were hijacked, and as a result a portion of their Internet traffic was misdirected to flow through Belarusian and Icelandic ISPs.'  They said:  'We have BGP routing data that show the second-by-second evolution of 21 Belarusian events in February and May of 2013, and 17 Icelandic events in July through August of 2013.'



"In a more recent blog post from 2015, these researchers point out that:  'For security analysts reviewing alert logs, it is important to appreciate that the IP addresses identified as the source of incidents can and are regularly spoofed.  For example, an attack that appeared to come from a Comcast IP located in New Jersey may really have been from a hijacker located in Eastern Europe, briefly commandeering Comcast's IP space.  It's interesting to note that all six cases discussed above were conducted from either Europe or Russia.'"



Okay, now, they write:  "Obviously, such incredibly apparent and large-scale attacks have little chance of surviving for long periods of time, which is one of the key requirements for running an advanced persistent threat operation.  It's therefore not feasible to perform the attack through man-in-the-middle traffic hijacking, unless the attackers have direct control over some high-traffic network points, such as backbone routers and fiber optics."  And of course that's unusual, too.  They said:  "There are signs that such attacks are becoming more common, but there is a much simpler way to hijack traffic-based Internet traffic.



"Enter satellite link DVB-S hijacking."  They said:  "The hijacking of satellite DVB-S links has been described a few times in the past, and a presentation on hijacking satellite DVB links was delivered at Black Hat in 2010 by an S21Sec researcher.  To hijack satellite DVB-S links, one needs the following:  a satellite dish - the size depends on geographical position and satellite; a low-noise block downconverter," typically called an LNB, and that's generally part of the satellite that you get mounted on your roof if you're subscribing to Dish network or whatever.  You also need "a dedicated DVB-S tuner" - which takes the form of a PCIe card these days - "and a PC, preferably running Linux."



They said:  "While the dish and the LNB are more-or-less standard, the card is perhaps the most important component.  Currently, the best DVB-S cards are made by a company called TBS Technologies.  The TBS-6922SE is the best entry-level card for the task."  And that can be had for about a hundred bucks.  "The TBS card is particularly well-suited to this task because it has dedicated Linux kernel drivers and supports a function known as 'brute-force scan' which allows wide-frequency ranges to be tested for interesting signals.  Of course, other PCI or PCIe cards might work as well, while in general the USB-based cards are relatively poor and should be avoided.



"Unlike full duplex satellite-based Internet, the downstream-only Internet links are used to accelerate Internet downloads and are very inexpensive and easy to deploy.  They're also inherently insecure and use no encryption to obfuscate the traffic.  This creates the possibility for abuse."



Okay.  So Kaspersky's article, as I said, did not go into any more detail about how this works.  They switched to providing tables of IP ranges that had been observed in the past and noted the satellite Internet service providers that were using those ranges.  But fortunately, we have all the information we need to understand the advantage this gives to anyone who is attempting to hide their command-and-control server.  The key is that these Internet communication satellites have extremely broad coverage areas, coupled with the fact that, just like the Internet, the IP packet traffic being carried is not, itself, encrypted.  As we know, TCP and UDP are not encrypted protocols.  They're just carriers of data that today is typically encrypted.  That is, the data they're carrying is encrypted, but they themselves, the actual underlying protocol is not an encrypted protocol.



Okay.  So imagine that some nasty advanced persistent threat malware has been surreptitiously placed into a high-value computer, and that more than anything the bad guys do not want their command-and-control infrastructure - which this malware will be reaching out to, to receive instructions and updates and things - to be discovered, commandeered, and shut down.  Presumably this APT threat group has many such infestations which are all reusing the same infrastructure.  So the loss of that command-and-control server would cripple the entire network that they had established.  



Okay.  So they have their APT malware periodically send a UDP packet to the IP of a previously chosen customer - probably a big stable customer - of a given satellite-based Internet provider.  Having the malware send an outbound UDP packet has the effect of opening up return paths through any NAT routing and firewalls that would otherwise prevent unsolicited traffic from entering the enterprise's network and reaching the malware-laden machine.  So you want the malware to initiate communications, which actually works in favor of this whole architecture.



So this UDP packet is sent out to a previously selected customer of a satellite ISP.  So it will be received first by that ISP.  So this is a block of the ISP's IP space.  It comes to the ISP.  But unlike other ISPs, the received packet is beamed upstream directly at a chosen communications satellite.  This causes it to then be rebroadcast out across the entire coverage area of the satellite indiscriminately.  Somewhere, down on the ground, is that subscriber of that Internet ISP.



But also somewhere else, anywhere else within that satellite's large coverage area, the malicious command-and-control server is silently lurking with its own satellite dish passively aimed up at the ISP's broadcasting satellite.  It patiently listens for any UDP packets addressed to that IP.  Since the subscriber will likely have their own NAT router or firewall that will simply ignore any unsolicited nonsense, as everything has to these days, and since that subscriber may have been pre-selected to make sure that that's true, their receipt of that incoming packet will be ignored; right?  It's just a radio packet coming in on their satellite dish.



But it will be what the malicious command-and-control server base station has been waiting for.  Upon receiving that UDP packet, the base station can reply by sending its own UDP packet via terrestrial ground Internet, since there's no need for it to be returned to space.  Right?  It's just an IP packet.  So they can drop it on any IP connection and it'll find its way back to the original malware that initiated it.  This allows the command and control system to send whatever commands it may wish back to the querying machine.  And the traffic doesn't need to only be UDP.  It's just sort of easier for this example.  But nothing prevents the listening command-and-control base station from establishing a three-way handshake and bringing up an encrypted TCP connection.



The key to the hack is that it's the world's largest air gap.  The outbound traffic is being sprayed over a huge geographic area, to be picked up by a totally passive satellite dish that there's no way of locating, and it could be anywhere.  It could even be mobile, you know, within the range of the satellite.  And the command-and-control system's IP address being used is someone else's, not theirs, and so it's an air gapped man-in-the-middle traffic interception attack that is going to work and prevent the command-and-control server from ever being discovered.  So unfortunately you have to give the bad guys some credit for this hack.  It's pretty slick.



Okay.  So Apple just updated its developer program to further crack down on developers who are abusing some of its API features which are being used to collect data on user devices.  And they're doing that as an underhanded means of tracking them online.  Apple said that even if a user has given an app permission to track their activity, fingerprinting the underlying device is still not allowed, yet it is still going on.



So with the release of iOS 17 and macOS Sonoma this fall, developers who want to continue to have access to these features, which could and have been used to enable persistent device-level tracking, are going to have to provide a valid reason to Apple for having that right.  Apps that don't provide a good reason will not be accepted on the App Store as soon as iOS 17 rolls out, and Apple begins to enforce this policy.  And Leo, I'm astonished by the apparent value added by this tracking.  I mean, tracking, it must be that it provides so much more benefit to advertisers above and beyond just putting their ad on a page where it makes sense for their ad to appear.



LEO:  Well, I have strong opinions about this.  Advertisers think it provides value.  There's a lot of evidence that personalized ads don't in fact work better.



STEVE:  I agree.



LEO:  Yeah.  But if you're an advertiser, think about it.  You would, I mean, there's a famous saying that I know that half of my ads work, I just don't know which half.  They would love some idea that they're hitting an audience that's interested in buying, for instance.  They haven't been able to, you know, on TV you really can't do that if you buy network television.



STEVE:  Right, right.



LEO:  That's why it's mostly brands on network television.  They know, well, we're enhancing the brand, Pepsi or Budweiser.



STEVE:  Right.



LEO:  And so that million dollar ad on Super Bowl is worth it.  But for podcasts and websites and a lot of the digital world...



STEVE:  We have some targeting.



LEO:  You can target.  You know, Facebook and Google live on this.  And it makes them feel better.  I don't know if, you know, there's a third category of advertising which is the advertising we do, which is called Direct Response advertising.  That's why we always have a URL.



STEVE:  Right.



LEO:  Or, you know, on late-night TV you'd see an 800 number or an offer code.  That's another way of an advertiser kind of reassuring themselves that their advertising is working.  They're all imperfect.  And all the studies I've seen say that tracking is not a very effective way of, you know, that targeting your ads doesn't really make that much of a difference.  But advertisers believe it.  And maybe even if they know better, the agencies need something.  They're grasping at straws.



STEVE:  Maybe what's happened is that this is all to support that sketchy data broker business.



LEO:  No, no, no, I don't - I think that's a wonderful side business for the companies that sell it.  But remember Google doesn't sell the data brokers space because they sell the data brokers.



STEVE:  So they're doing it for their own purposes, only internal.



LEO:  Well, they're doing it because advertisers demand it.  I mean, that's why we do it.  We do very limited tracking.  As you know, a podcast, it's impossible to know with RSS feed anything but the IP address of the visiting computer.  And we don't do more than that.  But we do use services, a variety of different services.  Right now we're using a thing called Podsights that they're an independent third party.  We send them the IP addresses of people who listen to Security Now!.  And the advertiser sends them the list of IP addresses of people who visited their site.  The third party goes, okay, 32% of the people who heard the ad visited your site.  They don't give the information to the advertiser.  We don't get the advertiser's information.



STEVE:  So there's no matching of IPs.



LEO:  It's only done by the third-party in a private way.  And, even that I resisted.  But honestly, we would not be able to sell advertising because - and that's the thing.  The advertisers, I don't think it's...



STEVE:  They're spoiled; right?  They've gotten spoiled.



LEO:  It's not even they're spoiled.  They just - they have a faith, a firmly held belief that this information helps them.  And they refuse to buy - they'll only buy ads where they can get that information.  Frankly, we're lucky.  We have a hard time selling ads against people like Google and Facebook, who will say, I can give you 25-year-old to 30-year-old men in Petaluma, California.  Would you like that?  Or I can give you people with income over $100,000 who live in the Northwest.  Would you like that?  We can't do that.  You know, all we can say - so we are losing, frankly.  We're losing out to Facebook and Google, which have about 88% of all the online ad sales, because they offer that kind of information.  So they're going to keep doing it.



And, you know, you see Google doing all sorts of maneuvers to get us to trust them.  So they've turned off cookies, and we were talking the other day about this new web integrity initiative that they're proposing, they're going to build into Chrome.  That's just one more way of them knowing who's there. And advertisers insist on it.  So they think they have to do it.  Whether anybody believes in it working, I don't know.  But they think they have to do it.  We have to do what we have to do, or we would have zero advertisers.  As it is, we lose a lot of ads because we can't give them, you know, people just go, well, I'm going to buy Facebook, or I'm going to buy Google.



STEVE:  Wow.



LEO:  So some of our advertising, not so much on this show, but some of our advertising is now direct insertion where we use a company, LibSyn company called AdvertiseCast.  We just started doing this.  And we pause, put a little trigger in there, and they stick in an ad.  And those, advertisers like them a little bit better because they can geographically target.  Your IP address has a rough geographic location.  So when one of our shows airs in Spain, for instance, a Spanish advertiser will buy that, knowing that, well, I'm only - because they don't want to buy U.S. listeners because they're not customers.



STEVE:  Right, right.



LEO:  So they'll have an ad, and they say, well, we know these are the people who listen to the show in Spain.  Here, you can have that.  So it's another form of targeting.  But, you know, advertisers, they demand it.  And if you're an ad-supported media company, you have to find a way to balance your - we believe in our community, and especially your listeners, they don't want to be tracked.



STEVE:  No.  And in this case, so we have Apple who's trying to thwart the surreptitious, underhanded device tracking.  They haven't...



LEO:  But they have all those ads themselves, this information themselves.  So they have first - this is what we were talking about earlier, as you heard on MacBreak Weekly, is first-party tracking, like Facebook and Google and Apple do.  And of course what they're really saying is we want this to ourselves.  We don't want some app on your phone to have the information we have, the relationship with the customer we have.  They're not saying we don't want advertising, we don't want to track you.  They're saying, we don't want them to track you so we can.  It's our advantage.  So I'm a little - I'm cynical about this whole thing.



STEVE:  Yup.  Well, and there's a different form of tracking that you also touched on, and that's a more deliberate form, and that's back into the deep dive that we took a couple months ago on AirTag tracking technology.



LEO:  Oh, yes.



STEVE:  As we know, this AirTag tracking technology is Bluetooth based, so it's inherently crowd-sourced.  So this of course relates to the Apple and Google agreement.  It's in both parties' interests, Apple and Google, to have a single common standard which they share so that both Apple and Android handsets can provide the tracking location feedback for each other's ecosystems.  And so what they announced when we talked about this a couple months ago was a joint specification.  But it was really indistinguishable from what Apple had already been doing for several years with their AirTags.  So what appeared to have actually happened was that Apple had opened their specification for Google, and Google was happy to take it because, you know, they already had an established ecosystem, and then people would be able to use their Android phones as track feedback devices, as well.  So it's good for everybody.



Last Thursday's news is that Google would soon be adding "Unknown Tracking Alerts" to Android.  They said in their announcement, "Unknown tracker alerts, which we announced at I/O 2023, are beginning to roll out in Android 6.0+ users this month."  And they also said, "Unknown tracker alerts currently work with Apple AirTags."  And of course other third-party tags.  They said:  "We'll continue to work with tag manufacturers to expand this important protection to other tracking tags over time through our joint industry specification."  Now, what you had seen was story that said that was going to be put on hold.



LEO:  Yeah.



STEVE:  To the end of the year, which, I mean, and so I'm not sure if unknown tracking alerts, I mean, that's only - unknown tracking alerts is one aspect of the whole AirTag tracking.  The other side being, you know, you own AirTags, and your device is telling you where they're located.  So that's different than being aware of an AirTag that is traveling with you.  So maybe we're talking about two different things, or maybe we're talking about everything being on hold till the end of the year.  Now I'm not sure.



LEO:  Yeah.  So I was fooled, too, because the headline of the article I was reading said, "It's rolling out."  And the last paragraph of the article is - so Google announced it at Google I/O in May.  I mean, the problem is you can have AirTags following you around.  Unless you've installed an app on your Android phone, it doesn't know about AirTags.



STEVE:  Right.  Well, installed and it's running.



LEO:  And it's running, and by the way doesn't work very well.  So it was reasonable for Google and Apple to try to solve this problem by Google building it into Android and so forth.  And so Google announced it in May that they were going to do this at Google I/O.  They had thought they were going to put it in Android.  I think Android 14 is soon.  But this article...



STEVE:  And I guess 6.0 is the kernel version.



LEO:  Right.  So this article, I'm reading along, and they're going to do it, they're going to do it.  And then the last paragraph of the article is Google has announced it's putting this off until the end of the year because of the Apple Google Consortium.  They want to work it out between the two of them.  So I don't think it's in there now.  I know I was very confused by this personally.  So I don't - they've promised it.  And we need it.  But is it here?  I don't know.



STEVE:  Yeah.  And so what I picked up on said, you know, are beginning to roll out to Android 6.0+ users this month.  So...



LEO:  I saw that, too.  And that was the same article that then said at the end of the article, well, except, no. 



STEVE:  Except not.



LEO:  I'm very confused by the whole thing.  Let me see if I can find the article I read because I bet it was the same as the one you read.  I bookmarked it in my thing here.  And I think it was almost - it was as if they had written the whole article and then did a "never mind" on the whole thing.  So, yeah, the headline of this article is, just as you said, "Android will now warn about unknown Bluetooth trackers like AirTag traveling with you."  Sarah Perez writing for TechCrunch, July 27th.  "Google today will begin to roll out a new safety feature, unknown traffic alerts."  But then go down to the bottom, same article.  "Today, however, Google says this update is on hold."



STEVE:  Wow.



LEO:  Wait a minute.  Also announced, Google said it would update its - okay.  So I guess the alerts are there, but they are not updating the Find My network to work with third party.  I guess that's just what you said.



STEVE:  Okay.  Okay.



LEO:  So if you read this carefully, which I didn't, apparently, the update that's on hold is updating Find My Device to work with third-party trackers.  So they are going to - but then it says the decision was made to roll out these updates because Google is now working in partnership with Apple to finalize the joint unwanted tracker alert specification by year-end.



STEVE:  Wow.  Really confusing.



LEO:  I think we are getting the alerts.  I think it was just a poorly written article, that we are getting alerts.



STEVE:  I think that is right.  With Android 6.0 kernel, you'll begin to be told if something is traveling with you, the other stuff to come later.



LEO:  Yeah.



STEVE:  And it does, it says, currently work with Apple AirTags.  We will continue to work with tag manufacturers to expand this important protection.  So I don't understand why it's not working with everybody because, you know [crosstalk] unified standard.



LEO:  It all works the same; right?  Yeah.



STEVE:  Yeah.  But apparently not.



LEO:  Also announced, Google said it would update its Find My Device network to help users locate other missing belongings, which can be located by third-party Bluetooth trackers.  Now, Google doesn't sell a tracker.  So anything Google works with is third party, including AirTags, Tile, Chipolo.



STEVE:  And I would say they don't "yet" sell one because, boy, I'm astonished by how popular Apple AirTags are.



LEO:  Yeah.  Oh, they're dominant.



STEVE:  They shared the numbers, it was millions of them were selling.



LEO:  And I think what, really, there's lots of ways to track people.



STEVE:  And you know there's one fewer AirTag in use now, Leo, after you...



LEO:  By the way, Burke gave me a much better hammer for next time I want to destroy something.  I now have a mini sledgehammer.  He didn't understand fully my plan.



STEVE:  Okay.  So the National Defense Authorization Act, which successfully passed through the U.S. Senate last week, included a provision requiring the National Academy of Public Administration, whatever the hell that is, to conduct an assessment on the feasibility of establishing a new, formal, seventh branch of the U.S. military, which we've talked about several times, the U.S. Cyber Force.  So this does appear to be happening.



Since many of our listeners have explained that wearing ridiculous camouflage clothing indoors is a bizarre requirement of the U.S. military - now, that's my word "bizarre," not theirs - perhaps at least the Cyber Force's camo could have some cool cyber theme, like maybe like those green falling and fading symbols from The Matrix?  Or maybe just do the whole thing as in ones and zeroes.  That would be very cool; right?  Like make camo out of ones and zeroes.



LEO:  Well, remember it's supposed to be camo.  I think why can't they just make something that makes you invisible?  I mean, let's do it.



STEVE:  That'd be really good.



LEO:  Yeah, yeah.



STEVE:  Make it stealth, a stealth camo.  That'd be good.



LEO:  Ooh, yeah.



STEVE:  Anyway, I do hope that someone gives this as much thought and serious consideration as is clearly needed because this U.S. Cyber Force, if they're going to have to wear some ridiculous outfit, let's make it techie and cool.



LEO:  So a number of our listeners are saying, including in the Discord and somebody in the U.K. that they do have these alerts now on their Android phone.  So it did roll out, yeah.



STEVE:  Oh, cool.  Yay.  Good.  Thank you.



LEO:  [Crosstalk] listeners.



STEVE:  Super use for feedback, yeah.  Now, the other wrinkle is that both the Army and the Air Force, you know, obviously well-established branches of the military, have recently created their own new specialized cyber teams to support their traditional "kinetic teams," as we're calling them, you know, with cyber tasks related to intelligence gathering, electronic warfare, and sensors.  And I think that makes sense, since those cyber teams which support the traditional kinetic forms of warfare are probably going to be highly targeted and specialized for their specific tasks; whereas the military's new seventh branch would be far more wide-ranging and not at all focused upon specific current Army and Air Force military operations.



So anyway, but through all this it is quite obvious that cyber - I know you love that term standing by itself, Leo - cyber has well and truly arrived, both on the front lines and soon in dimly lit dens filled with monitors and empty caffeinated beverage cans.



LEO:  And pizza boxes.



STEVE:  So I want to know what they're going to be wearing.  That's all I'm saying is, you know...



LEO:  We should explain.



STEVE:  So for some reason this really matters to me.



LEO:  We should explain that a couple of weeks ago Steve found a photo of the Cyber Defense Command, and they were all wearing BDUs, Battle Dress Uniforms, that were camouflaged.  But obviously they're not in the jungle, so...



STEVE:  They're in a room.  They're not even - you can't even observe them from satellite reconnaissance.



LEO:  And we have to find some stealth uniforms for them.



STEVE:  Yeah.



LEO:  Tempest uniforms.



STEVE:  Okay.  So meanwhile, Russia continues to separate itself from the West.  The Russian Parliament just passed three bills which, once signed into law by Putin, will ban Russian citizens from participating - I know.



LEO:  This is crazy.



STEVE:  ...in the activities of foreign nonprofit organizations that have not specifically registered with the Russian government, and none have.  Commentary about this over on opennet.ru notes that an unintended side effect will be that Russians using open source software would be prevented from contributing in any way to those projects, even from submitting bug reports.  Now, as we know, today's open source software includes Linux, Firefox, most major database systems, and programming languages.  Now, I read the entire piece after having Google translate it into English for me, and it only talked about the unintended consequences.  I was unable to determine what the intended consequences of the three pending bills would be.  Why would Russia think this was a good idea?



LEO:  I know one of the reasons repressive regimes pass bills like this is for selective prosecution.



STEVE:  Okay.



LEO:  So, you know, they need a way to get you.



STEVE:  If they want to stomp on somebody they have a law.



LEO:  Oh, what's the company of Linux doing there?  You're in trouble, big boy.  That kind of thing.  Yeah.



STEVE:  Yeah.



LEO:  I mean, I can't imagine they want to stop all open source.



STEVE:  I mean, they're using it.



LEO:  Yeah.  The Russian official operating system is a Linux-based operating system.



STEVE:  Yes.  And there are lots of really good Russian teams that are doing good work.



LEO:  Well, maybe that's who they're stomping on.



STEVE:  You want to check the source code, but still.



LEO:  Yeah.  Maybe that's what it's all about.  You know, as kind of retaliation for the sanctions or something.



STEVE:  We don't want you to have any of our stuff; right.  So VirusTotal is out with their look at 2023 to date.  It's always interesting since they've got a good snapshot, since everybody is submitting stuff to them, you know, whenever I, as I've mentioned before, when I download some old archive from some sketchy-looking site, I immediately hand it to VirusTotal to see what it thinks, just because, you know, it's better to be safe than sorry.  So they get a really good snapshot of this.  So they have some main takeaways from their most recent update.



First of all, email attachments, to no one's surprise, continue to be the most popular way to spread malware.  However, traditional file types - Excel, RTF (Rich Text Format files), CAB, and compressed formats - are becoming less popular.  Although the use of PDFs slowly decreased for the last few months, starting in June of 2023, the biggest peak in PDF usage was observed during 2023 compared to the previous two years.  So PDFs are still a big deal, maybe a little summer slump for some reason.  However, the big changes are in OneNote.  OneNote and JavaScript, both distributed through HTML, are the most rapidly growing formats for malicious attachments in 2023, with OneNote emerging this year 2023 as a reliable alternative for attackers to the traditional use of macros in other Office products.



Malicious OneNote files usually embed an additional malicious file.  So OneNote is just sort of serving as a recognizable container that seems benign.  And I guess leave it to Microsoft, their various security permissions allow OneNote to be opened when you click on something on a web page, so yeah, let's have OneNote bring it in.  So OneNote files usually embed an additional malicious file - a VBA, HTML and JavaScript, PowerShell, or some combination of those.  And as happens with malicious Office attachments, the attempt is then made to convince the user to allow its execution.



Payloads vary from one malware family to another, but many of them access external URLs to then download a DLL file which is camouflaged as a .PNG, you know, which is an old trick used to bypass simple firewall rules or just to appear less suspicious to anybody who knows to look.



The most usual kill chain, as it was noted and stated, where OneNote format is involved is three steps.  The victim receives an email with a OneNote attachment.  The mail body encourages the victim to click on a button to see a hidden or distorted image or document.  Second, this button executes a script - VB script, a powershell, or whatever.  And that will launch a payload, either embedded into the same script or downloaded from an external resource.  And then finally the external payload might be yet another OneNote file, an image file renamed as a ".bat" file, a DLL file that's loaded into memory or even a Windows executable.



So we have inherently dangerous capabilities mixed with social engineering attacks.  And only one mistake made by one curious or inattentive employee within a major organization is all that's required to invite the malware in to set up shop and, who knows, contact a satellite Internet provider in order to say, hey, I made it in, what do you want me to do?



Following behind OneNote, ISO image files for malware are now a flexible alternative for both widespread and targeted attacks.  And their distribution as heavily compressed attachments makes them difficult to scan by some security solutions.  So says VirusTotal.  ISO files are being disguised as legitimate installation packages for a variety of software, including Windows, Telegram, AnyDesk, and Crypto Notepad, among others.



Virus Total said that - they said:  "Our data shows that there was an increase in the number of malicious files attached to emails between March and April of 2023.  In terms of suspicious attachments, for the past two years we have observed spikes in the number of suspicious PDF files linked to malicious campaigns.  These files can be used for a variety of purposes, such as exploiting vulnerabilities or phishing, which is what happens most of the time."



And they said during 2023 so far they saw a significant increase in the use of JavaScript distributed alongside HTML, used in sophisticated phishing attacks which were designed to steal victims' credentials.  Excel, RTF, CAB, and compressed formats, as I mentioned before, and Word, interestingly, seem to be declining in popularity along with the others as malicious attachments compared to OneNote and JavaScript.  So that's the wrap-up on what's been happening so far in 2023.  And we should have already taken a break, Leo, but let's do it now.



LEO:  I'm ready.



STEVE:  And I'm going to share some amazing feedback from our listeners.



LEO:  Okay, Steve.  On with the show.



STEVE:  It's funny, too, when you're talking about password managers, I just - I can't imagine life without one.



LEO:  It's so much easier once you're used to it; right?



STEVE:  Well, and I think that - yes, that.  And maybe 10 years ago, 20 years ago, well, I mean, people had four or five online accounts, you know what I mean, because there wasn't that much to do.  There wasn't that much going on online.  Now our lives are online.  And, you know, I mean, all of our utilities we have accounts for, and all of our various services we have accounts, and if you want to grab a car and drive somewhere, I mean, I just, you know, all of the airlines you have accounts, I mean, everything.  And so if they're going to all have their own password, you just have no choice.



LEO:  You have to.  Even if it were memorable you'd have to.  But I can't tell you how many people I know in my own personal family even, who know better, but you know, it's just, well, [mumbling].  Patrick Delahanty, our engineer, says his dad, who is a U.S. attorney, has a little black book of passwords.  The problem is, yeah, you can do that.  But then you have to generate unique passwords each time.  It's just easier to use a password manager and let it do the heavy lifting.



STEVE:  Yeah.



LEO:  I think easier than putting it in a notebook.  You don't have to write it down.  You don't have to write it down.  You don't have to remember.  You don't have to look it up.  It just does it.  Anyway.



STEVE:  Okay.  So some feedback.  Jeff Parrish, he said:  "Thank you for another great episode.  I am IT for a healthcare facility, and this episode" - referring to last week - "made me review the HTML of our EHR provider.  I have now contacted them about the Google Analytics tracking they have on their site after we are logged in."  So that was cool and useful to at least one of our listeners.



Actually, another one, Robert C. Covington, he said:  "Longtime listener.  I oversee cybersecurity for a large children's hospital system.  Your podcast transcripts are frequently on my screen during team meetings."



LEO:  Yay.  Wow, that's awesome.



STEVE:  Yeah.  He said:  "Regarding website tracking and the recent OCR notice referenced in Episode 932" - last week - "there is a side consequence I've not heard mentioned.  Cyber insurance companies are now declining to cover any legal actions arising out of website tracking and collection of personal health information.  This is sending many healthcare orgs scrambling to get tracking tools off their websites.  Keep up the excellent work.  Robert Covington."  He said, oh, he said:  "P.S.:  You fell into the classic trap on 932. It is HIPAA, not HIPPA."  So thank you for the correction, Robert.  And very interesting.  That will certainly remove tracking from healthcare if they know that they're not going to get any insurance coverage from their providers if they do that, and anyone gets called out for having personal health information disclosed.  If there's trackers on there, sorry, your insurance won't cover that.  Wow.



Jon Dagle said:  "Hi, Steve.  Thanks for the shout-out on the 25th July episode."  That was last week.  He said:  "I am the 'neat guy' who you saw on TWiS (This Week in Space) talking about orbital debris."  And he said, he was joking, he said:  "I'm fairly sure you weren't referring to Geof, and I'm really sure you weren't referring to Rod, ha ha."  So he said:  "Thank you for your kind mention."  He said:  "I've been a Security Now! listener since Episode 1, proud SpinRite owner," he says, "and somewhere I have a certificate for a TWIT Brick."  He said:  "Pretty sure I've not missed a single episode, at least not a whole one.  At the beginning I was in the U.S. Air Force.  I stuck around for hobbyist purposes and with a plan to go into cybersecurity, but I made a detour into space policy.



"Orbital debris is a clear and present concern, if not actual danger."  He said:  "The space advocacy organization where I work considers this one of a handful of high priorities.  While there are a number of public sources for tracking objects in orbit, they don't all agree.  According to orbit.ing-now.com, a relatively approachable source," using that as a reference.  And he said:  "A high-level summary is available here," and I have a link to it, it's a long URL, nanoavionics.com/blog/how-many-satellites-are-in-space, and that's all hyphenated.  And I think I clicked that, and it showed a picture of the Earth.  And if that's an accurate depiction, it is a little sobering.  And I think it may be accurate because it actually shows some of Elon's satellite trains.



LEO:  4,500 SpaceX satellites, which is half of all satellites are SpaceX.



STEVE:  Yes, yes.



LEO:  This is not - not SpaceX, Starlink, which is from SpaceX.



STEVE:  Starlink from SpaceX, yes.



LEO:  I thought Starlink, oh, I was so happy when I, you know, oh, we're going to low-cost Internet coverage to every corner of the globe.  First of all, it's not low cost.  It's very expensive.  And second, he's going to put 42,000 satellites up.



STEVE:  Yes.



LEO:  This is only one tenth.



STEVE:  It's going to be Starjunk instead of Starlink.



LEO:  I mean, this is horrific.



STEVE:  So Jon said:  "There are about 7,700-8,400 active human-made satellites in orbit around our planet.  The vast majority, 90%, are in low-Earth orbit."  And so that's less than 1,000 kilometers up.  "About one third of these have been added in the past few years, mainly by SpaceX Starlink.  About 7% of the total are in geostationary orbit, where these are LEO satellites we were just talking about, the GEO are geostationary," he said, "with the remainder in medium-Earth orbit, very few of those."  He said:  "Almost 2,300 'inactive' satellites," meaning they're up there but they died or they're dead or their battery ran down or something.  And he said:  "Thanks for the shout-out and," he said, "the 'brush with greatness.'"  Jonathan, Washington, DC.  He's the Policy Chair for the National Space Society.  And, yeah, you had that on the picture, that beautiful picture of the Earth.  And I think, I can't see it there, but it showed like Elon's Starlink trains.



LEO:  Yeah.  There's lines, yeah.



STEVE:  Yes.



LEO:  The New York Times this Sunday had an article about the concern, the geopolitical concern that Elon, who is, let us say, seemingly slightly erratic, controls this Starlink system, and the Ukraine military relies on it for military communications.  And they're concerned.  They asked in May, the Times reported, they asked the federal government what's the deal with this Elon, and the government basically went, uh, we don't know.



STEVE:  Well, and we're also in bed with him, right, because now we're contracting with him to launch our major space payloads.



LEO:  Yeah.  I bet they're regretting that a little bit right now.  I mean, he just seems quite erratic.  I'll find this New York Times picture because it's actually animated, and it's quite good.  It's really, this looks like similar data because, yeah, you can see these Starlink trains in it.



STEVE:  Yeah.  So we have another listener, John Sutherland, whose Twitter handle is @JohnOrion, which I got a kick out of.  He said:  "I wanted to offer a bit of knowledge I had about U.S. military satellites.  I was active duty and what is now Space Force for 11 years, and I'm currently a contractor still supporting space.  I 'flew' SAT-COM for four years, then taught for seven years."



LEO:  Wow, that's cool.  What an audience.  We have amazing people in the audience.



STEVE:  We have amazing listeners, Leo.



LEO:  Blows me away.



STEVE:  It is great.  He said:  "I taught both classified and unclassified classes, so I am very familiar with where the line is for what's classified.  I can go right up to that line.  Having just finished the second part of Satellite Insecurity" -meaning last week's podcast - "I can share that, luckily, most of the problems you talked about are not as true for U.S. DoD satellites."



LEO:  Ah, I bet not, yeah.



STEVE:  Whew.  Thank goodness.



LEO:  Yeah, we're well-protected, I bet, yeah.



STEVE:  Yeah.  He said:  "The preconceptions that attackers would not have the equipment was never the case.  China and Russia have always had similar ground station capabilities as we have.  The oldest satellites I've worked with were developed in the late '80s, and they were highly encrypted and rolled keys constantly.  For communication satellites the data is just routed so encryption is as good as it could be on Earth and not subject to the satellites' age.  Controlling the satellites, i.e., moving them, changing configuration, is done with separate antennas that are monitored, and any communication with them is watched in real-time.  If someone did break this encryption, it would quickly be learned.



"As for physical attacks" - and this gets a little interesting with his choice of words.  "As for physical attacks, the arms of attacking satellites is only a start.  When we table-topped attacks and planned responses, TTP (tactics, techniques, and procedures), we looked at jamming, ASATs" - which he'll explain in a second - "mechanical arms, and lasers, jamming being the most common and ones we have actually seen happen.  Most jammers are big ground-based semi-trucks or ships that just try to overpower the uplink."  So they're just blasting the same satellite target, hoping that it won't be able to receive the actual signal.



He said:  "We have many mitigations to this, and I taught a class on RF Attack and Defense as part of operators' advanced training.  ASATs," as he uses the term, "as you talked about with blowing up satellites from the ground are extremely unlikely at this point.  We are much more concerned with small satellites with explosives, the idea being that an adversary would place and leave something small on a foreign satellite that could be triggered on demand at any time in the future."  Whoa.  So they're mining satellites without the satellites' knowledge.  They creep up, stick something sticky on the side that's a bomb with a radio, and then leave.  And that can then be detonated in the future.  So, I mean, what a mess, Leo.  Can you imagine, like everybody's satellites have all these bombs stuck to them from other hostile nations?



LEO:  Good lord.



STEVE:  It just...



LEO:  I really want to ask these - by the way, here's The New York Times animation.  This is 10 minutes of Starlink satellites.



STEVE:  Wait, in the future.



LEO:  No, no.  This is as of July, launched as of July 10th.  This is current.



STEVE:  Now.  Look at all of them.



LEO:  Yes.  Yes.



STEVE:  Oh, my lord.



LEO:  Yes.  I'm wondering if we're having second thoughts about letting Elon launch all of these.  This is crazy.



STEVE:  Wow.



LEO:  This is half of the entire satellite load.



STEVE:  And they're only in a train before they've distributed themselves?



LEO:  Yeah, they deploy from the train.  But if you look, it looks like there's groups of two and three in some places.  It's a really interesting - there's definitely method to the madness.  Yeah, those trains you see are not yet deployed.  They launch that way, and then they slowly deploy.  Isn't that wild?



STEVE:  Wow.



LEO:  I want our satellite experts, though, to tell me if I should worry about the Kessler Syndrome, Kessler Effect or not.



STEVE:  Right.



LEO:  So if you blow up a satellite, and then debris from the satellite then blows up five more satellites, and debris from those satellites blows up 25 more satellites, and on and on and on, could you occlude the night sky?  Or worse?



STEVE:  This has been demonstrated with dominos. 



LEO:  Yeah.  I'm starting to worry.



STEVE:  And it's not good.



LEO:  I know there are missions, that we're running missions and I think China is running missions to snarf up satellites like the Moonraker thing we were talking about.  But, boy, I just, I mean, what happens?  I mean, I guess when they've reached the end of life they just go through the atmosphere and burn up.



STEVE:  All I can say is we should hold onto our DVD collections because we do not want to become too dependent on space.



LEO:  On the Internet.



STEVE:  Yeah, on space-based Internet.



LEO:  Yeah, crazy.



STEVE:  So he finished, saying:  "I cannot talk to the mechanical arms as the line beyond which I cannot talk is around this.  But it's safe to say this has been looked at and is in some level of development by both sides."  He said:  "Lasers are not a threat to all types of satellites, but China and Russia have used lasers to blind sensors of low-flying 'spy' satellites.  This is hard to guard against, but we do equip satellites with shutters now.  And for satellites lacking shutters, we only need to spin them around."



And he finished:  "There's more that cannot be talked about, but with your level of technical knowledge and a little imagination you could get close to guessing what's going on.  I can tell you I've never been surprised when I got a security briefing."  So, very cool.



LEO:  Jiminy Christmas.  Thank you,  We have wonderful listeners.  We thank you all.  Just really fascinating.



STEVE:  And another one, Mikael Falkvidd.  Mikael is on the board of OWASP in Gothenburg, Sweden, and he's the guy who invited me to present SQRL to their group.  Well, it turns out that Mikael knows more than a little about satellite software.  He said:  "Regarding authenticated TeleCommands to satellites."  Now, we talked about this last week; right?  The idea being that TeleCommands are ways you tell satellites to do things.  But what the guys who reversed the firmware found was there was a surprising lack of authentication.



So Michael said:  "What satellite programmers are most afraid of is bit flips caused by single-event upset" - what is termed an SEU, a single-event upset.



LEO:  You mean cosmic rays?



STEVE:  Yes.  He says:  "...which happen due to radiation in space."  He said:  "Imagine that an SEU flips a bit in the key used to authenticate the TeleCommand."



LEO:  Right.



STEVE:  "Authentication would fail.  And guessing which bit or bits flipped could take some time."



LEO:  That's why you have ECC.  I mean, we have ways...



STEVE:  He says:  "There are of course mitigations, for example, using error correction codes or storing the key in multiple places.  But complexity is the enemy of reliability, and resources (compute, flash, ram) onboard satellites have been very scarce historically.  And people want reliable satellites, so they are hesitant to introduce new features.  'Flight-proven' [he has in quotes] is the mantra, so the old ways live on.  The risk of losing the satellite because of an SEU (single-event upset) has been deemed higher than the risk that the satellite is hacked.  Not an excuse today, but that's how the industry is."  And then he finished, saying:  "I have written software for two satellites."  And he said - yeah, like you said, Leo, our listeners are amazing.



LEO:  Wow.



STEVE:  "SEUs are also one of the reasons TeleCommands exist to write to any memory location.  NASA used this feature to restore a bit flip on Voyager 2 in 2010."



LEO:  Vger.



STEVE:  Vger, 33 years after its launch.



LEO:  Wow.



STEVE:  So Mikael also provided a link to a summary from JPL, the Jet Propulsion Laboratory in Pasadena, which documented events surrounding exactly this happening back in May of 2010.  Somewhat astonishingly, Voyager 2 remains alive and functioning to this day, though something happened with it just last week, which I'll get to in a second.  We last checked in on Voyager 2 nearly five years ago when, on November 5th of 2018, it became only the second spacecraft to ever exit our solar system's heliosphere.



And remember, Leo, we considered whether this event might break the simulation that Elon, among others, appear to be convinced we're all living within.  But so far the simulation appears to be holding.  We were wondering if Voyager 2 exited the heliosphere, was there a maximum radius at which the simulation would still be functioning and whether Voyager 2 might just spontaneously disappear because it got too far away.



Anyway, let's turn the calendar back 13 years to May 6th of 2010, when JPL wrote, they said:  "Engineers have shifted NASA's Voyager 2 spacecraft into a mode that transmits only spacecraft health and status data while they diagnose an unexpected change in the pattern of returning data.  Preliminary engineering data received on May 1st" - this would be May 1st of 2010 - "show the spacecraft is basically healthy, and that the source of the issue is the flight data system, which is responsible for formatting the data to send back to Earth.  The change in the data return pattern has prevented mission managers from decoding science data.



"The first changes in the return of data packets from Voyager 2, which is near the edge of our solar system, appeared on April 22nd.  Mission team members have been working to troubleshoot and resume the regular flow of science data.  Because of a planned roll maneuver and moratorium on sending commands, engineers got their first chance to send commands to the spacecraft on April 30th.  It takes nearly 13 hours for signals to reach the spacecraft, and nearly 13 hours for signals to come down to NASA's Deep Space Network on Earth.



"Voyager 2 launched on August 20th, 1977 [so, wow] about two weeks before its twin spacecraft, Voyager 1.  The two spacecraft are the most distant human-made objects, out at the edge of the heliosphere, the bubble the sun creates around the solar system.  Mission managers expect Voyager 1 to leave our solar system and enter interstellar space in the next five years or so, with Voyager 2 on track to enter interstellar space shortly afterward.  Voyager 1 is in good health and performing normally.



"Ed Stone, Voyager project scientist at the California Institute of Technology in Pasadena, said:  'Voyager 2's initial mission was a four-year journey to Saturn, but it is still returning data 33 years later.  It has already given us remarkable views of Uranus and Neptune, planets we had never seen up close before.  We will know soon what it will take for it to continue its epic journey of discovery.'"  Meaning at the point where he's talking about this, something broke, and Voyager 2 is no longer sending data back, the science data that they wanted.



And he said:  "The original goals of the two Voyager spacecraft were to explore Jupiter and Saturn.  As part of a mission extension, Voyager 2 also flew to Uranus in 1986 and Neptune in 1989, taking advantage of a once-in-a-176-year alignment to take a grand tour of the outer planets."  I just love this.  It is just so cool, you know, real science.



"Among its many findings, Voyager 2 discovered Neptune's Great Dark Spot and 450-meter-per-second (1,000 mph) winds.  It also detected geysers erupting from the pinkish-hued nitrogen ice that forms the polar cap of Neptune's moon Triton.  Working in concert with Voyager 1, it also helped discover actively erupting volcanoes on Jupiter's moon Io, and waves and kinks in Saturn's icy rings created by tugs of nearby moons.  Voyager 2 is about 13.8 billion kilometers, 8.6 billion miles, from Earth.  Voyager 1 is about 16.9 billion kilometers, 10.5 billion miles from Earth.  The Voyagers were built by JPL, which continues to operate both spacecraft.  Caltech manages JPL for NASA."



Okay.  So May 6th, 2010, and something is broken and has gone wrong with Voyager 2 such that the spacecraft's science data is no longer being properly formatted.  Eleven days later on May 17th, 2010, we learn what went wrong:  "Engineers at NASA's JPL said Monday, May 17th, that one flip of a bit in the memory of an onboard computer appears to have caused the change in the science data pattern returning from Voyager 2.  A value in a single memory location was changed from a zero to a one.  On May 12th" - so that was - yeah.  So, "On May 12th, engineers received a full memory readout from the flight data system computer, which formats the data to send back to Earth.  They isolated the one bit in the memory that had changed, and they recreated the effect on a clone computer at JPL.  They found the effect agrees with the data coming down from the spacecraft.  They are planning to reset the bit to its normal state on Wednesday, May 19th."



And then three days later on May 20th we have the report of the conclusion of this high-stakes drama:  "Engineers have successfully corrected the memory on NASA's Voyager 2 spacecraft by resetting a computer bit that had flipped.  Reset commands were beamed up to the spacecraft yesterday, Wednesday, May 19th; and engineering data received today confirm that the reset was successful.  The Voyager team will continue monitoring the engineering data, and if the bit remains properly reset, commands to switch to the science data mode will be beamed up to Voyager 2 on Saturday, May 22nd.  Receipt of science data would then resume on Sunday, May 23rd."



And all of that did happen on schedule.  But I also noted that something else happened just last week.  NASA's blog posting Friday, July 28th of this year, read:  "A series of planned commands sent to NASA's Voyager 2 spacecraft" - right, still going strong - "on July 21st" - so toward the end of, like, just a couple weeks ago, toward the end of last month - "inadvertently caused the antenna to point 2 degrees away from Earth."  Now, when you're billions of miles away, 2 degrees, baby, I mean, you might as well be looking in the other direction.  So, "As a result, Voyager 2 is currently unable to receive commands [whoops] or transmit data [whoops] back to Earth.



"Voyager 2 is currently located almost 12.4 billion miles from Earth, and this change has interrupted communications [no kidding] between Voyager 2 and the ground antennas of the Deep Space Network.  Data being sent by the spacecraft is no longer reaching the Deep Space Network, and the spacecraft is not receiving commands from ground controllers."  Right.  "Voyager 2, however, is programmed to reset its orientation multiple times each year to keep its antenna pointed at Earth.  The next reset will occur on October 15th, which should enable communication to resume.  The mission team expects Voyager 2 to remain on its planned trajectory during the quiet period.  Voyager 1, which is almost 15 billion miles from Earth, continues to operate normally."



And, finally, a couple of interesting tidbits about the Voyager probes:  Uplink communications to the Voyagers is via S-band at 16 bits/sec, while an X-band transmitter provides downlink telemetry at 160 bits/sec normally, and 1.4 kbps for playback of high-rate plasma wave data.  Although I think that I saw that the plasma wave science equipment has been turned off due to power consumption.  All data are transmitted from and received at the spacecraft via the 3.7 meter high-gain antenna.  So that's the big high-gain dish, and obviously being a dish it's pointy, so you've got to point it in the right direction.



Electrical power is supplied by three Radioisotope Thermoelectric Generators (RTGs).  The current power levels are about 249 watts for each spacecraft.  As the electrical power decreases, power loads on the spacecraft must be turned off in order to avoid having demand exceed supply.  Otherwise the voltage would drop.  As loads are turned off, some spacecraft capabilities are eliminated.



So NASA maintains an extremely cool real-time Voyager status page which continuously shows the location of both spacecraft and other interesting tidbits such as which science modules are currently turned on and off, given the amount of available power.  So I created a shortcut, grc.sc/voyager, because the page is so cool.  We haven't looked at it since we last talked about the Voyager probes:  grc.sc/voyager.  Or you can just Google "Voyager Mission Status," and that will bring up as the first link that page.  I mean, and it's updating as you watch it on the fly, how far both of these probes are, and also which science modules are turned on and off.  So anyway, big thanks to our satellite-informed listeners for their information and feedback.



LEO:  And we won't lose Voyager because it's going to reorient, so that's good news, yeah.



STEVE:  Right, right.



LEO:  I do want to correct myself.  It's not Vger.  I was looking it up.  I thought, well, which one was Vger, Voyager 1 or Voyager 2?  Neither. 



STEVE:  Oh.



LEO:  Vger - is this a spoiler now?  No, I won't tell you what I'm talking about.  If you know, then you know.  Vger was Voyager 6, which was, remember, this is a movie that came out in 1979, which was to be launched in 1999.  



STEVE:  Ah.



LEO:  And of course there is no Voyager 6.



STEVE:  It's a future Voyager that we haven't launched yet.



LEO:  It's a future Voyager, yeah.



STEVE:  Of course.



LEO:  Which explains how it got so smart.  Because by 1999 AI was happening.  It's funny how we thought all this stuff would be happening by now.  Anyway, great story.



STEVE:  Oh, Leo, everyone wants to know where their flying cars are.



LEO:  Yeah, yeah.



STEVE:  You know?



LEO:  Yup.



STEVE:  Yeah.  And now I know that would be a very bad idea, so, yeah.



LEO:  Voyager 2's been out there for 45 years.



STEVE:  Unbelievable.



LEO:  Isn't that amazing?



STEVE:  That is really - that is.



LEO:  Yeah.



STEVE:  Wow.  So Jon David Schober, he said:  "Hey, Steve.  On SN-932 I heard you talking about how you're keeping the rack of servers at Level 3, and not moving to the cloud.  In case you wanted some interesting reading, here is a blog post from David Hansson, founder of 37signals and Basecamp and creator of Ruby on Rails."



LEO:  DHH, David Heinemeier Hansson, yes.



STEVE:  Yes, David, DHH, yes.  "He discusses how they regret moving their business to AWS, and how expensive everything was, and how much better life is being back on their own hardware."



So first of all, Jon, thanks very much for the pointer.  Since this topic is quite near and dear to my heart, and since I think it might also be extremely interesting to a large number of our listeners, I want to share the blog post that Jon pointed to.  As Jon said, this was written by David H. Hansson, and it was posted just last October 19th, 2022, titled "Why we're leaving the cloud."



David wrote:  "Basecamp has had one foot in the cloud for well over a decade, and HEY (H-E-Y) has been running there exclusively since it was launched two years ago.  We've run extensively in both Amazon's cloud and Google's cloud.  We've run on bare metal virtual machines.  We've run on Kubernetes.  We've seen all the cloud has to offer, and tried most of it.  It's finally time to conclude:  Renting computers is (mostly)" - he has in parens - "a bad deal for medium-sized companies like ours with stable growth.  The savings promised in reduced complexity never materialized.  So we're making our plans to leave."



He continues:  "The cloud excels at two ends of the spectrum, where only one end was ever relevant to us.  The first end is when your application is so small and low traffic that you really do save on complexity by starting with fully managed services.  This is the shining path that Heroku forged, and one that has since been paved by Render and others.  It remains a fabulous way to get started when you have no customers, and it'll carry you quite far even once you start having some."  He says, parens:  "(Then you'll later be faced with a Good Problem once the bills grow into the stratosphere as usage picks up, but that's a reasonable trade-off.)"



He says:  "The second" - meaning the second use case - "is when your load is highly irregular.  When you have wild swings or towering peaks in usage.  When the baseline is a sliver of your largest needs.  Or when you have no idea whether you need ten servers or a hundred.  There's nothing like the cloud when that happens, like we learned when we launched HEY, and suddenly 300,000 users signed up to try our service in three weeks instead of our forecast of 30,000 in six months.



"But neither of those conditions apply to us today."  And I would say neither of them apply to me, GRC, and actually probably to TWiT.  He says:  "They never did for Basecamp.  Yet by continuing to operate in the cloud, we're paying an at times almost absurd premium for the possibility that it could.  It's like paying a quarter of your house's value for earthquake insurance when you don't live anywhere near a fault line.  Yeah, sure, if somehow a quake two states over opens the earth so wide it cracks your foundation, you might be happy to have it, but it doesn't feel proportional; does it?



"Let's take HEY as an example.  We're paying over half a million dollars per year for database (RDS, relational database) and search (Elastic Search) services from Amazon.  Yes, when you're processing email for many tens of thousands of customers, there's a lot of data to analyze and store, but this still strikes me as rather absurd.  Do you know how many insanely beefy servers you could purchase on a budget of half a million dollars per year?



"Now the argument always goes:  Sure, but you have to manage these machines.  The cloud is so much simpler.  The savings will all be there in labor costs.  Except, no," he says.  "Anyone who thinks running a major service like HEY or Basecamp in the cloud is simple has clearly never tried.  Some things are simple; others are more complex.  But on the whole, I've yet to hear of organizations at our scale being able to materially shrink their operations team just because they moved to the cloud.



"It was a wonderful marketing coup, though.  Sold with analogies like, 'Well, you don't run your own power plant either; do you?'  Or 'Are new infrastructure services really your core competency?'  Then lathered up with a thick coat of new-new-new paint, and The Cloud [he has in caps] has beamed so brightly only the luddites would consider running their own servers in its shadow.



"Meanwhile, Amazon in particular is printing profits, renting out servers at obscene margins.  AWS profit margin is almost 30%," and he says, "$18.5 billion in profits on $62.2 billion in revenue, despite huge investments in future capacity and new services.  This margin is bound to soar now that 'the firm said it plans to extend the useful life of its servers from four years to five, and its networking equipment from five years to six in the future.'  Which is fine.  Of course it's expensive to rent your computers from someone else.  But it's never presented in those terms.  The cloud is sold as computing on demand, which sounds futuristic and cool, and very much not like something as mundane as 'renting computers,' even though that's mostly what it is.



"But this isn't just about cost.  It's also about what kind of Internet we want to operate in the future.  It strikes me as downright tragic that this decentralized wonder of the world is now largely operating on computers owned only by a handful of mega corporations.  If one of the primary AWS regions goes down, seemingly half the Internet is offline along with it.  This is not what DARPA designed.



"Thus I consider it a duty that we at 37signals do our part to swim against the stream.  We have a business model that's incredibly compatible with owning hardware and writing it off over many years.  Growth trajectories are mostly predictable.  Expert staff who might as well employ their talents operating our own machines as those belonging to Amazon or Google.  And I think there are plenty of other companies in similar boats.



"But before we can more broadly set sail back toward lower-cost and decentralized shores, we need to turn the rudder of our collective conversation away from the cloud-serving marketing nonsense about running your own power plant.  Up until very recently, everyone ran their own servers, and much of the progress in tooling that enabled the cloud is available for your own machines, as well.  Don't let the entrenched cloud interests dazzle you into believing that running your own setup is too complicated.  Everyone and their dog did it to get the Internet off the ground in the first place, and it's only gotten easier since.  It's time to part the clouds and let the Internet sunshine through."  So anyway.



LEO:  He's kind of a - he's a crackpot, but okay.



STEVE:  Yeah.



LEO:  There's a lot of reasons you'd want a cloud.  For AI training, for instance, you're not going to go out and buy a thousand cards from NVIDIA and a bunch of servers and stuff just for the training, and then what?  And then just let them sit in the basement?



STEVE:  Well, you just gave a perfect use case for the cloud, and I've heard that suggested.  You would use the cloud to train the model [crosstalk] run the model.



LEO:  And then run it locally.  Yeah, lots of people do that.



STEVE:  Yes.



LEO:  I mean, I think [crosstalk] is very common.



STEVE:  But Leo, you...



LEO:  You say you're not in the cloud, but Level 3 isn't on-prem.  Aren't you in the cloud?



STEVE:  Well, everybody has some Tier 1 service provider.  I mean, so you have an IP.



LEO:  But your servers are in your house.



STEVE:  No, no, my servers are a short drive away in a datacenter.



LEO:  But that's not the cloud because you own the hardware?



STEVE:  Correct.



LEO:  Okay.  All right.  I mean, my website is right down the hall.  It literally is on-prem here.



STEVE:  Except you've talked about how expensive Mastodon is.  Mastodon for me would be free.



LEO:  Mastodon's running in the cloud, yeah.



STEVE:  No matter how big it gets.  So that's a...



LEO:  Well, there's an example.  I wanted to run Mastodon in the cloud because I didn't want to maintain it and run it off the servers here, and because we don't have enough local bandwidth to run it.  I mean, obviously 37signals can afford to buy many, many gigabits of bandwidth; right?  I mean, come on.  He's kind of a crackpot.  He's a well-known crackpot.  But you know, since he put that out, he has succeeded.  They are all off the cloud now, yeah.



STEVE:  Cool.



LEO:  We'll see.  I'd like to see what his bills are for running it locally.  The problem really is that he doesn't see those bills because it comes in the form of rent and electricity and air conditioning and things that he doesn't consider cloud costs.



STEVE:  Well, I pay about a grand a month for all of GRC and all of my servers and all of my bandwidth.



LEO:  But you're kind of a cloud because you're running in a network operations center.  You're not running on-prem.



STEVE:  Well, no, he's talking about renting machines.



LEO:  Oh, he's talking about the same thing, a colo?  Yeah.  Yeah.  A colo is going backwards a little bit, I think.  But okay, fine.  It's whatever.  There's a lot, I mean, there are a lot of businesses who will dispute that.



STEVE:  And that of course was the whole point of his blog post was that is sane to go backwards.



LEO:  Okay.



STEVE:  That, you know, the promise of the cloud did not materialize.  Anyway, I wanted to share it with our listeners.



LEO:  No, it's good, yeah.



STEVE:  It is my position.  It is what I'm doing.  And I have fixed costs.  I could run Mastodon servers till the cows came home, and it wouldn't cost me anything more.



LEO:  Right.  Except your time.



STEVE:  No.  No.  I mean, I maybe visit Level 3 annually.  My servers are typically up for three or four years at a time.



LEO:  Right, right.



STEVE:  So, yeah, I mean, it's just not - it's not a problem for me.  But, you know, I built the stuff right once, so I don't have to be continually nursing them.  And Leo, we're an hour and a half in, and we haven't even gotten to our main topic.  Let's take our third break and talk about TETRA:BURST.



LEO:  Okay.  You didn't want to play...



ROBOT:  Danger, danger.



LEO:  Okay, that's fine.  That's fine.  And now, back to Security Now!.



STEVE:  Leo, we do need to explain the "Danger Will Robinson" sound effect.



LEO:  Okay.  Go ahead.



STEVE:  Steven Perry, he sent a note.  He said:  "Hi, Steve."



LEO:  He's obviously a regular, by the way, in our Discord.  We love Steven.



STEVE:  Ah.  He said:  "I was listening to yesterday's Security Now! episode and wondered if anyone had ever shared with you and Leo a little bit of trivia about the show 'Lost in Space.'"  Which of course we both cut our teeth on.



LEO:  Mm-hmm.



STEVE:  You know, as kids.  "Everyone knows and uses the catchphrase 'Danger, Will Robinson.'"  Of course one of our faves.  He says:  "But did you know that it was only ever said once in the entire run of the show?"



LEO:  Wow.



STEVE:  "It was Season 3, Episode 11 when it happened."



LEO:  But who's counting.



STEVE:  "It was never said again."  Yeah.  "But that is the phrase we all know and love about the show.  Thought I'd pass it along.  Have a good day."  Well, I was astonished by that.  I did a little bit of looking around.  The Internet agrees with Steven.  And apparently one of the reasons is that the robot was always waving its arms around, saying "Danger! Danger!"



LEO:  That's what he said was "Danger! Danger!"  We add the Will Robinson so you know what it means.  If I just said "Danger! Danger!" you wouldn't know.  But anyway.



STEVE:  Yeah, you'd think, what?



ROBOT:  I am sorry, Will Robinson.  I am afraid I goofed.



LEO:  I have many, by the way, many robotic quotes.



ROBOT:  A robot does not live by programming alone.  Some culture is require to keep my tapes in balance.



LEO:  Little did we know, in the future they're going to still use tapes in the robots.



STEVE:  Yeah, yeah.  Actually, it's funny how the use of that term has hung on.  I mean, people are still saying "Did you tape..."



ROBOT:  Danger, danger.  Danger, danger. 



STEVE:  Okay.  So by far the news that was most forwarded to me this past week was that the encrypted security of a globally used, "secure" in air quotes, radio communication system whose security has been trusted and relied upon worldwide, turns out not to be as secure as everyone hoped and was led to believe. And moreover, the system's insecurity was well known and kept secret by those whose commercial interests depended upon the system being trusted, when it was not trustworthy.



Wired did a beautiful job of describing the situation in their story last week titled "Code Kept Secret for Years Reveals Its Flaw - a Backdoor."  And they followed that with "A secret encryption cipher baked into radio systems used by critical infrastructure workers, police, and others" - meaning lots of military - "around the world is finally seeing sunlight.  Researchers say it isn't pretty."



Now, I'm going to share Wired's coverage of this while liberally interjecting my own commentary.  So here's what Wired described.  They said:  "For more than 25 years, a technology used for critical data and voice radio communications around the world has been shrouded in secrecy to prevent anyone from closely scrutinizing its security properties for vulnerabilities."  Now, okay.  Anybody, if you've listened to this podcast for only one of our almost 18 years, you know that anytime you hear the technology was kept private to prevent anyone from scrutinizing its security properties for vulnerabilities is not good news.



Anyway, but Wired said:  "Now it's finally getting a public airing, thanks to a small group of researchers in the Netherlands who got their hands on it and found serious flaws, including a deliberate backdoor.  The backdoor, known for years by vendors that sold the technology, but not necessarily by customers, exists in an encryption algorithm baked into radios sold for commercial use in critical infrastructure.  It's used to transmit encrypted data and commands in pipelines, railways, the electric grid, mass transit, and freight trains.  It would allow someone to snoop on communications to learn how a system works, then potentially send commands to the radios that could trigger blackouts, halt gas pipeline flows, or reroute trains.



"Researchers found a second vulnerability in a different part of the same radio technology that is used in more specialized systems sold exclusively to police forces, prison personnel, military, intelligence agencies, and emergency services, such as the C2000 communication system used by Dutch police, fire brigades, ambulance services, and Ministry of Defense for mission-critical voice and data communications.  The flaw would let someone decrypt encrypted voice and data communications and send fraudulent messages to spread misinformation or redirect personnel and forces during critical times.



"Three Dutch security analysts discovered the vulnerabilities, five in total, in a European radio standard called TETRA" - which stands for Terrestrial Trunked Radio - "which is used in radios made by Motorola, DAMM, Hytera, and others.  The standard has been used in radios since the '90s, but the flaws remained unknown because encryption algorithms used in TETRA were kept secret until now.



"The technology is not widely used in the U.S." - well, not widely, but it is here - "where other radio standards are more commonly deployed.  But Caleb Mathis, a consultant with Ampere Industrial Security, conducted open source research for Wired and uncovered contracts, press releases, and other documentation showing TETRA-based radios are used in at least two dozen critical infrastructures in the U.S.  Because TETRA is embedded in radios supplied through resellers and system integrators like PowerTrunk, it's difficult to identify who might be using them and for what.  But Mathis helped Wired identify several electric utilities, a state border control agency, an oil refinery, chemical plants, a major mass transit system on the East Coast, three international airports that use them for communications among security and ground crew personnel, and a U.S. Army training base.



"The researchers with Midnight Blue in the Netherlands discovered the TETRA vulnerabilities - which they're calling TETRA:BURST - in 2021."  Okay?  So two years ago they discovered this - "but agreed not to disclose them publicly until radio manufacturers could create patches and mitigations."  And we know how that typically goes.  "Not all of the issues can be fixed with a patch, however, and it's not clear which manufacturers have prepared them for customers.  Motorola, one of the largest radio vendors, did not respond to repeated inquiries from Wired.



"The Dutch National Cyber Security Centre assumed the responsibility of notifying radio vendors and computer emergency response teams around the world about the problems, and of coordinating a timeframe for when the researchers should publicly disclose the issues."  And as I said at the top of the show, next week is Black Hat, and all will be revealed there.



"In a brief email, NCSC spokesperson Miral Scheffer called TETRA 'a crucial foundation for mission-critical communication in the Netherlands and around the world,' and emphasized the need for such communications to always be reliable and secure, 'especially during crisis situations.'  She confirmed the vulnerabilities would let an attacker in the vicinity of impacted radios 'intercept, manipulate or disturb' communications and said the NCSC had informed various organizations and governments, including Germany, Denmark, Belgium, and England, advising them how to proceed.  A spokesperson for DHS's CISA [here] said they're aware of the vulnerabilities, but would not comment further.  The researchers say anyone using radio technologies should check with their manufacturer to determine if their devices are using TETRA, and what fixes or mitigations are available.



"The researchers plan to present their findings at the Black Hat security conference in Las Vegas, when they will release detailed technical analysis as well as the secret TETRA encryption algorithms that have been unavailable to the public until now.  They hope others with more expertise will dig into the algorithms to see if they can find other issues.  So TETRA was developed in the '90s by the European Telecommunications Standards Institute, or ETSI.  The standard includes four encryption algorithms, TEA1, TEA2, 3, and 4" - so I'll just call them TEA 1, 2, 3, and 4 - "that can be used by radio manufacturers in different products, depending on their intended use and customer."



Okay.  So, as I said, whoa, wait, what?  The four different encryption algorithms can be used by radio manufacturers in different products depending upon their intended use and customer.  So if that doesn't smell fishy, I don't know what does.  So Wired explains this.  Wired says:  "TEA1 is for commercial uses.  For radios used in critical infrastructure in Europe and the rest of the world, though, it is also designed for use by public safety agencies and military, according to an ETSI document, and the researchers found police agencies that use it.



"TEA2 is restricted for use in Europe by police, emergency services, military, and intelligence agencies."  Okay.  So TEA1 is for commercial uses, whereas TEA2 is restricted for use in Europe by police, emergency services, military and intelligence agencies?  What's the difference?  "TEA3," Wired writes, "is available for police and emergency services outside Europe, in countries deemed friendly to the EU, like Mexico and India.  Those not considered friendly, such as Iran, only had the option to use TEA1.  TEA4, another commercial algorithm, is hardly used, the researchers said.



"The vast majority of police forces around the world, aside from the U.S., use TETRA-based radio technology.  After conducting open source research, TETRA is used by police forces in Belgium and the Scandinavian countries; East European countries like Serbia, Moldova, Bulgaria, and Macedonia; as well as in the Middle East in Iran, Iraq, Lebanon, and Syria.  Additionally, the Ministries of Defense in Bulgaria, Kazakhstan, and Syria use it.  The Polish military counterintelligence agency uses it, as do the Finnish defense forces, and Lebanon and Saudi Arabia's intelligence service, to name a few.



"Critical infrastructure in the U.S. and other countries use TETRA for machine-to-machine communication in SCADA and other industrial control system settings, especially in widely distributed pipelines, railways, and electric grids where wired and cellular communications may not be available."



And now, get a load of this blast from the past:  "Although the standard itself is publicly available for review," meaning the paper printed standard saying this is what we're going to offer you for your radio to use, "the encryption algorithms are only available under a signed NDA to trusted parties, such as radio manufacturers.  The vendors have to include protections in their products to make it difficult for anyone to extract the algorithms and analyze them."  Oh, boy.



"To obtain the algorithms, the researchers purchased an off-the-shelf Motorola MTM5400 radio and spent four months locating and extracting the algorithms from the secure enclave in the radio's firmware.  They had to use a number of zero-day exploits to defeat Motorola protections, which they reported to Motorola to fix.  Once they reverse-engineered the algorithms, the first vulnerability they found was a backdoor in TEA1."



Okay.  So first of all, huge props to these guys.  No one made it easy for them to obtain the information they needed.  In fact, their efforts were deliberately being thwarted at every turn by the use of requiring a signed NDA, which they were not able to agree to because they wanted to disclose it, and a secure enclave.  And they needed to find zero-day exploits, brand new zero-day exploits and then use them to crack the lid off the code.



And let's also just pause for a moment to thank our lucky stars that this reverse engineering conduct has been deemed legal.  If white hat hackers like these guys could be jailed for conducting research in the interest of improving the security of the products they're examining, even when doing so is not in the interest of those who are working hard to keep those secrets, the world would be far less secure, and only the bad guys would be pursuing such reverse engineering.  They would not be agreeing to keep their secrets quiet.  They would never be disclosing them because they would then be turning around and leveraging them.  And all of the stuff that we talk about on this podcast constantly which is being reverse engineered at significant effort and cost by good guy researchers, none of that would be happening because doing so would be illegal.  Thank goodness that decision was made, making it so that this kind of research is safe.



So here's what they found.  All four TETRA encryption algorithms use 80-bit keys, which the researchers say, and I would agree, even more than two decades after their release, still provides sufficient security to prevent someone from cracking them.  And I'll note that the keys are rotated, and they're dynamically changing.  So it's not like they're just fixed 80-bit keys.  They're ephemeral, so they're not around long enough for that to be a problem.  But they are around for a while.



TEA1 has a "feature," in quotes, that reduces its encryption key length to just 32 bits, which the researchers were able to crack in less than a minute using a standard laptop and samples of just four cipher texts, which of course you get by putting a radio up in the air and receiving some of this encrypted communication.  Brian Murgatroyd, the chair of the technical body at ETSI, the people behind this, responsible for the TETRA standard, objects to calling this a backdoor.  He says when they developed the standard they needed an algorithm for commercial use that could meet export requirements - now, remember this is more than two decades ago - to be used outside Europe, and that in 1995 a 32-bit key still provided security, although he acknowledges that with today's computing power that's no longer the case.  Remember, these guys, the researchers cracked the key in less than a minute.



Matthew Green, our well-known Johns Hopkins University cryptographer and professor, calls the weakened key "a disaster."  He said:  "I wouldn't say it's equivalent to using no encryption, but it's really, really bad."  Gregor Leander, a professor of computer science and cryptographer with a security research team known as CASA at Ruhr University Bochum in Germany, says it would be "stupid," not mincing any words, for critical infrastructure to use TEA1, especially without adding end-to-end encryption on top of it.  He said:  "Nobody should rely on this."  Murgatroyd insists the most anyone can do with the backdoor is decrypt and eavesdrop on data and conversations.  TETRA has strong authentication, he says, that would prevent anyone from injecting false communication.



"That's not true," says Wetzels, one of the researchers.  TETRA only requires that devices authenticate themselves to the network, but data and voice communications between radios are not digitally signed or authenticated.  The radios and base stations trust that any device that has the proper encryption key is authenticated, so someone who can crack the key as the researchers did can encrypt their own messages with it and send them to base stations and other radios.



While the TEA1 weakness has been withheld from the public, it's apparently widely known in the industry and governments.  In a 2006 U.S. State Department cable leaked to WikiLeaks, the U.S. embassy in Rome describes an Italian radio manufacturer asking about exporting TETRA radio systems to municipal police forces in Iran.  The U.S. pushed back on the plan, so the company representative reminded the U.S. that encryption in the TETRA-based radio system they planned to sell to Iran is less than 40-bits - indeed, 256 times less than 40 bits because it's 32 bits, implying that the U.S. should not object to the sale because the system isn't using a strong key.



The second major vulnerability the researchers found isn't in one of the secret algorithms, but it affects all of them.  All of them.  The issue lies in the standard itself and how TETRA handles time syncing and keystream generation.  When a TETRA radio contacts a base station, they initiate communication with a time sync.  The network broadcasts the time, and the radio establishes that it's in sync.  Then they both generate the same keystream, which is tied to that timestamp, to encrypt the subsequent communication.  



Wetzels says:  "The problem is that the network broadcasts the time in packets that are unauthenticated and unencrypted."  As a result, you can time spoof.  An attacker can use a simple device - and actually, Leo, you probably have one in your pocket.



LEO:  No, I gave it to Father Robert to take to Black Hat.



STEVE:  Oh, yeah, that's good, he'll get some use out of it - "to intercept and collect encrypted communication passing between a radio and base station, while noting the timestamp that's initiated the communication.  Then he can use a rogue base station to contact the same radio or a different one in the same network and broadcast the time that matches the time associated with the intercepted communication.  Basically, you know, resetting them to the key that he already has that was decrypted earlier.  The radio is dumb and believes the correct time is whatever the base station says it is.  So it will generate the keystream that was used at the time to encrypt the communication the attacker collected.  The attacker recovers that keystream and can use it to decrypt the communication collected earlier.



"To inject false messages, he would use his base station to tell a radio that the time is tomorrow noon, and ask the radio to generate the keystream associated with that future time.  Once the attacker has it, he can use the keystream to encrypt his rogue messages, and the next day at noon send them to a target radio using the correct keystream for that time.  In other words, it was really badly designed, even in 1995.  There were all kinds of holes in the system, not just secret algorithms for encryption.



"Wetzels imagines Mexican drug cartels could use this to intercept police communications to eavesdrop on investigations and operations or deceive police with false messages sent to radios. The attacker needs to be near a target radio, but the proximity is only dependent on the strength of the rogue base station's signal and the terrain."  He says:  "You can do this within a distance of tens of meters.  The rogue base station would cost $5,000 or less."



So ETSI's Murgatroyd downplays the attack, saying TETRA's strong authentication requirements - oh, boy - would prevent a non-authenticated base station from injecting messages.  Wetzel disagrees, saying TETRA only requires devices to authenticate to the network, not to each other.  The researchers didn't find any weaknesses in the TEA2 algorithm used by police, military, and emergency services in Europe, but they did initially think they found another backdoor in TEA3.  Given that TEA3 is the exportable version of TEA2, there was good reason to believe it might also have a backdoor to meet export requirements.



Anyway, we basically have a system which is full of holes, has been used for, what, 28 years, since 1995, is known to be insecure, never received the upgrading that it should have received.  But as I said, that never happened.  So, as I and Wired noted, in eight days all the wraps will be coming off of this when the research team presents their work and findings during Black Hat in Las Vegas.  With TETRA we have a legacy, encrypted, radio communications system being widely used today throughout the entire world, including in the U.S.  And it not only contained multiple really exploitable flaws that were only fixed after security researchers cracked it open and shamed its creators with the threat of disclosure, and even now they're not actually saying, okay, yeah, you got us, you're right, it also contained deliberately weakened encryption which most of the world was given to use while some agencies knew of the weakness and were apparently leveraging that knowledge for eavesdropping.



And now we learn that the ETSI group who did all of this has replaced their earlier flawed work with more of the same. Keeping their encryption secret after rotating the original TEA1 through 4 ciphers out, there are now new ones; and they, too, are kept secret.  Even though we have well-vetted, well-tested, well-functioning, lightweight, high-performance encryption.  Nobody should be rolling their own any longer.  It's just crazy.  Why would anyone ever trust these people?



LEO:  So true.  This reminds me of SS7, although SS7 is still around, the sideband that is totally hackable in every phone.  It's still around just because you can't - it's too hard to change; right?



STEVE:  Right.  Well, we do have the requirement for encryption intersystem, but that's what has not happened.



LEO:  Right.



STEVE:  Intrasystem encryption has happened, and they're supposed to be doing intersystem.  But the problem is apparently they're making too much money out of spam.



LEO:  Yeah, right.  There you go.



STEVE:  They really don't want to limit it.



LEO:  There you go.  They don't want to limit it, yeah.  Ah.



ROBOT:  Does not compute.



LEO:  That's just the way it is.  Even the robot has an opinion on that one.  Well, that concludes this thrilling, gripping edition of Security Now! as we edge into our 19th year, a couple more weeks.



STEVE:  Coming up on it.



LEO:  Wow.  Only 66 episodes left.  Guess we're counting down, too.  Steve Gibson's at GRC.com, which is proudly not in the cloud.  All you have to do is go to GRC.com, and then you will see all sorts of good stuff, including SpinRite, the world's best mass storage maintenance and recovery utility.  You need this if you've got hard drives or solid-state drives.  Version 6.0 is still there, but it is soon to be replaced by 6.1.  You will get a free upgrade when 6.1 comes out if you buy today.  GRC.com.  You can also get the show there.



Copyright (c) 2023 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.






GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#934

DATE:		August 8, 2023

TITLE:		Revisiting Global Privacy Control

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-934.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  What was it that also just, last week, happened with Voyager 2?  What did Tenable's CEO Amit Yoran have to say about Microsoft's security practices?  And what did Bruce Schneier have to say about the recent attack on Azure by Chinese hackers?  There's more to AI than ChatGPT.  What did some academic researchers in the UK accomplish by adding new deep learning modeling to a classic and previously weak attack?  And after discussing some interesting listener feedback from the prior week, we're going to revisit a topic we covered when it was young because it's beginning to show signs that it might have a life of its own and may not be destined to fall by the wayside, as all brokers of personal information would hope.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  Whither Voyager 2?  We've got an update on that story. You know Steve paid a lot of attention to Amit Joran's screed against Microsoft, on Microsoft's own social network, LinkedIn.  What did Amit say, and why does Steve agree, coming up.  Also Bruce Schneier on the recent attack on Azure by Chinese hackers.  And then we'll talk about Global Privacy Control, how to turn it on, why you want to turn it on, and how Yahoo is responding.  It's pretty amazing.  All of that coming up and a whole lot more, this week on Security Now!.  Stay tuned.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 934, recorded Tuesday, August 8th, 2023:  Revisiting Global Privacy Control.



It's time for Security Now!, the show where we cover the week's security news, reassure you that there is no reason to set your hair on fire with this guy right here, whose hair apparently was set on fire some time ago, Mr. Steve Gibson.  Hello, Steve.



STEVE GIBSON: Now, the question is, is it W-E-E-K security or W-E-A-K security.



LEO:  No.  Nothing weakly about the security here.



STEVE:  Nothing weak, weaklings, no.



LEO:  Weaklings, no weaklings.



STEVE:  Okay.  So today's topic, we are going to revisit something that we talked about a little over a year ago, in May of 2022.  And I didn't plan this, but when I went to TechCrunch, something happened.  And so I thought, whoa, what?  Wait.  So today's topic for Episode #934 for 8/8/23 is Revisiting Global Privacy Control.  But before we get to that, we're going to update on what happened last week with Voyager 2.  Something new happened in that, I mean, it's providing more news than JPL and NASA wishes it was producing.



LEO:  Or that anything 12 billon miles away should, really.



STEVE:  Yeah, that's right, yeah.  The good news is they haven't lost it, although it'd be easy, you know, to lose track of it. Also we're going to answer the question what did Tenable's CEO Amit Yoran have to say about Microsoft's security practices?  And what did Bruce Schneier have to say about the recent attack on Azure by Chinese hackers?  Also, there's more to AI than ChatGPT.  What did some academic researchers in the UK accomplish by adding new deep learning modeling to a classic and previously weak attack?  And after discussing some interesting listener feedback from the prior week, we're going to revisit, as I said, a topic that we covered when it was young because it's beginning to show signs that it might have a life of its own and may not be destined to fall by the wayside, as all brokers of our personal information would hope.  And of course we've got another great Picture of the Week.  So I think a fun podcast for our listeners.



LEO:  Always.  Always fun with Steve.  And informative.  We can't forget that.  Tell you another thing we need.  We need a Picture of the Week, Mr. Gibson.



STEVE:  So, okay.  Now, normally I'm able to explain on an audio podcast what the photo is that we're looking at.



LEO:  You're not going to attempt that with this one, huh?



STEVE:  Well, this is a bit challenging.  Okay.  So here's the problem that an industrious person set out to solve.  They had one of the newer style stereo plug connectors, like remember the old days, headphone jacks were large plugs.  Technically they're called "quarter-inch phone plugs," or "phono plugs."  But then newer headphones have the little smaller stereo connector, you know, like the kind that would plug into our smartphones when smartphones still had headphone connectors.  So the problem is, so a person has a signal source with that kind of a connector on the end.  Yet he wants to connect it to the old-school RCA-style mono plug that we used to have on the back of stereos, for example, and in this case it's the tape-in connectors.  And so there's two of them, right, because each one is monaural.



LEO:  All you have to do is you state the problem.  How do I get a mini jack output into dual RCA, left and right RCA connectors?  How do I solve that?



STEVE:  That was said much more simply, Leo.  Yes.  There you go.  That's the...



LEO:  And this is the wildest solution ever.  So he's got an adapter, first thing he's got, he's got an adaptor on the thing; right?



STEVE:  Exactly.  And remember that during that transition phase in headphones, and you still see it around, there is a - it is an adapter from the mini headphone connector to the old-school large one, if you want to plug it into something with a regular quarter-inch headphone socket.  So he plugs the little guy into the adapter.  Now it's a quarter-inch size.  The problem is, you know, you've got a pair of RCA monaural connectors.



LEO:  Now, you could go to the store and buy...



STEVE:  Yes.  That would have been...



LEO:  ...a connector.



STEVE:  I'm sure I have some...



LEO:  I do, yes.



STEVE:  ...in my random adapter box, you know.



LEO:  But no, this is an emergency.



STEVE:  Well, it was an emergency, or he had some extra thick, very heavy-gauge copper wire around.



LEO:  Yeah.  And praise to him for using heavy-gauge copper here.



STEVE:  Oh, yeah, yeah.  You've got to do that.  It's sort of like a coat hanger.  I mean, you know, it took some effort to bend this copper, this solid copper wire.  Again, now, here's where I've already, like, painted myself in the corner.  I don't know how to describe this except to say imagine that you created like an eyelet with the copper so that it was an eyelet that then had a straight piece.  And then you slid the eyelet down onto the quarter-inch plug.  And now remember that the quarter-inch headphone plug has rings; right?  It's got some insulator rings.  And so it's actually three different conductive zones that you want to connect to.



LEO:  You've got your tip, and you've got your ring; right?



STEVE:  Your ring and your tip, exactly.



LEO:  Ring and tip, yeah.



STEVE:  And so he's got copper wrapped around the ring and the tip, which then goes into the RCA, the monaural RCA red and white for left and right plugs.



LEO:  Right.



STEVE:  Then, to anchor the whole thing, near the base of this he wraps the copper around, and then does a big loop, a big U-shaped loop because he needs to connect to the ground side of these RCA plugs.  Anyway, really you need to download the show notes.  It's worth it if you download the show notes to look at the first page.



LEO:  You'll never figure it out if you're just listening.  It's crazy.



STEVE:  No.  But beautiful.  I mean, it's just...



LEO:  Elegant.



STEVE:  Absolutely, yeah,



LEO:  It's an elegant solution.



STEVE:  And maybe this was just until the Amazon delivery came.



LEO:  Probably, yeah.



STEVE:  Couldn't wait to get the - had to have an adapter right now for whatever purpose.  Anyway, hats off to the anonymous inventor of this approach.  I don't think there's a market for this because, you know, Amazon will sell it to you for $3 and deliver it in the afternoon.



Okay.  So when we last left the Voyager 2 space probe, it had received a series of mistaken commands from ground control which caused it to turn 2 degrees away from Earth.  Now, at its present distance of 12.3 billion miles, 2 degrees might as well be 90 degrees.  I mean, it's missing the Earth by a long shot.  So this meant that no more data could be received, nor could any corrective commands be sent to the probe.  It wouldn't hear them.  Now, the good news is that as long as all is going well, Voyager has a fail-safe system that was expected to perform an automatic reorientation this coming October.  Actually it's on the 15th of October.  But, you know, that's still three months away; right?



So NASA wrote:  "Voyager 2 is programmed to reset its orientation multiple times each year to keep its antenna pointed at Earth.  The next reset will occur on October 15th, which should enable communication to resume.  The mission team expects Voyager 2 to remain on its planned trajectory" - let's hope that happens - "during the quiet period."



But then last week we received an update from NASA, on August 1st, 2023:  "Using multiple antennas, NASA's Deep Space Network was able to detect a carrier signal from Voyager 2.  A carrier signal is what the spacecraft uses to send data back to Earth. The signal is too faint for data to be extracted, but the detection confirms that the spacecraft is at least still operating.  The spacecraft also continues on its expected trajectory.  Although the mission expects the spacecraft to point its antenna at Earth in mid-October, the team will attempt to command Voyager sooner, while its antenna is still pointed away from Earth.  To do this, a DSN" - that's the abbreviation for Deep Space Network - "a Deep Space Network antenna will be used to 'shout' the command to Voyager to turn its antenna.  This intermediary attempt may not work, in which case the team will wait for the spacecraft to automatically reset its orientation in October.  Either way, once the spacecraft's antenna is realigned with Earth, communication should resume."



That was on August 1st.  That was when we did the podcast last week, on Tuesday.  Then August 4th, three days later, "NASA has reestablished full communications with Voyager 2.  The agency's Deep Space Network facility in Canberra, Australia, sent the equivalent of an interstellar 'shout'" - meaning I guess they cranked the power up to max, or maybe 11, and said, you know, point here.



Anyway, "More than 12.3 billion miles to Voyager 2, instructing the spacecraft to reorient itself and turn its antenna back to Earth.  With a one-way speed-of-light delay of 18.5 hours for the command to reach Voyager, it took 37 hours for the mission controllers to learn whether the command worked.  At 12:29 a.m. EDT on August 4th, the spacecraft began returning science and telemetry data, indicating it is operating normally and that it remains on its expected trajectory."



So, yay.  You can just imagine the breath-holding that was going on during those 37 hours.  But really, the entire project is an incredible engineering accomplishment.  You know, these guys should be so proud of what they have done. 



LEO:  And you can't, you know, you can't just fire up a telescope and look for it.  It's beyond Pluto.  I mean, it's not invisible, it's gone.



STEVE:  It's gone, Leo, it's gone.  I mean, we fully expected it to simply dissolve when it left the Earth simulation, you know.



LEO:  This is so cool.



STEVE:  But it's just incredible.



LEO:  It's really neat.  It's just the neatest thing.



STEVE:  You know.  And when it didn't die after its first planetary encounter, they said, well, huh.  Let's keep going.  I mean, what the hell?  You know?



LEO:  This stuff is so over-engineered, I mean, look at Perseverance.  Look at, I mean, it's amazing.



STEVE:  Exactly.  The Rovers that just, like, you know, like they got covered in dust, and they wound down, and then the dust blew off, and it came back.  Hello.



LEO:  Hello.  I'm back.



STEVE:  What did I miss?



LEO:  What's up?  What's up?



STEVE:  Wow.



LEO:  Just very feel-good story, just great.



STEVE:  Okay.  So everyone who listens to this podcast knows that I often become upset with Microsoft's behavior and with their performance.  



LEO:  Oh, you had a friend this time, didn't you.  I know where you're going with this one.



STEVE:  I sometimes feel odd since I can imagine someone reasonably saying, if you have so much trouble with Microsoft, why don't you just switch to Mac or Linux?  And it's true that I do love Windows, and I have very little trouble with it myself.  But due to their size and their dominance, Microsoft's behavior matters and affects the world, regardless of what desktop platform I've personally chosen.  And since this podcast covers security, it also needs to explore Microsoft's many behaviors related to security.



Well, last Wednesday, August 2nd, someone else weighed in on Microsoft's security practices from their own perspective and significant experience.  Since I sometimes feel a bit self-conscious tearing into Microsoft over and over, I wanted to share this additional viewpoint.  But for what this individual - I'm getting a little excited, as you can see.  For what this individual wrote to have any weight and bearing, you need to know something about the posting's author, as I mentioned at the top of the show, Amit Yoran.



Wikipedia informs us:  "Amit Yoran is chairman and chief executive officer of Tenable, a position held since January 3rd, 2017.  Previously, Yoran was president of computer and network security company RSA."  We've heard of them.  "Yoran joined RSA during his tenure as CEO of NetWitness Corp., which was acquired by RSA's parent company, EMC, in April of 2011.  Prior to his time at NetWitness, Yoran was the National Cyber Security Division director within the United States Department of Homeland Security.  He took up the post in September 2003 and served as the initial director of the US-CERT.  That's, of course, the U.S. Department of Defense Computer Emergency Response Team.  He resigned from his position at US-CERT in October of '04.



"Earlier in his career, Yoran was a co-founder and CEO of Riptech, which was acquired by Symantec in August 2002.  He also served on the board of directors of Cyota (acquired by RSA), Guardium (acquired by IBM), Guidance Software, and other Internet security technology companies.  Yoran is a graduate of the United States Military Academy and served as one of the founding members of the U.S. Department of Defense's Computer Emergency Response Team.  He has a master's degree in computer science."  In other words... 



LEO:  Very impressive guy.  Very.



STEVE:  ... this guy has earned, yes, he has earned some street cred by being, like, in the middle of computer security for many years.  His LinkedIn posting last Wednesday is titled:  "Microsoft:  The Truth Is Even Worse Than You Think."



LEO:  Oh, boy.



STEVE:  So here's what Amit wrote and posted publicly on LinkedIn, a platform Microsoft purchased.  He wrote:  



"Last week, Senator Ron Wyden sent a letter to the Cybersecurity and Infrastructure Security Agency (CISA), the Department of Justice, and the Federal Trade Commission, asking that they hold Microsoft accountable for a repeated pattern of negligent cybersecurity practices, which has enabled Chinese espionage against the United States government.  According to data from Google Project Zero, Microsoft products have accounted for an aggregate 42.5% of all zero-days discovered since 2014."



He writes:  "Microsoft's lack of transparency applies to breaches, irresponsible security practices, and to vulnerabilities, all of which expose their customers to risks that they are deliberately kept in the dark about.  In March 2023" - so, right, just this past March - "a member of Tenable's Research team was investigating Microsoft's Azure platform and related services.  The researcher discovered an issue which would enable an unauthenticated attacker to access cross-tenant applications and sensitive data, such as authentication secrets.  To give you an idea of how bad this is, our team very quickly discovered authentication secrets to a bank.  They were so concerned about the seriousness and the ethics of the issue that we immediately notified Microsoft.



"Did Microsoft quickly fix the issue that could effectively lead to the breach of multiple customers' networks and services?  Of course not.  They took more than 90 days to implement a partial fix, and only for new applications loaded in the service.  That means that, as of today" - and he wrote this last week - "the bank I referenced above is still vulnerable, more than 120 days since we reported the issue, as are all of the other organizations that had launched the service prior to the fix. And, to the best of our knowledge, they still have no idea they are at risk, and therefore can't make an informed decision about compensating controls and other risk-mitigating actions.



"Microsoft claims that they will fix the issue by the end of September" - meaning end of next month - "four months after we notified them.  That's grossly irresponsible, if not blatantly negligent.  We know about the issue, Microsoft knows about the issue, and hopefully threat actors don't.  Cloud providers," he says, "have long espoused the shared responsibility model.  That model is irretrievably broken if your cloud vendor doesn't notify you of issues as they arise, and apply fixes openly.



"What you hear from Microsoft is 'just trust us,' but what you get back is very little transparency and a culture of toxic obfuscation.  How can a CISO, board of directors, or executive team believe that Microsoft will do the right thing, given the fact patterns and current behaviors?  Microsoft's track record puts us all at risk.  And it's even worse than we thought."



LEO:  Wow.



STEVE:  "A culture of toxic obfuscation."



LEO:  Toxic obfuscation.  I love that.



STEVE:  Okay, now, by looking at the facts through the years, we've documented a great many instances where Microsoft's behavior, whether apparently deliberate or inadvertent - yet either way quite difficult to see as anything other than "We're so big we don't need to care, and you can't make us" - has clearly damaged their own customers, even significantly.  But their enterprise and government customers are as captive as I am.  I'm held captive by my decades-long investment in Windows, and by the fact that there is no viable alternative to Windows for some of the things I want to do.  I depend upon many tools that are only hosted on Windows.  And Microsoft's big enterprise customers have invested massively in their own solutions which are also not portable to any other platform.  The incredible power this position gives Microsoft should not be underestimated.  It leaves the entire world asking, "Please, sir, may I have some more soup?"



Amit Yoran's posting on LinkedIn prompted an interview by CyberScoop.  They, in turn, CyberScoop, wrote:  "Veteran cybersecurity executive Amit Yoran accused Microsoft on Wednesday of dragging its feet on fixing a critical vulnerability affecting its Azure platform, and said the tech giant's slow response illustrates a negligent approach to security.  His harsh public critique of Microsoft  a relatively rare event for a high-profile corporate figure in cybersecurity  follows criticism from lawmakers and researchers alike after a recent cyberattack affecting U.S. government officials resulted from a Microsoft security lapse.



"As the CEO of Tenable, a firm that helps companies understand and mitigate their cybersecurity vulnerabilities, Yoran said he works with hundreds of companies every year to disclose and patch vulnerabilities.  Microsoft, he said, consistently fails to proactively and professionally address vulnerabilities in their products.



"Yoran told CyberScoop in an interview:  'In Microsoft's case you have a culture which denies the criticality of vulnerabilities.'  According to a timeline in a limited blog published to Tenable's website, Microsoft acknowledged the issue the same day it was disclosed on March 30th, and confirmed it four days later.  Tenable asked for an update on June 27th, 90 days later, and was told on July 6th that it was fixed, but Tenable says it was merely a partial fix."  Okay, now, where have we heard that before?  How many times on this podcast have we noted that someone at Microsoft, who was shown a serious vulnerability by a security researcher, and even given a fix for it, apparently didn't even take the time or care to actually understand the underlying problem, and so only half patched it to resolve one of the problem's symptoms.



Anyway, CyberScoop continues:  "On July 21st, Microsoft told Tenable that it would take until September 28th for a complete fix.  Tenable agreed to withhold technical details and proofs of concept until September 28th.  In his blog post, Yoran described Microsoft's approach to addressing the issue as 'grossly irresponsible, if not blatantly negligent.'  Yoran wrote that:  'More than 120 days since the vulnerability was reported, the bank in question remains vulnerable,' adding that many vulnerable organizations 'still have no idea they're at risk and therefore can't make an informed decision about compensating controls and other risk mitigating actions.'"



And then we heard from Microsoft.  Get this.  They wrote:  "A spokesperson for Microsoft said that the company 'appreciates the collaboration with the security community to responsibly disclose product issues,' and that security updates are ultimately 'a delicate balance between timeliness and quality, while ensuring maximized customer protection with minimized customer disruption.'"  Wow.



"Microsoft said Friday in a blog post that the issue has 'been fully addressed for all customers.'"  So I guess Yoran got their attention with his blog post in LinkedIn.  And, oh, what do you know, just two days later it's been completely fixed.  It was going to take until September 28th, but shine a bright light on the problem, oh, look, it's all fixed.  They said no customer remediation action is required, and that all affected customers were notified via email starting Friday.  Microsoft said its investigation "identified anomalous access only by the security researcher that reported the incident, and no other actors."



LEO:  No one's using this.  We don't have to worry.



STEVE:  Yeah, no, don't worry about it.  So they say:  "Yoran's broadside against Microsoft comes amid growing scrutiny of Microsoft in Washington after one of the company's products was abused by hackers based in China to steal the email messages of senior U.S. officials.  In that incident, hackers based in China were able to steal an encryption key that they could then use to forge authentication tokens, and security researchers have sharply criticized the company for not only allowing an encryption key to be stolen, but for building a computing architecture in which tokens could be forged in this way at all.



"The incident spurred Oregon's Senator Ron Wyden to call Microsoft 'negligent' in its security practices and request that the Department of Justice investigate whether Microsoft's actions in the incident broke the law."  Okay, now, I'll just say good luck with that, Washington.  A long time ago when Microsoft was much smaller and far less powerful, it was nearly impossible to hold its behavior to account.  There's just no possibility of doing so any longer.



"While Microsoft has insisted that the Chinese operation was highly targeted, research by the cloud security company Wiz suggests the incident may have been more broad than first understood, a claim Microsoft has dismissed as speculative."  You know, right.  Because Microsoft dismissed this as speculative because, after all, "a delicate balance is required between timeliness and quality, while ensuring maximized customer protection."



They said:  "The vulnerability discovered by Tenable allowed 'an unauthenticated attacker to access cross-tenant applications and sensitive data, such as authentication secrets,' according to Yoran's blog post.  It appears" - and everybody agrees - "that vulnerability does not exploit the same types of authentication flaws seen in the recent incident involving Chinese hackers, but may add pressure on Microsoft to improve its security practices."  Okay.  We can hope.  Unlikely.



"Industry professionals and government officials pointed out that the Chinese operation was only detected because a government agency was paying additional money for more sensitive logging capabilities.  Microsoft later reversed that policy."  Basically it was charging people for better logging of the activity on their cloud platforms; and they got a lot, they got into a lot of hot water for making money for, like, just offering more logging that, like, cost them nothing.



"Yoran, who has grown increasingly critical of Microsoft in recent years, told CyberScoop that the company's dominant position in the technology ecosystem makes many computer security researchers hesitant to speak up about its security practices, but that doing so is especially important given the ubiquity of its products."  To which I say, "Exactly."



And finally:  "Microsoft is a pretty strategic problem in the security space given the pervasiveness of their software, of their infrastructure," Yoran said.  "I also think they have to be part of the solution."  Well, yeah, because no one can make them do anything.



So, you know, I'm not a fan of complaining about problems that no one has any power to resolve.  As an engineer and technologist I most enjoy discovering and sharing solutions to problems.  But ignoring truly important issues in a podcast that's focused upon security seems negligent, too.  So we'll just keep perspective, discuss problems, and celebrate those companies who do act quickly and responsibly in the best interests of the users of their products.



But there is the issue of that recent serious attack by Chinese hackers.  Several weeks ago, while working on a previous episode of this podcast, I saw this news that's referred to in the CyberScoop piece.  I suppose I let it slide past because, well, what's that expression about beating a dead horse?  At some point I'm sure that we all get tired of complaints about Microsoft.  Sort of like how many ransomware attacks are we going to detail here?  At some point, what's the point?  But saturation shouldn't keep us from covering important security events, and this Chinese attack was very important and quite significant.



The best way to deal with it today is to refer to a well-known industry expert who very nicely framed what happened.  He is Bruce Schneier, and Bruce posted under the title "Microsoft Signing Key Stolen by Chinese."  Bruce wrote:  "A bunch of networks, including U.S. government networks, have been hacked by the Chinese.  The hackers," he said, "used forged authentication tokens to access user email, using a stolen Microsoft Azure account customer signing key.  Congress wants answers.  The phrase 'negligent security practices' is being tossed about, and with good reason," says Bruce.  "Master signing keys are not supposed to be left around, waiting to be stolen."



He said:  "Actually, two things went badly wrong here.  The first is that Azure accepted an expired signing key, implying a vulnerability in whatever is supposed to check key validity.  The second is that this key was supposed to remain in the system's Hardware Security Module (HSM), and not be in software.  This implies a really serious breach of good security practice.  The fact that Microsoft has not been forthcoming about the details of what happened tells me," says Bruce, "that the details are really bad."



And he says:  "I believe this all traces back to SolarWinds.  In addition to Russia inserting malware into a SolarWinds update, China used a different SolarWinds vulnerability to break into networks.  We know that Russia accessed Microsoft source code in that attack.  I have heard from informed government officials that China used their SolarWinds vulnerability to break into Microsoft and access source code, including Azure's."  He says:  "I think we are grossly underestimating the long-term results of the SolarWinds attacks.  That backdoored update was downloaded by over 14,000 networks worldwide.  Organizations patched their networks, but not before Russia and others used the vulnerability to enter those networks.  And once someone is in a network, it's really hard to be sure that you've kicked them out."



Bruce finishes:  "Sophisticated threat actors are realizing that stealing source code of infrastructure providers, and then combing that code for vulnerabilities, is an excellent way to break into organizations who use those infrastructure providers.  Attackers like Russia and China, and presumably the U.S., as well, are prioritizing going after those providers."



So Bruce nicely and succinctly explained what happened with the Microsoft Azure mess.  In short, they first deeply screwed up, then they failed to take responsibility for their screw-up.  And only now Washington is starting to wonder how Microsoft became this powerful.  Well, I've got a news flash for you.



I also thought of Bruce Schneier recently in another context because I love to quote one of his pithy observations, which is "Attacks always get better; they never get worse."  While that's kind of obvious, reminding ourselves of its truth serves as a nice reality check.  And in this case it explains what recently happened with the classic "attack" of listening to someone typing on a keyboard.  And with that, keys, Leo.  We're going to listen to you telling us about an advertiser.



LEO:  I'll type out an ad.  All right, Steve.  Yeah, boy, when I read that screed I thought of you immediately.



STEVE:  Yeah.



LEO:  You know, I think part of the problem with Microsoft is just that they have so many, it's such a big install base.  I think anybody with a big install base like that would have similar problems.  But anyway.



STEVE:  Well, yes.  When I saw him note the percentage of zero-days that were theirs, it was like, well, yeah.



LEO:  Of course.  It should be 100%.  It could be 100%; right?



STEVE:  Well, or if you were to break Microsoft up into individual organizations, each responsible for one of their different products, then, you know, the zero-days would be spread out the way they are among everyone else.  So, you know, it is the fact that they're such a behemoth.  On the other hand, that's a problem.  I mean, I used to know people at Microsoft who were really nice people.  Brad Silverberg was a great guy.  And, you know, the really good guys, they're gone.  You know, they took their money and left.  And now we're just left with kind of an unaccountable monstrosity.



LEO:  There's always Linux and Mac, if you ever want to change.



STEVE:  Hey, I love my unaccountable monstrosity.



LEO:  I know you do.  I know you do.



STEVE:  Well, yes.  So, okay.  As I was saying, Bruce Schneier reminds us always that attacks always get better, they never get worse.



LEO:  In the sense that they always get better for the bad guy.  They always get worse for the good guy.



STEVE:  Yes.



LEO:  Let's be clear.



STEVE:  Yes, exactly that.  Right, right.  Okay, so in this case, this explains exactly what happened recently with the classic attack of listening to someone typing on a keyboard.  Although significant controversy, understandably I think, surrounds questions regarding the current and future impact of ChatGPT-style conversational AI models, a huge amount of far less glamorous, yet nonetheless important, work is being done by applying some of these newly emerging AI-ish techniques to previously explored domains.



We've talked before about the concept of having a smartphone resting on a desk surface with its microphone passively listening to the keystrokes being typed nearby.  If this were practical, it would represent acoustic side-channel leakage from the keyboard.  And since confidential information might be entered through that keyboard, and since in general no one wants or expects to have their keystrokes surreptitiously monitored and recorded, it would represent an attack.  And speaking of attacks, as we said, they always get better.



Last Thursday, on August 3rd, a trio of researchers from three different universities in the UK published a paper for the 2023 IEEE European Symposium on Security and Privacy Workshops.  Their paper is titled "A Practical Deep Learning-Based Acoustic Side-Channel Attack on Keyboards."  Here's what they described from their research and of its success.



They said:  "With recent developments in deep learning, the ubiquity of microphones, and the rise in online services via personal devices, acoustic side-channel attacks present a greater threat to keyboards than ever.  This paper presents a practical implementation of a state-of-the-art deep learning model in order to classify laptop keystrokes using a smartphone integrated microphone.  When trained on keystrokes recorded by a nearby phone, the classifier achieved an accuracy of 95%, the highest accuracy seen without the use of a language model.  When trained on keystrokes recorded using the video conferencing software Zoom, an accuracy of 93% was achieved, a new best for the medium.  Our results prove the practicality of these side-channel attacks via off-the-shelf equipment and algorithms.  We discuss a series of mitigation methods to protect users against these series of attacks."



So this is a phenomenal level of recognition, 95%, for an outboard external microphone that's simply listening to keystrokes from a keyboard nearby.  And to only lose 2% accuracy when significantly compressing the audio through Zoom is equally astonishing.  Imagine being able to process the recorded sounds of someone typing after the fact through a compressed connection to be able to obtain a near-perfect rendition of what they originally keyed.  This is achieved essentially by utilizing far more of the total available information than any previous efforts have managed.



For anyone who wants the details, I've included a link to the entire 21-page research report.  But I think we already have the gist of the idea.  And there's an important lesson here for us.  Regardless of the outcome of the debate over the true longer term value of ChatGPT-style interaction, I think it's very clear that something has happened recently, and that the world has been changed.  We're still not sure of the "what" and "how" of all of these changes; and I'm also certain that they're still underway.  Research like this demonstrates that applications of the new deep learning models have only just begun to be explored.  I expect we're going to be seeing some very significant discoveries in the future relative to security, once these relatively new capabilities become more widely available.  And lord only knows what those side-channel attack masters at the Ben-Gurion University of the Negev in Israel are going to come up with.



LEO:  They've really made this their thing, haven't they.



STEVE:  Oh, my god.  Once they add deep learning modeling to their many bags of tricks.



LEO:  Oh, you're right.  You're right.



STEVE:  Yeah.  I mean, oh, boy.



LEO:  Wow.  Wow.  



STEVE:  Yeah.  I don't think we're going to have to wait long, either, because these guys tend to be on top of things.



Okay.  So some feedback from our listeners.  Rusty, tweeting as @rusty0101, he has another take on the "in the cloud or on the ground" discussion.  He said:  "Listening to this week's SN, with the discussion of running things in the cloud.  I'd noted that more and more people are running their own power stations, either with solar, wind, or water-wheel systems, including Amazon for at least one of their AWS sites.  I think that's becoming less and less of a useful counter argument," he said.  "Additionally," he said, "there have been recent cloud providers who've decommissioned equipment that was providing cloud services right up until it was shut down, and apparently end users didn't get the word for some reason, some of whom have lost significant functionality as a result.  Perhaps that's not going to be an issue for some of the larger providers, but if you are trying to work within a budget, there may be storm clouds on the way."



Okay.  So I think there's no question that there's a real and vital role for cloud-based services.  I'm not intending to suggest otherwise.  But there can also be a bit of a gold rush mentality of imagining that the only reason there's still anything that's not "in the cloud" is inertia, and that eventually everything will be.  I think the reality is there's probably a place for both.  And that's the point that I had intended to make.



I also sort of liked that whole notion that we're losing the inherent distributed nature of the Internet, which is one of the ways that it got so much of its strength and robustness.  You know, we all feel that everybody aggregating around a Chromium-based web browser is not a good idea because we end up with a monoculture.  So it certainly is the case that if anything like a really bad problem ever hit AWS, it would impact a huge portion of the users of the Internet, the providers of services.  And so that's not the way it used to be.  But we'll see what happens.



Alan C. Bonnici, he said:  "Hi, Steve.  I heard you speak about Authy and decided to give it a try."  Actually, I was not talking about Authy.



LEO:  Yeah, you're not in favor of Authy, yeah.  I've mentioned Authy back in the day, it was from Twilio.  But you and I both have come up with better solutions, I think, since.



STEVE:  Right.  Anyway, so basically he's talking about some two-factor authenticator, you know, a TOTP-based approach.  He said:  "So I reset the two-factor authentication code on a Gmail account to generate a new code.  What is strange is that the TOTP in my password manager is different from Authy.  I managed to log in with both.  Could it be clock differences between my desktop and my phone?  If yes, why would both work?  Fan of the show."



Okay.  So Alan did follow up a bit later to confirm that it was indeed a clock difference.  As to why both of the different codes would work, many authentication receivers, you know, authenticators, will continue to accept a recent, if not 100% current code.  When they receive a code, and the present one doesn't work, they may try the next one that's about to come up, or they might try the previous one that was just technically obsoleted, and maybe even the one before that.  The point is that for the system to work, both endpoints need to share, not only the same secret key, and there there's no fudge factor, of course, because that keys the pseudo-random sequence.



But they must also agree upon the current time of day.  In today's Internet-connected world, it's easy for devices to be within very close clock agreement because we're all able to get the time from the Internet.  But it's also reasonable to make some allowances for them not being.  And of course there is the whole man-in-the-middle, literally a person in the middle, reading the code from their phone and then turning around and transferring it by keying it in on their keyboard.  And so that's going to introduce a little bit of a delay, and thus some need for fudging.



So that's all that was going on was that the authenticator would much rather be a little bit tolerant and accept, you know, as long as only one use of the code is possible, that is, as long as the code when used cannot be used immediately again, I don't see any reason for giving the user a bit of leeway.  And also cutting down on user frustration of the system saying sorry, that code's invalid when you're quite sure that it just was.



Joe LaGreca, he said:  "I'm finally ready to leave the Google Chrome browser.  Which browser do you use or recommend?"  And he said, "Firefox?"  And of course Firefox is where both, Leo, you and I are, and I'm completely happy with the choice.



LEO:  Me, too.  I'm never changing.



STEVE:  Yeah.



LEO:  Well, I shouldn't say that, but...



STEVE:  I know, it's hard to say never.  So some time ago I did try using Bing because I was curious about tabs down the left-hand column of the browser.  But frankly, I was stunned when I encountered some sites that it would not render.  I thought, what?  You know, who knows?  Anyway, I had been using Chrome for a while, again, just to see how it compared to my longtime previous use of Firefox.



Today, having satisfied a little bit of my wanderlust, I'm back to Firefox.  And, you know, many things about it are just exactly right for me.  I need to use an add-on to get my tabs to run down the left side of the browser.  But there's a slick session manager that allows me to save entire browser sessions.  And I use that when I'm working on the podcast in order to change locations and have all the tabs sent to a different location.  So anyway, yes, I am 100% Firefox.  And when we get to talking about today's topic, which we'll get to shortly, you'll learn another reason why it continues to be my choice.



Someone whose name is Seven in Twitter, he said:  "Apologies in advance if this is a topic you've covered ad nauseam.  I listened to SN religiously from Episode 1 through several hundred, but I had to take a few years off from extracurricular listening.  I've since subscribed to Club TWiT" - thank you very much.



LEO:  Yay.



STEVE:  "...and returned to attending weekly services."



LEO:  Good.



STEVE:  He's back.  He's got the religion again.  He says:  "I don't know if my question will be simple enough to address in a DM, but perhaps at least with a suggestion where to start.  After receiving a notification that one of my accounts was compromised INCLUDING" - he has in all caps - "the password, I have come to fully realize that no passwords are safe.  Period.  I use 2FA wherever possible, but of course two-factor authentication support isn't consistent across all services.  Is there a best way to simplify the process of not relying on passwords alone?"  He says:  "Is there a simple answer to the question WWSGD?"



LEO:  What would Steve Gibson do?



STEVE:  Exactly.



LEO:  I love it.



STEVE:  Took me a moment...



LEO:  I need a button.



STEVE:  Took me a moment to parse WWSGD, but it's clear that that's what he meant.  Okay.  So I'll expand a bit on Seven's question by answering, "What does Steve think about the current and probable future state of identity authentication over the Internet?"



One way to view our current security environment, and I'll discuss a second way after this, is to see that what's developing is a spreading spectrum of options.  This is always what we get when new and better solutions at last start being adopted.  The reason we wind up with a spectrum that spreads is that the appearance of new and better solutions doesn't automatically kill off the older and less secure solutions.



Despite the fact that two-factor authentication has been widely available now for, what, a decade or two, most sites still don't offer it as an option.  Partly that's due to inertia, and partly due to a lack of perceived need, and partly because making logon more difficult increases support overhead to some degree.  And now we have Passkeys which represents yet another step forward.  But will Passkeys kill off two-factor authentication and passwords?  No.  Over time, more sites will be offering passkey support, maybe again as the perceived need at the higher end of the security spectrum manifests.



Eventually, support for two-factor authentication and Passkeys will be baked into servers, and servers will be taking more responsibility for authentication.  After all, servers were doing HTTP without any security not that long ago.  And it's only recently that sort of by universal agreement the whole industry just said, okay, we're just going to all decide we need to have it.  And EFF and Let's Encrypt made that more practical by not having people paying for certificates constantly.



But we also know that even as two-factor support and Passkeys becomes more available, many sites still won't care.  They'll feel that identifying their visitors with an email address and a password is sufficient.  And for many sites they're probably correct.  More and more often, Internet users are being asked, after all, to create an account as a requirement just to get in the front door.  Why?  Probably because it forces its visitors to turn over an email address for the receipt of follow-up spam.  It allows a site you may never choose to visit again to continue to plague you into the future.  And it may also be that sites will then be able to further monetize your existence by selling whatever information they managed to accumulate about you.  This is one place where today's topic "Global Privacy Control" may turn out to be relevant.  We'll see.



But the other fact is that email, and one's control over an email address, remains the ultimate fallback when anyone is unable to remember how to logon.  I've joked here in the past that "I forgot my password" link appearing underneath every password prompt makes a strong case for not even bothering with remembering any passwords.  Just bang on the keyboard for a while when you're creating an account, then click the "I forgot my password" link whenever you want to come back.  And I actually think that people would probably do that if it weren't actually quicker and easier to have a password manager remember and then fill in the answer for you.  But what does that mean about the actual security being delivered?



And from an actual security standpoint, I have to say that what's really infuriating and even somewhat confounding is to see a two-factor authentication prompt followed by a link saying:  "I'm unable to use my authenticator right now."  What?  What's the point of requiring one if you can just say "My dog ate it?" and then be allowed to logon without it?



LEO:  Usually you have to jump through some hoops, though.



STEVE:  Well.



LEO:  I mean, it's not just the password at that point, is it?  I guess it depends on the site.



STEVE:  Yeah.  It's your email.  Wikipedia now has an entry on the topic "Security Theater," defining it as "Security theater is the practice of taking security measures that are considered to provide the feeling of improved security while doing little or nothing to achieve it."  And rightfully, Wikipedia references Bruce Schneier as the originator of that perfect term.



And we know that Passkeys are going to be just the same; right?  Since no one ever wants increased authentication security to actually prevent anyone from authenticating, there will always be the "get out of jail free" card of "just click the link we sent to the email address you have on file with us so we know it's really you."



So to Seven's original question I would reply:  If you want the most security possible, the only thing you can do is to take advantage of the most secure authentication option available on a site-by-site basis.  Use a password manager to remember your random and long secrets.  Use an authenticator app to generate your one-time passwords.  As Passkeys become available, use them wherever possible.  But always keep in mind something that has not received enough attention anywhere by anyone.  And I sincerely hope that it does not receive more attention by the bad guys, which is that because security is about the lowest common denominator, and due to the ubiquitous role that email continues to play as the ultimate "my dog ate my authentication" authentication recovery - which might be better named "total authentication bypass" - any entity who is able to obtain access, even transiently, to your email flow obtains unfettered access to your entire online life.



LEO:  That's why you've got to secure your email.  Let's face it.



STEVE:  Exactly.



LEO:  That's really a...



STEVE:  Email security is really ultimately important.



LEO:  There are a number of sites now that don't use passwords, that just use email.  So they say, what's your email.  You say it.  They say, okay, we're going to send you a link.  Click that, and you're going to log in.  That's become more and more prevalent.  I guess that's not more secure, but that's just in response to people not doing passwords right.



STEVE:  Right, yeah.  I mean, I'm sure that their poor support people said they got just tired of receiving email, I forgot my password.



LEO:  Yeah, right.



STEVE:  So they said, okay, screw it.  I mean, after all, they were going to send a password recovery link anyway.



LEO:  Right.



STEVE:  So short-circuit the whole process.



LEO:  Right, and just assume everybody's forgotten their password.



STEVE:  And I have to say, Leo, when I was putting this together yesterday, I sort of had to pause and think, okay, what's wrong with just using email?  Why don't we automate, somehow automate, like, the email link receipt and forwarding process somehow?



LEO:  Yeah.



STEVE:  Like, you know?  Because as I said, that's what this all devolves into anyway.



LEO:  Well, we've always said the weakest link in your password recovery process is your security.  Right?  If there's a way to get a password, that's the weakest link.  So if it's email, it's email.  So you might as well just say, okay, we're just going to use email, not bother with anything else.  Especially if you're going to ignore two-factor.



STEVE:  Everything else gives you an oops, it doesn't work.



LEO:  Email me, yeah.



STEVE:  Or I'm standing on my head, so send me an email link and then let me in.  And you can always get in.



LEO:  That is a good way, that is a good reason, I guess, to use these unique email generators; right?  So that every site has a unique email.  Because, well, mostly it's just - really it would...



STEVE:  It forwards in, it always forwards into your master email, yeah.



LEO:  Yeah.  But mostly what it says is keep your gosh darn email secure as you possibly can.  Have two-factor on that, yeah.



STEVE:  Yup.



LEO:  And I don't think Google, if you lose your two-factor, just says oh, fine, we'll email you something.  I think you have to jump through more hoops than just that.  I believe so.  But I might be wrong.  I haven't tried it.



STEVE:  Yeah.



LEO:  Have an email provider that really lets you lock the sucker down.



STEVE:  Well, and actually this is why one of the features that I built into SQRL was after you got comfortable with SQRL, there was a checkbox in the config where you could say, "Please disable all other authentication."



LEO:  No recovery, yeah.  I don't want any...



STEVE:  Period.



LEO:  Yeah, there are no other ways to recover.



STEVE:  Otherwise you don't actually get any additional security.



LEO:  Yeah.  I'm now looking, what happens if I lose my 2FA token on Fastmail?  I'm just curious because that's where everything is.  It uses your phone.  You get a code sent to your phone.



STEVE:  Okay.  That's...  



LEO:  Better than nothing; right?



STEVE:  Better than nothing.  Although, you know, SMS is not great.



LEO:  Right.  That's another - so one more thing, another in the list of things to do is make sure that you have either PIN-protected or somehow protected your cell phone account so that somebody can't SIM jack you.  And actually the FTC is moving forward on regulation on that because that's really important.  I'm trusting Google Fi not to let people, you know, steal my number.  I think Google's pretty good at that.



STEVE:  Trust is good.



LEO:  Well, yeah, but trust is good if it's merited.



STEVE:  I know.  That's kind of what I meant.



LEO:  It's got to be earned first.  All right.  Now I have no slogan whatsoever for this ridiculously named, but I think very potent technology, Global Privacy Control.



STEVE:  So today's podcast adventure was triggered when I followed a news link yesterday over to TechCrunch.  The screen darkened with an overlay, as screens do these days when a site wants to bring something to its visitor's attention.  And I was left staring at an interesting notice from TechCrunch.  It had their TechCrunch logo, which I've always thought was kind of cool, in the upper left-hand corner.  And the headline read:  "Review your Global Privacy Control preferences."  And the notice said:  "You're using Global Privacy Control (GPC).  This leads to a lower-quality experience on Yahoo..."



LEO:  What?



STEVE:  "...by blocking certain editorial content, including embedded tweets, YouTube videos, and third-party ads that are relevant to your interests."  Huh.  Okay.  That was interesting.  It gets better.  "To enhance your Yahoo experience, allow us to share and sell your personal information."



LEO:  This would be so much of a better experience that way.



STEVE:  Right, right, because, you know, it's clearly in my best interest to have my Yahoo experience enhanced.



LEO:  Sure.



STEVE:  By allowing them to share and sell my personal information.  No doubt about it.



LEO:  Makes it so much better.



STEVE:  And there's more.  They continue:  "This includes technical identifiers like your IP address and cookie IDs, but does not include things like personal emails or contact information."  And then this notice concludes with:  "This won't affect your GPC settings for other websites, and you can always change this preference in Privacy Controls."  And then at the bottom of this little pop-up I was presented with two options, Allow or Don't Allow.  And you can probably guess which one I chose.



Okay.  Now, there are several bits of good news here.  One is that someone made them do this.



LEO:  You made me.



STEVE:  That's right.  They didn't want to do this.  They didn't want to ask me this question.  The other is that the only reason I received this notice was that I took my own advice back on May 3rd of last year, 2022, as a result of our podcast 869, which was titled "Global Privacy Control."  I flipped a switch that's built into Firefox and then promptly forgot about it.  But that switch remained flipped.  And I should note that perhaps it's no surprise that the switch is missing from Chrome.  However, in addition to Firefox, which incorporates it natively, it is present in both the Brave and the DuckDuckGo privacy browsers, and it can be added to Chrome with the use of a third-party extension.  We'll get to all that later.



So we have many things to talk about here.  First of all, to clear up one question, I was visiting TechCrunch, and I was informed by that pop-up that Yahoo wanted me to drop my pants.



LEO:  I like that.  Might as well.



STEVE:  Wikipedia explains this by writing:  "In 2010, AOL acquired TechCrunch for approximately $25 million."



LEO:  Yeah, they've got to make that back somehow.



STEVE:  Uh-huh.  "Following the 2015 acquisition of AOL and Yahoo by Verizon, the site was owned by Verizon Media from 2015 through 2021.  In 2021 Verizon sold its media assets, including AOL, Yahoo, and TechCrunch, to the private equity firm Apollo Global Management, and Apollo integrated them into a new entity called Yahoo! Inc."



LEO:  Sigh.



STEVE:  So that's why I went to TechCrunch, and I was talking to  Yahoo.  The next thing that caught my eye in that pop-up was their term "technical identifiers," which the notice was hoping I would be willing to allow them to share and sell.  Seems we should know what those are.  The term in that pop-up was also a link which took me to a Yahoo page titled "Collection, Use, and Linking of Technical Identifiers," where they write:  "Yahoo uses" - meaning TechCrunch because that's a parent, global - "uses different technical identifiers to make its consumer services available on most platforms, browsers, and devices."  And again, right, "to make its services available."  Well, they work just fine without those, but no, we need those technical identifiers.  Sorry, didn't mean to editorialize.  They continue:  "Yahoo also uses these technical identifiers to provide our digital advertising services on our properties and for our business partners.



"As detailed in our Cookie Policy, these technical identifiers include:  browser cookie identifiers (sometimes referred to as 'cookie IDs') and browser local storage identifiers; mobile device identifiers, such as the Android advertising ID or the Apple Identifier for Advertising (IDFA); platform or operating system-based identifiers, such as those offered on smart or connected TVs or media streaming devices; partner-supplied technical identifiers; encrypted or one-way cryptographic hashes of personal information such as email addresses, phone numbers, account identifiers, derivatives, or escalated versions of these identifiers.



"Now, of course, when you cryptographically hash something like your phone number, it becomes an identifier for you, not of you, but enabling cross-everything association of you.  And of course email addresses, phone numbers, account identifiers and so forth.  Household-based identifiers; IP addresses; probabilistic non-unique identifiers, right, like fingerprints.  They can't be sure that's you, but eh, close.  Identifiers generated from the combination of various device, browser, or operating system attributes, such as the operating system or browser version; 'cohort,' audience, or group identifiers, such as 'sports enthusiasts.'"  Okay.  That won't work for me, but you get the idea.  



"The storage, generation, and collection methods of these identifiers may also vary, depending on the context.  For instance, some browsers and devices offer limited technical identifier support and/or limited cookie support, so non-cookie-based identifiers may be used in these cases.  Examples of these devices include:  smart or connected TVs, over-the-top (OTT) streaming devices such as a Roku device, and similar interactive media players; digital-out-of-home (DOOH) billboards and similar media devices; browsers enabled with intelligent tracking prevention (ITP), privacy sandbox, or similar cookie-blocking technology."  Oh, no.  "Certain apps, mobile devices, or installed software, where permitted and applicable; certain Internet-of-Things devices."  In other words, every effing thing we could...



LEO:  Everything you've got.



STEVE:  Yes, we paid.  We have rooms of techies trying to find some way to track you, hook onto you, see where you went, what  you're thinking, what you like, what you're doing.  We want it all.



LEO:  This is a great document because they're basically, I mean, they figure nobody's going to read this.  But they're revealing everything.  Right?  They're just...



STEVE:  Yes.



LEO:  I mean, it's really telling, yeah.



STEVE:  Yes.  It is every frigging thing anybody we ever had was able to think of.  So they say:  "The collection methods for technical identifiers" - and remember, that's where they wanted me to click OK, yeah, let's go.  "The collection methods for technical identifiers and associated data depend on the context, as described here.  When using the Internet in a browser, for example Chrome, our consumer services and digital advertising services may use standard cookies, Javascript code, libraries" - that's rather generic, but okay - "and/or dynamic HTML tags, web beacons, and similar technologies.



"In mobile apps, our consumer services and digital advertising services may use mobile software development kits" - again, generic - "local or remote application programming interfaces" - same - "and similar client or server-side code.  In other cases, we may exchange data and files such as log files with our partners in 'offline' contexts using secure server-to-server transfer methods, APIs, cloud services, mutual agents or technology services providers" - maybe alien technology, who knows? - "and other industry-standard methods.



"Technical identifiers may be used to identify a user across multiple devices, often referred to as 'cross-device-linking' or 'cross-device identifier resolution.'  As a result, technical identifiers that are presumed to belong to a particular user, device, or household can be linked to one another, and the associated technical identifier may be used to reference data, personalize advertisements, or tailor experiences.  This process may be implemented and used by us or in coordination with our advertising partners as part of our digital advertising services."  Just click YES here to proceed.  Oh, my lord.



So, in short, "technical identifiers" amounts to pretty much anything and everything they can possibly get their hands on to track me and associate me with any members of my family and presumably coworkers through instances of shared IP addresses, tracking us across any and all of our devices using every trick and technique that's available to them.



Thank god I said no.  But also thank goodness I was asked, and had the opportunity to say no.



LEO:  Good point.



STEVE:  Not everyone is given the option.  You have to ask for it.  And we have California's state legislature and Attorney General, as well as those in Colorado and Connecticut, to thank for this.  I'll explain all that in a moment.  But this is certainly not my first visit to TechCrunch recently.  I've been popping over to TechCrunch from time to time with Firefox, following links to news to share with this audience, and this is the first time I've ever seen that pop-up.  So this is new behavior.  I received that notice because ever since we first talked about this in May of 2022, my Firefox browser has been broadcasting the standardized GPC - Global Privacy Control - signal to indicate that I do not wish to have my "online experiences enhanced," thank you very much, at the cost of my, my family's, and my company's privacy.



Okay.  Before I move on, I need to note that while digging deeper into what was up with TechCrunch, I followed TechCrunch's "Your Privacy Choices" link which you can find at the bottom of their pages.  The first interesting thing is that the right-hand side of the page has specific subpages for California, Colorado, Connecticut...



LEO:  Interesting.



STEVE:  Uh-huh, as I mentioned before.  And there's also one for Virginia.  The California page I was taken to has two switch settings.  Oh.  And I should also note that somewhere I did see something about IP geolocation, which is obviously how they decided that I'm in California.



LEO:  Right.



STEVE:  Because I hadn't logged into them.  So that's going on also.  And I tried to go back and find it, but I couldn't.  So anyway, the California page I was taken to has two switch settings.  Due to my previous reply to the pop-up, the first one was turned off and was set to "Don't Allow."  That corresponded to "Allow the Sale and Sharing of My Personal Information."  That's now off.  But there was a second switch, and it was still turned on and set to "Don't Limit."  And that one corresponds to "Limit the Use of My Sensitive Personal Information."  And the page explains - and Leo, if you go to TechCrunch and scroll to the bottom and click on Your Privacy Choices, you'll see this.



So under this "Limit the Use of My Sensitive Personal Information," the page explains:  "In connection with providing our services, we may use sensitive personal" - listen to this - "sensitive personal information such as precise location data and email content data.  Among other purposes, we use such data to help understand" - oh, and both of yours are on - "to help understand your interests so we can show you more relevant ads and content."



LEO:  Look how long it's taking to turn it off.  This is definitely wait for 30 seconds before refreshing page.  Holy cow.



STEVE:  Actually, I think you'll find that the Firefox has died now.  I don't think you can close that tab.  I don't think there's anything you can do.



LEO:  Oh, even worse.  Oh, my gosh.  I broke - you broke Firefox.  Wow.



STEVE:  Yeah, they really don't want you to turn that off.



LEO:  Oh, my goodness.



STEVE:  It's there.  Uh-huh.



LEO:  Don't flip that - how did you turn on your Global Privacy Control?  Is that now a switch in Firefox, or is it automatic?



STEVE:  Yes.



LEO:  Okay.



STEVE:  Yes.  It's a switch in Firefox.  And I presume that if you find it and turn it on, and then go back to TechCrunch, you'll get the pop-up experience that I got.



LEO:  Yeah.  Yeah.



STEVE:  So that would be interesting to see.



LEO:  Do I have to do about:config?  That's how we did it last time, when we talked about this last time.



STEVE:  I'm sure it's there, yes, about:config.  And then maybe just in the search box maybe "GPC" or type "global privacy" and see if it finds it.



LEO:  Yeah.  Okay.



STEVE:  So they said:  "To opt out of the use and limit our use of such information" - that is, my sensitive personal information - "to only those purposes" - get this - "only those purposes permitted by California law, select 'Limit.'  This may make the content and ads that you see less relevant to you."  Oh, boohoo.  Okay.  So after doing some additional research I figured out what's going on for anyone in California, and to some other degree the other states - "why Yahoo! is showing two switches.  Under the California" - and you found it in Firefox?



LEO:  I found it.



STEVE:  Yeah.



LEO:  And now I want to enable it.  And now it's enabled.  Now let's go to TechCrunch, see what they say.  Wow.  You know, I go to TechCrunch every day.  Nothing so far.



STEVE:  At least get the pop-up.  For me it grayed out.  So it'll see, it'll be interesting to see if at some point, since you do go every day, and presumably it sees you in California...



LEO:  Yeah, it does, because I'm getting California Privacy Laws.



STEVE:  Ah, ah, good point, exactly.  Right, right, right.



LEO:  Yeah.  Yeah.  But every time I turn this off it crashes.  How about if I limit the use of my sensitive personal information.  Oh.  Oh, this is so bad.



STEVE:  This is so bad, Leo.



LEO:  The FTC needs to jump right in on this.  This is terrible.



STEVE:  It's so bad.



LEO:  Oh, my goodness.



STEVE:  Okay.  So I figured out what's going on in for anyone in California and why Yahoo is showing two switches for us.  Under the California Consumer Privacy Act (CCPA), California consumers have the right to opt-out of the sale and sharing and the use of their personal information.  Okay.  Those three things:  sale, sharing, and use.  But the Global Privacy Control as it is presently defined - and it's quite unlikely to ever have its strength broadened - ONLY applies to those first two of the three:  personal information sales and sharing.  The GPC does not also cover the use of personal information.  But California law does.  So if Californians want to prohibit the use of their personal information, beyond its sale and sharing, which can be done globally with the GPC setting, that will still need to be done on a site-by-site basis.  So, and Leo, one thing you might try is restarting Firefox.



LEO:  Oh, just close it all out, yeah, you're right.  Start over.



STEVE:  Yeah, having put that on.  Because now that your GPC is on, that upper switch in TechCrunch should really not be on.  Because your browser is now shouting no, no, no.



LEO:  Do not.  No, no, no.



STEVE:  And California law requires that this be obeyed.



LEO:  We can only hope that there are massive penalties for not because I restarted it, and I'm still getting a normal page.  Let me try one more time to switch that switch.  Geez, Louise.



STEVE:  Or see if the switch is already flipped for you.



LEO:  Oh, yeah, good, okay.  Yeah, let me check that.



STEVE:  Yeah, it ought to be turned off.



LEO:  Oh, okay.  I didn't get any pop-up, though, of saying anything.



STEVE:  No, no.



LEO:  All right.  Let's see.  Is it already turned off?  No.



STEVE:  Oh, boy.



LEO:  No, they're just ignoring it.  And of course - so I clicked a button.  We'll be back later.  Bye-bye.



STEVE:  Okay.  So here's how the GPC, the topic of today,  places itself, its need, and the role it's filling.  The GPC's formal specification explains:  "Building websites today often requires relying on services provided by businesses other than the one which a person chooses to interact with.  The result is a natural consequence of the increasing complexity of web technology and the division of labor between different service providers.  While this architecture can be used in the service of better web experiences, it can also be abused to violate privacy.  While data can be shared with service providers for limited operational purposes, it can also be shared with third parties or used for behavioral targeting in ways that many users find objectionable.



"Several legal frameworks exist  and more are on the way  within which people have the right to request that their privacy be protected, including requests that their data not be sold or shared beyond the business with which they intend to interact.  Requiring that people manually express their rights for each and every site they visit is, however, impractical."  And then the spec quotes the California Attorney General, saying:  "Given the ease and frequency by which personal information is collected and sold when a customer visits a website, consumers should have a similarly easy ability to request to opt-out globally.  This regulation [in California] offers consumers a global choice to opt out of the sale of personal information, as opposed to going website by website to make individual requests with each business each time they use a new browser or a new device."



So the spec says:  "This specification addresses the issue by providing a way to signal, through an HTTP header or the DOM (Document Object Model), a person's assertion of their applicable rights to prevent the sale of their data, the sharing of their data with third parties, and the use of their data for cross-site targeted advertising.  This signal is equivalent, for example, to the 'global privacy control' in the CCPA regulations."  And also, subsequent to the passing of the regulation, the Attorney General formally acknowledged or asserted that this GPC signal is within the scope of what California considers a global assertion that an individual wants this to hold.



Okay.  So what's also annoying, though, now that I've woken up to this, is that I should have never received that pop-up in the first place.  My browser's GPC setting is not the default.  And it's not even available from Chrome without an add-on.  So if a browser is broadcasting it, and for example yours hadn't been, Leo, if a browser is broadcasting it, it's because this is what its owner meant and wants.  Which means that the pop-up I received was TechCrunch's "Are you really sure this is what you want here?  Would you consider changing your mind, pretty please with a cherry on top?"



I'll also note that all four of the states that have enacted GPC-specific legislation have differing definitions and language in their laws. So each of those four pages, where TechCrunch's parent company Yahoo! is juggling legislation, differs from the others.  This means that we now have state-by-state privacy laws, and that Yahoo! is desperately clinging to the leverage of every bit of personal information available - its sales, sharing, and internal use - that they can, on a state-by-state basis.



Okay.  So now let's step back a bit to get some perspective on the whole Global Privacy Control issue.  I found a great write-up at a site called firewallsdontstopdragons.com.  And the guy gets most of this right, and I'll note where he made a couple mistakes.



But he writes:  "You are tracked mercilessly today when you surf the web, either on your computer or your smartphone.  Websites use several different techniques to identify you and record as much data about you as they can.  While marketers will claim that you have the power to opt out of most tracking, this is frankly impossible to do, practically speaking.  There are simply too many trackers, many of which you'll never know about.  There's a newish initiative that aims to address this problem called Global Privacy Control, or GPC.  GPC is a browser setting that lets you automatically tell every website you visit to stop collecting your data.  Sounds good; right?  But it also may sound familiar.



"Back in 2009, a group of researchers had a brilliant idea:  Why don't we give users a way to tell every website they visit that they don't want to be tracked?  They came up with a simple, global Do Not Track (DNT) flag that users could set on their web browser once and forget it.  Their browser would, in turn, tell every website you visited they visited that they did not wish to be tracked.



"The obvious problem here is that websites at that time were under precisely zero obligation to comply.  But there were also a couple interesting twists to the story.  At one point, Microsoft took it upon themselves to automatically enable the DNT flag for Internet Explorer users.  Advertisers were outraged because the flag was supposed to be an affirmative action taken by the user.  They used this move as another reason to ignore the flag.  And in an ironic twist, the very fact that your browser set this flag now made you more trackable.



"It turns out that DNT was a little ahead of its time.  Without any legal reason to comply, it never caught on and was eventually abandoned.  If it had only held out a bit longer, it might have been relevant.  The European Union's General Data Protection Regulation (GDPR) was just coming online around the same time DNT was abandoned.  However, the GDPR user consent verbiage didn't seem to explicitly recognize DNT.



"Enter Global Privacy Control.  From everything I can see," he wrote, "it's really just DNT 2.0.  However, this time there are legal requirements, at least in some regions, to actually require compliance.  In particular, the California Consumer Privacy Act (CCPA) and subsequent California Privacy Rights Act (CPRA) have explicit language requiring sites to honor these automated requests not to be tracked.  Similar laws have been passed in Nevada, Utah, Colorado, Virginia, and Connecticut, with others coming.  GPC may yet succeed where DNT failed."



Okay, now I'll pause here just to note that just as the terms "Do Not Track" and "Global Privacy Control" sound like different things, indeed they are.  So as much as I like what this author has written, everyone who follows this podcast knows that I'm a stickler for detail.  So when he says that GPC is really just DNT 2.0, that's only true inasmuch as it's a global beacon that browsers can be configured to send.  That part of GPC is the same as DNT.  But just to be clear, GPC is explicitly not about tracking.  As I've been careful to say, it's about prohibiting the sales and sharing of personal information.  This author continues to make some good points, however, about how to enable global privacy control.



He writes:  "This is not a slam dunk.  For one thing, there is no U.S. federal law requiring companies to respect GPC.  Also, the GDPR interpretation of GPC sadly seems a little weak."  Well, give them a little time.  I bet they'll fix that.  "There are still too many regions that have no privacy regulations.  And the various regulations that do exist need to be 'harmonized' with one another on what GPC really means.  For example, does the request apply only to further data collection, or should it apply to data already collected?  Does it apply to the user, or just the device that set the GPC flag?



"If you're lucky enough," he writes, "to live in a region that has privacy laws, it's a no-brainer.  Just enable it.  But even if you don't, there's no reason you shouldn't go ahead and register your desire not to be tracked."  Which I'll correct to remind everybody not to have your personal information sold or shared.  But otherwise he's right.  He says:  "Then, whenever and wherever this request is required to be honored, you'll get the benefit."



And he finishes:  "Thankfully, it's pretty easy to do.  And if you're already using privacy tools, you may find that GPC has already been enabled.  The test is simple:  Go to the Global Privacy Control website.  If you see a green dot and 'GPC signal detected' at the top, you're good."  And so the site is globalprivacycontrol.org.  Just go https://globalprivacycontrol.org.  And there is a little banner that you get at the very top.  You should see a green dot to confirm that your browser is currently sending that out.



LEO:  Oh, mine's not.  Which is why I perhaps was getting...



STEVE:  Yes, good.  So now google how to enable GPC for Firefox while I keep going.



LEO:  I turned it on in the about:config, but I guess that wasn't enough.  I'll have to...



STEVE:  There must be something else, yeah.



LEO:  Yeah.



STEVE:  So Colorado's Privacy Act (CPA) and Connecticut's Data Privacy Act (CDPA) both recently went into effect...



LEO:  Oh, there's two options, that's all.  I have to do both.  Okay.



STEVE:  Ah, good.  They both went into effect on July 1st; right?  So only a little over a month ago.  And like California's CPRA, those states' legislation require companies to honor the GPC.  But Virginia's apparently doesn't, so that's unfortunate.  Virginia has some laws, but it's not - it didn't anticipate GPC.  I see you've got two things there now.



LEO:  There we go.  There, now they're both enabled.  Ah, okay.



STEVE:  And so first go to globalprivacycontrol.org.



LEO:  And make sure, yeah.



STEVE:  And see if you get a green dot.



LEO:  Mm-hmm.  Okay.



STEVE:  And then back to TechCrunch.



LEO:  Yeah.



STEVE:  So today, Firefox, Brave, and the DuckDuckGo privacy browser all support the GPC.  As for browser extensions for Chrome and other Chromium browsers that do not yet natively offer - yup, you're green now.



LEO:  Green light.  All right.



STEVE:  There's Abine, which is from the DeleteMe people, I think a sponsor of the TWiT network.  Disconnect, OptMeowt - where Meow, remember, is the cat's meow, so O-P-T-M-E-O-W-T, OptMeowt - and Privacy Badger, which - and that's a name I just hate, but that's from the EFF.  Badger, really, Badger?  Couldn't like, you know...



LEO:  It's like a honey badger.  It just doesn't give up.



STEVE:  How about Privacy Fairy or something, I don't know, but not badger.  I have a link here at the end of the show notes to the GPC page which maintains a list of available extensions.  So at the moment those in California, Colorado, and Connecticut, the three C's, have the advantage of state laws which compel compliance with their residents' GPC request.



LEO:  Bravo.



STEVE:  It doesn't appear that websites serving Virginians, which does have similar privacy laws, are similarly bound to follow the GPC signal.  But what we need now that the GPC exists and is gaining some traction will first be for additional states, which is easier to do, to step up and add their voices with their own statewide legislation.  Then we need the U.S. federal government to take this initiative national.  At that point everyone will be on equal footing with the ability to opt into this, and thus opt out of having your personal data sold and shared.  And then we can imagine a day when a federal law won't require the presence of a GPC beacon.  Well, we can dream; can't we?  Leo, you and I may not see that day, but maybe our grandchildren.  Well, I don't have any, but, you know.



LEO:  Nor do I, yet.



STEVE:  You will.



LEO:  Yeah, yeah.  Very good stuff.  And gosh, I hope you're right.  That's all I can say.



STEVE:  If nothing else, it doesn't look like it's going to die.  California, Colorado, and Connecticut are requiring it.  As a consequence of my having turned it on, I got the pop-up with Yahoo saying are you sure?  And, boy, am I glad I was asked.  So it'll be interesting to hear back from our listeners as they experience the effects of having this turned on.  I can't imagine why everybody would not turn this thing on.



LEO:  Yeah, yeah.  I'm going to go through all my browsers.  Obviously, you can't in Chrome.  But are there Chrome plugins that will let you enable it?



STEVE:  Yes.  Abine has one, and Disconnect is one.  And that horrible Privacy Badger thing, I mean horribly named, I'm sure it's a good thing.



LEO:  They'll let you turn it on in Chrome because Google doesn't want this.



STEVE:  No.



LEO:  But we're going to make them.  And Yahoo, it's still spinning.



STEVE:  Oh, my lord.



LEO:  Yeah.  Who knows, you know.  I turned off all of the tracking protection because, you know, I have to say Firefox has very good tracking protection, disabled all of that.  Turned off UBlock Origin for it.



STEVE:  That was just what I was going to ask.



LEO:  Yeah, no, turned it off.



STEVE:  What about starting it up, there's like a startup with no extensions.



LEO:  I might have to do that.



STEVE:  Where it starts up clean.



LEO:  And we also have quite a bit of stuff in our network, our company network.



STEVE:  Well, Leo, it happened to me.  I couldn't do it either.  I got that thing, I couldn't even close the page.  I had to go to Task Manager and abort the process to get out.



LEO:  Wow.  I can at least close the page.  You know, we have so many, and rightly so, perimeter protections on here and stuff.  I just don't know if it's something we're doing or something they're doing.  It doesn't look like they're compliant right now because when I go to TechCrunch.com and click the privacy choices, those switches are still on, and I cannot turn them off.



STEVE:  Yup.  And your browser is broadcasting a, you know, eff off signal.



LEO:  It's really frustrating.  So frustrating.  Okay.  But Steve, see, if we listen to the show, right, we know.  We are informed.  And that is the first step into changing the world.  That's because of this guy right here, Steve Gibson.  GRC.com is his website.  He has the podcast there.



Copyright (c) 2023 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#935

DATE:		August 15, 2023

TITLE:		"Topics" Arrives

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-935.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  Today, we have a birthday to celebrate.  And then I wound up encountering so many interesting thoughts shared by our terrific listeners that, once I had written everything that I wanted to say regarding the emergence of Google's long-awaited Topics system to replace tracking while still giving advertisers what they need, I'd filled up 18 pages of show notes and run out of space for other news.  So next week I'll catch up with everything else that's been happening.  But the topic of Topics is, I think, important enough to have most of a podcast for itself.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  We're going to talk about Google's final, I guess, proposal for advertising without onerous tracking.  It's called Topics, and Steve says it's a good thing.  We'll also talk about Password rules.  Sometimes they can get ridiculous.  And then Steve has some very good news for all of us.  Stay tuned.  You'll be celebrating next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 935, recorded August 15th, 2023:  "Topics" Arrives.



It's time for Security Now!, the show where we cover the latest in security breaches, what's going on with MOVEit ransomware with this guy right here, Steve Gibson.  Hello, Steve.



STEVE GIBSON:  All that good stuff.



LEO:  It's the MOVEit show these days, I swear to god.



STEVE:  Yeah, for the middle of August.



LEO:  Yes.



STEVE:  And, okay.  So there's a lot going on.  Today's topic is Topics, which has officially arrived from Google at long last.  But we're going to start by celebrating a birthday.



LEO:  Oh.



STEVE:  It's not only 25 years ago that the iMac was created, but it's this podcast's birthday, as well.



LEO:  Not quite so old.



STEVE:  Not quite so old, no.



LEO:  Almost.  Close; right?



STEVE:  So, yes.  So we're going to talk about that.  Then I wound up encountering so many interesting thoughts shared by our terrific listeners that, once I had written everything that I wanted to say regarding the emergence of Google's long-awaited Topics system, which replaces tracking, while still giving advertisers what they need, I had already filled up 18 pages of show notes, and I ran out of space for any other news.  So next week I'll catch up with everything else that's been happening.  But the topic of Topics is, I think, important enough to have  most of a podcast for itself in any event.  And so we're going to dig into everything about that.  And of course we've got a terrific Picture of the Week.  So I think another great podcast for our listeners.  And as you can see, another accident here.



LEO:  Oh, my goodness.  You've got to be careful with those straight-edge razors; you know?  You could really get in trouble.



STEVE:  Yeah, it was so dumb because I had just finished a day of working on SpinRite.  And so I showered, and then I shaved.  And my mind was a thousand miles away, thinking about some detail of SpinRite, I wasn't even paying attention; you know?  And so that's a lesson is like, Gibson, it's only going to, you know, give it three minutes.  That's all it'll take.



LEO:  I have something, too, after your Picture of the Week.



STEVE:  Oh, that's right. You said you had a physical...



LEO:  I have a little physical thing, a little Picture of the Week.  If you can tell me what this is...



STEVE:  An IoT birdhouse?



LEO:  Well, in a manner of speaking.  It certainly looks like a birdhouse.  There's no room for birds in this thing, though.



STEVE:  Is there a little red light on the...



LEO:  There is a little red light going on and off on there; isn't there.  I wonder what that is.  Well, we'll show you in just a little bit.  I think you'll be very - you'll be, I think, pleased to see what it is.



STEVE:  Okay.



LEO:  All right, Steve Gibson.  I am prepared for a Picture of the Week, if you would like.



STEVE:  So this one is another one that's going to require a bit of description, which I think I can give it.  It's a great picture if you just see the show notes.  You'll go, ah, that's great, just as you did when you saw them, Leo.  So imagine - this is apparently a hotel which has a very vertical profile.  And I think on the outward facing edge or side, you know, face of the hotel, is probably a staircase which is enclosed, but at each of the hotel's floors where there's like a plateau between the stairs going up or down to the next level, is an opening.  And this opening would be square except that the lower half is cut in a ways.  Anyway, the overall effect is that the opening at each floor has exactly the profile of an RJ45 Ethernet jack, you know, Ethernet socket.  And somebody looking at the hotel said, you know, that looks like...



LEO:  Hey, it looks like my powered switch.



STEVE:  That's exactly right.  That looks familiar; you know?  Where have I seen this before?  Oh, it's like on every router and every hub and so forth.



LEO:  Really do.



STEVE:  So this person very cleverly positioned themselves in the right place, got a - it looks like maybe a four or five-inch RJ45 Ethernet, you know, wired Ethernet jumper, and is holding it close to the lens with the hotel in the distance, such that the Ethernet connectors are the same size in this perspective as the openings in the side of the hotel, looking exactly like you could plug this cable into the side of the hotel.



LEO:  Lovely.  Lovely.



STEVE:  And of course the caption is "We just need a jumper here."  Because that's what the...



LEO:  That's the short one is a jumper.  I love it.



STEVE:  Yes.  A little jumper, a little Ethernet jumper.



LEO:  Now...



STEVE:  Okay.



LEO:  I have one for you.  If you've listened to this show, and especially if you've listened to the holiday episodes, you know that Steve in his youth created something called the Portable Dog Killer; right?  Which was intended really not to kill dogs, but to chase them away by playing high-pitched sounds that only the dog could hear; right?



STEVE:  Yup.



LEO:  Meet the Portable Dog Killer from PetSafe.  This is - the reason it looks like a bird feeder or a bird house is, if you've got a neighbor with a dog that bothers you, just as that dog used to bother little Steve, you hang this on the tree right by the fence.  When you turn it on, it's got a microphone up here that listens for barking.  And when it hears it - and by the way, that's why the red light's going off.  I don't know, maybe we're - I don't know if we broadcast wide enough frequencies.  People, if you're hearing a very high-pitched tone right now, I apologize.  That's the - it's called the PetSafe OBC-1000.  See, I think the PDK would have been better.  But we have it here because Burke, as you might know, has his little dog Lily, and she barks a lot.  So Mikah told him about this and said, if you buy this and put it in the hall...



STEVE:  We will all thank you.



LEO:  No, I love it.  Lily can bark all she wants.  Doesn't bother me.  But what I thought was interesting is you thought of this years ago, never commercialized it.  But here it is as a commercial product.



STEVE:  Very cool.



LEO:  The PetSafe Portable Dog Killer in the form of a birdhouse.  Isn't that amazing?  See, you were ahead of your time.



STEVE:  Again, I was, what, 15 or 16.  So to me, calling it the Portable Dog Killer, I mean, and it looked like a laser gun.  So, you know, and we were all - we were raised on "Lost in Space" and "Star Trek."  And so, you know, that's what I was going to call it.



LEO:  I think this is commercially probably a little more acceptable.



STEVE:  I think so, too.



LEO:  Just, yeah, keep it arm's length away from human ears to avoid hearing damage.  If a dog begins barking while you're setting it up, mounting or hanging the - it's called the Outdoor Bark Control, OBC-1000.



STEVE:  Ah, there we go.



LEO:  Yes.  But Burke came running in and said, you should turn that off if you're going to put it on the air.  It might deafen our listeners.



STEVE:  Okay.  So Leo, you and I recorded Episode 1 of Security Now!...



LEO:  Oh, my.



STEVE:  ...on August 19th of 2005.



LEO:  Oh my, Wow.



STEVE:  Today is August 15th of 2023.



LEO:  Holy cow.



STEVE:  Uh-huh.  Which means that with today's podcast number 935, we will have finished our 18th year.  And next week's podcast will be the start of our 19th year.  And when I went back to check the date, you've got it on the screen, of that first podcast - which was, by the way, all of 18 minutes long - I think probably I just...



LEO:  You want to hear a little bit of it?



STEVE:  I just took a little stop between...



LEO:  I think it sounds exactly the same.  I'll be honest.  Oh, no, we didn't have our music yet.  This was some other music I was using.



STEVE:  Oh, that's very laid back, yeah.



LEO:  "This is Leo Laporte, and I'd like to introduce a brand new podcast to the TWiT lineup..."



STEVE:  Aww.



LEO:  "...Security Now! with Steve Gibson.  This is Episode 1 for August 18th, 2005."  I like the music.  "You all know Steve Gibson.  Of course he's on TWiT regularly, This Week in Tech.  We've known him for a long time.  He's been a regular on The Screen Savers and Call for Help.  And, you know, he's well known to computer users everywhere for his products.  He's very well known to consumers for" - I used to give you a better intro - "SpinRite, which was the inspiration for Norton Disk Doctor and still runs rings around it, is the ultimate hard drive diagnostic, recovery, and file-saving tool."  It still is.  I say that still every week, except now I say "mass storage."  "He's also been a very active consumer advocate, working really hard to help folks..."



I think my voice sounds higher, but I don't think it actually is.  I think it's a little bit of a bit slip in this thing.  Let me play a little bit...



STEVE:  It's funny because I do, hearing your voice, I remember that Leo.



LEO:  Yeah, the young Leo.  And you're on Skype, so the quality of your audio is terrible, even on Screen Savers.



[Episode 1 continues playing in the background]



You don't sound different, but it sounds phased a little bit.  We got you a PR 40, more bandwidth, started using Zoom.



STEVE:  Yeah.  And I was on the other side of that T-1 line, so I didn't have...



LEO:  Oh, you had 1.44Mb.



STEVE:  I didn't have today's bandwidth.



LEO:  There you go.  That makes a big difference.  If people want to hear this, it's still on the website, and the shortcut is TWiT.tv/sn1.  Sn1, the first Security Now!.



STEVE:  So that episode was titled "As the Worm Turns."



LEO:  Yeah.



STEVE:  The first Internet worms of 2005.



LEO:  Holy cow.



STEVE:  I know.  And its description made me shake my head because it reads:  "How a never-disclosed Windows vulnerability was quickly reverse engineered from the patches to fix it and turned into more than 12 potent and damaging Internet worms in three days."



LEO:  Wow.



STEVE:  What does this mean for the future of Internet security?  And here we are, having just, you know, celebrating the 18th birthday of this podcast, and so much has changed, and so much has not.  So anyway, thanks to feedback from our amazing listeners, one of the things that's been driven home for me during the past 12 years is how much this podcast means to our listeners and, I suppose, how much it would be missed, at least for a while, if it were to ever end. Now, obviously,  it's going to end sometime.  Unfortunately, Leo...



LEO:  Unfortunately, in 65 episodes.



STEVE:  We're not both going to live forever.  When William Shatner, who is currently 92...



LEO:  Who is going to live forever, I believe, yes.



STEVE:  I think apparently.  But, you know, he's in remarkable physical and mental health, and of course he recently took that quite emotional for him ride into orbit and back.  He was asked about his secret to long life.  And he replied simply, "Don't die!"  Right?  That's it.  Don't die.



LEO:  Yeah.



STEVE:  So I'll confess that while my middle name is not Tiberius, I'm going to do everything to follow the Shatner plan.  And thanks to our listeners, it occurs to me that perhaps this podcast should follow the Shatner plan, too.  As all of our listeners know, I've been talking about ending my involvement with Episode 999.  I'm here after 18 years, Leo, due to our gentleman's agreement to do a podcast together.  And I think that I should remain here as long as that's what everyone wants.



LEO:  What?  Oh, my god.  You just made, not only me and everybody in this building, but about 100,000 listeners extremely happy.  You're saying you're willing to go beyond 999?



STEVE:  So, yes.



LEO:  How are we going to get the four digits?  Can we start over at zero?



STEVE:  I've written some code in my life.  I can figure out how to change three digits into four.



LEO:  Steve.  Be still my heart.  Oh, my gosh.  Thank you, on behalf of everybody listening.



STEVE:  So it seems to be the case that even after 18 years, everybody still wants this.



LEO:  Yes.  Yes.



STEVE:  And, you know, and I feel as though we have a lot of leverage, by which I mean that this podcast appears to matter to a lot of people.



LEO:  Yeah, that's true.



STEVE:  And that's enough for me.



LEO:  Lisa just came running in.  What?  Let me see if I can find a shot with her in - well, you'll have to come around here and do it.  She's very happy.



LISA:  Yay.



LEO:  Yay.



LISA:  And we still need to come down and harass you.  Now I'll stop because everyone will get mad.  But I was like...



LEO:  There's a lot of excitement, and I can hear the cheers in the other room.



LISA:  Oh, everyone was freaking out.



LEO:  Patrick Delahanty's jumping up and down.  Yeah, that's really great.  So by the way,  Patrick just told me, he's updated TWiT's code, in fact he did it a couple years ago, to support more than 999 podcasts.  So Steve, if you're willing to go 999, I'll do it, too.



STEVE:  Well, you know, Leo, these first 935, they just flew by.  You know?  It's like...



LEO:  I was really feeling like you were kind of getting tired of doing it, to be honest with you.  But you're not.



STEVE:  Well, certainly as we head into year 19, I think we've at least established that we're not about to run out of material.



LEO:  Yeah.  By the way, in the description of Security Now! 1, it says "this short podcast."  It literally, I thought at the time that it would be a short podcast.  But bad guys just don't rest.  Hey, thank you, Steve, and congratulations.  That's really, really good news.



STEVE:  Well, it feels - I've known for some time, and I thought, this is the occasion where, on our 18th birthday, that I should just say, you know, why stop doing a good thing?



LEO:  Oh, thank you, Steve.  On behalf of the entire Internet community, thank you.



STEVE:  So a bunch of Closing the Loop feedback from our listeners.  Some guy whose handle is gimix, G-I-M-I-X, and then cubed, Gimix^3, his actual name appears to be Jordi, he said:  "Hey Steve, today" - and this was on Friday - "Chrome greeted me with this dialog."  And I have a picture of it in the show notes.  And it says:  "Turn on an ad privacy feature."  And it goes into some details.



And so he writes:  "Google adding a privacy feature?  And asking me PERMISSION to turn it on?  Aha.  Extremely suspicious.  I guess that's FLoC, which was implemented anyway, even if the community pushed back.  Any advice on whether we should turn it on or not?  Maybe material for the next episode!  Greetings from Barcelona, longtime listener, Jordi."



I received Jordi's note, as I said, on Friday, and I knew what it was.  This was not FLoC, this was the long-promised rollout of Topics, which is Google's replacement for hidden tracking-based profiling in favor of transparent site-based interest profiling, which is performed entirely client-side, in TNO style, and is entirely under the control of each individual Chrome user.  I mean, it's a breakthrough.  But I was greeted by it when I opened Chrome yesterday.  Since I'm no longer a Chrome frequent flier, I hadn't seen the dialog when Jordi had tweeted the picture of it to me.



Since the Chromium browser's Topics API is finally making its appearance, and since there's reason to believe that this will be the system that changes the world, it's time to revisit what it is and how it works, which we're going to do shortly.  So anyway, Jordi, thank you for being the first to point it out.  I got the dialog of my own, and everybody else who's using Chrome will have been receiving this.  So we're going to talk about exactly what this is and how it works.



LEO:  And why you should use Firefox.



STEVE:  And why you should use Firefox.  Well, actually, I'm hoping Firefox will adopt this.  I sincerely am.



LEO:  Oh, okay.  That's a twist.  Okay.



STEVE:  Oh, yeah, yeah.  I think - and you and I are going to have some fun with this, Leo, because there are, you know, multiple sides to this.



Okay, now, someone who chose the unfortunate handle "burnedeye," I don't know why, sent something very interesting.  So burnedeye tweeted:  "Hi, Steve.  You may find this interesting.  I know you're a Firefox user.  So am I.  Recently I have found out about this fantastic extension called 'Firefox Multi-Account Containers.'  It solves an issue of being automatically logged into, for example, all Google services, when you only want one, for example, YouTube.



"You can isolate YouTube.com into its own container, where you can be signed in while all the other Google sites such as Google.com, Gmail, et cetera, can stay in a default container where you're not signed in, and those sites won't be affected by the fact that you're signed into YouTube."  He says:  "It's a browsing tab virtualization, in a way."  He said:  "Love it, just as I love the Security Now! podcast.  Longtime fan, Tim."



Okay.  So I was not aware of this slick browser extension, and it does solve a problem I sometimes have.  Its author explains thus.  He says:  "The Firefox Multi-Account Containers extension lets you carve out a separate box for each of your online lives.  No more opening a different browser just to check your work email.  Under the hood, it separates website storage into tab-specific Containers.  Cookies downloaded by one Container are not available to other Containers.  You can even integrate individual Containers with Mozilla's VPN to protect your browsing end location.



"With the Firefox Multi-Account Containers extension, you can sign into two different accounts on the same site, for example, you could sign into your work email and home email in two different Container tabs, even if they use the same server; keep different kinds of browsing far away from each other, for example, you might use one Container tab for managing your Checking Account and a different Container tab for searching for new songs by your favorite band; avoid leaving social network footprints all over the web," he says, "for example, you could use a Container tab for signing into a social network, and use a different tab for visiting online news sites, keeping your social identity separate from tracking scripts on news sites; and protect your browsing activity in individual Containers using Mozilla VPN, so you can shop while traveling abroad, but check your bank account from a server in your home country."



And finally:  "After installing the Firefox Multi-Account Containers extension, click the Containers icon to edit your Containers.  Change their colors, names, icons.  Long-click the new tab button to open a new Container tab."



Anyway, okay.  Though I haven't tried it, this seems very cool.  The one thing I would caution is that he writes "keeping your social identity separate from tracking scripts on news sites."  But as we know, and as we saw quite vividly last week with the lengths Yahoo!, in that example, goes to in order to track people, containerizing explicit authentication cookies is not the same as fully anonymizing one's appearance on the web.  I'm certain that someone examining the queries being admitted by, or JavaScript running in, adjacent containers would be able to detect that they're running side-by-side in the same browser.  Nothing is simpler than noticing that the queries are all emerging onto the Internet from the same private IP address.



You know, he addresses this simple IP-based tracking with the feature of an automatic tie-in with Mozilla's VPN service.  But this is not to say that the idea of being able to be logged into the same service under multiple identities with a single browser is not extremely useful.  I often, you know, wish I had a different account at the same place.  And I do tend to use a different browser if I need to.  I remember when I was messing around with SQRL a lot, that was something I was having to do.  So anyway, I wanted to share Tim's tweet so that we all know about it.  And those of us who Firefox users are able to take advantage of it.



Matthew Dudek, he said:  "Hi again.  Have a question about Full Disk Encryption on SSDs.  If an SSD is already in use in an unencrypted state, is it then impossible to fully encrypt it with BitLocker or VeraCrypt due to data stored in inaccessible blocks because of over-provisioning and wear-leveling?  How can one encrypt an in-use SSD and guarantee all data is sufficiently scrambled?  Thanks again!"



Okay.  So since I plan to fork the early work on SpinRite 7 into a separate product named Beyond Recall, I will eventually acquire a great deal of firsthand experience solving these problems.  Matthew is correct in assuming that wear leveling, defective region sparing, and over-provisioning inherently take data-containing mass storage regions out of service, rendering them inaccessible through the mass storage device's normal data API.  There are two different means for fully erasing all traces of data, even data that's inaccessible.  The problem is that they erase all traces of data.



What Matthew wants to do is to take an SSD that's already seen some use and add external full disk encryption to that existing device while not leaving the inaccessible regions, which might still contain some previously in-use unencrypted data, unencrypted.  And as far as I know, that's not possible.  He'd like there to be some command to destructively and permanently erase only all of the inaccessible regions.  But I'm not aware that any such command exists.  This means that he would need to copy the drive's contents to another device, then arrange to securely erase the entire drive, which would and does include all user-inaccessible areas, then implement BitLocker or VeraCrypt or whatever encryption on that drive, then restore the drive's original contents.



So unfortunately, no way I know of, of just doing it in place.  That inaccessible previously unencrypted data will stay there until you perform a secure erase on the drive, which is the only thing that will get rid of it because you can't get to it otherwise.  Most of these storage controllers have manufacturer proprietary, undocumented, typically unknown, you know, backdoors that allow you to manipulate stuff.  That's something I may end up getting into, depending upon how things go with Beyond Recall.  But, you know, nothing known publicly.



Trevor Welch said:  "Hi, Steve.  I have a SpinRite question.  I have a mixed array of drives, hard drives, SATA SSDs, M.2 SSDs, and probably even some weird proprietary external hard drives.  I'm finally getting my act together and going to be backing them up to a NAS and backing that up to the cloud.  Is there any advantage to running SpinRite on any of these drives before I do this?  Or do I run the risk of maybe pushing one of the drives into its demise while running SpinRite, and being unable to then copy all the information off?  Some of these disks are very old, 10-15 years maybe, and are in unknown condition as of right now.  So I just want to make sure I give myself the best chance of being able to get as much data as I can off of them."  And he says:  "Love the show and excited for SpinRite 6.1 to come out.  Maybe an announcement on Tuesday?"



Well, okay.  No announcement today.  As always with a project of this size, with as many moving pieces as this has, working to get to the "there's nothing left to be done" state reveals additional things that need to be done.  And I'm reminded of that old thought puzzle which suggests that it's impossible to actually get to your destination because before you can, you first need to get halfway there.  Then halfway there again, and halfway again, and halfway again, and so on indefinitely, thus theoretically unable to ever reach the goal.  But, you know, we are down, the good news is, to very few remaining known things that need addressing.  So yeah, it's looking like one of these weeks very soon I will have an announcement for the listeners of this podcast.



As for Trevor's question, I don't see any reason to run SpinRite on any of those drives ahead of time.  Just try copying all of their data off at the file level, you know, file by file.  Copying programs are notoriously finicky about hitting errors during their work, but there are copying utilities that will retry for a while, then skip over any trouble they encounter.  Even the old XCOPY program from back in the MS-DOS day, which is still present, it has an "ignore errors during copying" option.  And I'm a fan of Robocopy on Windows.  It's got all kinds of extra feature switches that allow this to happen.



The one thing those tools won't and cannot do is deal with problems in the file system's metadata, which SpinRite can do, since they depend upon that metadata to find the file names and the file content locations in order to copy it.  But otherwise, it's usually possible to get most of the data from a drive.  But if any trouble is encountered along the way, I'd say by all means let SpinRite have the drive to see whether it's able to fix those areas that may have been troublesome.



Matt G, tweeting from @mpgagne, he wrote:  "This might be the most annoying password rule list I have ever seen."



LEO:  Oh, this is a game, I bet.  Is this the game?



STEVE:  Well, this is really - no.  This is really interesting because it's got some hidden gotchas.  He said:  "You can't use a password generator because any repeat of a single character makes the password invalid."  He said:  "Thought you would enjoy."



LEO:  Oh, please.  That's terrible.



STEVE:  Okay, now, actually this is - there are so many good things here.  So, okay.  Here's what the rule list is.  And by the way, as we'll see from the bottom rule, this is JPMorganChase's password creation guidance.  So first, must be 8-32 characters long.  Must include at least one uppercase, one lowercase, and one number.  Okay.  Those are kind of standard.  Must not have special characters or punctuation.  Okay.



LEO:  I don't like that.



STEVE:  Get this.  Must be different than your previous 24 passwords.



LEO:  What?  Where did that number come from?  What?



STEVE:  I know.  Oh, it gets better, Leo.  Must not include your email ID partly or fully.



LEO:  Okay.



STEVE:  Must not include your first name or last name.



LEO:  That's fair.  All right.



STEVE:  Okay.  What if your last name or first name has, well, okay.



LEO:  Is monkey?  What are you going to do then?  Huh, huh, huh? 



STEVE:  Here's a problem.  Must not include more than 2 identical characters.



LEO:  That makes no sense.



STEVE:  Okay.  Get this.  Must not include more than 2 consecutive characters.



LEO:  Wait, what?



STEVE:  I know.  I know.  And you can't have more than 2 consecutive characters.  Okay.  Must not use the name of the financial institution - JPM, MORGAN, JPMORGAN, CHASE, JPMORGANCHASE, or JPMC.



LEO:  I wonder how much those last couple, not the last last one, but the consecutive characters and identical characters, reduces the password space?  That must cut it way down.



STEVE:  Oh, Leo, you have been paying attention to this podcast.  I credit you that, definitely.  Matt's tweet included a screenshot of those requirements, which we just shared.



Now, there are three problems that come to mind.  First, one of the rules reads "Must not include more than 2 consecutive characters."  Yet we know from the first rule that the minimum password length is 8 characters.  So it's unclear how you create any password longer than two characters if you must not include more than two consecutive characters.



LEO:  Well, you have to put a number in between them; right?  They mean alphabetic characters?



STEVE:  It's not clear.  They just said "characters."



LEO:  Characters.



STEVE:  Yeah.  This guy was a character, whoever wrote this.



LEO:  Yeah.



STEVE:  Now, so it must be that the author of this rule meant to say "2 consecutive identical characters."



LEO:  Oh.



STEVE:  Right?



LEO:  Maybe.



STEVE:  Except that the preceding rule is...



LEO:  You can't have any identical characters at all.



STEVE:  But no, it says:  "Must not include more than 2 identical characters."  Now, again, here the digit "2" is not necessary because you could just say "identical characters."  Right?  Because it's redundant.  But okay, fine.  So that means that the misworded following rule, the "must not include more than 2 identical characters" is redundant because it's fully covered by the one that precedes it.



Okay.  But aside from that grammatical nitpicking, there are two bigger problems.  The very first rule states that the password must be 8-32 characters long.  Okay.  So a minimum of eight characters?  Really?  An eight-character password is sufficient, given all of the rest of the rigmarole that JPMorgan Chase customers are being put through?



LEO:  Nonsense.  Just nonsense.



STEVE:  And when you think about it, Leo, given how difficult they've made it to create any password...



LEO:  Yeah, because your password manager doesn't know these rules.



STEVE:  No, no.  So, no, it's got to be done by hand.  But given how difficult they've made it to create any password that somehow manages to get through the gauntlet of those rules, users would be hugely incentivized to quit after somehow working out an eight-character string that qualifies.



LEO:  Yeah, you're right.  You're guaranteeing an eight-character password.



STEVE:  That is exactly right.  And that brings us to the third and worst problem of all.  These ridiculously - exactly as you immediately saw - these ridiculously onerous rules are going to drive users to create the shortest possible passwords while at the same time making brute force guessing vastly easier and more practical.  Any intelligent brute forcer will be informed by the same limiting rules as the password creators.  So this dramatically and incredibly reduces the possible brute force search space.  You know, "No special characters or punctuation?"  Wow.



LEO:  I always hate it when I see that one.  I hate that.



STEVE:  Yes, that's so stupid.  Talk about dramatically reducing the alphabet size and thus the search space.  Those same rules which make it difficult for a customer to create a qualifying password, automatically discards a vast universe of passwords that would have been possible, and that an attacker would have needed to try.  But now the attacker already knows that those would not have qualified.



LEO:  Oh, don't worry about those.  Yeah, that's great.



STEVE:  Imagine all of the guesses where more than two characters are the same.  None of those ever need to be tried.



LEO:  Unbelievable.



STEVE:  What a perfect real-world example of someone thinking that they're being quite clever by forcing their customers into compliance, when they're inconveniencing those customers, while at the same time making things far easier for the attackers.  Wow.



LEO:  Unbelievable.  And we see this all the time.



STEVE:  At this point, yeah, it's just, you know, trying to make it better and making it worse.



LEO:  Go ahead.  You want to take a break?



STEVE:  I do, yeah.



LEO:  I just want to know if you've seen the Password Game. Because this is so fun, from Neal.fun.  Have you ever heard of this?  Okay.



STEVE:  I haven't, no.



LEO:  Okay.  I'm not going to show you this if you have anything to do tonight.



STEVE:  Uh-huh.  Whoa.



LEO:  Okay.  So it's at Neal.fun.



STEVE:  Maybe don't show it to me until after SpinRite 6.1 ships.



LEO:  It's to make fun of all of these.  Okay.  So I'm going to do monkey.  It has to include a number, 123.  Okay.  Uppercase letter, okay, I'll make the "m" an uppercase.



STEVE:  Oh, yeah.  That's good.



LEO:  The digits in your passwords must add up to 25.  So 3 plus 8 is 11, plus 9 is 20, and then a 5.  Okay.  Password must include a month of the year.  June.  Okay.  One of our sponsors, Shell.  Roman numerals should multiple to 35.  Uh-oh.  So this has to be maybe - so that's seven times five.  So maybe this will be VII, and then this will be V.  Okay.  Oh.  CAPTCHA.  D22BDD22EZ.



STEVE:  Now, does it always give you the same requirements?  Or do those...



LEO:  Uh-oh.  Yes.  So I just ruined it by doing the CAPTCHA because there was a 22 in there.  So now I can eliminate everything but a 3.  Okay.  It's 22.  So it's 2 plus 2 is 4.  Okay.  I get it.  13, 20, 5.  Okay.  Here's where I stopped.  Your password must include today's Wordle answer.  By the way, it is today's Wordle answer.  I've done this before.  John and I have both done this.  So you now go off to solve Wordle.



STEVE:  It checks?



LEO:  It checks.  So you now go off to solve Wordle.  It will give you a chess problem in just a bit.  At one point your password starts to catch on fire, and you have to [indiscernible].



STEVE:  Anyway, for our listeners, it's Neal, N-E-A-L, dot F-U-N.



LEO:  Yes, Password Game.  He does a lot of fun games.  I guarantee you, because you're smart, you will dig this.  I have never gotten past the fire.  John says he's gotten past the fire.  What's the last step you got to, John?  How far did you get?  I think he said he got 25 rules, something like that.  It goes on.  The guy who created it, Neal, said he's never finished it.  You will enjoy it, Steve.



STEVE:  Wow.



LEO:  All right.  Now we're going to take a break.  Please, folks, do not go off and do that.  Stay tuned.  You can do it tonight.  Enjoy the rest of Security Now! before you engage in this.  It's like the paper clip game.  Once you start, you know, you kiss the rest of the day goodbye.



STEVE:  Anybody who's ever looked at underpasses being built in California has noticed that they are no longer brittle.  They are resilient.



LEO:  That's right.



STEVE:  They are something sitting in a pocket so that if an earthquake shakes it, it doesn't crumble, it just rocks around a little bit and hopefully returns close to where it started.



LEO:  What's the worst kind of building to be in in an earthquake?  It's not a wood frame building.  It's a brick building.  Those are deadly, yeah.



STEVE:  No give.



LEO:  No give.



STEVE:  So Josh Randall, he said:  "In SN-934, you mention that many sites now require you to create an account with an email so that they can spam you later.  That's precisely why I use DuckDuckGo's email alias - that's duckduckgo.com/email - which I recall you were rather negative about a few episodes back.  When I create an account with any new service or site now, I let DDG create a new, random email address for me that is an alias to my main DDG account, which is in turn an alias to my actual email.  I can receive messages at my actual email through any of these aliased DDG emails, and reply to those messages without ever revealing my actual email.  And if any site or service ends up spamming me through one of my random aliases, I can simply deactivate it and, poof, no more spam."



Okay.  So let me just be clear about my negativity.  I wasn't aimed at DuckDuckGo specifically.  I'm 100% behind the use of email aliases.  I think, as the kids would say, Leo, they rock, and I use them myself all the time.  I have hundreds of email aliases.  But the thing that's different is that the aliases I use are created by my own email server at GRC.com, which I'm not going to decide to suddenly terminate.  Or if I were to, it would be my decision, under my control.  Given the importance of email as our account recovery and proof of identity, I would be nervous to be asking any third-party provider for such a service if I used it in such a widespread fashion.



Unlike a use-it-once credit card number, an email address needs to be inherently static and persistent.  If DuckDuckGo were ever to decide to terminate that service, it would create a significant inconvenience for its users who are using it in this fashion, who would need to manually change every one of their registered email addresses everywhere they had ever used them.  Again, I love the idea, and I get it that not everyone is able to run their own email server, especially since consumer ISPs actively block their customers from running local email transports.  But anyway, so love the idea.  And I guess I wish there were a way to do it that seemed really safe.



LEO:  Yeah.  I mean, I never even thought about that.  I have my own, you know, at Fastmail I have my own domains, so I can have an infinite number of @domains.  So I just do that.  And those are all unique.  But maybe a little bit more guessable than, you know, xyz@zmzdd.



STEVE:  Well, and as you know, because you've been following me for a while, my email address changes annually.



LEO:  Yeah, you're smart.  That's such a good idea, yeah.  Because you don't want anybody to reach you.



STEVE:  Right.  Or, you know, they can on Twitter or by talking to Sue or Greg.



LEO:  If you know the algorithm, if you are a friend of Steve's you will know the algorithm, and then you can reach him.



STEVE:  David Halliday, he said:  "In SN-933 you highlight that Russians are now prohibited from contributing to open source.  However, any GPL product makes it very clear that the user MUST contribute to the project for the license to be valid.  Thus the new Russian Astra Linux-based OS cannot be legally possible."



LEO:  Oh.



STEVE:  Okay, now, as I understand it, the GPL requires that any improvements which are made, thanks to having had access to the source code, must be returned to the project.  And so, yes, I think that David's point is correct, not that Russia will be particularly concerned about violating the licenses of the West.  Russia has essentially stolen Linux from the Linux project; but with the rising political tensions, who's surprised by that?



Ooh, some math.  Thomas Apalenek, hope I didn't mangle your last name too bad, A-P-A-L-E-N-E-K.  He said:  "Re:  Satellite Crowding.  Steve.  I really enjoyed the episodes and satellite hacking and related satellite info.  However, the follow-up discussions on swarms of satellites needs to be put in perspective."  For which we thank Thomas.  "A 3D graphic of white dots showing all the satellites and debris orbiting the earth does look a little bit frightening.  However, if the satellite size were displayed at the correct scale relative to the earth on the graph, you wouldn't actually see any satellites at all, except for possibly the International Space Station.  Starlink's 40,000-plus satellites, in particular, sounds like a lot.  But if you imagine 40,000 cars equally spaced over the entire surface of the Earth, it doesn't seem nearly so bad.



"The effective surface area of a 300km orbital shell is about 560 million square kilometers.  An 1,100km orbital shell is about 700 million square kilometers.  The Starlink orbital plan includes three shells ranging from 340km to about 1,100km.  The debris issue is a concern, and launch agencies do need to make sure it doesn't get out of control.  For now, all of the items in orbit are, on average, thousands of miles away from each other.  The skies won't be darkening anytime soon."  And again, Thomas, thank you for that very valuable perspective.  That's very useful.



LEO:  That's a good, excellent point.  Space is big.



STEVE:  Space is big.



LEO:  Really big.



STEVE:  Yes.  Yes.  Which is why, you know, things are zooming around and mostly not hitting each other.



LEO:  Right.



STEVE:  When they do of course it's pretty spectacular.  But anyway.  Brian Norwood said:  "Catching up on this week's episode, and this is what came to mind on the Voyager 2 segment with the shout."  He said:  "Next year we'll find out we let a Borg equivalent know exactly where we are."  Remember that NASA was able to regain control.



LEO:  [Crosstalk].



STEVE:  Well, he said, remember, NASA put out a high-power pulse in order to get Voyager 2 to reorient itself.  Now, I think I've noted before as a sci-fi enthusiast looking at all the trouble we have right here on Mother Earth, where we're all basically variations on humanity, that as I grow older, and hopefully somewhat wiser, I am coming to appreciate where I once used to bemoan how difficult it appears to be to travel between the stars, there's that famous and pithy observation that "Good fences make good neighbors."  So these days I'm much more satisfied with reading fantastical stories and books than actually having any first contact.  You know, let's hope they're not coming here to take our water because that would not be good.



Brian Gluck said:  "Can you please share the name of the session manager that you use in Firefox," which I mentioned last week.  He said:  "I had one that I loved that saved all of the open windows and tabs, but it stopped working a number of years ago.  I think it was called Session Restore, but I honestly don't recall.  I would love to know which one you're using.  I miss this functionality and don't like to download an add-on into Firefox without a recommendation from someone I trust."



Okay.  It's called "Tab Session Manager" as three words, and I just noticed that it's also available for Chrome and Edge.  And it really is a nice piece of work.  I've been using it for many years, so I'm happy to vouch for its value and stability.  It's free and user supported.  And having just written all that, I just sent its author $25 dollars through PayPal since I would like to keep it around.  It is, you know, it's open source, it's on GitHub, the guy's maintaining it, and Firefox does sometimes make a radical change that requires rewrites of their add-ons, which is exactly how Brian's earlier thing, that Session Restore, you know, some guy created it and wandered away.  Then Firefox changed and broke the extension.  So the fact that this thing is being maintained is a good thing.



And finally, Rusty tweeted:  "@SGgrc How many people have pointed out that the 2 degrees off that Voyager was pointing at 12.3 billion miles was a bit under 430 million miles, or about five times the average distance between the Earth and the Sun."  He says:  "It's fantastic that they were both able to hear and send."



And so, Rusty, you take the prize as the only listener whose tweet I've seen took the time to do the math and give us a calculated answer.  My own intuition suggested that this had to be the case, but it's nice to have some numbers to go along with it.  So about five times the distance between the Earth and Sun is how far off angle, away from us, that beam was passing when Voyager was trying to talk to us.  We weren't hearing anything that it was sending any longer.



Okay.  And Leo, let's do our last break, just so that we can do Topics in one whole piece.



LEO:  I like it.  By the way, I'm making some good progress on our password.



STEVE:  Oh, my goodness.



LEO:  I did have to identify this country.  And it took me a little while, you know.  But the name of the hotel was the giveaway.  I looked that up on the Internet.  So now I just have to get a leap year in here without changing the addition.  Maybe I'd better do the ad instead.



STEVE:  What is that black blob?



LEO:  Oh, that's the current phase of the Moon as an emoji.  Yeah, it's a waning crescent right now.



STEVE:  Oh.



LEO:  So it says insert the...



STEVE:  I did get some crap from kids in school over the "waning Gibbon."



LEO:  The waning Gibbon moon, yes.



STEVE:  That's right.



LEO:  Yes, this is the waning Gibbon.  Anyway, I'm only, you know, a fraction of the way along.  But I've got a great password.  I must say.



STEVE:  Okay, now...



LEO:  Does Chase let you use emojis in your password?



STEVE:  I want to say, though, you missed our first episode where we talked about Topics.  And I need to have your attention for this one because it's important for you to get this.  I'm just saying.



LEO:  Okay.  I'm not moving.  I had to get up and run up and down the hall and celebrate, and I did miss that first image.  But I won't miss this next one.  I'm paying attention now.  All right, Steve.  I am paying attention.  I am not leaving this seat.  I am listening to every word.  Go right ahead.



STEVE:  I think you're going to find this useful, as our listeners will.  So, okay.  The right answer doesn't always present itself the first time.  When we encountered "FLoC,"  which should stand for "Failed Learning of Cohorts" instead of "Federated Learning of Cohorts," you explained, Leo, that in referring to it as FLoC, Google was clearly struggling to keep with a bird theme.  It turns out they were struggling too hard and the FLoC flew the coop.



LEO:  Yes.



STEVE:  But as I said, the right answer doesn't always present itself the first time.  Sometimes it's necessary to experiment and iterate.  The reason SpinRite took three years is that there were a lot of avenues I went down that I ended up backing out of.  It's like, okay, well, that approach didn't work to be a universal solution, so let's try this.  So the design of Topics, which is Google's final solution for the replacement of profiling by tracking, shows that Google has learned a great deal from their previous attempts.  And we not only have a system that's ready for prime time, it's beginning to roll out now.  It's in Chrome today.



Now, we initially covered the Topics API about a year and a half ago, immediately following Google's first announcement of this proposed new system.  This occurred, Leo, during one of your rare absences from the podcast.  Security Now!'s February 1st, 2022 podcast was titled "The Topics API."



LEO:  Oh, I know why.  I was in Portugal at the time and having a grand time.  So there.



STEVE:  Okay.  Well, we're glad for you.



LEO:  Yes.



STEVE:  Jason held down the fort.  And I have always regretted that you missed that discussion because having you understand this is critical because, you know, you appear, not surprisingly, on many podcasts here.



LEO:  Yes, I do.  I'm listening.



STEVE:  So fortunately, we have another shot at this.  And this time it matters even more because, unlike FLoC, Topics is probably going to fundamentally change the way the Internet works.



LEO:  Hmm.



STEVE:  Now, as we noted last week, DNT stood for Do Not Track, whereas GPC (Global Privacy Control) is an explicit request for privacy enforcement.  In other words, they're not the same thing.  GPC is not about tracking, even though much of the tech press refers to it as an anti-tracking measure.  Similarly, Google's Topics solution is not some new means for tracking users on the Internet, even though virtually all of the tech press is calling it that.  I think the problem is that since tracking is all we've ever known, and change is difficult, everything is assumed to be some form of tracking.



But as everyone is going to understand by the time we're finished here today, Google's Topics system is explicitly and almost painfully a non-tracking solution.  If I may be permitted to use the term, it is truly a privacy-forward system for allowing websites to learn a little something about the topics which may interest their visitors.  Period.  Full stop.  Again, "Topics is a privacy-forward system for allowing websites to learn a little something about the topics which may interest their visitors."



It is a means for allowing Google, you know, the Internet's massive advertising behemoth, to continue to deliver user-relevant advertising without tracking.  Topics does not utilize any sort of tracking.  None.  It's basically Google seeing the writing on the wall that tracking may not be permitted indefinitely, but they'd like to continue to exist indefinitely.  They know that it might eventually be outlawed, at least in some jurisdictions.  So they want to have some sort of user-profiling replacement ready if that happens.  And in fact they've already announced that Chrome will begin deprecating its support for tracking via cookies starting next year, in 2024.



Now, I used the term "user-profiling" just now, deliberately, because while that's what Topics is, it is an entirely different and vastly weaker form of profiling from what we're accustomed to.  With profiling via tracking, many hidden entities on the Internet know everywhere we go.  A visit to a site for erectile dysfunction gets logged into many hidden databases over which we have no control, and that information is then made available for sale.



LEO:  I didn't do it.  Someone else was using my computer.



STEVE:  That's right.  Somebody borrowed your computer.  Yeah.  You left it unattended at Starbucks.



LEO:  Yeah, that's it.  It was Lisa.  Yeah, that's it, yeah.



STEVE:  So it's hardly surprising that's not anything that anyone wants to have everywhere you go, everything you do, being secretly logged into hidden databases over which we have no control.  Under the Topics system, and assuming an absence of tracking which is a separate issue we'll get to in a minute, only your local browser sees that visit.  And that visit is not recorded.  It only knows that this is a site having the topic of "Health and Wellness," so that "Health and Wellness" topic gets added to the topics you have shown an interest in for the week.  That's it.



Thanks to the creation of this new technology that we're going to go into detail, Topics provides a means by which a user's web browser may learn by inference about its users' current interests, by virtue of where they take their browser on the Internet.  And the browser is then able to judiciously make a few of those interests known to websites and advertisers who ask.



Okay, now, before we get into the technology, I want to make it clear that, as usual, we're here to talk about technology, and that I'm deliberately agnostic on matters relating to the non-technical questions of whether or not any profiling is a good thing, whether or not it is driven by tracking.  Is any form of profiling okay?  I get it.  That question is controversial.  There are those who feel very strongly that all use of the Internet should be anonymized to every degree possible.  Like anybody at the EFF, apparently.  They feel that it is their right to minimize the value they present to a website - and its supporting advertisers - by remaining as anonymous and as unknown as possible.  They feel that every visit to any website should be siloed, with no other site or third-party content provider having any information about them that they don't supply to the site being visited.



I get that.  And, you know, Leo, you and I were just discussing here last week whether the value obtained by user profiling is really worth what advertisers believe it's worth.  Now, that's not something I can speak to.  Google and advertisers and sites like Yahoo! clearly believe it is.  Or perhaps they just want it because that information is available, and they'd like to know who they're spending their advertising money on.  Perhaps that feedback allows them to better tune the content that their sites offer.  You know, okay, whatever.



What I do know, though, is that user profiling via tracking represents the height of privacy intrusion.  As far as I know, an immutable record of every website I have ever visited is squirreled away in multiple massive hidden and inaccessible-to-me profiling databases.  And I have zero control over that.  That's the world we're in today.  But if Topics succeeds, and Google would appear to be in the position to singlehandedly deliver its success, it is a far less intrusive profiling technology.  And in addition to being a much weaker information gatherer, Google has chosen to provide its users complete control over the Topics their browser presents to the world, including turning it off altogether for full anonymity.  I'll explain that further in a minute.



So if only on that basis, Topics at least represents a huge step in the right direction.  Yes, by default some interest profiling remains.  But the means of obtaining those significantly weakened profiles is no longer tracking.  And users have complete visibility into their online profile and are able to curate, edit, and even delete any of it or all of it as they choose.  So it's a compromise.  But there are many websites begging for our support.  My feeling is, if voluntarily letting them know something about who we are allows them to generate, as they claim, significantly more revenue from our visit, is that too high a price to pay?  Again, it's an individual decision.  But now, in a world with Topics, at least, it's one we're able to make.



Another of the arguments presented by the naysayers is that if Topics is embraced we have no guarantee that tracking will end, and that we won't merely be adding another powerful profiling technology on top of the existing mix.  And those people are right. But as I noted above, Google is planning to begin the deprecation of explicit tracking support via cookies once Topics has come online.  And if Google truly wants to prevent tracking once they no longer need it, neither it nor its bad press, they're in the perfect position to do so.  Ultimately, though, as with GPC, it may come down to legislation.  And that's fine, if legislation is what's required.



Do Not Track might yet be reborn, this time enforced by local or national legislation making it illegal to track someone who has indicated that they don't wish to be tracked.  And the European Union is likely to outlaw it altogether.  If a viable alternative to profiling via tracking is present, that is, an alternative to profiling via tracking is present, right, as it will then would be thanks to Topics, it will be difficult for the trackers to make the case for why users need to be tracked to support the Internet's advertisers and content creators.



Okay.  So three and a half weeks ago, on July 20th, TechCrunch noted that Topics was finally arriving.  They wrote:  "Google continues the rollout of its Privacy Sandbox APIs, its replacement for tracking cookies for the online advertising industry.  Today, right on schedule and in time for the launch of Chrome 115 into the stable release channel, Google announced that it will now start enabling the relevance and measurement APIs in its browser.  This will be a gradual rollout, with Google aiming for a 99% availability by mid-August."  And here we are on the 15th, and my Chrome got it.



"At this point, Google doesn't expect," they said, "to make any major changes to the APIs.  This includes virtually all of the core Privacy Sandbox features, including Topics, Protected Audience, Attribution Reporting, Private Aggregation, Shared Storage, and Fenced Frames.  It's worth noting that, for the time being, Privacy Sandbox will run in parallel with third-party cookies in the browser.  It won't be until early 2024 that Google will deprecate third-party cookies for 1% of Chrome's users."  That's, you know, to make sure nothing unforeseen and horrible breaks.  "After that," they wrote, "the process will speed up, and Google will deprecate these cookies for all users by the second half of 2024.



"The AdTech industry," they said, "has been able to test its readiness for the eventual third-party cookie deprecation, in part through the Relevance and Measurement origin trial.  With these features moving into general availability, Google will end this trial, and revoke the tokens to run experiments on September 20th, 2023.  For Chrome users, Google will now start rolling out its user interface" - and that's what has happened - "to allow them to manage Privacy Sandbox data in the browser, including ad topics, site-suggested ads, and ad measurement data.  This rollout will run in parallel with the API releases.  Google will soon make enrollment and attestation a mandatory process for AdTech companies that want to access these APIs on Chrome and Android, though they will be able to continue to do some local testing, as well."



And finally they said:  "Google notes in their recent announcement:  'Shipping these APIs is another key milestone in the ongoing Privacy Sandbox timeline.  This marks the beginning of the transition from sites testing in the origin trial to integrating these APIs in production.  We'll be keeping you updated as we progress through enabling the APIs, to the opt-in testing with labels in fourth quarter 2023, the 1% third-party cookie deprecation in first quarter 2024, heading towards the full third-party cookie phaseout in the third quarter of 2024.'"



Okay.  So here's how Topics works.  The essence of Topics are individual topic tokens - zero, one, or many - which are assigned to individual websites.  For example, my GRC.com site might be associated with Computers and Electronics/Network Security, and Computers and Electronics/Programming, and Networking/Internet Security.  So when someone visited GRC.com, their own web browser would record their interest in the topics associated with GRC.com, those topics, those three.  But their visit to GRC.com itself would never be recorded other than in their regular local browser history as is always done.  The only thing retained by the browser to indicate their interest in those topics would be those three numbered parameters.



For example, in Google's current 349-topic list, which they refer to as a "taxonomy," there's "Arts and Entertainment" as a general topic if nothing more specific is available.  But then there's "Arts and Entertainment," and then under that "Acting and Theater," and "Comics," "Concerts and Music Festivals," "Dance," "Entertainment Industry," "Humor."  And under "Humor" is the subtopic "Live Comedy."  And it goes on like that with "Arts and Entertainment" having a total of 56 token entries before we switch to "Autos and Vehicles," which has 29 subcategories, which brings us to "Beauty and Fitness" and so on.  You get the idea.



So here's how Google's specification explains this.  They said:  "The topics are selected from an advertising taxonomy.  The initial taxonomy proposed for experimentation will include somewhere between a few hundred and a few thousand topics."  They said:  "Our initial design includes around 350."  And I counted them, it's 349.  "As a point of reference, the IAB Audience Taxonomy contains around 1,500 individual topics and will attempt to exclude sensitive topics."  And they said:  "We're planning to engage with external partners to help define this.  The eventual goal is for the taxonomy to be sourced from an external party that incorporates feedback and ideas from across the industry."



Okay.  Under today's tracking technology, if someone were to visit a website, for example, concerning the termination of pregnancy, that visit would be tracked and recorded by unknown and unknowable third parties.  But under the forthcoming Topics system, the topic associated with that site might be "Health and Wellness" and nothing more specific.  So this really is privacy centric.  And as we'll see, Google has bent over backwards to ensure that the topics a user's browser volunteers cannot themselves be used as a tracking beacon.



Google explains:  "The topics will be inferred by the browser. The browser will leverage a classifier model to map site hostnames to topics.  The classifier weights will be public, perhaps built by an external partner, and will improve over time.  It may make sense for sites to provide their own topics via meta tags, headers, or JavaScript, but that remains an open discussion for later."  I have a link in the show notes to the official IAB Taxonomy in the form of an Excel spreadsheet, for anyone who's interested, so you can just sort of see what an existing advertising taxonomy looks like.



Okay.  So as a user roams around the web, their browser infers the topics of the various sites they visit using this classifier which is built into the browser, which maps the site's domain name to the topics that are relevant for it.  For any given domain, the classifier may return nothing, no topics, one topic, or more.  There's no set limit, though between one and three is what's expected.  So it is possible that a site might map to no topics and so doesn't add to the user's accumulating topic history.  Or it's possible that a site adds several topics or increases the "popularity weighting" of the user's existing topics by being another instance of their interest in health and wellness, for example.



Web browsers which support the Topics API divide the flow of days into discrete epochs which are each one week long.  But it would be more precise to say "one week worth of seconds long" since there is no alignment to any calendar.  Each browser instance chooses for itself randomly when each week-long epoch begins and ends.  All of a user's browsing activity is grouped into these week-long epochs, and only the most recently finished previous three epochs are retained.



In other words, as soon as the currently open epoch closes at the end of its weekly cycle, or its week-long cycle, the Topics it contains becomes the most recent and is then available along with the two next most recent epochs, and the one that had been the oldest of the previous three is completely discarded.  This has the effect of causing every user's browser to completely forget everything it had acquired and knew about its user on a rolling basis every four weeks.  This is neat, since when a user's interests change, the Topics which reflect their new interests will realign with them automatically.



In the past we've talked about the DOM, that standardized Document Object Model, which is what our browsers, the way our browsers represent pages that we visit.  One of the objects that's always present in this model, for example, is the "document" object.  And it has properties like the "body," which is the document's body text; "images," which is a collection of all the images present in the document, and "links" which is a collection of all of the links present in the document.  The Topics API adds the "browsingTopics" property to every page's document object.



When a website or one of its advertisers queries the document's "browsingTopics," they will receive up to three topics from the topic taxonomy, one from each - now, here, one from each of the preceding three weeks - and those three topics will be returned in random order.  When a visitor visits a site, the site will be able to obtain up to three topics which might help it or its advertisers to choose a more relevant ad.  It was decided to provide three topics, that is, you know, three, so that a site that doesn't see visitors often - that is, a specific visitor - will still obtain sufficient information about the visitor it doesn't see often to choose something useful.  And a granularity of one week was used between a user's topic updates so that sites and advertisers which are seen much more often by a user's browser will learn at most one new topic per week.



And not all websites receive the same weekly topic from any given user.  Here's how that works.  For each week, once that epoch closes and the topics have been selected that were the most popular for the preceding week, for each week the user's top five, which were obtained from the web domains they visited during the preceding week, are determined using that topics classifier.  And again, at no time does the user's browser contact any external servers for help with choosing.  It is all done locally.  It's all client-side.  So five master topics are chosen from an examination of the preceding week's browsing history.  And one additional topic is chosen completely at random from the entire taxonomy.  So it will have nothing to do with the user's usage history at any time.  It's just it's completely random.



Okay.  Then, when the "document.browsingTopics()" API is called by a site, or typically an advertiser on a site that the user is visiting, or JavaScript running on its ads, the topic returned for each of the three weeks, the one topic returned for each one of the three weeks, which remember are also returned in random order, so that means nothing, is chosen from one of those top five plus that one random topic as follows, with a 5% chance, so one in 20, the randomly chosen topic will be returned.  And that's put in there as a deliberate wildcard.



Otherwise, the other 19 out of 20 times, the value of an HMAC hash is computed from a static per_user_private_key, so every single browser instance is unique; the week number, that is, as the browser is creating and closing these epochs, it will be incrementing the week number.  So a static user private key, the week number, and the document page's website domain name.  They're all mashed and hashed together, and the result is then taken modulo 5 to produce a uniformly distributed value from 0 to 4.  That is used to choose one of the user's top five topics for that week.  The same thing is done for the top five topics during each of the earlier two weeks.  And it's those three topics which are returned in random order.



Now, this may seem overly complicated, but it's quite clever and privacy enforcing.  The use of an HMAC keyed by a per-user secret, the week number, and the domain of the webpage means that, for that week, the user's browser will always present the same one-of-five topics, except for that 5% chance of the wildcard, to anyone querying from the same site; but that every different site will also see an unpredictable but constant for them one-of-five topics for that user for that week.  This is done to minimize the amount of information being disclosed since this guarantees that no site will receive more than one fixed topic per user per week.  And each site only ever receives one of the five real topics, which makes it impractical to cross-correlate the same user over time.



This 5% noise is introduced to ensure that each topic has a minimum number of members, as well as to provide some amount of plausible deniability, meaning that no topic can be regarded as absolute.  It may have been deliberately chosen at random.  And remember that the exact point in time where a user's week ends and the next one begins, that's fixed also, but chosen at random by their browser.  So this introduces some additional uncertainty and noise, since not everyone's web browser will calculate new topics at the same time on the same day, nor will it be changing them.



Now, there's one last extremely tricky bit that's a bit of a mind-bender.  But it's an important privacy safeguard for the entire system.  It addresses the problem that FLoC had and which you, Leo, mentioned several times on other TWiT podcasts, and probably this one.  The problem was that FLoC was broadcasting a token which was a condensation of the person's web browsing history, and thus by extension a condensation of them.  And those who knew how to interpret the token would know what it meant.  And this token was being presented to any website they visited without prompting.  So you quite correctly identified this as introducing a significant reduction in user privacy.  You'd go to a site you'd never visited before, and immediately your browser would be telling them a lot about you, even though you'd never been there before.



So this problem did not fall upon deaf ears, and the Topics API incorporates a mitigation for this.  It's worth reminding everyone that there's nothing whatsoever salacious or even really very interesting about the list of topics.  They are really quite bland and dry.  But they make sense from an advertiser's standpoint.  So the first point is there's just no way for anything very personal to be revealed or represented by these topics.  But even given that, the Topics API incorporates a very strong filter.  And here's the mind-bending part.  I'll explain it, then I'll provide an example because the explanation won't do it.



Not every website that calls this API will receive all of a user's three chosen browser topics.  Only API callers that observed the user's browser visiting some site which mapped to the topic in question that would be returned within the prior three weeks qualifies to receive the topic.  I know, it's hard.  In other words, if an advertiser on a website did not call the API sometime in the past three weeks for that user's browser, when they were visiting some site which mapped to a topic that would be returned now, then the topic will be filtered out and will not be included in the three-topic the array returned by the API.  Fewer topics will be returned.  Since, as I said, this is somewhat mind-bending, here's an example.



During the previous couple of weeks a user has been browsing a lot about travel.  So for the time being the browser has learned about them and chosen to represent their travel-related interest to the world as they visit other sites.  So now suppose that they're at a site about gaming, and an advertiser on that site runs JavaScript in an ad insertion frame which queries the document.browsingTopics() API to receive the three chosen topics of interest to the user, and they of course come in no particular order, from among the topics that the user's browser has chosen to offer anyone querying about its user while they're on this gaming site.  Remember that each week the top five topics of interest are chosen, and one of those five is selected from each of the previous three weeks based upon a hash of the domain name being visited.



Okay.  So only if that advertiser, advertising on the gaming site, had queried that user's browsingTopics API sometime during the previous three weeks while that browser was at some other site whose topics matched the topic the user's browser had chosen to offer at this site now, would that topic be allowed to be returned and thus be presented to that advertiser.



In other words, in order to obtain an interest topic from a user when they are wandering around anywhere on the Internet, an advertiser must have previously asked their browser about them during the past three weeks while they were on a site whose topics may have contributed to the topic they are offering this week.  This prevents the problem that FLoC had of recklessly blabbing about a user's interests.



Here's another way of thinking about it.  In a world with Topics and nothing else, assuming that we've blocked third-party cookies, fingerprinting, and all other tracking mechanisms, or maybe outlawed it, the advertiser doesn't know who the user is.  But they will have recently queried the user's browser while the user was at a website whose topics matched the topic the user is now offering.  Google explains that this extra topic information filtering is intended to "prevent the direct dissemination of user information to more parties than the technology the API is replacing," in other words, third-party cookies and other tracking mechanisms.  Another way of phrasing it is that this ensures that third parties don't learn more about a user's past than they could have with cookies.



Okay.  As I've mentioned, the history window which limits the inclusion of topics available to each caller is three weeks.  Only the topics of sites that use the API, or host ads that query the API, will contribute to the weekly calculation of topics.  Moreover, only sites that were navigated to via a deliberate user gesture are included, as opposed to a redirect, for example.  So it's not possible to bounce users around to load up their browser with topics.  That won't work.  If the API cannot be used for any reason, if it's disabled by the user or by a response header, then the page visit will not contribute to the weekly calculation.  So it's easy for sites to opt out if they choose.



If the user opts out of the Topics API themselves by disabling it, which is trivial, it's a switch you can throw in Chrome.  Or if they're in incognito mode, which automatically completely disables the Topics system, or the user has cleared their browser history, no topics will be returned.



So the goal of the Topics APIs is to take a step toward increasing user privacy, compared to our current state, which would not be difficult, while still providing enough relevant information to advertisers that websites can continue to thrive, but without the need for invasive tracking enabled via existing tracking methods.



One other crucial aspect of the whole Topics solution is the user's understanding and sense of control.  Very, very few users are ever going to understand what we've just described.  They don't really need to.  But what they will see is the list of the topics that their own browser has accumulated over the past three to four weeks.  It's displayed right there in their browser, on that page.  And they are completely free to either delete any topic they choose, or even permanently disable it from ever coming back.



Google wants this to succeed, so they want to give users of this system all of the control that they can imagine.  The API's human-readable taxonomy, which is what is displayed on the page where you can just see the topics for yourself, enables people to learn about and control the topics that may be suggested about them by their browser.  And as I said, they're free to delete or disable any of them.  Clearly, one of the things that we all complained about with FLoC was that it was this bizarre, you know, completely opaque token blob that meant nothing to anyone unless you had the magic formula for what all the bits meant.  And Google completely fixed that.



And also unlike FLoC, which built its hash from every site visited, only sites that include code which calls the Topics API are included in the browsing history eligible for topic frequency calculations.  In other words, sites are not eligible for contributing to topic frequency calculations unless the site or an embedded service, you know, an ad or whatever, has taken action of calling the API.



So that's the operation of Google's Topics API, which is now in place in Chrome today, and presumably in other Chromium browsers which choose to implement it.  It'll be there.  They'd have to turn it off and remove it if they didn't want it.  We know what the EFF would say.  They want nothing less, they're satisfied with nothing less than pure and absolute anonymity.  But website operators say that would cost them dearly.  I have no way to judge how much revenue websites would lose if targeted advertising were eliminated.  And I certainly wouldn't shed a tear over the end of any companies whose entire business model was secretly tracking users against their wishes and selling this information, you know, under the table.



To me, the Topics API feels as if Google is finally getting really serious about offering an alternative to tracking; one which, apparently, advertisers can live with and which will provide the websites which use it value.  After all, Google is a massive advertiser themselves, and they plan to shut down Chrome's third-party cookies starting in 2024, with full elimination by the middle of next year.  And if they will shut down the redirects that the use of Google Shirts creates, I would be very happy with that, too, because that annoys the crap out of me.  



Anyway, once they've done that, we can imagine that Chrome will become extremely tracking hostile.  Topics gives users not only a sense of control, but actual control, by allowing them to view, delete, disable the topics that are currently being judiciously doled out about them.  And this brings us to an interesting question:  If our personal interests are separated from tracking, so that the things we're interested in can be shared cleanly, and if that minimal sharing provides significantly greater advertising revenue to the sites we frequent, are we willing to voluntarily give sites that benefit?  I know with absolute clarity that I am.  If it's really that valuable, please accept it as my micropayment.  You know, the EFF's absolutists will disagree with me.  And that's fine.  Topics finally makes that an individual personal choice that each user of Chrome can make for themselves.



LEO:  So is the chief difference between this and FLoC the fact that it doesn't broadcast it?  It's kind of private?  Otherwise, it seems similar.  It's like a modified FLoC to me.



STEVE:  So maybe anything that creates information could be considered a modified FLoC.  What's different is there's nothing about it that's opaque, so it doesn't use this weird hash of websites that no one can interpret.



LEO:  Right, right.



STEVE:  That was a big problem.  So what Google has done is they have created a very simple - basically it's simple.  The browser itself, Chrome, will know what few topics TWiT.tv, the domain TWiT.tv, stands for.  Out of a list of about 1,500, there will be some number of topics, zero through three, that are associated with TWiT.tv.  And that is built into Chrome.



LEO:  You said 1,500.  You mean 349?



STEVE:  No, 349 is just their current working R&D...



LEO:  Oh, but it could be as much as 1,500.  Okay.



STEVE:  Yes.  Yeah, there really isn't a limit on the number, but the IAB currently operates with 1,500.



LEO:  Yeah, they have 1,500, yeah.  But it's ridiculous.  It's demographics and stuff like that, that probably the website isn't going to show.



STEVE:  There will be no race, no gender, nothing.



LEO:  So that eliminates a number of the IAB topics.



STEVE:  And so the point is that a - so Chrome knows what TWiT.tv is.



LEO:  Right.



STEVE:  And you'll have from that taxonomy, you know, up to three things that are associated with TWiT.  So when someone visits the TWiT site, the browser sees that their user has visited TWiT.tv and internally knows those three topics.  This is this classifier.  It is built into Chrome.  All the domains are built into Chrome.  And what it is that they're associated - and the up to three topics that they're associated with.  So the browser dumps those topics into a bin.  It just accumulates them.  And at the end of a week, all of the topics associated with all of the sites the visitor went to, at the end of a week this epoch ends.



So at that point the five topics that were hit the most are chosen as the things the visitor cared most about in the previous week.  And the five topics that were chosen from the week before are moved down to the second week, and the topics that were from the second week are moved to the third week.  So we have three weeks.  Each week has five topics.  So now when an advertiser queries the browser at a site, based on the domain where the advertiser is querying, the browser itself will select one from each of the five topics for each of the three weeks, deliberately scramble them, and say here are some things that the user of this browser is interested in.



And that's it.  That's the entire system.  Coupled with a user interface where the user is able to go and look at what those topics are and say, no, I'm not interested in that, and delete it.  Or, oh, I don't ever want to be associated with this, and so disable it so that it can never be affiliated with them.  So there's no tracking.  Google has gone to extremes to disclose a minimal amount of still useful information about the user.  And that's it.



LEO:  Yeah.



STEVE:  And it's in there now.



LEO:  This seems fair.  I mean, honestly...



STEVE:  I think it is really - I think it is really fair, Leo.



LEO:  This is how we sell ads on TWiT is, you know, your topic is tech.  And in the case of your show it's security.  And it's enterprise.  And those might be the three topics.



STEVE:  Exactly, exactly.



LEO:  And I have no problem with that.  That's stuff that a human could deduce anyway.



STEVE:  Exactly.



LEO:  So it's not - yeah.  And it just automates something that a human would do anyway, I think.



STEVE:  Yes.  And I think that the reason we're having trouble with this is that we have had such an adversarial relationship with, you know, the big bad Internet and how it wants, you know, and cookies planted and, you know, Panopticlick and fingerprinting and all of this crap.  I mean, so it's difficult to say, wait a minute, you mean that's what this is?  And Google's, like, that's what Google wants?  It's like, yes.



LEO:  There's a lot to be said for a privacy-forward system that still allows advertising to be topic-based.



STEVE:  Yes.



LEO:  As you said at the beginning, who knows if that works or not?  But at least we know advertisers demand it.  So we're going to have to give it to them.



STEVE:  And that means that they pay sites for it.



LEO:  Right.  Yeah, I mean, we are advertising-supported.  And we don't, you know, we don't have to worry about this kind of stuff because we're a podcast, not a website.  But if you're The Verge, this is good news.  This is something you want.



STEVE:  Yes, yes.  And really for me I do have an interest in health and wellness.  And so if saying yeah, I have an interest in health and wellness, if that's a micropayment that I can make to sites that I visit, I want to support them that way.



LEO:  To be clear, you say that only by your behavior, by visiting that site.



STEVE:  Correct.



LEO:  That's where it gets that information from.  It's really what it's saying is it's categorizing sites.  And then when you come to visit, now they've got - and are they attaching that to an identifier?  They are; right?  So it does follow you around.  You visited a health and wellness site.  So when you go to visit something else, it knows that's you.  Yes?



STEVE:  Well, so you visit something else in that browser.  So it's your browser...



LEO:  Right.  It's in the browser.  Yeah, yeah.



STEVE:  It's all TNO.  It's all local.



LEO:  Right, right.  



STEVE:  And that's the other cool thing about it is that it is transparent, and it is local.



LEO:  Yeah.  I think this is a good thing.  I hope, I mean...



STEVE:  It is.  I mean, it's like it's the answer we want.



LEO:  And you're right, I mean, groups like EFF, which I support, probably come from the point of view that no ad is a good ad.  And as a result, nothing's going to make them happy except, you know...



STEVE:  Yeah, who supports them?



LEO:  Well, yeah, right.  No, they're not ad-supported.  I support them by giving them money.  Which comes from ads on your show.  So I guess, yeah...



STEVE:  And I support Wikipedia.



LEO:  Same thing.



STEVE:  I get my little monthly notice, and thank you.



LEO:  Yeah, that's what I do with EFF.  Yes.  This is good.  This is very good.  I'll be curious to see.  I think at some point EFF and others are going to have to say, look, we're going to have something.  Let's choose something that does the least harm.  And this seems to be it.  Finally.



STEVE:  Yeah.



LEO:  This was Google's kind of plan all along was try some stuff, see what people said.



STEVE:  Yup.



LEO:  That's why it was always in beta.  Interesting.  Now, you said it's going to propagate to all other Chromium-based browsers.  This may not because it's not going to Chromium, I presume.  Or is it?



STEVE:  I think it's in Chromium.



LEO:  Oh, that's interesting.



STEVE:  I don't know.  I think it's in Chromium.  And now you can see why I want it in Firefox.  It's not a bad thing. 



LEO:  No, no.



STEVE:  It's a good thing.



LEO:  Yeah.  They finally got to a point, it was almost a silent negotiation, where they finally said, well, okay.  Will this work?  No?  All right.  Well, how about this?



STEVE:  I think that's, yeah, I think it's exactly right is that they came up with something that provides a minimum of information, yet something useful to advertisers.



LEO:  Well, we'll definitely be talking about it tomorrow on This Week in Google.



STEVE:  Yup.



LEO:  And I will be looking more into it myself.  Very interesting.  As always, you know.  And, boy, thank you for the very good news at the top of this show that you might - and by the way, you know, you're an at-will podcaster.  If at some point, Episode 1004 you go, well, that was a terrible idea, I quit, nobody's going to stop you.  Right?



STEVE:  Well, and as I said, I've been feeling this for a while.  I just - it's hard for me to imagine, like, you know, killing something that's good.  Like, you know...



LEO:  Yeah.  Thank you.  Well, and needed.  And you're doing something that's making a difference.



STEVE:  Everyone says so.



LEO:  Yeah.



STEVE:  So I'm going to thank everybody.



LEO:  Thank you for doing it.  By the way, yes, it is in Chromium.  I'm just looking at the Chromium site.



STEVE:  Cool.



LEO:  It is part of Chromium.  And we will definitely talk about it.  And I think the world will be talking about the Steve Gibson promise that at least we're going to get an Episode 1000.  I can't promise any more than that.  Steve, you're the greatest.  I appreciate it.  Catch Steve at GRC.com.  Just like on Episode 1.



STEVE:  Bye.



Copyright (c) 2023 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.










GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#936

DATE:		August 22, 2023

TITLE:		When Heuristics Backfire

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-936.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  Which Linux distro is selling itself to private equity capital, and what could possibly go wrong?  Will Android soon be talking to the sky?  What's up with the trouble SanDisk and Western Digital are in over their SSDs?  Are children still being tracked on YouTube's "made for kids" channels?  Has cryptocurrency become any safer, and what dangers are posed by the use of multi-party wallets?  Is FIDO2 ready with post-quantum crypto?  What's the latest on HTTPS by Default?  And after looking at some feedback from our terrific listeners, we're going to examine the nature of heuristic programming algorithms with a case study in what can go wrong.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  He's going to talk about the Linux distro that's being bought by private equity.  Oh, no.  We'll also talk about what's going on with those SanDisk and Western Digital SSDs and why you should not buy them.  And then a fascinating case study of a problem Microsoft has, maybe you've heard about it, with setting server clocks to ridiculous values, and maybe why it's happening.  Steve does a deep dive that's fascinating.  All that and more coming up next on Security Now!.	



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 936, recorded Tuesday, August 22nd, 2023:  When Heuristics Backfire.



It's time for Security Now!, the show where we cover the latest news from the security ecosphere with this guy right here, Mr. Steve Gibson of GRC.com.  Hello, Steve.



STEVE GIBSON:  Hello, Leo.  Great to be with you as we embark on year 19...



LEO:  Oh, my goodness.	



STEVE:  ...of this illustrious podcast.



LEO:  And as we learned last week, there may be a year 20 and even a 21.



STEVE:  I think that, you know, I was shedding a tear to myself...  



LEO:  For your free time?	



STEVE:  That we would probably not be together when Unix time wrapped around.



LEO:  2038, huh?



STEVE:  Yeah, I thought, you know...



LEO:  I don't know if I'll be here.



STEVE:  Maybe.



LEO:  You will.  Good.  I'm glad to hear it.  That's exciting, yes.



STEVE:  So, we're going to - today's episode is titled "When Heuristics Backfire," inspired by actually the Norwegian engineer who brought this back to everyone's attention after seven years of pain and anguish by sending me a tweet to that effect.  But anyway, so but of course we're going to get to that.  We're going to answer some questions first.



Which Linux distro is selling itself to private equity capital, and what could possibly go wrong with that plan?  Will Android soon be talking to the sky?  What's up with the trouble SanDisk and Western Digital are in over their SSDs?  Are children still being tracked on YouTube's "made for kids" channels?  Has cryptocurrency become any safer, and what dangers are posed by the use of multi-party wallets?  Is FIDO2 ready with post-quantum crypto?  What's the latest on HTTPS by Default?



And, after looking at some feedback from our terrific listeners, which will generate some additional commentary as opposed to like one word, oh, that was a good idea - we do have some of those, though - but we're going to examine the nature of heuristic programming algorithms with a case study in what can go wrong.



LEO:  What could possibly, you didn't say "possibly."  And by the way, I'm just doing a little research on Wolfram Alpha of how many weeks from today to the end of the Unix Epoch.



STEVE:  Ah.



LEO:  And I think he got it wrong.  I think Wolfram Alpha thinks that the end of the Unix Epoch is 53 years from now, which is outright no.



STEVE:  You are exactly right.  It's 2038.  The geeks are well aware.



LEO:  Yeah, yeah, yeah.  So let me just - what's the date?  Is it December 31st, 2038?



STEVE:  You could add 4.3 billion to January 1st, 1970.



LEO:  Well, let's say it's roughly 801 weeks.  So we will be at Episode 1737.



STEVE:  Oh, piece of cake.



LEO:  Easy.



STEVE:  We're already halfway there.



LEO:  Yeah.  Think of it that way.  We're halfway there.



STEVE:  We've already passed - we're over the hump.



LEO:  So, yeah, starting counting.  15 years, four months, nine days.  And, you know what, if you decided at any point, maybe even today, that that would be the last episode, at the end of the Unix Epoch we'll stop.



STEVE:  Honey, I have a USB cord.  All I do is pull it, and that's it.  I'm gone.



LEO:  The ripcord.  It's gone.  He's dead.  He's history.  Well, anyway, we'll endeavor to make it another 15 years.  That would be good.  That would be good.



STEVE:  Yeah.  I think that's, you know, as our illustrious William Shatner said, "Don't die."



LEO:  Just don't die.



STEVE:  It's not a problem.



LEO:  That's both of our jobs now for the next 15 years.



STEVE:  Don't die.



LEO:  Do not die.



STEVE:  And I love the word because it would be see everything; right?



LEO:  Right.  Well, it comes from - was it Bessemer's conception of a prison, the panopticon, where prisoners would be under surveillance, but they wouldn't know.  They knew that they could be, but they wouldn't know when the jailers were watching them.  So that the jailers would not have to watch all the time because the prisoners just - they wouldn't know.  They'd just think we could be watching.  And we all live in a panopticon now, actually, come to think of it.  That's a depressing thought.  All right.  Picture of the Week time, Steve.



STEVE:  So this was one I had in the pile in reserve.  And I liked it, and I think the caption is perfect for it.  It shows sort of an idyllic setting.  We have a slow-moving river in the background, and a little grassy knoll sort of lawn thing in the front, and what looks like a gate from the 1400s.  I mean, it's like out of time somehow, two stone pillars on either side of a gate.  But there's no gateage going off in either direction.  So it's just this gate segment; right?  I mean, there's like nothing to keep you from going around.  And we've of course had many of these similar gates that we've had fun with on the show in the past.



But in this case, we have four sheep.  And for reasons that are not explained by the photo, they are stuck behind the gate.  Actually, one of them is kind of peeking around, wondering if maybe it might be all right to proceed.  The other three, no.  They're just at the gate, waiting apparently for it to open, despite the fact that you could easily go around on either side.  And of course I gave it the caption, "This Is Why They're Sheep."



LEO:  Yes.



STEVE:  So yes, because they don't know any better, they're just like, oh, the gate's closed.  Damn.



LEO:  Love it.  See, this explains all the previous gate-without-wall pictures you've had.  They're for sheep.



STEVE:  Yeah.  They were, exactly, the sheep had apparently not yet arrived.  We know had they arrived they could not have left.  So they would still be there waiting for the gate to open.  But, or maybe some nice person would come along and say, oh, you poor sheep.



LEO:  Aw.



STEVE:  We'll let you through.



LEO:  And clearly this one sticking his head around the side is the troublemaker, the black sheep.



STEVE:  Yeah, exactly, he's looking around, eh, why are we standing here on this side of the gate that has no sides?  Don't know.



Anyway, I wanted to take just a moment of everyone's time to thank the many listeners who let me know after last week's podcast how glad they were to learn that I would not be retiring from this weekly communication after podcast 999.  It had become clear to me that my plan to quit, for no particular reason, after 999 would have felt very weird as that day was approaching; right?  I mean, it would have been, like, okay, uh.  But anyway...



LEO:  Sad.  It would have been sad. 



STEVE:  Now we know that's not going to happen.  So yay, and thank you, everyone, for sharing your sentiments with me.



LEO:  Yeah.  We're so happy.



STEVE:  I'm not certain whether it will matter to anyone here, but I wanted to note that the currently public company SUSE, you know, spelled S-U-S-E, the company behind the OpenSUSE Linux distro, has announced its plans to delist itself from the Frankfurt stock exchange and allow itself to be purchased and taken over by a private equity firm.



LEO:  Oh, god, no.



STEVE:  I know.  Their announcement read, in part:  "EQT Private Equity has underscored its commitment to supporting the company strategically and financially, and to cooperate closely with SUSE's CEO and its leadership team."  Dirk-Peter van Leeuwen, CEO of SUSE, said:  "I believe in the strategic opportunity of taking the company private.  It gives us the right setting to grow the business and deliver on our strategy with the new leadership team in place."



LEO:  Oh, no.



STEVE:  Uh-huh.  "EQT Private Equity's and SUSE's partnership," he says, "in a private setting has been fruitful before, and we're excited about the long-term potential of the company and our continued collaboration."



Right.  Now, the whole truth is, as you've already been bemoaning properly, Leo, it's unclear what this means for the future of OpenSUSE.  In general, as we've witnessed a number of times in our own PC and security industry, private equity firms do not purchase tech companies out of their love of tech.  They typically purchase them because they perceive pockets of unexploited profit potential and some means of squeezing money in the short term from the golden goose.  This is typically done through aggressive layoffs, cost cutting, selling off the less profitable pieces, and in general squeezing the remaining life - and the future - out of their new acquisition.



Users who have made their own investments in the future of such enterprises, like by adopting OpenSUSE as their Linux, should be forewarned that, if nothing else, change is coming.  We don't know what kind of change, but it may not be good.  So anyway, I just thought I wanted to let everyone know, OpenSUSE may not be quite so open in the future.



LEO:  I mean, Novell's owned SUSE forever.  So, I mean, but Open has always been the open version.



STEVE:  Yeah, yeah, yeah.



LEO:  That's terrible.



STEVE:  Yeah.  Okay.  If imitation is the strongest form of flattery, it appears that Google is enamored of the satellite communications capabilities that Apple added to iOS 14.  It appears that Google is testing a similar new Android feature that will allow its users to send SOS messages via a satellite connection in the case of an emergency.  Now, this nascent feature was spotted in the code for Android 14, which is slated for release, like, any moment, like within days.  It's supposed to be the end of August, and we don't have much August left.  The feature is unannounced, and we should not expect to see it going live anytime soon.



According to Android OS expert Mishaal Rahman, the feature is not part of the main Android 14 OS and is just tagged as "demo code" in the source.  So, you know, it's going to need accompanying radio hardware, I'd imagine, in order to talk to satellites.  But anyway, Android users can probably assume that a feature similar to Apple's will be arriving at some point.



I should mention, in case any of our listeners might be impacted by this news, that SanDisk, my own preferred manufacturer of solid-state mass storage, and Leo...



LEO:  Not anymore.  Not anymore.  Not anymore.



STEVE:  Yeah, I know, I know.  SanDisk and Western Digital are both currently in the dog house over what is alleged to be their willing and knowing sale of defective SSD products.  And not just a little bit defective.  The allegations are disturbing.  Here's the start of what Ars Technica had to say about this.



Ars wrote:  "Amid ongoing pressure to address claims that its SanDisk Extreme SSDs are still erasing data and becoming unmountable despite a firmware fix, Western Digital is facing a lawsuit over its storage drives.  A lawsuit filed last Wednesday accuses the company of knowingly selling defective SSDs.  Western Digital branded SanDisk series of Extreme V2 and Extreme Pro V2 portable SSDs, and they're often being recommended, or were, by tech review sites."  Ars wrote:  "If you've considered a portable drive, it's likely you've come across the popular series in your search."  And of course I'm wanting to let our listeners know because they are popular, and they may not have heard about this yet.



So Ars wrote:  "Numerous owners of the drives, including Ars Technica's own Lee Hutchinson, encountered a problem where the drives seemingly erased data and became unreadable.  Lee saw two drives fill approximately halfway before showing read and write errors.  Disconnecting and reconnecting showed the drive was unformatted and empty.  Wiping and formatting did not resolve things."



LEO:  Oh, really.  You mean your data didn't magically come back when you wiped and formatted, huh.



STEVE:  Wow, yeah.  "Complaints about the drives littered SanDisk's forums and Reddit" - and Ars then provided four example links of typical postings - "for at least four months before Western Digital released a firmware fix in late May.  The page for the update claims products currently shipping are not affected.  But the company never noted customers' lost data claims.  It did, however, name the affected drives."



So we have the SanDisk Extreme Portable 4TB, the Extreme Pro Portable 4TB, same thing in 2TB and 1TB, that is, the SanDisk Extreme Pro Portage.  And also the Western Digital My Passport 4TB.  And then they finish:  "Subsequent reports from The Verge, which received a replacement SSD, and some Reddit users, though, claimed the drives were still broken.  Western Digital didn't answer requests for comment about newfound grievances."



Okay.  So for what it's worth, I just wanted to inform our listeners that this has been and apparently still is a problem.  And if I were storing important data on any of those named drives, I'd be a little nervous.  I wouldn't maybe store anymore data on them.



Now, hearing that - and I actually have sort of maybe an explanation of why.  Hearing that the drives started having trouble when they became around half full was interesting to me.  As an engineer, I have been astounded and shaking my head when I learn the lengths to which today's SSDs are going to achieve the storage densities they offer.



For example, get a load of this:  Remember first that NAND-style SSD storage cells, which is what we are using, the storage cell is really just a capacitor.  It's an itty-bitty - and yes, that's an engineering term - it's an itty-bitty spec of metal sitting on top of an insulating coating.  The act of writing to that cell involves cranking up the voltage internally to deliberately break down that layer of insulation to inject and strand electrons on that itty-bitty island of metal.  Once there, their presence can be sensed thanks to the magic of field-effect transistors.  But the point is, the number of electrons which have been stranded on that electrostatic island is an inherently analog quantity.



Now, the designers of the earliest FLASH memory, which regarded the fact that all of this worked at all to be a miracle, were quite content to only either fully fill or fully empty the islands in order to store zeroes and ones there.  But then management came along and demanded more density.  The engineers explained that they had already made the electrostatic islands as itty-bitty as possible.  But then one of them made the mistake of suggesting during a brainstorming meeting that they might be able to store two bits of data on each island by storing varying amounts of electrons, thus doubling the effective storage density of the same number of itty-bitty memory cells.



Instead of storing a boring old zero or one, it would be possible to store a zero, a one third, a two thirds, or a one.  Since this gives us four separate storage levels, that's two bits of data.  Whereas the original NAND memory cells were known as SLC for single-level cell, this next generation were MLC for multi-level cell.  And, of course, once you've opened Pandora's Box of storing what are essentially analog levels of electrostatic charge in cells, why stop at two bits?  How about three or four?



Okay.  So with this bit of background, here's what I learned that made me shake my head.  The SSD manufacturers are aware that storing fewer bits per cell is better.  It's faster to read and write, and it's more reliable.  So believe it or not, today's most advanced SSDs start out by storing fewer bits per cell because it's faster and more reliable.  It makes their product benchmark faster when it's mostly empty.



But as their users continue to fill them up with data, the SSD begins to run out of storage at that lower, faster, and safer density.  At some point of fullness it needs to switch over to storing more bits per cell in order to actually deliver its promised and rated full storage capacity.  To do that, it starts reading and rewriting existing data which was stored at the lower bit density into higher bit density.  Essentially, it's on-the-fly cramming more bits into fewer cells because, as it turns out, its owner actually wants to use the storage capacity that they purchased.



The extra complexity that had to be built into the controller to track and accomplish all of this is somewhat mindboggling, but it's obviously worth it to its product's developers.  The flakiness inherent in how far SSD storage density has been pushed is one of the reasons I became certain that a continuing investment in SpinRite would be warranted into the future.



Okay, now, only SanDisk's engineers know why, when their drives are storing about half of their rated capacity, they suddenly developed a problem that crashed the entire drive.  It could be that the drive had actually filled itself with data stored at half-density, and now needed to get serious about doubling-up on density; and that some flaw in the drive's firmware was then triggered.  We'll likely never know.  This brings to mind something else that happened and was discovered last week by one of SpinRite's testers on one of their thumb drives.  But I've already taken up a lot of time on this, so I'm going to share that interesting story about something else that happened and was really interesting next week.



LEO:  Whew.  Wow.  By the way, just to correct the record, I just looked through my Amazon orders.  I've never bought a SanDisk SSD.  I always do the Samsung EVOs.



STEVE:  Oh, you're right, Samsung is also what I was thinking.



LEO:  Yeah.  We like the Samsungs.



STEVE:  Yeah, yeah, yeah.



LEO:  I don't have any SanDisks.  SanDisk I buy their SD cards, their solid-state memory cards, yeah.



STEVE:  Yes, and I have a bunch of SanDisk thumb drives because I like those.



LEO:  Yes, and I do, too.



STEVE:  It's the Samsung NVME, beautiful memory.



LEO:  Or even like the 2.5-inch SSDs, yeah.



STEVE:  Or even the big, yeah, SSDs, yes. Good.  I'm glad you [crosstalk] that.



LEO:  Samsung is not SanDisk, thank goodness.



STEVE:  Right, right.  Okay.  I ran across a blurb of news that read:  "YouTube children's privacy:  An Adalytics report found that advertisers are still tracking viewers of videos made for kids, despite a 2019 promise from YouTube to stop delivering personalized ads on these types of videos.  And senators are now seeking a formal inquiry into the company for breaching its U.S.  COPPA (C-O-P-P-A) laws."



COPPA stands for Children's Online Privacy Protection Act.  And so I was curious about this, so I followed the Adalytics link.  Adalytics appears to be a scrupulously neutral adtech industry watchdog.  They make a point of saying they don't take, they do not accept any money from the adtech industry because they don't feel they could do so without an inherent conflict of interest, which I think is obviously true.  Their report is long, but I'll share just the top few takeaways which will give everyone the gist.



They said:  "First, YouTube's CEO said in 2019 that the platform would 'limit data collection and use online videos made for kids only to what is needed to support the operation of the service."  So, which, you know, is like being logged on, I suppose.  "However, YouTube appears to be setting or transmitting 'advertising' cookies and identifiers on the devices of viewers who are watching 'made for kids' videos as of last month, July 2023."  So that promise was not kept.



"Second, YouTube's CEO said in 2019 that the platform would stop serving personalized ads on 'made for kids' content.  However, demographically and behaviorally personalized ad campaigns appear to have ads being served on 'made for kids' YouTube channels as of last month, July 2023.



"YouTube is serving ads from many 'adult' Fortune 500 advertisers and major media agencies on YouTube channels that are labeled as 'made for kids.'  These include major brands such as Mars, Procter & Gamble, Ford, Colgate-Palmolive, Samsung, and many others.  In some 'adult' brands personalized ad campaigns, the top YouTube channels by clicks or click-through rate are popular 'made for kids' YouTube channels."  In other words, kids are clicking on these things.  And get this:  "Such as 'ChuChu TV Nursery Rhymes & Kids Songs,' 'CoComelon Nursery Rhymes & Kids Songs,' or 'Kids Diana Show.'"  Click-throughs are happening for the ads that are on those, and they're going to adult brands, not kids' brands.



"The viewers of 'made for kids' YouTube videos appear to be clicking on ads and brands websites such as Michigan State Police, Disney, BMW, Hyundai, and Verizon.  And those companies are harvesting and sharing metadata on those viewers with dozens of data brokers upon click-through.  This raises the possibility that brands have 'data poisoned' their first-party datasets with data derived from thousands of viewers of 'made for kids' videos."



Anyway, all of this sounds like YouTube is absolutely not taking the requirements of COPPA for granted, I mean COPPA seriously.  They are, you know, just like blowing it off.  Anyway, the report goes on and on like that.  And this is clearly not what the COPPA act had in mind.  So it's hardly surprising that the U.S. Senate, which has been all worked up over TikTok recently, as we know, is wondering what's going on with this popular U.S.-owned property?  In the show notes I titled this story "You asked for it" since that's clearly what Google is doing.  You know?  I mean, they're just blatantly ignoring this.  This entire tracking and data-brokering industry seems to me needs a flushing, and at this rate that might not be too far away.



Since we were just talking about the danger of losing access to an email forwarding service, I got a kick out of this little bit of news:  Apparently the operators of the 8Base ransomware operation lost all of the data that they had previously stolen from their victims when the AnonFiles cloud hosting file storage service closed down without notice last week.



LEO:  Aww.  Aww.



STEVE:  And I saw elsewhere the announcement from AnonFiles.  They just said:  "We are so disgusted, and we are so fed up with trying to offer, like trying to do the right thing, and offer for the value and for the sake of the Internet an anonymous file hosting service.  We are being overrun with malicious use of our anonymous files hosting."  So they pulled the plug, and with it they took a bunch of data that had been exfiltrated from some victims of the 8Base ransomware group.  So yay for that.



Okay.  Meanwhile, the blockchain security firm CertiK, who often provides interesting tidbits that we talk about here, says that a group of Canadian scammers are responsible for stealing millions of dollars during the past few years.  The group's members operate by hacking the Discord servers used by cryptocurrency communities and posting phishing links to hijack their users' wallets.  CertiK claims to have identified the real-world identity of one of the group's members.  Going by the pseudonym of "Faint," (F-A-I-N-T), the individual is believed to have stolen more than $1 million worth of assets since late 2022.  Faint is the group's second publicly-identified member after another blockchain investigator exposed a hacker named Soup last month.



So anyway, I just tossed that one out there to remind everyone that nothing much appears to have changed in the cryptosphere of late.  It remains a wildly immature and insecure mess still.



In one other bit of crypto news, there's another example of this fundamental immaturity within the entire industry.  Major crypto-industry players jumped onto an unproven and not fully tested algorithm due to the promise of what it offered.  And of course isn't that the story of this entire cryptocurrency phenomenon?  Earlier this month, during Black Hat, researchers from Verichains disclosed three vulnerabilities in a cryptographic protocol called TSS which is used to create multi-party crypto wallets known as MPCs.  Okay.  Here's how Binance, one of the prominent users of this new TSS algorithm, describes it on their site.  TSS is short for Threshold Signature Scheme.



They said:  "Threshold Signature Scheme (TSS) is a cryptographic primitive for distributed key generation and signing.  The use of TSS in blockchain clients is a new paradigm that can provide numerous benefits, especially in terms of security."  Whoops.  "In the broader sense," they wrote, "TSS can influence the design of key management systems such as crypto wallets, and lead the way for native support in DeFi use cases.  Having said that," they wrote, "TSS is still a new technology, so the risks and limitations should be considered."



Unfortunately, apparently they didn't take their own advice.  Or if they did, they didn't consider them enough because the last part they got right.  



The Verichains researchers named their attacks on TSS "TSSHOCK". TSSHOCK exploits vulnerabilities in some of these MPC, this multi-party crypto wallet technology, through their implementation of the threshold elliptic curve digital signature algorithm (ECDSA).  And so this is a new thresholding style.  The exploit of TSSHOCK can allow threat actors to steal cryptocurrency from individual users or major institutions while leaving no trace of the attack on the client site.



And here's the good part:  Major companies like Binance, Zengo, Multichain, THORChain, and ING Bank use exactly these vulnerable threshold elliptic curve DSA software implementations.  And on top of that, TSSHOCK is the second major multi-party crypto wallet vulnerability disclosed this month, the first being BitForge.  And Leo, after taking our second break, I'm going to tell everybody what happened earlier this month with BitForge.



LEO:  Oh, lord.  I bet it's good.  Can't wait.  Yeah, it's always fascinating.  By the way, I did a little research.  The Unix Epoch ends when it crosses over the 32-bit threshold on January 19th.  So...



STEVE:  Early 2023 [sic].



LEO:  Early 2023 [sic].



STEVE:  In which case I'm sure we'll still be here.



LEO:  We only have 752 more episodes after this.



STEVE:  Yeah, like I said, we're like way past the halfway point.



LEO:  So I just wanted to give you a heads-up, you know.



STEVE:  I'm feeling great, Leo.  I hope you're doing great.



LEO:  It's only 14 years.  I think I can make - I'll be 80.  That's not so bad.



STEVE:  You know, our listeners who were having their first children born at the beginning of the podcast will be grandparents by the time...



LEO:  You know what, let's put a pin in it.  I'm planning on it.



STEVE:  Okay.



LEO:  June - because we could then talk about, you know, the new Y2K problem, the Y238 problem.



STEVE:  That's right.



LEO:  And then wrap things up.  January 19th, which is a Tuesday, by the way.



STEVE:  Perfect.



LEO:  2038.



STEVE:  Perfect.  I like...



LEO:  We don't even have to do a special.



STEVE:  So for the sake of painting a full picture of just how rocky things still are in the crypto industry, I should also touch on this earlier discovery which I referred to, BitForge, which is made by a group known as Fireblocks.  Two weeks ago, on August 9th, Fireblocks posted their discovery under the title  "Fireblocks Researchers Uncover Vulnerabilities Impacting Dozens of Major Wallet Providers."



And they explained in their posting:  "Today, Fireblocks Cryptography Research Team announced the discovery of multiple zero-day vulnerabilities in some of the most used cryptographic multi-party computation (MPC) protocols, including GG-18, GG-20, and implementations of Lindell 17.  If left unremediated, the exposures would allow attackers and malicious insiders to drain funds from the wallets of millions of retail and institutional customers in seconds, with no knowledge to the user or vendor.



"The series of vulnerabilities, dubbed BitForge, impact popular wallet providers like Coinbase WaaS."  I guess that's Wallet as a Service, to which you have to ask, what could possibly go wrong with that?  We also have Zengo and Binance using this.  They said:  "Following industry-standard 90-day responsible disclosure processes, Coinbase WaaS and Zengo have since fixed and resolved the identified issues."  They don't mention Binance there.  They said:  "In addition, the academic papers which had the details of their flaws redacted have been revised.  The Fireblocks Cryptography Research Team findings were presented during the Black Hat USA conference on Wednesday, August 9th, and will be shared at Defcon on Thursday the 10th."



Pavel Berengoltz, Co-founder and Chief Technology Officer at Fireblocks, said:  "As decentralized finance and Web3 continue to gain popularity, the need for secure wallet and key management providers is evident.  While we are encouraged to see that MPC" - that's the multi-party stuff - "is now ubiquitous" - wow, I'm not encouraged, but okay - "is now ubiquitous within the digital asset industry, it's evident from our findings, and our subsequent disclosure process, that not all MPC developers and teams are created equal.  Companies leveraging Web3 technology should work closely with security experts with the know-how and resources to stay ahead of and mitigate vulnerabilities.  Maintaining and updating core infrastructure technologies, like Web3 wallets, is crucial" - get this - "in preventing thefts and attacks, which amounted to" - ready for this - "nearly $500 million in just the first half" - just the first half, Leo, of this past year 2023.



LEO:  Half billion here, half billion there, it starts to add up.



STEVE:  Yeah.  And where is this, like who has all this money that's being lost?



LEO:  Well, they don't anymore.



STEVE:  It's just an astonishing, astonishing amount of money is being drained out of people.  It's like, wow.  Anyway, "Wallet as a Service."



LEO:  No.  Wrong.  Yeah, I'll tell you what the service is.



STEVE:  Yeah.  Wow.



LEO:  We'll take your money.  Then we'll take your money.



STEVE:  Wallet as a drain plug.



LEO:  Yeah.



STEVE:  Anyway, from the beginning of this cryptocurrency odyssey, our one constant piece of advice here has been to keep your wallet safely offline.



LEO:  And then don't forget your password.  Whatever you do.



STEVE:  Do not format the hard disk where you had your 50 bitcoins.



LEO:  That's right.



STEVE:  Boy, that was a dumb one.



LEO:  Don't even - just forget that ever happened.



STEVE:  Anyway, enough said.



LEO:  Enough said.



STEVE:  If you want to play with crypto, just please be careful and heed the age-old investment advice to never gamble more than you're able to lose.



LEO:  See, we - this was - we didn't gamble anything.  It cost  you nothing to get that 50 bitcoin.



STEVE:  Only heartache after I formatted the hard drive.



LEO:  Yeah, I never have purchased bitcoin.  It was all donated.  So we didn't gamble anything.  But people were buying bitcoin.  Not now, I hope not, anyway.



STEVE:  Well, and, you know, in all fairness, when I was in Boston with you, Leo, when we did the event there and had a whole bunch of our podcast listeners show up, we made some bitcoin millionaires.



LEO:  What?



STEVE:  There was a guy came up and said, "Thank you, Steve.  I listened to your bitcoin podcast.  I got involved.  I'm now retired.  I've never had to work again."



LEO:  Holy cow.



STEVE:  And this was a young guy.



LEO:  Nice, very nice.



STEVE:  Yeah.  So that was very cool.



LEO:  Yeah.



STEVE:  Unfortunately, I didn't follow my own advice.



LEO:  Oops.  Oh, god.



STEVE:  Okay.  So does anyone want a "quantum resilient" FIDO2 security key?



LEO:  Yes.



STEVE:  Well, it may be of more interest once it's possible to use Passkeys widely.  But in any event, Google has developed the first-ever version of a FIDO2 security key that includes protections against quantum computing attacks.



LEO:  Wow.  I didn't know we'd had a way to do that.



STEVE:  Yeah.  Well, remember we have those nascent algorithms.



LEO:  Yeah.



STEVE:  Like Dilithium.  And of course, yeah, because of quantum.  And they exist.  So the implementation comes from a new version of what's known as OpenSK, as in Open Security Key, which is an open-source project on GitHub that provides firmware for security keys.  Google says this new OpenSK firmware version uses a novel ECC [Elliptic Curve Crypto] Dilithium hybrid signature scheme its engineers developed along with academics from ETH Zurich.  The project doesn't yet appear to be, like, to be production ready.  It's definitely still experimental and not ready for prime time.  But it appears that when quantum computers eventually happen, we'll be ready, even if there still aren't many sites supporting Passkeys.  At least those that do, you'll be safe.



And actually, this is important for Passkeys because it uses public key crypto, which is the form of crypto which quantum computers are expected to crack.  Symmetric crypto, that's not quantum unsafe.  So all of the symmetric crypto that we have, you know, that's fine.  It doesn't need to be changed.  It's the public key crypto that is the problem.  So, you know, it's nice that we'll be ahead of the game.



Chrome 116, which is today's current release, contains a not-enabled-by-default feature which is scheduled to be enabled by default in Chrome 117 at the start of September, so a couple weeks from now.  The feature is called "safety check extensions," which will allow Google to display notifications to Chrome users when one or more browser extensions that they currently have installed and running are removed for some reason from the official Web Store.  The notifications will be shown when an extension is marked as malware, the extension is removed for Terms of Service violations, or when the extension is unpublished by its developer.



Now, that seems like an uncontroversial good thing, though it's a bit surprising that Chrome doesn't already do that.  But certainly good that they're going to be adding that.  It's in there now.  You can turn it on.  But it's not on by default.  It'll be on by default with the next release of Chrome.



And speaking of defaults, even with TLS certificates now being free, we're still not all onboard the HTTPS train.  So last Wednesday, in a posting titled "Towards HTTPS by default," Google announced their next move.  They wrote:  "For the past several years, more than 90% of Chrome's users' navigations have been to HTTPS sites, across all major platforms.  Thankfully, that means that most traffic is encrypted and authenticated, and thus safe from network attackers.  However," they wrote, "a stubborn 5 to 10% of traffic has remained on HTTP, allowing attackers to eavesdrop on or change that data."



Okay.  Now, just wait a minute.  It's not "stubborn," Google.  It's just ignoring you.  It doesn't care.  Imagine that.  Now, okay.  As we know, this is certainly traffic that really doesn't need any security of any kind and is 100% completely happy being unauthenticated and out in the open.  But it apparently annoys Google nevertheless.  I have an example in the show notes of exactly such a site.  I refer to it constantly while working on SpinRite since it contains a comprehensive list of every PC interrupt from 00 to FF, all of their sub-functions and their arguments.  It is wonderful, and I'm very thankful for it.  I've copied the entire site against the inevitable day when it finally disappears.  It's http://www.ctyme.com/intr/int.htm.  And yes, it is proudly HTTP.  And I'm pretty sure that it's never going to change.  As an aside, it also makes my own website look quite modern by comparison.  It even has the old mailbox icon for email.



Anyway, Google is still annoyed, so here's what they're going to do.  They said:  "Chrome shows a warning in the address bar when a connection to a site is not secure, but we believe this is insufficient.  Not only do many people not notice that warning, but by the time someone notices that warning, the damage may already be done."  Right.  You know, what?  The damage from looking up some interrupts in a table.  Anyway, maybe some dastardly Russian is going to do that to subvert SpinRite.  I don't know.



LEO:  We had this argument, not argument, discussion about my site where we don't have any logins, like...



STEVE:  Exactly.  Exactly.



LEO:  Why does it need to be secure?



STEVE:  Exactly.



LEO:  Yeah.  I mean, you can make an argument that it could be impersonated.  But then, still, what are they going to get?  There's no login.



STEVE:  It's just, yeah, exactly, it's just some data that someone wants to share on the Internet.  That's it.  It's some data that somebody wants to share on the Internet.



LEO:  The irony of this interrupt jump table is it's hosted by Computer Tyme, which is a web hosting company that is also not HTTPS.



STEVE:  And they're apparently, now, one can't really discern from that...



LEO:  Look at that, there's John C. Dvorak, and I get no spam.  This has got to be just a site that's been around for a while, and it's dead; yes?



STEVE:  I think you just scrolled past an Apache icon on the left.



LEO:  Oh, my god, this thing is ancient.  Uh, wow.  Computer Tyme.  I think this is the guy who used to provide Dvorak with his spam filtering.  Which is wild.  Outbound spam filtering.  So it's just some guy.  Yeah, he says, if you saw the John C. Dvorak article, "The Death of Email," in PC Magazine, or Show 18 on This Week in Tech.  Like this is literally 15 years ago.  We can provide you with the individual Marxmail accounts you want.  We do host John's email.  I wonder.  I wonder if it's still - anyway, very cute.  I guess he doesn't have time for HTTPS.



STEVE:  So it's a perfect example of a site that's like...



LEO:  It's doesn't need it.  Doesn't need it.



STEVE:  Yeah, stop bugging me, I just want to put some stuff on the Internet.



LEO:  His spam filter, by the way, is HTTPS.  So he knows enough to do it when it's needed.  



STEVE:  Right.



LEO:  Yeah.



STEVE:  Yeah, exactly.  And that's, you know, old school.



LEO:  Old school.



STEVE:  I would argue it's just fine.  Anyway, Google says the damage may already be done if you go to a site that isn't HTTPS.



LEO:  So there.



STEVE:  Literally.  "We believe," they say, they wrote, "We believe the web should be secure by default."  That's right.  No insecure traffic allowed.  Wow, you know, you'd think they were selling certificates.  But they're not.  I think they're getting a little far over their skis with this thing.  But after all, they're just a web browser; right?  Anyway.  So here it is.  "HTTPS-First Mode" - that's what they're calling this new thing.  "HTTPS-First Mode lets Chrome deliver on exactly that promise" - no insecure interrupt tables - "by getting explicit permission from you before connecting to a site insecurely."  Oh, great.  So anyway, thank goodness I'm over here on Firefox, where that's not happening.



They said:  "Our goal is to eventually enable this mode for everyone by default.  While the web isn't quite ready to universally enable HTTPS-First Mode today, we're announcing several important stepping stones toward that goal.  Chrome will automatically upgrade all http:// navigations to https://, even when you click on a link that explicitly declares http://."



They said:  "This works very similarly to HSTS upgrading, but Chrome will detect when those upgrades fail, for example, due to a site providing an invalid certificate or returning an HTTP 404, and will automatically fall back to the http:// that you originally asked for.  This change ensures that Chrome only ever uses insecure" - and I would argue not secured, but no - "insecure HTTP when HTTPS isn't truly available, and not because you clicked on an out-of-date insecure link.  We're currently experimenting," they said, "with this change in Chrome 115, working to standardize the behavior across the web, and plan to roll out this feature to everyone soon.  While this change cannot protect against active network attackers" - you know, fine - "it's a stepping stone towards HTTPS-First mode for everyone, and protects more traffic from passive network eavesdroppers."  Again, for sites nobody gives a crap about.



Okay.  So all that said, overall, this is a good thing.  It would be annoying to need to push past another warning screen before being allowed to access an HTTP site.  That does seem like overdoing it.  But certainly trying first to access a URL over port 443, bring up a TLS handshake, and see whether that http:// URL is also available via HTTPS, that makes sense.



However, it is worth noting that technically HTTP URLs and HTTPS URLs are actually referring to different resources.  Nothing anywhere ever says that the URLs of those differing protocols always or ever needs to or should refer to the same web resources.  It's true that they generally do.  Most web servers serve the same content regardless of whether their visitors come in via HTTP or HTTPS.  And these days, most attempts to come in via HTTP are immediately redirected to the same URL over HTTPS to, like, fix the link.



But that doesn't necessarily need to be so.  It's just the way things have typically evolved over time.  So I can see why Google is being cautious.  They clearly want to kill HTTP entirely, and I'll bet they even want to remove its support from Chrome.  But it doesn't look like it's going to go away quietly.  So it's putting up a bit of a fight.



One last bit before we get to our listener feedback.  I wanted to note that WinRAR is now at v6.23.  I've long been a fan, and I'm a registered owner of WinRAR.  So I wanted to note that a recent update to v6.23 earlier this month closed a couple of maliciously exploitable holes that attackers could use in a targeted attack to run their own code on their victim's system.  So if you are, like I am, a WinRAR user, you want to probably go over there and grab it.  I looked.  I fired up my copy.  Nowhere is there an update, check for update, check for new version or anything like that.  There is a link to go to their website automatically, but they make you do that.



And actually I'm not sure that's a bad thing, although it would be nice to be notified if like an important update were made available.  This actually happened earlier this month, you know, weeks ago, and I didn't know about it until I ran across a mention of it elsewhere.  So anyway, now all of our listeners know.



LEO:  That's why you have to listen.  We're going to tell you.



STEVE:  That's exactly right.  That's exactly right.  Until 2032, when the Internet will finally end.



LEO:  2038, yes.  Don't rush me.  Don't rush me.



STEVE:  Oh, no, no, no.  That's right.  Got to wait to the bitter end.



So iam-py-test, that's his Twitter handle, he said:  "Hello.  Hope your weekend is going well.  Relating to this week's episode, I thought you might find it interesting to know uBlock Origin disables Topics by default."



LEO:  Oh, interesting.



STEVE:  Uh-huh.



LEO:  You can block it.



STEVE:  Yes, by modifying the Permissions Policy header.  "Thanks for the podcast and have a nice day."  So as we mentioned last week, there is - again, Google is wanting to be really, you know, really kind and gentle about this.  It is possible for a website to add a header that turns off Topics completely and so that every time the web browser receives any content from a site, if that header, there's a permissions policy header says no Topics, then they're disabled for that site.



And I suppose, you know, we should have expected this from Gorhill; right?  I mean, he makes Dvorak seem all giddy.



LEO:  He's a cranky fella, he is.



STEVE:  Yeah, yeah, he really is.  In the GitHub dialog on this, Gorhill quoted a comment from someone else which read - and he was quoting stephenhawk8054, who said:  "Gave more thoughts to this, and I think we should add it to uBlock filters - Privacy list.  I don't think a user interface component for this is the way to go, given that so far it's an API supported only by one browser, and that the API is to serve advertisers in the first place." 



Okay.  Now, as we know, Topics is a new component of the underlying Chromium browser core.  All of the many Chromium-based web browsers will at least have the opportunity to support it easily, if they don't deliberately block it.  And initially, you know, some of those may choose to block it...



LEO:  I'm sure Brave will, and yeah, yeah.



STEVE:  ...just because they - yes, exactly.



LEO:  Privacy-focused ones, yeah.



STEVE:  Right.  And, you know, this is the sort of major change that can be expected to take time, and will require a great deal of re-education, which we, at least, began on this podcast last week.  Just look.  Google's like all pissed off that HTTP still works.  And that's, you know, good luck on killing that off.



LEO:  Takes a while.



STEVE:  Does take a while.  Everyone here enjoys at least knowing where the bleeding edge is, and we always find it.  So some patience will be needed.  The EFF will likely never come around to the idea of helping websites to monetize their visitors by providing advertisers with any information at all about who's visiting their pages.  You know, and who knows, maybe they're right.  It might be that all this profiling which grew out of tracking which was itself an unintended side effect of cookies has always been something that data brokers have just been getting away with only until the much slower-moving government regulations finally catch up.  You know?  Perhaps absolute anonymity is what we will eventually get, thanks to legal frameworks rather than technological frameworks.  So anyway, no matter what happens, we're going to have fun following it here on the podcast.



LEO:  There's a whitepaper which you probably read from Mozilla about this.  And their complaint, we've talked about it since your show last week on several of our shows, their complaint was that it's possible to deanonymize this topic information.  If you visit enough sites, you know, it's possible to deanonymize it.  And that's of course always the concern.  If you can keep it anonymous I think it's...



STEVE:  I didn't see a whitepaper.  I did not see the whitepaper.  I will look at it.  I don't know how that's possible, so I'm wanting to find out.



LEO:  Yeah.  Well, you know, they may be making the assumption, which isn't probably wrong, that deanonymization is easier in many cases that one would expect, you know.  We've seen a lot of cases where, wow, you can deanonymize...



STEVE:  I'm curious because there's a whole bunch of anti-deanonymization stuff in there.



LEO:  No, I know, I know.  Yeah.



STEVE:  Anyway, so if Topics were to succeed, and it got universal cross-browser adoption and industry endorsement, while Gorhill's uBlock Origin continues to block it, then we'll ask Gorhill for an easy-to-use exception switch, as he has done on his UI for other things, in order to turn it back on.  Or if he refuses to do that, then he just may feel like, okay, blocking it is our job, then I'm sure it's possible to tweak the rules in order to allow it.



LEO:  It may be moot.  Has he said anything about Manifest 3?  Because Google is planning to, I think, do what will essentially disable uBlock Origin in the long run.



STEVE:  Yup.  You're right.



LEO:  So it may be moot.  You may, you know, as I have said and you have said, might be better just to use Mozilla and have done with it.  I mean, that was Cory Doctorow's complaint is not so much Topics, but just that Google's attempt to really monopolize, they already monopolize online advertising, both as a buyer and seller.  And they would love to monopolize browsers, as well.  And that's, you know, that's a problem.



STEVE:  And I'm kind of feeling sad about this because I'm feeling less and less charitable toward Google.



LEO:  Yes, I feel sad about it, as well.  And I feel the same way.



STEVE:  You know?



LEO:  Yup.



STEVE:  When I put a search into Google, I used to get a clean page of links.



LEO:  Not anymore.



STEVE:  And now it's not that anymore.  



LEO:  Unh-unh.



STEVE:  It's really - it's really changed.  So The Real Veran Dontic, he said:  "Hi, Mr. Gibson.  I love Security Now!.  I noticed a slight logical flaw on SN-935 regarding the GPL.  The GPL only requires that you make available the source code of your modified GPL code with the distribution of that code.  Russian citizens could safely honor the GPL without actively contributing to open source."



Okay, now, first of all, it sounds like he's been listening to Ant with this Mr. Gibson business.  I appreciate the gesture of respect.  But please, everyone, we've been together for 18 years.  I'm just Steve.  And I do appreciate Veran's clarification about the GPL after I fumbled my mention of it last week.  But even so, assuming that Russia invests in modifying Linux to suit their own purposes and thus acquires an inherent feeling of propriety over "their result," who would imagine that Vladimir Putin's minions will have any interest in providing their citizens with the source code, any more than Microsoft does with Windows; or that they would make it available to anyone, including the Linux Project, in any form or fashion?  What benefit would Russia accrue from doing so?



So I'd be willing to bet that what we're going to see here is the Linux source forked and never shared again.  I would be surprised if anything else happened.  But anyway, you know, thank you, Veran, for the clarification.



Emma Sax tweeted:  "I just finished listening to your latest podcast, and I've been doing some thinking on your concerns about an email forwarding service, like Fastmail, just choosing to shut down.  I've come to the conclusion that that concern is no different from Gmail or Outlook shutting down their servers, which millions of people rely on.  At any point, any common email providers could just choose to not host email anymore, and tons of people would be forced to change their emails all over.



"The only way to get around it is by running your own email server, which you mention.  And most people aren't willing to go that far.  So for most people, we might as well just accept that we're relying on other peoples' servers for things.  And so just do your research to make sure you're choosing one that seems to be the most reliable to you, as in owned by a big company versus a single person, et cetera."



And thank you, Emma.  I think you're exactly right.  I think I'm sensitive to the fact that we do see services come and go, even from major providers.  Mozilla once offered a terrific file transfer service which they, too, shut down because it was being abused to transfer malware.  And we've seen several companies stop offering ephemeral credit card numbers, which was a useful service while it was there.  The difference with those services is that they don't incur the same level of persistent dependence that an email forwarding service does when it's used to anchor online account recovery.



So Emma's right.  I think that the optimal thing to do is to choose your email forwarding provider with your eyes open and with an awareness of the importance of any such service's continuing support and their existence going forward.  And if you ever receive any notice of any pending service discontinuation, don't wait to migrate your existing forwarded accounts to some other provider.  So, you know, just recognize the level and nature of that dependency.



AMSather, he said:  "Hey, Steve.  Topics sounds great, but what's to stop site operators from storing the Topics supplied to them and aggregating it to sell to advertisers?"  He said:  "Love the show.  Longtime listener and SpinRite 6 owner.  So happy you're not stopping after 999."



Okay.  This is one of the trickier bits of the Topics API.  And I do hope that its subtlety does not keep it from being appreciated.  Remember that brain twisting part about which topics a requestor could receive from a browser?  One of the things to appreciate about the browser's role in this - that is, in Topics - is just how much of the burden for enforcing the user's privacy has been placed on the browser.  Virtually all of it.  And because it's client-side, that's where its strength and privacy enforcement comes from.  The browser, with Topics, needs to keep track of all kinds of information for its user.  But fortunately, that's only needed on a moving four-week window, which is actually part of the clever tradeoff that's been made.  It does not need to keep it longer than four weeks.



So in order to obtain those three interest Topics from a user's browser when the user is visiting a website, the requestor - who would typically be an advertiser serving ads across the Internet - must have previously queried that user's browser at some site which the browser associates with each of the topics that would be returned to the requesting advertiser.  And I know it's just like this is hard to put into words, so here's, again, an example.



Three topics will be chosen, one for each of the past three weeks, based upon the domain the user is visiting because of that hash function which chooses one topic from among the top five during each of the three past weeks.  Okay.  So say that one of the three selected topics which would be returned to someone requesting them on a given site, an advertiser, say that one of the three topics is "Fish."  This would happen if the user's use of their web browser during the previous three weeks has temporarily taught their web browser, because of the web sites they've chosen to navigate to, that they currently have an interest in fish.  So "Fish" would be among their current top interests.



But at this time they are not at a website that their web browser associates with fish.  They're at a site that their web browser associates with gaming, for example.  But nonetheless, an advertiser on that gaming site is asking for the three chosen topics of interest to this user now.  And here's the crux:  "Fish" will only be returned as one of the three topics to that advertiser if sometime during the preceding three weeks that same advertiser queried for this user's three topics at a site which the user's browser associates with fish."



In other words, in order to be told that this user at this gaming site has an interest in fish, that same advertiser needs to have recently previously encountered this user's browser at a website that the browser associates with fish.  And unless that's true, "Fish" will be redacted.  It'll just be eliminated from the three topics that will be returned to the user, and nothing will be substituted in its place.  The advertiser just gets less information.  So anyway, there's just no easy way to explain this, but that's it again.



Okay.  But so what does that mean?  Because now back to our listener's question, what does that mean?  He said:  "What's to stop advertisers from storing the topics supplied to them and aggregating them to sell to advertisers?"  And that's part of the subtle beauty of this system.  Ask yourself what topics a browser will return to any static website.  Unlike advertisers, websites don't encounter their visitors' browsers at other websites.  Only third-party advertisers have that sort of cross-website reach.  Websites have a first-party relationship with their visitors.  So they never see them anywhere else.  That means that the tricky Topics filter will only ever return the same topics to a website that the browser associates with that website.  This means that websites are unable to learn anything they didn't already know about their visitors.



And of course note that this makes it completely different from FLoC, which was blabbing with that cryptic hashed token to every site someone visited, whether or not they'd ever even been there before.  Not so with Topics.  Again, I'm a little worried because it is complicated.  And, you know, the world has a hard time dealing with complicated things, especially when they're asked, you know, doesn't that sound great?  They go, uh, what's for dinner?  So anyway.  Let's hope.



Robert Gauld, G-A-U-L-D.  He said:  "@SGgrc In SN-935, I think consecutive characters refers to sequences as in ABC, def, 567, et cetera."



Okay.  So, yes.  Thanks to Robert and many other listeners.  Remember, Leo, that ridiculous set of password rules?  And we had a lot of fun because they said no two consecutive characters.  And I said, wait a minute.  How can you - or was it never more than two consecutive characters?  So how could you have a password greater than two characters if you can't have more than two that are consecutive.  Anyway, he, Robert, and many of our listeners said, you know, Steve, I think they meant sequential.



LEO:  Oh.  Yeah, but they didn't say sequential, so...



STEVE:  They did not say sequential.  So, you know, and there were several of those rules that were kind of fumbles.  Remember, like, one was a proper subset of the other, so you didn't even need that second one because the first one, you know, was a full superset of what the second one was.  I mean, that mess, that set of rules was a mess.  But anyway, I wanted to thank all of our listeners who said, you know, I think they didn't mean consecutive, they meant sequential.



LEO:  Yeah.  Okay.



STEVE:  And finally, yeah, Rob Mitchell said:  "You know what would be great?  If you posted your Picture of the Week for each episode here on Twitter.  You could include a link to each episode, so you'd also be doing some promotion, too.  I rarely take the time to find the photo of the week, but I'd surely find it here."



And Rob, your request, and actually I saw it several times, it must have been in the wind somehow this past week because no one ever mentioned it before.  But a bunch of people did this time, maybe because the pictures lately have been a little obscure, and they've required a great deal of explanation in order for me to do it over a non-visual medium.  Anyway, from now on, and I already did it earlier today, the podcast Picture of the Week will accompany the link to the show notes on Twitter.



LEO:  Now you're using Twitter right.



STEVE:  That's right.



LEO:  Just as they change their name to X.



STEVE:  Speaking of which, Leo, he took TweetDeck away from me.



LEO:  He did.



STEVE:  That [muttering].



LEO:  Yeah, people are very upset about that, yeah.



STEVE:  Oh, my god.  Using the normal interface is awful.



LEO:  Yeah.



STEVE:  No, TweetDeck was a godsend.  But on the other hand, I refuse to pay that SOB $89 a year or something?  I'm not paying for this.  So I'll tough it out with this awful user interface that is the web browser.



LEO:  Took me years to get you to use Twitter.  Now it's going to take me years to get you off it, I know.  But that's all right.  You don't like change.  I got it.



STEVE:  I still - I do not.  I do not like change.  My first wife's abbreviation for me was COH, and I was worried to ask her what that stood for.



LEO:  What's that stand for?



STEVE:  Fortunately it stood for Creature of Habit.



LEO:  Yes.  That's true.  That's true.



STEVE:  So I'm programming in assembly language, darn it.  And that's what I learned when I was four, and I'm sticking with it.  Anyway, our last break, and then we're going to talk about when heuristics backfire.



LEO:  Oh.  That's me.  Okay.  I'm thinking about COH.  I kind of am, too, come to think of it, actually.  There's nothing wrong with that.  You know, you know it well.



STEVE:  Yes.  I only wear - all of our neighbors only see Lorrie and me walking in black.



LEO:  Black.



STEVE:  And they often comment, "Why are you in all black?"  It's like, well...



LEO:  Easy.



STEVE:  It's very easy.



LEO:  It's always color-coordinated.



STEVE:  It hides the soy sauce.



LEO:  Now, if this were another show, I would have that be the show title, but okay.  All right.



STEVE:  Okay.  So Wikipedia defines the term "heuristic."  First of all, heuristic comes from the Ancient Greek heurisco, and there it meant "to find or discover."  So a heuristic technique is any approach, Wikipedia says, to problem-solving or self-discovery that employs a practical method that is not guaranteed to be optimal, perfect, or rational, but is nevertheless sufficient for reaching an immediate short-term goal or approximation.  Where finding an optimal solution is impossible or impractical, heuristic methods can be used to speed up the process of finding a satisfactory solution.  Heuristics can be mental shortcuts that ease the cognitive load of making a decision.  Examples that employ heuristics include using trial and error, a rule of thumb, or an educated guess.



Okay.  So before we all take some lessons from the implementation of a heuristic that has been causing Microsoft enterprise customers years of mysterious pain, I want to note that I'm a big fan of heuristic methods.  I often use heuristics in my own code when I'm dealing with uncertainties.  An example from the nearly finished SpinRite 6.1 code comes to mind.  Modern operating systems all incorporate the concept of device drivers because the operating system needs to be informed how to talk to the wide range of peripherals it might encounter.  For all SpinRites from v1.0 through 6.0, this was never an issue because SpinRite was able to access the system's mass storage hardware through the BIOS which functioned as an abstraction layer.  Essentially, it contained permanent built-in device drivers so that it knew how to talk to the hardware on its own motherboard.



The problem was that the BIOS APIs were designed 40 years ago.  And mass storage devices have evolved, not only in their capacity, but also in the rich metadata that they're able to offer to anyone who knows how to ask.  Unfortunately, the BIOS never did.  So to pull off what SpinRite 6.1 needed to do, it could no longer use the BIOS to serve as its intermediary.  It needed to talk to the motherboard hardware directly in order to gain direct access to all of the modern hardware capabilities.



But unlike all modern operating systems which are able to use a rich collection of modular device drivers to talk to the hardware, SpinRite needed to have what was essentially a single universal built-in driver that could figure out, for itself and on its own, how to talk to any hardware that it might encounter.  And we're talking about 40 years' worth, from long since disappeared manufacturers and so forth.  And that's where heuristics comes in.



While SpinRite is starting up, it spends its time literally getting to know the hardware it's running on by performing a series of experiments - in a classic heuristic process - to work out and learn exactly how the underlying hardware operates.  It tries things and learns things and keeps a record of what it has learned about every mass storage adapter and device in the system so that it then knows how to interface with each one.  There was an appreciable amount of time during these past three years where I wasn't 100% certain that I was going to be able to successfully weave a path through all of the hardware owned by myself and by SpinRite's now 759 individual testers.



But a path was found, and it's been quite a while since any hardware has stumped SpinRite.  It doesn't matter how old or new or from which manufacturer.  SpinRite 6.1 now has what is essentially a universal smart driver, driven by a heuristic learning process, that's able to interface with any IDE, ATA or AHCI mass storage hardware that has ever been built.  And one non-obvious feature of the inherent flexibility of this approach is that the benefit may not only be retrospective.  It's likely also prospective, giving SpinRite the ability to figure out things that not only came before it, but may also arrive after it.  



So that's an example of the application of heuristics.  One way to think of it is as code that figures things out, often working from an initial starting place of uncertainty.  The trouble is, being somewhat "rule of thumb" and "inexact," anyone deploying heuristic approaches needs to build in safeguards against their code drawing the wrong conclusions.  And this appears to be where a heuristic approach incorporated into Windows Servers by Microsoft back in 2016 - actually it's not just Windows Servers, it's from Windows 10 on, both desktop and server platforms - back in 2016 has fallen down, backfired, and caused unappreciated problems for the past seven years.



Last week, a Norwegian data center engineer named "Simen," spelled S-I-M-E-N, reached out to me via Twitter DM and explained that people who listen to this podcast said that I'd probably be interested in what he had discovered.  Given that it became today's topic, it's clear that our podcast listeners have come to know me pretty well.  The best way to introduce the seven-year-old problem that Simen uncovered would be to share Ars Technica's coverage of it, which Simen pointed me to in his direct message because it's about him.



So last Wednesday's piece in Ars Technica was titled "Windows feature that resets system clocks based on random data [ouch] is wreaking havoc," and the subtitle was "Windows Secure Time Seeding resets clocks months or years off the correct time."  Ars wrote:  "A few months ago, an engineer in a data center in Norway encountered some perplexing errors that caused a Windows server to suddenly reset its clock to 55 days in the future.  The engineer relied on the server to maintain a routing table that tracked cell phone numbers in real time as they moved from one carrier to the other.  A jump of eight weeks had dire consequences because it caused numbers that had yet to be transferred to be listed as having already been moved, and numbers that had already been transferred to be reported as pending.



"The engineer, who asked to be identified only by his first name, Simen, wrote in an email:  'With these updated routing tables, a lot of people were unable to make calls, as we didn't have the correct state.  We would route incoming and outgoing calls to the wrong operators.  This meant, for example, children could not reach their parents and vice versa.'"



Ars wrote:  "Simen had experienced a similar error last August when a machine running Windows Server 2019 reset its clock to January 2023 and then changed it back a short time later."



LEO:  Geez.



STEVE:  I know, Leo.  It gets worse.  "Troubleshooting the cause of that mysterious reset was hampered because the engineers didn't discover it until after event logs had been purged.  The newer jump of 55 days, on a machine running Windows Server 2016, prompted him to once again search for a cause, and this time he found it.



"The culprit was a little-known feature in Windows known as Secure Time Seeding.  Microsoft introduced the Secure Time Seeding timekeeping feature in 2016 as a way to ensure that system clocks were accurate.  Windows systems with clocks set to the wrong time can cause disastrous errors when they can't properly parse timestamps in digital certificates, or they execute jobs too early, too late, or out of prescribed order.  Secure Time Seeding, Microsoft said, was a hedge against failures in battery-powered onboard devices designed to keep accurate time even when the machine is powered down."  Right?  The old CMOS clock.  Remember that?  We still have those.



Microsoft engineers wrote - this is Microsoft, or Ars quoting Microsoft:  "'You may ask, why doesn't the device ask the nearest time server for the correct time over the network?  Since the server is not in a state to communicate securely over the network, it cannot obtain time securely over the network as well, unless you choose to ignore network security, or at least punch some holes into it by making exceptions.  To avoid making security exceptions, Secure Time Seeding sets the time based on data inside an SSL handshake the machine makes with remote servers.'"



Okay.  So when I read I thought, what?  Ars explains some things we know, but adds some new bits.  They wrote:  "These handshakes occur between two devices connecting using Secure Sockets Layer protocol, the mechanism that provides encrypted HTTPS sessions.  It's also known as Transport Layer Security," they say, as we know.  "Because Secure Time Seeding (STS) used SSL certificates in Windows already stored locally, it could ensure that the machine was securely connected to the remote server.  'The mechanism,' Microsoft engineers wrote, 'helped us to break the cyclical dependency between client system time and security keys, including SSL certificates.'"



Okay, now, backing up a little bit, Simen wasn't the only person encountering wild, spontaneous fluctuations in Windows system clocks used in mission-critical environments.  Sometime last year, a separate engineer named Ken began seeing similar time drifts.  They were limited to two or three servers and occurred every few months.  Sometimes, the clock times jumped by a matter of weeks.  Other times, the times changed to as late as the year - Leo, this will even be after us - the year 2159.



LEO:  That doesn't seem possible.



STEVE:  I know.  It should not be.



LEO:  That shouldn't happen.



STEVE:  So let's just pause here.



LEO:  So, oh, it sounds to me - let me see if I understand it.  It sounds to me - so you can't - this all makes sense so far, which is if I've lost contact with the outside world, I can't set my clock.  But I need to set my clock in order to get outside world access.  So I have some internal computers that I'm communicating with over SSL.  Yes?



STEVE:  Okay.



LEO:  Does SSL then say, hey, I think it's quarter past four?  What do you think?  It must be...



STEVE:  You're going to get to that.



LEO:  Okay.



STEVE:  There is a timestamp in SSL.



LEO:  That makes sense.  So I don't need, given that I've lost access to the Internet for the time being,  I don't need it to be right.  I just need to be roughly right.  So I'm going to presume that that SSL certificate isn't too far off.  So I think this - I understand the logic Microsoft had here.  So we're going to get an approximate time, which will then let us fix this problem maybe down the road.  But at least we'll be closer than if we're just making it up.  Right?



STEVE:  That's good, yes.



LEO:  Okay.



STEVE:  So unfortunately, and what you described is a useful heuristic with an exception.  And we'll see, I mean, like a bad exception.  But even so this is an example of a very poorly designed and thought out heuristic.  Now, we don't understand yet in detail what's going on.  No one does.  Microsoft is not telling us, and they don't share their source code.  So we don't know exactly what Windows is doing to obtain its time of day and date information.  But we don't need to yet.  The very fact that it's even possible, as you said, Leo, for a server last year in 2022 to believe that it's now 2159...



LEO:  Yeah, that should never happen.



STEVE:  ...137 years in the future conclusively demonstrates that the designer of this system left out a crucial concept which is very important in heuristic systems.  My own name for them is "Sanity Checks," and our listeners will have heard me refer to them from time to time because they're integral to creating robust systems.  As the name suggests, a sanity check is a "reasonability filter" that's applied once an answer is obtained from a heuristic system.  It prevents believing ridiculous or impossible things.



Okay.  So what sanity checks might be applied here?  Well, the Microsoft engineers were bragging about how they could rely upon the machine's locally stored TLS certificates to allow them to obtain a secure connection.  So would it be reasonable to expect those certificates to have valid expiration dates?



LEO:  Yeah.



STEVE:  Right?



LEO:  That'd be reasonable, yeah.



STEVE:  But 137 years in the future...



LEO:  [Crosstalk], couldn't be.



STEVE:  ...all of those certificates will have expired. 



LEO:  Yeah.



STEVE:  So that would appear to fail...



LEO:  There's problem number one, yeah.  Yeah.



STEVE:  ...the reasonability test.  You know?  And what operating Windows server would have all of its certificates expired well more than 100 years ago?  Or how about asking the local Windows Update service for the timestamp on the most recently received monthly update?



LEO:  There you go.



STEVE:  Is it reasonable to imagine that the machine has not had a security or feature update in 137 years?  Given Microsoft's track record for bugs?



LEO:  Maybe.  Maybe.



STEVE:  Uh, no.



LEO:  I have a theory in my mind what's happening is you've got two servers that are disconnected.  So the first one asks the second one, well, what time is it?  And he tells him.  And then the second one asks the first one what's the time.  And they're advancing each other in a loop till they get hundreds of years in the future.



STEVE:  What would be bad.



LEO:  Your sanity check would have stopped that; right?  Just don't keep going.  And if it gets too far, you've obviously gone too far.  I don't know, they don't - you say they don't know why this is happening.



STEVE:  We're going to get to it, Leo.



LEO:  Okay, then.  All right.



STEVE:  It's worse than you can imagine.



LEO:  Oh, dear.  Okay.



STEVE:  So Ken, who also wants to remain anonymous, wrote in an email, but he's been driven crazy by this:  "It has exponentially grown to be more and more servers that are affected by this.  In total, we have around 20 servers (VMs) that have experienced this, out of 5,000.  So it's not a huge amount, but it is considerable, especially considering the damage this does.  It usually happens to database servers.  When a database server jumps in time, it wreaks havoc," he wrote, "and the backup won't run either, as long as the server has such a huge offset in time.  For our customers, this is crucial."



Okay.  So gee, seems like the backup system is smart enough to say "Uhhh, something's wrong here."  But Windows itself appears to be blissfully ignorant.  Simen and Ken, who both asked to be identified only by their first names because they weren't authorized by their employers to speak on the record, soon found that engineers and administrators had been reporting the same time resets since 2016.  In 2017, for instance, a Reddit user in a sysadmin forum reported that some Windows 10 machines the user administered for a university were reporting inaccurate times, in some cases by as many as 31 hours in the past.



Okay.  So here again, another missed opportunity for a sanity check.  In our current reality, time stubbornly only moves forward.  It turns out that this really simplifies things.  But that means that discovering that many files in its file system are suddenly timestamped 31 hours in the future might give any well-designed heuristic algorithm pause.  In any event, Ars writes:  "The Reddit user eventually discovered that the time changes were correlated to a Windows registry key."  And then they note HKEY_LOCAL_MACHINE\SYSTEM\CurrentControlSet\Services\W32Time\SecureTimeLimits.



"Additional investigation showed that the time changes were also linked to errors that reported valid SSL certificates used by the university website were invalid when some people tried to access it."  Of course, yeah, because the machine's time was now off.  



The admin reached the following conclusion:  Windows 10 and 11 have a feature called Secure Time which is on by default.  It correlates timestamp metadata from SSL packets, exactly like you've been surmising, Leo, and compares them in time with the machine's local time.  Then what this guy wrote is:  "It processes these various times by means of 'black magic' and sets the system clock accordingly.  Unfortunately, this feature has the tendency to 'flip out' and set the system time to a random time in the past or the future.  The flip out might be caused by issues with SSL traffic," this person surmises.



Other examples of people reporting the same behavior date back to 2016, shortly after the rollout of this STS, and many more recent reports of harmful STS-induced time changes have been reported.  One Reddit user wrote:  "We've run into a show-stopping issue where time on a bunch of production systems jumped forward 17 hours.  If you've been in the game more than a week, you know the havoc this can cause."



Okay.  So what's going on?  To determine the current time, we know two things.  STS pulls two pieces of metadata contained in the SSL handshake.  It pulls the ServerUnixTime, which is a date and time we've been talking about already this podcast, obtained from the number of seconds that have elapsed since 00:00:00 UTC on January 1st, 1970.  The other is cryptographically signed data obtained from the remote server's SSL certificate showing whether it has been revoked under a mechanism we've discussed here at length also, OCSP, the Online Certificate Status Protocol.



So, finally, here's how Ars explains what it was told by Microsoft's engineers.  Ars wrote:  "Microsoft engineers said they used the ServerUnixTime data 'assuming it is somewhat accurate'" - exactly as you suggested, Leo - "but went on to acknowledge in the same sentence that it 'can also be incorrect.'  To prevent STS from resetting system clocks based on data provided by a single out-of-sync remote server, STS makes randomly interspersed SSL connections to multiple servers to arrive at a reliable range for the current time.



"The mechanism then merges the ServerUnixTime with the OCSP validity period to produce the smallest possible time range and assigns it a confidence score.  When the confidence score reaches a sufficiently high threshold, Windows classifies the data as STSHC, short for Secure Time Seed High Confidence.  The STSHC is then used to monitor system clocks for gross errors and correct them."



So what we've all just heard is a perfect textbook example of a heuristic algorithm.  It doesn't get any more heuristic than that.  Unfortunately, it is also apparently somehow prone to misfiring badly to become "highly confident" about very wrong times.  Ars says:  "Despite the checks and balances built into STS to ensure it provides accurate time estimates, the time jumps indicate the feature sometimes makes wild guesses that are off by days, weeks, months, or even years."  Yes, even 137 years.  



Ken wrote in his email:  "At this point, we're not completely sure why secure time seeding is doing this.  Being so seemingly random, it's difficult to understand.  Microsoft hasn't really been helpful in trying to track this, either."  He said:  "I've sent over logs and information, but they haven't really followed up on this.  They seem more interested in closing the case."



Simen, meanwhile, said that he's also reported the time resets to multiple groups within Microsoft.  When reporting the problems on Microsoft's feedback hub in May, he said he received no company response.  He then reported it through the Microsoft Security Response Center in June.  The submission was closed as a "non-MSRC case" with no elaboration.



Simen then tapped a third party specializing in Microsoft cloud security to act as an intermediary.  The intermediary relayed a response from Microsoft recommending STS be turned off when the server receives reliable timekeeping through the Network Time Protocol.  Meaning if NTP, which is what everything has always used, is fine, then use it, and turn off STS, which obviously is able to override that.



Simen wrote in an email:  "Unfortunately, this recommendation isn't publicly available, and it is still far from enough to stop the wrongly designed feature to keep wreaking havoc around the world."  Simen said he believes the STS design is based on a fundamental misinterpretation of the TLS specification.  Microsoft's description of STS acknowledges that some SSL implementations don't put the current system time of the server in the ServerUnixTime field at all.  And here it comes, Leo.  "Indeed, these implementations, most notably the widely used OpenSSL code library starting in 2014, populate the field with random data."



LEO:  Well, that explains it.  Oh, boy.



STEVE:  Wow.



LEO:  Well, there you have it.



STEVE:  So they're getting...



LEO:  Geez.



STEVE:  They're interpreting noise in incoming SSL connections, and every so often enough of the noise happens to line up that they believe a wrong date and jump ahead or forward.



LEO:  Because the heuristic will reject a lot of terrible timestamps.  It does have some sanity checking.  It's just...



STEVE:  Well, but again, what we're seeing it do is obviously wrong.



LEO:  Yeah.



STEVE:  So it's got some - it has its - basically it's believing its own PR.



LEO:  Always a mistake.



STEVE:  It's receiving nonsense, and every so often the nonsense, you know, the planets align, the nonsense looks like sense, and it goes, oh, my god, it's 137 years earlier than, or, you know, later than I thought.  And it jumps the clock forward.  That's insane.



LEO:  Yeah.  Well, they misinterpreted the TLS spec, that's all.  Or made assumptions about it that were incorrect.



STEVE:  And then failed to put a reasonability filter on the result of their heuristic.



LEO:  Well, but also, well, I guess, I mean, they maybe don't want to admit it because it might break the whole STS mechanism.  If you can't trust what you're getting back, this whole system might be a faulty system.



STEVE:  The whole system is a faulty system.



LEO:  Yeah.



STEVE:  Basically, this is someone's bad design from...



LEO:  Well, it seemed like a good idea.



STEVE:  That they're in love with from seven years ago.



LEO:  Right.  They can't let go of it.



STEVE:  And it is just effing things up, and it's causing real damage to their customers.  And Microsoft doesn't want to hear about it.  They like it.  You know, I mean, I'm sure it got elevated to someone who said, no, no, it works.  It's got heuristics, you know.  



LEO:  Well, what would be worse, not having any system for correcting the time, which means you never have any hope of getting a time from an NTP server maybe, or trying to get something?  They just need better heuristics; right?  That's really the...



STEVE:  Yes, this is bad.



LEO:  It's not bad in theory, but knowing now that they could get random data, they need a much better...



STEVE:  And Leo, they could have fixed it seven years ago.



LEO:  Yeah.  Well, I'm trying to think that maybe they thought, well, we can't abandon this.  It's better than nothing.  But obviously it's not better than nothing.  So, yeah.



STEVE:  No.  We had working systems before 2016.



LEO:  Right.



STEVE:  Before they did this.  Things were fine.  They weren't wreaking this kind of havoc.



LEO:  They need a better heuristic.



STEVE:  So Simen said:  "The false assumption is that most SSL implementations return the server time."



LEO:  Right.



STEVE:  "This was probably true in a Microsoft-only ecosystem back when they implemented it."



LEO:  Because there was no OpenSSL being run, yes, right.



STEVE:  "But at that time, when STS was introduced, OpenSSL was already sending some random data."  Okay.  So now...



LEO:  Maybe they're hoping to get the OpenSSL guys to fix their code.  Maybe they think it's their problem.



STEVE:  It's probably a privacy problem.  I mean, I imagine OpenSSL is randomizing this because first of all it doesn't, obviously it doesn't have to be right because OpenSSL works just fine.



LEO:  Right.



STEVE:  And it's probably leaking information about the exact time that the OpenSSL server thinks it is, and you could use that in order to, like, to do some deanonymization stuff.



So while official Microsoft talking points play down the unreliability of STS, Ryan Ries, whose LinkedIn profile indicates he is a senior Windows escalation engineer at Microsoft, wasn't as reticent when discussing STS on Twitter last year.  And I have in the show notes a copy of Ryan's posting on Twitter.  He says, and he's got a big beard, so you know you can believe him...



LEO:  It's got to be true, yeah.



STEVE:  Big gray beard.



LEO:  Yeah, yeah.



STEVE:  He says:  "Hey, people.  If you manage Active Directory domain controllers, I want to give you some UNOFFICIAL [that's in all caps] advice that is solely my person opinion:  Disable Secure Time Seeding for w32time on your domain controllers."



LEO:  Ha-hah.  Well, there you go.



STEVE:  There it is.  January 31st, 2022.  So that was - he was heading in to New Year's Eve, and he thought, you know, let's just give everybody a head's up for the New Year.



So someone named Brian Clark replied to Ryan's tweet.  He said:  "And why do you think we should do this?"  And Ryan says:  "Because it's just a matter of time - wink - before it bites you in the butt."



LEO:  Wow.  So they've known about this.



STEVE:  Yup.



LEO:  Made no attempt to fix it.



STEVE:  Nope.



LEO:  [Heavy sigh]



STEVE:  So again, this was a senior Windows escalation engineer at Microsoft.



LEO:  He'd clearly see it happen.  He'd seen it happen.  He knew.



STEVE:  Yup.  Of course.  Several hours after Ars - I think this is what's happened to Microsoft, Leo.  It has become too large and sprawling.  It's just no longer responsible for itself.  Anyway, several hours after Ars first posted this, a Microsoft representative emailed Ars the following statement.  This is from Microsoft:  "Secure Time Seeding feature is a heuristic-based method of timekeeping that also helps correct system time in case of certain software/firmware/hardware timekeeping failures.  The feature has been enabled by default in all default Windows configurations and has been shown to function as intended in default configurations.



"Time distribution is unique to each deployment, and customers often configure their machines to their particular needs.  Given the heuristic nature of Secure Time Seeding and the variety of possible deployments used by our customers, we have provided the ability to disable this feature if it does not suit your needs.  Our understanding is that there are likely unique, proprietary, complex factors in deployments where customers are experiencing Secure Time Seeding issues, and these customers do not benefit from this feature" - right - "as it is currently implemented."  Of course they're not going to change the implementation apparently.  "In these isolated cases, the only course of action we can recommend is to disable this feature in their deployments."  In other words...



LEO:  It's your fault for using OpenSSL is what they're saying, I think.



STEVE:  Yup.  For having it around.



LEO:  Yup.



STEVE:  Exactly.  And they said:  "We agree that the overall direction of technology with the adoption of TLS v1.3 and other deployments in this area could make Secure Time Seeding decreasingly effective over time."  What do you want to bet, Leo, that TLS 1.3 explicitly randomizes that field for the sake of privacy, and this is the way they're saying it.  "We agree that the overall direction of technology" - Jesus - "with the adoption of TLS 1.3 and other developments in this area could make Secure Time Seeding decreasingly effective over time."



LEO:  But that's not our fault.



STEVE:  That's right.  "But we are not aware of any bugs arising from their use."



LEO:  It would be possible to build in a sanity check that would at least prevent the most egregious...



STEVE:  Yes.  Yes.



LEO:  So they could fix this.



STEVE:  Leo, look at the recent Windows Update time.  When did the most recent Windows Updates occur?  Was it 137 years ago that you updated Windows?



LEO:  Yeah.  There's ways to look at this.



STEVE:  Unlikely.  Was it six months?  No.  It was, you know, within a month or two.  And you've got - you have file system activity.



LEO:  Yeah.



STEVE:  You know?  Is it reasonable that no files have been written in the last month on a domain controller?



LEO:  Well, can you query - there are things you could query about that; right?



STEVE:  Yes.



LEO:  Because we can presume that the machine I'm using is not accurate.



STEVE:  Time is littered, time is, you know, machines are littered with time clues.



LEO:  Interesting.  Seems like this is solvable.  But they may have said, you know, it's just going to get worse and worse.  And increasingly, especially with TLS 1.3, we're not going to get any real-time signals that we can trust.  So we just want people to turn it off.



STEVE:  So we're going to keep using them.



LEO:  Yeah, well, that's not the right answer, obviously.



STEVE:  So the well-known personality HD Moore, you know, the CTO and co-founder at runZero who also developed the Metasploit Project Foundation, the pen-testing suite, he speculated on Signal that the cause is some sort of logic bug in Microsoft's code.



LEO:  Yeah.



STEVE:  So HD Moore wrote:  "If OpenSSL has been setting random Unix times in TLS responses for a long period of time, but this bug is showing up infrequently, then it's likely harder to trigger than just forcing a bunch of outbound TLS connections to a server with bogus timestamp replies.  If it was that easy, it would happen far more frequently.  Either the STS logic relies on different root certificates as the signer, or some variety in the hostnames/IPs, or only triggers on certain flavors of random timestamp like values dividable by 1024 or something.  It smells like a logic bug that is triggered infrequently by fully random timestamps and likely just some subset of values and with some other conditions, like multiple requests in some period of time to multiple certs, et cetera."



And then Ars wraps up their coverage of this by writing:  "As the creator and lead developer of the Metasploit exploit framework, a pen tester, and chief security officer, Moore has a deep background in security.  He speculated that it might be possible for malicious actors to exploit STS to breach Windows systems that don't have STS turned off.  One possible exploit would work with an attack technique known as Server Side Request Forgery.



"Microsoft's repeated refusal to engage with customers experiencing these problems means that, for the foreseeable future, Windows will by default continue to automatically reset system clocks based on values that remote third parties include in SSL handshakes.  Further, it means that it will be incumbent on individual admins to manually turn off STS when it causes problems."  Of course by that time it's too late because it caused a problem, which could be really bad.



LEO:  Yeah.



STEVE:  "That in turn," they said, "is likely to keep fueling criticism that the feature as it has existed for the past seven years does more harm than good."



LEO:  Wow.



STEVE:  Simen wrote:  "STS is more like malware than an actual feature."



LEO:  Holy cow.



STEVE:  He said:  "I'm amazed that the developers didn't see it, that QA didn't see it, and that they even wrote about it publicly without anyone raising a red flag.  And that nobody at Microsoft has acted when being made aware of it."



LEO:  Yeah.



STEVE:  So, wow.  In the registry, HKEY_LOCAL_MACHINE\SYSTEM\

CurrentControlSet\Services\W32Time\Config contains a REG_DWORD value named UtilizeSslTimeData.  That makes it pretty clear.  And now that we know that the SSL time data that's present in OpenSSL's handshakes is random noise, that doesn't seem like a good thing to leave enabled.  In my Win10 machine, that value was set to one to enable this feature in Windows.  It is now set to zero, and anyone else who's concerned by this can set theirs to zero.  After rebooting Windows, it will rely upon your local machine's clock and the reliable Network Time Protocol to set it and keep it set correctly.  End of story.



LEO:  Well, well, well.



STEVE:  I started out observing heuristic rules of thumb approaches can be extremely valuable.  They make SpinRite 6.1 possible as it is.  But they need to be designed with care and protected from going badly wrong.  It appears that Microsoft has done neither of these things in a system that has already and may still damage their users.



LEO:  Jiminy Christmas.



STEVE:  And they don't care.



LEO:  It's a fascinating case study, though, in attempting to solve a problem with a very kind of clever hack, but not - kind of not doing it right, I guess.



STEVE:  Yeah.



LEO:  This is the answer.  And that's the problem with clever hacks.



STEVE:  I would have titled this podcast "You're doing it wrong."  



LEO:  You're doing it wrong.  Wow.  And seemed to be unwilling to fix it, which is even weirder.  Wow.  Very good...



STEVE:  You know?



LEO:  Really good case study, I think; you know?  I like this idea that you should start doing like a professor of law, you know, where you come up with a case study, and what do we learn from this.  I love this.  You could do more of that.  It's fun, it's really fun.



STEVE:  I know that our listeners do, too.



LEO:  Yeah, really good stuff.



STEVE:  So when I can, I certainly will.



LEO:  Yeah, thank you, Steve.  Steve Gibson is at GRC.com.  That's his website, the Gibson Research Corporation.  Well, you'll find many wonderful things there.  Of course SpinRite is his bread and butter, the world's best mass storage maintenance and recovery utility.  Perfect for your failing SanDisk SSD.



STEVE:  Bye.



Copyright (c) 2023 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#937

DATE:		August 29, 2023

TITLE:		The Man in the Middle

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-937.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we have a really wonderful picture of the week in the form of a techie "what we say" and "what we mean" counterpoint.  So we're going to start off spending a bit of time with that.  Then we're going to see whether updating to that latest WinRAR version might be more important than was clear last week.  And while HTTPS is important for the public Internet, do we need it for our local networks?  What about using our own portable domain for email?  Does Google's new Topics system unfairly favor monopolies?  If uBlock Origin blacks ads, why does it also need to block Topics?  Just how narrow (or wide) is Voyager 2's antenna beam, and what does 2 degrees off-axis really mean?  Do end users need to worry about that wacky Windows time setting mess?  And what's the whole story about Unix time in TLS handshakes?  What can be done about fake mass storage drives flooding the market?  And finally, let's look at man-in-the-middle attacks.  How practical are they, and what's been their history?



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  He's going to give you some more information about that WinRAR hack, how it works, and how to fix it.  We'll also talk a little bit about the Unix time story and TLS handshakes.  Do they really need it?  And do we really need HTTPS?  Sometimes it turns out we don't.  And then Steve's going to blow the lid off of an explosive story, hard drives that lie.  It's all coming up next on Security Now!. 



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 937, recorded Tuesday, August 29th, 2023:  The Man in the Middle.



It's time for Security Now!, the show where we cover your security, your privacy, your online life with this guy right here, Mr. Steven Gibson, our man of the hour.  Hi, Steve.



STEVE GIBSON:  Yo, Leo.  The last - watch out for that Super Glue.  Can you believe that it is the end of August?



LEO:  Yeah.



STEVE:  Wow.



LEO:  You know, I kind of can believe it because we have a guy here, one of our employees who's been delegated to put up holiday decorations.  



STEVE:  Oh.



LEO:  And when I came in this morning, it said it right on the desk:  It's fall.  And I said, it's not fall, it's August.	



STEVE:  Unfortunately, it's now fire season.



LEO:  Yeah.  It is a red - we got a red flag warning today in Northern California.



STEVE:  Yikes.



LEO:  Yikes is right.



STEVE:  So we're at Security Now! Episode 937, and we're going to go until we have five digits.  So nobody needs to worry.



LEO:  Woohoo.



STEVE:  About running out after three.



LEO:  Steve and I have committed to making it to the end of the Unix epoch.



STEVE:  That would be good, yeah.



LEO:  I like that idea, yeah.



STEVE:  I do.  So this week we have a really wonderful Picture of the Week in the form of a sort of a techie "what we say" and "what we mean" counterpoint.  So we're going to start off by spending a bit of time having fun with that.  Then we're going to see whether updating to the latest WinRAR version might be more important than was clear last week.  And while HTTPS is important for the public Internet, do we need it for our local networks?  What about using our own portable domain for email?  That's an idea.  Does Google's new Topics system unfairly favor monopolies?  And if so, how?  If uBlock Origin blocks ads, why does it also need to block Topics?



LEO:  Oh, hmm.



STEVE:  Yeah.  Just how narrow (or wide) is Voyager 2's antenna beam, and what does "2 degrees off-axis" really mean?  We have the math now, thanks to one of our listeners.



LEO:  Ooh.



STEVE:  Do end users need to worry about that wacky Windows time setting mess on their own desktops?  And what's the whole story about Unix time in TLS handshakes anyway?  What can be done about fake mass storage drives which are apparently now flooding the market?  And finally, let's look at man-in-the-middle attacks, thus the name of today's podcast.  How practical are they, and what's been their history?



LEO:  Interesting.  Interesting.  You know, you're doing something now that all successful podcasts, TV shows, radio shows do, which is... 



STEVE:  Not giving up.



LEO:  Not giving up.  Actually, you know what, that's number one, it really is.  Longevity, especially in radio, and I imagine in podcasting, makes a difference.  But calling back to previous episodes.  So we have a few topics this week that call back to previous episodes.  But I, you know, if you have missed those episodes, A, you should have listened to them.  Why didn't you listen to them?  But B, don't worry, we'll fill you in before we get too far.



STEVE:  That's right.  And if not, you'll have FOMO.



LEO:  FOMO.  No missing out, unh-unh.



STEVE:  What out for the FOMO.



LEO:  There are 937 episodes.  If you have not listened to all 937 at this point, you must go back and listen to them.  They're all available at TWiT.tv/sn or on Steve's site.



STEVE:  And if you don't know what we mean when we refer to the Portable Dog Killer...



LEO:  Oh, lord.



STEVE:  Oh, well.



LEO:  By the way, I forgot to ask you.  You sent me a wonderful fidget spinner when they were big, about four years ago.



STEVE:  Yeah.



LEO:  Do you still fidget?



STEVE:  Yeah, actually.  I have a...



LEO:  What do you use?



STEVE:  I have a little fidget ring right here.



LEO:  Oh.



STEVE:  Which is kind of nice because...



LEO:  Oh, it's got ball bearings in there, and you can spin it?



STEVE:  Yeah.  And so the outer ring spins.



LEO:  Oh.



STEVE:  It's kind of cool.



LEO:  Do you keep it on your finger and spin it that way?



STEVE:  Yeah, I do, yes, exactly.



LEO:  Oh, interesting.



STEVE:  Yeah.



LEO:  I do not.  But should I at some point have the urge to fidget, I have my spinner right here, just in case, in the drawer.



STEVE:  This is sort of related to the little tray at the top of my keyboard which is just full of little paraphernalia.  I don't know, you know, I just - it collects things. 



LEO:  Oh.  Oh.  You have - the keyboard has a tray, or just the keyboard holder has a tray?



STEVE:  The tray itself is sort of - it was the top of that Northgate keyboard.  It had a little tray [crosstalk]...



LEO:  For paperclips, thumbtacks, mints.



STEVE:  Oh, and boy, stuff finds its way in there and then it never leaves.



LEO:  Lint-covered mints.  I am prepared now for a rather lengthy Picture of the Week.



STEVE:  And I think worthwhile.  So, you know, it's referred to as "spin," right, where you sort of put the best face on something...



LEO:  Yes, yes.



STEVE:  ...when you have a need to do so.  So when we say a "horrible hack," what we mean is a horrible hack that I didn't write.



LEO:  Somebody else's horrible hack.



STEVE:  That's right.  But when it's a "temporary workaround," it's a horrible hack that I did write.



LEO:  Yes, yes.



STEVE:  So when we say "It's broken," then that means there are bugs in your code.  When we say, "Well, it has a few issues," then that means there are bugs in my code.



LEO:  Wow.



STEVE:  "Obscure" is "Someone else's code doesn't have comments."  "Self-documenting" is "My code doesn't need comments."  We have "That's why it's an awesome language" says someone.  Which actually means "It's my favorite language, and it's really easy to do something in it."  Or "You're thinking in the wrong mindset" is "It's my favorite language, and it's really hard to do something in it."  Like LISP.



LEO:  Like LISP, exactly, yes, yes.



STEVE:  That's right.  And then we have "I can read this Perl script," which actually means "I wrote this Perl script."



LEO:  And even then sometimes not, but okay, yes.



STEVE:  Oh, boy.



LEO:  I wrote this Perl script in the last week.  Let's make it that.



STEVE:  Say I recently wrote it, exactly.



LEO:  Recently, yes.



STEVE:  And then there's "I can't read this Perl script," which means "I didn't write this Perl script."



LEO:  Or I wrote it last week.



STEVE:  It's impossible to read anybody else's.



LEO:  Right.



STEVE:  Then we have, oh, that's "bad structure," which means "Someone else's code is badly organized."  Or "complex structure" is "My code is badly organized."  A "bug" is the absence of a feature I like.  "Out of scope" is the absence of a feature I don't like.



LEO:  Yes.



STEVE:  A "clean solution" is "It works, and I understand it."  Then "We need to rewrite it" is "It works, but I don't understand it."  Then "Emacs is better than Vi" means "It's too peaceful here, let's start a flame war."



LEO:  Yes.



STEVE:  And "Vi is better than Emacs" means "It's too peaceful here, let's start a flame war."  And then of course "IMHO" is the famous abbreviation which actually means "You're wrong."



LEO:  Yes.  In my humble opinion, you're full of it.



STEVE:  That's right.



LEO:  Yup, yup, yup.



STEVE:  Okay.  Then we have a couple that don't have two alternatives.  We have "legacy code," which actually means "It works, but no one knows how."  And then I love this, Leo.  We have CTRL+X, CTRL+C, lowercase quit, CTRL+\, [ESC] [ESC] CTRL+C.



LEO:  Oh, what's that?



STEVE:  Which actually means I don't know how to quit Vi.



LEO:  Yes.  Been there.  Done that.



STEVE:  Oh, god, so have I.



LEO:  Yes.



STEVE:  It's like, oh, how do I - what do I do here?



LEO:  Colon Q.  Oh, lord.



STEVE:  So what is a "suboptimal implementation" is actually "The worst errors ever inflicted on mankind."  And then "These test environments are too brittle" actually means "Works on my machine.  Have you tried rebooting?"



LEO:  Oh, lord.



STEVE:  And then we have "Proof-of-Concept" is "What I wrote."  "Perfect solution" is "How sales and marketing are promoting it."



LEO:  Very nice.  



STEVE:  So anyway...



LEO:  Where did you get that?  It's really the Table of the Week, John [crosstalk] pointed out.



STEVE:  That was the Table of the Week.  That was one of our great listeners who ran across it.  And he said:  "Steve, I saw this, and I thought of you."



LEO:  Nice.



STEVE:  And I said that immediately got promoted to the "must read" on the next episode of Security Now!.  I actually did have something else in place.  And it's like, no.  That'll go next week.  This one we have to play with.



LEO:  Well, it's funny because it's true.  I mean, it's all true.



STEVE:  Yes, yes.  As I said, spin is a thing; right?  And it's sort of the nature of politics in the tech world.



Okay.  So last week I mentioned the recent release of WinRAR 6.23, and I noted then that it fixed a pair of critical vulnerabilities that could have been used to execute code on its users' systems.  Well, then came the news that this pair of vulnerabilities had been discovered by bad guys and had been actively exploited at least as far back as last April.  Maybe sooner, and that wasn't seen.



BleepingComputer's headline last Wednesday, after last Tuesday's podcast, was "WinRAR zero-days exploited since April to hack trading accounts."  And since this really forms a cautionary tale which might catch any of us if we were to even briefly drop our guard, I want to share some of the details which BleepingComputer shared from Group-IB who were the ones who made the original discovery.  I've edited what BleepingComputer wrote for a bit for the podcast.



But basically what they said was:  "A WinRAR zero-day vulnerability tracked as CVE-2023-38831 was actively exploited to install malware when clicking on harmless files in an archive, allowing the hackers to breach online cryptocurrency trading accounts.  The vulnerability has been under active exploitation since April 2023, being used to distribute various malware families, including DarkMe, GuLoader, and the Remcos RAT, Remote Access Trojan."  And really that's got to be one of the best abbreviations or I guess acronyms that we've seen in a long time, calling it a RAT, which is, you know, it's perfect for Remote Access Trojan.



Anyway, "The WinRAR zero-day vulnerability," they said, "allowed the threat actors to create malicious .RAR and .ZIP archives containing innocuous files such as JPG images, text files, and PDF docs.  However, when a user opens a file, the flaw will cause a script to be executed that installs malware on the device.  BleepingComputer tested a malicious archive shared by Group-IB, who discovered the campaign, and simply double-clicking on a PDF caused a command script to be executed to install malware.



"The zero-day was fixed in WinRAR version 6.23, released on August 2nd, which also resolves several other security issues, including this flaw that can trigger command execution upon opening a specially crafted RAR file.  In a report released last Wednesday, researchers from Group-IB said they discovered the WinRAR zero-day being used to target cryptocurrency and stock trading forums, where the hackers pretended to be other enthusiasts sharing their trading strategies.



"Their forum posts contained links to specially crafted WinRAR ZIP or RAR archives that pretended to include the shared trading strategy, consisting of PDFs, text files, and images.  The fact that those archives target traders is demonstrated by the forum post titles, like 'Best Personal Strategy to Trade with Bitcoin.'  The malicious archives were distributed on at least eight public trading forums, infecting at least a confirmed 130 traders' devices.  The total number of victims and financial losses resulting from the campaign are unknown."



Anyway, so essentially there was a file that was innocent, innocuous, and a folder of the same name.  And when you opened the file, for reasons of this bug, which had been figured out by bad guys, a command script in the folder was executed and would cause malware to be installed on this victim machine.



LEO:  Oh.  That explains it.  Because, you know, we talk a lot about, you know, PDF, for instance.  Because it's an interpreter, it's really writing - it's code, and the PDF is actually code, and then the PDF reader is an interpreter, interpreting the code.  But it's been a long time since I wrote - I wrote a long time ago an ARC decoder.  But I remember it was mostly lookup tables.  It wasn't really doing interpretation.  So this makes sense.  It's not that the RAR file format is being interpreted, it's that you're including something hidden in the folder that then gets executed.



STEVE:  Yeah.  And so - exactly.  It was the thing in the folder that got executed, and that was unseen.  And there was a bug in RAR that allowed a specially crafted archive to cause the folder's contents to be executed.  And then what it did, even though you were clicking on the PDF, it didn't actually launch, it didn't open and launch the PDF, so the script did that so that the user thought that their action was actually successful, even though it was a script that ran that action for them in order to hide the fact of what it was doing in the background.



LEO:  That's an unusually nice finesse because usually they don't care.



STEVE:  Right.  They've already got you by the point.



LEO:  They got you, yeah, right.  That's nice.



STEVE:  Yeah.  Although they're wanting to get in there in order to suck someone's cryptocurrency trading out, so they probably do want to be on the DL in order to get that done.



LEO:  They don't want to make you suspicious, yeah.  Well, it makes a lot of sense to me.  I was wondering if WinRAR had some sort of weird interpreter built in, but no.  Okay.



STEVE:  So, okay, think about this.  WinRAR has been around forever.  It's a trusted and robust archiving utility.  I use it myself because it's highly configurable, much more so than ZIP, for example.  And when configured to use large compression block sizes and to generate a so-called "solid" archive, which is one that's not easily editable after the fact, it could achieve unmatched compression levels.  So naturally it's my choice.  So I'm trying to imagine what would happen if someone who appeared to be trustworthy were to post a link to a .RAR archive in a public forum.  You know, I guess my first thought would be to wonder why it wasn't a .ZIP, since that would be more common.  But I might assume that the guy was more techie and, you know, knew about archiving since RAR does outperform ZIP.



I would hope that I would have the presence of mind to drop anything like that first into VirusTotal for its quick analysis and evaluation.  And I'll just note for everyone here that it's possible to avoid even downloading a suspicious file first, since VirusTotal can be given a link to download and obtain and then analyze a file without us first needing to do so.



LEO:  Oh, that's nice.  I like that.



STEVE:  So that's an - yes.  So you're able to just like right-click on the link in the forum, copy the URL, go over to VirusTotal, and then copy that URL into VirusTotal.  It will download it and perform its analysis.  And presumably it would have exploded in red alerts and everything.



LEO:  Do we know that it would have discovered it?  I mean...



STEVE:  You know, it's able to open all of these archives and self-decompressing files.



LEO:  So it would see - it would see the malicious EXE.



STEVE:  Yeah.  It would have, yeah, it would have absolutely seen this remote access trojan and gone, you know, "Warning, Will Robinson."  Anyway, so in any event, I could imagine that I might open the RAR file since I trust WinRAR.  You know?  And that's really the crux of it; right?  In this instance my years-long trust of WinRAR would have been misplaced.  And if my guard was down, I might have been bitten.



So I'm not sure what lesson we take away from this.  You know, "Never trust nothin' and live in a cave" isn't a practical strategy.  But being unfailingly skeptical of anything being offered over the Internet by someone who you don't actually know probably is a practical strategy.  And in fact it's increasingly a survival strategy on the Internet.



And this points to the other mistake those victims made.  They didn't really know the person who was offering the download.  You know?  I mean, it was just someone - and that's the whole idea; right?  They wanted what the guy was offering.  They couldn't have really known them, or they would have never downloaded the file being offered, or at least they couldn't have had the requirement of really knowing them or they wouldn't have downloaded the file.  You know, that old adage of "never taking candy from a stranger," you know, turns out to be as useful for adults as well as it is children.  And here some guy who they didn't really know was saying, hey, here's my strategy, and boy, I'm getting rich on bitcoin.  So check this out.



So anyway, I suppose our takeaway is just to refresh our healthy skepticism of the Internet and to remember to try to never drop our guard.  All it takes, as we've often said, is one mistake to ruin our whole day.



So anyway, as it happens, I mentioned this update to WinRAR last week because I'm a WinRAR user, and there were two exploitable zero-days.  And then it turns out, yep, not only were they exploitable, but they had been actively exploited and were biting people.  So for what it's worth, you heard me mention that.  If you're also a WinRAR user, I would increase the priority of updating your copy to 6.23.



LEO:  It's such an old program.  Actually, it's modern, but it's old.  Does it automatically look for updates and update itself?  I bet it doesn't.



STEVE:  No.  No.



LEO:  No.  Okay.



STEVE:  And in fact, it's funny you mention that because when I saw that, I opened my copy of WinRAR and went poking around, you know, the Help and About and all that.  No opportunity to update itself.  There is a click to open the website, and that takes you then to...



LEO:  It's on you, man.  It's on you.



STEVE:  Yeah.



LEO:  Check the version number.



STEVE:  And you know, there is a vulnerability.  The opposite example of WinRAR is Notepad++.  And boy, I don't know what the author of that is doing, but apparently he has too much free time.



LEO:  Oh, a lot of updates? 



STEVE:  Because, oh, my god, every time I use it, it's like, oh, there's a new version.  It's like, no shit.  Of course there is.  It's been an hour.



LEO:  My Xbox does that.  I only play a game once every few weeks.  And invariably it'll say, well, I have to download a 14GB file to do that.  It's like, no, I don't - please.  Thank you.



STEVE:  Yeah.  Now, the good news is Notepad++ is elegant about updating.  It'll say I have to close Notepad in order to update.  It's like, oh, okay, fine.  So it closes it, and then it updates it, and then it says "You want to reopen it?"  It's like yes.  And Notepad++ does reopen the things that it last had open.  So it's not bad.  But the point is the more you do these little incremental updates, the more opportunity there is to make a mistake.



LEO:  True.



STEVE:  And, you know, if somebody were to infect his server and get a bad version in there, the whole world would suddenly have it.



LEO:  Man-in-the-middle attack, something like that?  Yeah.



STEVE:  So, yeah.



LEO:  But this is why I like Linux, and I like the Package Manager idea, which is that if you install everything with a Package Manager, all you have to do is periodically run your Package Manager, and it will update everything that it's installed.  That's a very nice - Windows is slowly moving in that direction with Winget.  But that's the way to do it, I think.



STEVE:  Yup.  Yup.  Okay.  So this is another of those episodes where I've recently received so many great feedback questions from our listeners that we basically have a listener-driven podcast.  But lots to say about some of the things that they brought up.



John May, he sent me an - is it an X that he sent me, Leo?  I hate that.  Maybe.  I got an X from John May.  Anyway, it's actually a tweet, but don't tell Elon.  He said:  "Steve, I've been watching SN since I retired from my IT job in 2020.  In this week's Episode 936" - so that was last week - he said, "you talked about HTTP going the way of the dodo, if Google has their way.  What about on private subnets that cannot be routed over the Internet?  Why pop a message or hinder access if the traffic is staying local?  I have many local devices I access via HTTP and don't want to add certificates.  Hopefully these will be exempt."  And he said:  "Glad 999 is not the end."



Okay.  So yes.  The reason we need the encryption and authentication that TLS provides to HTTPS on the Internet is that the flow of packet traffic between the endpoints is out of our control on public networks and potentially exposed to the whims of bad guys.  But that's not the case within a closed private network where the network components and all packet transit are local, typically kept within a single residence.  As our local devices such as routers, network attached storage, webcams, home assistants, and other IoT gadgets have become more capable, the traditional control panels, you know, filled with lights and buttons and switches, have been all been replaced by web pages published by a simple web server running in the device.



And this is the point that John May is making.  I'm sure many of us have had to fight with our web browsers when we wish to connect to a local router or other web interface over a simple HTTP connection.  As John May worries, Google appears to be poised to make this somewhat more difficult in the future, maybe to the extent of saying no more HTTP.  But I think that's probably going too far.  We'll see.



Now, one solution would be to set policy based upon the distinction between publicly routable and private non-routable network IPs.  To be clear, when we're talking about non-routable IPs, we're talking about the three networks defined in the early days of the Internet, much to the credit of its early designers because they had so much foresight, which were specified by RFC 1918, a low-numbered RFC from the beginning.  The most common that we all see when we're behind a consumer router is the address range that begins with 192.168.  Actually it's 192.168.0.0, extending through 192.168.255.255.  And that's a group of 65,536 IPs, though for consumer use we usually keep the third octet fixed at, typically at a 0 or a 1, and use the 256 IPs by changing the last, rightmost lower octet.



In addition to the 192.168/16 network, which has been defined by that RFC, it also specifies a private network which is 16 times larger than that one, which goes from 172.16 through 172.31.  And the third and final one is 16 times larger than that one, so it's 256 times larger than the 192.168 network, and that's the one that consists of all IPs beginning with 10-dot.  And all three of the other octets fall within that 10-dot network.



So yes.  Because none of those will be routed over the Internet, it would be tempting to have browsers willing to simply trust all connections to or from IPs within those three private ranges and allow HTTP connections there.  But doing so would mean that we trust every IP within that range.  In a small residential setting that probably makes sense.  But many large corporations also use these same large private network ranges internally for their Intranets.  And in such settings, access to raw network traffic is probably not well protected.  You know, there's like networking closets around where it's easy to tap into the network flow, so eavesdropping over those networks becomes feasible, and widely allowing HTTP across large corporate networks would probably be extending trust too far.



We typically don't have such security concerns within a small local network, but adopting a general browser policy of not requiring TLS connections for private networks might be too permissive since not all private networks are, as we know, fully trusted.  Fortunately, a middle ground that probably makes sense is the use of a self-signed certificate.



As we know, publicly-trusted certificates are signed by a certificate authority that the web browser already trusts.  So the browser inherently trusts any certificates that any of its already trusted certificate authorities have signed.  Which brings us to the question, who has signed those certificate authority certificates which it already trusts?  It turns out that in every case the certificate authority has signed its own certificate.  Certificate authority root certificates are self-signed, and they're trusted simply because they've been placed into the browser's root certificate store.



And in turn, that means that nothing prevents an appliance like a router or a NAS from creating and signing its own certificate.  When connecting over HTTPS with TLS, the first time a browser encounters a self-signed certificate, it will typically balk, complain, and say to its user, "Hey!  This guy is trying to pawn off a self-signed certificate which, naturally, wasn't signed by anyone I already trust.  What should I do?  Do you want me to trust it?"  To which the user can reply "Yes, trust this certificate from now on."  The procedure varies from browser to browser and by version, but basically the self-signed certificate gets added to the browser's internal store of trusted root certificates, and from that point on it will be possible to establish regular TLS/HTTPS connections with encryption and a limited level of authentication.



I say that it's a "limited level of authentication" since, as long as the device in question keeps the private key of the certificate it created to itself, no other device on the network, or anywhere else for that matter, can impersonate it.  The user's web browser will have stored and been told to trust the web serving device's matching public key.  That's what gets stored in the root store, the public key.  So if the user's connection were to ever be intercepted by some other device, there's no way for that other device to reuse the trusted device's public key since it would never have access to its matching private key.  So you do get a level of authentication even using a self-signed certificate, as long as the entity which created that self-signed certificate doesn't let go of its private key.  



Many years ago I used OpenSSL to create a self-signed certificate with an expiration date 100 years in the future.  And I did that because I didn't want to be hassled by the need to keep updating my own self-signed certificates...



LEO:  Yeah, that's fair.



STEVE:  ...you know, as it would expire.  And it worked great at the time.  I've not tried to do that recently, and I'm wondering, maybe somebody knows and will let me know, whether today's nanny-browsers would complain that the certificate's expiration date is too far in the future.  Remember that, you know - and maybe self-signed certificates are allowed to be exceptions.  But we know that web certs now can only be valid for up to 13 months in the future.  So maybe the day of creating a 100-year life self-signed cert is over.



In any event, for now we're able to tell our browsers to trust local HTTP connections.  So, you know, they're not happy about it, but they'll do it.  In Firefox, for example, I get a red slash across the padlock when I connect to my local ASUS router or my local Synology NAS.  But when I connect to my local pfSense firewall router, the little padlock shows an exclamation point.  And if I drill down, like click on the padlock in Firefox and then say, like, show me more, I get a popup.  And I'm looking, I put it in the show notes for anyone who's curious, and what I see is that pfSense created a self-signed cert that I had previously asked Firefox to trust.  And so it says the website is 192.168.0.1.  And so that IP is in the cert and has to match, in the same way that a normal certificate has a domain name which has to match.



LEO:  Is this screenshot from Windows XP?  It looks a little dated.



STEVE:  Now, Leo.



LEO:  Just a little bit dated.  I may be wrong, but...



STEVE:  Yeah, it's Windows 7.



LEO:  Okay.  It's nice, I like it.  I remember those days.



STEVE:  So anyway, and then it says "Verified by."  And the verified by where it would normally say DigiCert, for example, it says "pfSense webConfigurator Self-Signed Certificate."  Anyway, so - oh, and I clicked on it.  That self-signed certificate is valid from January 3rd of 2020 through January 25th of 2025.  So that's not bad.  That's four and a half years.  That's a nice range.  And that does suggest that self-signed certs are allowed to have a longer life than current certificate authority signed certificates, which are limited to 13 months.  Remember, that's that 397 days.



So anyway, I noted that Chrome was not happy with the pfSense self-signed certificate since I had not bothered to also tell Chrome to trust it.  But I don't really care because I don't use Chrome that much.  Anyway, there is a simple process for adding the certificate to Chrome's root trust store.



So anyway, I just kind of wanted to go over that for background.  I can't see any of the browsers removing the ability to add our own trust roots in the form of self-signed certificates.



LEO:  No, you have to.  You have to.



STEVE:  And it's something that you only need - yeah, exactly.  And it's something that you only need to do once for the life of that certificate.  So it would be nice...



LEO:  It is on you to go, whoa, this is self-signed because it's my device, as opposed to I'm out on the Internet and it's self-signed.



STEVE:  Right.



LEO:  My Synology lets me do either that or use Let's Encrypt.  And I actually would prefer to use Let's Encrypt, but I have to open a port for it and blah blah blah blah.  It's kind of annoying.



STEVE:  Exactly.  Exactly.  You have to have a public domain name on the Internet in order to do that.  So, yup.



LEO:  Which I do.  I do.  But then the script that it runs has to get out through that port and blah blah blah.



STEVE:  Yup, yup.



LEO:  So I just use the self-signed certificate.



STEVE:  Yeah.  Or you just say, you know, in fact for me I just said, yeah, I don't care if the Synology NAS has a red slash through the padlock because I'm happy to talk to it on HTTP.  I mean, I'm looking at it.  It's sitting right here next to me.  So no bad guys are in there.



Jack Skinner said:  "Steve, I was listening" - oh, you know, Leo, it's 36 minutes in.  Let's take a break.



LEO:  Let's take a break.  Good thinking.  Nice time management.  Well done.  Much better than mine.  I usually get to the ads three quarters of the way, and I go, "Oh, god, I've got a problem here."  Okay.  On we go.  Xeets.  You like xeets, by the way, X-E-E-T?  Xeet?  I xeeted it.



STEVE:  Yeah.  Jack Skinner said:  "Steve, I was listening to this week's podcast and had some feedback in regards to having third parties host your email versus running your own.  There's a third option.  Purchase your own domain name and use a third party that allows its use."



LEO:  Oh, I do that.  Right.  Of course.



STEVE:  It's brilliant.  That's like the...



LEO:  So if Fastmail went away, it's still going to my email address.  I just make it go somewhere else.



STEVE:  Exactly.  Yeah, duh.



LEO:  Oh, but wait.  If we're using the obfuscated address, that might not work because they have to translate it.  So what I do - which is why, by the way, I don't use those weird obfuscated addresses that 1Password and Bitwarden and others will let you use.  I just have a domain, I won't say what it is, and then I can use anything with Fastmail, anything I want, at that domain.



STEVE:  Multiple accounts, yes.



LEO:  Yeah.  So every single, when I signed up for Verizon, it was Verizon at this, I'll say it, laporte.email.  And so every time I sign up for something it's that person's name or that company's name at laporte.email.  So I do control that.  If Fastmail went away, it'd still work.



STEVE:  Exactly.  So he says:  "Then if the third-party host shuts down, you simply point your domain name to a new provider, and you have access to setup and use those email addresses again."



Anyway, yes, Jack, terrific idea.  It moves a user's email away from an end user's ISP who we know is probably hostile to having an email server running out of their home, I mean, you virtually can't do it any longer.  You know, but the user still maintains full control over their domain.  So, and I did a quick Google search.  There's a bunch of email providers.  Zoho is one.  Blue something is another.  Anyway, there's a bunch that will allow you to bring your own domain to them.  And then what you want is the ability to have multiple accounts.  And so you just create email accounts.



LEO:  Right.



STEVE:  And they also talk about, you know, having email routing services and so forth.  So anyway, yes, that's [crosstalk].



LEO:  And that's what Fastmail, our sponsor, does.  They do the DNS for at least a dozen of my domains.  So I can get email at any of those domains.  Whenever I set up a new website, I don't want to handle the email at that website.  I have Fastmail do it.  And so I can use the MX record to point to Fastmail, and the other record, the A record, to point to the website.



STEVE:  And being able to use your own name, I mean, it's like the reverse of AOL; right?



LEO:  Oh, yes.



STEVE:  It's a good domain instead of an embarrassing domain.



LEO:  Right.  No one at this point should ever be using a company's name as their domain for their email.  Even if using Gmail you should be using your Gibson.com or something.  You shouldn't be using Gmail.com. 



STEVE:  And also, Jack said, he said:  "Also, related to Google Topics.  With the restriction that an advertiser will only get a topic if they have first seen that user on a different website related to that topic, do you think that this will give Google added power to be a monopoly in this space?"  He said:  "It seems like sites will have the incentive to use a single advertiser."



So that's sort of an interesting point.  That actually had occurred to me before.  The fact that an advertiser is only able to obtain topics for a visitor at a given site when that advertiser has previously seen them at a site that's associated with the topics that were going to be provided, favors larger advertisers with greater reach since the user's browser will have encountered that advertiser at many more different websites, thus they'll be more likely to qualify for receiving topic information about that user.



Now, the answer to the question Jack asks at the end is unclear to me.  That is, he says, it seems like sites will have an incentive to use a single advertiser.  If websites are reimbursed by advertisers based on how well targeted their visitors are - which is to say, how many topics are being returned to the advertisers on their site - then larger advertisers having greater reach would have superior targeting.  If they were to reimburse more, then that site might choose to place more of their ads with them than with some advertiser who's paying less.  But that logic would appear to apply just as well to today's advertising climate as much as in tomorrow's Topics-based system, if that's what ends up surviving.



So anyway, it's unclear to me how Topics furthers a monopoly.  But it certainly does further the larger advertisers because the way Google has designed this, Topics are being filtered essentially based on the reach of an advertiser.  So it does sort of suggest we're going to see further consolidation and fewer larger advertisers on the Internet.  And I don't really have any basic sense for what the spread of advertiser size and reach and so forth is.  Are advertisers hugely geographically oriented, or oriented by country, or language, or what?



So anyway, it certainly is interesting that we have topics that we do.  And I forgot to even bring up the issue of language.  Obviously Topics would be language-independent because it's actually an index number into a list of topics.  And so they would be, you know, index numbers are language neutral.  And then the lists get translated into whatever language.



So anyway, Donn Edwards said:  "Hi, Steve.  If uBlock Origin is blocking all the ads on a website anyway, what difference does it make if Google Topics is on or off?"  He says:  "As far as I can tell, a typical website doesn't use the Topics API, only the sites that actually serve the ads.  So if no ads are served, no topics are registered with the browser.  Am I wrong?"



And anyway, so that's a good point.  I said, but - this gave me an opportunity to say at least the way I use uBlock Origin is to allow "Acceptable Ads."  And I've got that in quotes because that's like a term.  You know, I'm not, and we talked about this a long time ago, Leo, back in the Adblock Plus days, I'm not against seeing ads on websites if it helps them to generate revenue.  And as I've said, I'm not even against them knowing a little something about me, as long as it's not invasive of my privacy, if, again, it helps them to generate revenue.  I'm against having things flashing at me and screaming at me and jumping up and down, you know, in a futile attempt to get my attention.  That's annoying.



So there is this Acceptable Ads initiative which identifies advertisers who agree NOT to do any of that.  So it's not the case that as a uBlock Origin user I'm seeing no ads, I'm just not being driven into an epileptic seizure by the tame and considerate ads that I do see.  And for anyone who doesn't know about this, it's AcceptableAds.com is the site that sort of runs all of this.



And there was some controversy about larger advertisers being able to buy themselves into that list of acceptable ads.  But all I can tell you is that, when I look at somebody's web experience who's not doing this, it's like I can't believe they survive.  And whereas mine, I'm like unconscious of there being any ads.  They're just not, you know, in any way annoying to me.  So again, I'm happy to provide some support.  If putting the ad in front of me generates some revenue for the website, great.  It feels a bit like a cheat because I'm not clicking on any ads.  But fine.  I'm happy to pay the price.



LEO:  What brand of coffee do you buy?  What kind of laundry detergent do you use?



STEVE:  Yeah.  Fortunately...



LEO:  Brands work.  I hate to tell you.



STEVE:  Yeah.  I do, yeah, I...



LEO:  You may not click on the ads.



STEVE:  So you mean just sort of seeing it in front of you.



LEO:  Ads work.



STEVE:  And then you...



LEO:  Yeah.



STEVE:  Yeah.



LEO:  Yeah.  I mean, you don't buy a no-name laundry detergent.  You buy Tide.  Right?



STEVE:  Right.



LEO:  You don't go to Joe's Coffee Shop, you go to Starbucks.  So the ad doesn't have to work by getting you to click it.  I'm not a fan of ads either.  But in fact I think one of the reasons Netflix and others have done so well without ads is people are thrilled to watch a TV show, I loved watching "The Diplomat" without any ads.  Right?  And you can even see when you're watching these shows, it originally was on, what, FX?  You could see where the denouement happens, and they pause because it's time to sell Tide, and then they come back because there's no ad.



STEVE:  And in fact the ad is so disruptive that they sometimes go back a little bit in the show to remind you...



LEO:  Right, right, what you might have forgotten.



STEVE:  ...what you were watching, exactly.



LEO:  Yes.



STEVE:  If you were the first purchaser of a TiVo, I was second in line.



LEO:  Yeah, yeah, yeah.



STEVE:  Because I cannot watch live broadcast television.  It's just, it's impossible.



LEO:  All I will say, though, is you can't both not watch ads and complain about the subscription.  You've got to, somehow, this stuff has to be paid for.  It ain't free.  And, you know, I know that because TWiT doesn't exist in a vacuum.  We have to either join the Club for 7 bucks a month or watch ads.  I don't care.  You do whichever one you prefer.  But you've got to pay, I mean, Steve's not cheap, folks.  He's not that expensive, either, but, you know.  We've got to pay people.  We've got to pay the lights.



STEVE:  Okay.  So a bit of math here, which I loved.  Philip Le Riche, he said:  "Voyager 2's antenna does not produce a pencil beam.  The 3.7 meter dish at S-band, which is around 10cm wavelength, has a limited resolution of around the arctan of 0.1/3.7, or 1.5 degrees.  So who knows, that might be around 10dB down at 2 degrees off axis.  So not so incredible that they were able to boost the power to shout and get a command through.  To say its beam was off the earth by 5AU is therefore somewhat misleading."



So Philip, I love the math, thank you.  And that does explain why they even bothered to shout at Voyager 2.  I'm sure they knew exactly how attenuated their signal would be and thought that there was a chance that the message might get through anyway.  Anyway, very cool.  We have the best listeners, Leo.



Peter Chase said:  "Re the Windows time problem.  I use an ancient program called Dimension 4 which polls various time servers."



LEO:  Oh, wow.  There's a memory.  Wow.



STEVE:  Do you know Dimension 4?



LEO:  Yeah, oh, yeah.  Long time ago.



STEVE:  Didn't ring a bell for me.  He said:  "It polls various  time servers," he says, "in my case every 30 minutes, to keep my computers on time.  After listening, I went in and turned OFF 'Set Time Automatically' in Windows settings.  Was this a relevant thing to do, or does this issue just apply to servers?"



LEO:  Oh, interesting question, yeah.  Dimension 4 comes from a day when you didn't have NNTP, reliable NNTP servers you could get the time from.  Remember you had to set time manually in the old days, yeah.



STEVE:  Yeah.  Yeah, yeah, yeah.  And I remember like listening to WWV go tick tock, tick tock, tick tock.



LEO:  The time at the tone.  Yeah, yeah.



STEVE:  So, okay.  So it appears to be relevant, that is, this whole STS thing that we talked about last week, and we'll talk again about it in a second, to Windows 10 and on, and Server 2016 and on.  In other words, yes, I know that my own Win10 workstation did have the "UtilizeSslTimeData" value in its registry.  So Windows 10 Pro has this STS time setting.  However, this is different from "Set Time Automatically," which I believe should be left on.  It's right there in the Control Panel, says Set Time Automatically.  Windows normally references an Internet time server at time.windows.com.  So I think it makes sense to leave that feature enabled.



And when I opened my Control Panel under Windows 10, it showed when the last time that server was polled, and Windows made sure that it was still set correctly.  So I would not turn off Set Time Automatically. I would, if you're concerned about this, I would turn off that UtilizeSslTimeData in the registry.  It's a DWORD normally set to 1, set it to 0.



And SKYNET, who tweets from @fairlane32, he said:  "Steve, I'm curious.  Is the STS 'feature,'" he has in quotes, "only in Domain Controllers?"  And we know now that it's not, it's in all Win10 and both desktop and server variants.  He says:  "I have two servers running Server 2019 Standard, and they've never exhibited different clocks.  They're not Domain Controllers.  That said, I do have a bunch of Dell Optiplexes," and he says, "as you know."  He's participating over in the SpinRite development work.  And he said:  "...that communicate with one of those servers because of our time and print management software for the public access computers.  I have had in the past few weeks a few workstations with an error message during boot up that says 'WARNING, clock not set!' and giving me the option to hit F1 to try again or F2 to go into the BIOS setup to configure the clock.



"I go to the BIOS, and the clock is off.  Not by 137 years, mind you, but it's still off.  After resetting it and rebooting, it's fine.  Sometimes I don't even bother.  I just reboot the machine, and Windows boots fine, and the clock shows the correct time.  Is this behavior because of STS" - no - "or is the local CMOS chip getting, shall we say, tired and sleepy?"  Which is the case.  And he says:  "Thanks for the info."



Okay.  So motherboards generally have a very long-life battery, which keeps a very low power oscillator running 24/7 to keep track of the time and date.  But the batteries do die over time, and sometimes the clock is not read properly.  Actually, I recently struggled with this with SpinRite, since some motherboards disable hardware interrupts and cause time to be lost while SpinRite is running because SpinRite's calling into the BIOS when it's talking to USB devices, and they literally - time freezes from the perspective of all programs running outside of the BIOS.  Hardware interrupts are just off.  You know, the keyboard won't work.  Nothing happens.



So I needed to be reading the motherboard's real-time clock on the fly, and it turns out many are surprisingly flaky.  Part of the problem is that this is ancient technology, and the real-time clock uses what's known as a "ripple counter" instead of what's known as a synchronous counter.  With a ripple counter, that means that each digit overflows into the next higher digit so that when a digit wraps around from 9 back to 0, its carry ripples to the next digit to the left in the counter.  So it's possible to sample the count while it's in the middle of updating and "rippling," and as a result you'll get a bad time reading.  So for SpinRite I designed another heuristic algorithm to figure out what time it really was without being fooled by inaccurate values.  And so far it appears that mine was designed somewhat more carefully than Microsoft's, you know, that Microsoft did for Windows 10 and later, and for their server.



And Superevr is his tweet handle.  He said:  "Strangely, the TLS spec going back to the very first SSLv1, has a client hello that includes" - and he's talking about the handshake - "that includes the Unix time stamp, but acknowledges" - this is SSLv1.  "The spec acknowledges that:  'Clocks are not required to be set correctly.'  In 2013, someone drafted an RFC titled 'Deprecating gmt_unix_time in TLS.'"  And he includes a link.  He says:  "But it doesn't look like it ever left the draft phase.  The acknowledgement of its futility raises the question of why it's there at all."



Okay.  So this brings us back to last week's discussion of heuristics.  I left last week's podcast curious myself.  So I did a bit of googling and digging around.  Here's what I learned:  We've spoken of cryptographic protocols which require a nonce, a nonce standing for a number used once.  Typically, nonces do not need to be secret, but they do need to be unique.  If nonces are not unique, then in the case of TLS, this opens the opportunity for replay attacks.  This brings us to the question of how to produce a nonce that's guaranteed to be unique.



One way, which was suggested in that earliest SSL document, SSLv1, was to concatenate the Unix time, which as we know is a 32-bit binary count that changes once per second, to concatenate that 32-bit binary count, the Unix time, with another 32 bits of random noise.  Together they would produce a 64-bit nonce.  Each endpoint's 64-bit nonce would be concatenated with the other's, that is, that's what the handshakes are sharing, among other things they are each sharing their 64-bit nonce.  They each then concatenate them to produce a final shared 128-bit nonce, which is then used to drive the TLS crypto.



Now, when you think about it, if the requirement - and it's a firm requirement - is for an endpoint to never make the mistake of reusing a nonce, one way to do this would be to start with that 32-bit Unix time which is going to change once per second, then have a second 32-bit count which just increments once every new connection.  The only way for such a system to issue a duplicate nonce would be for that second 32-bit connection attempt counter to wrap around and return to a previous count before the 32-bit Unix time, which changes every second, had changed, which of course means that you'd have a 32-bit count counting from zero to 4.3 billion before it wraps back around to zero, do that within a second.  4.3 billion TLS handshakes for one machine in a second?  No.  That's not going to happen.



So that's a useful solution.  You're not going to get a repeated nonce just by taking Unix time, changing it once per second, and concatenating it with a connection attempt counter, which is 32 bits.  So it's going to have 4.3 values before it repeats.  A second will have elapsed before that 4.3 billion count has had a chance to wrap around.



Now, okay.  Somebody might suggest that using a simple counter to form the second half of that 64-bit nonce is a bad idea since it would make the system's nonce easily predictable.  Now, that's not a known problem for TLS; but, you know, known problems have a tendency to become - or unknown problems have a tendency to become known problems.  So there's a simple and secure solution.  Take a 32-bit cipher, using a randomly chosen key, and use that to encrypt a 32-bit counter to produce an unpredictable sequence of 32 bits that would therefore never repeat in less than 4.3 billion uses.



Okay.  Anyway.  But here's the point.  The only requirement for that 64-bit handshake nonce is that it never repeats.  That's it.  The use of Unix time has always merely been a suggestion in the TLS specification, not a requirement.



LEO:  Interesting.



STEVE:  It was just one way that, you know, this could be done.  Which is why the OpenSSL folks decided to never do it.



LEO:  Yeah.



STEVE:  Back in December of 2013, so nearly a decade ago, right, because here we're in '23, two guys, one with the Tor project and the other with Google, produced that TLS Working Group Internet-Draft standard with the title "Deprecating gmt_unix_time in TLS."  And the brief abstract, it's one line, or it's actually two lines, reads:  "This memo deprecates the use of gmt_unix_time field for sending the current time in all versions of the TLS protocol's handshake.  A rationale is provided for this decision, and alternatives are discussed."



Okay.  I'm just going to share the introduction that's three paragraphs, and it's interesting.  They wrote a decade ago:  "Current versions of the TLS protocol, dating back to the SSL 3.0, describe a gmt_unix_time field, sent in the clear, as part of the TLS handshake."  And we know that it actually goes way back to SSLv1, but they're only talking about what was then current.  "While the exact format of this field is not strictly specified, typical implementations fill it with the time since the Unix epoch (January 1, 1970) in seconds.  This practice," they write, "is neither necessary nor safe."



LEO:  Wow.



STEVE:  Yeah.  "According to RFC 2246, the TLS Protocol Version 1.0, gmt_unix_time holds 'The current time and date in standard Unix 32-bit format (seconds since the midnight starting January 1st, 1970, GMT) according to the sender's internal clock.'"  And they're still quoting from the spec.  "Clocks are not required to be set correctly by the basic TLS Protocol; higher level or application protocols may define additional requirements."  So they said:  "This text is retained unchanged in RFC 4346 and RFC 5246," meaning subsequent iterations of the TLS spec.



They said:  "The gmt_unix_time field was first introduced in SSL 3.0, the predecessor to TLS 1.0.  The field was meant to preserve the protocol's robustness in the presence of unreliable random number generators that might generate the same random values more than once.  If this happened, then SSL would be vulnerable to various attacks based on the non-uniqueness of the Random fields in the ClientHello and ServerHello messages.  Using the time value in this way was meant to prevent the same Random value from being sent more than once, even in the presence of misbehaved random number generators."



So, you know, we spent a lot of time in the early days of this podcast talking about bad random number generators and how easy and common, unfortunately, it was to have them.  So the idea was pair up a 32-bit random value that may not be that random with something that is incrementing once per second because that way, even if the random value comes back around before too long, at least the time will have changed so that concatenation, the 64-bit concatenation of those two 32-bit values will itself be unique.



So anyway, they go on to explain that, as I suspected and mentioned last week, one of the problems with using Unix time was that it could make clients identifiable by serving as a form of fingerprint.  But mostly they noted that the only reason for using Unix time was that it was a convenient means for changing the bits in half of the 64-bit nonce every second to protect against a repetition in the other 32 bits from a poor random number generator.



Okay.  So this brings us back to Microsoft.  The bottom line is that Microsoft incorrectly assumed that every incoming TLS connection was supposed to contain the 32-bit Unix time of the other machine that it was connecting with.  And now we know that's not true.  It's at best a suggestion and a convenience for the implementation.  The original spec even explicitly states that connecting machines are not required to have the correct time, and that's still in today's spec.  It's just meant to be an incrementing value.  I'm certain that Microsoft built in some sanity filtering of their own.  That's why it mostly works.  And then they have that "confidence level" heuristic that we talked about last week.



But the underlying assumption that handshakes will contain GMT time, especially when the massively popular OpenSSL doesn't, that's flawed.  And then, as we've seen, they're apparently shockingly stubborn about fixing this issue.  It's hurting their users, and they apparently refuse to care.  I wanted to close the loop on the question of TLS handshakes and Unix time, and now we know what's going on.



Okay, now, Leo, last week I mentioned that I had a tantalizing something else that I wanted to share.



LEO:  Yes.



STEVE:  About SpinRite.



LEO:  I've been on tenterhooks ever since.



STEVE:  So let's take our last break, and then I'm going to talk about something that we discovered which is really quite terrifying.



LEO:  Oh, no.



STEVE:  Oh, yes.



LEO:  That's a good tease.  Now, I'm on tenterhooks, Steven.  Tell me.



STEVE:  Okay.  So I mentioned last week that I had something else I wanted to share.  One of our very prolific testers, whose handle in our newsgroup and in GitLab is "millQ," that's just how we've always known him, he reported a month ago, on July 27th, that one of his oldest, actually his oldest, 256MB flash memory devices - megabyte flash memory devices - was showing up RED in SpinRite.  RED means that there's something SpinRite doesn't like about the drive.  None of his other USB flash drives had any trouble, just that one.



Now, one of the great things about GitLab for me is its issue tracking.  I'm desperate not to let any problem fall through any cracks, but I cannot be everywhere all the time.  As it happened, when millQ filed this issue, the gang was testing the latest SpinRite build while I was working on the Windows app which installs SpinRite onto USB drives, diskettes, and CDs to make them bootable.  So the DOS side wasn't where I was focused at the time.  But once the Windows utility was ready for its first testing, I turned the gang loose on it and switched back to SpinRite.



The error that SpinRite was reporting was that millQ's flash drive had failed SpinRite's transfer confidence testing.  Although SpinRite would allow the user to retry the test or even to push past the failure if they want, SpinRite couldn't vouch for how it was working with that drive.



Now, remember that I mentioned last week that SpinRite now has an extremely flexible driver that learns about the environment it's in.  In order to make absolutely positively certain that the driver has settled upon the right parameters for a drive and controller, it performs a series of confidence tests on each drive it encounters.



It first fills a nine-sector buffer with pseudorandom noise and then copies it to a secondary buffer.  It then goes to the end of the drive to read the drive's last nine sectors into that first buffer, which should overwrite all nine sectors of that pseudorandom data that was preloaded there.  It then compares the two buffers to verify that none of the sectors match.  That assures us, and actually it assures SpinRite, that reading from the media into RAM is actually happening because the read will have changed all nine sectors of the read buffer, which will then no longer match the noise that was preloaded there.



It then takes the nine sectors that it read and copies them into the secondary buffer.  It inverts the primary buffer and writes the nine sectors of inverted data back to the drive.  It then refills the buffer with new pseudorandom data, then rereads the data which it just wrote, which should now be inverted, if it was actually written, over the buffer.  So it then reinverts the buffer then compares all nine sectors with the original drive data that was first read and saved.  And it verifies now that all nine sectors match between the two buffers.  And, finally, it rewrites the originally read data back to the drive, restoring it to its original contents.



Okay.  Now, SpinRite goes through those gymnastics to absolutely and positively confirm that it's able to properly both read and write to the drive across multiple sectors.  Those tests and comparisons will fail unless everything is working.  There are a number of other things that SpinRite does after that, but it was that confidence testing that millQ's flash drive was failing.  And what's significant is the drive wasn't complaining.  The drive was saying everything was fine.



Now, several years ago I might have doubted SpinRite more than the drive.  But we now have 764 registered users in GitLab, so SpinRite has been quite well pounded on by this point.  And we don't see drives failing this.  This stood out.  It was completely unusual.  I should note that read and write errors reported by the drive are fine if they occur at the time.  If the drive says that it's unable to successfully read or write, that's no problem since that's not what we're testing for here. So SpinRite will simply move nine sectors over toward the front of the drive and try again.  And this entire process is tolerant of storage errors.  Again, that's not what we're looking for.



So here's the "gotcha" of millQ's drive:  It was failing this test and was not reporting any errors.  SpinRite told it to read nine sectors, and it said it did.  And SpinRite confirmed that nine sectors had indeed been read.  Then SpinRite told it to write nine sectors, and it said it did.  But when SpinRite asked it to read those nine sectors back, they were not the nine sectors that had just been written.  This drive was accepting read commands and write commands and apparently ignoring the write commands.



Since SpinRite now has the ability to check an entire drive this way, millQ started SpinRite at its new Level 5 which reads, inverts, writes, reads and verifies, then reinverts, writes, then rereads and verifies.  What millQ discovered was that at exactly the halfway point, the drive silently stopped writing.  It also sped up considerably, which was interesting.



LEO:  Well, it wasn't doing anything else.



STEVE:  Exactly.  Since writing to flash is much slower than reading.  So millQ's drive was labeled 256MB.  And a query for the drive's size, which is what SpinRite did, declared itself to be 256MB, but it was only storing 128MB.  And here's the real gotcha:  No operating system would detect this.  Operating systems assume that, unless a drive reports an error when writing data, that data was properly written.  And modern drives take on a lot of responsibility for making sure that that statement is true.  What was diabolical was that the drive appeared to be functioning perfectly.  It was formatted with a FAT file system, and millQ had used it to successfully store and retrieve a great deal of data through the years.  But perhaps he'd never stored more than 128MB of data there.



Now, when this was discussed with GRC's spinrite.dev newsgroup, many people there, some who have a deep background in data recovery, were not the least bit surprised because it turns out that - and this is why I'm bringing this up - there is an epidemic of fake drives flooding the retail market.  I suppose in retrospect we should not be surprised. But I wanted to make sure all of our listeners were aware.



Since the FAT file system, which is the default file system for all such drives because it's universal, the FAT file system places all of its file system metadata at the front of the drive.  You know, the famous FAT, you know, stands for File Allocation Table, which is what gives the file system its name, and the root directory and so forth are all written at the front of the drive.



A 16MB compact flash card might have its firmware tweaked to lie about its size, then be relabeled and sold as a 512MB compact flash card.  Such a card will format without any trouble, and it will appear to be working perfectly.  Then a wedding photographer sticks this brand new card into their camera and spends Saturday taking photos before, during, and after a wedding, only to later discover that although a directory listing shows that all of the photos taken are there, the actual content of those files is not.  And those precious memories are irretrievably lost forever.



This, it turns out, is a surprisingly common occurrence.  Just google "fake ssd" or "fake thumb drive" and be prepared to get bored scrolling the screen.  It is endless.  When this came up last week I jumped onto Amazon and purchased the cheapest 2TB thumb drive they had.  It's silver and beautiful.  I have a picture of it in the show notes.  It's got that blue USB tongue to indicate USB 3.0.  And that 2TB cost me $26.88.



LEO:  And that's when your troubles began.



STEVE:  Exactly.  Exactly.



LEO:  By the way, you said in retail, but I think Amazon is probably the source of 99% of this crap.



STEVE:  And eBay.  Amazon and eBay.



LEO:  Oh, yeah, eBay.  And I guess, you know, Alibaba, if you've bought stuff there.



STEVE:  Yup, Alibaba and AliExpress, exactly.  So this thing arrived last Friday.  I plugged it into SpinRite, and the screen immediately turned red.  That drive immediately failed SpinRite's end-of-drive data storage test.  I was curious about what it was doing; so I stepped through the writing and reading and writing process at the end of the drive, and its behavior was really bizarre.  It seemed to sometimes write, but not reliably.  And as I kept poking at it, it suddenly stopped failing, and it began working correctly - at least for those nine sectors at the end.  You know, were some bad spots remapped so they're now good?  Maybe.



But remember we're talking about, if you want to believe this, 16 trillion bits of storage, that's right, you know 2TB, right, bytes.  So eight bits in a byte, 16 trillion bits of storage in a tiny sliver of metal-enclosed plastic.  I'm skeptical that such a drive contains much wear-leveling technology, or much technology at all.  I'm actually skeptical that it contains 16 trillion bits, but we'll get to that in a moment.



What I know is that it was not initially working, and that a bit of exercising it, you know, it sort of began to appear to work, at least at the end of the storage region.  Who knows if other "unexercised" areas of the drive function similarly.  Relying upon this brand new drive from Amazon would be a disaster.  We know that.  And remember, writing to it and reading from it produced no errors.  That is, it just said, yeah, fine, got the data, move along, everything's working.



LEO:  That's what's scary, yeah.



STEVE:  Yes.  So there's no indication that it's not storing everything, when in fact it may not be storing anything.  Now, one possibility is that not all flash storage is created equal. Chips are manufactured in large wafers, then cut into individual pieces and tested.  So there's some manufacturing testing that's done to qualify each chip for sale.  What happens to chips that fail to make the grade?  You know, say that they don't get an F, but perhaps they get a D-.  You know, you have to imagine that there's a market for chips that fail to make the grade.  Someone will buy them and package them for sale on Amazon or eBay or AliExpress or Alibaba.  It looks like memory.  It just isn't very good or reliable.  And maybe it isn't memory at all.



But I think this is a relatively new phenomenon that has arisen in the last few years.  I'm inclined to believe that millQ's drive, since it is so very old, may have just failed.  It probably once was a 256MB drive, and the chip that was providing the second half of its storage died.  But there is absolutely no doubt that fake mass storage is a very real problem today.  And Leo, I have another picture in the show notes, if you scroll down a bit.  This was sold as a high-end SSD.  For those who can't see the picture...



LEO:  Are those matchsticks in there?  What the hell?  What is this?



STEVE:  So that is an SSD case.  It was sold as a high-end SSD.  That's ballast to give it some weight.



LEO:  Oh, my god.  And then it's all held together with glue stick.



STEVE:  With hot glue.  



LEO:  Oh, hot glue.



STEVE:  Yup.  And so that's a thumb drive that was stuck into a little - to a USB-to-USB adapter.  They even ran wires from the light of the back of the thumb drive over around to an LED...



LEO:  Well, at least they took the extra time.



STEVE:  Exactly.  So that this thing would flash.



LEO:  Oh, my god.



STEVE:  And if you didn't know, I mean, so somebody purchased this, believing that it was an SSD with all of the endurance and longevity that you would...



LEO:  I love it.  To make it feel heavy they've got a couple little nuts in there, and I don't know what those copper things are, just some crap lying around the shop, just to give it some weight, some heft.



STEVE:  Yep, yep.



LEO:  Oh, my god.



STEVE:  So it doesn't feel empty.



LEO:  And then they glued it down so it wouldn't rattle.



STEVE:  Exactly.



LEO:  Wow.  This is awful.



STEVE:  This is what is out there, Leo.



LEO:  So this could be a big moneymaker for you if SpinRite could somehow - because, you know, obviously the drive controllers are trusting the drives, and the drives are lying. 



STEVE:  Yes.



LEO:  It would be really valuable if SpinRite could actually check and see what the capacity of a drive was.



STEVE:  So the first question is, how can we tell if a solid-state memory is fake?  Depending upon the cleverness of the memory controller, which is to say, to which lengths its perpetrator goes, there's actually only one way to know for sure, which is to simultaneously have every sector of the memory committed and in use, and then verified.  You'd fill the entire memory with deterministic pseudorandom data.



LEO:  That's not ideal.  That's going to take time and wear it out.



STEVE:  Yes.  Well, yes, one pass of writing, but you're right, it's not ideal.  This is done to prevent data compression which is apparently another trick...



LEO:  Oh, god.



STEVE:  ...which is being employed by some cheaters.  So the amount of data you can store is a function of what you store because they're compressing the data.  That's also been seen.  So once it's filled in this way, the entire memory would then be read to verify that it still contains the deterministic pseudorandom data.  This is the capability and the capacity that the memory claims it offers, and of course it's what you believe you've purchased.  Anything less is a cheat.  And there's actually no way to fake-out that test.



Now, if you really want to know for sure, it's necessary to do this because a much smaller memory could be arranged as an MRU, a Most Recently Used cache, so that if you moved through the memory, reading and writing it piecemeal, that would be fooled, even though the entire memory could not be used at once because it didn't exist.



Okay, now, Leo, I'm right where you were.  Performing this true full test is beyond the scope of SpinRite 6.1.  But now that I know this is happening out in the world, SpinRite 7 is going to incorporate this full pseudorandom fill-and-verify technology because this is the sort of ultimate assurance that SpinRite should be able to offer.



Now, having said that, in today's reality, it seems unlikely that scammers are going to go to the trouble of engineering an MRU cache to fake out lesser tests.  As it is today, SpinRite 6.1 will instantly disqualify any fake storage that places its reduced memory at the front of the drive.  Now, it could place some at the front and some at the end.  And then SpinRite, you know, wouldn't detect that; 7 will.  So - I forgot what I was going to say.  Oh.  So 6.1 would instantly detect it if there's no storage at the end, just as it did with millQ's drive.  And since this has been a recognized problem for a while, a handful of freeware utilities have been created to detect fake solid state storage.  I've not looked at them, so I can't vouch for their operation or behavior.  But they do exist, and they're easy to find.



So I don't know what I purchased from Amazon for $26.88.  It looks all shiny and new, but no way can that be trusted to store data.  I have a feeling that I'm going to run a little test of my own in the background to fill that drive with serialized data and see if it, like, actually can be read back after it's been written.  We'll see.  But we do know that it was not generating an error when I was telling it to write something, and it said it did, when in fact it didn't.



LEO:  This is really problematic because there's really no way to tell what you're getting.



STEVE:  Nope.



LEO:  Unless you open - I guess you could open it up.  But even then, it wouldn't necessarily tell you anything.



STEVE:  Nope.



LEO:  I bought a 10TB drive a while ago, and for the longest time I couldn't figure out why my Synology wasn't working.  And I realized, well, this drive is not doing 10TB, not doing what it said it was going to do.  And I wonder if that was also a scam drive.  Brand name.  I think it was probably Western Digital Red.  But who knows, it could have been, you know, it was bought on Amazon.  Some guy could have taken a label and stuck it on.



STEVE:  Well, and remember we once talked about and showed on the show a Seagate drive where the word "Seagate" was misspelled.



LEO:  Misspelled, yeah.



STEVE:  Because, you know, it looked close enough.  So unless you looked very closely, you didn't realize.



LEO:  I know you're going to put this in 7.  Couldn't you just write a little utility?  Just, you know, like Steve's little utilities that just fill the drive up, just said yeah, I see, I'm able to write a terabyte worth of stuff here.



STEVE:  Well, in order to do it right, it needs to be fast because these drives are big.



LEO:  Yeah, it's a lot of data.



STEVE:  And then it needs to follow that fill with a complete trimming of the drive.



LEO:  Right.



STEVE:  To formally release all the storage and show that it's no longer in use.



LEO:  This is a real problem.



STEVE:  It is a real problem.



LEO:  Golly.



STEVE:  Yeah.



LEO:  Oh, well.  What are you going to do with that 2TB thumb drive?  I'll take it off your hands if you don't want it.  I can use the storage.  Oh, lord.



STEVE:  Wow, I know.



LEO:  All right.  Man-in-the-middle time.



STEVE:  And finally.  Today's topic, The Man in the Middle, was inspired from another question from one of our listeners, Jon David Schober.  He said:  "Hey, Steve.  Quick question regarding insecure HTTP traffic.  Even if the site has no user interaction, like logins, wouldn't it being HTTP still put the user at risk for malicious injection of stuff like JavaScript miners, or overriding downloads to include malware?  Seems like something that HTTPS would secure, even if the site were just read-only."



Okay, now, that's certainly a great point, and many of our listeners wrote to say the same thing.  So I chose Jon's at random to represent all those who asked essentially the same question.  And everyone is correct in their observation that the lack of authentication and encryption would mean that if someone could arrange to place themselves in a man-in-the-middle position and intercept traffic flow in an intelligent fashion, then an unsecured conversation could be altered.



So I wanted to take a moment to expand upon this, to first establish a bit of perspective and to also examine the practicality of man-in-the-middle attacks which are, I think, far more easily said than done.  It's their practical difficulty that explains how we survived in a mostly unsecured Internet until Let's Encrypt came along relatively recently.  



We should remember that the lack of persistent encryption was the way most of the Internet worked until only very recently, and things mostly worked just fine without everything encrypted all the time.  As those of us who've been around for a while will recall, it was once the case that encryption was only employed on most websites when a login form was being delivered, and its user's name and password contents were being submitted.  The sense was that it was only during that brief interchange where eavesdropping really needed to be prevented.  I'll be the first to say that was probably never really true.  But at all other times, most websites were using more economical HTTP connections whose traffic was not safe from even eavesdropping.



Our long-term listeners will recall "Firesheep."  That was a Firefox add-on which provided a convenient user-interface for hijacking other people's logged-in session cookies, and thus their logged-in sessions.  As I reported at the time, I tried it at my local Starbucks.  And that is a brand we all recognize.  I never impersonated anyone, of course.  But the Firesheep web user interface quickly populated with all of the logged-on accounts of the other patrons with whom I was sharing free WiFi.  And all I would have had to do was click on one of those icons to be logged into the websites they were using as them.  It was a sobering experience.



Now, today, thanks to the pervasive use of TLS and HTTPS, we have communications privacy so that scenario cannot happen.  So it's definitely a good thing that we have more security today than we did in the past.  No question about it.  But the use of HTTPS and TLS is also, let's remember, not an absolute guarantee.  I'll remind our listeners of something that we covered at the time.  A few years ago, the headline of a story in BleepingComputer read:  "14,766 Let's Encrypt SSL Certificates Issued to PayPal Phishing Sites."  14,766.  Every one of those connections to those 14,766 - sounds like the number of votes I need - fake PayPal sites...



LEO:  I love it.



STEVE:  ...was authenticated, encrypted, super secure, immune to any man-in-the-middle interception, and malicious.  So, you know, just the fact that we have the padlock glowing and happy doesn't mean we're okay.  We know that the reason our web browsers have been gradually downplaying the increasingly pervasive use of the HTTPS that they've been driving, which boasts its padlock, is to not give their users a false sense of security just because there's a padlock, and all is well.  Well, thanks to Let's Encrypt, that may not be the case.



So how was it that we survived as long and as well as we did before most websites had their communications encrypted all the time?  The answer is, there is a world of practical difference between passive eavesdropping, which is trivial, you know, you can do it in your local Starbucks - or could - and active interception.  Eavesdropping, as in the Firesheep example, is trivial, whereas active traffic interception is not something that's practically available to common malicious hackers.  And that explains how the pre-encrypted Internet managed to survive until we got it encrypted.



I've often talked about practical traffic interception.  Once upon a time, antiviral content scanners were installed at the network borders of corporations to maximize and filter all of the traffic passing through on the fly.  Encrypted connections, however, posed a problem.  So the solution was to create "middleboxes," which would fully proxy such connections by intercepting and terminating the TLS handshakes at the box, then establish a second connection to their user's browser within the organization.  Since the traffic-intercepting middlebox needed to create certificates to stand in for those remote websites it was proxying, they needed to have some means for browsers to trust the certificates they were creating and presenting.



The controversial and underhanded way this was done for a while was by obtaining a certificate from an already trusted certificate authority where that certificate itself had bits set in it which permitted it to sign other certificates.  In that way it was possible for the middlebox to transparently create certificates on the fly as if it were a trusted authority.  The industry quickly clamped down on that practice since allowing random hardware to create globally trusted certificates was extremely dangerous and prone to abuse.  So today, any TLS proxying middlebox that wishes to create trusted certificates on behalf of other websites can only do so by installing, you guessed it, its own self-signed root certificate into every browser's trusted root store.



LEO:  Nice callback, Steve.



STEVE:  This essentially makes it a trusted certificate authority, but only within the organization that's behind it, being protected by it, and whose browsers have installed its certificate.



So both in the earlier unencrypted days and today, when nearly everything is encrypted, it's possible and practical for hardware to be installed into a network which allows for traffic interception; filtering; and, potentially, alteration.  But, again, such interception hardware resides in a privileged position that is not available to common hackers.



I was trying to think of an instance where traffic was actually being intercepted against the wishes and best interests of its users.  And I remembered that there was a time when some slimy ISPs were caught injecting their own crap into the unencrypted HTTP web pages of their customers.  Now, there is a perfect example of why we would want and would benefit from having end-to-end encryption of our web traffic.  No one wants to have their ISP monitoring their activity, literally everything that they do, let alone injecting their own crap into content that our web browsers receive.



So, yes, there is an example, which was not theoretical, of an entity in a privileged position - our ISP whom we're paying to carry our data and connect us to the Internet - betraying our trust and not only spying on our activity, but injecting its own traffic into our connections.  Again, to actually pull off a man-in-the-middle attack, however, requires that the attacker arrange to have a privileged position where they're able to intercept, capture, buffer, analyze the traffic that's flowing past.  It is easier said than done.  But as we've always seen, where there's a will, there's a way.  And as those examples show, adding encryption to connections makes that job decidedly more difficult.



In doing a little more brainstorming, I came up with one way that Google's new "always try HTTPS first" approach would help.  One of the other attacks we talked about in the past was the protocol downgrade.  In such an attack, an attacker would arrange to convert all of a webpage's HTTPS URLs to HTTP.  The browser wouldn't know any difference and would make its queries over HTTP.  But with Chrome's automatically trying HTTPS upgrade, those rewritten HTTPs would be treated as HTTPS's, thus thwarting the attempt to downgrade connections for the sake of keeping a browser's communication unencrypted.



I think that what Google is doing is a good thing.  Trying to upgrade an HTTP URL to HTTPS prevents any protocol downgrade horseplay and provides more security for its users.  And like many of our listeners who hope that we don't lose all use of good old HTTP - especially for local connections to our own appliances where encryption and authentication offer little, if any, value - I doubt that losing HTTP is in the cards anytime soon.  After all, it took quite a while for FTP to finally be deprecated and removed from our browsers, and the File Transfer Protocol was never as popular as HTTP remains today.  Actually I'm somewhat surprised by Google's statement that 5 to 10% of their traffic that they're monitoring of Chrome's own users remain over HTTP.  That seems surprisingly high today.



LEO:  Yeah.



STEVE:  You know, they can't just be all from me clicking on my non-encrypted...



LEO:  But it would include that; right?  Yeah, it would include that.  So, and I think that - I think we've talked about this before.  Somebody said that in enterprise it's very common that you would have an HTTP link because it's within the network of...



STEVE:  Okay.  That makes sense.  So, right, that Intranet traffic would not be encrypted, yes.



LEO:  Exactly, right, right.



STEVE:  Anyway, so answering the question does everything need to be HTTPS, I'm still not convinced.  I understand the arguments, and I agree with our listeners who pointed to the potential for man-in-the-middle attacks.  We've just reviewed how difficult, but possible, such attacks actually are.  They require an attacker to be in a privileged position with some serious hardware.  HTTPS makes that more difficult, but not impossible in every case.



It seems to me that the way things are today is probably right.  If some random website, like the example I gave last week of Ctyme.com, which publishes that wonderful list of PC and DOS interrupt APIs, if they don't care enough to encrypt their site, even now that certificates to do so are freely available with minimal effort, then that ought to be up to them.  Am I going to avoid using such a site because it's not encrypted?  No.  I am put in much greater danger by downloading files over HTTPS from file archives.  That's truly terrifying.  And I avoid doing so at all costs, but sometimes there's no choice.  And of course the first place that file goes is over to VirusTotal to have it scanned to make sure that, as far as anyone knows, it's okay.



As for HTTP-only sites, I think it ought to be the site's and their visitors' decision whether the site wants to remain unencrypted, and whether their visitors want to visit.  Overly demonizing the lack of authentication and encryption to me seems wrong, especially when Let's Encrypt is cranking out fraudulent TLS certificates at a phenomenal rate which are then being used to spoof legitimate websites which have full authentication and encryption.  So there.



LEO:  Yeah.  I mean, this always bugged me.  I'm reading, I've mentioned this many times, but I'm reading a book about political power called "The Power Broker" about a guy who designed New York City over the wishes of the electorate because he wasn't elected.  But the question, the big question is do the ends justify the means.  And so this always bugged me that Google, the ends Google was going for made sense.  HTTPS should be everywhere.  But the means bug the hell out of me, which is we will give you a better ranking in the search results if you do HTTPS.  Because those two shouldn't be related.



STEVE:  They're using their power.



LEO:  And it's a way of using your power to achieve something that on the face of it is good, but is it ever okay to misuse your power even if the ends justify the means.  And it really bugged me, and I still don't think so.



STEVE:  Yeah.  I agree.  And in fact you've just summed up exactly what infuriates everyone about national politics is the constant display of the people who have power abusing that power.



LEO:  Well, and if you read this book, you realize it's much worse than you thought.  And the guy who wrote it, Robert Caro, is now working on - he's got four volumes out, a fifth volume on his LBJ biography, and there's a perfect example of somebody you could look at and say, you know, in many cases what he did, like the Civil Rights Act, really was great.  But then if you look at the means, he was very corrupt; you know?  And is it okay ever to use your power in that way?  And I don't know if it is.  I think it ends up tarnishing the ends.



STEVE:  No, it's about ethics; right?



LEO:  It's ethics, yeah.



STEVE:  It's sort of beginning to be a lost form.



LEO:  Well, that's really the eye-opener on this Robert Moses book is politics has always been - it's the worst.  Just the worst.  And it's filled with people who have that notion that, well, okay, it's okay to cheat and lie and steal a little bit because I'm trying to do something good here.  And that's the problem; you know?  That's the problem.  Steve, always really a great show.  This is really fun.  I'm so glad that we are going to continue on.  Here we are at 937.  It's right about now that I would start to be thinking there's 62 more episodes, I don't know.



STEVE:  Yup.  I couldn't stand the countdown.



LEO:  No, I don't want the countdown.



STEVE:  It's gone.



LEO:  And you know what, as I said, you can quit at Episode 1000, if you want.  You're only really obligated to go to a thousand.  And then you've satisfied your promise.



STEVE:  No bait-and-switch here, baby.



LEO:  No, that's not even bait-and-switch.  And if at any time it becomes onerous, you just say, yeah, you know, I think I do want to retire.  Because I feel the same way; you know?  I'm going to keep doing this as long as it's fun.  As soon as it's not fun, there's no reason to beat yourself up.



STEVE:  Thanks, buddy.  Bye.



Copyright (c) 2023 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#938

DATE:		September 5, 2023

TITLE:		Apple Says No

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-938.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we have our first sneak peek at "ValiDrive," the freeware I decided to quickly create to allow any Windows user to check any of their USB-connected drives.  There's been another sighting of Google's Topics API; where was that?  Has Apple actually decided to open their iPhone to researchers?  And what did some quite sobering research reveal about our need to absolutely trust each and every browser extension we install, and why was that sort of obvious in retrospect?  We're then going to entertain some great feedback from our amazing listeners before we conclude by looking at the exclusive club which Apple's just-declared membership made complete.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  As always, a fact-filled fun tour through the world of security.  We'll talk about Steve giving in to pressure, mostly from me, and developing a free app that will test the integrity of your hard drive.  The details on ValiDrive coming up in just a little bit.  Then we'll also talk about why you've got to be really careful about the browser extensions you use, more careful than anybody ever realized.  And finally, Apple's response to the demand that they open up their encryption.  Steve has all the details, coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 938, recorded Tuesday, September 5th, 2023:  Apple Says No.



It's time for Security Now!, the moment you've been waiting for all week long.  Here he is, the man of the hour, the man of the week, Mr. Steve Gibson.  Hello, Steve.



STEVE GIBSON:  Hello, Leo.  Well, it is great to be with you for the beginning of September.  And aren't we about to change our clocks?  We had to wait till after...



LEO:  I think it's October now; right?  



STEVE:  They keep moving...



LEO:  It has to be after Halloween.  Actually it's November because the sugar industry...



STEVE:  Oh, that's right, because we want to keep the...



LEO:  Yes.



STEVE:  That's right, we want to keep the light on for the kiddies while they trick-or-treat.



LEO:  The sugar industry said, no, please change it after Halloween, please.



STEVE:  I don't think anyone trick-or-treats anymore.  They go to malls and, like, schools and things to do that.



LEO:  Too much stranger danger out there.



STEVE:  That's right, that's right.



LEO:  We don't let them out of the house.



STEVE:  So we have Security Now! Episode 938 for this beginning of September, the 5th, titled "Apple Says No."  And of course you know what we'll be talking about, Leo, because you've been talking about it on your previous podcasts.



LEO:  Yep.



STEVE:  But this week we have our first sneak peak at ValiDrive, which is the freeware I decided to quickly create to allow any Windows user to check any of their USB-connected drives.  I'm going to just show some screenshots before we begin deeply.  There's been another sighting of Google's Topics API.  Where was that?  Has Apple actually decided to open their iPhone to researchers?  And what did some quite sobering research reveal about our need to absolutely positively trust each and every browser extension we install?  And really, why was that sort of obvious in retrospect?



We're then going to entertain some great feedback from our amazing listeners.  I got a bunch of great stuff in the past week.  And then we're going to conclude by looking at the exclusive club which Apple's just-declared membership has made complete.  So I think another great podcast for our listeners.  And we have a terrific Picture of the Week, too.



LEO:  That's a very Yoda-like riddle you just told.  Apple just decided to complete - okay.  Well, we'll find out.  We'll find out.  And now, the moment you've all been waiting for - this is a good one, I did peek - the Picture of the Week.



STEVE:  So for those who haven't seen it, I am tweeting the pictures now every week to Twitter.  So for those 64,000-some followers, you may have seen it.  We've got a picture sort of - it looks like sort of a large central park somewhere.  And on the left side is a large, wide, paved road sort of running from our foreground well into the distance.  To the right side is, you know, the park, the green grass part where you can imagine couples and their newborns picnicking and frolicking.  Well, I presume this is not a busted water main.  Maybe it's just been a long rain.  But this is all flooded; right?  So the road on the right is completely flooded.  The water is up over the curb.  It's poured into the green grassy area.  So this is all flooded.  Except in the center.  In the center foreground is the drain.



LEO:  Which is an island of green in that...



STEVE:  Which is above the water level, all the water.  Just there, it's like the one dry spot in the entire picture is the drain into which no water is flowing because, as you said, it's an island.  And so I gave this picture the caption:  "Well, those civil engineers were too expensive, so they decided to hire the mayor's nephew."



LEO:  I can do a drain.  It's easy.



STEVE:  Yeah.  That's not a problem.  It's dumb.  It's just a park.  I'll just put a hole here.



LEO:  Simple, yeah.



STEVE:  Oh, yeah.  Yeah, well, you get what you pay for.



LEO:  Wow.



STEVE:  So anyway, another great picture, thanks to our great listeners, who provided this.  And, you know, Leo, I was thinking about your reaction during our podcast last week to this problem of bogus mass storage drives and the need for a quick nondestructive test for both new and existing drives.



LEO:  When we left the show last week you said, well, it'll be in the next SpinRite.  And I thought, well, that's reasonable.  But I said maybe you could write something.  Talk about a quick reaction.  I saw your tweet.  And it was like, wow.



STEVE:  Well, in fact it was - you were on MacBreak, no, you were on This Week in Google.  I happened to have it on in the background while I was writing code for this.



LEO:  Yeah.



STEVE:  And so I tweeted that I decided to briefly pause work on SpinRite 6.1 because this just seemed like too big a need, and something that I had all the pieces in place to do.  So in the show notes I've got four actual screenshots made by what I'm calling ValiDrive, with a tip of the hat to Paul Holder, who came up with...



LEO:  Excellent name.  Excellent.



STEVE:  Came up with the name, yeah.  And sure enough, I mean, what we're learning, I released it to my testing gang two days ago, on Sunday.  And they immediately jumped on it.  Remember the drive I told the story about is in the lower right.  That's the one where it was a 256MB drive which SpinRite rejected, and that's how we stumbled on this whole problem was that SpinRite was checking the very end of the drive, just to make sure that its own drivers were working correctly with this drive.  And it was being rejected.  And the guy, millQ, has god knows how many USB drives, but this was the only one that was having a problem.  So we drilled into the problem and discovered that my belief is, because it was so old, and it's only a 2:1 ratio between good and bad size, it only actually has exactly 128MB rather than 256MB.  And yes, I'm saying megabytes, not gigabytes.  That's how old...



LEO:  That is old, yeah.



STEVE:  ...that this drive is.  So I don't think it would behoove any cheater, you know, any fakery, to only cut the drive in half.  Typically you see like 2TB that only have 32GB because that's enough to hold the file allocation table at the front of the drive to make the drive look valid when in fact it isn't.  You know, it'll store only 32GB.  And if you try to use more than that, the drive says, yeah, I'm storing it, no problem.  Well, until you try to read it back.  So anyway, the little piece of freeware is working.  It's being tested.  I'll finish it up during this week, and I'll have an announcement I'm sure while you're back away, Leo.



LEO:  You know what, I kind of knew that you would do this because I could see you already thinking about what you would have to do to make it work.  Well, let's see.  We'd have to fill - we'd have to write to every sector.  Oh, we've got to worry about maybe being fooled - you were already working on it before the show even ended.  I knew you were going to do this.  I'm so glad you did.  Now, when do you think you'll be able to release it?



STEVE:  A couple days from now.



LEO:  Oh, quick, good.  This will be hugely valuable to people.



STEVE:  Yeah.  Actually, some of the people who have been testing it said that they think it's going to be GRC's number one most downloaded freeware.  Now, to be fair, though, the DNS Benchmark has more than 8.5 million downloads.



LEO:  Wow.



STEVE:  With 2,000 new downloads every single day.  So it's going to be tough for little ValiDrive to catch up to the DNS Benchmark.  But anyway, I'm glad I did it.  I'll be back to SpinRite by the end of the week.  We'll have a very useful piece of freeware that everyone will be - every Windows person will be able to...



LEO:  A must-have, yeah.



STEVE:  And I did tweet these screenshots, and there were some reactions from people saying, hey, what about if you're colorblind because...



LEO:  Oh, yeah.  Red green.



STEVE:  Because I'm using color in order to show valid storage, read errors, write errors, and missing storage.  I will have a monochrome option that will put different shape black-and-white characters in the cells in order to accommodate people who don't have full normal color discrimination ability.  So anyway, it's on its way.  And so I'm glad I did it, and we'll have a new gismo here shortly.  And yes, Leo, your intuition was correct.



LEO:  I'm just, I mean, this is right up your alley.  I saw on GitHub there are a few people who've written C++ programs to do this, but who are they?  And it's command-line and blah blah blah.



STEVE:  And they're destructive.  They wipe out your data.  You're able to run this on an existing drive.



LEO:  Well, that's not good.



STEVE:  No.



LEO:  Yeah.  So you read the data, save it, and then write to it, and then write it back.



STEVE:  Yeah.



LEO:  You are good.



STEVE:  And then I read it again, and then I rewrite it and reread it to make sure that it got written back correctly.



LEO:  And I think probably the reason people don't do that is that it must take some time to do that on a 2TB drive or more.



STEVE:  Yes.  And in fact somebody commented, they had one of those destructive things.  And I said, well.  And he said, you know, yours is taking maybe, I think it was like twice as long.  Well, twice as long is a few minutes.  Actually, many times it completes in 15 seconds.  And what many people are finding - oh, yeah, it's...



LEO:  That's fast.



STEVE:  It's only a few seconds.  And many people are noting that what they're seeing, even when they get a field of green, is the speed and the hesitation.  When I first started using it myself on some of mine, I thought, wow, it was like freezing.  Well, what is it doing when it's freezing?  So there's also little read and write lights that flicker back and forth while it's running as it's counting down the number of tests remaining.  So you can really see, like, what's going on.  And so even if your drives are good, it helps you spot problems because put in a high-quality drive, it just zips along.  Put in a cheesy Walmart drive, and it's like [mimicking bad drive].  It's like, god, what is wrong?



LEO:  Well, that tells you something, too.  Wow.



STEVE:  That's, yeah, that's going to be a goody.  Okay.  So...



LEO:  Thank you, Steve, from all of us, thank you very much.



STEVE:  Well, it's going to be fun.  So Google's Topics is coming to Android Apps near you.  And maybe this is a failure of imagination, but it hadn't occurred to me that Google's Topics system, which we've talked about now several times, might not only apply to websites.  In retrospect, it's so obvious that Google would also be assigning Topics to Android apps; and that advertisers, and apparently other apps, would also be able to query the device's local Topics API to obtain a few bread crumbs of information about, you know, the person who's using the app.



One of our listeners was kind enough to share a screen capture of what had just popped up on his Android phone.  Under the headline "New ads privacy features now available," the screen reads:  "Android now offers new privacy features that give you more choice over the ads you see."  This is very much like the text that the Chrome users got.  Android notes topics of interest based on the app you've used recently.  Also, apps you use can determine what you like.  Later, apps can ask for this information to show you personalized ads.  You can choose which topics and apps are used to show you ads.  To measure the performance of an ad, limited types of data are shared between ads.



Okay.  So, you know, we know I'm a fan of Google Topics.  I understand it, and I've tried to carefully explain the way it works, which is admittedly somewhat convoluted and open to misunderstanding because Google is trying to slice this thing very close.  Google wants access to some limited information about the users of their Chrome web browser and now their Android phones in an environment where users have become skittish about privacy and tracking.



And, you know, we all recognize the tradeoffs; right?  If websites insist that they receive some revenue, more revenue when advertisers have some information about their visitors, and advertisers are determined to obtain that information by any means possible, then Topics is the cleanest tradeoff compromise I can imagine.  If, eventually, once legislation catches up, Topics replaces all other forms of tracking and information gathering about me, then I'm all for Topics.  You know, that's a tradeoff that makes sense.



So I suppose I shouldn't feel any differently about Topics being extended outside of the browser.  If a user wants to use advertising-supported smartphone apps, then I suppose the same logic applies there; right?  And, you know, I should explain that I personally cannot, and do not, tolerate in-app advertising.  You know, if an app is something I want to use, please allow me to send a few dollars your way to turn off its ads.  I will do that happily.  Otherwise, I don't care how great it is, nothing is that great.  I will delete any app whose advertisements I am unable to silence.  But that's just me.



What we see all around us, pervasively, is that advertising works.  And Leo, as you noted last week, even if I refuse to click on some advertisement, the brand being advertised has been planted in my brain.  That's out of my control.  And the fact is we live in an advertisement-supported world.  This podcast is underwritten by a few high-quality enterprises that are willing to pay to make our listeners aware of their presence and offerings.  That's all they ask.



So Google is extending Topics beyond Chrome and into the underlying Android platform.  That only makes sense, really, in retrospect, as I said.  But I'm certain that Google will also allow Topics, as they do on Chrome, to be completely disabled if that's what its user chooses.  So again, props to Google for that.  I am 100% certain that before offering that full disablement option they thoroughly, and not just once, tested what I often call "the tyranny of the default."



So they absolutely know that nearly 100% of Android phone users will never know nor bother to disable their Android device's local Topics feedback.  And they also know that by allowing their more knowledgeable Android users - like every listener of this podcast - the option to disable Topics, by giving them the option to disable Topics they're retaining and comforting those users who would be upset by this local, albeit extremely mild, smartphone surveillance.  And, you know, if ads in apps are inevitable, if they're supporting the apps that you're using, then you might as well get as relevant an ad as possible, if you're got to have one anyway.  So anyway, I thought that was interesting.  It just hadn't occurred to me that Topics would be something that Android at large did more than just Chrome.  But, you know, it only makes sense.



I've often bemoaned the problem researchers have with helping Apple to find their own platform's security shortcomings because the platform is so thoroughly and utterly locked down.  But last week I was reminded that, for the past four years, since 2019, this has not been strictly true.  Last Wednesday's blog post from Apple's Security Research was titled:  "2024," which is, you know, next year, "2024 Apple Security Research Device Program now accepting applications."  And this window is one month, so jump if you're interested.  We've talked about this before, but I've been overlooking this truly marvelous exception to Apple's "no one gets in" stance.



In their Security Research's, Apple's Security Research's  overview of this, they explain:  "iPhone is the world's most secure consumer mobile device" - and I would agree with that completely - "which can make it challenging for even skilled security researchers to get started."  Or actually to get anywhere.  They said:  "We created the Apple Security Research Device Program to help new and experienced researchers accelerate their work with iOS.  Now accepting applications through October 31st, 2023.  Apply below."  And then under "How it works," they remind us.



They said:  "The Security Research Device (SRD) is a specially fused iPhone that allows you to perform iOS security research without having to bypass its security features.  Shell access is available, and you can run any tools, choose your own entitlements, and even customize the kernel.  Using the SRD allows you to confidently report all your findings to Apple without the risk of losing access to the inner layers of iOS security."  And I guess that means that the phone won't suddenly lock you out.  Anyway, they said:  "Plus, any vulnerabilities that you discover with the SRD are automatically considered for Apple Security Bounty."  Which, you know, has ranged up to $100,000 in some cases.



Then elsewhere they elaborate this a bit.  They said:  "iPhone is the most secure consumer mobile device on the market, and the depth and breadth of sophisticated protections that defend users can make it very challenging to get started with iPhone security research.  The central feature of SRDP" - which is the program - "is the Security Research Device, the SRD, a specially-built hardware variant of iPhone 14 Pro that's designed exclusively for security research, with tooling and options that allow researchers to configure or disable many advanced security protections of iOS that cannot be disabled on normal iPhone hardware in the hands of users.



"Among other features, researchers can use a Security Research Device to install and boot custom kernels; run arbitrary code with any entitlements, including as platform and as root outside the sandbox; set Non-Volatile RAM variables; install and boot custom firmware for Secure Page Table Monitor and Trusted Execution Monitor, which are new in iOS 17."  And they said:  "Even when reported vulnerabilities are patched, the SRD makes it possible to continue security research on an updated device.  All SRDP participants are encouraged to ask questions and exchange detailed feedback with Apple security engineers."



And in another place, explaining about eligibility for the program and some constraints, they said:  "The SRD is intended for use in a controlled setting for security research only.  If your application is approved" - that is, your application to join the program - "we," said Apple, "will provide you an SRD as a 12-month renewable loan.  During this time, the device remains the property of Apple."  So, you know, you don't have to buy it.  They're saying, "Here, but it's still ours."  "The SRD is not meant for personal use or daily carry, and must remain on the premises of program participants at all times.  Access to and use of the SRD must be limited to people authorized by Apple.



"If you use the SRD to find, test, validate, verify, or confirm a vulnerability, you must promptly report it to us and, if the bug is in third-party code, to the appropriate third party.  Our ultimate goal is to protect users, so if you find a vulnerability without using the SRD for any aspect of your work, we'd still like to receive your report.  We review all research that's submitted to us and consider all eligible reports for rewards through the Apple Security Bounty.



"Participation in the Security Research Device Program is subject to review of your application.  To be eligible for the Security Research Device Program, you must have a proven track record of success in finding security issues on Apple platforms, or other modern operating systems and platforms."  So, you know, have some pedigree.  "Be based in an eligible country or region."*  And there's a little asterisk on that we'll get to in a minute.  "Be the legal age of majority in the jurisdiction in which you reside," they said, "18 years of age in many countries.  And not be employed by Apple currently or in the past 12 months.



"To enroll as a company, university, or other type of organization, you must be authorized to act on your organization's behalf.  Additional users must be approved by Apple in advance and will need to individually accept the program terms."  Now, where they said "be based in an eligible country or region,"* I looked at the bottom of the page where the asterisk was referring, and there was a very long list of qualifying countries.  Notably absent, and they were alphabetized, so it was easy to spot, were China, Russia, and North Korea.



LEO:  Yeah, there's a good group to be in.



STEVE:  So sorry there, Vladimir.  You and your minions will not be authorized.  You may have some underhanded, surreptitious way of getting your hand on a phone.  You know, and Leo, I was thinking, I wouldn't be at all surprised if these things are not geolocked also.



LEO:  Oh, I'm sure they are.  Oh, hell, yes.



STEVE:  I'll bet there's some tethering on this thing.



LEO:  Apple knows how to do this stuff very well, believe me.



STEVE:  Yeah, yeah.



LEO:  This is good.  This is great that they're doing this.



STEVE:  It is so cool.



LEO:  I presume that one of the things is - because this is always the complaint of researchers.  They couldn't get into these phones to know whether they were compromised or not.  I mean, that complaint continues because a normal phone you still can't get into to know if it's compromised or not.  But at least they can research zero-days and so forth.



STEVE:  Yeah.  I'm sure it reflects many prior years of researchers complaining about exactly that; right?  That they're just like, well, we'd like to help Apple.  There's all this cool tech in there.  And oh, by the way, it does seem to be having lots of problems with zero-days.  Maybe we could find some of those, but we can't get in.  So anyway, I wanted to correct the record of my recent statements that it just wasn't possible to conduct meaningful research into iPhone security.  Bravo Apple.  Once again, I think they're doing the right thing.



LEO:  Has there been any reaction from the security researchers on this?  Like is this what they want?  Is it enough?



STEVE:  Oh, yeah, yeah, yeah, yeah, yeah.  I mean, the problem is Apple said that there's a limited number of these that they want to have floating around, you know, they aren't going to be able to honor every request.  But in some of the text I noted that even universities like security education programs could qualify.



LEO:  Ah.



STEVE:  So students at universities could have access to these special, you know, iPhones.



LEO:  I'm sure Matthew Green is applying right now; you know?  That's great, yeah.



STEVE:  Yeah.



LEO:  And they do really good work because they're not constrained by commercial necessity.  So they can spend months trying to break into this stuff.  It's almost always out of universities that the toughest hacks come, like...



STEVE:  All the research that we talk about.  As a matter of fact, we've got some right here.



LEO:  Oh.



STEVE:  This is on the need to REALLY, in all bold caps, trust every web browser extension we install.  This sobering research has recently come from researchers at, what do you know, the University of Wisconsin-Madison.  As part of their exploration into what a malicious web extension can and might do, even today when operating under the more restrictive Manifest V3 protocol that Chrome introduced which has since been adopted by most browsers, these researchers discovered that their proof-of-concept extension is able to steal plaintext passwords from a website's HTML source.  And wait till you hear which websites were vulnerable, found to be vulnerable.



Thanks to the unrestricted access to the DOM tree - that's the web page's Document Object Model which is organized logically as a tree structure.  It describes the document.  But more recently, I mean, everything that you see on the page is in this Document Object Model.  It's just like it's a - it is the web page.



LEO:  And stuff that you don't see on the page, more importantly; right?



STEVE:  Yes.



LEO:  Hidden CS, yeah.



STEVE:  Yes.  So that's what the browser uses to drive its renderer and all of the scripting that it also runs.  So they demonstrated that the coarse-grained permission model under which we're all now operating, which also covers browsers' text input fields, violates the principles of least privilege.  They found that numerous websites with millions of visitors - and I'm not going to stomp on the news of which websites.  We'll get there in a minute.  But, boy - including some Google and Cloudflare portals, store passwords in plaintext within the HTML source of their web pages - just sloppiness - thus allowing for their ready retrieval by extensions.



So their research paper is titled:  "Exposing and Addressing Security Vulnerabilities in Browser Text Input Fields."  This is what they explain in their paper's Abstract.  They said:  "In this work, we perform a comprehensive analysis of the security of input text fields in web browsers.  We find that browsers' coarse-grained permission model violates two security design principles:  least privilege and complete mediation.  We further uncover two vulnerabilities in input fields, including the alarming discovery of passwords in plaintext within the HTML source code of web pages.



"To demonstrate the real-world impact of these vulnerabilities, we design a proof-of-concept extension, leveraging techniques from static and dynamic code injection attacks to bypass the web store review process."  In other words, they snuck it in.  "Our measurements and case studies reveal that these vulnerabilities are prevalent across various websites, with sensitive user information, such as passwords" - but not restricted to, we're talking social security numbers, credit card numbers, you name it - "exposed in the HTML source code of even high-traffic sites like Google and Cloudflare.  We find that a significant percentage (12.5%) of extensions possess the necessary permissions to exploit these vulnerabilities and identify 190 extensions that directly access password fields.



"Finally, we propose two countermeasures to address these risks:  a bolt-on JavaScript package for immediate adoption by website developers, allowing them to protect their sensitive input fields; and a browser-level solution that alerts users when an extension accesses sensitive input fields.  Our research highlights the urgent need for improved security measures to protect sensitive user information online."



Okay.  Now, the Manifest V3 protocol prohibits extensions from fetching code hosted remotely that could help evade detection, and prevents the use of eval statements that lead to arbitrary code execution.  However, as the researchers explained, Manifest V3 does not introduce a security boundary between extensions and web pages, so the problem with content scripts remains.



To test Google's Web Store review process, they created a Chrome extension capable of password-grabbing attacks, and then uploaded it to the extensions repository.  Their extension posed as a GPT-based assistant that can capture the HTML source code when the user attempts to login on a page by means of a regex; abuse CSS selectors, you know, the web page CSS, to select target input fields and extract user inputs using the .value function; and perform element substitution to replace Javascript-based obfuscated fields with unsafe password fields, all of which they explain in their research doc.



The extension does not contain obvious malicious code, so it evades static detection and does not fetch code from external sources, which of course would be dynamic injection.  So it is fully Manifest V3-compliant.  This resulted in the extension passing the review, being accepted on Chrome's Web Store.  So the security checks failed to catch the potential threat, which in this case was very real.



Now, of course, the researchers followed strict ethical standards to ensure no actual data was collected or misused.  They deactivated the data-receiving server component while only keeping the element-targeting server active.  Also, the extension was set to "unpublished" at all times so that it would not gather many downloads.  And it was promptly removed from the store following its approval.  That is, as soon as they saw that it got in and were able to verify that this thing was able to slip by.



Okay.  Subsequent measurements showed that from the top 10,000 websites, roughly 1,100 - that's where that 12.5% figure came from - are storing user passwords in plain text form within the HTML Document Object Model.  And extensions script have access to the Document Object Model, thus access to plaintext passwords.  So this is a fundamentally insecure design.  The designers of those 1,100 websites, that is, 1,100 out of the top 10,000 that these guys looked at, the designers of those websites either wrongly assume that the contents of their page's document object model are inaccessible, which is not true, or they never stopped to consider it.



In addition, another 7,300 websites from that same set of the top 10,000 were found vulnerable to DOM API access and direct extraction of the user's input values.  Several of those, including widely used ad blockers and shopping apps, boast millions of installations.  So this thing is widespread.  Okay.  Now, is everybody sitting down?  Notable websites lacked the required protection and are thus vulnerable right now.  Those include gmail.com.



LEO:  Whoops.  Oh, nobody uses that, thank goodness.  Holy cow.



STEVE:  Which has plaintext passwords stored in HTML source code.



LEO:  What?



STEVE:  Cloudflare.com, plaintext passwords in HTML source code.



LEO:  Passwords to what?



STEVE:  The users' passwords.



LEO:  What?



STEVE:  Are available in plaintext in the HTML source.  Facebook.com, user inputs can be extracted via the DOM API.  Citibank.com, user inputs can be extracted via the DOM API.  Irs.gov, social security numbers are visible in plaintext form on the web page source code.  Capitalone.com, SSNs are visible in plaintext form on the web page source code.  Usenix.org, same thing, social security numbers.  Amazon.com, credit card details including the security code and ZIP code are visible in plaintext form on the page's source code, available to all extensions.  Yes, it is that bad.



LEO:  Holy cow.



STEVE:  The V3 Manifest was a tradeoff.  Due to the way the industry's existing websites and popular extensions had been coded, further limiting extension use would have broken too much existing code, so it wasn't done.  When a Google spokesperson was asked about this, they confirmed that they're looking into the matter - you think? - and pointed to Chrome's Extensions Security FAQ that does not consider access to password fields to be a security problem "as long as the relevant permissions are properly obtained."  Right.  Let's hope this gets fixed soon.



LEO:  I can understand, though, why you need access to the DOM if you're an extension.  I mean, that's kind of what an extension does.



STEVE:  That's what you do.



LEO:  And Gorhill has complained about Manifest 3 making it impossible to do uBlock Origin because even the little restrictions that it offers make it hard to do adblocking.  So I understand, boy, this is a problem.



STEVE:  It is a Catch-22, Leo, yes.



LEO:  Yeah.  The web was really not designed to be secure.  I mean, that's what we're fundamentally seeing.



STEVE:  No, no.  And we tried to turn web into web apps as if they're the same.  And you know, we stretched our browsers mightily in order to do that.  In the Takeaways section 5.3 of their paper, they write:  "This is a systemic issue."  They said:  "Our measurement studies on the top 10K websites show that we could extract passwords from all the login pages with passwords.  The widespread presence of these vulnerabilities indicates a systemic issue in the design and implementation of password fields."



And they talk specifically about password managers.  Now, think about that.  We take it for granted; right?  But any and all password managers must by design be a third-party extension which has direct access to any website's password fields.  They said under "Role of Password Managers:  The widespread use of password managers may partially explain the prevalence of vulnerabilities, where password values are obscured, but can be accessed via JavaScript.  These tools enhance the user's experience by automating the process of entering passwords, storing the encrypted passwords, and later auto-filling these fields when required.  This functionality reduces the cognitive load on users and encourages the use of complex, unique passwords for each site, thereby enhancing overall security.



"However, for password managers to function effectively, they require access to password fields via JavaScript.  This necessity creates an inherent security vulnerability.  While the password fields may appear obscured to users, any JavaScript code running on the page, including potentially malicious scripts, can access these fields and read their values.  This interaction between password managers and these vulnerabilities presents a tradeoff between usability and security.  While password managers improve usability and promote better password practices, their operation necessitates JavaScript access to password fields that inherently creates a security risk."



Essentially, these guys just demonstrated that you don't even have to be a password manager to obtain password manager level access.  And they were able to successfully sneak their universal password extraction extension code past Google's incoming filters without any trouble.  So their 26-page paper is marvelously clear, and none of this stuff is fancy or complex.  Its content would be entirely accessible to anyone familiar with modern web page construction and operation.  Any of our listeners who are responsible for the design of their organization's secret-accepting web pages might well benefit from making sure their own sites are protected.  I've included the link to the research PDF for anyone who is interested.  And to improve its availability, it's also this week's GRC shortcut, so you can find it at grc.sc/938, grc.sc/938.  And the PDF link is also in the show notes.



So this is important.  Again, kind of in retrospect, it's like, well, duh.  Of course password managers need to be able to do this.  It turns out it's not just password managers that can.  They found 190 existing extensions that had this capability.  And I don't think there's 190 password managers out there.  So a lot of apps are doing it, and they created one and slipped it past Google and could do it, too.



LEO:  Wow.



STEVE:  Yeah.



LEO:  Amazing.  All right.  Let's close the loop.



STEVE:  So, yeah.  Peter Gowdy tweeted.  He said:  "Hi, Steve.  I took note of your Global Privacy Control episode, and just added the Privacy Badger extension to Vivaldi.  There doesn't seem to be a solution for mobile that I could find, even in Firefox mobile.  Is there a mobile Global Privacy Control solution that you know of?"



And I'm not surprised that support is still lagging since, as we know, change always comes much more slowly than we expect or hope or wish.  But I'm pretty sure that, once additional legislation appears, and we know that it exists in three states beginning with the letter "C," and it is spreading both here in the U.S. and also in Europe, I think we can assume that the Global Privacy Control switch will become a universal feature of browsers.  Again, it'll just take some time, but we'll get there.



Longstanding friend of the show Alex Neihaus, he tweeted:  "Hi.  Re: Microsoft's 'doesn't care' about the STS issue" - remember that's the secure time setting, whatever it was that that stood for, that we've talked about several times, you know, using time in the TLS handshakes in order to set the clocks.  He said:  "Despite it being known for decades as being unreliable."  He said, this is Alex saying:  "I don't think they deliberately or maliciously mis-engineered the feature.  I think they just didn't do the research.  Most people think that Microsoft developers are first-rate.  But management there has reduced costs, which has encouraged use of offshore and lower-experienced engineers.  Unlike us Boomers, devs today rarely go as deep as you did to understand the issue.  The engineer was simply and probably impatient, saw the field in the hello message, and went for it.



"You're most likely correct that they don't want to admit they're wrong because it raises the question I am posing here about their engineering prowess.  So it was most likely a combo of poor engineering and design, coupled with hubris today, that prevents them from recognizing the deeper issue."



LEO:  I also want to point out that - you know who Alex Neihaus is; right?



STEVE:  Yeah.



LEO:  He was a guy who - he was the first club member, bought the first ads on this show with Astaro.  And we thank him so much because he has put us on the map, thanks to Alex.  Well, you put us on the map, but Alex helped do it.



STEVE:  And I don't disagree with anything Alex wrote.



LEO:  He's sharp.



STEVE:  Yes.  Everyone here knows how infinitely tolerant I am of mistakes.  You know?  They happen, and anyone can make them.  And I'm sure one of these days I'll make a big one, so I'll be glad that I've always been saying this.  You know, there are many...



LEO:  Getting defensive, I understand.



STEVE:  You know, I'm crossing my fingers.  I'm as careful as I can be.  But there are many adages that begin with, "If you're not making mistakes, dot dot dot."  You know?  And typical endings for that are "then you're not trying hard enough."  Or "then you're not making decisions."  But the most famous one appears to be, "If you're not making mistakes, then you're not doing anything."  The point of all of these is the clear acknowledgement that mistakes are a natural and unavoidable consequence of our interaction with the world.  You know?  You do something.  The feedback from what you did, which was presumably not what you expected, informs you that a mistake was made somewhere.  So with the benefit of the new information, you correct the mistake.



My entire problem with Microsoft is that we see example after example, this being just the latest, where this feedback system appears to be completely absent.  Whether it's well-meaning security researchers informing Microsoft of serious problems they've found, or their high-end enterprise customers for seven years telling them:  "Hey, my Windows server clocks are getting messed up, and it's really screwing up everything."  Microsoft no longer appears to care.  And to Alex's point, though coming at this from a different angle, I think this all boils down to simple economics:  Caring costs money.  And Microsoft no longer needs to care because not caring costs them nothing.



That's really the crux of it today.  There's no longer any sign of ethics for ethics' sake.  You know, that's long gone.  It's simply about profit.  We're all aware of the expression "Don't fix it if it's not broken."  Microsoft has extended this to "Don't fix it even if it is broken."  And what we get is a system that, you know, it mostly works.  It could be better.  But I guess it's good enough.  And, you know, again, I always want to add the caveat, I'm sitting in front of Windows.  I love my Windows.  I don't ever want to have it taken away from me.  So, you know, I want it to be as good as it can be.  But darn it, it could be better.



LEO:  I'm coming over, and I'm taking it away from you.  No more.



STEVE:  Listener Michael, he tweeted:  "Dear Steve.  Just listened to another awesome Security Now! from you.  I have a question about VirusTotal, if I'm not bugging you.  What's the probability that it could have false positives?  I'm asking specifically because of a program I've used since Windows 7 called Winaero Tweaker, which lets me customize Windows so that it's more usable and easier.  It's not flagged by Windows Defender, nor by Malwarebytes.  I guess what I am asking is, in your opinion, is Winaero Tweaker okay to use, and is VirusTotal ever wrong?  Thank you.  Michael."



Okay.  So if we were to use majority voting, then I have never seen VirusTotal make a mistake.  But if you require zero detections in order to be comfortable, you know, out of the 66 different AV scanners that VirusTotal polls, then that's actually somewhat rare.  When using any modern AV scanner it's important to understand today's context.  The original AV scanners operated by spotting specific code that had been previously identified as being part of a piece of specific known malware.  This was quite effective and rarely generated false positive alarms because the AV was actually finding the malware that used some specific code.  But then of course malware evolved to avoid direct code recognition by scrambling itself, encrypting itself, compressing itself, and even becoming "polymorphic."  Remember the polymorphic viruses which self-rearranged in order to appear as unique as possible from one instance of itself to another.



Then later, as a consequence of this back-and-forth cat-and-mouse game that malware was playing with AV scanners, the scanners began looking at the operating system functions that a program was using and judging whether some things a program like the OS API calls a program might request fall outside of some arbitrary norm.  For example, maybe a DNS lookup.  There's nothing malicious about a program doing a DNS lookup.  But most programs that want to connect to a remote resource just issue an HTTPS request, and the operating system performs the DNS lookup itself to obtain the connection's IP address.  So in this example, any program that wanders away from an arbitrary tightly defined norm might trigger a false positive alert, not because it did anything wrong, but simply because it was found to be doing things that someone judged as unusual.



The final outcome of decades of this back-and-forth contest between AV and malware is the increasing use of a specific program's reputation.  The way things have turned out, reputation is the ultimate source of trust.  So in that sense things are the same in cyberspace as they are in the real world.  You know, we trust people who have earned a reputation.  We have the ability to easily obtain unspoofable cryptographic signatures of specific code. This means that for all intents and purposes it's impossible to change the code in any deliberate way without also changing the code's resulting cryptographic signature.  So without actually knowing anything about a program, the persistent connectivity provided by the Internet allows a program's use, and its signature, to be tracked over time.  If a program is out and about for a few months without anyone complaining or it causing anyone any trouble, then that code, as identified by its unique signature, will have established a good reputation and will therefore become trusted.



The trouble is that any newly created code will have an unknown signature that won't have had any chance to earn a reputation.  And as a creator of new utilities, this is a problem I run into all the time.  Two days ago, last Sunday, the first people to download the completely harmless and freshly assembled ValiDrive Windows application had Windows 10 immediately quarantining the download, complaining that it was "not commonly downloaded."  Well, yeah, no kidding.



LEO:  It's brand new.



STEVE:  It had never been downloaded before.  So here we had this brand new, never-before-seen cryptographic signature, and Windows said, whoa, what's this?  Where did this come from?  Off with its head.



So to make matters worse, actually, part of it was my fault.  I was in a hurry to get the code into everyone's hands, so I hadn't stopped to digitally sign the executable file with GRC's code-signing certificate.  As soon as the first several complaints came in, I did that.  And now things appear to have calmed down since.  GRC has a spotless reputation since we've never had an incident of any kind.  But even so, code-signing certificates do get stolen.  You know, mine are all locked up in hardware security modules, so at least they can't be stolen easily.  You know, you have to have physical proximity, and that's unlikely to happen because Level 3 is behind multiple barriers and guards and cameras and alarms and everything.



So anyway, but the point is, in general, certificates do get stolen.  So just being signed by someone, even with a perfect reputation, isn't 100% assurance.  I did just check ValiDrive with VirusTotal, and there were three false positive "detections" after querying 66 antiviral systems.  Cybereason, Cylance, and Trapmine didn't like it, but no one else had any complaints.  So as I said, the majority voting, I've never seen any of my stuff objected to by more than a handful.  But anyway.  So it certainly can happen.



As for Winaero Tweaker, I just grabbed a copy of the setup executable from Winaero.com, which is the publisher, and I dropped it onto VirusTotal.  And I received a 100% clean bill of health, with VirusTotal saying that zero out of 43 scanners found it to be suspicious.  Now, I noted, however, that the executable program, the setup program, was not signed, which would make me suspicious and uncomfortable since it's almost becoming required these days.  A digital signature on executable content is something I always check for now.  And needless to say, I always and only obtain such programs, especially if they are unsigned, from the original website source.  You don't want to get it from somebody who's like, oh, yeah, we also offer it for download.



But for what it's worth, it's version 1.55, which I just downloaded directly from their site.  Other than it being not signed, it looks fine.  I did note that it was published in June, so if Michael had grabbed it shortly after its publication, also not being signed, then he could have found that VirusTotal or whatever hadn't had a chance yet to get to know it, to have other people upload it and say what do you think, scan this for me.  At this point it looks like it's fine.  But anyway, the point is, can it false positive?  Yes, unfortunately, happens to me all the time.  You ought to see that anything that's been around for a few months will have acquired a reputation, and that reputation now is really the only shield that a program has.  So that's the current status of AV scanning.



Rick said:  "Steve, on Acceptable Ads and uBlock Origin, how are you doing it?  I looked around, but only found this old thread."  He provided a link in his tweet.  He said:  "While it's true that the list it points to is current, Gorhill himself slammed it, though what he's saying about that particular list doesn't seem to apply anymore."



Okay.  So I did some digging and refreshing of my memory, and it turns out that I was wrong about uBlock Origin and Acceptable Ads.  We discussed all of this after 2014 when it was all happening, but I'd forgotten the details.  uBlock was initially developed by a guy named Raymond Hill, better known by his handle of Gorhill, and it was released in June of 2014.  The extension relied upon community-maintained blocklists, while also adding some extra features.  Not long after that, Raymond transferred the project's official repository to a guy named Chris Aljoudi since he was frustrated with all of the incoming requests.  That's sort of not Gorhill's style.  Curmudgeon is, you know, we remember him sort of fondly along with John Dvorak.  



It turns out that Chris, the guy who obtained the repository, was somewhat less than honest and respectful.  He immediately removed all credit from Raymond Hill, portraying himself as the uBlock's original developer.  And he started accepting donations showcasing overblown expenses to turn the project into a profit center.  Rather than development, Chris was focused more on the business and advertisement side, wanting to milk uBlock for all it was worth.  Consequently, Gorhill decided to simply continue working on his extension.  But that unfortunately resulted in a naming collision where Chrome saw Chris' uBlock as being the original, and Gorhill's as being the interloper.  So Gorhill lost, and Chrome yanked his from the extension repository.



Thus was born uBlock Origin, and here comes the difference that matters:  The original uBlock worked with the Acceptable Ads policy and still does.  But Gorhill, being Gorhill, wasn't interested in making any exceptions to his extension's ad blocking, especially when exceptions to the Acceptable Ads policy had the reputation of being available to the highest bidder.  That's not his style at all.  Having watched all this drama unfold, at the time we all went with the original extension's original author, since no one felt any particular sympathy for Chris, whose conduct did not appear to be very honorable.  And choosing uBlock Origin also meant no longer being able to allow Acceptable Ads, which I would otherwise have no problem doing.



So that's the story.  I'm still disinclined to move away from uBlock Origin since I have the strong sense that curmudgeonly Raymond Hill will always have our backs.  I feel much less sure of that from Chris Aljoudi, who is the guy behind uBlock, based on his conduct after getting it.  So anyway, Rick, you're right.  I was wrong about Acceptable Ads.  So I'm glad you brought it up and I was able to correct the record.



Somebody whose Twitter handle is #LoveThyNeighbor, he said:  "Steve, can the ReadSpeed utility analyze a drive connected via a USB port?  It appears that I can only see drives connected and enumerated on the internal IDE, SATA, or SCSI busses of the computer.  Is there a way to have ReadSpeed analyze a USB-connected drive?  I faithfully listen to Security Now!, so hope to hear a response there as I am not on Elon's repugnant X site very often.  SpinRite user since Version 1 and listener to Security Now since Episode 1.  Thanks for all you do."



Okay, LoveThyNeighbor.  Unfortunately, the short answer is no.  The ReadSpeed DOS utility was a natural offshoot of the early work on SpinRite 6.1.  Specifically, I believed that I had nailed down the operation of what would become 6.1's new native IDE, ATA, and AHCI drivers with parallel and serial ATA drives.  And we had discovered the surprising slow performance at the highly-used frontend of many SSD devices.  Since I thought that ReadSpeed might be broadly useful, it was spun out sort of as an offshoot along the way.  So it won't be until we get to SpinRite 7 that USB and NVMe devices will be added to that collection.



That said, I do expect to be dropping some similar freeware in the early days of SpinRite 7's development, since I'll be anxious to get feedback about this emerging software's, you know, SpinRite 7's dual booting over BIOS and UEFI.  And as we know I'll be writing it under a new, that RTOS32 operating system.  So at that point we will finally be able to talk natively to all drives, which will be, you know, the real benefit going forward for 7 and beyond.



Austin Wise.  He tweeted:  "Re man-in-the-middle attacks and HTTPS on Security Now! 937."  Boy, and it's going to be a long time, Leo, before I am able to live down my comment that I don't think there's anything that wrong with HTTP.  There's still some places for it.  I don't think any of our listeners agree.  But...



LEO:  Well, you know, Dave Winer, who is a father of RSS, wrote a very nice piece on his scripting blog that said HTTP was critical to the development of the web because it's easy to implement, and it should still be used in cases where it's safe to use, and there are plenty.  And there's a lot to be said for simplicity, especially so that anybody can create a site.



STEVE:  Yeah, in fact, somewhere I just saw, I don't know where it was, but it was talking about the nature of security.  And it said keep it simple, keep it simple, keep it simple.  I mean, and we know, how many times have we talked about complexity being the enemy of security?



LEO:  It's not hard to generate a Let's Encrypt certificate and make your site HTTPS.  I've done it on a number of places.



STEVE:  Yup.



LEO:  But still, you know, there are places where there is no login, there's no passwords in the DOM, there's no reason that you should worry about a man in the middle.  And I think those sites should be allowed to continue HTTP.  But Google doesn't, that's for sure.



STEVE:  Well, and Leo, you just raised a perfect example.  You could have TLS 1.3 with the fancy longest bit encryption key.  And if you've got a funky extension in your web browser, doesn't matter if it's HTTPS, it's still sucking all your passwords and social security numbers and credit card details right off to wherever.



LEO:  Holy cow.



STEVE:  Lower Mongolia.  No, I think actually Lower Mongolia I think they're able to get iPhones.  But not Russia and China.



LEO:  Good, okay.  For all the security researchers there.



STEVE:  We have a listener from Google.  He said:  "If an attacker is on a local network, like a coffee shop WiFi, they might not need a privileged position" - which of course was what I was talking about last week - "to modify traffic.  See ARP spoofing."



LEO:  Right.  Right.  And the widespread availability of things like the WiFi Pineapple make that something even not so sophisticated people can do.



STEVE:  Yes.  He said:  "Also the integrity features of TLS are useful even if you trust the network.  Random bit flips in packets, like from a misbehaving router, will be detected and cause the connection to terminate."



LEO:  That's a good point.  That's a good point.



STEVE:  "This prevents downloading of corrupted data."  That's true.  Although the underlying protocols do have checksums, which also catch that.  And that's why retransmissions are happening often.  And he said:  "And regarding Leo's mention of companies using HTTP services for internal sites.  This is true of Google, which pervasively uses sites like http://go/" - no dot in it, just go/ - "for short links and http://b/" - just the letter b/ - "for bugs and more.  But we have a proxy auto config file in our browsers that make sure all such services are sent over a HTTPS proxy to prevent man-in-the-middle attacks.  All that said," he wrote, "I hope browsers continue to support HTTP for years to come.  It is such a versatile protocol, it would be a shame to lose it completely.  Love the show, Austin."



LEO:  There you go.  That's sensible.  That's a sensible answer.  I like it.



STEVE:  Yup.  Austin sounds like a Google engineer, and he makes some great points.  Way back in the early days of this podcast we spent a good deal of time, Leo, you'll remember fondly, exploring the details of low-level hacks and attacks such as ARP spoofing.  For those who don't know, ARP stands for the Address Resolution Protocol.  It's the protocol glue that links the 48-bit physical Ethernet MAC addresses of everyone's Ethernet hardware to the 32-bit logical Internet Protocol (IP) addresses which the Internet uses.  When Internet Protocol data needs to go to someone, it's addressed to them by their IP address, but sent to their Ethernet MAC address.  The ARP table provides the mapping, the lookup between their current IP address and their device's physical Ethernet MAC address.  And it's the ARP protocol that's used to populate and maintain all ARP tables that are on the local Ethernet network.



So here's the point that Austin was making:  If someone can arrange to interfere with the proper operation of this Address Resolution Protocol, it's possible to confuse the data in the network's ARP tables to misdirect and redirect traffic to the wrong Ethernet address; and ARP spoofing is able to cause exactly such misdirection.  So Austin is 100% correct that in an open WiFi setting it would be possible for an attacker to arrange to intercept traffic by, for example, causing clients of a router to believe that the attacker's MAC address was the address of the network's gateway.  And thus they would send all their traffic there instead of to the router.



And indeed the use of TLS and HTTPS would completely prevent any such attacks.  Well, actually it would prevent their success.  The traffic would still get routed to him, but he couldn't do anything with it because it would be encrypted, and he wouldn't have the ability to intercept it.  I mean, it would be much more complex to do so.  He'd have to get a certificate that was trusted into the victim's browser somehow and then perform a full proxying of HTTPS and TLS.  So again, I think we've pretty much established where things stand.  HTTPS, definitely the way to be secure.  But it's not enough because you could have a hinky browser extension and still be in trouble.  But definitely better than not.  Yet HTTP still useful.



JediHagrid tweeted:  "My IT Director at work suggested I message you.  I found a SQL file containing user and employee information on a website, as well as social media secure tokens.  I've tried calling the company, I signed up for LinkedIn premium for the free month in order to message the COO, and I've tried telling Brian Krebs.  Maybe I'm thinking too much into this.  Maybe it's not that big a deal. You're the last person I'm going to notify, and if nothing happens, then I guess nothing happens. The file is still on their server, and you can see it here."



This was a DM, so it was a private connection between the two of us.  He sent me the link.  I've redacted the domain name in the show notes.  And he said:  "The file is" - and again, I redacted the name - "dot sql.  You can search it using Notepad++.  There are .gov customer emails, people who are applying for jobs' addresses, everything in plaintext.  I'm not sure what else to do."



Okay, now, as I said, since the URL he provided which was not redacted was definitely a going national concern, I suggested that he shoot a report off to CISA.  They have a web facility for receiving reports of things like this that people find, at  https://www.cisa.gov/report.  And they also accept email directly at:  report@cisa.gov.  So for Jedi Hagrid and for all of our listeners, I thought that was useful to share, that you could just send email to report@cisa.gov if you stumble across something that you think that is big enough and worthy of coming to our cybersecurity, information security agencies' attention.  And they'll certainly know how to contact the right people and get their attention.



LEO:  Having said that, though, he may have just found a pastebin with personal information in it that's part of a hacker site, you know, I mean, this stuff must be all over; right?



STEVE:  No, it was on their site.



LEO:  Oh, it was on the site of the company?



STEVE:  It's an active running open SQL file that they left exposed.



LEO:  Oh.



STEVE:  Yes.



LEO:  That's, yes.  And the company didn't want to hear about it.  That's...



STEVE:  They, you know, the receptionist said, "A what file?"



LEO:  I don't know what you're talking about, yeah.



STEVE:  Yeah, uh-huh.



LEO:  We get - there is a scam we get.  Fairly frequently emails from people say I found an issue on your website, and I would like a bug bounty.  Please don't ignore this because it's a hazard.  And usually they say something that shows that this is just a generic email, things like, you know, the logins on your website or something are compromised.



STEVE:  Yeah.



LEO:  So, you know, companies get this stuff all the time, and it's a scam.  It's a known scam.  So maybe that's why they're ignoring it.



STEVE:  Yeah.  In this case they shouldn't because it's an...



LEO:  No, it's serious.



STEVE:  An actual live SQL file on their - you know.  And Leo, if I told you the name of the company, you'd be like, what?



LEO:  Oh, okay, got it, okay.



STEVE:  Yeah, yeah.



LEO:  I hope CISA pays attention.



STEVE:  Oh, CISA will pay attention if they get this email.



LEO:  Oh, good.



STEVE:  The CEO will be getting a call from the government.  So anyway, two last things, a quickie.  Simoncroft08 said:  "Hi, Steve.  On SN-936 you talked about multilevel cells in SSD storage.  This is also a concern with USB thumb drives and SD cards.  When used in industrial applications, SD media may be storing programs controlling machines where errors cannot be tolerated.  Industrial environments will have voltage spikes and transients which can flip bits.  Consequently, vendors are now selling specifically SLC, single-level cell storage SD cards for this market.  The capacities are much smaller because of this, typically around 2GB, but that is plenty for most controllers.  Cheers, Simon."  He said:  "PS, the heuristics story was an ouch.  Glad I'm now retired from sysadmin life."



So anyway, I thought this was an interesting angle.  The inherently lower reliability of multilevel cell storage is well understood, and in environments where endurance and reliability trumps maximum storage density, SLC has a much better chance of remaining solid.  And finally, Martin Biggs brings us the "Duh, why didn't I think of that?" Head Smacker of the Week.



Martin writes:  "Hi, Steve.  I'm listening to this week's episode."  And that was last week, Episode 937 of Security Now!.  "You've just described that you can get VirusTotal to check a file before you download it.  The problem with this, though, is that if a malicious site recognizes that VirusTotal is downloading the file" - duh - "then the site can serve VirusTotal a safe version.  Then once you're secure in the knowledge that VirusTotal says the file is safe, the malicious site can happily serve you the malicious file.



"As I cannot find a way of downloading the file directly from VirusTotal" - that is, once they've received it - he says:  "I think the better option is to download the file onto your computer, then upload it to VirusTotal yourself for checking.  This way you can be certain that it is checking the same file that you have.  Thank you for the podcast, and I am glad that you've decided to continue past those dreaded three nines.  Regards, Martin."



And, as I said, that's a head smacker.  You know, I don't know for certain what a download query from VirusTotal looks like and whether they may have taken any precautions to mask their downloading.  But Martin is 100% correct.  As long as the possibility exists that VirusTotal would be receiving and checking a different file than the user downloads, there is no choice but to get it first and provide it to VirusTotal.  So nice catch, Martin, and thank you.



LEO:  Now, let's talk applesauce with Mr. Gibson.



STEVE:  So in a rare occurrence, Apple chose to publicly share a mildly threatening private letter it received last Wednesday which was addressed to Apple's CEO, of course, Tim Cook.  The letter was from a CSAM (Child Sexual Abuse Material) activist by the name of Sarah Gardner.  And Apple must have decided that their best strategy was to get out ahead of this, since they shared Sarah's letter as the preface to theirs, which they also shared in full.  And, I mean, shared publicly.



In terms of the way the future is going to take shape, the biggest thing happening today, I think, in the public policy sphere is the debate and struggle over the tradeoff between privacy and surveillance.  The devices we all now carry with us 24/7 are capable of providing more of either - privacy or surveillance - than anything ever before.  Since this is a significant move on Apple's part, representing a definitive change of stance and policy, I want to share first Sarah's unsubtle letter, followed by Apple's response.



So this is Sarah Gardner.  In the letter that was published, it was redacted, but I'm sure it was written to Tim Apple.  She said:  "Dear Tim.  This exact time two years ago we were so excited that Apple, the most valuable and prestigious tech company in the world, acknowledged that child sexual abuse images and videos have no place in iCloud.  It was an announcement that took bravery and vision - we can live in a world where user privacy and child safety can coexist.



"That is why it was so disappointing when you paused, and then quietly killed this plan in December of 2022.  We firmly believe that the solution you unveiled not only positioned Apple as a global leader in user privacy, but also promised to eradicate millions of child sexual abuse images and videos from iCloud.  The detection of these images and videos respects the privacy of survivors who have endured these abhorrent crimes - a privilege they undeniably deserve.



"I'm writing to let you know that I am part of a developing initiative involving concerned child safety experts and advocates who intend to engage with you and your company, Apple, on your continued delay in implementing critical technology that can detect child sexual abuse images and videos in iCloud.  We are asking you to honor your original intention to detect, report, and remove child sexual abuse images and videos from iCloud; create a robust reporting mechanism for users to report child sexual abuse images and videos to Apple.



"We want to alert you to our presence and our intention to take our very reasonable requests public in one week's time.  Should you want to discuss our campaign over the course of the next week, or after we've launched, I can be reached at this email address.  We welcome the opportunity to discuss these important issues with you and hear what Apple plans to do in order to address these concerns.  Child sexual abuse is a difficult issue that no one wants to talk about, which is why it gets silenced and left behind.  We are here to make sure that doesn't happen.  Kind regards, Sarah Gardner, CEO of the Heat Initiative."



Okay.  So in other words, Tim Cook, you have one week to intercede in our intention to start making loud noises about your refusal to do your duty, she says, or else.



Okay.  So this Sarah Gardner is the former Vice President of External Affairs for the nonprofit organization Thorn, which apparently they want to be one in Apple's side, which works to use new technologies to combat child exploitation online and sex trafficking.  Two years ago, in 2021, Thorn loudly applauded Apple's plan to develop an iCloud CSAM scanning feature.  In a statement to WIRED over this recent event, Sarah Gardner wrote: "Apple is one of the most successful companies in the world with an army of world-class engineers.  It is their responsibility to design a safe, privacy-forward environment that allows for the detection of known child sexual abuse images and videos.  For as long as people can still share and store a known image of a child being raped in iCloud," she wrote, "we will demand that they do better."



Okay.  So the following day, last Thursday, August 31st, Erik Neuenschwander, Apple's Director of User Privacy and Child Safety, replied on behalf of Tim Cook to Sarah Gardner.  He addressed the letter to Ms. Sarah Gardner, CEO, Heat Initiative:  "Dear Ms. Gardner.  Thank you for your recent letter inquiring about the ways Apple helps keep children safe."  Right.  "We're grateful for the tireless efforts of the child safety community and believe that there is much good that we can do by working together.  Child sexual abuse material is abhorrent, and we are committed to breaking the chain of coercion and influence that makes children susceptible to it.  We're proud of the contributions we've made so far and intend to continue working collaboratively with child safety organizations, technologists, and governments on enduring solutions that help protect the most vulnerable members of our society.



"Our goal has been and always will be to create technology that empowers and enriches people's lives, while helping to keep them safe.  With respect to helping kids stay safe, we have made meaningful contributions toward this goal by developing a number of innovative technologies.  We've deepened our commitment to the Communication Safety feature that we first made available in December 2021.  Communication Safety is designed to intervene and offer helpful resources to children when they receive or attempt to send messages that contain nudity.  The goal is to disrupt grooming of children by making it harder for predators to normalize this behavior.



"In our latest releases, we've expanded the feature to more easily and more broadly protect children.  First, the feature is on by default for all child accounts.  Second, it is expanded to also cover video content in addition to still images.  And we have expanded these protections in more areas across the system including AirDrop, the Photo picker, FaceTime messages, and Contact Posters in the Phone app.  In addition, a new Sensitive Content Warning feature helps all users avoid seeing unwanted nude images and videos when receiving them in Messages, an AirDrop, a FaceTime video message, and the Phone app when receiving a Contact Poster.



"To expand these protections beyond our built-in capabilities, we have also made them available to third parties.  Developers of communication apps are actively incorporating this advanced technology into their products.  These features all use privacy-preserving technology  all image and video processing occurs on device, meaning Apple does not get access to the content.  We intend to continue investing in these kinds of innovative technologies because we believe it's the right thing to do.  As you note, we decided not to proceed with the proposal for a hybrid client-server approach to CSAM detection for iCloud Photos from a few years ago, for a number of good reasons."



And I'll just interrupt here to note that this is the technology that Apple had proposed and the public immediately rejected.  There was an almost audible nationwide gasp at the idea of having users' phones containing those hashed libraries of known CSAM images.  The idea of that creeped everyone out, and it became clear that it was a non-starter.  It was also clear that Apple was truly trying to innovate within the bounds of user privacy.



What's clear is that, if the entire task of somehow recognizing such content with high accuracy cannot be done locally on each user's device, then every single image and video and text conversation must be filtered through some external central authority.  And that is clearly far beyond anything that Apple will consider.



So Erik's note continues:  "Having consulted extensively with child safety advocates, human rights organizations, privacy and security technologists, and academics, and having considered scanning technology from virtually every angle, we concluded it was not practically possible to implement without ultimately imperiling the security and privacy of our users."



He said:  "Scanning of personal data in the cloud is regularly used by companies to monetize the information of their users.  While some companies have justified those practices, we've chosen a very different path, one that prioritizes the security and privacy of our users.  Scanning every user's privately stored iCloud content would in our estimation pose serious unintended consequences for our users.  Threats to user data are undeniably growing  globally the total number of data breaches more than tripled between 2013 and 2021, exposing 1.1 billion personal records in 2021 alone.  As threats become increasingly sophisticated, we are committed to providing our users with the best data security in the world, and we constantly identify and mitigate emerging threats to users' personal data, on device and in the cloud.  Scanning every user's privately stored iCloud data would create new threat vectors for data thieves to find and exploit. 



"It would also inject the potential for a slippery slope of unintended consequences.  Scanning for one type of content, for instance, opens the door for bulk surveillance and could create a desire to search other encrypted messaging systems across content types (such as images, videos, text, or audio) and content categories.  How can users be assured that a tool for one type of surveillance has not been reconfigured to surveil for other content such as political activity or religious persecution?  Tools for mass surveillance have widespread negative implications for freedom of speech and, by extension, democracy as a whole.  Also, designing this technology for one government could require applications for other countries across new data types.



"Scanning systems are also not foolproof, and there is documented evidence from other platforms that innocent parties have been swept into dystopian dragnets that have made them victims when they have done nothing more than share perfectly normal and appropriate pictures of their babies.



"We firmly believe that there is much good that we can do when we work together and collaboratively.  As we've done in the past, we would be happy to meet with you to continue our conversation about these important issues and how to balance the different equities we've outlined above.  We remain interested, for instance, in working with the child safety community on efforts finding ways we can help streamline user reports to law enforcement, growing the adoption of child safety tools, and developing new shared resources between companies to fight grooming and exploitation.  We look forward to continuing the discussion.  Sincerely, Erik Neuenschwander, Director, User Privacy and Child Safety."



So I think that one statement from Apple entirely explains their, by now, extremely well-considered position, where they said:  "...and having considered scanning technology from virtually every angle, we concluded it was not practically possible to implement without ultimately imperiling the security and privacy of our users."  In other words, we want to do it.  We tried to do it.  If we could do it, we would do it.  But ultimately we're not willing to compromise our users' privacy and security to make what is ultimately a tradeoff.



Now, the significance of Apple's position, stated as clearly and emphatically as Apple just has, is that it runs directly afoul of the legislation that is currently pending and working its way through the European Union's lengthy ratifying process.  Recall that a little over a year ago, when the updated final draft legislation leaked, the Johns Hopkins cryptography professor Matthew Green tweeted:  "This document" - meaning the EU proposed draft legislation, which was then finalized.  "This document is the most terrifying thing I've ever seen.  It describes the most sophisticated mass surveillance machinery ever deployed outside of China and the USSR.  Not an exaggeration."  And Jan Penfrat of the European Digital Rights advocacy group echoed Matthew's concern.  She wrote:  "This looks like a shameful general surveillance law entirely unfitting any free democracy."



So in our ongoing coverage of this, we were previously able to quote the official positions of the many various third-party messaging systems.  And at the time we noted and commented that Apple was missing from the fray.  I think it's safe to say that they are missing no longer, and that collectively the entire now mobile messaging industry has formed a single unified front.  The question is, what happens next?  What happens when the EU puts their shiny new communications regulations into effect, expected around the end of this year?  Who will be the first to blink?  Will the regulations be present but unenforced?  Will the government or some upstart third party offer surveillance messaging separately?  And if they did, would it matter?  Would anyone use it?  As I noted at the top, this will determine the shape of the future.  What'll that shape be?



LEO:  Yeah.  And of course the question I asked the MacBreak Weekly panel was can Apple continue on its principled stand, or are they going to be forced into some sort of compromise?  And I just don't see them surviving.  I don't know.



STEVE:  It's going to be made illegal, Leo.  And the question is, what happens when "illegal" hits, like, the iceberg?  Does it sink, or does the iceberg melt? 



LEO:  It's so hard not to become discouraged these days.  That's just all I can say.  It's just so hard.  There are so many areas in which we aren't doing what is obviously the right thing.  No one supports child pornography or CSAM.



STEVE:  No.  No. 



LEO:  That's not the issue.



STEVE:  I mean, and even hearing her talk about photos of children being raped, you know, it's just gut-wrenching.



LEO:  It's intentionally, though, gut-wrenching.



STEVE:  Yes.  Yes.



LEO:  And this was what really bothers me about these people is they are, I mean, maybe in their hearts they're doing what they believe the right thing.  But they're using really emotional language, and this is a much more complicated thing.  Yes, no one's in favor of CSAM.  Well, obviously the predators are, but no one else.  No normal person is in favor of it.  But also I don't think Saudi Arabia should be able to ask for pictures of gay men from Apple.  And I don't think China should be able to ask for pictures of Winnie the Pooh from Apple and have their content scanned.  And that's what happens.  You start doing this, that's that slippery slope.



STEVE:  And the other slope is, yeah, in the U.K. legislation, or actually it's the comments from the legislators, they invariably slip in, oh, you know, and terrorism.



LEO:  Oh, yeah, throw that in.



STEVE:  So, you know, and those other people that we're worried about, we want to keep an eye on them, too.



LEO:  Yeah.  That's the problem is one man's terrorism is another person's freedom.  Yeah.  I mean...



STEVE:  So bravo to Apple.  I mean, I am so glad.  And clearly Erik's letter was meant to be, like, to lay it down to preempt this campaign that what's-her-face is threatening to - I just, I just...



LEO:  It's already happening.  If you go to their website, they're already...



STEVE:  I just chuckle at it.  It's like, my god.



LEO:  I think Apple's doing everything they can, but I think it's in the face of a firestorm, and it's going to be very difficult to...



STEVE:  It's going to be law.  It's going to be legislation.  The only thing that will prevent tracking is legislation.  The only thing that could cause any of these companies to buckle would be legislation.  And Leo, what shape will the technology take then?



LEO:  Right.



STEVE:  I mean, we've gone over and over this.  I mean, it's...



LEO:  Well, you know, if you're listening to this show, this just argues that you should probably get an open source computer and phone and make sure you've established and downloaded all the encryption technologies you might think you'd ever want and use it.  But any commercial device is going to be encumbered by these laws.  So you're going to have to roll your own.



STEVE:  I don't think, I can't imagine that open source could be endangered.  I mean...



LEO:  How?



STEVE:  Famously, our government tried to limit the export of crypto to only 40-bit keys because they had some hardware that was able to crack that.



LEO:  Right.  I mean, forget your phone with Android on it or your phone from Apple because these companies have to obey the law.  It turns us all into outlaws.  But we've been there before.  And encryption is, I think, and privacy are pretty darned important.  Oh, well.



STEVE:  And what politician wants to use a phone which is not safe for them?  You know?  Isn't that two-faced?  I mean, you know, oh, yeah, all of my messages are going to go through some third-party filter?  Wait a minute.  No, no.  Not mine, just everybody else's.



LEO:  Just everybody else's.  Thank you very much.  Hey, our intrepid staff, JammerB and Burke, have apparently for the first time ever watched the show, and they're wondering.  Burke came in and said, "That thing over Steve's left shoulder, that white thing, is that new?"  And I said, well, I actually did the research.  I said it appeared between episodes 568 and 569, back in July of 2016.  But I didn't know either.  I said, no, I think it's been there.  And I actually did the work.  What is that thing?



STEVE:  What are we pointing at?  



LEO:  Next to the tape, the mag tape, over your left shoulder.  No, left.



STEVE:  Oh, other left.



LEO:  Other left.  The white-faced box next to the mag tape.



STEVE:  Ah.  That is a field test device for old-school mainframe hard drives.



LEO:  Wow.



STEVE:  You would unplug the hard drive from the mainframe, plug it into this box, and it would do things like exercise the heads and move it back and forth and perform read and write tests.



LEO:  I know that you received it in the week of July 12th, 2016, and it has been in the shot ever since.  Did somebody send you that?  Where did you get it?



STEVE:  I don't remember.  It's got blinky lights.  I probably intended to wire them up so they'd be flashing at one point.  But then, you know...



LEO:  It's only been seven years.  I mean, honestly, let's not go crazy here.



STEVE:  Yeah.



LEO:  Before Episode 999 I want them wired up.  Pretty soon his whole wall will be blinking lights.  Steve, you're a treasure.  You really are an international treasure.  We're very grateful for the work you do.



STEVE:  Bye.



Copyright (c) 2023 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.






GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#939

DATE:		September 12, 2023

TITLE:		LastMess

HOSTS:	Steve Gibson & Jason Howell

SOURCE:	https://media.grc.com/sn/sn-939.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we share some exciting and hopeful news about the UK's Online Child Safety legislation.  What does it suggest for the future?  How was it that Microsoft's super-secret authentication key escaped into the hands of Chinese attackers who then used it to breach secure enterprise email?  What, if any, lessons did Microsoft learn?  Why am I more glad than ever that I'm driving a 19-year-old car after the Mozilla Foundation shared what they learned about all of today's automobiles?  And then, after sharing and exploring some feedback from our listeners, we're going to examine the horrifying evidence that the data stolen from the LastPass breach is being successfully decrypted and used against LastPass users.



SHOW TEASE:  Coming up next on Security Now!, it's me, Jason Howell, filling in for Leo Laporte.  But of course you check out Security Now! because you want to hear Steve Gibson share everything that there is to know this week about, well, security news.  And actually there's news about the UK's Online Child Safety legislation that might be good news for encryption.  Steve tells you all about that.  Why Steve's happier than ever before to be driving a 19-year-old vehicle makes me rethink my purchase of a new vehicle just last year.  Tons of feedback, some really great feedback from you, the listeners and viewers of this show.  And then finally Steve ends things off by sharing why it's pretty clear that the data stolen during that devastating LastPass hack is actually being decrypted and cashed in on.  You won't want to miss it.  Security Now! is next.



JASON HOWELL:  This is Security Now! with Steve Gibson, Episode 939, recorded Tuesday, September 12th, 2023:  LastMess.



It's time for Security Now!.  I am Jason Howell, filling in for Leo Laporte, who is, I think, on the other side of the country right now.  But, you know, you know who's going to be here with me, of course, none other than Steve Gibson.  It's good to see you again, Steve.



STEVE GIBSON:  Jason, it's great to be with you.  Leo is with his mom.



JASON:  That's right.



STEVE:  Who everyone thinks is the cutest thing they have ever seen.	



JASON:  Yeah.  She's been on the network a number of times, and it's always an enjoyable experience when she's on.  So, yeah, he's assisting her.  So, it's good to see you.



STEVE:  So we're here for Security - Security Now!.



JASON:  Security Not Now.



STEVE:  I was going to say Security Nine because it's Episode 939, on 09/12/23.  So, yeah.  This week we're going to share some exciting and hopeful news about the UK's Online Child Safety legislation, and we're crossing our fingers.  And once again the tech press kind of launched a little prematurely, but then got corrected, and there's some fun story there.  And then we're going to explore what that suggests for the future.  We also learn how it was that Microsoft's super-secret authentication key escaped into the hands of Chinese attackers, who were then able to use it to breach the secure enterprise email to some dramatic effect.  Also, what did Microsoft learn from that, if anything?



Also we're going to look at why I am more glad than ever that the car I'm driving is 19 years old, still goes great.  And this is after the Mozilla Foundation shared what they learned about all of today's automobiles from a privacy standpoint.  It's, well, there was only one thing they've ever encountered that was worse, and I don't remember what it is, but it's in the show notes, so we'll get there.  And then, after sharing and exploring some feedback from our listeners, believe it or not, we're going to examine the horrifying evidence that supports the belief that the data stolen from the LastPass breach is being successfully decrypted and being used against LastPass users.



JASON:  Oh, dear.		



STEVE:  As a consequence, this podcast is titled "LastMess."



JASON:  Probably not the last mess from the LastPass mess.



STEVE:  So, yeah.  



JASON:  But holy moly.  That just, yeah, it just keeps on giving, unfortunately.  Bleah.  All right.  Well, we will get there.  That's a whole lot of interesting stuff to talk about today on Security Now!.  Before we get to all the news, we've got a picture that makes a whole lot of sense when you look at it.  I mean, I guess it made sense to someone to do it, but, yeah, this is...



STEVE:  Yeah, we may have shown this before.  But it came up again, and it's just such a perfect picture for the podcast.  I gave this picture the title "The plumber's contract didn't say anything about moving any rocks."  And so we have this, it's not clear which way the water is flowing through this pipe.  But the pipe is trying to go - it comes from offscreen on the left, and it wants to go to a pipe which disappears into the ground over on the right.  Unfortunately, there is a big honking rock, like, blocking the pipe.  And the rock looks like, it doesn't look like it's glued down or anything.  I mean, it looks like it's a big rock.  You can presumably move the rock.  For whatever reason, the plumber did not want to move this rock.  Instead, he did - okay.



Those of us who are old enough to remember "The Three Stooges" will remember that famous plumbing episode where, I don't know, Moe or Curly or somebody was in the bathtub, and they tried to plumb around a leak.  And anyway, so this pipe basically circumnavigates the rock in pretty much as few pieces as possible.  So the water is flowing, and the rock stays where it originally was.  So anyway, one of our fun pictures, I think.



JASON:  I mean, it really does look like that rock could be moved.  But maybe...



STEVE:  Doesn't it?  It's like, what...



JASON:  I mean, maybe it's not movable.  Maybe it's like firmly embedded into the ground in a way that would require it to be destroyed.  And I'm a plumber.  I'm not a rock destroyer.



STEVE:  And you can kind of - you can see behind it.  There's like a rock ledge or like a wall.



JASON:  Yeah.



STEVE:  But this rock does not appear to be part of that.



JASON:  No.



STEVE:  It looks like it's separately sitting on the ground.  And I don't know.



JASON:  I mean, apparently there was some reason why that rock wasn't going anywhere.  Maybe it's a...



STEVE:  Jason, that's exactly where I was - I was about to say, there's a story here.



JASON:  Yes, that we do not know the answer.



STEVE:  And we will never...



JASON:  But I want to know the answer.



STEVE:  We will never know.  Now, given the spread and breadth and ingenuity of our listeners, I wouldn't be surprised if one of them finds this at some point and goes and tries to move it.  So if any of our listeners encounters this particular rock, we want to know.



JASON:  Yes.



STEVE:  Can it be moved?  Or...



JASON:  Yeah.  If you run into this rock, if you find yourself out in the world, and you cross paths with this rock, just do us a favor and see if you can nudge it.  Does it even shake in place?  And if it shakes in place, then we've got our answer.



STEVE:  Give it a kick.  Maybe get out a crowbar if you have one handy.



JASON:  Sure.  



STEVE:  We want to know.



JASON:  Sure.



STEVE:  So just a little follow-up on my current side project.  I was getting ready.  Remember I talked about ValiDrive last week, which arose from one of the SpinRite 6.1 testers having SpinRite rejecting one of his many smart - one of his smart drives.  One of his many thumb drives.  And then we dug into it, and we figured out that this was a fake drive.  So I decided - and Leo was really moved by this revelation, and he said, whoa, you know, we need to know about these.



So I created this little freeware called ValiDrive, which I had expected I would finish with last week.  But just as we were kind of getting ready to get there, one of our listeners showed that a drive which we believe is fake, and I've now, since then, I've absolutely confirmed it, is passing ValiDrive's test when it should not.



So what's happening is ValiDrive jumps around the drive in a random sequence, checking 576 equally spaced out locations to verify that there is actual storage there.  I think what's happening is that some read caching that exists in the chain between my code and the actual USB stick somewhere is generating false positives.  So we're seeing ValiDrive is showing green when it should be showing red.  Anyway, as a consequence I didn't finish, I didn't publish this thing yet.  We'll have it next week.  In fact, I was cheating a little bit, Jason, while you were reading our first advertiser sponsor.  I was wondering why an undocumented command I'm using wasn't being - anyway.  I'm at work on it literally as we speak.



JASON:  You're amazing, Steve.  You're podcasting while you're validating code.  That's amazing.



STEVE:  So it'll be soon, and it'll be working correctly.  Okay.  So who blinked?  What can only be called wonderful and welcome news surfaced in the middle of last week from the UK.  Now, the short version is, the UK appears to have blinked.  And in the face of all secure messaging apps - and now as we've just recently covered, Apple taking their stand, which was the topic of last week's podcast, saying firmly, unh-unh, we're not doing this - everybody said they were going to pull their services from the UK rather than sacrifice the privacy of their users, rather than compromising in any way.  The UK apparently said, "Oh.  Well, we never said that we wanted you to do that."  Uh-huh.  Right.



But of course nothing involving politicians and bureaucracies is ever clean and simple.  And the details here are at least somewhat interesting.  What first happened was that last Tuesday the Financial Times was the one who broke the story.  And then the tech press jumped on it because this was big news.



9to5Mac's headline was:  "Future of iMessage safe in the UK as government backs down on encryption."  Wired carried the headline:  "Britain Admits Defeat in Controversial Fight to Break Encryption."  And their subhead was:  "The UK government has admitted that the technology needed to securely scan encrypted messages sent on Signal and WhatsApp doesn't exist, weakening its controversial Online Safety Bill."  Computerworld's story began with "UK rolls back controversial encryption rules of Online Safety Bill."  CyberScoop headlined their coverage "UK lawmakers back down on encryption-busting 'spy clause'."  And Infosecurity Magazine's headline was "UK Government Backs Down on Anti-Encryption Stance."  All right?  Okay.  So anyway, everybody gets the idea.  This is what all the headlines were.



Unfortunately, much as those were all attention commanding and welcome headlines, none of that was true.  Well, or at least they were all probably deliberate oversimplification and exaggeration click-bait which, predictably, did not sit well with the UK.  The politicians there didn't like those headlines.  So the following day, last Thursday, we see follow-up headlines such as "UK tries to claim it hasn't backed down on encryption at all," and Reuters' headline was "UK has not backed down in tech encryption row, minister says."



And so anyway, here's what Reuters explained because their coverage is short and succinct.  So LONDON, Sept 7 (Reuters):  "Britain will require social media companies to take action to stop child abuse on their platforms, and if necessary work to develop technology to scan encrypted messages as a last resort, technology minister Michelle Donelan said on Thursday."  And we've talked about dear Michelle in the past.  She's the one who's in charge of this.



So Reuters said:  "Platforms including Meta's WhatsApp and Signal have been fighting Britain's Online Safety Bill, which is currently being scrutinized by lawmakers because they say it could threaten the end-to-end encryption that underpins their messaging services.  Junior minister Stephen Parkinson appeared to concede ground to the tech companies' arguments on Wednesday, saying in parliament's upper chamber that the Ofcom" - that's their communications regulator - "would only require them to scan content where 'technically feasible.'"  Okay, now, that's the first time anyone had heard that.  Of course, and then Reuters reminds us that:  "Tech companies have said scanning messages and end-to-end encryption are fundamentally incompatible."  



Okay.  So in other words, "not technically feasible."  So essentially, by admitting and facing reality, this junior minister Stephen Parkinson set off a firestorm.  Was the fire set deliberately?  Did the senior minister set this up to have junior drop this and then hide?  You know?  Maybe I'm being too cynical.  I don't know.  Reuters continues their coverage, saying senior technology minister Michelle Donelan, however, denied the following day, on Thursday, that the bill had been watered down in the final stages before it becomes law.



She told Times Radio:  "We have not changed the bill at all."  Okay, which doesn't seem to be true, but that's what she said.  "If there was a situation where the mitigations that the social media providers are taking are not enough" - and, okay, we already know they won't be.  She said:  "And if after further work with the regulator they still cannot demonstrate that they can meet the requirements within the bill, then the conversation about technology around encryption takes place."  Okay.  Huh?  What does that mean?  Anyway, she said:  "Further work to develop the technology was needed, but added that government-funded research had shown it was possible."  Okay, all of this is new information; right?  So, okay.



So that's the official CYA story from the UK's senior technology minister.  But that's not the whole story because the Online Safety Bill actually was amended, despite the fact that Michelle Donelan just said it had not been changed at all, and she's desperately trying to obfuscate that fact.  So here's the way AppleInsider explained what happened.  They said:  "Despite introducing a clause that means its Online Safety Bill is no longer a concern for Apple, WhatsApp, or users, the UK government is insisting with a straight face that it's still exactly as tough on Big Tech as before.



"On Wednesday, the UK Parliament debated an Online Safety Bill that, in its original form, would have seen Apple, WhatsApp, Signal, and more shutter their messaging and social media services in the country.  Bowing to that pressure," wrote AppleInsider, "the UK regulator Ofcom introduced a face-saving clause that effectively stopped the country's nonsensical demands to break end-to-end encryption.  Except the Conservative government that was pushing for this  against the advice of security experts and even an ex-MI5 head  insists that it has not even blinked."



As spotted by Reuters, UK technology minister Michelle Donelan told Times Radio the same thing I just shared:  "We haven't changed the bill at all.  If there was a situation where the mitigations that the social media providers are taking was not enough, and if after further work with the regulator they still can't demonstrate that they can meet the requirements within the bill, then the conversation about technology around encryption takes place."  Anyway, I don't think anyone's ever going to listen to her or take her seriously again.



But AppleInsider says:  "Ofcom's amendment to the bill said that firms such as Apple would be ordered to open up their encryption only 'where technically feasible and where technology has been accredited as meeting minimum standards of accuracy in detecting only child sexual abuse and exploitation content.'"  In other words, you know, not really.  But we had to say something; right?



So AppleInsider opines, saying:  "There is no technology today that will allow only the good guys to break end-to-end encryption, and there never will be."  Period.  "Consequently," they write, "the Tory government can argue  and is arguing  that no word has been changed in the bill."  Ah, but words have been added, and they neuter the entire - yeah, we didn't change anything.



JASON:  We didn't change anything.  We just...



STEVE:  But we added a few more words there to the end.  



JASON:  ...sprinkled a little on top, yeah.



STEVE:  Yeah, exactly, they just kind of like, oh.  And AppleInsider says they neuter the entire nonsensical and unenforceable plan.  Okay.  So that's the story.  And it's big news because of the critically important precedent that this sets.  For their coverage of this, Wired interviewed Signal's quite outspoken president, Meredith Whittaker, who we've also often quoted here.  So here's what Wired wrote of what Meredith had to say.  They said:  "Meredith Whittaker, president of the Signal Foundation which operates the Signal messaging service said:  'It's absolutely a victory.  It commits to not using broken tech or broken techniques to undermine end-to-end encryption.'"



And then Wired said:  "Whittaker acknowledges that, okay, it's not enough that the law simply won't be aggressively enforced.  But it's major.  She said:  'We can recognize a win without claiming that this is the final victory.'"



Then Wired continues, saying:  "The implications of the British government backing down, even partially, will reverberate far beyond the UK, Whittaker says.  Security services around the world have been pushing for measures to weaken end-to-end encryption, and there is a similar battle going on in Europe over CSAM, where the European Union Commissioner in Charge of Home Affairs has been pushing similar, unproven technologies.  Whittaker said:  'It's huge in terms of arresting the type of permissive international precedent that this would set.  The UK was the first jurisdiction to be pushing this kind of mass surveillance.  It stops that momentum.  And that's huge for the world.'"



And yes.  I believe this is authentically a huge deal.  No one has any real problem with face-saving legislation being created to allow the politicians to tell their CSAM activists that they now have powerful new legislation on the books which will, the moment it can be shown to be technically feasible to do this with the required level of accuracy, compel all encrypted messaging providers to protect the children.  And those politicians can truthfully state that this was the strongest legislation they were able to obtain.  Because indeed it was.



We know that this will in no way pacify Sarah Gardner, whom we talked about last week, after she threatened Tim Cook at Apple with her forthcoming pressure campaign, which starts this week, to compel Apple to perform client-side scanning for known CSAM imagery.  But Sarah appears to be a lost cause.  She has no problem demanding whatever concessions to everyone else's security and privacy might be needed to even incrementally offer improved protection for children.  Everyone is for improving child protection, but there's no way to do that without compromising everyone's security and privacy, including the children's.



So the free world appears to have just taken the first big step toward the resolution of the encryption dilemma.  It's going to be interesting now to see what the European Union does.  You know, maybe they'll also just put the same sort of caveat into their legislation, and everyone can continue ignoring it, which would be wonderful.



JASON:  And then does that end up trickling down here into the U.S. as the U.S. government tries to pursue a no-encryption policy?



STEVE:  You know, what we see with things like the EU and the GDPR, annoying as that GDPR is, it's helping us, I think, and U.S. politicians to say, oh, yeah, I guess, you know, privacy really is good.  You know, maybe we should have some of that here, too. 



JASON:  It's at least forcing more of a conversation and actual, you know, taking a look, yeah, at these issues.



STEVE:  Yeah.  And all of the encryption providers can say, when the U.S. tries to do this, hey, you know...



JASON:  Over there.



STEVE:  Yeah, exactly.  Over there they worked it out.  They're okay with this.  So just cool your jets.



JASON:  Yeah, yeah, interesting.



STEVE:  Okay, so as we know from July, a Chinese-based attacker known as Storm-0558 somehow managed to acquire one of Microsoft's, what was supposed to be a very secret key which then allowed it to forge login tokens which they were able to use to access the private email of OWA and Outlook.com users to, like, serious effect.  In the wake of these revelations, the entire security world has been left wondering exactly how Microsoft had managed to fumble the crucial protection of this very important key.



So last Wednesday, after a series of preliminary blog postings, Microsoft finally provided what may be the conclusion of their investigation, and it did answer some of these questions.  Here's what Microsoft shared.  They wrote:  "Microsoft maintains a highly isolated and restricted production environment.  Controls for Microsoft employee access to production infrastructure include background checks, dedicated accounts, secure access workstations, and multifactor authentication using hardware token devices.  Controls in this environment also prevent the use of email, conferencing, web research, and other collaboration tools which can lead to common account compromise vectors such as malware infections or phishing, as well as restricting access to systems and data using Just in Time and Just Enough Access policies."



Okay, so that paragraph went to explaining, like, all the things they did and designed on purpose to prevent anything like this from ever happening.  And they go on:  "Our corporate environment, which also requires secure authentication and secure devices, allows for email, conferencing, web research, and other collaboration tools."  So in other words, there's the production environment, and then the corporate environment.  They said:  "While these tools are important, they also make users vulnerable to spear phishing, token stealing malware, and other account compromise vectors.  For this reason, by policy and as part of our Zero-Trust and 'assume breach' mindset, key material should not leave our production environment."  That is, the first one that's highly protected.



"Our investigation found that a consumer signing system, okay, a consumer signing system crash in April of 2021" - right, so 2.5 years ago - "a consumer signing system crash in April of 2021 resulted in a snapshot of the crashed process, a crash dump.  The crash dumps, which redact sensitive information, should not include the signing key."  And I'll just put a pin in this here and add that, okay, the crash dumps should not need to redact sensitive information since a signing key should never be in RAM to be dumped after a crash.  They should be in a hardware security module.  What the heck is a signing key ever doing in RAM?  But we'll get back to that.



So Microsoft says:  "In this case, a race condition allowed the key..." - which obviously was present in RAM to be put out in a dump.  They said:  "...to be present in the crash dump."  Okay.  "A race condition allowed the key to be present in the crash dump.  This issue has been corrected."  So if we had a bell, we would ring it.  Ring, you know, ding.  There's the first bug fixed.  "The key material's presence in the crash dump was not detected by our system."  Ding.  They said:  "This issue has been corrected."  Bug number two.



"We found that this crash dump, believed at the time not to contain key material, was subsequently moved from the isolated production network into our debugging environment on the Internet-connected corporate network."  Which we've already said is not as secure as a production environment.  They said:  "This is consistent with our standard debugging processes."  Then, "Our credential scanning methods did not detect its presence."  Ding.  Bug number three.  "This issue has been corrected."  Okay.  So, so far we've got three strikes.



They continue:  "After April 2021" - that's when this crash occurred - "when the key was leaked to the corporate environment" - and remember the key was leaked because it passed through three bugs, none of which should have existed.  They've been fixed now.  Okay.  "When the key was leaked to the corporate environment in the crash dump, the Storm-0558 actor was able to successfully compromise a Microsoft engineer's corporate account.  This account had access to the debugging environment containing the crash dump which incorrectly contained the key.  Due to log retention policies, we don't have logs with specific evidence of this exfiltration by this actor, but this was the most probable mechanism by which the actor acquired the key."



Okay.  So why was a consumer key able to access enterprise email?  Right?  Microsoft explains:  "To meet growing customer demand to support applications which work with both consumer and enterprise applications, Microsoft introduced a common key metadata publishing endpoint in September of 2018.  As part of this converged offering, Microsoft updated documentation to clarify the requirements for key scope validation, which key to use for enterprise accounts and which to use for consumer accounts.



"As part of a pre-existing library of documentation and helper APIs, Microsoft provided an API to help validate the signatures cryptographically, but did not update these libraries to perform this scope validation automatically."  Ding.  Number four.  "This issue," they say, "has been corrected.  The mail systems were updated to use the common metadata endpoint in 2022.  Developers in the mail system incorrectly assumed libraries performed complete validation and did not add the required issuer scope validation.  Thus, the mail system would accept a request for enterprise email using a security token signed with the consumer key."  Ding.  Number five.  "This issue has been corrected using the updated libraries."



Finally:  "Microsoft is continuously hardening systems as part of our defense in depth strategy.  Investments which have been made related to MSA key management are covered in the" - and they have a blog, the storm-0558 blog.  "Items detailed in this blog are a subset of these overall investments."  In other words, as a consequence of this, we are now doing better than we were before.  They said:  "We are summarizing the improvements specific to these findings here for clarity."



We have four bullet points:  "Identified and resolved race condition that allowed the signing key to be present in crash dumps.  Enhanced prevention, detection, and response for key material erroneously included in crash dumps.  Enhanced credential scanning to better detect presence of signing key in the debugging environment.  And finally, released enhanced libraries to automate key scope validation in authentication libraries, and clarified related documentation."



And really, those last two things, the library kind of stuff, synchronization and so forth, that just kind of feels like huge corporation stuff; like yeah, it's understandable that something changed over in Department A, and Department B was using it, but they didn't get theirs refreshed and updated.  I mean, yeah.



Okay.  So as Microsoft has explained this mess-up, a series of five separate and previously undiscovered bugs, all of which they have since found and fixed, were responsible for allowing a key, which should have never left Microsoft, to be exfiltrated by Chinese attackers and then used to remotely compromise what should have been high-security enterprise email.  It's obvious that the key should have never been allowed to leave Microsoft.



But as I said before, what they appear to have conveniently skipped over is why that key ever left the HSM, you know, the Hardware Security Module, which was the only place it should have ever existed.  It's really worth having all of us note that not one of those five flaws would have caused any trouble if that secret key had not been in the system's RAM at the time of that fateful crash.  This is precisely why HSMs exist.  It's why, for example, GRC's code signing keys do not ever exist in any RAM.  They are sequestered in hardware, completely inaccessible to the outside world once installed there, and only able to be used to sign signature hashes.  You cannot query the hardware for the key.  It won't give it to you.  It will only agree to use it until it expires.  You know?  



I've always said that anyone can make a mistake.  In this instance Microsoft made five big ones.  But policy is a different matter.  And Microsoft completely dodged the question of how they could have ever had a policy to allow a crucial signing key to be present in RAM.  That's just never okay, and it got them in trouble here.  So anyway, at least now we understand how it happened.  It was a bunch of mistakes.  I'm actually, I'm impressed that they have all of those buggy things in the pipeline that were meant to work and meant to catch this problem.  I mean, there was an intention to prevent this from  happening.  Again, they're doing an awful lot of work which could have been resolved by having this in a hardware security module in the first place.  So I don't get that.  But the fact that they have those things demonstrated noble intent.



Unfortunately, they were buggy.  And so the key slipped right past all three of them in that chain.  But kind of cool that it was there.  But anyway, at least we know how it happened.  It was a miss.  And they're going to do better.  But really the number one takeaway from this entire debacle is don't have important keys in RAM, ever.  No matter how tightly you believe you have protected them from ever being divulged.  That's why we have HSMs, and they're not even expensive.  I'm sure Microsoft can afford one.  I've got several.



Jason, let's tell our listeners why we're here, and then I'm going to explain why I'm glad my car is as old as it is.



JASON:  I can't wait for you to explain to me why I shouldn't be driving these new cars, or newer cars.  At least newer than 19 years old.



STEVE:  Sorry to tell you, my friend, that Tesla is the worst.



JASON:  I'm not that surprised to hear that.  We're going to talk all about that coming up next.  Here's where the world comes crashing down on us if we have a new car or a newer than 19-year-old car like Steve has, apparently.  We might not be so happy about that after hearing this story.



STEVE:  Well, to be, as we often say, to be forewarned is to be forearmed.  



JASON:  Sure.



STEVE:  I titled this piece "The car I drive is 19 years old, and I'm more glad than ever."  The reason is, it's a car.  It's not a continuously connected mobile entertainment system on wheels.



JASON:  Right.



STEVE:  It's a car.  It does what a car is supposed to do.  It moves my butt from one place to another.  The reason I'm more glad than ever that all my car does is move me, is that I read the research that was just conducted and published last Wednesday by The Mozilla Foundation.  They titled this "It's Official:  Cars Are the Worst Product Category We Have Ever Reviewed for Privacy."



JASON:  Oh, no.



STEVE:  And a subhead might be "25 car brands tested and 25 car brands failed."  So here's what their research uncovered.  I've edited their posting a little bit for the podcast.  They said:  "Ah, the wind in your hair, the open road ahead, and not a care in the world except for all the trackers, cameras, microphones, and sensors capturing your every move.  Ugh.  Modern cars are a privacy nightmare," they wrote.  



"Car makers have been bragging about their cars being 'computers on wheels' for years to promote their advanced features.  However, the conversation about what driving a computer means for its occupants' privacy hasn't really caught up."  As we'll see.  I make this point later.  I think that's exactly the case.  It's happened quickly, and we haven't caught up.



They said:  "While we worried that our doorbells and watches that connect us to the Internet might be spying on us, car brands quietly entered the data business by turning their vehicles into powerful data-gobbling machines.  Machines that, because of all those brag-worthy bells and whistles, have an unmatched power to watch, listen, and collect information about what you do and where you go in your car.



"All 25 car brands we researched earned our 'Privacy Not Included' warning label, making cars the official worst category of products for privacy that we have ever reviewed.  The car brands we researched are terrible at privacy and security.  For one thing, they collect too much personal data - every single one of them.  We reviewed 25 car brands in our research, and we handed out 25 'dings'" - you know, as in a dent in your car - "25 'dings' for how those companies collect and use data and personal information."  That's right.  "Every car brand we looked at collects more personal data than necessary and uses that information for a reason other than to operate your vehicle and manage their relationship with you.



"For context, only 23% of the, by comparison, only 23" - I'm sorry.  "Only 63% by comparison of the mental health apps, and they said "another product category that stinks at privacy we reviewed this year received this 'ding.'  But it was 100% for automobiles."  So cars are worse than mental health apps at managing privacy is their point.



They said:  "And car companies have so many more data-collecting opportunities than other products and apps we use, more than even smart devices in our homes or cell phones we take wherever we go.  They can collect personal information from how you interact with your car, the connected services you use in your car, the car's app - which provides a gateway to information on your phone - and can gather even more information about you from third-party sources like Sirius XM or Google Maps.  It's a mess.



"The ways car companies collect and share your data are so vast and complicated that we wrote an entire piece on how that works.  The gist is they can collect super intimate information about you, from your medical information, your genetic information, to your 'sex life.'"  And they put in parens "(seriously), to how fast you drive, where you drive, and what songs you play in your car - in huge quantities.  They then use it to invent more data about you through inferences about things like your intelligence, your abilities, and your interests."  And get this.  "Most - 84% - share or sell that data."  Okay.  And just to stop here, that was the surprise to me.  It's like, what?  They are data retailers.  They are retailing the data that they're collecting about their drivers.



Mozilla said:  "It's bad enough for the behemoth corporations that own the car brands to have all that personal information in their possession to use for their own research, marketing, or the ultra-vague 'business purposes.'  But then, most (84%) of the car brands we researched say they can share your personal data with service providers, data brokers, or other businesses we know little to nothing about.  And worse, 19 of the 25, (76%) say they can sell your personal data.  A surprising number" - 56% of the total 25 - "also say they can share your information with the government or law enforcement in response to a 'request.'"  And, they write, "Not merely a high bar court order, but something as easy as an 'informal request.'  A very low bar.  Car companies' willingness to share your data is beyond creepy," writes Mozilla.  "It has the potential to cause real harm and inspired our worst cars-and-privacy nightmares.



"And keep in mind," they say, "that we only know what companies do with your personal data because of the privacy laws that make it illegal not to disclose that information, such as California's Consumer Privacy Act.  So-called anonymized and aggregated data can and probably is shared, too, with vehicle data hubs - who are the data brokers of the auto industry - and others.  So while you're getting from point A to point B, you're also funding your car's thriving side-hustle in the data business in more ways than one.



"Next, most" - 92% in their study - "give drivers little to no control over their personal data.  All but two of the 25 car brands we reviewed earned our 'ding' for data control, meaning only two car brands, Renault and Dacia, both owned by the same parent company, say that all drivers have the right to have their personal data deleted.  None of the others do.  While we would like to think this deviation from the norm is one car company taking a stand for drivers' privacy, it's probably no coincidence that these cars are only available in Europe, which is protected by the robust General Data Protection Regulation, the GDPR privacy law.  In other words, car brands often do whatever they can legally get away with to your personal data."



They wrote:  "We could not confirm whether any of them meet our Minimum Security Standards."  They said:  "It's so strange to us that dating apps and sex toys publish more detailed security information than cars.  Even though the car brands we researched each had several long-winded privacy policies - Toyota wins with 12 - we could not find confirmation that any of the brands meet our Minimum Security Standards.  Our main concern is that we can't tell whether any of the cars encrypt the personal information that sits on the car.  And that's the bare minimum.  We don't call them our 'state-of-the-art security standards,' after all.  They're our minimum security standards.  We reached out, as we always do, by email to ask for clarity; but most of the car companies completely ignored us.  Those who at least responded (Mercedes-Benz, Honda, and technically Ford) still didn't completely answer our basic security questions.



"A failure to properly address cybersecurity might explain their frankly embarrassing security and privacy track records.  We only looked at the last three years, but still found plenty to go on with 17 (68%) of the car brands earning the 'bad track record' ding for leaks, hacks, and breaches that threatened their drivers' privacy."



Okay.  So then in the article they provide a car-by-car table of these transgressions, several columns of classification of problems by 25 rows for each of the car brands.  But frankly the table's not worth examining.  They're all really bad.  I think that, as I said, I think we're seeing a classic case of oversight.  This is a recently emerged feature category that no one has yet really focused on.  And, boy, I really do hope that the privacy people take a look at this and say, whoa, what?  It's still the Wild West out there.



So they had then a few additional points.  They said:  "Tesla is only the second product we have ever reviewed to receive all of our privacy 'dings.'  The first was an AI chatbot that we reviewed earlier this year."  They said:  "What set them apart was earning the 'untrustworthy AI' ding.  Tesla's AI-powered autopilot was reportedly involved in 17 deaths and 736 crashes and is currently the subject of multiple government investigations."  So, ouch.



"Nissan earned its second-to-last spot for collecting some of the creepiest categories of data we have ever seen."  They wrote:  "It's worth reading the review in full, but you should know it includes your 'sexual activity.'  Not to be outdone, Kia also mentions they can collect information about your 'sex life' in their privacy policy.  Oh, and six car companies say they can collect your 'genetic information' or 'genetic characteristics.'"  They said:  "Yes, reading car privacy policies is a scary endeavor."



Okay.  Now, I'll just interject here to suggest that the fact that it can be done doesn't mean that it is being done or has ever been done.  These sorts of statements in privacy policies feel like overly broad policies that arise after some wingnut brings an unfounded lawsuit against an automaker.  You know, the firm's attorneys will then overreact by adding a clause stating for example they cannot be held responsible for anything that happens if you're picked up by space aliens while operating their motor vehicle.  This is not meant to suggest that those things will happen if you're abducted.  They're just saying, if they should, then don't go suing us because the policy you already agreed to by driving our car says it's not our fault if something happens.  So, you know.



And with regard to references to sexual activity and sex life, that could refer to the car's GPS recording, that GPS is recording where you're going.  And so if it's used after the fact to infer something about the driver based upon when they went where, well, then again they've included a broad exclusion because of some past lawsuit that they suffered.  So that's probably what that is about.  I hope.  



Mozilla also said:  "None of the car brands use language that meets Mozilla's privacy standard about sharing information with the government or law enforcement, but Hyundai goes above and beyond.  In their privacy policy, it says they'll comply with 'lawful requests, whether formal or informal.'  All of the car brands on this list except for Tesla, Renault, and Dacia signed on to a list of Consumer Protection Principles from the U.S. automotive industry group Alliance for Automotive Innovation, Inc.  The list includes great privacy-preserving principles such as 'data minimization,' 'transparency,' and 'choice.'



"But how many of the car brands actually follow these principles?  Zero.  It's interesting if only because it means the car companies do clearly know what they should be doing to respect your privacy, even though they absolutely don't do it.  This is usually where we'd encourage you to read our reviews, and to choose the products you can trust when you can.  But unfortunately cars aren't really like that.  Sure, there are some steps you can take to protect more of your privacy, and we've listed them all in each of our reviews under 'Tips to protect yourself.'  They're definitely worth doing.  You can also avoid using your car's app or limit its permissions on your phone.



"But compared to all the data collection you can't control, these steps feel like tiny drops in a massive bucket.  Plus, you deserve to benefit from all the features you pay for without also having to give up your privacy."  And they finish:  "We spent over 600 hours researching the car brands' privacy practices.  That's three times as much time per product than we normally spend.  Even so, we were left with so many questions.  None of the privacy policies promise a full picture of how your data is used and shared.  If three privacy researchers" - that's how many they had on this project - "can barely get to the bottom of what's going on with cars, how does the average time-pressed person stand a chance?"



JASON:  No kidding.



STEVE:  "Many people have lifestyles that require driving.  So unlike a smart faucet or voice assistant, you don't have the same freedom to opt out of the whole thing and not drive a car at all.  We've talked about the murky ways that companies can manipulate your consent.  And car companies are no exception. Often they ignore your consent.  Sometimes they assume it.  Car companies do that by assuming that you have read and agreed to their policies before you step foot in their cars.  Subaru's privacy policy even says that the passengers of a car that used connected services have 'consented' to allow them to use - and maybe even sell - their personal information just by being in the car.  So when car companies say they have your 'consent' or won't do something 'without your consent,' it often means what it should."



So as I said, I think this is an area that has until now escaped oversight.  Hopefully, research like this which puts the problem squarely on the map will eventually help that to happen.  And, you know, wow.  We are driving around inside of connected computers, Jason; and they do indeed have sensors galore.  They're connected.  They know where we are.  They know what time it is.  They know everything we do with our computerized entertainment systems, what stations we're listening to.  And you could imagine.



JASON:  Filled with cameras.



STEVE:  Are they not going to monetize that?  Why would we imagine they would not monetize that?



JASON:  Oh, of course.  Yeah, I mean, none of this is that, I mean, it's shocking, but it's also unsurprising; right?



STEVE:  I could say it's saddening.  I don't know that it's shock - you know, just it's saddening.



JASON:  Yeah, yeah, yeah.  It is saddening.  But at the same time like we live in the data economy.  And man, at this point I just feel so beaten down about, like, how my data is used all over the place.  Like it would be easy for me to be, like, well, but I carry around a smartphone, and it does a lot of those things.  And apparently I've said that that's okay because I still have a smartphone and have for years.  But, I mean, you're right, vehicles, you know, have the potential of having cars inside that can be monitored potentially.  And that's just one example.  And it's always following you wherever you go.  So that data has value.  That's the unfortunate reality.



STEVE:  Yeah.  And I guess it seems to me that the minimum we could request is the ability for transparency, to know exactly what data is being collected, and then the ability to ask for it to be deleted.  And, you know, true.  Most drivers never will.  They're not listening to this podcast.  They're just not concerned.  They go, oh, yeah, well, whatever.  But the good news is there is legislation which is moving in this direction, which gives consumers control if they want it.  And at this point the automobile industry is way behind on that score, obviously.



Okay.  So we've got some neat feedback from our listeners.  Someone whose name, I know him from years of transacting with him, his handle is @ramriot.  But his name, he's named himself in Twitter "418," which of course is one of the error codes that can be returned by HTTP.  It's about, I think it's Are You a Teapot?  Anyway, so his is "418:  Tea Ready?"



Anyway, he said:  "Hi, Steve.  This DOM" - meaning the document object model for our web browsers.  "This DOM issue is a tough but old nut, raised again in connection to extensions," which we talked about last week.  "Would this be an opportunity for browser vendors to tighten up the Same-Origin rules for access to form fields?  You know, perhaps make them write-only, immutable objects when accessed cross-origin?"



And so great point and question.  I strongly suspect that the real problem at this point that we face from a practical standpoint is breakage of the already existing quite rich browser extension ecosystem, not to mention the loss of third-party password managers, which do have to poke into every website's forms in order to do their work.  But even more broadly, there would be breakage that would result from any further tightening of access by extensions.  We already saw the uproar that Chrome's rather modest move to the V3 Manifest caused.  And the trouble we were talking about last week was after this move.  So these are things that you can still do, even under the V3 Manifest.  So things remain extremely permissive today.



Just think of the degree to which we must trust today's browser.  And that's - this is really sobering, when you think about it.  The degree to which we must trust today's browser, the browser itself.  Through it passes everything.  Nearly our entire interaction with the world today is through our chosen browser.  Interactive applications have moved or are moving from desktop applications to browser-hosted apps.



All of our usernames and our passwords and the private information we fill out as we interact with anything - the IRS or credit bureaus or loan applications or our doctors' offices or dating sites or any retailer - everything we do today passes through our browser.  And now we're reminded that, if our password managers are able to see everything we do, then so are other extensions which we might trust far less.  Yet here they sit, watching, because they provide some little browser doodle that we like, and we don't want to now live without.



We started off without much concern for browser extension security and privacy many years ago.  But now that we feel we need more security and privacy, it's difficult to take it back without sacrificing the rich feature set and environment that these extensions provide to us.  Both Firefox and Chrome are aware of this problem, which is why both of them allow their users to decide which extensions should be allowed to follow them into the browser's private viewing mode.



And depending upon how extension-laden any user's normal browsing is, it might be worthwhile to consider trimming back on the extensions that are allowed to run, well, I would argue first normally, but also specifically, in Chrome's Incognito mode or browsers' private window modes.  So you're able to then switch into there and have many fewer things watching what you do.  It's not just the browser that's watching.  And as I've said, we have to utterly and absolutely trust the browser because it sees everything we do.  Turns out extensions, which we may trust far less than the browser, who knows where they came from, are able to see what's going on, too.



The alternative, and I've heard that some of our listeners are doing this, would be to reserve a secondary browser, because we certainly don't lack for choice of browsers today,  Everyone can use Firefox or Chrome.  So reserve a secondary browser which is running, you know, maybe only your password manager and nothing else.  Obviously we're trusting our password manager a lot also.  So have a browser that only does that, so it's able to log you in places, but then you don't have any other perhaps sketchy extensions that are doing things.  You may not want to live there without all your extensions, but that's where you might want to go when, you know, you're doing something that is much more confidential.



In any event, we do currently have a problem that's going to require some eventual resolution because right now, as we noted last week, the need to trust every extension with everything we do, just like we do with our password manager and our browser, that's a problem.



Someone posting as "person typing #22," I guess he feels he's rather generic, he said:  "Hey again, Steve.  In last week's SN-938 the tradeoff between security and convenience was mentioned with respect to websites and browser extensions like password managers.  I figured it was worth mentioning that on the Mac and on iOS, I use Apple's universal AutoFill with a compatible password manager."  And he says:  "1Password is an example, and I use KeePassium."  He said:  "For most browsing, I use Firefox.  But to log into banking and similar sites I use Safari on the Mac without ANY extensions.  The OS itself recognizes websites' password fields and allows me to choose a password to autofill from my password manager.  I feel like this provides lots of both security and convenience."



And I completely agree, that is a great solution.  It's sort of a solution using the dual-browser approach, but it also strengthens the isolation from even the password manager by interposing an OS that's as secure as Apple has been able to design, any OS, into the path.  I think that's very nice.



And I had a great comment from a guy whose name I don't know because he uses what is now we're calling X, you know, what used to be known as Twitter, so infrequently that it wouldn't let him log in.  So he posted from his wife's account.  Anyway, whoever he is, he said:  "Hello, Steve.  Thank you for the many great shows.  I've been listening since Episode 1 and was overjoyed to hear you're not stopping at 999.



"With regard to the web extension security research story, I helped develop a web application used internally by the majority of banks in the U.S.  A few years ago we implemented Content Security Policy (CSP) headers.  CSP has a wonderful feature where all violations can be reported to the website to help fix bad rules.  During the initial rollout we reviewed the violations and found that javascript was being injected into our sites by browser extensions.  A few of these extensions seemed to have questionable intentions and were likely installed by adware.  I do not think the research paper's solution of adding a secure input element or alerting the user of nefarious activities is adequate since an extension can alter the source before the DOM is rendered, and therefore could strip out these protections.



"A website can try their best to obfuscate input and output; but at the end of the day, a browser extension can access or modify headers (including cookies), requests, and responses.  It is an ideal position for a man-in-the middle attack while the user thinks their connection is secure and private.  Maybe something similar to CSP or HSTS where a site with sensitive information could request the browser to disable all browser extensions could help protect users.  Of course, sites with advertisements would quickly abuse this power to block good extensions like uBlock Origin, so maybe this would just add complexity to an already impossible problem.  Where I work," he said, "browser extensions have been disabled.  It is annoying that many useful tools are blocked, and yet I cannot argue with their decision."



So wow.  You know, this guy's workplace said nope, sorry, there's just too much danger there.  All browser extensions are disabled.  And, you know, it would be a pain not to have the benefit of a built-in password manager.  But on the other hand, you know, browsers are now universally offering their own built-in password managers, so he's not without autofill.



What was interesting was that the moment I heard this listener talk about some means to allow websites to force-disable extensions, I was reminded that exactly such a proposal was floated through the industry, I'm not exactly sure when, like a month ago.  I don't recall the details, and I don't think we discussed it here.  But I believe I remember Leo, Stacy, and Jeff discussing it on This Week in Google.  And I also recall that many naysayers were suggesting that this was a just a slimy way of disabling ad blockers.  So, you know, right.  To this listener's point, this is all a mess.



Anthony Bosio, he said:  "I think Topics might" - meaning, you know, Google's Topics solution for profiling, providing some information about what people are currently interested in.  He said:  "I think Topics might be DOA.  People are interpreting it and spreading it basically as 'shares your browser history with other sites.'"



Okay.  So let's hope that this is just the initial uninformed reaction to anything that's new.  Given Chrome's massive market power, any technology Google creates and enforces by virtue of foreclosing on all alternatives - which they have said they're going to do next year - is going to succeed because there won't be any alternative to using it.  So I believe Topics cannot be DOA if Google doesn't want it to be.  Then to that we add that Topics is also an extremely benign, non-tracking, privacy-enforcing system.  And I expect that, while it may take some time for the less technical types to catch up and to understand it, it's where the entire industry is going to go.  And to that I say yay.



Barbara said:  "Some downloadable software basically is a stub that phones home to download the rest while installing.  I don't think giving the stub to VirusTotal would be helpful."  And of course she's referring to our previous conversation about sending things to VirusTotal and having it, you know, sending things you download to VirusTotal and having it check on them before you trust them.  And I think Barbara makes a very good point.  As we know, not all software we download is in the entire package.  We're now often seeing a much smaller "installer" that immediately connects back to home base to download the entire package, which is often many modules deep.  The promise is that you select the things you want, and it only downloads those things that you have said you want and intend to use.  But again, it doesn't give you a chance to check all of them against VirusTotal.



Someone tweeting from the handle Skynet said:  "Hi, Steve.  Regarding Martin's 'duh' about VirusTotal being served ostensibly 'clean' files from a malicious source, how would such a site even know who or when they would be doing this to know to send a clean executable?  Do websites even actively monitor who's downloading their content?  And, if so, wouldn't they have to time it so as to know when to give VirusTotal a clean one instead of a malicious one?"  He says:  "I don't get where Martin is getting this idea from.  You'd have to be checking logs of IP addresses; wouldn't you?  And by the time they'd discovered that, 'Oh, look, VirusTotal is trying to get one of our most malicious executables, quick, give them this one instead?'  I don't see how it works.  Even with some redirect link I'd think it would be too late to detect that it was VirusTotal asking for the file; no?  Am I being a doofus for missing a big duh?  Please explain."



Okay.  So no one's being a doofus here.  When I created ShieldsUP! 24 years ago, back in 1999, it was because I knew that the IP address of anyone connecting to my web server was immediately known to the server.  So I was able to return custom webpage results based upon the security I had detected at their connecting IP.  So it would, in fact, be simple for the IP address blocks which have been assigned to known security researchers and VirusTotal to cause different 'clean' software to be delivered on demand.  And we've seen other non-web server examples of this where malware is actually aware of the IP addresses of researchers and acts differently and, like, changes its behavior to be non-malicious when it realizes that known researchers are downloading it or examining it.  So, yeah, this actually does work and can happen.



E. Remington says:  "Hi, Steve.  One email provider people overlook is iCloud.  You can set up your own domain."  Which, by the way, I had no idea of.  He said:  "While iCloud limits you to," he says, "if I remember right, five email addresses, by using a form like 'something plus emailaddress@yourdomain.com," you can have an infinite number of email addresses."  And, he says, RFC 822, which is like the original email, and all of its updates have supported the idea of using the + symbol added to a tag in order to differentiate it, you know, basically that the tag is ignored, and it goes into your main ID for the domain.  So anyway, I'm glad that Remington mentioned iCloud since he's correct that iCloud as an email provider is easily overlooked.  And, you know, certainly they are reputable.



Magnify247 said:  "Steve, with Windows 12 being prepped for 2024, will InControl be updated, or will the current version allow for the version and release application to be locked accordingly?"



Well, you know the old expression about "fool me once."  As we know, the predecessor to "InControl" was "Never10."  I would have named the next one "Never11" except, after Microsoft changed what was clearly their original intention for Windows 10 to be the last Windows ever - which everyone recalls, even if now Microsoft claims it was never what they said - I decided that I had to drop any major version numbering from the utility.  So "InControl" gets to live on without any further name changes.



And since it's all about controlling just a few registry keys which Microsoft officially supports, it should keep working, as long as Microsoft honors those settings.  And since their enterprise users depend upon those, I can't see anything changes.  So I think when Windows 12 occurs next year, InControl will probably continue to work.  And of course, if not, if something changes, I'll update it.  But I don't expect that I'm going to have to.



Christian P. said:  "Hi, Steve.  Just an observation about the concern from the user on the last Security Now! podcast about testing a file directly from VirusTotal, and the risk of the file being swapped based on the source of the test.  You can remove any risk of testing the file directly by getting VirusTotal to download the file, then recheck the hash before you execute it.  VirusTotal reports the SHA-256, and the hash is also in the URL of the result.



"So perhaps a sensible process," he says, "would be to get VirusTotal to download and check.  Then, if that looks largely okay, then download directly to your system and test again with VirusTotal.  The second test should take you to the same page.  VirusTotal doesn't automatically re-test files that have already been submitted.  It just recomputes the hash and looks up the last submitted report.  You do have the option to reanalyze, but there is little point if the hash is the same.  Perhaps if a new scanning has been added since the last test.  Anyway," he says, "great podcast, et cetera."



Okay.  So I wanted to share this since the way the world is evolving, keeping VirusTotal in one's back pocket I think makes a lot of sense.  Christian is right about the way VirusTotal operates.  Before it does anything else, it first calculates the SHA-256 hash of the file that it either downloaded or that the user submitted.  It then checks to see whether that file's hash already exists in its known library of previously scanned files.  And if so, it just returns the previous result.  No need to rescan since it already did.  And the matching SHA-256 signature is absolute proof that the file has not changed from the one that it previously scanned.



And Christian's suggestion of then uploading your own copy of the hopefully identical file that VirusTotal first approved of to see whether they indeed match makes sense.  I wanted to also note that Windows has for a while now had a built-in "hashfile" command which is an actual subcommand of the "certutil" which also allows a user to quickly and easily generate an SHA-256 hash of any file they may wish to check on their own.  So you open a command prompt and say certutil, C-E-R-T-U-T-I-L, space hyphen hashfile space, then the path to the file, then space SHA256 and hit ENTER.  And you'll immediately get the SHA-256 hash of the file which you can then manually check if you wish.



And lastly, InspClousseau says:  "@SGgrc Steve, what DNS services do you recommend for children under 10 to avoid unsafe and unsuitable sites?"  I think that Leo uses and recommends OpenDNS.  They are definitely reputable, they've been acquired by Cisco, and they have a free family use tier which they refer to as their "Family Shield" service.  Using it is as easy as configuring the family's router to use OpenDNS's servers at two very specific IP addresses:  208.67.222.123 and 208.67.220.123.  Once you've done that, you can go to welcome.opendns.com, which will confirm that you're now using their filtered DNS.



And, you know, I suppose if you wanted a bit more proof, you know, that it was working, you could also try going over to Pornhub and see whether that works.  And I would expect you will not be able to go there when you are using the family-safe Family Shield service.  So anyway, a little quickie, and I'm glad that InspClousseau thought to ask.



So Jason, let's share our last sponsor piece and then, wow, spill the beans.



JASON:  No.



STEVE:  Speaking of, yeah, when LastPass finally really screwed this, and we had the podcast "Leaving LastPass," which was where I finally said, okay, there's just no more excuse for this.  You know, a lot of people were nervous about what that meant.  And their nervousness was based on or should have been on their password and the number of iterations that LastPass had been using for them.  And we know that unfortunately not everybody was set to the iteration count of 100,100, which was where we last left it when we last visited this issue for LastPass.  Many people were set to one.  Some were set to 500.  Some were set to 5,000.  It turns out that there is pretty convincing evidence now, which is what we're going to share, that the data that was stolen is being decrypted and is being used to hurt hopefully previous LastPass users.



JASON:  Yeah.  Wow.  All right.  Well, we are going to, like I said, spill the beans on all of this.  And, you know, a couple of spit takes.  Oh, my goodness.  That's coming up next.  The LastPass saga, it continues.  And I feel like every time we check in on it, it's worse than it was before.  Really happy I'm not with LastPass anymore, but I'm super curious to hear all about this.



STEVE:  Nothing I'm going to say is going to disabuse you of that concern, Jason.



JASON:  Indeed.



STEVE:  Any regular listener of this podcast can probably guess that today's title of, as I said, LastMess, will have something to do with LastPass.  So there's growing significant circumstantial evidence which, under the circumstances, is probably the only sort of evidence anyone would ever be able to obtain, which suggests that the encrypted LastPass Vault data, which LastPass had been storing for its many users, and which they famously had exfiltrated from their backup location, is now being and has successfully been decrypted.  I don't mean all of it, I mean incrementally, but that's bad enough; right?  And it's being used by those unknown cyber assailants.



Brian Krebs reported the news of this last Tuesday on his KrebsOnSecurity site under the title "Experts Fear Crooks Are Cracking Keys Stolen in LastPass Breach."  So here is some of what Brian reported.  He wrote:  "In November of 2022, the password manager service LastPass disclosed a breach in which hackers stole password vaults containing both encrypted and plaintext data for more than 25 million users.  Since then, a steady trickle of six-figure cryptocurrency heists targeting security-conscious people throughout the tech industry has led some security experts to conclude that crooks likely have succeeded at cracking open some of the stolen LastPass vaults.



"Taylor Monahan is lead product manager of MetaMask, a popular software cryptocurrency wallet used to interact with the Ethereum blockchain.  Since late December 2022, Monahan and other researchers have identified a highly reliable set of clues that they say connect recent thefts targeting more than 150 people.  Collectively, these individuals have been robbed of more than $35 million worth of their cryptocurrency.



"Monahan said virtually all of the victims she has assisted were longtime cryptocurrency investors and security-minded individuals.  Importantly, none appeared to have suffered the sorts of attacks that typically preface a high-dollar crypto heist, such as the compromise of one's email and/or mobile phone accounts.



"Monahan wrote: 'The victim profile remains the most striking thing.  They truly all are reasonably secure.  They are also deeply integrated into this ecosystem, including employees of reputable crypto orgs, VCs (venture capitalists), people who built DeFi protocols, deploy contracts, and run full nodes.'



"Monahan has been documenting the crypto thefts via Twitter (now 'X') since March of 2023, frequently expressing frustration in the search for a common cause among these victims.  Then on August 28th Monahan said she'd concluded that the common thread among nearly every victim was that they'd previously used LastPass to store their 'seed phrase,' the private key needed to unlock access to their cryptocurrency investments."



Brian Krebs included a screenshot in his coverage of Taylor's tweets.  On August 28th she tweeted:  "The diversity of key types drained is remarkable:  12 and 24 word seeds generated via all types of hardware and software wallets.  Ethereum Presale wallet JSONs.  Wallet.dats.  Private key generated via MEW and others."



And she also noted that the diversity of the chains and coins which had been drained was striking.  So it wasn't as if there was a fault in any specific chain or crypto contract that had been exploited, or service.  There was no common denominator - well, except for LastPass.



Again, Brian writes:  "Armed with your secret seed phrase, anyone can instantly access all of the cryptocurrency holdings tied to that cryptographic key and move the funds anywhere they like.  This is why the best practice for many cybersecurity enthusiasts has long been to store their seed phrases either in some type of encrypted container, such as a password manager; or else inside an offline, special-purpose hardware encryption device such as a Trezor or Ledger wallet.



"Nick Bax, director of analytics at Unciphered, a cryptocurrency wallet recovery company, said:  'The seed phrase is literally the money.  If you have my seed phrase, you can copy and paste that into your wallet.  Then you can see all my accounts, and you can transfer my funds.'



"Bax said he closely reviewed the massive trove of cryptocurrency theft data that Taylor Monahan and others had collected and linked together.  He said:  'It's one of the broadest and most complex cryptocurrency investigations I've ever seen.  I ran my own analysis on top of their data and reached the same conclusion that Taylor reported.  The threat actor moved stolen funds from multiple victims to the same blockchain addresses, making it possible to strongly link those victims.'



"Bax, Monahan and others interviewed for this story say they've identified a unique signature that links the theft of more than $35 million in crypto from more than 150 confirmed victims, with between two and five high-dollar heists happening each month since December 2022."  So in other words, it's not all at once.  It's even following the pattern of brute-forcing something, and the common link of somethings is LastPass.



Anyway, Brian says:  "The researchers have published findings about the dramatic similarities in the ways that victim funds were stolen and laundered through specific cryptocurrency exchanges.  They also learned the attackers frequently grouped together victims by sending their cryptocurrencies to the same destination crypto wallet.  By identifying points of overlap in these destination addresses, the researchers were then able to track down and interview new victims.  For example, the researchers said their methodology identified a recent multi-million dollar crypto heist victim as an employee at Chainalysis, a blockchain analysis firm that works closely with law enforcement agencies to help track down cybercriminals and money launderers."



Okay.  Just to make sure everyone is following this, based on what they were seeing, they followed victims to the bad guys.  Then they looked at all of the transactions on the bad guys' wallets that identified new victims.  Then they looked up the known victims' wallets and were able to find, for example, in this case essentially new victims, in this case an employee at Chainalysis.  Then they went to Chainalysis and said, hey, has anybody had a problem?



Brian writes:  "Chainalysis confirmed that the employee had indeed suffered a high-dollar cryptocurrency heist late last month, but otherwise declined to comment further.  Bax said the only obvious commonality between the victims who agreed to be interviewed was that they had all stored the seed phrases for their cryptocurrency wallets in LastPass."



Bax told Brian Krebs:  "On top of the overlapping indicators of compromise, there are more circumstantial behavioral patterns and tradecraft which are also consistent between different thefts and support this conclusion.  I'm confident enough," he said, "that this is a real problem that I've been urging my family and friends who use LastPass to change all of their passwords and migrate any crypto that may have been exposed, despite knowing full well how tedious that is."



Brian Krebs asked LastPass for any comment, about which Brian wrote.  He said:  "LastPass declined to answer questions about the research highlighted in this story, citing an ongoing law enforcement investigation and pending litigation against the company in response to its 2022 data breach."



Yup.  That's the standard dodge, of course.  LastPass said in a written statement to Brian:  "Last year's incident remains the subject of an ongoing investigation by law enforcement and is also the subject of pending litigation."  Uh-huh.  Perhaps some additional litigation now.



Anyway, they continued:  "Since last year's attack on LastPass, we have remained in contact with law enforcement and continue to do so.  We've shared various technical information, Indicators of Compromise, and threat actor tactics, techniques, and procedures with our law enforcement contacts, as well as our internal and external threat intelligence and forensic partners in an effort to try and help identify parties responsible.  In the meantime, we encourage any security researchers to share any useful information they believe they may have with our Threat Intelligence team by contacting securitydisclosure@lastpass.com."



So Brian's reporting then covers for his readers everything that this podcast already covered for our listeners back at the time, you know, things like how crucial the PBKDF iteration count is for increasing the difficulty of cracking the user's password by brute force.  How the early LastPass users may have originally had iteration counts of 1 or 500, and how despite LastPass moving the defaults upward over time as necessary to keep ahead of brute force cracking capabilities, for reasons that no one has ever explained, many of the original much-too-low original defaults remained in place.



Nicholas Weaver, a researcher at University of California Berkeley, their International Science Institute, and he also lectures at UC Davis, said about brute force attacks that:  "You just crunch and crunch and crunch with GPUs, with a priority list of targets that you target."  He said that a password or passphrase with average complexity, such as "Correct Horse Battery Staple," is only secure against online attacks, and that its roughly 40 bits of entropy means that a graphics card can blow through it in no time.



An Nvidia 3090 can do roughly four million password guesses per second with an iteration count of 1,000.  But that would go down to 8,000 per second with 500,000 iterations, which is why he says iteration counts matter so much.  So a combination of 'not that strong of a password' and an 'old vault with a low iteration count' would make it theoretically crackable.  It would take real work, but the work is worth it given the high value of the targets.



And here's something else that Brian reported which is very interesting.  Brian interviewed one of the victims tracked down by Monahan.  This person is a software engineer and a startup founder who was recently robbed of approximately $3.4 million worth of different cryptocurrencies.  This engineer agreed to tell his story in exchange for anonymity because he's still trying to claw back his losses.  Good luck.  For his reporting, Brian refers to this person as "Connor," which is not his real name.



So Brian writes:  "Connor said he began using LastPass roughly a decade ago, and that he also stored the seed passphrase for his primary cryptocurrency wallet inside LastPass.  Connor chose to protect his LastPass vault with an eight-character master password that included numbers and symbols."  Okay, so, maybe around 50 bits of entropy.  Connor said:  "I thought at the time that the bigger risk was losing a piece of paper with my seed phrase on it.  I had it in a bank security deposit vault before that, but then I started thinking, 'Hey, the bank might close or burn down, and I could lose my seed phrase.'



"Those seed phrases sat in his LastPass vault for years.  Then, early on the morning of Sunday, August 17th, 2023, Connor was awoken" - meaning just recently, right, a couple weeks ago - "Connor was awoken by a service he'd set up to monitor his cryptocurrency addresses for any unusual activity.  Someone was draining funds from his accounts, fast.



"Like other victims interviewed for this story, Connor didn't suffer the usual indignities that typically presage a cryptocurrency robbery, such as account takeovers of his email inbox or mobile phone number.  Connor said he doesn't know the number of iterations his master password was given originally, or what it was set at when the LastPass user vault data was stolen last year.  But he said he recently logged into his LastPass account, and the system forced him to upgrade to the new 600,000 iterations setting."  Which we know as too little, too late.



"He said:  'Because I set up my LastPass account so early, I'm pretty sure I had whatever weak settings or iterations it originally had.'  Connor said he's kicking himself because he recently started the process of migrating his cryptocurrency to a new wallet protected by a new seed phrase.  But he never finished that migration process, and then he got hacked.  He said:  'I had set up a brand new wallet with new keys.  And I had that wallet ready to go two months ago, but had been procrastinating moving things to the new wallet.'"  And I thank Connor for his honesty.



JASON:  No kidding.



STEVE:  Wow.  "Nicholas Weaver, the UC Berkeley researcher, said what we all know, which is that LastPass deserves blame for not having upgraded iteration counts for all users a long time ago, and called LastPass's latest forced updates 'a stunning indictment of the negligence on the part of LastPass.'"  Meaning they could and should have done this years and years ago.



"He said:  'That they never even notified all those with iteration counts of less than 100,000, who are really vulnerable to brute force even with eight-character random passwords or "correct horse battery staple" type passphrases, is outright negligence.'  He said:  'I would personally advocate that nobody ever uses LastPass again, not because they were hacked, not because they had an architecture that makes such hacking a problem, but because of their consistent refusal to address how they screwed up and take proactive efforts to protect their customers.'



"Bax and Monahan both acknowledged that their research alone can probably never conclusively tie dozens of high-dollar crypto heists over the past year to the LastPass breach.  But Bax says at this point he doesn't see any other possible explanation.  He said:  'Some might say it's dangerous to assert a strong connection here, but I'd say it's dangerous to assert there isn't one.  I was arguing with my fianc about this last night,' he said.  'She's waiting for LastPass to tell her to change everything.  Meanwhile, I'm telling her to do it now.'"



So, yeah.  Based upon our own experience with LastPass, anyone who is waiting for them to do anything like take responsibility, which might open them to additional litigation, is probably going to be waiting for quite a while.  As for all of our listeners whose LastPass Vaults were exfiltrated back at the time of the heist, I'm fairly certain that most of us likely have little to fear.  The bad guys obtained a massive treasure trove of encrypted information for more than 25 million individual users.  But decrypting it, while, yes, technically feasible on a one-by-one instance, depending upon passphrase length and iteration count, is still a time-consuming and  massive undertaking.  So attacks on the vault repository are going to be highly targeted.  They want money, plain and simple.  They're not going to waste the cost of decrypting someone's vault unless they're fairly certain that a cash equivalent pot of gold awaits them if they are successful.



Anyone whose iteration count was high - 100,100, which was what we last told all of our listeners to switch to when we talked about it years prior to all of this, or even greater than that, rather than the 1 that it started at, the 500 that it moved to, or the 5,000 that it moved to, all without it ever being done proactively, retroactively, you know, and anybody whose iteration count was high, you know, we set it to 100,100 the last time we talked about it, years before all this happened, you're almost certainly safe.  On the other hand, somebody whose iteration count was one, 500, or 5,000, that's a problem.



Also a high-entropy passphrase, which all of our listeners hopefully had, that would be protection.  But now we know that the bad guys didn't just grab these 25 million vaults and say, oh darn, they're all encrypted.  No, they're attacking them.  They're going after pots of gold.  So if by chance you did own cryptocurrency, you do own cryptocurrency, and your key, your passphrase or wallet key was in LastPass, absolutely don't hesitate.  Create a new wallet, move the cryptocurrency to the new wallet.  It really only looks like these attacks are going to be targeted, and I know that all of our listeners have moved over to Bitwarden now anyway.  But remember, it was the contents at the time of the theft that matters.



JASON:  Right.



STEVE:  Not what we did afterwards.



JASON:  Right.



STEVE:  And, you know, and this guy Bax said he's telling his friends, you know, you really need to go change all the passwords that LastPass was storing for you at the time of the theft.  A pain in the butt, yes.  But, you know.  And again, to me, it seemed really unlikely.  There's 25 million of us.  And if the iteration count was high, and you're just, you know, some random guy, then I don't think you're ever going to get it decrypted.  And, you know, there's no money; right?



JASON:  Right.



STEVE:  They want money.



JASON:  Right.  And the longer it goes not decrypted, the staler that data gets for them.  So probably I'm guessing the lower likelihood that they have to target something that they don't know has some sort of a major payload.  Don't be like Connor.  Don't set half of this stuff up if you think there is really something to save on your end.



STEVE:  And then don't follow through.



JASON:  And then don't follow through.  That's just so heartbreaking.



STEVE:  The other thing is we were just talking about the power of somebody getting your email address because email is how all the password recovery occurs.



JASON:  Yeah, oh, yeah.



STEVE:  So if you do nothing else, change your email password.



JASON:  Yeah.  Yeah.



STEVE:  Just because that's what they will go for.  They'll go for your email password, get into your email, then start clicking on sites that they know you have access to and start doing the whole account takeover routine.



JASON:  Yeah.  Indeed.  All right.  Well, I am happy that I'm no longer with LastPass.  It is disappointing that we're still not getting any sort of like real resolute confirmation, slash, you know, just stating that they totally and completely messed up in so many ways.  I mean, this is just...



STEVE:  They're owned by a hedge fund.  We're never going to find out.



JASON:  Never going to get it.



STEVE:  They no longer care.  You know?  The guys we knew and cared about are long gone.  They cashed out.  It's all about private equity and suck as much money out of their corporate install base as they can.



JASON:  Ooh, man, you put it that way.  Well, Steve, thank you for diving deep into that.  That was fascinating, if not scary.  If you're still with LastPass, I think you have yet another reason to get out.  Thank you, everybody.  We'll see you next time on Security Now!.  Bye-bye.



STEVE:  Bye.



Copyright (c) 2023 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.










GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#940

DATE:		September 19, 2023

TITLE:		When Hashes Collide

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-940.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week, after quickly filling Leo in on last week's two most important pieces of news, guided by some great questions and comments from our listeners, we're going to look into the operating of hardware security modules (HSMs), fast file hash calculations, browser identity segregation, the non-hysterical requirements for truly and securely erasing data from mass storage, a cool way of monitoring the approaching end of Unix time, my plans to leave Twitter, and what I think will be a very interesting deep dive into cryptographic hashes and the value of deliberately creating hash collisions.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is in the house.  So am I.  And we have lots to talk about.  We'll talk about fast file hash calculations, why you don't have to overwrite your hard drive over and over and over again.  Steve explains the issues in wiping your hard drive, whether an SSD or a spinning drive, and then why he's leaving Twitter.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 940, recorded Tuesday, September 19th, 2023:  When Hashes Collide.



It's time for Security Now!, the show where we cover the latest news from the security world with the king of all security news, Steve Gibson.  Hi, Steve.



STEVE GIBSON:  My hand looks pretty big.  I'm going to move it back further so it gets smaller.



LEO:  They told me - so I did some of the shows from Mom's house last week, and they told me my head was too big.  So I'm going to do the same thing next time.



STEVE:  Well, let me tell you, I know.



LEO:  It's easy to get too close, exactly.  Thank you, Jason Howell, for filling in for me last week.  I really appreciate that.



STEVE:  Oh, and by the way, I did want to mention, because you were concerned about audio volume, the last two podcasts have been absolutely uniform.



LEO:  Good.



STEVE:  So whatever it was that was causing those weird volume diminishes, diminishments, diminishings...



LEO:  Diminuendos.



STEVE:  Didn't happen.



LEO:  We fixed them.



STEVE:  Yes.



LEO:  Good.



STEVE:  So.  We have a really fun podcast.  This is going to be a goodie.  As I said before we began recording, we're going to start by quickly filling you in on two important pieces of info.  One I already know you know about because you mentioned it on the previous Sunday show, before your absence last week.  But just need to keep synchronized, and you may have some things to add, as well, of course.  Then, guided by some great questions and comments from our listeners, we're going to look into the operating, the more detailed operation of hardware security modules, since that question came up; the need for fast file hash calculations; browser identity segregation; the non-hysterical requirements for truly and securely erasing data from mass storage; a cool way of monitoring the approaching end of Unix time and you know, maybe the end of the simulation that we're all in.



LEO:  I forgot that was your theory.



STEVE:  You know.



LEO:  I hope to god I'm here in 2038, that's all I can say.



STEVE:  Also my plans to leave Twitter, Leo, and what I think will be a very interesting deep dive into cryptographic hashes and the value of deliberately creating hash collisions, thus Episode 940 of Security Now! is titled "When Hashes Collide."



LEO:  Well, you're going to leave Twitter?  I can't wait to hear that.



STEVE:  Yeah.  Probably.



LEO:  Maybe.



STEVE:  Maybe.  Probably.



LEO:  He's talking about now charging everybody, which I think would cause a grand exodus, to be honest.



STEVE:  And Leo, that is the catalyst, as a matter of fact.



LEO:  Yeah, yeah, yeah.  That's a little - that's going a little...



STEVE:  If that happens, that ends it, I mean, it just - that's the last straw.



LEO:  I think he'll back down on that one.  I can't imagine going through with that.  But anyway, we'll find out.



STEVE:  Because it will kill Twitter.  It will objectively kill it.



LEO:  Yeah, yeah.  Picture of the Week time, Steve.



STEVE:  So, yeah.  We have - it's a two-frame picture.  We have, like, two stuffed-shirt-looking guys, dudes, wearing coats and ties.  And this looks like a real event, maybe from TV, because even though the second shot is the person over whose shoulder we're looking in the first frame, you can see like the brick in the building is the same.  So it's actually, you know, this kid was actually, you know, facing these two stuffed-shirt guys.  And in the background you can also read what looks like Campus Place and Imperial College and then something about Engineering New something.  So anyway, the caption, the question being posed in the first frame is these two stuffed shirts are looking at him saying, "So."  This is a job interview.  "What makes you suitable for this job?"  And this very serious-looking kid who does look sufficiently uncomfortable in his shirt and tie, which is good, he says:  "I hacked your computer and invited myself for this interview."



LEO:  That's one way to get a job.



STEVE:  So there.



LEO:  I love it.



STEVE:  Okay.  So Leo, I needed to make sure you knew about this.  Last week's podcast was titled "LastMess," and that's because...



LEO:  Wait, there's more?



STEVE:  Oh, baby.  Yes.



LEO:  Oh, geez.



STEVE:  Brian Krebs reported the very strong and still mounting evidence that selective decryption of the stolen LastPass vaults has been occurring  and that the cryptocurrency keys stored in the decryption targets have had their funds emptied.



LEO:  I did see that story.  We actually reported on it on TWiT before I left.



STEVE:  Oh, okay.



LEO:  Once LastPass was hacked, then the next question was are they going to be able to decrypt the vaults.



STEVE:  Right.



LEO:  Brute-force decrypt them.  Or is it a nation-state or maybe a competitor who's just trying to smear LastPass?  And so we were just waiting.  And it does seem like there's some evidence that - and what they're doing is, makes sense, cherry-picking the most valuable accounts.



STEVE:  They have no interest in logging into Facebook as Aunt Mabel.



LEO:  No.



STEVE:  You know, these guys, they want one thing:  money.  And so they're going to target LastPass early adopters with low iteration counts because LastPass screwed up by never increasing those proactively.  And the analysts who looked at this noted that these are not neophytes.  These are people in the crypto industry.  In fact, that's how they're being targeted; right?  Because we know that the email address is in the clear.  It's in plaintext.



So by scanning through email addresses in the 25 million-plus vaults that were exfiltrated and stolen, they're able to identify targets who they know by their email address the companies that they're with, and go, hey, there's a good chance that this guy has cryptocurrency.  He may have made the mistake of giving LastPass his keys, didn't know it was a mistake at the time.



LEO:  Well, a mistake I wish I had made, but didn't.  So I can't get into my wallet.  I kept looking in LastPass.  So I don't have to worry about this.  But that's interesting.



STEVE:  And the iteration count is also in the clear, as it must be, because you need to know the count in order to know how to iterate, in order to decrypt the vault.



LEO:  Unbelievable.  Unbelievable.



STEVE:  So they know where the low-hanging fruit is by low iteration counts.  They know who the people are, meaning that they're likely targets.  And then if the iteration count is low, they can put a whole bunch of GPUs on the task of doing a brute-force crack.  Apparently they're able to crack a couple, I don't remember if it was a couple per week or per month.  But it's been, you know, it must be per week because there have been enough of them since the theft.  And apparently about $35 million worth of LastPass users' cryptocurrency has been stolen.



LEO:  And that was the giveaway, or maybe not giveaway, but the clue that it might be because of the LastPass breach.



STEVE:  The one common factor.



LEO:  Because that was what was in common among all these accounts.



STEVE:  Yup.  Yup.



LEO:  And it makes sense.



STEVE:  [Crosstalk] they're security-aware, they were early adopters.



LEO:  They probably had strong passwords, but they were early adopters.  And so the PBKDF2...



STEVE:  Well, actually one guy admitted to only having an eight-character password and a low iteration count.  It was like, ooh.



LEO:  He was the first.



STEVE:  And this guy thought, you know, maybe - this was two months ago, before the news broke.  He said, maybe I should move my cryptocurrency.  He lost $3.5 million.  So he had already set up another wallet.  And he was just procrastinating.  He said, yeah, you know, he's like, he was ready to go.  And he didn't do the transfer.  So the 3.5 million was still sitting in his wallet whose keys were stored in LastPass, and now it's gone.



LEO:  Wow.



STEVE:  So no new home for you.  Okay.  Oh, the other thing I just wanted to make sure, and I know you did talk about this, was the U.K. apparently blinking on this issue of, you know, no-exception CSAM monitoring.  The OFAC or OFAM, their regulatory body, added a clause to this, you know, the online child protection act added the clause "where technically feasible and where technology has been accredited as meeting minimum standards of accuracy in detecting only child sexual abuse and exploitation content."



Well, that wasn't there originally.  That got added.  And so as far as all of the secure crypto companies are concerned, that's their out card.  I mean, they're done.  This is no longer a problem.  The politicians get to say, oh, look, we enacted really like the strongest legislation we could.  And all, you know, Apple and Signal and WhatsApp and everybody is like, okay.  Oh, Telegram.  Okay, yeah, great, glad you did that.  You know, we will be able to keep offering our services because it is not technically feasible to do what you have asked us to do.  And this added clause here late, just before the legislation closed, you know, gives us our out.



So of course now the big question is, what will the EU do?  But the gal that's running Signal was extremely happy, saying that this broke the inertia that had been mounting among the various nation-states about this, and it looks like now it's probably not going to happen.  Again, they'll all claim that they enacted great legislation.



LEO:  Right.



STEVE:  So they can always get themselves reelected.



LEO:  It's perfect.



STEVE:  And it won't matter.



LEO:  It's a perfect solution.



STEVE:  Yes.  Everybody wins.



LEO:  Everybody wins.



STEVE:  Everybody wins.  Okay.  So Chris Smith tweeted.  He said:  "I'll politely ask that you ponder the scalability of your personal HSM and whether it could even keep up with the I/O of your own domain.  Yes, it would be awesome if prevailing Enterprise-class HSMs could commonly do more than just securely hold the keys.  Rainbow crypto accelerator cards paved the way for ubiquitous TLS, and perhaps someday Enterprise HSM will similarly step up."  Okay, now, so Leo, what Chris is referring to is that we learned how it was - we learned last week how it was that Microsoft lost that key.



LEO:  Wasn't that a story?



STEVE:  Yes.



LEO:  And I'm sure you said this.  But credit to Microsoft for being very honest about this.  Very forthright.



STEVE:  Except for one thing.



LEO:  Oh.



STEVE:  So, yes, honest.



LEO:  It was a very detailed technical description of what happened.



STEVE:  It was a technical description.  The fumble was the key was in RAM.



LEO:  Yeah.



STEVE:  When the server crashed.  Which caused a snapshot of the crash dump to then, as a consequence of five bugs...



LEO:  I've seen security experts tweet.  You would be amazed at how many crash dumps contain secrets.  That's the first place you look; right?



STEVE:  Yes.  And of course, and so Microsoft knew this; right?  So they actually had filters, secret filters in the pipeline that this crash dump moved through, and they all missed it.  So they said, so there were like five bugs that all had to be there for this crash dump to make it onto a developer's computer out of the production environment into the operating environment, which then allowed China to come in and grab it.  And but, okay.  So yes.  There were bugs.  But the original sin was that this key was in RAM.  And this brings us to Chris's point because I was saying that even GRC, I have a cute little HSM, which is where my cosigning...



LEO:  What is an HSM?  What is that?



STEVE:  So, yes.  And that's, okay, so that's the issue.  That's a Hardware Security Module.  Okay.  So we actually have two questions here.  We have Chris's.  So he's referring to my comment from last week about Microsoft having lost control of their secret signing key, which then allowed bad guys to sign on their behalf and thus gain access to their customers' enterprise email.  And so the point is it should have never been in RAM.



The question raised is why, in this day and age, where we have hardware security modules - so an HSM is very much like a TPM, a Trusted Platform Module, which we all now have built into our motherboards.  So maybe Microsoft will address this oversight.  But I made the point that even GRC's code signing keys are locked up in HSMs and are thus completely inaccessible.  Crash or no crash, my keys never exist in RAM and are thus never subject to theft.



Now, Chris reminds us that cryptographic signing imposes significant overhead, like compared to none, right, because that's what you're comparing it to.  Either you don't have any signing, or you do.  And if you're going to sign something, there's a computational cost to doing that.  So he said:  "And that the use of custom hardware for accelerating this process has historically been necessary."  Okay.  True.  But the hardware for doing this, and at today's Internet scale, exists today.



Now, it's true that my little USB-attached HSM dongle would melt down if it were asked to sign at the rate that Microsoft probably needs.  But apparently Microsoft already had software doing that on a general-purpose Intel chip running Windows, which is what crashed.  Whereas, to Chris's point, special-purpose hardware can always be faster.  So my point, for whatever reason, was Microsoft was not using such hardware.  And it is difficult to discount that as being anything less than sloppiness.  And it did bite them.



Now, in the show notes, Leo for the next question I have a picture of the HSM that I'm using.



LEO:  Why, it's just a dinky little USB key.



STEVE:  Yes.  That's it.  Okay.  So Rich said:  "Hey, Steve.  Thank you for all your contribution to the security community, and delighted to hear you're extending your tenure to four-digit episode numbers.  In Episode 939 you mentioned that the GRC code-signing key is sequestered in a Hardware Security Module.  I'm familiar with HSMs in finance and banking, but would you consider a technical teardown of what HSMs are, and how they operate without exposing the contained key material, for the Security Now! audience?  Many thanks, Rich, in Sunny Devon, UK."



LEO:  Oh, that's a nice question.  And Devon is not sunny, so okay.



STEVE:  Oh.  So maybe it was a - that's why he made a point - oh.  It'd be like Sunny Seattle.



LEO:  Yeah.  Well, when it is sunny, though, it's such a red-letter day I can see why he might sign it that way.  It's like a big deal.



STEVE:  Yeah, so, yeah.  Okay.  So Rich's question, of course, followed nicely from Chris's.  So the idea is simplicity itself.  The particular key I use came from DigiCert already containing the code signing key I obtained from them.



LEO:  Oh.  That's cool.



STEVE:  Now, that was several years ago, and I've since updated it several times.  What's significant is that the signing keys which this little dongle stores are write-only.  This little appliance - and this is true of HSMs on any scale - lack the ability to divulge their secrets.  They just don't have it.  Once a secret has been written and stored in the device's non-volatile memory, very much like a little memory stick, it can be used, but it cannot be divulged.



And this brings us to the obvious question:  How can the key be used without it ever being divulged?  The answer is that this HSM is not just storage for keys.  It contains a fully working cryptographic microcomputer.  So when I want to have some of my code signed, the signing software outside of the HSM creates a 256-bit SHA-256 cryptographic hash of my code.  Then the resulting 32 bytes of the code's hash are sent into the HSM, where they're encrypted using its internal private key, which never leaves.  It can't leave.  There's no read key command.  All you can do is give it something to do, and it does it.



So the 32 bytes of the hash of my code are sent into the key, and the encrypted result is returned.  So the whole point of this is that all the work is done inside the HSM, by the HSM, and only the encrypted result is returned.  There is no way for the key to ever be extracted from the hardware dongle.  It doesn't have any "read key" command, only "write key."



And just to complete a statement of this entire process, the so-called "signature" that's then attached to my code is the matching public key, that is, the public key that matches the private key inside the dongle.  So it's the matching public key which has been signed by a trusted root certificate authority, in my case by DigiCert.  So when someone downloads and wishes to verify the signature of my code, the signature of its attached public code signing key is first checked to verify that the public key was signed by DigiCert.  So that verifies DigiCert's assertion that this public key belongs to GRC.  Then that public key is used to decrypt that encrypted hash which is the signature.  And the private key can only correctly decrypt the hash if the hash was encrypted by its matching private key which GRC has and which is embedded in that HSM.



So then the rest of the code that was downloaded is hashed with SHA-256, and that hash is compared to the decrypted hash which the private key decrypted.  Only if they match do we know a whole bunch of things.  We know that GRC's matching private key encrypted the code, and that because the hashes match, not one single bit of the code has changed.  Otherwise the hashes would never match.  So although this whole process does have a bunch of moving parts, when everything lines up, it works.



LEO:  It's quite clever.  That's quite clever, yeah.



STEVE:  And it is really clever.



LEO:  I'm thinking about, I don't know, there must be some analogy we can use to describe this.  Essentially the key knows who you are.  But it has a secret of yours that only the key has and will never reveal.  But it can use that secret to create now public keys which it can then send out into the real world.  And then somebody getting that public key - this is how public key crypto works - getting that public key knows that it's you because the public key can only be created by somebody who knows your secret, in effect; right?  That's how I log into my SSH server, which is it has my public key.  It doesn't have my private key.  Only I have my private key.  So when I log in with SSH, SSH says, yeah, this is him because the public key matches the private key, which only I have.  And then they've added this really nice additional feature because you still need this hash of your code to verify the code's not been modified.



STEVE:  Right.



LEO:  So they take that hash.  They I guess hash it, right, with your public key?  Or...



STEVE:  Well, so yeah.  So they take the hash, I take the hash when I'm publishing this.



LEO:  Right.



STEVE:  And I encrypted the hash.



LEO:  Ah.



STEVE:  With my private key.



LEO:  Ah, you've encrypted it.  So it only can be decrypted by you, essentially, by your key.



STEVE:  No, it's encrypted by me.  But it could only be decrypted...



LEO:  By the public key.



STEVE:  ...by the public key.



LEO:  Got it.



STEVE:  And the public key is signed by DigiCert attesting that it's GRC's public key.



LEO:  Very clever.  That's a nice system.  I like that.



STEVE:  It's beautiful, yeah.



LEO:  That's taking kind of the simple public key crypto I use to do my SSH and adding this additional feature that it verifies some code.



STEVE:  Yes.



LEO:  You could do that, when you download code from open source, you download the code, but you'll also download a hash that needs to match the key that is the public key that is posted on the key servers to make sure that that developer did write this code, and it's unmodified.  So we see that all the time without this certificate thing.



STEVE:  Right.  And the nice thing for Windows users is if you right-click on the file and bring up the little property dialog, one of the tabs up there will say "digital signature."  And so you're able to click on it, and it says, you know, this signature's been verified, and it shows who signed it.



LEO:  So it shows DigiCert in this case, yeah.



STEVE:  Yes, yeah.  And so not only does Windows itself, you know, all of Windows' wacky security stuff, see that a valid signature signed the code, which tends to calm down Windows antiviral kneejerk reactions, but then the user is able to easily see who signed it themselves at a given time.



LEO:  Very nice.  Very nice.



STEVE:  It's just a slick system.



LEO:  Public key crypto is amazing.  It's brilliant.



STEVE:  Yeah.  It is so cool.  And Leo, you are going to love this podcast's topic when we get there.



LEO:  Okay.  Okay, good.



STEVE:  Because something more very cool is going to happen.  Okay.  So Clint, and I withheld his last name so as not to embarrass him, he said:  "Hey, Steve.  Longtime Security Now! follower.  My name is Clint.  I wanted to bring to your attention that I just fell for a Verizon wireless fraud call."



LEO:  Oh.



STEVE:  Uh-huh.  He said:  "Since I wasn't aware of Verizon's fraud policy" - meaning that they will never call you - "I responded to a call that said someone tried to log into my account.  In doing so, I actually gave them information they obviously didn't already have.  After calling to talk to Verizon directly, I've changed my account pin and my password.  I'm working on letting Social Security and other services I have know of potential activity that would not be me.  Hopefully nothing comes of this as I changed my info quickly.  But I'm still being proactive on this.  Just thought I would share this in case it's something everyone needs to be aware of new that's going around."



Okay.  So it's not that new.  But this seems significant since Clint explains that he's also a longtime Security Now! podcast listener.  The lesson here is, not knowing whether or not this is Verizon's policy, which really shouldn't factor in, it's "skepticism first."  And never, for one moment, let down our guard.  It's unfortunate because it's not the way humans are built; right?  We tend to believe our senses, which is what stage magicians depend upon.  So when something reasonable and believable appears to happen, most people's first reaction is to believe it and to take some action.  Clint now knows that his first reaction should have been to place his own call to Verizon, rather than accepting a call ostensibly from Verizon.  And in fact he knew it then; right?



But these scammers count on their targets getting caught up in the moment and reacting to an emergency rather than taking the time to think things through.  So I cannot say this often enough, which is why I've decided to say it again.  It doesn't matter how many bits of encryption you have.  The human factor remains the weakest link, and the bad guys are incessantly clever.  I dread the day when I will make such a mistake; you know?  And it could happen, you know, to any of us.  If I could type with my fingers crossed, I would.  So anyway, just another reminder.



Stand_Bold said:  "Greetings from a dedicated listener from Bharat, India."  He says:  "I found your site around 2004, and it's always given me peace of mind.  It was comforting to learn everything was secure then, and thanks to your weekly episodes, I remain confident.  Referring to last week's episode about checksum verification with VirusTotal, here's a handy tip for Windows users unfamiliar with the command line."



Because I had mentioned that you could use certutil-hashfile and the filename space SHA-256.  And somebody subsequently noted that SHA-256 is the default, so you don't actually have to be specific about that.  Anyway, so he said:  "Install the 7-Zip file archiver."  He says:  "This adds a CRC SHA verification option to the Windows right-click menu."  He said:  "I frequently use it to confirm file checksums.  By the way, it's delightful to learn you'll be continuing the show for the foreseeable future.  Many thanks."



Okay.  So this listener's comment caused me to check my own Windows right-click file context menu, and I have a checksumming option there, too.  I wanted to mention that there are a number of simple checksumming add-ons for Windows and Linux desktops.  My file properties dialog has a "checksums" tab.  I was just talking about how it also has a "digital signatures" tab when the file you've right-clicked on has a digital signature.  I always get a "checksums" tab.  And I recalled deliberately installing it many years ago.  And right-clicking on a file and using this tab is the approach I prefer.  I think it makes sense for file checksumming to be, because it's so useful, I mean, it's like a signature; right?  So it now allows you to very quickly make sure that two things are the same.  The shell extension is a solution I've been using for years.  And having something at your fingertips means, as we know, you'll tend to do it more often.



So I have a couple links in the show note.  The one I was using was - it's called "hashcheck."  It's open source.  It has not been updated since 2009, unfortunately.  It's only unfortunate because it works great because it does not support SHA-256, which I think any state-of-the-art checksummer probably should.  It's not that SHA-1, which it does support, is not good enough.  It's that VirusTotal is using SHA-256, and other things you may be wanting to cross-reference with use SHA-256.  So it should, too.  For what it's worth, the source code is published.  He offers the source code.  It's got a Visual Studio project in it.  So it would not be difficult for one of our listeners to update that.



And in fact, if one of our listeners wants to, I will be happy to tell everybody about it because it's really very clean and very cool.  And I'm not going to take the time away from my work to do that because I know that we have some listeners who could.  So the link is in the show notes.  There are also two other solutions on GitHub, both open source, one called HashCheck and one called OpenHashTab.  So the options abound.  And I just wanted to say, hey, you know, it's easy to add that.  Oh, and there is also one for Linux desktops for the rest of our population.



LEO:  We do it by command line, Steve.  We don't need no extensions.



STEVE:  Leo, I know that you get out your remarkable tablet, and you just do the math by hand.



LEO:  I calculate it by hand, of course.



STEVE:  That's what real - real men calculate 256 SHA by hand.



LEO:  Yeah, no, there's plenty of command line tools that'll do that on Linux, and a lot of times in Linux you've got the terminal open, and you just say, well, check the hash on this.



STEVE:  Yup.  Okay.  So finally, before our second break, Chris Shearer said:  "Hello, Steve.  This week you mentioned running a separate browser for segregating extensions.'"  Actually for segregating, like, you know, logged-on cookies and so forth.  I was saying, if you normally use Firefox, you could use Chrome and not have extensions loaded there and so forth.



Anyway, he said:  "Chrome and Edge have a 'profiles' feature where you can run the browser separately as a different identity with different extensions, sessions, cookies, et cetera.  I've used this to maintain sessions to multiple Azure, Gmail, or internal app sessions simultaneously.  This is great for separating dev, prod, test, personal, et cetera.  They even get separate toolbar icons in Windows so you can launch right into the one you want."  Meaning launch the browser with the profile that you want.



He said:  "In Edge and Chrome you click your avatar or icon, and there's a profiles area where there is an option to make a new one, or use a guest profile, either of which will be completely vanilla."  Then he finishes:  "I believe Firefox has similar features, but I don't use it as often."  He says:  "999 and beyond plus."



Okay.  So I had completely overlooked profiles, so I'm glad Chris reminded me and anyone else who might have a use for them.  As always, I strongly prefer using features that are built into our existing systems rather than adding superfluous and unnecessary extras.  And sure enough, in Firefox, entering "about:profiles" took me to my "profiles" page where I saw that I currently have only one profile in Firefox, but there is an option to create more.  So that's very cool.  And thanks, Chris, for pointing that out.  And, you know, and that would allow you to have like your banking or super secure profile where you don't load all the other extensions, all of those requiring trust because they all have access to whatever you do while you're working with your browser.  So a very nice tip.  And Leo, let's tell our listeners why we're here.



LEO:  Yes.



STEVE:  And then I'm going to do a deep dive into what's actually required to securely erase mass storage.



LEO:  That's a good subject.  And we've talked about this before, especially with SSDs.  It's not...



STEVE:  The question is this whole business of overwriting hard drives.



LEO:  Right.  Oh, oh, yeah, yeah.  Okay, good.  Can't wait, yeah.



STEVE:  Uh-huh.  Okay.  So Atli, A-T-L-I, Atli Davidsson, he said:  "Hi, Steve.  Longtime listener and SpinRite owner.  I'm giving away an external USB hard drive and want to securely erase the drive.  A few years back I stumbled upon the format 'P' option to specify how many times to overwrite the drive.  There's a lot of mixed information on how many times to overwrite the drive, so I picked seven, as some article suggested.  What would be the sufficient times to overwrite the drive?  Some say three.  Some say one.  I used three times for my SSD a few days back, and now using seven for a large spinning drive which will probably take days.  I probably should have at least swapped those numbers and used a larger value for the SSD?  Any thoughts?  Looking forward to the next few hundred Security Now! episodes."



Okay.  So as I've mentioned previously, since Atli's problem is going to become increasingly common for many of us, GRC's planned second commercial product to follow SpinRite 7 will be an easy-to-use, fast, and extremely capable secure data erasure utility.  Its sole purpose will be to safely and securely remove all data from any form of mass storage.  It will understand every nuance, strategy, and variation available and will take care of every detail.  But since that future utility is not yet available, here's what needs to be done in the meantime.  And of course even after it's available you could still do this and be doing just as good a job.  But it won't be, you know, that advice won't be updated constantly and automatic, and it won't, you know, you won't necessarily know all the features that each device offers.  So that's where it makes sense, I think, to bring everything together into a single place.



Okay.  But the first thing to appreciate is that there are similarities and differences between the wiping needs for spinning versus solid-state storage.  The differences are largely due to their differing physics for bit storage, and their similarities are largely due to the need for defect management and wear leveling.  So let's look at solid-state storage first.



The first thing to note is that "electrostatic data storage," which is the physics underlying all current solid-state storage, has no notion of remnants or residual data.  That is, in the individual bits.  The individual bits of data are stored by injecting and removing electrons to and from individual storage cells.  This process leaves no trace.  Once those electrons have been drained off, nothing remains to indicate what that cell may have ever contained, if anything.  Consequently, nothing whatsoever is gained from multiple writes to an SSD.  The bigger problem with SSDs is wear leveling.



As I've mentioned before, this process of electrostatically injecting and extracting electrons across an insulating barrier inherently fatigues that barrier over time.  Earlier single-level cell (SLC) storage was rated to have an endurance of at least 100,000 write cycles.  The move to multi-level cell (MLC) storage has reduced that by a factor of 10 to around 10,000.  Okay.  Still, 10,000 writes to every bit is a lot of writing.  So that's why in general solid-state storage is still regarded as being more reliable than mechanical storage.



But the problem is that, if only 5GB of a 128GB drive, solid-state drive, was written over and over, those remaining 123GB would remain brand new and unused, while the first 5GB that was being read over and over and over would be fatiguing due to the stress of being written over and over.  Which is a real thing for this form of solid-state storage.  So the process known as "wear leveling" was quickly introduced.  Wear leveling introduces a "mapping layer" which is interposed between the user, you know, in the outside world, and the physical media.  This is known as the FTL, for Flash Translation Layer.



So now say, just say for example that the first 4K bytes of memory is read from a Flash memory.  The user then makes some changes to that data and rewrites it back to the same place.  What actually happens is that the rewrite occurs to some lesser-used physical region of memory; and the FTL, that flash translation layer, is updated to point the LOGICAL address of that memory region to the PHYSICAL location where the updated memory now actually resides.  This beautifully spreads the fatiguing that's being caused by writes out across the entire physical surface of the memory to prevent the overuse of any highly written memory addresses.



But there's a downside to this when it comes to data erasure.  Remember that the 4K bytes were originally read from the beginning of the drive, at that location.  But then they were rewritten to another location.  And the FTL was updated so that the next read of that same 4K byte region will be from the most recently written location where the updated data was stored.



But what happened to the original 4K byte region that was read first?  When the FTL is updated to point its address to the new location, it becomes stranded.  It is literally unaddressed physical memory.  If the FTL does not have a mapping for it, it cannot be reached from the outside.  It's still there.  It still contains an obsoleted copy of the 4K bytes region's previous data.  But there's no way to access it any longer.  And if there's no way to access it any longer, there's no way to deliberately overwrite it with all zeroes or whatever, to assure that its old data has been permanently eliminated.  You know, you can't get to it.



Now, okay.  You might say, if there's no way to access it, then no one else can either.  But that's probably not true.  Solid state memory controllers, which is where this FTL lives, contain undocumented manufacturing commands to allow external manipulation of the FTL.  So while it's not easy to do, it's likely that there are powers higher up that could perform recovery of all obsoleted data, even after all of the user accessible data had been wiped clean.  And if the information was important enough, it's also possible to desolder the memory chips, which are separate from their controller, and access them directly, thus bypassing the FTL entirely.  And there are videos on YouTube showing how this can be done.



It is for all of these reasons that solid-state mass storage added special "secure erase" features to erase data that cannot be accessed through the normal I/O operations of reading and writing from the outside.  When a secure erase command is sent to solid-state memory, all of the device's memory is wiped clean, whether or not it is currently accessible to its user.  This command is carefully obeyed by today's manufacturers since there's no upside to being found not to be doing this correctly.



So in short, the only thing that needs to be done for any solid-state drive which supports the secure erase feature is to arrange to trigger the command, then sit back and wait for the command to complete.  Once that's done, the device will be free to be discarded or given away to somebody else.  It will be empty.



LEO:  So, I mean, this is a big deal because for a long time we said you can't really effectively erase solid-state because of this slack space problem.



STEVE:  Right.



LEO:  And that's good to know.



STEVE:  You actually can.



LEO:  Yeah.  That's really good to know.



STEVE:  Although you need to know how to issue the command.  There are various levels of command.  Some drives will allow you to securely erase only the unused area so you get to keep the used area.



LEO:  This is why we need Steve to write an app, by the way.



STEVE:  That's why I believe there will be a place for this.



LEO:  Complex, yeah.



STEVE:  And for what it's worth, I would still do one pass of overwriting just because, you know, belt and suspenders.  Do one pass of overwriting, which does not fatigue the device very much when you consider it's got 10,000 available.  And then tell it now erase yourself, and that way, you know, you know for sure.  But what's interesting, Leo, is the story with hard drives is somewhat murkier due to their long history and some bad advice that has outgrown its usefulness.  And that's coupled with some panic, you know, that aliens may have more advanced technology than the NSA, which would enable them to somehow unearth layers of previously written data from long ago.  Okay.



LEO:  Here comes my favorite line of the whole thing.  Go ahead.  Keep going.



STEVE:  So here's what I believe to be true for any hard drives manufactured since around the year 2000.  The earliest hard drives used an extremely simple encoding of user data into magnetic flux reversals.  It was a modified form of frequency modulation, so we of course call it MFM.  Then, by dividing time into smaller quanta, and using the precise timing of flux reversals to convey additional information, 50% more data could be stored using the same total number of flux reversals.  We called that run-length limited coding, or RLL.



LEO:  Oh, I remember those days, yeah.



STEVE:  And that was probably the last time that it might have even been theoretically possible to recover any data that had previously been overwritten.  Everything since then has been hysteria.



LEO:  I love your line in here.  Go ahead, say it.  It's a miracle that it's possible to read back even what was most recently written.



STEVE:  Yes.



LEO:  That they work at all is a miracle.



STEVE:  That's right.  When discussing solid-state memory previously, I was careful to mention that once the electrons had been removed from a storage cell, there was no remnant of the data which had previously been recorded.  No footprint remains.  That's not entirely true for magnetic storage, and that was the original source of the hysteria.



To some degree, magnetism is additive because residual magnetism arises from the alignment of individual magnetic domains, and it's their vector sum that determines the resulting overall magnetic field vector.  Thus the greater the number of domains having the same alignment, the stronger the resulting field.  So when a new magnetic field is imposed upon a ferrous substance which was previously magnetized, the resulting magnetic field will be a composite which is influenced by the field that was there previously.



If the previous field was aligned with the newly imposed field, the result will be a somewhat stronger field.  But if the previous field had the opposite alignment, although the new field will dominate, the result may be ever so slightly weaker as a result of the memory of the previous opposite field still being present.  So in theory, if we assume that a new magnetic field pattern was uniformly written, tiny variations occurring when that magnetic field is later read back might be due to the data that was previously present.  In other words, something remains, however tiny.



That is the source of the concern over the need to overwrite and overwrite and overwrite a hard drive in order to adequately push it back, push what was there back into history, you know, the original data; essentially burying it so that it is beyond any recovery.  None of that has been necessary for the past 20 years.



This was only theoretically possible when the encoding between the user's data and the drive's flux reversal patterns were trivial, as they were back in the early MFM and RLL days.  They have not been trivial for the past 20 years.  Today, and here's your line, Leo, it's truly, and I mean this, a miracle that it is possible to read back even what was most recently written.  It's just astonishing how dense these hard drives have become.  And many drives have become utterly dependent upon their built-in error correction to accomplish even that.  Today's magnetic flux reversal patterns which are recorded onto the disc's surface bear only a distant relationship to the data that the user asked to be recorded.  Where it was once true that bits of user data created flux reversal events on the drive, today large blocks of user data are massaged and manipulated.  There's something called "whitening" is done to them, you know, they're bleached.



LEO:  It's kind of amazing these things work at all.



STEVE:  It is insane, Leo, how far this technology has gone.



LEO:  It's because of the density of the bits; right?  I mean, we're just storing...



STEVE:  Yes.  It's because of the density and because it is so difficult to get any more of them in the same amount of space.  Yet they just, you know, management keeps demanding it, so the engineers go, well, I guess we could run it through a pasta maker in order to...



LEO:  No, let's whiten it.



STEVE:  I don't know, shred it and then recombine it or something.



LEO:  I've got to say, though, a lot of credit to technology.  I mean, you and I both, I'm sure back in 2000, thought there's no way we'll still be using spinning drives in 2023.



STEVE:  People had been predicting the demise of the spinning hard drive for a decade.



LEO:  Since the '90s, yeah.



STEVE:  Yes.  And we're still here, baby.



LEO:  They keep upping the density.  I mean, you can get 20TB drives now.



STEVE:  It's insane.



LEO:  It's unbelievable.



STEVE:  Okay.  So these large blocks of user data are massaged and manipulated, and then the result is what's written back down.  And then somehow this process is reversed.  And after it's reversed, it's unmassaged and dewhitened, and the contrast is turned back up.  And then error correction is used in order to give you back what you asked to have written in the first place.  That's what's actually going on.  So given all of these facts, here is my own plan for GRC's Secure Spinning Disc Data Erasure Program, which will closely follow after SpinRite 7.



Actually, I'm going to use pretty much all of SpinRite 7 in order to create the secure wiping utility.  When GRC's program is asked to securely erase a spinning hard drive, it will use an extremely high quality random data generator to fill the entire drive with noise.  Pure noise.  Then it will go back to the beginning and do it again.  And that's it.  If its user wishes to follow that with a wipe to all ones or all zeroes for cosmetic purposes so that the drive appears to be empty rather than full of noise, they may elect to do so.  But that won't be needed for security.



The theory is that it's always possible to read what was most recently written.  Right?  I mean, that's what a drive does.  So in this case that'll be the noise that was written by the final second pass over the drive.  Then also, by the sheerest, most distant theory of data recovery that really no longer has any support, it might theoretically be possible, and you may need to consult the aliens for this, to get something back from that first noise writing pass.  But all that anyone would then ever get back is noise, the noise that was first written, absolutely unpredictable noise with no rhyme nor reason.  And THAT noise will have absolutely completely obscured any data that was underneath it.



LEO:  Yeah.  I've been saying this for years.  You don't need these 18 writes and overwrites and all that stuff.



STEVE:  No.  That's - yes.



LEO:  That's long gone.



STEVE:  Given today's complexity of the mapping between the data the user writes and the pattern of flex reversals that result, there is no possibility of recovery of any original user data following two passes of writing pure noise on top of that data.  And thanks to SpinRite's speed improvements, this two-pass noise wipe will be able to be done at probably around .25TB per hour.  So, you know, because this utility will be using all of SpinRite 7's technology.  So it will be feasible to wipe multi-terabyte drives in a reasonable amount of time and absolutely know that the data is gone.



Now, spinning hard drives do still also have the problem of inaccessible regions.  When defects are detected after the drive is in the user's hands, those defective regions which contain some user data at the time are spared out, taken out of service, and are no longer accessible.  Although the amount of user data is generally very small, if such spinning drives offer any form of "secure erase" feature, and many do now, GRC's utility will follow its two passes of noise writing with the drive's own secure erase command, which is designed to eradicate data even from those spared out sectors in order to get to all the final little bits.



And one final note before I close this answer:  There are also drives that have built-in AES encryption hardware that's always active.  When the drive comes from the factory it will have an initial default key already established.  So it looks like just any regular drive.  But this means that any user data that is ever written to such a drive will have first passed through the AES cipher using the drive's built-in key, and that everything that's ever written to the physical drive media will be encrypted on the drive, whether it's spinning or it's solid-state.  Both technologies now have this available.



And that means that securely eradicating all recoverable traces of the user's data will be as simple as replacing and overwriting that one key with a new randomly generated key.  At that point nothing that was ever previously written to the drive will ever be recoverable.  And again, GRC's utility will know if the drive offers that feature, and offer it.  And, you know, if you wanted to, belt and suspenders and, I don't know, a parachute, you could still do something that took longer, but there's really no point in doing that.



So to finish answering Atli's question:  Due to the physics of electrostatic storage, that is, SSDs, there's no need to ever perform more than a single erasure pass over any current solid state storage if you did not trust the drive's secure erase command.  Then, if possible, follow that with a secure erase command if one's available.  The drive's manufacturer will typically have a utility that knows how to do that with their own drives.  And due to the physics of electromagnetic storage, it might theoretically be useful to perform a second overwriting pass, and if possible I would do that with random noise.



That's what GRC will do.  And again, follow that with a secure erasure command if one is available.  And, finally, if you're lucky enough to have a drive with built-in encryption, just instruct it to forever forget its old key and obtain another one, and the entire drive will then be scrambled with nonsense that nobody will ever be able to decode.  So now everyone knows everything I know about securely erasing data from today's mass storage devices.



LEO:  That's so huge because you've debunked, I mean, I've tried to tell people this for years, but you've debunked years of misinformation.  Please still run defrag disk optimization on their SSDs and things like that.  I mean, we carry old habits with us, I guess, is the problem.



STEVE:  Yeah.



LEO:  And thankfully people listen to you, so good job.  Good job.



STEVE:  Okay.  So I think I have two more pieces, and then we'll get to our main topic.



Ethan Stone, he said:  "Hi, Steve.  Apropos of yesterday's show, here's a link to the California Attorney General's 7/31 press release regarding a new investigation of 'connected vehicles.'"  Okay.  So Ethan is referring back to last week's coverage of the Mozilla Foundation's revelations which they titled "It's Official: Cars Are the Worst Product Category We Have Ever Reviewed for Privacy."  And Leo, that's another one of our main topics that you missed last week.



LEO:  I read that article.  It was incredible.  Unbelievable.



STEVE:  Okay.  So you know that they're even saying that they can report on the sex life of the occupants of the car.



LEO:  Yeah, unbelievable, yeah.



STEVE:  So, yeah.  So anyway, for what it's worth, the California Privacy Protection Agency, that's the CPPA, enforcement division, they wrote:  "...today announced a review of data privacy practices by connected vehicle manufacturers and related connected vehicle technologies.  These vehicles are embedded with several features including location sharing, web-based entertainment, smartphone integration, and cameras.  Data privacy considerations are critical because these vehicles often automatically gather consumers' locations, personal preferences, and details about their daily lives."



CPPA's Executive Director, Ashkan Soltani, said:  "Modern vehicles are effectively connected computers on wheels.  They're able to collect a wealth of information via built-in apps, sensors, and cameras, which can monitor people both inside and near the vehicle.  Our Enforcement Division is making inquiries into the connected vehicle space to understand how these companies are complying with California law when they collect and use consumers' data."



I think we can presume that, since then, Soltani or his people been informed of Mozilla's connected vehicle privacy research.  That ought to make the CPPA's hair curl, or more likely catch on fire.  So, yeah, Leo, I'm glad you saw that.  And, you know, yikes.



LEO:  Mozilla's been doing great work, I have to say, along these lines, these privacy lines.  That's really good, yeah.



STEVE:  Yeah.  So I have two pieces of miscellany.  The first of two pieces - oh, Leo, this is so cool.  Go to grc.sc/2038, obviously 2038, grc.sc/2038.  Okay.  This is courtesy of our listener Christopher Loessl who sent the link to a super-cool "end of the world as we've known it" Unix 32-bit countdown clock.



LEO:  Oh, that's cool.



STEVE:  It is beautifully done.  Okay.  I just really admire this.  It's a simple, very clean JavaScript app that runs in the local browser.  Since 32-bit Unix time - similar to a 32-bit IP address - can be broken down into four eight-bit bytes, this clock has four hands, with the position of each hand representing the current value of its associated byte of 32-bit Unix time.  It's brilliant.  Since a byte can have 256 hex values ranging from 00 to FF, the clock's face has 256 index marks numbered from 00 to FF.  So, collectively, the four hands are able to represent the current 32-bit Unix time, since each  hand is a byte of that time.  Unfortunately, Unix time is a 32-bit SIGNED value.  Whoops.  So we only get half of the normal unsigned 32-bit value range.



LEO:  Did they do that so they could have negative time?  Why did they...



STEVE:  I just don't think anyone paid attention.



LEO:  They never thought that people would still be using it in 2038, that's obvious.



STEVE:  I don't think they even thought about it.  I think they just said, okay, 32 bits, that's big.



LEO:  That's huge.



STEVE:  And we'll make it an integer.  Unfortunately, unless you said "unsigned integer," it's a signed integer.



LEO:  Right.  Remember, though, this is 1970 or '69, when an eight-bit computer was like, wow.  So a 32-bit value must have seemed infinite.



STEVE:  Well, and that's why, of course, the Internet Protocol, IP, only got 32 bits.  Because they were like, come on, 4.3 billion computers?  We only have two.  We have two right now.



LEO:  Exactly.



STEVE:  One on each end of this packet link.



LEO:  Yeah.



STEVE:  So if Unix time had originally been defined as an unsigned integer, we would have 68 additional years added to the 15 that we still have left.



LEO:  We're almost 90% of the way to the end of time.



STEVE:  Correct.



LEO:  Wow.



STEVE:  Before the time would wrap around.  So anyway, so what's going to happen is when the red hand on this clock gets down to pointing straight down at 80, Unix time goes negative because the high...



LEO:  [Laughing]



STEVE:  It's not good, Leo.  It's not good.  Nope.  What's going to happen is the simulation that we've all been enjoying so much will crunch in on itself.



LEO:  Oh, no.



STEVE:  And that's otherwise known as "game over."



LEO:  Okay.  Yikes.



STEVE:  Yeah.  But this is a wonderful clock.  Grc.sc/2038.  I commend it highly.  It's just a beautiful...



LEO:  So the green hand is seconds.



STEVE:  Well, yes.  And you could see above it the lowest byte of Unix time in hex right above the clock dials...



LEO:  Is the last few digits, yeah, yeah, yeah.



STEVE:  Yes.  And so that's the one you're able to see moving.



LEO:  Right.



STEVE:  And then every time the green one goes around once, the black one, which is the second most significant byte...



LEO:  Yeah, goes up one.



STEVE:  It clicks up by one.



LEO:  Yeah.  And then there's the orange, which is whatever the next four significant bytes.  But there ain't no red.  Actually, the orange is bytes two and three, or three and four.



STEVE:  Is the third most significant.



LEO:  Yeah, yeah.  I'm sorry.  And then the red is the first two.  Yeah, yeah, yeah.



STEVE:  Right.  And so as soon as red gets halfway around, unfortunately halfway because it's a signed value...



LEO:  Close.



STEVE:  ...it hits eight zero.  That means the high bit is on in the 32-bit signed value, and that's a negative event.



LEO:  Wow.  Wow.



STEVE:  And so, again, that's - at that point it's over.



LEO:  We will solve this.  We solved the 1999 problem.  We'll solve this one.



STEVE:  I've got sun coming in off of - look at that.



LEO:  That's apocalyptic.



STEVE:  Not only in the U.K. do we have sun.



LEO:  Yes.



STEVE:  Maybe.  Okay.  So speaking of "game over," yesterday Elon Musk complained that, to exactly no one's surprise, since taking the helm at Twitter, advertising revenue has dropped 60%.  Apparently, everyone except Elon knows exactly why this has occurred.  Major brands have alternatives, and they don't wish to have their ads appearing next to controversial and often obnoxious tweets posted by the raving lunatics who Elon has deliberately allowed back onto his platform.  You know, Elon is all about the free market.  Well, this is the free market in action, Elon.  Objectively, he is managing to destroy Twitter, now renamed "X," which as an aside is about the worst branding imaginable.



LEO:  Oh, yeah.



STEVE:  You know a brand is bad when everyone is continually forced to refer to it as "X, formerly known as Twitter."  In other words, the letter "X" is not sufficiently distinctive to establish a brand.



LEO:  Well, go ahead, do a Google search for "X" and tell me what you find.  I mean, it's nuts.  Whoever thought this was a good idea?  By the way, he's named all his children with X, as well.  So he just loves this letter for some reason.



STEVE:  Wow.  Wow.  He's been casting around for more revenue.  First, as we know, he decided that he'd make people pay for the use of the blue verified seal.  I had no problem whatsoever with losing my years-long verified designation since my entire use of Twitter is to connect with our listeners here.  And no one here needs me to be any more verified than I already am.



But then a month ago, in mid-August, Elon decided that hadn't worked.  So he decided to also take away the use of the TweetDeck UI for all non-paying users.  That was my sole interface to Twitter, so losing it was painful.  I refused to pay this extortionist any money.  For several weeks I attempted to use the regular Twitter web browser user interface.  I hated it in so many ways.  I tried to get accustomed to it, but I finally gave up.



So I decided two weeks ago that because I was using Twitter for the podcast, to broadcast each week's show note link, and now also the Picture of the Week, and to communicate with our Twitter-using listeners, I would pay this man $8 per month - $2 per Security Now! Episode - in return for having access to a user interface that made assembling these show notes, communicating with our listeners, and announcing each week's show note link far less painful.  So for the past two weeks I've been back to using TweetDeck, and joy has returned to the valley.



But still not satisfied with his revenue stream, and apparently unable to combat the storm of automated bot postings - which, by the way, the previous management with a much larger staff was managing to hold at bay - yesterday Elon announced that everyone, yes, everyone, all users of Twitter, would soon need to pay for access to his service.  Elon has now stated that he will be moving Twitter - X - behind a paywall.  And I suspect that if he really goes through with it, this will finally spell the end of Twitter, at least the Twitter that we've known.  Now, many of our listeners have communicated that they don't use Twitter and never will.  Period.  I mean, like three quarters of the listeners of this podcast I'm estimating.



LEO:  Oh, yeah.  It's just tech journalists and crazy people.



STEVE:  Well, I have 64,000 followers who are our listeners.



LEO:  I have a half a million followers.  I don't know if they're listeners, though.



STEVE:  Yeah, well, I know that mine, I'm saying that all of this communication that I share is via Twitter.



LEO:  Right.



STEVE:  So, for example, just yesterday I received a DM from a listener who had just joined Twitter to send me that DM.  He said, "I created an account so I could send this message to you."



LEO:  See?  You see the damage you're doing, Steve?  Get off of that place.



STEVE:  So, yes.  This is not acceptable, and it's apparently about to become even less acceptable.  So I wanted to let all of our listeners know that because I value it so highly, I am arranging for an alternative conduit for our communication.



LEO:  Oh, good.  Thank you.



STEVE:  Now, this sentiment of "I don't use Twitter and I never will" is strong among the majority of our listeners.  I understand that.



LEO:  Well, especially as time goes by.



STEVE:  Yes.



LEO:  It's gotten worse and worse and worse.



STEVE:  Yes.  I also understand, though, that it's actually less specific than Twitter.  The real underlying sentiment is "I don't use social networking, and I never will."



LEO:  Well, that's true, yeah, yeah.



STEVE:  So Twitter was just the specific instance of social networking that was most prominent at the time.



LEO:  Right.  By the way, you may not remember it, but some of our listeners do.  I worked for years to get you on Twitter.  You were very resistant.



STEVE:  Yeah.



LEO:  But this is a decade ago.  But you did not want to get on there.  You were one of them.



STEVE:  I was one of our listeners.



LEO:  Yeah.



STEVE:  And many of them have outlasted me in this.  So the point is, I don't believe that moving to Mastodon, Discord, Bluesky, Threads, LinkedIn, Tumblr, Instagram, or any other...



LEO:  Pebble, got to have Pebble.



STEVE:  I don't even know what that one is.



LEO:  Pebble is T2.  They rebranded to Pebble.



STEVE:  So I don't think that's the solution, you know, for this particular community, for our listeners.  Even the fact that there are so many disjoint and competing platforms now demonstrates the problem.



LEO:  Right.



STEVE:  The answer, I believe, is good old-fashioned email.



LEO:  Oh.  Okay.



STEVE:  We all have it.  We all already have it.  It works, and it does exactly what we all want.  I've previously mentioned that it has been my intention to set up good old tried-and-true emailing lists.  I'll create one for Security Now! and invite this podcast's listeners to join it.  Each Tuesday before the podcast I'll send a link to the week's show notes, a brief summary of the podcast, and the Picture of the Week, much as I've been doing on Twitter.  And anyone will be free to unsubscribe, resubscribe, subscribe multiple accounts, do whatever they want.  Since I very much want the communication to be both ways, I also need a means for receiving email from our listeners while avoiding the inevitable spam.  And I believe I have an innovation that will make that practical, too.



So I just wanted to make certain that everyone knew, in advance of the apparent pending end of Twitter as a practical social media communications platform for us, that I would be arranging for its replacement.  And I already know that this will be good news for the majority of our listeners who are not, who never have been, and never will be participating on Twitter or any other social media platform.  So.



LEO:  Good.  I think email is as universal as you can get.



STEVE:  Yup, it is.  And I think I have an extremely cool way of avoiding incoming spam which all of our listeners are going to get a huge kick out of, even get a kick out of using, just to try it.  So I will have news of that here at some point.



LEO:  Good.



STEVE:  And in the meantime, Leo, our last sponsor, and then I'm going to share something I think everyone's going to find really interesting.



LEO:  Are you going to completely abandon Twitter?  Or what's the deal?  Are you going to stick around until he starts charging?



STEVE:  The only thing that email replacement doesn't create is a community.  And there is, although I don't participate in it, there are a lot of people, you know, who are tweeting @SGgrc and seeing each other's tweets, and so it kind of creates a community there.  But if I, I mean, literally, the only thing I use it for is to talk to our listeners and to receive DMs and to look at my feed.  So, and, you know, I've been complaining.  It's like, I can't also have a Mastodon presence because I just can't hold - I'm already, you know, I'm so spread thin.  I've got GRC's NNTP newsgroups.  I've got GRC's forums.  I would love just to move everybody to email.  So that's my plan.



LEO:  Yeah.  Well, if you ever change your mind, TWiT.social is always open for you, and all your friends are there.



STEVE:  Well, I'm already @SGgrc over at Infosec.social.  I created, I grabbed that because that's what...



LEO:  Yeah, just to have it, yeah, yeah, yeah.



STEVE:  Just in case it ever happens.  But, I mean, there are so many listeners, Leo, who say "I don't do Twitter."



LEO:  Right.  Yeah, well, those people probably wouldn't do Mastodon, either.  So that makes sense.



STEVE:  They won't go anywhere; right.



LEO:  Yeah, yeah.  And now let's get back to Steve and our subject of the day.



STEVE:  When Hashes Collide.



LEO:  Colliding hashes.



STEVE:  So obviously, security and privacy news is important and interesting.  But our listeners always tell me that they really enjoy our occasional deep dives into theory and wish we would do more of those.  That's kept me on the lookout for such opportunities; and this week, in addition to our previous discussion of secure data erasure, I found another one thanks to a question posed by one of our listeners.



Ray V. tweeted:  "In addition to being an avid listener of Security Now!, I'm also a System Administrator for a large public library system in Ohio.  Libraries often struggle with data, being especially sensitive around data related to patrons and patron behavior in terms of borrowing, library program attendance, reference questions, et cetera.  The common practice is for libraries to aggregate and then promptly destroy this data within a short time frame, which is typically one month.



"However, administrators and local government officials, who are often instrumental in allocating library funding and guiding operational strategies, frequently ask questions on a larger timescale than one month to validate the library's significance and its operational strategies.  Disaggregation of this data to answer these types of questions is very difficult and arguably impossible.  This puts people like me, and many others like me, in a tough spot in terms of storing and later using sensitive data to provide the answers to these questions of pretty serious consequence, like what should we spend money on, or why should we continue to exist.



"I'm sure you're aware, but there are many interesting historical reasons for this sensitivity, and organizations like the American Library Association (ALA) and other international library associations have even codified the protection of patron privacy into their codes of ethics.  For example, the ALA's Code of Ethics states:  'We protect each library user's right to privacy and confidentiality with respect to information sought or received and resources consulted, borrowed, acquired or transmitted.'  While I deeply respect and admire this stance, it doesn't provide a solution for those of us wrestling with the aforementioned existential questions.  In this context, I'd be immensely grateful if you could share your thoughts on the technique of 'pseudonymization.'"  And he gives a link to Wikipedia's definition of "pseudonymization," he says, "for personally identifiable information (PII) data."



So Ray's note to me links, as I said, to Wikipedia's article on pseudonymization which begins by explaining:  "Pseudonymization is a data management and de-identification procedure by which personally identifiable information fields within a data record are replaced by one or more artificial identifiers, or pseudonyms.  A single pseudonym for each replaced field or collection of replaced fields makes the data record less identifiable while remaining suitable for data analysis and data processing."



So, okay.  Clearly what Ray is asking for is some way to process the data that is collected, and in such a way that it can be used for statistical analysis and essentially who's doing what, but while also deliberately obscuring the who.  So the first thing that springs to mind is that this is a perfect application for cryptographic hashing.  As we know, a hash is a one-way information-lossy transform that's able to take input of any length and turn it into an output of fixed length.  Today's most popular cryptographic hash we've referred to a number of times on this show already, SHA-256.  It converts anything that's input, like someone's password, into a fixed length 256-bit binary blob.  And unlike a cryptographic cipher, which was designed to be reversible to allow for decryption, hashes are deliberately designed to not be reversible.  They are a one-way scrambling of all of the bits which are input to the hash.  Which is, of course, by the way, why we call it a "hash"; right?  It makes a hash out of whatever we put in.



Okay.  However, the output of today's super-strong hashes can be overly long for some applications.  We don't always need to use all of the bits produced by today's hashes.  And this is what I want to explore today:  the applications for, and the implications of, the deliberate use of fewer bits, far fewer bits, of a cryptographic hash.



Turning to Ray's use, it's not necessary for him to be able to uniquely identify every individual known human and unknown alien throughout the universe.  Ray only needs to aggregate and disambiguate the human patrons to his local library system, which is a far smaller population.  So I wanted to use this question and Ray's particular application to talk about something we have never discussed before, which is the use of only a portion of an entire cryptographic hash.



Those listeners who've been with us from the start may recall that part of the definition of a cryptographically strong hash is that the result is some number of bits where, for any given input block, every single bit of the output will have a 50/50 chance of being a zero or a one.  In other words, there is no bias to any of the individual bits.  And if any one bit of the input is changed, on average, some set, some collection of half of the output bits will invert.  So that's just very cool math.  You know, think about that.  All of the bits are exactly equally relevant.  Every bit has exactly a 50/50 chance of being a zero or a one.  And when a single bit of the hash's input is changed, on average some half of the hash's output bits will be changed.



So one way to think of this is that when something is put into the hash, what comes out is a large pseudorandom number.  During repeated uses, as long as the same thing is put in, the same thing will come out.  And here's the important thing to understand:  Every single bit of that pseudorandom number output is independent of all the other bits.  That bit, any one of the bits doesn't care about any of its neighbors.  And given any input, it will have an equal chance of being a zero or a one.



What that tells us is that there is no need whatsoever to use it all.  The only reason modern cryptographic hashes have become longer is to increase the universe of possible outputs and to keep the hash's internal algorithm being a black box which remains highly resistant to cryptographic analysis.  We never want powerful computers to be able to unscramble the eggs.  But 256 bits is not what Ray needs to uniquely identify his library's patrons.  256 bits gives us 115*10^75 individual combinations.  That's 115 followed by 75 zeroes.  Ray's application is far better served by only using enough bits to reduce the probability of a hash collision to an acceptable level.  A hash collision occurs when two different inputs result in the same output, given the number of bits.



Remember that, counterintuitive though it might be, all of the bits of a good hash's output are equally valuable.  So say that Ray just took the lowest eight-bit byte from the hash of his patrons' aggregated personal information.  Perhaps he just uses their email address, for example.  Since an eight-bit byte can have 256 possible combinations, this would have the effect of dividing the entire population of the library's patrons evenly into 256 groups.  No matter what personal data is put in, a number from zero to 255 will come out.  And they're evenly divided because the hash is good.  If all the bits have an equal chance of being a one or a zero, then given a population, you pour them through the hash, even if you only use eight bits from the output, they're going to be evenly spread among those essentially 256 bins.  They're going to be grouped by those 256 possibilities.



Now, that one byte is colliding because we've ignored the other 31 bytes on purpose in this example; right?  So even though two different email addresses might wind up hashing to the same least significant byte, they're absolutely different everywhere else.  But we're deliberately choosing to ignore all those for the purpose of this exploration.



So presuming that the library has thousands of local patrons, there will obviously be collisions since all of those thousands will get grouped into just those 256 bins.  The downside of using so few bits is that it results in a loss of resolution.  It's no longer possible to track specific individuals since multiple individuals will definitely be sharing the same eight-bit byte identifier.  For some applications, this may be an advantage since it would tend to enhance the privacy of an individual.  With so many patrons grouped into so few bins, thus guaranteed to be sharing bins, no claim could be made by looking at any one bin about the behavior of any one individual due to the presence of these deliberately engineered hash collisions.



So say that eight bits is too few.  I think it probably is.  Ray needs more resolution because he wants to be able to perform some useful data analysis.  He wants to be able to collect meaningful statistical data by keeping his post-anonymized, or rather pseudonymized, patrons more separated, but not provably separate.  He wants to leave a modicum of deliberate uncertainty, introduced by hashing collisions, to keep it from being possible to prove that any one patron had some specific behavior in the past.



So now the question is, how many patrons does Ray's library have?  How many bits from the hash should he use?  How separated should his patrons be?  And what's an acceptably low level of collision to create behavior deniability?  And this, of course, brings us to real-world statistical hash collisions and the famous "birthday paradox."



The birthday paradox refers to the surprising fact that assuming 365 days in a year, we'll ignore leap years for this purpose, and equally probable birthday occurrences throughout the year, which is to say an even distribution of birthdays, the number of people required in any group for there to be a 50% chance that any two of them will share the same birthday is, surprisingly, only 23.  Given 365 days in a year, the number being as low as 23, for there to be a 50% chance of a birthday collision often surprises non-statisticians.



LEO:  Yeah, it's not really a paradox, it's really just an example of innumeracy among people, I guess.



STEVE:  Right.  Right, example.



LEO:  It's not that it's any given date.  It's any two will share the same date.  And there's a big difference.



STEVE:  Yes, yes.  And in fact it's exactly that.  Every pair of people in the group is compared, and a group of 23 people contains 253 pairs of people.  So there is a great deal more opportunity for birthday collision than might have intuitively been assumed.



LEO:  Exactly.  Right, right.



STEVE:  So we have two different goals here with a different property.  Ray wants sufficient resolution among his library's patrons in order to do data processing.  Meaning that he doesn't want there to be a very high likelihood of a birthday paradox collision among any of his patrons.  So to make that determination we rely upon the birthday paradox math.  And in this case, the more bits of the hash we use, the lower the chance of any collision, and the more distinct the categorization.



But as Ray explained in his note, libraries also have the goal of protecting the privacy of their patrons against government or law enforcement queries into the past behavior of any individual patron.  Since privacy protection is enhanced by using fewer bits from the hash, these two goals are conflicting, and a compromise must be reached.  This is made worse by the fact that the birthday paradox does not apply in this second instance of privacy and plausible deniability.  Here, the question is not what is the chance that any two patrons will share the same hash bin.  The question is what is the chance that someone else shares the same bin as the targeted individual.  And there, unfortunately, the chances are far lower for any given number of hash bits.



So let's put some numbers to this.  I found a nice webpage containing a calculator for these birthday paradox questions.  It allows the input of the number of items, which would be the number of patrons, and the number of buckets or bins.  I've also made it this week's GRC shortcut of the week, so you can get to it, grc.sc/840.  I'm sorry, I did say 840.  It's Episode 940.  Oops.  Typo.  So the link is in the show notes, and I will fix this to be a shortcut at 940 as soon as we're done recording this.  So GRC.sc/940.  And in the meantime you can also get to it at 840 because I fumble-fingered on my typo, on the shortcut.



Okay.  So going to this page, if we test it first with the classic birthday paradox, by putting 23 items into 365 buckets, that web page tells us that the chance of a collision between any two is 50.73%, just over 50%, so that looks right.  If we take one person away from the group, the page shows that the chance of collision drops to 47.57%.  So this confirms that 23 people is the minimum number needed to reach a 50% chance of collision.  



I have no idea how large Ray's patron population may be.  So let's use 5,000 patrons as a working number.  That sounds like it's probably in the ballpark.  So how many hash bits would we need to use for there to be a 50% chance of a collision among that population of 5,000 patrons?  When you think about it, that would provide very adequate separation among his patrons, for there to only be a 50% chance that any two of them would collide.



So poking the numbers into that page, and I did some experimenting because you have to know what powers of two, how many bins various powers of two are, it turns out that using just 24 bits, three bytes, of the hash's value, instead of all 256 bits, will result in a 52.5% chance of any two patrons from a population of 5,000 having the same 24-bit identifier when their personally identifiable information is hashed.  Now, it turns out three bytes is also sort of cool because those 24 bits, instead of being divided into three bytes of eight bits, can be divided into four groups of six bits.  That's also 24; right?  And six bits can be mapped into a 64-character alphabet of printable characters.  This means that each of the library's 5,000, more or less, patrons can have their personally identifiable information hashed and turned into a four-character printable token which probably, but not certainly, uniquely identifies them within that 5,000 patron population.



This protects their privacy, allows their pseudonymous history to be collected over time into bins identified only by those tokens, with only a 52.5% chance that any two of the 5,000 patrons will be assigned the same four-character token, thus having their individual histories mixed.  This is low enough to provide sufficient inter-patron separation, and also high enough to introduce sufficient uncertainty into the data collection system to prevent the token-tagged history from being legally evidentiary.  That is, you cannot prove that all of any person's tokens are only from their borrowing habits because the system is deliberately fuzzy just enough to preserve the value of statistics, but to keep it from being useful as legal evidence.



So the goal of this exercise was to highlight another less-appreciated and extremely cool aspect of cryptographically strong hashes, which is that all bits are created equal, that there can be some very useful real-world applications for deliberately using some many fewer number of bits, and that which bits are used makes absolutely no difference.



LEO:  Well.  Very nice.  And it's cool that somebody put this hash collision calculator online.



STEVE:  Yeah.  Isn't that neat?  Yeah.



LEO:  Yeah.  It's very fun.



STEVE:  And the thing I did not mention is that I was using powers of two, right, because I'm just using some number of bits.  The problem is, when you get up like to 24 bits to be the number of bins, and then if you want to add one more bit, you go from 24 to 25, well, that, you know, that's a big jump.  That's a big jump in value.  So the other way to do this, what I'm trying to say is you might want a number between what binary bits can provide you.  And you can get that by taking the entire 256-bit value and doing a long division by the number that you want.



So say that you didn't - you'd want it somewhere between, just to make it easy, 128 and 256.  That's seven bits or eight bits; right?  Say that you wanted for some reason exactly 150, which falls between 128 and 256.  Well, you can get 150 equally distributed if you take the entire large hash and divide it by 150.  You basically take it modulus 150.  And there you'll get a result between zero and 149 equally distributed values.  So anyway, just a little addition to that.  But anyway, a very cool application for a cryptographically strong hash where you don't want it to be, you know, to uniquely identify one from a population.



LEO:  Right.



STEVE:  The way we do if we're signing code, for example.



LEO:  Right.  People are saying, "We were told there'd be no math in this show."  Well, you were told wrong.  I'm sorry.  That's exactly what this show is all about.  Very interesting stuff.  You know, if some of that went over your head, the show notes are a great place to go.  Steve's got them at GRC.com.  And you can actually read what Steve just said.  Transcripts will also help.  And then work it out on your own because it's worth doing, and I think it'll become clear if you do.  But sometimes it's hard just to hear it and understand it.



Copyright (c) 2023 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.






GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#941

DATE:		September 26, 2023

TITLE:		We Told You So!

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-941.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we're chock full of questions!  Why is my new ValiDrive freeware not published yet?  Why did Apple quietly remove PDF rendering from the Mac after 39 years?  Has the NSA been hacking China?  What mistake did Microsoft recently make that would require the use of a bigger hard drive?  Why did Signal just announce their use of post-quantum crypto?  What's the big hurry?  Is it possible to create a new web browser from scratch?  And if not, why not?  Does public key crypto really go both ways?  Can pure math generate pure random numbers?  One of our listeners believes he has.



Could encrypting an entire hard drive, then throwing away the key, be used in place of the random noise wiping I'm a big fan of?  Why hasn't the Unix time problem been fixed yet?  Or has it?  Will all of the stolen LastPass vaults eventually be decrypted?  Am I really leaving Twitter?  And, finally, why in the world is this episode titled "We Told You So!"?  The answers to those questions and more will be revealed by the time we're done here today.  Welcome to Episode 941 of TWiT's Security Now! podcast.



SHOW TEASE:  Hey, it's time for Security Now!.  I'm Ant Pruitt, sitting in for Mr. Leo Laporte while he's out enjoying a Green Bay Packers game.  That is still so funny to say.  This week I'm sitting with Mr. Steve Gibson as he goes through some interesting news here in the world of cybersecurity.  We have the NSA hacked Huawei - well, yeah, several years ago.  Is that really news?  We also take a look again at what's been going on with LastPass and some of the implications regarding their previous breach.  And also that algorithm doesn't quite add up.  Y'all stay tuned.



ANT PRUITT:  This is Security Now!, Episode 941, recorded Tuesday, September 26th, 2023:  We Told You So!



Hey, what's going on, everybody?  I am Ant Pruitt, and this is Security Now! here on TWiT.tv with the one and only, the man of the hour, or couple hours, Mr. Steve Gibson.  How you doing, sir?



STEVE GIBSON:  Yes, I think we can probably fill our listeners' time with a couple hours of all kinds of neat security information and news.  And this week we are, you know, no exception, we are chock full of questions.



ANT:  Oh, yeah.



STEVE:  Why is my new ValiDrive freeware not published yet?  Why did Apple quietly remove PDF rendering from the Mac after 39 years?  Has the NSA been hacking China?  What mistake did Microsoft recently make that would require the use of a bigger hard drive?  Why did Signal just announce their use of post-quantum crypto?  What's the big hurry?  Is it possible to create a new web browser from scratch?  And if not, why not?  Does public key crypto really go both ways?  Can pure math generate pure random numbers?  One of our listeners believes he has.



Could encrypting an entire hard drive, then throwing away the key, be used in place of random noise wiping, which is what I'm a big fan of?  Why hasn't the Unix time problem been fixed yet?  Or has it?  Will all of the stolen LastPass vaults eventually be decrypted?  Am I really leaving Twitter?  And, finally, why in the world is this episode titled "We Told You So!"?  The answers to those questions and more will be revealed by the time we're done here today.  Welcome to Episode 941 of TWiT's Security Now! podcast.



ANT:  Oh, man, this is going to be a lot of fun.  And I've got to tell you, I'm looking forward to that LastPass discussion because I have some thoughts, and I'm sure you're going to set me straight, as well as set everyone else straight here on the show.  And for folks that are tuning in and thinking, wow, Leo's voice is really different, or wow, boy, he's got a super tan going, no.



Mr. Laporte and the family, they are out of town having a good old time up in Lambeau Field, or actually Green Bay, Wisconsin, having a good time birthday celebration with their son as well as some local TWiT fans if they're in town, too, and they're going to hang out together.  It's going to be a good time.  So if you're in that area, be sure to go by and say hello.  I'm going to hang out today and get as much knowledge as I can from Mr. Steve on all of this infosec stuff because it's always fascinating.  So Mr. Steve, what's going on now?  What you got for us today to start the show?



STEVE:  Well, we always have a Picture of the Week.  So this week's no different.  This one is a six-frame cartoon which shows the evolution of a particular well-known product category.  This is a toaster.  So we have the original toaster in the first frame which, you know, it's got a little handle on the side and it makes toast.



ANT:  Perfect.	



STEVE:  Now, we're going to then - right, exactly.  Unfortunately, that's probably the optimal condition for the toaster because then the next frame it's been WiFi enabled.  So we've got a screen n the side showing an hourglass.  This makes toast after making you wait for a firmware update.



ANT:  Oh, boy.



STEVE:  So not clear that was actually a benefit.  Then we move to the data-driven toaster, which makes toast by watching how you like toast.  Then we evolve to the Toast-as-a-



Service, which makes toast for $5.99 a month.  And there's a screen on the side of this toaster that says "Log In."  So that's right.  Now you've got to log into your toaster in order to, you know, brown your bread.  The fifth frame here is the ad-supported toaster.  It's got a larger screen because it wants to make sure you see the ad, and it makes toast and lets you know that Smuckers is on sale today.  And finally, in the final frame of this, we've got the AI-enabled toaster.  It's got that red glowing eyeball from the Hal 9000 of space, no, not space.



ANT:  No, the Hitchhiker's Guide?  Is that the one?



STEVE:  No, no, no, it's Kubrick's...



JOHN SLANINA:  "2001:  A Space Odyssey."



STEVE:  Oh, thank you, John, of course.



ANT:  Thank you, Mr. JammerB.  And thank you, chatroom.  They got it.



STEVE:  Just blanking on it.  Anyway, so this AI-enabled toaster is actually responding to us.  It says:  "Toast?  I'm afraid I can't do that, Dave."  So, yes.  Not exactly an advance as far as toast goes, which sort of tells you sometimes you just need to keep the microprocessors away from your appliances because there's no real value to be added there. 



ANT:  What's so funny is I'm looking at those images.  And as you're going through each cell, my brain is automatically assigning a big tech company to each cell.  It fits a lot of them quite perfectly, for example, you know, the ad-supported or as-a-service could clearly be Amazon, easily, you know.



STEVE:  Right.



ANT:  But anyway, what's going on with ValiDrive, sir?



STEVE:  ValiDrive.  So I've received messages from listeners who are anxious to start testing their various USB flash drives with GRC's forthcoming ValiDrive freeware utility.  You may not know, Ant, but it turns out that there are a lot of counterfeit or fake flash drives now on the market.



ANT:  Oh, yeah, I've heard.  They've made them super fast, supposedly, and there's nothing in there but some standard, like a V90 chip in there, which should be something better.



STEVE:  Right.  There is that, and also the big problem is thumb drives that claim to be one or two terabytes, but only have a 32GB chip in there.  And what's diabolical is that the memory is at the beginning of the drive, where the file system is.  So it looks like it's formatted.  You can even be storing data there.  And so the file system is there.  And of course we actually heard a story where someone purchased a - and you'll appreciate this as a photographer - got a compact flash card that was counterfeit, stuck it in their camera, took wedding photos throughout all of the wedding process, came back, and the photos were not stored because all of the file system, filenames were there, but actually out in the drive's real estate there was no memory.  So the camera thought it was writing them, but they were not actually being recorded.



So anyway, so as is usual for me, this little side project is taking longer than I expected.  I announced it a couple weeks ago, I guess like three or four weeks ago.  And since I paused the completion of SpinRite v6.1, right on the verge of getting that finished, I do feel a great deal of pressure to get ValiDrive finished and back to work on SpinRite.  But I don't believe that I'm going to look back and feel this was a mistake.  All of my previous low-level drive work has been in DOS, where of course SpinRite still lives.  And in DOS it's possible to own the entire machine.



It turned out that Windows's USB chain fought back and interfered with ValiDrive's operation much more than I expected.  So getting this done correctly was quite involved.  But it has made ValiDrive unique because it now incorporates a bunch of technology that's not available elsewhere.  And along the way I've developed a lot of new code that's going to be very useful for future USB work under Windows, like for GRC's secure drive wiping utility, which will happen in the future.  And although that doesn't help us today, it will in the future.  And, you know, I always take the long view since I plan to be around and active for quite a while yet.



ANT:  Please and thank you.



GRC:  So today, ValiDrive finally appears to be working well, and it's being heavily tested and used by GRC's testing group.  It has opened my eyes about just how severe this fake USB drive problem has become while none of us were really paying that much attention to it.  So I'm not quite ready to turn it loose because once it's finished I don't want the distraction of needing to keep coming back to fix little issues that I ignored out of a rush to publish.  You know, I want to get back to SpinRite 6.1 and then immediately on to SpinRite 7.  So little things like not saving its reports properly on screens where Windows font sizing is set to other than 100%, or some of its UI text not appearing when a user's screen is set to high-contrast mode, or when a user may be unable to discern closely similar colors clearly, you know, they've got a little bit of a colorblindness problem.



Anyway, those are details that I need to put behind me so that what's published will be finished and probably useful for years to come.  Anyway, I also mentioned creating and saving reports.  So yes, there's been a little bit of feature creep along the way.  It's a little more than I had initially expected to get finished.  Anyway, it's become very nice, and it's a useful utility which I'll formally announce and publish as soon as possible.  Anyway, so I just wanted to get that out of the way because people have been saying, hey, where is it?  So it's all working well, and it's coming soon.



ANT:  Outstanding.



STEVE:  So the Eclectic Light Company posted a blog entry that had an interesting piece yesterday which they titled "Postscript's sudden death in Sonoma."  Now, they don't mean Sonoma, California.  Well, actually, Apple was referring to Sonoma, California.  But in this case "Sonoma" refers to the most recent major macOS release.  And their article has some interesting observations about Postscript as a dangerous interpreter, which shouldn't surprise any of our listeners.  So I want to share what they wrote.  And I'll comment, you know, a little bit inline and on the other side.



They said:  "If there's one language that's been at the heart of the Macintosh for the last 39 years, it's PostScript, the page description language developed by the founders of Adobe, the late John Warnock, and Adobe's team of engineers.  It brought the Mac's first commercial success in desktop publishing, in PostScript fonts, and early PostScript printers including Apple's game-changing LaserWriter.  Although Mac OS X never inherited NeXTStep's Display PostScript, its descendant Quartz and Core Graphics are still based on PostScript's relative," which is the PDF.  So just to be clear, it's PostScript which is the issue here being discussed, not PDFs per se.  PDFs are still around and obviously in heavy use.



So they said:  "Following a short illness that started in macOS Monterey 12.3, PostScript has died suddenly in Sonoma.  The first sign passed almost unnoticed in Apple's release notes to macOS 12.3, where it recorded the so-called 'deprecation' of PostScript in WebKit."  They said, Apple said:  "Support for inline viewing of PostScript files is no longer available."  So that was just sort of like in passing in WebKit.  But it sort of did indicate where things were headed.



"Then in macOS 13.0, Preview lost the ability to convert PostScript and EPS files."  Apple said:  "The Preview app included with your Mac supports PostScript (.ps) and Encapsulated PostScript (.eps) files in macOS Monterey or earlier.  Starting with macOS Ventura, Preview no longer supports these files."  So again, a little more deprecation of this.  They said:  "Other apps that can view or convert .ps and .eps files are available from the App Store and elsewhere.  Finally," wrote the blog, "the complete removal of support for PostScript and EPS was recorded as another deprecation in the release notes for Sonoma."



Okay.  So PostScript, they finished saying, "is an old stack-based interpreted language designed at a time when code security had barely been conceived, and malicious software hardly existed.  Among its attractive features is the fact that any PostScript object can be treated as data, or executed as part of a program, and can itself generate new objects that can in turn be executed."



Okay, now I'll just pause here for a minute to observe that Windows Metafiles, which turn Windows drawing primitives into an interpreted format, originally had that same capability of executing code.  That's just the way things were done back then. When this was rediscovered many years later, everyone freaked out, thinking it was a horrific bug.  And many people thought that I was nuts when I calmly observed that it was clearly, originally, deliberate.  You know, it's definitely a bad idea today because so prone to abuse, but it was entirely reasonable at the time.  Everyone just forgot it was there.  You know, as this article says:  "PostScript is an old stack-based interpreted language designed at a time when code security had barely been conceived, and malicious software hardly existed."  You know, exactly.



ANT:  Right, makes sense, yeah.



STEVE:  So, and I'll just make one other note.  Although stack-based languages can be brittle, back at the time, defining a stack-based language was a terrific choice by Adobe because stack-based representations and interpretations can be incredibly dense and efficient.  And that's what you would want in a page description language where there are effectively no processor cycles and no easy memory.  So, you know, PostScript's design was brilliant.



So anyway, their article continues:  "More recently, security researchers have drawn attention to the fact that Postscript is a gift for anyone wishing to write and distribute malicious code.  As it's effectively an image format, embedding malware inside a PostScript file could enable that to be run without user interaction, as is the case with other graphics formats."  You know, you just, you know, any kind of preview will cause that file format to be interpreted, and we've seen exactly that happening in the not-too-distant past.



ANT:  Right.



STEVE:  The article says:  "There are three major PostScript interpreters today in common use:  There's still Adobe's Distiller engine, which is built into its Acrobat products; there's Apple's PSNormalizer engine, which is built into macOS; and Artifex's open source engine, built into Ghostscript.  And that's one that's widely used in Linux and other platforms" because it's open source.



They wrote:  "Research into those engines has so far been relatively limited, but has revealed some serious vulnerabilities.  Most recently, Kai Lu and Brett Stone-Gross of Zscaler's ThreatLabz published an account of three vulnerabilities they found in Distiller, and one in Apple's PSNormalizer.  That was in 2022.  Those were fixed by Adobe and Apple last year, in the later case in macOS Monterey, which was v12.5 released on the 20th of July of last year, 2022, and its equivalent security updates for Big Sur and Catalina."  You know, they had to push those back also.  And from the dates, I suspect that the removal of support for inline viewing of PostScript files in WebKit in Monterey 12.3 may also have been part of Apple's mitigations.



Anyway, they wrote:  "Apple was most probably prompted into conducting a security audit of their PSNormalizer as a result of the vulnerability which was reported, and would have been faced with the choice of either re-engineering it or removing it from macOS completely."  The problem is it was just too dangerous as it was.  And these guys note:  "Unlike the PDF engine in Quartz, PSNormalizer is now little used and has no significant role any longer in macOS."



So, I mean, that is a reason to remove something that is inherently dangerous.  So "The first step was to make it inaccessible from the GUI by disabling the feature in Ventura's Preview, then following that in Sonoma by removing PSNormalizer altogether, so removing its command tool, which is pstopdf, and the Core Graphics' CGPSConverter.



"This leaves those still wishing to convert PostScript files with a choice between Adobe's Distiller," which you would have to pay for because it's in their Acrobat products, "or Artifex's Ghostscript."  You know, and to be fair, that's had its own share of vulnerabilities.  Again, this is not, you know, PDF is a fundamentally dangerous format.  It is not easy to get it right because it is so old and so complex.



Oh, and the article did mention there's is also a third option, which would be to run a late version of macOS Monterey, where you still have PostScript, in a lightweight Virtual Machine, and that way you could continue to use Apple's PSNormalizer through Preview there.  And, you know, for most people that would probably be the cheapest and simplest option.



Anyway, John Warnock, the co-founder of Adobe and the driving force behind PostScript, died on the 19th of August this year.  And his page description language had brought success to the Mac for almost 39 years.  And even though he hasn't, the PDF format has lived on.



So anyway, to Apple I say "bravo."  It is always difficult to kill off features that have any audience; right?  Somebody's using it.  Somebody's going to be unhappy.  And as we've seen through the years, Apple takes some heat whenever they decide to break the status quo in the interest of a better future.  Postscript, as I said, is a big, old, and very dangerous interpreter.  There is no doubt that some people will complain.  That's inevitable.  But the fact that something that's inherently dangerous could be removed with relatively little repercussion suggests that Apple has once again made the right call.  So, yeah.  I just wanted to point out that it's gone from macOS.  And, you know, PDF is still there, but not sort of the underlying PostScript interpreter.  And that's, you know, that's certainly best.



ANT:  You referenced just some of the issues that we've had and known about over the years with PDF format.  And yet we still continue to fight to get those squared away.  But it's still a problem.  Are there any alternatives for PDF?  I mean, we can't necessarily say TIF or PNGs or anything like that.  Everything is fairly open to being compromised.  Do we have any other options out there for something that's going to give us the same performance and reliability of a PDF?



STEVE:  Well, yeah.  The problem is that image formats are not a replacement for a page description language.  The reason we have, well, the reason you're looking at a PDF of the show notes is that it is the show notes captured in basically in a printed format.  And that's what makes the PDF format unique.  It also is not easy to do, which is what makes it complicated.  And as we know, complexity is the enemy of security.  So if something's going to be complex, there's probably ways to hack it, and it's...



ANT:  There's probably a hole somewhere; right?



STEVE:  Yup, exactly.  So has the NSA hacked Huawei?  And the answer is, well, uh, yeah, back in 2009.  What's weird is that Last Tuesday China's Ministry of State Security published an extremely rare, extremely as in the first time ever, official statement on its WeChat account.  It formally accused the U.S. National Security Agency of hacking and maintaining access to servers at Huawei's headquarters since 2009.  Now, okay.  What's interesting is not that this is news, it's that it's not news.  So I'm mentioning this because this reflects a relatively sudden change in stance for China, and it suggests that this might just be the beginning.  The question is, the beginning of what?



Both The New York Times and Der Spiegel originally reported this back in 2014, based upon documents from the Snowden leaks which disclosed a program called SHOTGIANT, which was an NSA operation to compromise Huawei's network for the purpose of finding links which existed between, or they thought might exist, between Huawei and the Chinese PLA, to learn Huawei's internal corporate structure, and also to identify ways to exploit Huawei's equipment, which at the time was being widely adopted in both the U.S. and by our allies and adversaries.  So this is the first time the Chinese government has ever publicly confirmed the NSA's Huawei hack.  And they posted it on their WeChat channel.



So sort of seems more political than not.  The Chinese Ministry of State Security statement didn't go into any technical details about the actual hacking.  You know, like they apparently didn't know any more than we've all known since 2014.  It just recycled information that was published by The New York Times and Der Spiegel and the Snowden leaks.  It does, however, spend a lot of time accusing the U.S. of using, and believe it or not I'm quoting this, "the despicable tactics of the 'Matrix' to maintain a 'cyber hegemony.'"  To that end, Chinese officials claim that the U.S. is doing all of the intellectual property stealing and then using its allies and PR machine to hype, exaggerate, and smear China on "Chinese cyber secret stealing issue."



So a cyber threat analyst at the Taiwan-based security firm TeamT5 was quoted saying:  "Considering the close relationship between China's cybersecurity firms and the Chinese government, our team surmises that these reports could be part of China's strategic distraction when they're accused of massive surveillance systems and espionage operations."



So, you know, there have been several other recent reports of NSA penetration into China's space, you know, into their networks.  And I suppose none of us assumed that the cyber intrusions were all going one way,  you know, like that there was no pushback against China's much publicized intrusions into the U.S.  You know, we have an NSA, and all of those people dressed in camo must be up to something.  So it seems as though the Chinese government may have changed their policy.  Rather than pretending to be invulnerable, they've decided that the better strategy is to acknowledge that the U.S. is also intruding into Chinese affairs.



This might also be a reaction to China's very high profile and heavily publicized intrusion into Microsoft recently and their exploitation of their access to enterprise email.  But there does appear to be a difference.  The evidence we have suggests that when we get into their networks, we just snoop around to gather intelligence.  When they get into our networks, proactive damage results.  So, I mean, it's not just espionage, it's attacks.



ANT:  Depends on who you ask.  That's either way is still an attack.



STEVE:  Yeah.



ANT:  Just because I didn't come in there and break something, that still doesn't say that I'm less wrong than you.



STEVE:  Oh, no.  They're certainly not happy to have NSA establishing a persistent presence within their country's internal private networks.  As we recently saw, that Chinese attack on Microsoft took a great deal of effort; you know?  That demonstrated that they have some serious cyber skills.  But sometimes you just trip over a pot of gold.  That was the case when a misconfigured Azure Shared Access Signature, a so-called "SAS token," resulted in 38TB of hypersensitive Microsoft data being exposed, not just for the taking, well, for the taking, but more than that.



Okay.  So what happened?  A cloud security-focused group known as Wiz Research - just to note that's not "whiz."  There's no H.  It's just Wiz, as in Wizard.  They stumbled over a trove  and I mean "trove" as in "we're going to need to get a bigger drive over here," wow  trove of Microsoft data.  It was all exposed and sitting there out on the Internet.  The Wiz wizards explained.  They said:  As part of the Wiz Research Team's ongoing work on accidental exposure of cloud-hosted data, the team scanned the Internet for misconfigured storage containers.  In this process, we found a GitHub repository under the Microsoft organization named robust-models-transfer."  And, you know, as an aside, it sounds like maybe the models were robust, but the security was not so much.



They said:  "The repository belongs to Microsoft's AI research division, and its purpose is to provide open-source code and AI models for image recognition."  They said:  "Readers of the repository were instructed to download the models from an Azure Storage URL."  Okay, so that sounds kind of deliberate; right?  Like that doesn't sound so bad.  But, okay, now we're getting to the bad part.



They wrote:  "This URL allowed access to more than just open-source models.  It was configured to grant permissions on the entire storage account, exposing additional private data by mistake."  They said:  "Our scan" - and actually they did a little more than scanning we'll get to in a second.  "Our scan shows that this account contained 38TB of additional data, including Microsoft employees' personal computer backups."



ANT:  Oh, boy.



STEVE:  "The backups contained sensitive personal data, including passwords to Microsoft services, secret keys, and over 30,000 internal Microsoft Teams messages from 359 Microsoft employees."  And that's where you say "Whoopsie!"  Now, it also occurs to me from this report that you don't know that there are 30,000 internal Microsoft Teams messages from exactly 359 different Microsoft employees without doing a great deal of data analysis and counting things.



ANT:  Right.



STEVE:  I mean, if you've got 38TB of data, you've got to dig around in there.  They also knew that there were employees' personal computer backups, and that those backups contained sensitive personal data, including passwords to Microsoft services and secret keys.  So, wow.  As I said, they probably did need to get a bigger drive to hold all of that.



ANT:  Unbelievable.



STEVE:  I know.  So the report continues to explain.  "In addition," they said, "to the overly permissive access scope, the token was also misconfigured to allow 'full control' permissions instead of read-only.  Meaning not only could an attacker view all the files in the storage account, they could delete and overwrite existing files, as well."  In other words, you know, change anything they wanted.  Okay.  Now, wait a minute.



ANT:  Can you imagine an attacker showing up and getting in, and it was like, yes, just celebrating?  It's like, it can't be this easy.



STEVE:  Like I said, the Chinese cyberattack that got a hold of Microsoft's corporate email, that required some skills.  Here Microsoft literally left the door open.



ANT:  Door's open.



STEVE:  And really, you know, these guys said, meaning not only could an attacker view all the files in the storage account, they could delete and overwrite them.  But, you know, this really does cause us to question the use of the term "attacker" here.  If you pick up a lost USB thumb drive in a parking lot, have you attacked anyone?  It seems to me you're just observant.  Right?  You just saw the drive sitting there.  So if Microsoft is waving their arms around and saying:  "Hey, come over here and download a bunch of stuff using this URL.  Oh, and while you're here, how would you like to read 30,000 pieces of private internal corporate communications from 359 of our employees and poke around in some of their computers' backups?"  You know, does that qualify as an attack, if you say "Well, thank you, that does sound interesting."



ANT:  Microsoft has a weird way of doing honeypots.



STEVE:  Wow.  So anyway, these Wiz Wizards said:  "This is particularly interesting considering the repository's original purpose, providing AI models for use in training code.  The repository instructs users to download a model data file from the SAS link and feed it into a script.  The file's format is .ckpt, which is a format produced by the TensorFlow library.  It's formatted using Python's pickle formatter, which is prone to arbitrary code execution by design.  Meaning, an attacker could have injected malicious code into all the AI models in this storage account, and every user who trusts Microsoft's GitHub repository would then have been infected by it."  In other words, this could have been, like, really bad.



They said:  "However, it's important to note this storage account wasn't directly exposed to the public; in fact, it was a private storage account."  Here's what happened.  "The Microsoft developers used an Azure mechanism called 'SAS tokens,' which allows the creation of a shareable link granting access to an Azure Storage account's data.  This means that, upon inspection, the storage account would still seem to be completely private.  But as we now know, it was anything but.



"In Azure, a Shared Access Signature, SAS token, is a signed URL that grants access to Azure Storage data.  The access level can be customized by the user who creates the URL with permissions ranging from read-only to full control, while the scope can be either just a single file, just a container, or an entire storage account."  And as we know, that's what happened here.  "The expiration time is also completely customizable, allowing the user to create never-expiring access tokens.  This granularity provides great agility for users, but it also creates the risk of granting too much access.  In the most permissive case, as was the case with Microsoft's token above, the token can allow full control permissions on the entire account forever, essentially providing the same level of access as the account key itself."



So, okay.  We can be glad, or at least hope, that adversarial cyberattackers did not stumble upon this.  We can be glad that the Wiz Research guys did, and that they promptly, after doing apparently a bunch of counting to see just how much of what was there, gave Microsoft a heads-up about this little misconfiguration mistake.



And I noted that Microsoft's own Microsoft Research, you know, MSRC blog posting for this incident was titled, got to love this:  "Microsoft mitigated exposure of internal information in a storage account due to overly permissive SAS token."  Okay, right.



ANT:  Overly permissive.



STEVE:  Overly permissive.  And they mitigated the exposure.  Wasn't that nice.  I won't spend any more time on this other than to note that in Microsoft's sharing of what they called their "learnings" - yes, they actually did call it that - their learnings from this, they never got around to mentioning just how much of their highly sensitive data had been flapping in the breeze.



ANT:  Well, of course they wouldn't because that's how big tech works.  That's how big corporations work.



STEVE:  That's right.



ANT:  Yeah, we had an attack, so let's just keep this story as minimal as possible; you know?



STEVE:  Yup.  Okay.



ANT:  Oh, man.



STEVE:  Okay.  One more story, then we'll take a break.



ANT:  All right.



STEVE:  Signal's PQXDH Quantum-Resistant Encryption.  Now, the Signal encrypted messaging platform was originally named "Axolotl," believe it or not.



ANT:  Wait, what?



STEVE:  Yes.  It was called Axolotl.



ANT:  I'm glad they had a branding meeting.



STEVE:  A-X-O-L-O-T-L, which still seems like it's missing some vowels or something.  Axolotl.  They called it that, it's named after a newt, N-E-W-T, a newt which has self-healing powers.  Because the Signal protocol has some of that.  In some ways Signal, which uses a resynchronizing cryptographic ratchet system, is a, as I said, a similarly self-healing protocol.  It was originally designed by Moxie Marlinspike of what was then named Whisper Systems, and then wisely renamed from "Axolotl" to "Signal."



ANT:  We have a lot of people in our live Discord giving us GIFs of Axolotls, apparently.  And I will say I still stand by branding statement.



STEVE:  Yes.  So of course it hit the big-time when this protocol was adopted by Meta to be the secure messaging protocol for their WhatsApp messenger.  And if the phrase "resynchronizing cryptographic ratchet system" doesn't ring any bells, if you're a listener who joined us after April 12th of 2016, or you're a longtime listener who would be interested in a refresher, I did one of our famous deep dives into this truly lovely messaging protocol which, thanks to Meta's adoption, is now in use by more than a billion people worldwide.  Anyway, look for Security Now! Episode 555 which we titled "WhatsApp."  And that's where I do a deep dive into Axolotl, which is now fortunately rebranded as Signal.



Okay.  The reason we're talking about Signal today is that Signal, the company, made headlines last week in the tech community with their announcement that their already extremely clever and well-designed, very secure Signal protocol was being upgraded with something they called "PQXDH quantum resistant encryption."  And if I didn't already have a topic for today's podcast, this would have been it.  But we have room for both.



Okay.  Signal's move might at first appear premature, since the threat posed by quantum computers to public key crypto, which we rely upon today, remains purely theoretical and may well remain so for the foreseeable future.  We've had some fun in the past at quantum computing's expense, noting that breaking RSA-style crypto, which would require determining the prime number factors of a massive number represented by more than 4,000 binary bits, appears to be safe for now since it was considered to be a breakthrough and a huge accomplishment when today's most advanced quantum computer successfully factored the number 35.  So that's the best we've been able to do so far.  We need to factor a 4,000-bit number in order to break RSA-style crypto.  So it seems to be, the point is, very safe for the time being.



Now, given that, it might appear that Signal's move to an overtly quantum resistant protocol is premature.  But if nothing else, it's brilliant marketing.  Okay.  But there's more to it than that.  The security world has coined another abbreviation which is pronounced "Handle" because the abbreviation is H-N-D-L.  HNDL stands for "Harvest Now, Decrypt Later."  And we know that our dearly beloved NSA has built a truly massive, somewhere between 1 and 1.5 million square foot data center of some sort  out in the boonies of Utah.



So there's more than a passing chance that the "Harvesting Now" first half of "Harvest Now, Decrypt Later" strategy is already well underway.  They're sucking in all the messaging that's going on.  They can't decrypt it today, but they assume they're going to be able to decrypt it someday.  So grab it now, harvest it now, decrypt it later.  In other words, you know, what the lesson is then for us now is if you want your secrets today to remain secret past the foreseeable future, it's never too soon to begin encrypting under post-quantum crypto technology.  And that's what makes Signal's announcement last week significant.  Okay.  So what exactly has Signal done?



Signal's current shared secret key agreement protocol is known as X3DH.  The "DH" is short for Diffie-Hellman, which is a well-established key agreement system.  We've discussed key agreement protocols in general, and Diffie-Hellman in particular, many times in the past on the podcast.  And I know it pretty well since it plays a large part in SQRL's more tricky security features, you know, the features that I designed into my own SQRL technology.



Okay.  Briefly, a key agreement protocol allows two ends of an insecure and public connection to exchange some information in plain sight while each of them obtain a shared secret which no one else is able to get.  I know.  You're scratching your head, Ant.  I know.  It's really very cool technology.  It seems counterintuitive that their conversation could be completely known, while they each arrive at the same secret that only they know, but it works.



ANT:  Wow.



STEVE:  The "X3" in the X3DH refers to the "X25519" elliptic curve, which is the flavor that Signal's Diffie-Hellman key agreement, and actually mine, I chose the same one, has traditionally used.  Okay.  And what's really cool is that they're still going to use this in the future.  What they decided to do was to add another key agreement protocol - this one believed to be quantum-safe - to their existing system in such a way that BOTH of the protocols, the old and reliable elliptic curve Diffie-Hellman and the "believed to be safe today and tomorrow" newfangled quantum-safe system, both would need to be simultaneously broken and cracked in order for an attacker to obtain the shared secret that's used to decrypt Signal's communications.



Signal selected one of the NIST contest finalists that's believed to be the best, the one known as CRYSTALS-Kyber, K-Y-B-E-R.  And we've talked about it previously.  They chose it because it's built upon a solid foundation.  But they also wisely decided that, since it's new and unproven - and actually one of the other contest finalists turned out to be broken, breakable by conventional computers, so whoops.  It's a good thing they didn't use that one.  You know, again, not yet proven.  That's why they're keeping the Diffie-Hellman around, just to have a belt and suspenders approach.  Anyway, so because it's new and unproven, they decided not to depend upon it solely.  So Signal's original X3DH has been renamed PQXDH for post quantum, you know, the PQ is for post-quantum and the XDH for the existing elliptic curve Diffie-Hellman, which survives in the new combined protocol.



And here's the coolest part.  This new PQXDH is already present and supported in the latest versions of Signal's client applications, and it's already in use protecting any conversations initiated after both sides of the chat are using the latest Signal software which supports both the original and the updated PQXDH protocol.  Then, over time in the future, after sufficient time has passed for everyone to be using the new Signal and to have updated, they plan to disable all use of the old non-quantum-enhanced X3DH-only protocol in favor of all new conversations, which would then be requiring PQXDH for all new chats.



So, you know, this is great; you know?  It's clear that we are moving into a post-quantum world, even though it seems like we're still a long way away from quantum computers being a threat to our existing crypto.  You know, this whole concept of Harvest Now, Decrypt Later, that makes a lot of sense.  Now that Signal has led the pack of messaging apps by introducing quantum-safe messaging, the rest of the pack, who may have been caught by surprise, you know, caught off guard, they'll have no choice but to figure out how to do the same, you know, because they don't want to get left behind.  So it's great.  Essentially this means that we will be, you know, moving to the next generation of crypto in advance of its clear need.  And that makes sense, too, because at some point we will be surprised when a quantum computer is created which is able to do this.  We might as well be ready for it.



ANT:  Yeah.  Now, did you all speak last week about what was going on with Signal in the EU and the EU's new plan to pass a bill basically giving them a backdoor to everything?



STEVE:  So we've talked a lot about the privacy issues.  What happened in the UK was they...



ANT:  Not EU, UK, sorry.



STEVE:  Right.  They amended the legislation to basically give the messaging apps an out.  They said you must, must, must scan where technically feasible.  And that "where technically feasible" clause says, oh, okay, fine, it's not technically feasible.



ANT:  Okay.



STEVE:  So the politicians get to say we did, you know, we implemented strong new legislation, whereas none of the messaging apps are worried about it anymore.  



ANT:  Oh, okay.



STEVE:  So what will be interesting is to see what the EU does.  Will they also back down?  Because it really looks like any government that says we must have you scanning all content, they're simply going to lose all cryptographic services in their country.  And they can't afford to do that.



ANT:  Right, yeah.  I believe I saw Signal's leadership saying, yeah, we'll just step out of here.  See you later.



STEVE:  Oh, yeah.  Yeah, yeah, yeah.



ANT:  When it comes down to it.



STEVE:  Yup.



ANT:  I totally get that.  Folks, this is Security Now!, Episode 941 with Mr. Steve Gibson, dropping some great knowledge and information on us in the world of cybersecurity and information security.  Mr. Steve, it's time for us to close up the loop.  What you got?



STEVE:  Yep, we've got some really interesting listener feedback that drives some, I think, interesting observations.  Our listener, Dustin Smith, put me onto a blog posting which was originally written 3.5 years ago on March 18th of 2020; yet if nothing, it's more relevant today than it was then.  This blog posting was written by a guy name Drew DeVault. It's an on-point rant about what modern web browsers have become, and I think everyone here will find it interesting.



Here's what he wrote.  He said:  "Since the first browser war between Netscape and Internet Explorer, web browsers have been using their features as their primary means of competing with each other.  This strategy of unlimited scope and perpetual feature creep is reckless and has been allowed to go on for far too long."  He says:  "I used wget to download all 1,217 of the W3C specifications."  That's, you know, the World Wide Web Consortium, the W3C.  "So all 1,217 of the World Wide Web Consortium specifications which have been published at the time of writing."



And he said:  "Not counting WebGL, which is maintained by Khronos."  He said:  "Web browsers need to implement a substantial subset of this specification in order to provide a modern web experience."  He said:  "I ran a word count on all of these specifications.  How complex would you guess the web is?"



Okay.  He said:  "The total word count of the W3C specification catalog is 114 million words at the time of writing."



ANT:  Wow.



STEVE:  If you added the combined word counts of the C11, which is, you know, C language 11, C++17, UEFI, USB 3.2, and POSIX specifications, all 8,754 published RFCs, and the combined word counts of everything on Wikipedia's list of longest novels, you would still be 12 million words short of the W3C specifications.



ANT:  Wow.



STEVE:  Now, I mean, okay.  So it is that complex.



ANT:  Wow.



STEVE:  So he said:  "I conclude that it is impossible to build a new web browser.  The complexity of the web is obscene.  The creation of a new web browser would be comparable in effort to the Apollo program or the Manhattan project."  He said:  "It is impossible to, first, implement the web correctly."  Impossible.  "Second, implement the web securely.  And third, implement the web at all."



He said:  "Starting work on a bespoke browser engine with the intention of competing with Google or Mozilla is a fool's errand.  The last serious attempt to make a new browser, which was known as Servo, has become one part incubator for Firefox refactoring, one part playground for bored Mozilla engineers to mess with technology no one wants, and zero parts viable modern browser.  But," he said, "WebVR is cool; right?  Right?  And the consequences of this are obvious."  He says:  "Browsers are the most expensive piece of software a typical consumer computer runs.  They're infamous for using all of your RAM, pinning CPU and I/O, draining your battery, et cetera.  Combined, web browsers are responsible for more than 8,000 CVEs" - you know, exploits, flaws, bugs - "alone."



And then Drew switches from establishing some facts to making a very interesting observation.  He writes:  "Because of the monopoly created by the insurmountable task of building a competitive alternative, browsers have also been free to stop being the 'user agent' and start being the agents of their creators instead.  Firefox is filling up with ads, tracking, and mandatory plugins.  Chrome is used as a means for Google to efficiently track your eyeballs and muscle their anti-technologies like DRM and AMP into the ecosystem.  The browser duopoly is only growing stronger, too, as Microsoft drops Edge, and WebKit falls well behind its competition."  Now, remember, this is 3.5 years ago, so of course that part is pretty much history.



He said:  "The major projects are open source, and usually when an open-source project misbehaves, we're able to fork it to offer an alternative.  But even this is an impossible task where web browsers are concerned.  The number of W3C specifications grows at an average rate of 200 new subspecs per year, or about four million new words of specification, or about one POSIX every four to six months.  How can a new team of any forked browser project possibly keep up with this on top of implementing and maintaining the outrageous scope web browsers already have now?"  He finishes:  "The browser wars have been allowed to continue for far too long.  They should have long ago focused on competing in terms of performance and stability, not in adding new web 'features,'" he has in air quotes.  "This is absolutely ridiculous and has to stop."



ANT:  Okay.  So I need to interject here because I get where Mr. Drew is coming from with his stop adding features and whatnot.  But let's think about this.  Let's just think about vehicles; okay?  We can have the Honda Civic, you know, simple four-cylinder car.  But then we have the Honda Accord with a slightly larger engine.  And then we have this, I don't know, like a Ford Mustang with an even bigger engine, so we had these fast cars.  And everybody's like, huh, I don't need that fast car.  But then you throw in Bluetooth features.  Ooh.  I want that.  And that's what's happening here.  People are sold on features.  And these large companies, Google and Firefox, they get that.  If they want to stay in business, they're going to continue to add features.  That's the nature of consumerism; right?



STEVE:  Okay.  Maybe the right analogy, or trying to find a good comparison, not everybody can launch stuff into space, into orbit.  You know?  It requires so much infrastructure, so much technology, so much other stuff, that there are only a few ways of getting something launched into orbit.  So similarly, his point is there's just no way to create a third browser.  So there is a duopoly, there is a lock-in at this point.  There's no way for there to be competition.  And what we see when there's a lack of competition is then the rules change so that the browser creators no longer have to create the best product anymore.



ANT:  Okay.  



STEVE:  And so that's the point he's making.



ANT:  And what's up with these specifications from the W3C?  Do you think it's a bad idea, considering we know about all of the security issues behind the browser?  [Crosstalk] come from.



STEVE:  I don't think it's a good idea.  No.  In fact, it is the implementation of all these new things which is where new problems get created.  No, I'm often, I'm always saying, if we would just leave this stuff alone, we'd be able to fix the problems, and then we'd have some security.  But there are incentives for not leaving it alone.  You know?



ANT:  There it is.



STEVE:  Everybody wants to force you to upgrade to the newest operation system, the newest browser.  Everybody wants the newest features.  You know, arguably, it's cool that you're now able to do web conferencing with just your browser.  It used to be that you had to have some extra software in order to do it.  Now, you know, you don't need that.



So I guess the only thing that I would change about what Drew wrote is his last line.  He said:  "They should have long ago focused on competing in terms of performance and stability, not in adding new web features.  This is absolutely ridiculous, and it has to stop."  Okay.  I would say, I would change it to:  "This is unfortunate in the extreme, but there doesn't appear to be any way for the industry to change that course."  And I think, I mean, you know, he says it has to stop.  I would argue, how is it ever going to?  Why would it?



And I would further add that, to their credit, Microsoft perceived very early on just how important the web browser would be to the future.  Remember, they attempted to build it into Windows, and they called it Internet Explorer.  And of course they got taken to court and sued over antitrust issues because, you know, they built this browser into their operating system because again, to their credit, they knew this was the future.  And as we know, what the web has become even outgrew their, Microsoft's, their ability to browse it.  Once upon a time, IE was the most widely used browser.  It attaining a peak of 95% market share about 20 years ago in 2003.



ANT:  And was still horrible.



STEVE:  Yes.  This podcast began two years later, and we all witnessed IE's decline over this period of time, leading to its inevitable death.  After making a massive investment in a brand new post-IE engine, Microsoft, again to their credit, recognized that a modern web browser was too big for even them, and that they'd be better off just hanging bells and whistles off a browser, you know, Chromium, which they got from the Chrome project, that someone else was maintaining and evolving.



So are there any takeaways for us here?  No.  None that I can think of.  Drew's observations, which I think are valuable, pair beautifully with what I've noted to be true of operating systems.  Just as no one can create a competitive and useful, truly new operating system from scratch any longer, any operating system that a user is going to sit in front of will need to host a web browser that is all by itself also too massive and complex to create today from scratch.



So I also think all of this was absolutely inevitable.  And that further suggests that it wasn't a mistake.  It's not a consequence of inertia, which we often observe elsewhere.  And we're almost certainly going to continue moving down this path from here on.  You know, this is where we're headed.



So another piece of feedback from an anonymous person who actually called himself "Anonymous."  He said:  "Hello, Mr. Gibson."  It sounds like Ant.  "Hello, Mr. Gibson.  I hope I can ask you a question over the last podcast from Security Now!.  Around 30 minutes in, you say that you encrypt with the private key and decrypt with the public key.  Is this right?  I think it's reverse.  What I learned is that you encrypt with the public key and decrypt with the private key.  Is this right, or do I have wrong information?  Thanks for your response."



Okay.  So one of the powerful beauties of public key cipher technology is that the process does work in either direction.  It's obviously very useful to encrypt something using someone's public key that you know only they will be able to decrypt with their secret private key.  PGP is a perfect example of this.



ANT:  Right.  Right.



STEVE:  And digital signatures of all kinds is the best example of this being done in reverse.  In fact, PGP does this, too.  To provide authentication of the sender of a message, the sender uses their private key to sign the file which the recipient is able to verify using the sender's public key.  So when the recipient is verifying the signature, they're decrypting a hash of the document that the sender encrypted with their private key.



So the last time we talked about public key technology I was really pleased with the way that I conceptualized the public and private key distinction used by non-elliptic curve RSA-style crypto.  Okay.  So you start by obtaining a private key which is just a really large prime number.  So you just get a big prime number.  And remember that, counterintuitive though it is, prime numbers do not become more scarce as we go farther and farther out, like farther and farther down the numbers.  You'd sort of think they would, right, because the definition of prime number is that it's only divisible by one and itself.  You know, like no even numbers are prime.  But then no every third number is prime because it could be divided by three.  And no even number is prime because it could be divided by two.



ANT:  Two, yeah.



STEVE:  And then every fifth number is not prime because it can be divided by five.  And so on.  So since, you know, because of that, you'd think that would all kind of pile up.  And so, like, it'd be hard to find big numbers way out there that aren't divisible by something that's come before.  But it turns out that's not the way it works.  There's always plenty of them.  Okay.  So you take a really big prime number, and that's our private key.  



Okay.  Now, we want to arrange to hide that private key inside the public key.  We do this by taking another very large prime number and multiplying that new prime number with the original private key which was also a prime number.  Though I've simplified things, it's that multiplied key is the public key. The public key contains the private key.  But there's no way to know what it is after they've been multiplied together because to this day, despite all the best brains in math - and, I mean, this has received a lot of attention through the years - no one has ever come up with a practical way to "unmultiply" those two primes through the process of prime factorization.



And, by the way, that is the danger posed by quantum computing.  There are reasons to believe that some future quantum computer might be able to finally factor a very large public key which contains those two primes, one of which is the private key.  If you could factor that back into its original two prime numbers, then you'd have the private key, which could then be used to decrypt things where you were assuming that was not possible.



So the true magic of public key crypto is that that private key, which has been hidden inside the public key by multiplying it with another prime, that it is still able to perform encryption and decryption operations despite being completely hidden by its multiplication by another large prime number.  Anyway, it is just so cool.



ANT:  So first off, excuse me while I let all the steam come out of my ears because I'm like, wow, what in the heck?  And secondly, with quantum computing, do you feel like this is still something in the near future, to be able to get this?



STEVE:  No.  No.



ANT:  I wasn't going to sit here and say I was concerned.  But I wanted to ask you first, though.



STEVE:  Yeah.  As I mentioned before, the best we've seen a quantum computer do is factor the number 35.  Right? 



ANT:  Okay.



STEVE:  Into 5 and 7.  So that's like, whoa, it worked.  But, I mean, so no.  We're not in, I mean, now, okay.  There could be a breakthrough; right?  There could be some fundamental, you know, crazy ass, like no one expects it, comes out of right field, breakthrough.  Could happen.  And so that's why we already have, we're already beginning to standardize on next-generation quantum safe crypto so that we will be using that if such a breakthrough happened because if it happened today, you know, I often use the phrase "It's the end of the world as we know it."  That probably really would be.



ANT:  Yeah.



STEVE:  It would be, you know, if you were to use really, really bad, and like the amount of badness is how many reallys you put in front, you could be saying "really, really, really, really, really" all day long.  It would be really bad.  So, yeah.  But for now, it looks like we're safe. 



ANT:  Yeah.



STEVE:  Okay, now, here's a particularly interesting piece of feedback.  A listener whose name I will just say is Jerry, and I'll explain why I anonymized him in a minute, because initially it wasn't.  He wrote to me.  He said:  "Your assertion that 'no deterministic mathematical algorithm can generate a random result' is incorrect."  He said:  "Non-deterministic mathematical algorithms (processes) can generate true random results, not 'pseudorandom' numbers."  And he finished:  "I developed such an algorithm."



ANT:  Okay.



STEVE:  Okay.  So first of all, I said "no deterministic mathematical algorithm," not "non-deterministic mathematical algorithm."  Now, the problem is that the phrase "non-deterministic mathematical algorithm" is, as far as I know, an oxymoron.  You know, one plus one equals two.  And wait a minute, one plus one still equals two.  And, oh, one plus one, yes, it still equals two.  All mathematical algorithms are inherently deterministic.  Now, if I made a mistake on my math homework in elementary school, explaining to my teacher that this was my implementation of a non-deterministic mathematical algorithm, I'm pretty certain that would not have gone over very well, nor would it have changed my grade.



ANT:  I can see little Steve Gibson right now at the front of the class, at the chalkboard.



STEVE:  This is a non-deterministic mathematical algorithm.  That's why I didn't multiply these two six-digit numbers correctly.  Okay.  So, Jerry, if you indeed have developed a non-deterministic mathematical algorithm which is capable of generating true random results, there's probably a Nobel Prize waiting for you in Stockholm.  And more importantly, what you may have actually done is uncovered a provable flaw in the simulation we're all living within.  Okay.  But seriously, you have my attention.  You have access to an audience.  Are you just going to leave us all hanging like that?  Because that's all he said.



Okay.  Now, in a bizarre coincidence, just as I was getting ready to save as PDF, to cast these show notes from Google Docs where I write them every week into their PDF form, I happened to glance down at my phone and saw a reply tweet from this person.  Yesterday, when I encountered his assertion, not knowing how to respond to something which clearly seemed impossible, I simply replied, "I hope you've submitted this for a Nobel Prize."  Now, it turns out he didn't take that very well.



ANT:  I wonder why.



STEVE:  Well, you know, certainly not in the somewhat carefree spirit, you know, it was meant.  So, okay.  The first thing I did was to anonymize his identity in the show notes since I certainly don't want to embarrass him from this dialogue.  What he tweeted in reply to my short comment about the Nobel Prize, and actually I'm kind of serious about that because really, I mean, if he really did this, it would be worthy of a Nobel Prize.  Anyway, this morning, as I was getting ready to finish the show notes, I saw his tweet.



He replied:  "I'm very surprised by your reply.  I've been a customer and promoter of your products and services for 30-plus years.  I have read numerous articles of yours.  I never expected sarcasm from you."  He said:  "My bad.  I developed these and similar algorithms more than 45 years ago.  I have always expected others to develop like algorithms, but to date this has not occurred."



Okay, that was his tweet this morning.  Now, again, I'm having a little bit of trouble taking this seriously, or at least as seriously as he is.  You know, when he says, "I've always expected others to develop like algorithms, but to date this has not occurred," I'll admit my first impulse is to say, "Gee, I wonder why that is?"  Okay.  Instead, this what I replied to him.



I said:  "Hi, Jerry.  Okay.  So I suppose my reply was somewhat sarcastic.  And I certainly would love to be proven wrong in my assertion that, to quote you quoting me, 'no deterministic mathematical algorithm can generate a random result.'  The trouble is," I wrote, "I still believe that to be true.  And not just a little bit true, but very true.  If you're a mathematician, then you understand that the way to proceed from here would be to offer up some proof of your assertion, which appears to defy the laws and principles of mathematics and reality.  I'd be more than happy to eat my sarcasm if you can demonstrate that I'm wrong, exclamation point, smiley face."



ANT:  So tactful, sir.



STEVE:  I'm serious, really.  I mean, this would be great, you know?  I'll buy his plane ticket to Stockholm.



ANT:  I've never heard a more polite smart-assery from anyone, sir.  This was really well done.



STEVE:  For what it's worth, if I do hear anything more from Jerry, I will let everyone know because that would be big.  Now, it must be that there's a miss - there's a confusion of definition.  When I'm saying "no deterministic mathematical algorithm can generate a random result," and he's talking about a non-deterministic mathematical algorithm, which again I think is an oxymoron, okay, we're at cross-purposes here.  But anyway, it was an interesting bit of feedback that I had intended to share, and now I'm able to share a bit of dialogue.



Todd said:  "Hey, Steve.  I listened to Episode 940 where you explain securely wiping drives by writing random bits to all sectors of spinning drives.  Lately, I've been using VeraCrypt to encrypt old drives before I get rid of them," and he said, "(using GRC's password page to generate a secure password).  Is this as secure as your upcoming software that writes random noise?"



And I think that's very clever.  You know?  And so if you were to do it twice under different keys, then it would be the equivalent of writing random noise, just as GRC's solution will be.  And I would not argue that the second pass is absolutely necessary.  You know, I might offer a single-pass option for those in a hurry with my own product.  VeraCrypt, like its ancestor TrueCrypt, uses XTS cipher mode encryption, which is the industry standard and NIST-approved way of encrypting block-addressed mass storage using what's known as a tweakable block cipher.



And we also know that any good modern cipher, like AES, generates pseudorandom noise that's indistinguishable from true random noise.  It'll be slower than GRC's product, since it's reading and writing in order to encrypt what was already there.  And I don't know how large the blocks are that it's using.  If they were really small, that would slow it down.  Presumably it knows how to do that quickly.  It won't be able to access and wipe the latent data that might exist in grown-defect spared out sectors.  But it is truly a nifty solution until GRC's product is ready.  The VeraCrypt encrypt and then discard the key solution is here today, and it's free.  So anyway, Todd, thanks for sharing the idea.  I think it's pretty clever, and it's a great application.  And I cannot think of any downside to it.



Bill Melvin said:  "Hi, Steve.  I have a thought regarding the discussion on wiping hard drives on show 940.  I assume TrueCrypt and VeraCrypt hidden volumes look like random noise.  It seems to me that doing an extra pass of writing zeros to a drive takes you from 'plausible deniability' to absolute impossibility.  In an abundance of paranoia, why take the risk of being badgered over something that's not there. Thank you for Security Now!, which I've enjoyed for years.  I have my SpinRite license and am looking forward to the new version."



Okay.  So when I talked about my idea, my concept for the best secure wipe is to wipe with random noise.  The point he's making is that random noise looks like something, even if it's not.  So if a drive was filled with random noise, it might induce someone to, in the extreme, put you under some coercion, asking you for what the key was to decrypt the drive, when it was actually just random noise.  So he says do another wipe to zeroes so that nobody will think there's anything there.  Then they won't come asking for the key.  And I think he makes a good point.



Mark Kozel says:  "I found your description of SSD wear-leveling fascinating.  It is amazing what engineers have come up with to handle issues like this, and it got me wondering.  I have a portable SSD with a single multi-gigabyte VeraCrypt blob on it.  I open the blob a few times a week and make changes to files inside.  After closing the blob, everything appears to be unaltered, as far as Windows knows.  So I set VeraCrypt to change the file modification date on the blob when I realized Syncthing needed that to detect a change and sync the blob to my NAS.  Does the wear-leveling capability work on the blob?  Is it active inside the blob when I have it open in VeraCrypt?  Love the show and continuing education credits I get towards my Security+ certification by listening."



ANT:  Woohoo!



STEVE:  So, Mark, the answer is yes.  The wear-leveling is occurring at the lowest possible level in the chain of data flow, just before the data is written to the drive.  It is not visible to anyone reading or writing to and from the drive, so it functions whether the data being read or written is encrypted or not.  And one slick benefit of using VeraCrypt to encrypt the blob on the SSD is that, since VeraCrypt always leaves the data on the SSD encrypted, only decrypting it on the fly as necessary when it's read into the computer, as wear leveling causes blob updates to be written to dispersed regions of the drive, thus leaving obsoleted bits of the blob behind, those obsoleted and isolated blob bits are also encrypted.



So that drive will never need to be wiped, at least not to protect anything that was ever stored in the blob.  Once VeraCrypt's volume header has been overwritten multiple times, there's no way to ever recover any of that blob's data, period.  Meaning that the wear-leveling which you may not have access to is not a concern as it would be if you stored unencrypted data on the drive, which you then wanted to secure, because all of that wear-leveled data would have been previously encrypted bits in the blob.  So a very nice application.



Wayne Patton asked:  "How difficult would it be to change the signed integer to an unsigned something else and recompile and fix Unix time?"  So he's of course talking about the fact that we were recently talking about how Unix time, the signed integer which is - technically it's 32 bits, but because it's signed we only get 31 bits' worth of range; how because it's signed, Unix time will be - the original 32-bit Unix time will be wrapping around in 2038.



I should have then, but I failed to mention that this Unix time problem was fixed in the Unix Kernel back in June of 2018, after 2.5 years of research into the right way to do it without breaking other things in Linux.  In the process, more than 80, eight zero, source files had to be changed.  So this Y2038 problem is no longer any concern for Linux kernels ever since.  The worry is that before this time, all embedded, for example, Linux kernels of the sort used for industrial automation, satellites, elevators, and who knows what else, will have this trouble.  And in those markets there's much more of a "if it's not broken, don't fix it" mentality.



So the concern is not that mainstream Linux or any of the various major Unixes will break.  They've all been fixed.  It's that some satellite that's been in orbit since before this was fixed may become suddenly confused and do - who knows what it might do?  So maybe a long forgotten pump on an oil rig will misbehave.  We don't know.  Anyway, it's going to be interesting to see whether it's another non-event like Y2K turned out to be, or whether something interesting might occur.  And, you know, stay tuned.  We'll be here in 2038.



ANT:  Oh, Y2K.  I've got memories of that, fond memories of that.



STEVE:  Yeah.  So B0wter, he said:  "Hello, Steve.  Thank you for the many great episodes of Security Now!.  I've just listened to Episode 939 and have a thought about what you said regarding the LastPass hack.  You mentioned that hackers will go after those accounts where they suspect a big payout.  I think you're right about that.  But I also think we must not let our guard down.  All accounts will eventually be decrypted, and all accounts will be available for purchase on the dark web.  This will be a problem for all those, including me, who used LastPass to store information like Social Security numbers and other personal IDs.  I think it's important to take precautions while we still can.  Kind regards, Johannes."



Okay.  And in case you're not aware, Ant, we talked about this last week.  There's very good evidence leading us to believe that selective decryption of specific high-value LastPass vaults which were stolen in that LastPass breach are being, that is to say, have been decrypted.  And the cryptocurrency wallets of people who stored their master crypto keys in their LastPass vaults have been emptied.  And about $35 million or so of cryptocurrency has been transferred.



ANT:  Yeah, I saw reports on that.  It's pretty scary.



STEVE:  Yeah.  So I wanted to share Johannes's note because I understand his concern, and I imagine it is shared.  But I also doubt that the vast majority of the more than 25 million users whose encrypted data was stolen are actually ever in any real danger.  Even the advent of quantum computing won't make any difference here since we're talking about brute-forcing symmetric crypto, which quantum computing does not threaten.  The cost of performing 100,100 rounds, which is the level we were at when the breach occurred, where most people probably had  their iteration counts set, and we know some were a lot lower than that, and that's a concern.  But the cost of performing 100,100 rounds of brute force against a hopefully high-entropy password is truly prohibitive.



These crooks appear to be spending significant money on cracking gear or cloud compute cycles, and they appear to be cracking only a couple of vaults per week.  That's the rate at which this cracking is occurring because it is really expensive to do this.  That's perhaps a hundred or so per year against a population of more than 25 million vaults.



It's true that someday they might eventually get around to opportunistic attacks against non-high profile targets whose iteration counts are low.  But even then, if the passphrase is good, there's no clear reason to believe that any actual cash lies on the other end of the crack, that is, they wouldn't know until they performed the crack.  And it still takes a significant investment of time and money to turn someone's vault back into plaintext.  It really is not ever going to be easy.



So, you know, the original design of LastPass, which is what caused me to endorse it, was strong.  And as long as the iteration count is high, it's always going to be expensive to crack the vault.  So, you know, I cannot tell anyone that there's absolutely nothing to worry about.  So my intention is only to present some I think probably accurate perspective.  Cracking these vaults will never be free, and these people are probably not ones to throw away good money.  It will cost money.  And I think that the attacks will continue to be targeted, and that they will probably peter out over time, and that it's not the case that they will all be cracked.  There's just no reason to.  It's just - it is too expensive.



Kevin van Haaren, he said:  "Listening to the LastMess episode," which is what I called that podcast, he said, "I realized I had some info in my LastPass vault in addition to passwords, things like SSH private keys, recovery keys for all Apple devices and various cloud services.  I didn't even think of rotating all those when I moved off LastPass.  I'm also thinking that some of that should not be in my LastPass replacement.  SSH keys I'll probably keep on an encrypted disk image on my computer instead.  Not sure on recovery keys.  I may adopt your two-factor authentication authenticator app and just print out recovery keys and keep them someplace safe."



So I wanted to share this because I think Kevin's observation that just because today's password managers offer us synchronized encrypted vaults, which are capable of holding all kinds of other stuff, does not mean that they are the best place to store unrelated non-web secrets.  Password managers, they need our usernames and our passwords and, you know, credit cards; and perhaps some additional automatic form-fill information for convenience.  But things like SSH keys, server certificates, and other unrelated non-web passwords don't all need to be stored there in a single place.



Yes, it's convenient.  And that convenience can be seductive.  But when you combine the wisdom of not keeping all of our eggs in one basket and the old "fool me once" saying, with LastPass being that "once," and when we consider that today's computing environment does not have any more dangerously exposed and complex software, as we saw above, than our web browsers, it would be difficult to make a case for storing more in our browsers than is needed for web-facing activity.



I think a good cross-platform encrypted archiving program is probably a good solution.  It can contain all kinds of stuff.  It squeezes it down, and then any file synchronization tool, any of the cloud drives or third-party sync systems like Syncthing or Sync.com, can keep the copies synchronized.  I'll be interested to hear if any of our listeners have a better idea.



ANT:  Yeah, I wanted to talk to you about LastPass because I know where you stand on it, and I've been a LastPass user since before coming to TWiT because, you know, heard about them on TWiT and your recommendation and so forth.



STEVE:  Yeah.



ANT:  And when all of that went down with them, I did consider moving over to our sponsor, Bitwarden.



STEVE:  Bitwarden.



ANT:  I did consider that, and opened an account and started to transition.  And then I just found, you know what, I'm just going to change all of my passwords, period.  And I spent the weekend, a legit weekend, sitting at a laptop, going through everything and changing all of those passwords.  And then I said, you know what, I'm going to go ahead and just keep LastPass.  And I feel fine with that.  I updated those iterations and so forth.  So my thought is, if someone has my vault, I don't think they have my current password.  Not to mention there's also the aspect of multifactor authentication that I have on everything.



STEVE:  Yup.



ANT:  So I think I'm fine.  And I like to assume that, just because LastPass got themselves in this mess, they probably learned from it and took a lot of losses on that.  And it's probably going to make them better with their product going forward.



STEVE:  I really can't find any fault in that thinking.  We know how this happened now.  We know that a developer who was working at home had failed to keep his Plex server...



ANT:  Plex server; right?  Yeah.



STEVE:  ...up to date.  And they got in through a known vulnerability in the Plex server, onto his network, onto his home network.  Then they jumped from there, using his laptop connection, through the VPN, into the developer side of LastPass, and then that's how they set up shop.  I'm more comfortable knowing how this happened than if it was just a big mystery and they never found out, because we understand the path, and we now know there's no question that they have arranged not to allow anything like that to happen again.



ANT:  Right.



STEVE:  So, you know, I am of the opinion, you know, it's why I didn't jump ship when they first had the first announcement of there being some sort of problem.



ANT:  Right.



STEVE:  I think if you have a very high iteration count, and I think they're now recommending 600,000 or more for LastPass, there's no reason to believe that their technology is fundamentally defective.  I don't think it is.  And there is reason to believe that they've made themselves more secure.  Now, the only caveat I guess I would provide or suggest is that they're owned by a - I can't remember the name of it.  It's a monetary fund.



ANT:  Oh, like some disease?



STEVE:  An equity, a private equity fund, that's the word I was trying to remember, a private equity fund.  And private equity funds or firms, they're not really interested in the technology.



ANT:  True.



STEVE:  They're interested in profit.



ANT:  True.



STEVE:  You know?  They want to turn this thing into money.  So one of the ways you do that is you squeeze it.  You reduce your staff.  You reduce your, you know...



ANT:  All that overhead.



STEVE:  The backup, the off-premises backups.  And you reduce this, and you reduce that.  So I guess I would suggest that that's a warning sign that, you know, like the original founder of LastPass, he's gone.  He's no longer making sure that things continue to be done right.  So, you know, the fact that LastPass is owned by private equity, that spooks me a little bit because...



ANT:  Yeah, that's fair, that's fair.



STEVE:  Because they could be cutting corners, and we wouldn't know it until a problem happened.  Okay.  So Don K. said:  "Thank you, Steve.  You are the only reason I still use this Twitter or 'X' account.  So I can now look forward to deleting it as soon as your email solution arrives."



And Mark Newton said:  "I'm one of the few people who may hold out a little while and hope Elon turns this thing around.  The boy does have a track record of success.  Dropping the name Twitter was almost as dumb as buying the thing.  I'll give him another year.  It will either be turning around or Twitter/X will be a thing of the past."



Now, you weren't here, but last week I announced that I thought that Elon's statement the previous day, Monday a week ago, that he was going to charge everybody, it would no longer be free, I thought that spelled the end of Twitter.  So I explained that I would be putting together an email system in order to send what I'm normally sending by tweet and to create a spam-free means for people to get back to me.  



Now, invariably, some people took exception to that.  And I heard from people who took exception to what they called my "anti-Elon, anti-Twitter rant."  And as I just said, others confirmed what I suspected, which was that the only reason they still had Twitter was to easily receive my weekly Security Now! tweets.  Okay.  So allow me to clarify all this just a bit.



ANT:  Mm-hmm, go ahead.



STEVE:  I'm currently paid up for a year, since I cannot survive Twitter without TweetDeck.  And I am fine with that.  You didn't see me, no one saw me budge one inch when everything else that has happened so far, happened.  I'm also in no big hurry to leave.  My sole concern was that a switch to a 100% paid model - which Leo thinks may never happen, and I understand that, too - would represent a profound change to Twitter.  I don't know how big.  But, I mean, big enough that Leo says, nah, that'll never happen.  I mean, because it would be huge.



I'm already paid up, as I said, so it makes no difference to me one way or the other.  But if there are people, and I have heard from many since, who are only on Twitter to obtain my weekly tweets, then I disliked the idea of any of our listeners feeling that they needed to go "paid" just to continue to receive that Twitter feed from me.  I wanted those people to know that there would be an alternative means for obtaining that information.



GRC currently has no means at the moment for reaching any community outside of Twitter.  So as I've mentioned before, I had planned, and do, to remedy that once SpinRite is released.  So I'll have the ability to mail to lists, and it makes sense for me to have a list for Security Now! listeners of this podcast.  So that's where I stand.  Not anti-Elon, not anti-Twitter.  Just saying, if he's going to force people to pay, and our listeners are already only in Twitter because they want to get my tweets, I will create a different free channel that everyone will be able to use.



ANT:  I dig your stance.  That's a choice that you have every right to make.  If he were to make the platform a dollar a month, you know, which is not a lot, you can decline to use that service.



STEVE:  Right.  And again, as I said, I'm paid up for a year.  I'll be happy to keep tweeting the weekly Show Note links, the Picture of the Week, and a brief synopsis of the show's topics.  That seems fine.  But it no longer seems right for Twitter to be the only way for our listeners, of which there are many, to receive that information.



ANT:  Indeed.  Indeed.



STEVE:  Okay.  Two pieces of miscellany.  Konrad Zydron, he tweeted:  "About that HashCheck SHA-256 fork you mentioned in SN-940 last week," he said, "it's here since 2014."  And there's a GitHub link in the show notes.  And then he also said:  "Another fork with additional BLAKE3 algorithm and faster implementations of SHA-256, 512 and SHA3," and another link.



Okay.  So remember I was talking about having an easy way in Windows to obtain a useful hash of any file that you right-click on.  I talked about HashCheck.  Turns out what I learned is that the fork that I was wishing someone had created was indeed created nine years ago.  And it's on GitHub.



ANT:  Love it when that happens.



STEVE:  Yes.  So I made it, I created a GRC shortcut for it.  It's not the shortcut of the week.  The shortcut is hashcheck.  So the URL is grc.sc, you know, for shortcut, grc.sc/hashcheck.  If you use that link, that'll bounce you over to the GitHub release downloads page, and you can grab it.  It's digitally signed.  I immediately installed it.  It's great.  It's got all the modern hashes that we want.  It's exactly what I was asking for last week.  And it's existed for quite some time.  So thank you.  A bunch of people told me that there had been updates.  So I just quoted Konrad's.  Thank you very much.



ANT:  Nice.



STEVE:  And finally, last week's podcast was titled "When Hashes Collide."  And it's finally safe to say that it made one library information scientist very happy.  The guy who inspired the podcast and that title, he wrote:  "Hi, Steve!"  He said:  "I was absolutely floored while listening to the show on Friday morning to hear that my question was turned into a truly fascinating and very useful deep-dive.  Our patron population" -  that is, of his library - "is a bit bigger than 5,000 individuals.  It's actually closer to 400,000."  He said:  "I did say 'a large public library system in Ohio,' and Ohio is known to be the 'Land of the Libraries,' at least in certain circles.  But the topic you presented is still of course very much applicable."



And then he said:  "The idea of deliberately using the Birthday Paradox and further obscuring personally identifiable information by manipulating the number of bytes produced by the cryptographic hash is an absolutely brilliant way to add a meaningful layer of protection to the pseudonymized data.  Controlling the probability of collisions within the population of patrons is something I didn't even realize I was looking for to give a small degree of uncertainty to the pseudonyms.  Once again, an absolutely fantastic explanation of such a complex topic.  Bravo."



Okay, well, it's a bit embarrassing to read that, but thank you, Ray.  What most pleases me is that Ray "got it" completely.  It turned out to be exactly what he was looking for.  And he's already turned the concept into working code.  He calls it the "Stochastic Pseudonymizer," and for anyone who's curious I have a link to his GitHub project in the show notes.  So Ray, thank you for the closure, and for being the incentive and the inspiration for a little bit of deep-dive nerdism, which we had fun with last week.



ANT:  That's pretty cool.



STEVE:  Ant, after you tell us about our third and final sponsor, I'm going to explain to everyone why I titled today's podcast "We Told You So!"



ANT:  I've got to tell you, I'm looking forward to that.



STEVE:  We'll know who told who what.



ANT:  I am looking forward to that, sir.  Yes, this is another fine week here on Security Now! with Mr. Steve Gibson.  Now, Mr. Gibson, so you told us so?



STEVE:  Not exactly.



ANT:  Okay.



STEVE:  Last Thursday Apple quickly patched three iOS vulnerabilities.  Taken together, they created a zero-day chain that enabled a potent attack that The Citizen Lab in Canada and Google's TAG team, their Threat Analysis Group, TAG team, had jointly reverse engineered from the forensic evidence residue left behind by a successful targeted attack aimed at an Egyptian citizen who had made himself a political target by announcing his intention to run in a forthcoming election against Egypt's current head of government.



I named this episode "We told you so!" in acknowledgement of our many listeners who took exception to my assertion that not all websites in the world needed to be HTTPS.  What Citizen Lab and the TAG team discovered was that it was when this political target was led to a plain HTTP, non-HTTPS website, that his network traffic was intercepted on the fly by a man-in-the-middle middlebox.  That device returned an HTTP 307 Temporary Redirect, which caused his iPhone to be redirected to a spoofed website where the malicious content could be injected into his phone via this chain of three carefully designed exploits.  So, indeed, no argument.  You told me so.



Let's take a closer look at some of the interesting details here.  The target of the attack was Ahmed Eltantawy, a former Egyptian Member of Parliament.  Earlier this year in March he announced his intention to run in the upcoming Egyptian presidential election, stating that he planned to offer a "democratic" alternative to the current president, who some feel is not very democratic.  Following his announcement, he, his family members, and supporters have been subjected to harassment, including reports of arrests of 12 family members.



Egypt's current President Abdel Fattah el-Sisi has been in power since 2014, when he led the military overthrow of President Mohamed Morsi.  Sisi has been widely described as an autocrat.  Human rights groups, Amnesty International and Human Rights Watch, have documented widespread human rights abuses under el-Sisi's regime, including repression against civil society groups, activists, and political opposition.  So Egypt's current president doesn't appear to be a nice guy who plays fair or would be in favor of free and fair elections.



Ahmed became suspicious about the safety of his phone and reached out to Citizen Lab, who then performed a forensic analysis of his device.  Their analysis revealed numerous attempts to target Ahmed with Cytrox's Predator spyware.  Citizen Lab has previously documented Cytrox Predator infections targeting the devices of two other exiled Egyptians, an exiled politician, and the host of a popular news program who chose to remain anonymous.



Citizen Lab brought Google's TAG team into the project to perform some of the heavy reverse-engineering forensics which the TAG team excels at.  This allowed them to obtain the complete iOS exploit chain that had been targeted at Ahmed.  They initiated a responsible disclosure process with Apple, which assigned three CVEs to vulnerabilities associated with the chain.  There was CVE-2023-41991, which was a security breach.  A malicious app may be able to bypass signature validation.  Whoops.  992 in the kernel, a local attacker may be able to elevate their privileges.  And 993 in WebKit, processing web content may lead to arbitrary code execution.  And there again is a browser exploit, and we were just talking about the problem of browser security.



The zero-day chain was hosted on sec-flare[.]com, and the exploit also contacted verifyurl[.]me.  After fingerprinting those two websites, they scanned the Internet and identified a large number of other IPs that matched those sites' fingerprints.  In other words, there were other domains that were pointing to the same malware hosting sites, and they considered all of those IPs, and the domain names returned in TLS certificates when they matched the fingerprints, to be directly linked to Cytrox's Predator spyware.



They also observed that some of the domains they'd identified had names suggestive of tailoring towards specific countries or regions, including the Arabian Gulf, Southeast Asia, Angola, the Democratic Republic of the Congo, Egypt, Greece, Indonesia, Kazakhstan, Madagascar, Mongolia, the UAE, and Sudan.  So, wow.



By examining the final stage of the iOS exploit chain, which was an iOS payload, they were able to attribute, with very high confidence, that this was indeed Cytrox's Predator spyware, which they'd obtained another sample of back in 2021, two years ago.  Those two binaries, the old one and this new one, shared a key similarity which could only be accounted for through a common heritage.



Okay.  So what about this on-the-fly interception?  This month of September and last month in August, when the target, Ahmed, visited certain websites without HTTPS from his iPhone, using his Vodafone Egypt mobile data connection, he was silently redirected to a website at the domain "c.betly[.]me" via a on-the-fly network traffic injection.  And that domain was among those that matched Citizen Lab's fingerprint for Cytrox's Predator spyware.



The injection was triggered by a string match on the website specified in the HTTP Host header, as well as the value of the User-Agent header.  I'll note that while the target domain can also be seen at the start of a TLS handshake thanks to SNI (Server Name Indication), subsequent TLS encryption blinds anyone who's eavesdropping from seeing additional connection details such as the User-Agent header, which they use in this case.  But since HTTP leaves everything in the clear, the attackers were able to use these additional signals to reduce mis-targeting since they would not want non-targeted web traffic to be redirected.



So, as I said before, when the injection attack is triggered, the intercepting middlebox immediately returned a 307 Temporary Redirect to the user's iPhone.  That would be invisible to the user, and it also, the middlebox, blocks the legitimate reply coming back from the server, which would have come back from the authentic server.



The body of the page which was returned from the malicious redirected website included two iFrames, one which they described as containing apparently benign bait content.  It was a link to an APK file not containing any spyware.  But the second one was an invisible iFrame containing a Predator infection link hosted on that sec-flare[.]com site.



Next, they wanted to understand exactly where in the network this injection was being made.  So they performed a bunch of tricky IP packet logic to localize the malicious middlebox.  They needed to get far more sneaky than just using a trace route-style TTL (Time To Live) manipulation because they discovered that another benign traffic management middlebox was also in the path, and it was located close to Vodafone's subscribers.  The reason that was a problem is that all the traffic that passed through it had its TTL reset so it was not possible to, from the iPhone, send packets in a trace route style with differing TTLs causing a premature expiration of the packet that would allow you to map out the route.  As soon as you hit that close middlebox it reset all the TTLs.



Nonetheless, they were able to eventually determine that the malicious middlebox was on the interconnect between Telecom Egypt and Vodafone Egypt, which is exactly where you'd expect it to be if you wanted it to be able to reliably intercept and infect targeted smartphone users.  They were also able to identify the specific hardware by probing it remotely and comparing its responses to other known appliances.  It is a commercial off-the-shelf device made by a company called Sandvine who has offices in San Jose, California; Sweden; Ontario, Canada; and India. It's one of their PacketLogic devices, and if anyone's curious, it's sandvine.com/products/packetlogic.



I looked at the data sheet for this device.  It boasts features like policy-based traffic management capabilities that include asymmetric traffic control, traffic shaping and filtering, traffic flow classification and prioritization, traffic monitoring, and packet rewrite.  In other words, a full spyware capability.  Also they said content intelligence enables network operators, cloud providers, and high-speed enterprises to combine the policy management capabilities of PacketLogic with industry-leading URL content categorization functionality.  HTTP header enrichment leveraging PacketLogic's stateful awareness and subscriber awareness.  Anyway, this is exact - it's a commercial box.  You don't need to build your own.  You can buy one of these things, stick it anywhere you want, and perform this kind of traffic interception using commercial off-the-site hardware.



ANT:  This is just one SKU or multiple SKUs?  The website looks like it has multiple SKUs there.



STEVE:  Oh, yeah.  They've got - basically it's how much traffic, what volume of traffic do you want.  And so they keep adding interface speed and more interfaces in order to get beefier and beefier devices.



ANT:  Got it.



STEVE:  But they're all part of that same family.  So place the device in the link between Vodafone Egypt and the rest of the Internet when you want to be able to mess with smartphone users.  Citizen Lab also noted that they're unable to conclude whether this device sits on the Telecom Egypt side or the Vodafone Egypt side of the link.  But it really doesn't matter.  They suspect it's within Vodafone Egypt's network because in order to be able to precisely target injections at a specific Vodafone subscriber, you'd require integration with Vodafone's subscriber database.  So that also suggests an intimate relationship between Vodafone and this device.  So that's also, you know, part of the package.  And given that the injection is operating all inside Egypt, that this class of spyware is sold to government agencies, and that Egypt is a known Predator customer, Citizen Lab believes that it's highly unlikely that this could have been occurring outside of the purview of Egyptian authorities.  In other words, this was sanctioned.



And as if all of this wasn't enough, Ahmed additionally received three SMS messages as far back as September of 2021, so two whole years ago in September, as well as more recently in May and September of this year.  All three posed as messages originating from WhatsApp.  The fraudulent messages invited him to visit a link contained within the message to "terminate" what the messages claimed was a new login to his WhatsApp account.  So, right, it was meant to invoke an immediate reaction of, oh crap, click the link because somebody else is logged in illegitimately.  



Interestingly, approximately 2 minutes and 30 seconds after Ahmed read the first September 15th, 2021 SMS message, the Predator spyware was installed on his phone.  The researchers suspect that he did click the message's link, and that triggered the installation.  Since the 2023 messages contain nearly identical bait content, they believe that those messages were also attempts to install the Predator spyware on Ahmed's phone.  And as if the point hasn't been driven home enough, Citizen Lab's write-up of their research concluded the following.



They said:  "Our research reveals the potential insecurities that run through the entire spectrum of the telecommunications ecosystem, including at the network layer, which can be exploited to inject malware on an unsuspecting user's device. Our Internet communications are routed through many networks and middleboxes, some of which can be misused for malicious purposes, particularly if network requests flowing through them are not protected with cryptography.  Although great strides have been made in recent years to 'encrypt the web,' users still occasionally visit websites without HTTPS, and a single non-HTTPS website visit can result in spyware infection."



ANT:  Just one.



STEVE:  Just one.  "This report should serve," they said, "as a reminder of the importance of achieving a 100% rate of HTTPS adoption."  So yes, our listeners told me so.  And just for the record, I never suggested that exactly this was not possible.  In fact, I outlined exactly this architecture for such attacks.  But there's also no question that HTTPS would have robustly blocked this particular attack.  It would not have blocked the SMS variant, which we saw did actually infect him two years ago, but certainly the HTTP attack which relied upon on-the-fly traffic interception would have been blocked.



Okay, now, unfortunately, the only way to really prevent such attacks would be to remove HTTP support from the Internet.  And that still seems extreme.



ANT:  Right.



STEVE:  I mean, it would break all non-HTTPS sites.  You simply couldn't get there.  The other countermeasure would be to display very scary warning messages from the browser before it takes you to a link you clicked on.  But as we know, users typically just push past anything that gets in their way.



ANT:  Exactly.



STEVE:  So that would be ineffective or at least less effective.  Now, another idea might be to begin neutering what web pages delivered over HTTP can do.  Like how about not allowing iFrames on such pages.  Or how about disabling all scripting on HTTP-only sites.  They would still load, but they would be grossly restricted to the sorts of old school content that such sites often have, while potentially dangerous and abuse-prone capabilities would be absent.  And this might incentivize the owners of such sites to step up to HTTPS which, after all, is no longer difficult or expensive.  You just have to want to do it.  And then you can use, you know, any of the free certificates that are being offered by DigiCert or Let's Encrypt or whomever.



ANT:  Right.



STEVE:  So we do have a problem.  Somebody got this, got an HTTP link in front of Ahmed.  And as we know, now browsers are not showing us HTTPS or P://.  Right?



ANT:  Right.



STEVE:  They're wanting to simplify the URLs.  So he clicked it.  Because it wasn't HTTPS he jumped to - he was trying to jump to maybe a benign HTTP site.  That got intercepted, redirected to a malicious HTTP server, which then loaded those two iFrames and compromised his phone using a chain of three still at the time active zero-days which Apple fixed in an emergency update last week.



So I don't know what the solution is.  But as our listeners said, "Gibson, you're wrong on this."  And yes, they told me so.



ANT:  I love that.  And I've got to tell you, trying to make the sites less effective, I don't know if that'll be a problem either, considering all the malicious sites could do is just stick a pop-up there similar to what we see now when you go to certain websites and say, "Hey, looks like you're running an ad block routine.  Turn that off half a second."  Or "It looks like you're disabling this.  Can you fix that just for a click in here so we could..."



STEVE:  Well, remember that the only way they can do that is running script.  You can only do a pop-up if you run JavaScript.  So my thought was don't have the browser allow any JavaScript on a non-HTTPS site.  I mean, really neuter the site so that it cannot be malicious.  All it can do is display plain vanilla content, you know, old-school.  In that case, you know, sites could still remain HTTP.  They just wouldn't - they'd have a very difficult time being malicious.



ANT:  Right, right.  Unfortunately, I still think that's a bit much to ask for some of the normal folks out here in these Internets.



STEVE:  What else; you know?  



ANT:  It's hard to get people to stop using the same password for everything.  Now you're telling them to configure their browser?  Oh, boy.  I don't think it's going to happen.  Damn.  Mr. Gibson.  This has been a ton of fun and a ton of great information.  I really enjoyed being here with you today on Security Now!.



STEVE:  It's great to have you, Ant.



ANT:  Learning more and more.



STEVE:  Thank you for standing in for Leo.  Bye.



Copyright (c) 2023 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#942

DATE:		October 3, 2023

TITLE:		Encrypting Client Hello

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-942.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  Just how irresponsible have the developers of the most popular email server on Earth been shown to be?  What nefarious intent has infiltrated AI dialogue?  Windows 11 now supports Passkeys.  But what does that mean for the browsers and add-ons that already do?  The tech press is warning about a new password stealing attack against users of public WiFi.  How does it work?  Are they right?  And just how worried should we be?  Why isn't there a Nobel Prize for math?  Was it due to a jealous husband?  Is our email address the only way for the LastPass vault decryptors to target their victims?  Is there any way to keep AI models from training on our website's content?  Does anyone have a shortcut for learning Syncthing?



Is it best not to keep lithium-ion batteries fully charged?  Where's a clever place to keep encrypted data offline, and what happens to old mathematicians?  After we answer those questions and more we're going to look at the hoops the Internet's designers have had to go through to keep eavesdroppers from learning which sites we visit.  Welcome to the Security Now! Podcast #942 for October 3rd, 2023.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  There's a big flaw in the world's most popular email server, and it's been there for a long time.  We'll talk about a new attack on WiFi and passwords that you don't have to worry about.  Whew.  And why is there no Nobel Prize for Mathematics?  Steve Gibson and a whole lot more coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 942, recorded Tuesday, October 3rd, 2023:  Encrypting Client Hello.



It's time for Security Now!, the show featuring the star of the TWiT network, Mr. Steve Gibson.  And I say that, Steve - hello, Steve.



STEVE GIBSON:  Hello, Leo.



LEO:  I saw that because I was in Green Bay, Wisconsin, as you know, last week.  Thank you, Ant, for filling in.  And Friday we had a meetup.  About 50 fans universally said, "Thank god Steve extended his show number beyond 999."  They were worried.  So they're very relieved.  Thank you, on behalf of all of Wisconsin.



STEVE:  I'm glad I did, and I'm glad Michael got to see his Packers.



LEO:  Oh, my.  It was a terrible game, but that's okay.  We had fun.  And if you're an NFL fan, going to Lambeau Field is really kind of going to Mecca.  So it was great for all of us.  We had a great time, yeah.



STEVE:  So Leo, if you've been short of sleep lately, once we get into the main topic today, that might be a good time.



LEO:  Oh, dear, no.  Why do you say that, Steve?



STEVE:  We're going to talk about, as I was writing this up, I was thinking, oh, wow, I mean, this is really interesting to a subset of our listeners.  I mean, the idea...



LEO:  Steve, Steve, you're not supposed to say that.  You're supposed to say, oh, you're going to want to stay tuned to find out about Encrypting Client Hello.



STEVE:  It's crucial that we talk about it.  But one of the things that I've liked best about the Internet is that it was designed in the beginning by some really smart people who basically kept it simple.  I mean, even when we tackled, okay, how does it work, like how does packet routing work, once we explained that a packet has an address, and when it goes to a router, the router looks at it and goes, oh, well, that means I need to send it down this wire, or I need to send it down that wire.  And then it goes to another router.  And that router does the same thing.



LEO:  Yeah.



STEVE:  I'm going to send it down that wire, or I'm going to send it down that wire.



LEO:  Yeah, it's very simple, yeah.



STEVE:  Yes.  It was simple.  And even as we've added more things, it's basically stayed simple.  Now that changes.



LEO:  Uh-oh.



STEVE:  It turns out that there has always been a privacy leak.  Even with HTTPS and TLS and all of our authentication and all of our encryption, there's still been a problem.  And here's where it gets - the story turns sad.



LEO:  Oh.



STEVE:  Because to fix it, I mean, to really fix it, we lost simplicity.  It stopped being clean in a way that it has always been.  So we're going to talk about this today, and we have to because it's going to be the way the world works in the future.  But oh, it's not simple.  We lost that elegance.  But first - but we're going to get to that.  We're going to answer a bunch of questions first because our listeners have said we like the question format.  So just how irresponsible have the developers of the most popular email server on Earth been shown to be?



LEO:  Oh, boy.



STEVE:  Bad.  What nefarious intent has infiltrated AI dialogue?  Windows 11 now supports Passkeys, but what does that mean for the browsers and the add-ons that already do?  The tech press is warning about a new password stealing attack against users of public WiFi.  Does it work?  Are they right?  And just how worried should we be?  Why isn't there a Nobel Prize for Math?  Was it due to a jealous husband?



LEO:  What?



STEVE:  Oh, yeah.  Is our email address the only way for the LastPass vault decryptors to target their victims?  Is there any way to keep AI models from training on our own websites' content?  And I'm not sure why you would want to, but okay.  Does anyone have a shortcut for learning Syncthing?  Is it best not to keep lithium-ion batteries fully charged?  Where's a clever place to keep encrypted data offline, and what happens to old mathematicians?  After we answer those questions and more, we're going to look at the hoops the Internet's designers have had to go through to keep eavesdroppers from learning which sites we visit.  Welcome to the Security Now! Podcast #942 for October 3rd, 2023.  Buckle up.



LEO:  And prepare to be bored, apparently, a little later on.  But, you know what, I know you're not going to...



STEVE:  No, no, no, no.  For the right people, this is one of those where they're going to be glad the propeller on their beanie is as large as it is because it needs to grab a lot of air.



LEO:  So this is a challenge.  That's good.



STEVE:  This is a good one.



LEO:  Here's a chance for you to stretch your brain a little bit.  



STEVE:  And of course we do have a good picture.



LEO:  We do.  I am ready for a picture.



STEVE:  So anyway, this is clever.  Some family has three cats.  And they thought, hey, that's nice, we'll get this cat food holder that's got three bowls.



LEO:  We have this exact product.



STEVE:  No kidding.



LEO:  Note the ears, yes.



STEVE:  Even with the kitty ears showing.



LEO:  Even with the ears.  And those bowls.  I know this product.



STEVE:  So anyway, this is a two-frame picture.  In the first frame we see the three bowls, sort of in a nice stand.



LEO:  Each cat gets their own, yeah.



STEVE:  Each cat has its own, exactly, his own food.  And the caption here is "Developer:  Makes a simple, intuitive UI."  And then in the second frame we see what the cats have done with this.  The white cat is standing behind the bowl on the left, but is eating from the bowl on the far right.



LEO:  This is so real.



STEVE:  Exactly.  The black cat is not dissuaded by this. It stuck its head underneath the white cat's tail, and it's decided to eat from the bowl on the left, even though it's standing in front of the bowl on the right.  And the tabby is in the middle with its head under both of the cats, the black and the white cats, which are crossed over to the opposing side bowls, and it's eating from the middle.  So essentially it's the only one that understood the user interface.  The other two not so much.  And the result is just sort of not what the designers intended.



LEO:  Typical.  They're cats.  C'mon, man, they're cats.



STEVE:  And then of course the second frame, where the first frame was labeled "Developer:  Makes a simple, intuitive UI," the second frame is labeled "Users."  Yeah, exactly.



Okay.  So here's the question.  What can make things worse for a very widely deployed, public-facing Internet server, like server family, which is found to be vulnerable to remote code execution by anyone, meaning any unauthenticated connection, thanks to classic buffer overruns?  What can make it worse?  Well, how about that server's publisher ignoring the ZDI, you know, the Zero-Day Initiative's attempts at responsible disclosure of these problems to them for over a year.



LEO:  Oh, boy.  Yikes.



STEVE:  And the server in question is the most popular server on the Internet.  It's the open source Exim (E-X-I-M).



LEO:  Oh, boy.  A lot of people use this.



STEVE:  Yes.  Three and a half million people at last count by Shodan.



LEO:  To reassure people, our sponsor Fastmail uses Cyrus, not Exim.  But a lot of others do.



STEVE:  Yup, lot of others.  So 3.5 million currently exposed online based on a recent Shodan search, most of them in the U.S., I think it's 1.9 million some were in the United States, followed by Russia and Germany in second and third places.  Back in June of 2022, ZDI reached out to Exim to inform them of multiple known highly critical problems that had been found by an anonymous researcher.  And we're talking about highly critical as in a CVS score of 9.8, which as we know, that's difficult to get.  On June 14th of last year, 2022, after asking for and receiving a contact, you know, contact information for the right person to speak to with Exim, ZDI reported the trouble.  So that was June 14th, 2022.



Then ZDI waited, and waited, and waited, until a little more than 10 months had passed.  On April 25th of this year they asked for an update.  Exim said "Huh?" and asked ZDI to please resend the reports.  On May 10th, ZDI re-sent the vulnerability reports.  Then another four months went by until finally last Monday the 25th of, what, September, ZDI again asked for an update while also informing Exim that, you know, we've been patient enough.  We're going to publish the entire case as a zero-day advisory in two days, which was last Wednesday.  And ZDI has written in their disclosure, they said:  "Given the nature of the vulnerability, the only salient mitigation strategy is to restrict interaction with the application."  Which is a very political way of saying "Unplug the server now."



Two days later, last Friday, BleepingComputer's headline read:  "Millions of Exim mail servers exposed to zero-day remote code execution attacks."  BleepingComputer wrote:  "A critical zero-day vulnerability in all versions" - all versions - "of Exim mail transfer agent (MTA) software can let unauthenticated attackers gain remote code execution on Internet-exposed servers."  Yes, that's all 3.5 million of them.



They said:  "Found by an anonymous security researcher and disclosed through Trend Micro's Zero Day Initiative, the security flaw is due to an out-of-bounds write weakness found in the SMTP service.  While this type of issue can lead to software crashes or corruption of data following successful exploitation, it can also be abused by attackers for code or command execution on vulnerable servers.  ZDI security advisory published on Wednesday, meaning two days before last Friday when BleepingComputer published this, explains:  'The specific flaw exists within the SMTP service, which listens on TCP port 25 by default.  The issue results from the lack of proper validation of user-supplied data, which can result in a write past the end of a buffer.  An attacker can leverage this vulnerability to execute code in the context of the service account.'"



And BleepingComputer continues:  "While ZDI reported the vulnerability to the Exim team in June of 2022 and re-sent info on the flaw at the vendor's request in May of 2023, the developers failed to provide an update on their patch progress.  As a result, ZDI published an advisory on September 27th, with details on the zero-day and a full timeline of all exchanges with the Exim team."



Okay.  So following ZDI's actions there was some back and forth on the Open Source Security mailing list, where ZDI write:  "ZDI reached out multiple times to the developers regarding multiple bug reports with little progress to show for it.  After our disclosure timeline was exceeded by many months, we notified the maintainer of our intent to publicly disclose these bugs, at which time we were told, 'You do what you do.'"



So for all of our listeners, not much imagination is required to know what's going to happen next.  The Exim email message transfer agent system is open source and is thus wide open for inspection by anyone.  And the ZDI write-up says that the flaw is in a component that handles authentication.  So now the world knows that somewhere around 3.5 million of those servers, all of which are publicly exposed to the Internet, contain multiple, classic, remotely exploitable buffer overrun flaws enabling remote code execution.  So get yourself some popcorn.  Get comfortable.  Sit back, relax, and be glad that you're not running a not-yet-patched Exim server on your network.  And watch what happens next.



I have a feeling we'll be talking about this for at least the next couple weeks because, as we know, even if patching was made available, and the people running the servers were notified, there would still be, you know, an exponential curve of patching with lots of machines never getting the message, being on their networks and being left unpatched.  The bad guys don't even have to reverse engineer from the binary.  They could just go, ooh, good, let's just read the code and find the problem.  So that's the world we live in today.



Speaking of BleepingComputer, they carried another piece of news recently, this one with the headline "Bing Chat responses infiltrated by ads pushing malware."  And of course Bing Chat is AI-powered now.  BleepingComputer explains.  They said:  "Malicious advertisements are now being injected into Microsoft's AI-powered Bing Chat responses, promoting fake download sites that distribute malware.  Bing Chat," they wrote, "powered by OpenAI's GPT4 engine, was introduced by Microsoft in February of 2023 to challenge Google's dominance in the search industry.  By offering users an interactive chat-based experience instead of the traditional search query and result format, Bing Chat aimed to make online searches more intuitive and user-friendly.  In March, Microsoft began injecting ads into Bing Chat conversations to generate revenue from this new platform.



"However, incorporating ads into Bing Chat has opened the door to threat actors, who increasingly take out search advertisements to distribute their malware.  Furthermore, conversing with AI-powered chat tools can instill unwarranted trust, potentially convincing users to click on ads, which is not the case when skimming through impersonal search results.  This conversational interaction can imbue AI-provided URLs with a misplaced sense of authority and trustworthiness, so the existing problem of malvertising in search platforms is amplified by the introduction of AI assistants.  The fact that these ads are labeled as promoted results when the user hovers over a link in Bing Chat conversations is likely too weak of a measure to mitigate the risk.



"Malicious ads spotted by Malwarebytes are pretending to be download sites for the popular 'Advanced IP Scanner' utility, which has been previously used by RomCom RAT and Somnia ransomware operators.  The researchers found that when you asked Bing Chat how to download Advanced IP Scanner, it would display a link to download it in the chat.  However, when you hover over an underlined link in a chat, Bing Chat may show an advertisement first, followed by the legitimate download link.  In this case, the sponsored link was a malvertisement pushing malware."



Okay.  So as we know, it's not that malvertising is new.  It's not.  But we know that the human factor is an enduring vulnerability.  It's the reason why phishing attacks remain among the most successful, no matter what else everyone does.  So I think their point is a good one.  There is something more cozy and personal about chatting with an AI.  For many users it will seem more authoritative.  So things it recommends will have more salience than links appearing in a typical Google search.  If you couple that with Microsoft's decision to monetize this facility through real-time ad delivery, and if bad ads are able to slip past Microsoft's screeners, as Malwarebytes has found is happening, then that's a formula for an updated form of exploitation.



We do have, however, some good Microsoft news.  They're rolling out support for Passkeys in Windows 11.  Last Tuesday, exactly a week ago on the 26th, Microsoft announced sweeping improvements to the Windows 11 desktop experience.  Among those is support for Passkeys.  This had been available in the Windows Insider program since June; but with last week's big update, Passkeys are now available for all Windows 11 users.  Under Windows 11, Passkeys are created through Windows Hello.  Users can manage their saved Passkeys by heading to Start, then Settings, then Accounts, then Passkeys.  So there is now a Passkeys entry under Accounts, which is under Windows Settings.



Now, Microsoft says Passkeys on the Windows 11 desktop will work with popular browsers, including its own Edge, and but also Chrome and Firefox.  What's unclear at this point is what that means when one of those browsers contains its own support for Passkeys.  You know, Chrome, Edge, Safari, and Opera all currently support Passkeys natively.  So how does that interact with the underlying OS support for Passkeys?  And of course cross-browser support through add-ons is coming soon.  So, okay, now there's a third player who's also going to be supporting Passkeys.  It's going to be interesting to see how all of this sorts out.  And of course, you know, we have the problem that the Passkeys themselves, as we know, are not readily transportable cross-environment.  It is possible to create another Passkey in a different environment, but that just creates basically a fork of your logon credentials with separate Passkeys in separate environments.  So again, we're going to have to see how this all works.



Two other things, though.  In the past we've talked about the power of whitelisting applications that have been approved to run.  So two other notable enterprise-related features have also appeared in this update to Windows 11.  One is enhancements to the built-in Windows Firewall.  The other is a new Custom App Control option to ensure that only approved and trusted apps are allowed onto devices to protect endpoints from rogue code.  Microsoft said, as part of this, they said:  "By preventing unwanted or malicious code from running, application control is a critical part of an overall security strategy.  Application control is often cited as one of the most effective means," they said, "of defending against malware."  And I definitely agree.  It is annoying in any system that's inherently dynamic to have this since any changes require multiple steps.  But dynamic systems are also the ones that are in the most danger because  things are changing on them.



So, you know, it's a classic case of you can either have security and jump through hoops in order to allow something that is not currently whitelisted to get trusted and whitelisted.  And of course, you know, whitelisting's no good unless you're very careful about that process.  Or not.  You know, you can just leave your system wide open.  So I do commend Microsoft for creating the option for users of Windows 11.  It is under any circumstances definitely a good thing.



Okay.  Now, here's a case where the tech press got a little overheated.  But it's certainly interesting, if nothing else.  A team of seven Chinese researchers at three different universities have done some interesting work.  And when I began writing this up I used the term "amazing work" rather than "interesting."  Their work is amazing, but their results are only interesting due to the impractical number of preconditions that need to be established in order for this to work.  And as we'll see, it renders it mostly of academic interest.  Stated another way:  "It kinda worked in the lab."



Despite that, predictably, most of the headline-driven tech press went nuts over this because the research paper, and I guess a lot of the press just read the headline, it was published a few weeks ago titled:  "Password-Stealing without Hacking:  WiFi Enabled Practical Keystroke Eavesdropping."  None of that is true, but makes a great headline.



Having actually read the paper, the issue I would take with their paper's title would be over their choice of the word "practical."  However, if you change the word "practical" to "barely theoretically possible on a good day when the wind is blowing in the right direction," you know, unfortunately that takes a lot of the punch out of the headline.



LEO:  It does, yeah.



STEVE:  And they do deserve to have some punch because what this group of researchers managed to pull off given a bizarre side-channel is impressive, even if it isn't even remotely practical.  And of course we can never fully discount Bruce Schneier's observation that "Attacks never get worse, they only ever get better."



LEO:  Other way around.  They never get better, they only get worse.



STEVE:  No, no, no.  No, no.  Attacks...  



LEO:  Oh, I hear what you're saying.  It depends whose point of view.  From the bad guys' point of view, they only get...



STEVE:  Correct.



LEO:  Right, okay.



STEVE:  Yes, because they're the attackers.



LEO:  They're getting better, yeah.  If you're the victim, they only get worse.



STEVE:  That's right, yes.  Okay.  So the underlying enabling technology that these engineers went for is the result of very clever engineers trying to squeeze ever more bandwidth out of an already bandwidth-constrained environment.  Ten years ago, a feature known as "beamforming" was introduced in WiFi 5, more formally known as 802.11ac.  The idea behind beamforming is simple physics, but it's still somewhat mindboggling to imagine that consumers are able to purchase something that does this without even knowing about it.



Modern WiFi access points contain multiple antennas.  Individually, each antenna is omni-directional.  It sends and receives uniformly in all directions.  But collectively, some magic can happen.  If the access point wishes, for example, to send a stronger signal to a receiver that's directly in front of it, sending the WiFi carriers "in phase" from its antennas will result in each antenna's carrier, you know, radio frequency carrier wave, summing with the others to produce the strongest signal where they are all in phase, which is either directly in front of or directly in back of the antenna array.



But what's cool is that off-axis, the physical distance between the access point's individual antennas will cause the individual radio frequency carriers to become out of phase with one another.  That is, off-axis.  The carrier waves will stop summing together to create a stronger signal and will even work to cancel each other out.  In other words, a properly driven antenna array is able to deliberately form transmission beams where the phases of their carrier signals align to strengthen their signals, and it will create "dead zones" where their carriers cancel each other out.  And this can also work in reverse on the receiving end to cause the array to be selective about where it is listening with the greatest sensitivity.  Again, the physics of this is simple.  But the idea that this is actually going on, and that we all now just take it for granted, boggles the mind.



Okay.  In order to pull this off in practice, the access point and each of its many mobile subscriber radios need to establish an explicit side channel where they're able to interact, not about the actual user data that may be flowing back and forth, but about the channel's metadata which describes their wireless relationship with all this beamforming in real-time.  There's a whole other dialogue going on in the background.  This metadata is known as BFI, Beamforming Feedback Information.  In real-time, WiFi 5, 802.11ac devices, including smartphones, are sending back detailed information about the signal they're receiving from the access point base station to which they're connected.



Now, okay.  Remember back in 2010, Apple hit a road bump on their way to world domination with the iPhone 4.  It turned out that the redesign of the phone's antenna system resulted in the phone's radio performance being unduly sensitive to its users' grip and hand position.  The term that was coined for this was "Antennagate," which led to Apple's official statement at the time, which read, this is Apple:  "Gripping any mobile phone will result in some attenuation of its antenna performance, with certain places being worse than others depending on the placement of the antennas.  This is a fact of life for every wireless phone.  If you ever experience this on your iPhone 4, avoid gripping it in the lower left corner in a way that covers both sides of the black strip in the metal band, or simply use one of many available cases," they said.



In other words, this physics of radio and antennas and the radio-attenuating properties of people's water-laden hands and fingers remains true today, though in the case of Apple's iPhones they've learned some valuable lessons, and it's less of a problem for them.



What these intrepid Chinese researchers discovered and then wrestled to the ground was that the motions of any smartphone user's fingers, as they move them around their phone's touchscreen while entering passwords, passcodes, and so forth - what should be completely private information - will naturally affect their phone's real-time signal reception, and that today's 802.11ac devices will be broadcasting the details of their phone's hand-motion-affected signal reception, in the clear and without encryption, in real-time back to the access point and also to anyone else nearby who might be interested in receiving and interpreting it.



Okay.  So the "interpretation" is the trick, which is why I characterize this as "barely theoretically possible on a good day when the wind is blowing in the right direction."  Essentially, they're getting nothing more than the one-dimensional received radio signal strength information, and they're managing to turn this into, well, something.



If they train, if they highly train a powerful recognition system on a single specific setting and individual, who's not changing the grip on their phone, where the system has already learned to associate the Beamforming Feedback Information-related signals to what's being entered, then under all of those constraints, their research shows that they are able to determine a single numerical key that the user has entered - and we should really say "reentered" - with 88.9% accuracy.  Considering that all they're receiving is the smartphone's received signal strength in real time, that's still impressive.



But despite all of their work, and through no fault of theirs, it falls far shy of justifying headlines such as these three, which were just recently printed:  "Exploit steals passwords by tapping into keystrokes."  And "New Cybersecurity Threat 'Wiki-Eve' Allows Hackers to Steal Passwords."  And finally, "Using Free WiFi?  Better Watch Your Passwords."  In other words, you don't have to be worried about anything.



If anyone may have encountered any of those or similar headlines in the past few weeks, I think it's safe to say that your personal keystrokes remain safe from arbitrary harvesting in public WiFi settings.



LEO:  What a relief.



STEVE:  You know?  Yes.  Bruce is right that attacks only ever get better, that is, the strength of attacks only ever increases.  But the limitations inherent in this one, to a single previously trained instance, means that it is only of theoretical interest at best.



And I did want to just note that while I was looking up Apple's iPhone 4 statement, I was greeted by Engadget's pop-up, which said:  "Review your global privacy control preferences."  And it said:  "You're using global privacy control."  Right.  How dare you?  They said:  "This leads to a lower quality experience on Yahoo! by blocking certain editorial content, including embedded tweets and YouTube videos and" - this is what they really care about - "third-party ads that are relevant to your interests."



Anyway, we've discussed this before where it then goes talking about the technical identifiers which they use.  And we looked at those, too, and they're truly horrifying.  So anyway, I was offered the option of allow or don't allow.  And, you know, I had already got what I wanted, so I said thanks anyway and went elsewhere.



LEO:  So why is there no Nobel Prize for mathematics, Steve?



STEVE:  Okay.  So a little bit of context from last week for you, Leo.  We had a listener who wrote to me and said that I was incorrect in an earlier statement I had made that no mathematical algorithm could be used to generate truly random numbers.  He said he had invented such a mathematical algorithm, I think it was 45 years before, and was surprised that no one had done that since.  So I sort of, you know, I mean, I explained why I still believed that it was not possible for a deterministic mathematical algorithm to produce truly random numbers, and suggested that if he actually had such a thing, he could probably win a Nobel Prize for Math.



Anyway, so Jaque Jarnell said:  "Hey Steve, quick correction to last pod Episode 941.  There is no Nobel Prize for Math."  So seeing that, that surprised me since it would seem that there's a lot to math that might be prizeworthy.  And there turned out to be a few interesting bits surrounding this.  What I immediately encountered was the statement that, on the Internet of course, where everything is true:  "No Nobel Prize is awarded for mathematics because a mathematician was carrying on an affair with Alfred Nobel's wife."



LEO:  What?  Oh, my god.



STEVE:  Now, that, you know, that would be quite petty, I think, you know, to punish all future mathematicians throughout time.  So Snopes, however, disabuses us of that fanciful notion.  They explain:  "The renowned Nobel Prize is the legacy of Swedish chemist, inventor, and industrialist Alfred Nobel, whose 1895 will specified that most of his fortune be set aside to establish a fund for the awarding of five annual prizes 'to those who, during the preceding year, shall have conferred the greatest benefit on mankind.'  The first Nobel Prizes were distributed on 10th of December, 1901, the fifth anniversary of Nobel's death, for achievements in the fields specified by Nobel:  physics, chemistry, medicine, literature, and peace.  And then a sixth prize was added, a sixth category, the category of economics was added by the Bank of Sweden starting in 1969."



And Snopes continues:  "In the century since the Nobel Foundation was established, many have speculated on the reasons why Alfred Nobel did not provide for a prize to be awarded for achievement in the field of mathematics.  Surely an eminent man of science such as Alfred Nobel could not simply have forgotten about mathematics, so he must have had a good reason for omitting it.  With no obvious reason at hand, people invented one, and as usual the invented tale had a bit of salaciousness to it.  It was said that Alfred Nobel deliberately avoided establishing a prize for mathematics out of vindictiveness because a prominent Swedish mathematician was carrying on an affair with his wife.  However, one little problem with that.  The wife theory is easily discounted since Nobel was never married."



LEO:  Oh, well, there you go.



STEVE:  Yeah.  I'll also note that the Nobel Prize Internet Archive has an extensive page addressing this question, since it has obviously puzzled many others.  However, to read that page, you'll need to tell your browser that it's okay to visit since it's HTTP only.



So Wisser tweeted from @wisser, he said:  "Hi, Steve.  Thanks for the show.  Just a short note.  The Nobel Prize is not awarded for mathematics.  Mathematicians" - as you said, Leo - "can only hope for the Fields medal.  Greetings from Stockholm."



Vampire tweeted:  "@SGgrc Why not establish a presence in the fediverse?  Its open nature seems right up your alley."  So just to address that, it's just a matter of being spread too thin and of needing to maintain a presence in too many places at once.  At the moment I have email, and GRC's old-school text-only newsgroups where I spend the bulk of my time, since there's a core group of similarly focused people there who are of incalculable value to me in helping to move projects to completion.  And GRC now maintains web forums.  And I still have Twitter, which remains effective.  So adding yet another venue to the mix would take away something from the others.  If anything, what I would prefer to do is to consolidate, rather than further spread things out.



Someone whose handle is Trunolimit, he says:  "Hi.  In regards to LastPass, is the email attached to the encrypted blob the only way bad guys know what blob to spend money decrypting?"  He said:  "When I signed up for LastPass, I made a brand new email that was never used anywhere else."  He says:  "I don't even get spam to that email."  Right, because, you know, you never exposed it apparently.



Okay.  The answer is, unfortunately, no.  Email is not the only way.  For reasons that were never made clear, since it would not appear to be necessary, the user's logon URLs were also left unencrypted in the LastPass vault.  This means that scans of the vault would be used to profile, could be used to profile users' interest in cryptocurrency-related sites to identify potential higher value targets, where decrypting their vault may reveal something that could be used to drain their money.  So given that, it's hard to imagine that the bad guys would not do so after they finish decrypting the vaults of other lower hanging fruit that seem may be hiding some money.



Simon Zerafa tweeted:  "Blocking AI scraping and similar from your web sites.  Robots.txt and other measures to try



to keep content from AI training models."  So Simon tweeted a link to a lengthy post by someone named Neil Clarke which was titled "Block the Bots that Feed AI Models by Scraping Your Website."  So I've included the link in the show notes in case it might be of interest to our listeners or someone our listeners know.  I have no interest in blocking bots from any of my content anywhere since it seems to me that AI bots may be the next-generation Internet search tool, as we talked about before with Microsoft's Bing Bot.  So I'd like to have GRC well represented in the learning models of AI things.  But the short version is that the well-known robots.txt file in the website root directories can also be used to block AI bot scraping just as it does for other sorts of web spiders.



You know, you need to know the name of the "User-Agent" that the bot uses.  For example, the string "CCBot" stands for "Common Crawl bot," which is used by ChatGPT, Bard, and others for training a number of their models.  Anyway, if you're interested in excluding AI training on your site's content, though it's likely too late for what's already been crawled, there are presumably ways to do that moving forward.  I've got a link in the show notes that lists all of the various user-agent strings that can be used if you want more comprehensive anti-AI bot scraping for yourself.



Someone whose Twitter handle is Apples Oranges said:  "Quoting Google Bard AI," so he says, quote, this is...



LEO:  Okay.  Consider the source.  It's an AI.  Okay, go ahead.



STEVE:  Uh-huh.  So this AI said:  "I personally believe that non-deterministic algorithms are not oxymorons.  I think that the term 'algorithm' can be used to describe any process that can be broken down into a series of steps, regardless of whether or not the process is deterministic.  In fact, non-deterministic algorithms are used in a variety of applications, including machine learning, artificial intelligence, and cryptography.  They're often used to solve problems that would be difficult or impossible to solve with deterministic algorithms."  Okay.  Now, that statement begins with "I personally believe."



LEO:  Problem number one.



STEVE:  Which puts me off, yes, it puts me off a bit since Bard is not a person.  So I wonder whether a non-person is able to "personally believe" anything at all.  Not to mention whether a machine can have beliefs.  I suppose if it said "my simulated personality believes," that would at least seem authentic.  You know, we are stepping into a weird world here.



LEO:  Yeah, we are, yeah.



STEVE:  Leo, I have an exceedingly, and I mean exceedingly, bright friend of many decades who has been spending a great deal of time chatting with one of the AIs, I don't remember which.  And he assures me - and the problem is I really respect this person's opinion.  He assures me that over time, with proper grooming, a true personality emerges.  And he did tell me that, apparently, praising it helps a lot.  Wow.



LEO:  My point would be where does that personality come from?  It comes from him, the person; right?  The person is a - and this is my big problem with a lot of conversation about AI is we humans are applying this layer of anthropomorphism on top of it.



STEVE:  Yes, that's exactly the right word, "anthropomorphism," right.  I mean, you know, cats look like they have something in mind when they're doing stuff.



LEO:  Sure.  They're thinking, oh, yeah, sure, right.  No, they're not.



STEVE:  It's like staring at you, thinking, you know, basically it's where's my dinner.



LEO:  This is how humans work.  We do that.  But it's not what - it's a machine.  Now, what about what it says about non-deterministic algorithms?



STEVE:  Well, yes.  As for whether the phrase non-deterministic algorithm is an oxymoron, I will readily concede that the phrase is used by people who don't consider it to be oxymoronic.  But mostly I was just having fun when I was talking about this before, and I believe it's still clear because I didn't say, you know, it was this other guy that talked about a non-deterministic algorithm.  I was talking about deterministic algorithms, and I believe it's still clear that any fully deterministic algorithm cannot produce truly random numbers.  Which, as I said, was my original statement.  It was our listener who introduced the idea of non-determinism about algorithms.  And I was just, you know, saying, well, can you have an algorithm that is not deterministic?



LEO:  Well, I'll give you an assertion.  An AI cannot produce a truly random number, I would guess, I would suggest; right?



STEVE:  If it were staring at the wall of...



LEO:  Lava lamps.



STEVE: ...of lava lamps, and...



LEO:  Or a capacitor hooked up or, you know, some other physical...



STEVE:  Yeah.  And so if you say that an algorithm is flipping a coin, then okay.  But, you know, again, it really comes down to our definition of algorithm.  Is an algorithm, you know, one plus one equals two?  Or can you say that the algorithm is ask a random number generator for a number?  Well, if that's what you're going to do, then I don't think that's an algorithm.



LEO:  Well, we know that quantum computing is not deterministic.  That's its chief advantage; right?  It's neither on nor off.  It could be any variety of states.  We know also that fuzzy logic, we talk all the time about using fuzzy logic, or fuzzing, right, to solve security issues.  That's not deterministic; right?  It's kind of randomized inputs.



STEVE:  Yeah, although there it doesn't have...



LEO:  It's deterministic, I guess.



STEVE:  There it could be pseudorandom.



LEO:  Right.



STEVE:  But, okay.  So, for example, one of the best ways, a little box to generate random numbers is to force current backwards through a diode.



LEO:  Right.



STEVE:  You reverse-bias a diode, which does not - it wants to prevent any current from flowing.  What it turns out is that there is some noise of individual electrons just through heat, and, I mean, true quantum-level stuff, crossing over that boundary junction in a diode.  You amplify that, and you clean it up, and that is a source of true entropy.  And so that's what all the little boxes that say, you know, contains a true random number generator, that's what they have.  You can also use a Geiger tube and stick it out, you know, hold it up in the air.  And when a neutron or whatever the heck it is flies by, or a charred particle passes, it's like, oh, look, I heard a tick on my Geiger tube.



LEO:  These are essentially chaotic inputs, unpredictable inputs.



STEVE:  Right, exactly.



LEO:  Somebody says in the Discord, and I think this is actually a good definition, non-deterministic algorithm would be different outputs for the same input.  Which, by the way, most of the time in programming it's something to be avoided.  Right?  The problem with pseudorandom number generators is eventually they repeat because they aren't really random.  But a diode, backwards diode, or a Geiger counter.



STEVE:  Yeah.



LEO:  It's going to be truly random.



STEVE:  Yeah.



LEO:  I guess dice.  A die is random.  Except there's little influences from the shape of the die.



STEVE:  Yeah.  If you actually, I mean, if it's not perfectly manufactured.  And also, you know, I mean, dies have different numbers of divots on their different faces.



LEO:  Right.  So it's going to...



STEVE:  So has anyone ever actually taken the time to compensate for that?  I don't know.



LEO:  Right.  No.



STEVE:  No.



LEO:  And we know that you can load die by shaving them, just changing the weight of one side slightly.  So that's sufficient.



STEVE:  Yes.  So Leila...



LEO:  Go ahead.  I love this subject because...



STEVE:  It is a fun subject.



LEO:  And to me, that is the criterion I use to determine between AI and coding.  Algorithms are inherently deterministic, in my opinion.  But in AI, because it's creating its own algorithms, is non-deterministic in that sense.  So if it's machine learning or a generated adversarial network, generated adversarial network or something like that, it's writing its own rules.  Not a human.  It may still be, frankly, a deterministic algorithm.  But we don't know what's going on inside that black box.  So from our point of view it isn't.  That to me is AI.  But, you know, so I'm curious.  It's mostly semantics.  You know.  We know what an AI is.



STEVE:  Yup.  So Leila Burrell-Davis, she tweeted.  She said:  "Hi, Steve.  I just came across an excellent introduction to Syncthing on YouTube and thought you might be interested in recommending it to your Security Now! listeners.  It's very well made, and the author is clearly not only extremely knowledgeable, but knows how to explain things to a technical audience.  It's made me think that maybe Syncthing is not too hard for me.  Have a quick look.  I'd be surprised if you don't watch the whole thing, 'Syncthing Made EASY.'"  Anyway, because I was in the middle of compiling the show, I was unable to take the time to watch.  But I quickly scanned the 30-minute video's 165 comments of universally strong praise.  Since Syncthing can be initially somewhat off-putting and confusing...



LEO:  Oh, this is TechCraft.  Yeah, they do good - he does very good stuff.  I would trust him, yeah.



STEVE:  Cool.  So I made it our Shortcut of the Week for any listeners who've heard Leo and I because both of us love and use Syncthing.



LEO:  Oh, yeah.



STEVE:  Grc.sc/842, today's podcast number.  It's about, I think it's 28 and some seconds long, 28 minutes long.  And apparently, you know, it's great.



LEO:  Yeah, it looks pretty good.  I agree with you.



STEVE:  So I recommend it.



LEO:  With my cursory scan of the thumbnails, it looks pretty good, yeah.



STEVE:  So FuerstOpus, he said:  "You suggested neutering what HTTP sites can do to prevent what happened in Egypt.  But I was wondering, couldn't the middlebox redirect send him to a malicious HTTPS site?  If so, scripting on HTTP sites isn't the problem, the HTTP connection is what's vulnerable.  Thanks, Scott."



And I think that's a very good point.  Since the middlebox was returning a 307 Temporary Redirect, and the URL bar was going to be changing anyway as a result, it would be less worrisome if the redirect was to a secured HTTPS site where the malware was delivered.  So I think that's a very good point, and I just wanted to share it with our listeners.



Someone whose - his picture in Twitter is a Cookie Monster.  And of course his handle is "Cookies?!"  He said:  "Heya, Steve.  What happens when the person responsible for securing the family's digital life (photo storage, general storage, accounts, et cetera) unfortunately and suddenly passes away?



"Since becoming a family man many years ago, I've been thinking about the best way to approach the conundrum.  Will your loved ones know what to do if you get run over by a bus or hit by a train?  The conundrum comes when there is that one person in the family who has set up all the accounts, the multifactor authentication, the recovery codes and so on.  The other one loves them, so puts up with all the extra hoops that they're being made to jump through, but may not fully understand all or any of it.  The issue is the line between ensuring there are no weaknesses in your setup versus ensuring your loved one has enough information to figure out how everything works and is able to access the family photos, digital files, and everything else.  Anyways, love to hear your thoughts on the matter.  P.S.:  I have limited time to listen to podcasts and audio books, so unfortunately I had missed the last eight months as I got up to Book 14 of the Silver Ships once you mentioned it!"



LEO:  Oh, wow.  That's great.  That's hysterical.



STEVE:  And he said:  "Plus Leo and his Screen Savers was the reason I asked my parents for Satellite TV when I was 14 in the early 2000s."



LEO:  Now I feel old.



STEVE:  Signed Matt from Australia.



LEO:  Yeah.  Way to go, Matt.



STEVE:  So I included Matt's question, even though it's not a new problem, due to its importance, which I think endures.  The question, of course, is how would the people we care about fare if, for whatever reason, we were to become unable to guide them through the process of unlocking whatever we had secured from the rest of the world.  The related problem is that this is also something of a moving target.  Perhaps we did at some point in the past take a snapshot of our security precautions, passwords, et cetera.  Is that snapshot still valid?  Over time we tend to make changes, like perhaps moving away from LastPass.  But that suggests that we should periodically revisit our preparations.  So anyway, just a reminder about that.



LEO:  In my desk drawer, in case anything happens to me, Steve, tell Lisa this, I have a piece of paper that says in the case of my death or dismemberment, and inside is all the passwords.  I even taped a YubiKey in there, my backup YubiKey in there.  But also Bitwarden and LastPass and most password managers have a way to specify an emergency contact, a recovery contact.  So I've done that with Lisa and other family members for my Bitwarden.  In fact, when we got our new iPhones I noticed you can do the same thing with your iPhone.  You can specify an emergency contact, emergency, not - contact's wrong because it's after your death or incapacitation, but an emergency person who has access to your account.  And so I've done that also.  And in fact I told Lisa and our son Michael about it, and they - we did that all.  And I think that - so I think there's an increasing awareness of this.



STEVE:  Good.



LEO:  And you're right, you've got to keep it up to date.  Although, if you do it the way Bitwarden and Apple do it you don't have to worry.



STEVE:  Yes.  Right.  If you provided means to get in to your master set of secrets, then the changing secrets are always current whenever anyone gets in.



LEO:  Yeah, yeah, good.  Yeah, Apple calls it Account Recovery Contact, just so you know, Account Recovery Contact.



STEVE:  Account Recovery Contact.  Interesting.  It will be interesting to look at how that works, exactly, because...



LEO:  This one maybe requires you to still be alive, come to think about it.



STEVE:  Or maybe it requires you not to deny their attempt.



LEO:  That's how Bitwarden works it.  It's a dead man switch.  So they send an email, and you set the time span, and you say, yeah, if I don't respond in seven days, give her access.



STEVE:  Let them in.



LEO:  Yup.



STEVE:  Yup.  Good.  So Mark W. Clemens said:  "I received a series of three" - I'm sorry.  I received a series of three interesting tweets from Mark W. Clemens.  He said, first:  "Hi again.  Maybe you or Leo could cover some security solutions with Shop Pay (Shopify)."  He said:  "I get stuck using it every once in a while as a customer to a store like Boll & Branch.  It will not take a Privacy card, and I am unable to delete my credit card later.  The concern is the capture of credit card information when, not if, Shopify gets penetrated, and customer data is stolen.  Thank you.  Mark Clemens."  And he says:  "(Using my name is fine.  I am your age and hard to embarrass.)"



So then I got an update tweet.  "Update," he said.  "I received this from http://privacy.com."  And he said:  "Shop Pay just attempted to charge $0.00 to your Shop Pay card, but the charge was declined because we've detected multiple cards used at Shop Pay.  This behavior is prohibited on our system to prevent exploitation of new customer or referral promotions.  If you have a special use case which requires this, please reach out directly to our team at support@privacy.com."



And then the third tweet from him.  He said:  "I sent you a DM on Twitter about http://privacy.com, and now I am getting calls from unknown individuals asking if I need credit card help.  Is Twitter monetizing the content of private DMs?  I think I will join you in leaving Twitter.  This was just too coincidental."



Okay, now, this, of course, is all anecdotal.  But Twitter is objectively hemorrhaging cash, and we are learning more about Elon every day.  So would anyone put it past him to monetize our supposedly private DMs?  They're only private inasmuch as they're only readily available to the conversation's participants.  Twitter never purported to be Signal, WhatsApp or iMessage.  And who knows what the fine print in the terms of service say?  And just to keep some perspective here, Google does something similar, as we know, with our email, which is private only, you know, to about the same degree.  Anyway, so who knows?  Anyway, just a heads up about something that might be happening in case anyone else cares.  I don't know one way or the other.



CPUGuru said:  "Bitwarden was warning me to update my PBKDF2 iterations and suggested that I export my passwords before I did it just in case.  Tried to do so under Edge, and it was blocked by Microsoft Defender SmartScreen as a 'potentially unwanted app.'"



LEO:  Yeah.  Yeah, sure, yeah, yeah.



STEVE:  He said:  "Had to log in under Chrome to do the export process.  Le Sigh," he said.  So I included this here because I am certain I'll be announcing the general availability of GRC's ValiDrive mass storage fraud detection utility on next week's podcast.  Except for a few typos and some cosmetics that I need to fix when someone has scaled their font sizes to other than 100%, it is finished, and it's been running beautifully for some time.  But it will be a brand new Windows utility when it's released, and Windows has become insanely over-protective about anything that has not yet had the chance to establish a reputation for itself.  The perceived safety of code is no longer about what it does or what it might do.  It's all about its reputation.



And annoying as that is for me, being a developer of always brand new code with no preexisting reputation, I 100% agree with and endorse this policy.  Unfortunately, reputation is the best defense we have today.  And for what it's worth, once ValiDrive has established itself, much as the DNS Benchmark, InControl, and GRC's many other freeware utilities have, it won't cause anybody any trouble.  But initially I can expect that it will, and I'll remind everybody about that again next week because what our testers have been testing, you know, versions of ValiDrive, and in many cases, you know, they've like had to fight with Windows in order to get a copy to run, even though they just freshly downloaded it from GRC, and I just built it from the source code.  You know, it's like it's minutes old.  But unfortunately, that's part of the problem these days.



Matthew N. Dudek, he said:  "Related to your discussion about securely erasing drives, Windows 10 has a function to reset the PC with a clean-the-drive option, so that the PC can be repurposed and reused like it was new.  Is the drive cleaner a secure enough erasure to prevent data from being recovered?  I have old PCs that were used in a medical office I want to give to another organization and want to make sure nothing can be recovered."



Okay.  So I poked around a bit to see whether I could determine what Windows is doing.  But Microsoft isn't being very clear about that.  And I didn't, like, take the time to set up Windows and do a clean-the-drive option and see what is left.  So at the moment, of all the things that have been suggested, I really like the idea that was suggested by one of our listeners, of using VeraCrypt to encrypt the entire drive with an insanely long and then discarded password.  It's a terrific and relatively simple way to cause ultra high-quality pseudorandom data to be written to the entire drive.



Since VeraCrypt has a portable operating mode, it doesn't even need to be installed.  It can be run from a thumb drive.  And if you just type a bunch of gibberish into the keyboard for your password, and encrypt an existing drive that way, and then throw that password away, you've just filled your drive with absolutely strong pseudorandom data.



LEO:  Although this is the argument for turning on file encryption before you use any media.



STEVE:  Yes.



LEO:  And, now, a Mac, it's on by default, FileVault's on by default.  I guess BitLocker is not available in Windows Home?  Or is it now?



STEVE:  You're right, I think it's not available.



LEO:  Yeah.  On both iOS and Android, as far as I know, the phones are encrypted by default.



STEVE:  Correct.  There is on-the-fly hardware encryption on the drive.  So it's just a matter of discarding that key and the entire drive is no longer recoverable.



LEO:  Whenever - and especially on SSDs we've talked about.  But in general, whenever I set up a new drive, I make sure they're encrypted.  That way I don't even think about it; right?  Because it's just garbage on that platter.



STEVE:  Right.



LEO:  Even the slack space.



STEVE:  Right.  Neil Baldridge said:  "Hi, Steve.  Over the years there have been various discussions about battery health, charging, et cetera.  And I recall you talking about it being best to keep a lithium-ion battery charged to minimize the charging cycles."  He said:  "Our company has been moving from ThinkPad laptops to Microsoft Surface laptops; and I have noticed that mine, my Surface 5 laptop, wants to keep 'smart charging' on unless I override it.  Smart charging caps the battery charge at about 80%.  If I need to have a full battery because of running disconnected from power, I have to remember to disable smart charging early enough to get a full charge.  Then, after a day or so, it will enable it again and limit the charge to about 80%.  Do you know if something has changed with laptop battery health?  Or maybe my understanding is just out of date.  Thanks for all of your contributions; and I, too, am grateful for the extension of Security Now!."



LEO:  First thing I would say, by the way, is if you keep it plugged in, you're not stopping charging.  It discharges a bit, recharges, discharges a bit, recharges.  There is no way to not keep a battery from charging by keeping it plugged in.  It's going to discharge over time, and you're going to charge it.  So his first premise is false.



STEVE:  Well, so his information's not out of date.  What he's referring to is a subtlety that I think I may have failed to make clear enough in our earlier discussions of this.  While it's true that unlike their NiCad predecessors, lithium-ion batteries do not like to be deeply discharged, they also really do not like to be overcharged.  Overcharging a lithium-ion battery is really, really bad for them.  So what's happened is that various laptop manufacturers have started getting smarter about their battery charging.  When they notice that the machine tends to be plugged in all the time, they'll deliberately begin resting the battery at some lower charge level which is much safer for its cells.  Then, if the battery is about to be used, as Neil said, you'll want to top it off shortly before going on the road.  So it actually is smarter, if you just generally use your laptop in an at-home, docked, and plugged-in way, setting it up so that the battery is not always fully charged, but is something more like 80 to 85% charged, is better for the battery health.



LEO:  No modern device you're buying will let you overcharge a lithium-ion battery, none.  In fact, the last time that happened those were those hoverboards.  They burst into flames and were immediately banned.  Everything you buy today has circuitry to prevent overcharging.  You don't have to worry about that.



STEVE:  Yup.



LEO:  But I think it is often possible to fully drain a battery.  There's nothing to stop you from doing that, either. 



STEVE:  Correct.  Although actually lithium-ion batteries also have a little circuit on the cell itself.



LEO:  It'll give up, yeah, yeah.



STEVE:  That deliberately disconnects it because it does not want to go all the way to zero.



LEO:  Right.  I think you could trust, basically, I always just tell people trust the manufacturer and use those default settings.  Because they know best the hardware.



STEVE:  Right.



LEO:  Don't try to outwit them.



STEVE:  Right.  Although disabling it in order to go to full charge prior to going on a road trip does make sense.



LEO:  Yeah.  I do that on my EV, yeah.



STEVE:  Yeah.  And this brings us to the smart recommendation of the week.  Kevin van Haaren said:  "Hey, Steve.  I've been thinking about the issue of securely storing things outside of my cloud-based password manager."  We talked about, you know, using your password manager to store the things that were relevant to the web.  But, you know, unless there's a reason to store other things there, it would make sense to maybe put them somewhere else.  Then you're not having the problem of worrying about what else might have been, might be decrypted in the event of a cloud breach like we're now seeing with LastPass.  So anyway, he said:  "I wanted a product where encryption was the main goal, rather than a compression product where encryption was an add-on."  And I had talked last week about using an encrypted archive as a typical solution.



He said:  "For this reason I decided to use a password manager, just not a cloud-based one.  And one I won't integrate with my browsers."  He said:  "I'm going to use KeePassXC.  It's open source, cross-platform, and uses standalone files.  You can include other files, not just passwords, in the encrypted database file.  It's been around forever, and started by forking the code base of the even older KeePassX.  They even recently had a code audit."  He said:  "Files in the database aren't compressed, but certs and ssh keys are tiny, and disk space is reasonably cheap."



And I just wanted to say I think that Kevin's rationale is sane.  I was wondering about the encryption of an archiver that was added as a secondary feature to the encryption.  And KeePassXC is a cross-desktop platform, you know, Windows, macOS, and Linux.  So anyway, I just wanted to share Kevin's solution.  And Kevin is certainly correct that the size of stored content is no longer a huge issue.  And for what it's worth it could be compressed before it's placed into KeePass, if you wanted that, too.  And you have to do it beforehand because, as we know, any resulting encrypted container cannot be compressed because anything properly encrypted contains zero entropy, making it impossible to compress.



And one final note:  Principal Archivist answered the question about mathematicians.  He said:  "Old mathematicians don't age," Leo.  "They just get irrational."



LEO:  Okay.



STEVE:  Okay.



LEO:  What is Hello, first of all?  I don't know what you're talking about.



STEVE:  Hello, actually Client Hello is the name of the first packet that our browser sends to a server.



LEO:  Oh, okay.  The SYN and the ACK.  The SYN.



STEVE:  Well, that's TCP.  The Client Hello is TLS.



LEO:  Okay, right.



STEVE:  So after establishing...



LEO:  The handshake at the beginning, yeah.



STEVE:  Exactly, exactly.  So today's topic was inspired by a tweet from Nick Sullivan.  We've referred to Nick many times in the past.  His title is Head of Research at Cloudflare, where he leads research in the fields of security and privacy, cryptography, Internet measurement, and emerging network paradigms.  Prior to working at Cloudflare, he developed encryption technology for Apple's Internet Services division.  He also co-wrote Symantec's Internet Security Threat Report, which we shared for years, and has degrees in both Computer Science and Pure Math.  So maybe a Fields Medal is in his future.



So here's what Nick tweeted, and it's the reason we're going to dig into this today.  Nick tweeted last week:  "Encrypted Client Hello (ECH) is a new proposed standard that improves encryption and metadata protection for connections online that use TLS for security.  After years of testing and refinement, it's finally happening."  He said:  "Chrome has been testing ECH for months, and is now enabling it by default in Chrome 117.  Firefox is not far behind.  Cloudflare just launched support for ECH" - again, Encrypted Client Hello - "for all customers.  These changes," he said, "amount to the removal of the hostname from cleartext for a huge chunk of Internet communication.  Considering how long the hostname has been in cleartext and how many products were built around that assumption, it's going to be an interesting rollout."



Following Nick's tweet, someone immediately replied to his tweet, saying:  "What kind of fallout do you expect to see?  I'm guessing a lot of enterprise and consumer parental and security controls will need to find new mechanisms.  What else?"  And Nick replied:  "National firewall and content filtering will be affected."  Someone else cautioned:  "Be very careful about implementing this in enterprise and education settings as it removes a key indicator of compromise used by cybersecurity defenses.  I know many CISOs are very concerned about the implications of ECH (Encrypted Client Hello)."



And of course what all of this tells us is that spying on, not the content of, but the fact of encrypted HTTPS TLS connections has remained a big deal, and that this new initiative is in the process of taking that away.  As Nick observes:  "Considering how long the hostname has been in cleartext and how many products were built around that assumption, it's going to be an interesting rollout."



Okay.  So let's back up a bit before we move forward.  Originally, the web primarily used HTTP.  A client, a web browser, would look up the IP address of the domain the user wanted to connect to and make a query.  So the browser would initiate then a TCP connection to the remote IP that it had obtained by querying DNS.  Then the client, the browser, would send an HTTP query using an HTTP verb, typically "GET," followed by the URL of the resource to be retrieved from the remote server.  The query would often contain some additional query headers, like "If-Modified-Since," which would allow the server to check the date of the resource the client was requesting and reply with "Not Modified" rather than resending something that the client already had.  This improved the web's efficiency.  So that's an example of a so-called "query header" that went along with the query.



But the most important query header, which has been mandatory since near the beginning of the web, was the "Host" header.  This told the server the name of the host from which the client was requesting the resource that was named after the "GET" verb. In the very early days of the Internet, this wasn't strictly necessary since the host's name was used to look up the IP of the server in the first place.  So the assumption was that the server answering TCP connections and responding to queries at that IP would be the host the user expected.



But then things became more complicated.  We wanted multiple websites to be able to share the same IP address.  That would mean that a domain name lookup for the IP of, for example, www.acme.com might return the same IP as a lookup for www.zebras.com.  This meant that two different websites were cohabitating at the same IP.  And that's what made the "Host" query header then mandatory.  If web browsers always included the name of the host's domain that they were querying, then the receiving end could examine the query to hand it off to whichever website matched the named host.  And all that worked great.



But then along came HTTPS with TLS, and things became more complicated due to a chicken-and-egg problem.  With HTTPS, an initial TCP connection is established as before.  But now the client indicates that it wishes to establish an authenticated and encrypted connection by sending a TLS ClientHello handshake packet.  The server likely expects this, especially if it has accepted the initial TCP connection at its port 443, which has been standardized for receiving incoming HTTPS connections.  But either way, upon receiving the client's ClientHello TLS-initiating handshake packet, the server needs to reply with its matching ServerHello TLS handshake packet.



The problem is, in a modern multi-homed environment, where many different possible websites and servers exist, which one should reply?  We're wanting to bring up an authenticated connection.  So the client needs to receive the server's certificate immediately for verification of the connected server's authenticity.  But again, the certificate for which server?  Which website?  Which domain?  Remember that the host header, which had been handling this for us in a pre-TLS HTTP-only world, is part of the client's HTTP query which hasn't happened yet since we still don't know who we're talking to.  And we have no encryption.  Like I said, a classic chicken-and-egg problem.



This dilemma was solved by moving the declaration of which server we wanted, which website or domain we wanted to talk to earlier in the handshaking connection dance.  It took the form of an early extension to the TLS protocol, back when it was still called SSL.  The extension is known as SNI, which stands for Server Name Indication.  So for a long time now, the initial ClientHello, which opened the TLS handshake, has included a declaration of which website, which domain the TLS handshake should be made with, which is to say, which server or domain should reply with a TLS certificate that's valid for the domain indicated by the client's SNI-enhanced ClientHello packet.



And so once again, all that worked great.  But Houston, we still have a problem.  Even though we're now using TLS to securely authenticate and encrypt our communications once the channel has been established, the identity of the server that the user is connecting to is still out in the open as plaintext for any and all to see.  The SNI extension data is sent in the clear so that the proper server may be selected for a response.  And anyone - any ISP, any carrier, any government, any censor or malicious intermediary - is able to monitor what amounts to HTTPS and TLS connection metadata.



Okay.  So now what?  The first attempt to resolve this conundrum was known, not surprisingly, as ESNI, which of course stands for Encrypted Server Name Indication.  Okay.  So if the client is going to encrypt the Server Name Indication extension data, the question is, encrypt it with what?  What encryption key is it going to use?  How can the web browser encrypt the SNI data for a server that it wants to talk to before it's ever talked to it?



Our mission today is to go in.  We're going to into this.  But beware that, as I said at the beginning, quite unfortunately, we are rapidly entering the land of the kludge, and it's not going to be getting any better anytime soon.  In fact it's going to be getting even more kludgy the more and more fully the brightest minds in the world attempt to solve this problem.  The short version is:  "This problem did not have any good clean solution."  And the longer version is:  "Yeah, but we still needed to solve it anyway."



So how did ESNI obtain an encryption key to use for encrypting the SNI data in the first ClientHello handshake packet?  Believe it or not, ESNI gets that key from DNS.  When an ESNI-aware browser looks up a domain's IP, it also asks for the server's ESNI public key for the same domain in an ESNI DNS text record.  So now the web client is able to use the targeted server's IP address to establish the connection and its DNS-published public key to encrypt the ESNI data for inclusion in its initial ClientHello packet.



Okay, well, there are several problems here.  The first is the DNS is an unencrypted and unauthenticated protocol.  Which means that anyone eavesdropping on a client's DNS queries will see them looking up domains and obtaining IP addresses and now ESNI public keys for specific domains.  Okay.  But wait.  We do have DNS over TLS, aka DoT, and DNS over HTTPS, known as DoH.  So believe it or not, the use of some form of DNS connection authentication and encryption is now required as part of this messy solution.  I did warn everyone that we had a kludge coming.  And it gets worse.



The unanswered question is, in a multi-homed environment where the incoming server-specifying SNI is encrypted, how does the receiving server know under which server's DNS-published public key the SNI was encrypted?  The answer is that a single public key is now shared among all domains that share a common IP address.  So a frontend ESNI-decryptor first receives any incoming TLS ClientHello message containing an ESNI extension.  That server knows the private key that's associated with any of the domains sharing that single IP.  So it decrypts the incoming ESNI data, determines the SNI target for the connection, and returns that server's certificate.



Wow.  Okay.  The problem here, aside from this being a growing mess, which it is, is that the SNI data is the only part of a connection's metadata exchange that's encrypted.  In other words, you know, the SNI didn't used to be encrypted.  It was in plaintext.  So everyone looking at those ClientHello packets go by knew who the user was asking to connect to.  So that was encrypted, creating ESNI.  And then also necessitating the mess of having DNS publish the public key which was being used to encrypt the SNI data which would then be decrypted by its recipient across however many domains were sharing that IP.  On the other hand, only the SNI data of the entire ClientHello is encrypted.  It turns out this leaves plenty of useful-to-snoopers connection metadata unencrypted.  It's not worth getting into the details because it turns out that all of this mess will have been transient.



But to give everyone a feel for it, here is a snippet of what Cloudflare wrote about ESNI, this proposal, nearly three years ago.  Cloudflare wrote:  "While ESNI took a significant step forward, it falls short of our goal of achieving full handshake encryption.  Apart from being incomplete  it only protects SNI  it is vulnerable to a handful of sophisticated attacks, which, while hard to pull off, point to theoretical weaknesses in the protocol's design that need to be addressed.



"ESNI was deployed by Cloudflare and enabled by Firefox, on an opt-in basis, in 2018, an experience that laid bare some of the challenges with relying on DNS for key distribution.  Cloudflare rotates its ESNI key every hour in order to minimize the collateral damage in case a key ever gets compromised.  DNS artifacts are sometimes cached for much longer, the result of which is that there is a decent chance of a client having a stale public key.  While Cloudflare's ESNI service tolerates this to a degree, every key must eventually expire.  The question that the ESNI protocol left open is how the client should proceed if decryption fails and it can't access the current public key, via DNS or otherwise."



Okay.  In other words, this is just an incredible mess.  Getting the privacy that we want is really difficult.  So the architects of all this set about coming up with a means for encrypting the entire ClientHello packet, leaving nothing of it in plaintext for any snooper to obtain.  And to their credit, they also solved the problem which that snippet there at the end mentioned with stale DNS data messing everything up.  So how do we get to the title of today's podcast, which is "Encrypting Client Hello"?  And it's arguably even messier.



The only thing I can imagine is that these people really wanted to solve this problem from where we stand today, even though there is no really good solution.  But stepping back from the details, which is we're about to step into further, I can appreciate that eventually, once all of this has been established, like sometime in the future, and is in place, and is just the way things are done, a major step forward will have been taken toward improving the privacy and security of the Internet.  It's just that things had been, as I said at the top of the show, so much simpler and more straightforward until we really required this level of privacy enforcement.



Okay.  In any event, ESNI, with all the time and effort that went into it, will eventually be fully replaced by its successor ECH, Encrypted ClientHello.  This would close the gap left open, and it will close the gap left open by ESNI.  It will protect all privacy-sensitive handshake parameters.  Okay.  So how does it operate?  Similar to ESNI, ECH initially uses a public key distributed by DNS because there's just no other practical way to distribute the key.  And it must be obtained using DoH, DNS over HTTPS, for the sake of preventing eavesdropping on that information.  And also security.



So we still have the DNS-is-in-the-loop problem.  This key is used during the client's initial server outreach.  That is, the public key obtained over DNS used during the client's initial server outreach.  But ECH, this successor protocol, has added some clever improvements and fallbacks when the DNS key distribution fails, which make the resulting protocol more robust in the face of DNS cache inconsistencies.



Where an ESNI server aborts the connection if the decryption of the SNI data should fail, an ECH server attempts to complete the handshake by dynamically supplying the client with a public key it can use to retry the connection.  Like I said, nothing is simple anymore.  Okay.  So if ECH encrypts the entire ClientHello packet, if its decryption fails, how can it possibly complete the handshake if it's unable to decrypt the ClientHello?



So  now we enter the mega kludge:  The ECH protocol actually uses nested ClientHello messages.  Unfortunately, moving it into the future, we're not going to have anything being simple here.  We're going to have what they call "ClientHelloOuter," which is sent unencrypted in the clear, and now "ClientHelloInner," which is encrypted and sent as an extension of the ClientHelloOuter.  That is, that's the way ClientHelloInner is attached.  TLS handshakes have a flexible extension mechanism.  And so you're able to just, I mean, and that's what SNI is.  It's an extension data field added to the Hello.  Well, so now this encrypted inner ClientHello will be an extension added to the unencrypted outer ClientHello.  If the decryption of the inner ClientHello succeeds, the server will proceed by using the inner ClientHello data.  Otherwise, it will use the unencrypted outer ClientHello.



Okay.  But hold on.  The outer ClientHello was never encrypted.  So we can't really use it for anything; right?  Unfortunately, that's true.  The outer ClientHello, while also a valid ClientHello message, is not used for the intended connection.  Instead, the handshake is completed by what's being called the ECH service provider, which is that - I've talked about it before in the ESNI context.  That is, it's something that answers all of the incoming connections.  It's the common server that initially fields, you know, all attempts to connect at that IP.  Using the unencrypted outer ClientHello, this ECH service provider signals to the client that its intended destination could not be reached due to a decryption failure.  Remember, because if the encryption had succeeded, then the connection would have been made.  In other words, the initial handshake's inner ClientHello could not be successfully decrypted.



And while sending that news back to the client, it also sends back the correct ECH public key, that is, it overrides the DNS which the client first attempted.  So if that fails, then this ECH service provider sends back the correct ECH public key, which the client can then use to retry the entire handshake.  In so doing, it's able to correct the client's copy of the connection's wrong ECH public key, which it would have received from DNS.  Or, you know, if Cloudflare is still rotating these public keys every hour, and the client has an ongoing relationship with some server that Cloudflare is hosting, well, that key is going to get stale.  So at some point a connection will fail.  In the failure, the updated public key is sent back to the client that uses that in order to encrypt the inner ClientHello packet and is able to resume connections until, once again, that key expires.



So there is a great deal of understandable concern over what breakage is going to occur with these changes.  ECH attempts, as I said, to hide the inner ClientHello by defining a new TLS handshake extension to contain it.  The hope is that random filtering middleboxes along the way, placed who knows where on the Internet, will just overlook any unknown new TLS extension types.  But no one knows for sure yet.  The people who initially began experimenting when TLS 1.3 was created were quite surprised that so many connections were failing.  It turned out that this was due to traffic filtering and analyzing middleboxes which were crashing when they encountered the original unexpected TLS v1.3 packets.



It was only after the 1.3 designers deliberately redesigned their shiny new protocol to much more closely resemble the existing 1.2 that they were then able to get their packets through, basically creating a lookalike v1.3 protocol that was finally able to succeed on the Internet.



So again, who knows what's going to happen once TLS ClientHello's incorporate a nested and encrypted inner ClientHello.  Hopefully, all will go well.  But in any event, as I started out noting, this is all actually happening.  Now, the announcements are probably more interesting.  Again, Chrome has been testing ECH for months, and is now enabling it by default in Chrome 117, with Firefox to follow closely behind.  And as of last Friday, Cloudflare launched their server-side support for ECH for all of their customers.



So it's happening.  I don't mean to come off sounding too pessimistic about all this.  I'm glad that this work has been done and that the world is moving forward.  Unfortunately, it does mean that, you know, the way that all of this stuff once worked, which was so clear and clean and simple and elegant, is going away.



LEO:  For good reason.



STEVE:  It also wasn't fully secure or private.



LEO:  Right.



STEVE:  And it turns out that, exactly, that creating true privacy for a massive global public network is not an easy thing to do.



LEO:  Well, as you said at the beginning, none of this was designed to be secure and private.



STEVE:  Right, there was no...



LEO:  And so it's all tacked on after the fact.



STEVE:  Yup.  Yup.  And now this is not a tack.  This is a box of nails.



LEO:  Let's hammer this in.  Steve Gibson always makes - see, that wasn't so hard.  Aren't you glad you listened?  Your brain is now about one-eighth of an inch bigger, full of all that great ECH information.  Thank you, Mr. Steve Gibson.



Copyright (c) 2023 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#943

DATE:		October 10, 2023

TITLE:		The Top 10 Cybersecurity Misconfigurations

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-943.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  How many people have downloaded GRC's latest freeware so far?  Do we believe what 23andMe have told the world about the leak of their customers' personal and private data?  What are the stats regarding all aspects of cyberattacks?  How's the Brave Browser doing?  Where and when is Google surreptitiously embedding tracking links into Google Docs exports?  What high profile enterprise was also compromised by the Progress Software MOVEit SQL injection?  What additional web browser just added and announced its support for Encrypted ClientHello?  What change did Google just make with the release of their Pixel 8 family of smartphones?  What cyber initiative did the U.S. Congress just overwhelming pass?  What's "dwell time" and why do we care?  And that's just the news.  We'll also be entertaining many of our listeners' questions, then starting into the first part of our examination of a really terrific document that was just published by the NSA and CISA.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  More victims of the MOVEit vulnerability.  Would you believe it?  We'll also talk about the future of the Brave browser, seems like things have gotten a little rocky.  Steve is very suspicious of 23andMe's explanation about their breach.  And then we're going to talk about CISA and their Top 10 Misconfiguration settings.  Maybe something you want to think about going forward.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 943, recorded Tuesday, October 10th, 2023:  The Top 10 Cybersecurity Misconfigurations.  This show is brought to you by members like you.  Thanks.



It's time for Security Now!, the show where we cover the latest news in security with this guy right here, he's the king, the man, the voice.  If you're not listening to Security Now! every week, you're missing it.  Steve Gibson.  Hello, Steve.



STEVE GIBSON:  So is it the latest news in security or in insecurity?



LEO:  In insecurity, yeah.	



STEVE:  Yeah.  One of my annoyances is when someone says you only have one choice.  Well, isn't a choice, does that mean two, a choice between two things?



LEO:  Yeah.  Oh, that's - now it's going to annoy me.



STEVE:  Have one choice?



LEO:  You're right.  If you only have one choice, it's not a choice.



STEVE:  You're right, exactly.  So you really have no choices if you have one.  And if you have two choices, then that's only one choice.



LEO:  So maybe that's what they mean.  You have two choices, but there's only one choice.



STEVE:  Well, why don't they say what they mean, Leo?  That's just no good.



LEO:  I was recently commissioned by a woman you and I both know well to write a teaser for Security Now!.  And we were doing these trailers for all the shows.  I'm not sure where they show up.  They show up somewhere important.



STEVE:  You don't know who sees it.



LEO:  I don't know what they're for.  But you know what?  Lisa says record them, I record them.  And as I'm writing, I'm thinking, so in each show I'm trying to write, you know, what I think makes this show an important show.  And here's what I say for Mr. G:  "When it comes to online security, zero-days, clever and not-so-clever hacks, Steve Gibson is the acknowledged expert.  If it's your job to protect the network, you need, you must, you must listen to Security Now!."  And I think that's accurate.  



STEVE:  Well, actually that fits perfectly with today's topic.  The NSA and CISA got together and produced a document based on the results of their extensive red and blue team operations.



LEO:  Oh, that's got to be interesting.



STEVE:  Oh, it's really good.  In fact, it's so good that already the title was the Top 10 Cybersecurity Misconfigurations.  I did not have room to say Part 1, or the title would have fallen off the edges of the PDF, and it wouldn't fit in the lower third of the video, which, you know, is all important.  But we're going to start into it this week and finish it up next week.  But we've got a lot of other stuff to talk about for Security Now! Episode 943 for October 10th.



One question is, how many people have downloaded GRC's latest freeware so far?  Do we believe what 23andMe have told the world about the leak of their customers' personal and private data?  What are the stats regarding all aspects of cyberattacks?  How's the Brave Browser doing?  Where and when is Google surreptitiously embedding tracking links into Google Docs exports?  What high-profile enterprise was also compromised by the Progress Software MOVEit SQL injection?  What additional web browser just added and announced its support for Encrypted ClientHello?  What did Google just change with the release of their Pixel 8 family of smartphones?  I heard you guys talking about it on MacBreak Weekly, and it's great news.



What cyber initiative did the U.S. Congress just overwhelming pass?  And what's "dwell time," and why do we care?  And that's just the news.  Then we'll also be entertaining many of our listeners' questions, and then starting into, as I said, the first part of our examination of a really terrific document that was just published by the NSA and CISA.  And of course we've got a great Picture of the Week.  So I think another great podcast for our listeners.



LEO:  You know, I was just thinking as you go through those questions, that would be a great security news quiz.  If you've been following the week's security, a couple of them I went, oh, I read that.  Oh, I knew that.  But a lot more I did not.  



STEVE:  Are you in the loop.



LEO:  If you did not score 100% on our security quiz, stay tuned.



STEVE:  Stay tuned for the answers.



LEO:  All your questions will be answered in this thrilling, gripping edition of Security Now!.  I think we're ready for a picture, Steve.



STEVE:  We are ready for the Picture of the Week.



LEO:  I'm scrolling up.  I'm going to see it for the first time here.  4000 years ago man built the pyramids.  Oh, my god.  That's terrible.  All right.  Tell us what we're looking at here.



STEVE:  Okay.  So the text that I put with this is "4000 years ago man built the pyramids.  And it's been downhill ever since."



LEO:  This is horrible.



STEVE:  So this is a picture, I don't know if this is the bottom of a stop sign or a parking meter...



LEO:  A light pole or something, yeah.



STEVE:  It looks like it, yeah, it's something municipal.  You kind of see the yellow curb running along the side.  And so this is on the sidewalk.  And it's really so sad.  So this pole comes to a base, a square base, rounded rectangle or rounded square base with four mounting holes in each of the four corners of the square.  Unfortunately, the location of the bolts coming up...



LEO:  They missed the holes.  Well, they got - what's weird is they got one.  Obviously the first one; right?  They put that in first.



STEVE:  So I think maybe it's, you know, the other caption would have been, "Oh, you meant that in centimeters."



LEO:  I think they drilled one hole.  That one worked.  But they forgot to drill the other ones?  Or maybe they put them in the wrong spot.



STEVE:  Well, I think what actually happened is that someone stole the first one of these, and it had a larger base.



LEO:  Oh, it had a larger base.  Oh, I bet you're right.



STEVE:  Because you can see that each of the three that didn't make it, they're also in a square pattern, but they're too far away to make the other three holes compared to the one that actually has a bolt to it.  And frankly, Leo, when you look at the way this is mounted, is it any surprise that the first one got stolen?  I mean, you know, you just come along with a regular hex wrench and, you know, make off with whatever this was that was...



LEO:  Well, why do you want it, is the question?



STEVE:  Oh, god.



LEO:  Oh, that is so horrible.



STEVE:  Another picture of humanity at its best.



LEO:  So that basically this heavy light standard is held down with one bolt and then washers on three corners.



STEVE:  And then washers, exactly, yeah.



LEO:  Oh, that's terrible.



STEVE:  Yeah, not good.  Okay.  But what is good is that since I know many of our listeners have been waiting for it, I want to start off by noting that GRC's latest freeware utility, that Leo, you had a hand in incubating when you were horrified with the news of fraudulent drives...



LEO:  Yeah.



STEVE:  It exists.  About somewhere around 14,000 copies have been downloaded.  About 4,000 a day at the moment.  I put it online on Friday evening.  And in the show notes these are actually the 12 USB drives I purchased from Amazon.  Every single one of them is fraudulent.



LEO:  What?



STEVE:  They're brand new, just purchased.  On the ValiDrive page I provide the 12 URLs to Amazon to these drives on Amazon for just for people to see for themselves.  I mean...



LEO:  Here's a telling point.  Like this SanDisk was supposed to be a SanDisk Extreme microSD card, SanDisk isn't there.  It just says "Extreme."  Right?



STEVE:  Correct.  Correct.



LEO:  So these are all like - and Amazon does nothing to stop this, no doubt.



STEVE:  No, nothing.  But, I mean, even - some of them came in really nice-looking, you know, like they were trying to copy Apple's packaging.  And I thought, oh, well, this'll be legitimate.  Eh, no.



LEO:  So these are all 1TB, and none of them actually were 1TB?



STEVE:  Not a single one.  In the lower right you see a 2TB.  Over in the lower left is a 256GB.  Not a single one of them was what they claimed.



LEO:  How big were they mostly?



STEVE:  They were all 32 or 64GB.



LEO:  You know what, they're probably little microSD cards in there; right?



STEVE:  Well, exactly.  So it's enough to take a FAT file system format.  They all look legitimate.  One, the fancy one, even had - it actually had a Lightning connector on one end and a USB on the other.  And on the drive was stored a PDF showing, like, how to use it.  I mean, it looked fantastic.  And it only had one quarter of the storage that you thought you were getting.  And the others are far less.



So anyway, I wanted to let everybody know that it exists.  We have had some - I think I've seen two instances where Windows Defender said, you know, like said that it was a virus and quarantined it.  But, you know, 14,000 downloads, and it's not affecting most people.  So as I said, this is - unfortunately we're in a land where it's going to take a while for the software to age enough so that basically its hash, its digital signature, acquires a reputation.  And it's the reputation that protects software from these false positives now, and nothing else.



Oh, the other really interesting thing I found that I think some of our listeners will find interesting is that some drives test pretty quickly under ValiDrive.  Other ones test, like, even legitimate drives very slowly.  And ValiDrive shows you if it's waiting to read or waiting to write.  That is, waiting for a read to return or a write request to return.  And after you've processed a drive, it gives you a report with a lot of statistics about the read and write speed, the average, the median speed, the standard deviation, and also something known as the coefficient of variance, which is the standard deviation over the mean.  So you actually get a sense for how much spread there is in the reading times and the writing times.  But what's really interesting is that many of these drives are surprisingly slow.



Well, okay.  The reason is, and I've mentioned this about the technology of NAND flash storage, is in order to write, to erase or to write data into NAND memory, you're having to push through a layer of insulation in order to inject or remove electrons.  That requires a degeneration of a higher voltage than normal, typically around 20 volts is required.  But USB devices only have access to five volts.  So there is something known as a voltage pump or a charge pump inside all NAND devices.  It has to be turned on, and then it has to basically pump up the five volts to 20 volts through a switched capacitor system in order to be able to start writing.  That takes time.



And so what ValiDrive is showing people is the amount of time it takes before that drive is able to do any writing.  SSDs typically scream along because they've got much more sophisticated electronics in them.  But even some low end modest thumb drives will also run very quickly because they've just been engineered well.  But there are definitely some thumb drives that just crawl along with ValiDrive because - so the reason this isn't normally a problem is that operating systems tend to write, you know, whole multi-megabyte files or multiple files at a time.  If you're like copying a directory or a whole bunch of photos over to a thumb drive, it's all being written at once.  So that charge pumping to get ready to write only has to happen once, and then that time is amortized out over all of the writing that you're doing.



But ValiDrive is like the worst case.  It only reads and writes little 4K pieces at a time.  And it's switching back and forth between reading and writing and reading and writing.  In order to read, you have to dump that high voltage.  So it gets dumped, then the reading happens.  Now ValiDrive wants to write that same spot.  So you have to wait again for the NAND to basically charge itself back up in order to be able to write.  So anyway, as always seems to happen when I get into these things, as happened when I started working on SpinRite 6 and the ReadSpeed utility came out, and we found out that many SSDs were slower at the front of them because they were having more trouble reading data that hadn't been written for a long time because that's where the operating system was, we always end up finding some interesting new stuff.



LEO:  Now I'm going to go - I have a drawer full of these suckers.



STEVE:  Yeah, it's interesting.



LEO:  I'm going to download ValiDrive and go through all of them.



STEVE:  And of course it's 95K written in assembler. 



LEO:  Yeah, of course.



STEVE:  And you don't have to install it and blah blah blah.



LEO:  Wow.



STEVE:  Anyway, neat new utility.  And the people over in the newsgroup were saying to me when I said, "What do you think about this," they said, "Oh, yeah, yeah, you've got to do this, it's going to be very popular."  I think they're probably right.



LEO:  14,000's a good number, and that's just probably people who are in the newsgroups or listen to this show.  I'm sure that if the general public finds it we've got to put it up on Reddit or somewhere.  It'll be [crosstalk].



STEVE:  I did tweet the news on Friday, and so...



LEO:  Nobody's on Twitter.  Come on.  Just a bunch of nut jobs.  Sorry.



STEVE:  Speaking of nut jobs.



LEO:  Yes.



STEVE:  Last week, after internal private customer data was found circulating on hacker forums, the well-known, actually I think it's number one, DNA aggregation, analysis and profiling service 23andMe announced that the accounts of some of its users had been accessed through credential stuffing attacks.  And of course we've talked about the  nature of those before.  So here's - because I think this is - I need to pick this apart a bit.



Here's the first three paragraphs of their announcement.  They said:  "We recently learned that certain 23andMe customer profile information that they opted into sharing through our DNA Relatives feature was compiled from individual 23andMe.com accounts without the account users' authorization.  After learning of suspicious activity, we immediately began an investigation.  While we're continuing to investigate this matter, we believe threat actors were able to access certain accounts in instances where users recycled their login credentials - that is, usernames and passwords that were used on 23andMe.com were the same as those used on other websites that had been previously hacked.



"We believe that the threat actor may have then, in violation of our Terms of Service, accessed 23andMe.com accounts without authorization and obtained information from certain accounts, including information about users' DNA Relatives profiles, to the extent a user opted into that service."



LEO:  Yeah, now I'm just looking at my - because I am one of those people.  I opted in.  I've used 23andMe.  They were a sponsor.  I got my whole family to do it.  And of course we're all sharing our data with one another, so that means we qualify.  My password is, I'm looking at it from one - probably from LastPass, but right now it's in Bitwarden.  And it's long.  And it's strong.  So it's unlikely - and I'm sure I didn't use it anywhere else.  So that means I'm probably okay; right?  It's password reuse that's the problem.



STEVE:  I don't believe them.



LEO:  Oh.  No.



STEVE:  The more I've thought about this, the less it appears to pass the smell test.



LEO:  Oh, no.



STEVE:  My problem is the sheer quantity of...



LEO:  How would they get that many records?



STEVE:  Yes.



LEO:  Yeah.



STEVE:  Yes.  The quantity of customer records that were apparently retrieved.  The attacker claims to have seven million records, which is half...



LEO:  They'd have to try like 10 times that number of passwords; right?



STEVE:  Well, users.  I mean, they're saying seven million customer records.



LEO:  Oh, so well part of the reason they could have a huge number is because, for instance with me, my account, you get in my account, you're going to get all the relatives; right?



STEVE:  Right, right.



LEO:  So it's a multiplier of some number.



STEVE:  Exactly.  So but they're saying they had half of 23andMe's user base.



LEO:  Yeah, no, that can't be credential stuffing. That can't be.



STEVE:  And also, and Leo, the nature of the data fields leaked looks very much like the internal raw database records.  The initial data leak on hacker forums consisted of one million records which was data for Ashkenazi Jewish people.



LEO:  Yeah, I'm one of those people, 4%.



STEVE:  And millions more are, we're told by the hackers, now available for purchase.  So if we're to believe 23andMe - think about this.  If we're to believe 23andMe, they're saying that all the attackers did was logon as some other valid customers, and that they were then able to obtain apparently complete records for millions of other 23andMe users?  In other words, any 23andMe user can login as themselves, then do the same thing that the attackers did?  Really?  That's what they want us to believe?



LEO:  Yeah.  Well, now I'm concerned because the other thing they said is no genetic information was leaked.  Now I'm really concerned.



STEVE:  We can't believe anything they've said because this just does not - and I noticed, if you read what they said, they were very careful to say we believe, we currently believe this.



LEO:  Yeah.



STEVE:  This is what we think has happened.



LEO:  We believe.  We think.  Those are waffle words.



STEVE:  Yes.  So they left themselves an out.  And again, they're asking us to believe that you could log in with your valid credentials and then do this.  Because they're saying that's all the bad guys did.  So, nah, I don't know.  It's not my business to pursue this.  But, you know, many people are quite sensitive to the disclosure of their very personal and private, especially their genetic information.



So I hope that someone in a position of authority digs a bit deeper and asks 23andMe to further explain what to me, from what we've seen so far, looks a lot more like public release or public response cover-up of a far larger problem.  And, you know, these guys should be grownup enough to know better than to do that, if that's what happened.  Again, don't know.  But it just seems really suspicious that apparently seven million records were exfiltrated, apparently by just logging in as other people, as like regular customers.  I mean, they could have created their own account and then logged in as themselves and then done this, we're being told.  So I don't know.  Something seems fishy to me.



Microsoft published their Digital Defense Report for 2023.  And the numbers make for some interesting reading.  It has a bunch of interesting facts worth sharing.  I've just got the bullet points here.  So, for example, of the 78% of IoT devices with known vulnerabilities on customer networks, okay, first of all 78% of IoT devices have known vulnerabilities on customer networks.  46% are not patchable.  In other words, I mean, I guess it's good that half of them are.  That seems like an optimistic number to me.  But still, half of them can't ever be fixed.  They just don't offer patching as an option, even though they're IoT devices and 78% of known vulnerabilities.  Whoops.  Yeah.



Since 2019, attacks targeting open-source software have grown on average 742%.  Since 2019.  So in four years attacks targeting open-source software have grown on average 742%.  Not surprising; right?  We talked about it last week.  You find out that with regard to the Exim mail server, there's an open source server.  You learn that there's a vulnerability in the authentication somewhere.  So hey, you've got the source, bad guys.  Dig through it and figure out where it is.  Much easier than reverse engineering it from the binary.  Also fewer than 15% of non-governmental organizations (NGOs) have cybersecurity experts on staff.  Fewer than 15%.  What, one in eight?  Wow.



Coin-mining activity was found in 4.2% of all incident response engagements.  So not so high.  You know, not like it's 100%.  But still, you know, so, what, that's one in 25, so not much at all.  17% of intrusions involved known remote monitoring and management tools.  In other words, so 17%, we know the way, you know, they're not having to hack things.  They are getting in through remote monitoring and management, you know, like remote desktop protocol, unfortunately.  But other things, too.



So-called adversary in the middle, this is now what we're calling - we used to call it man in the middle.  Now it's adversary in the middle.  So it's AITM instead of MITM.  Adversary in the middle phishing domains grew from 2,000 active domains in June of last year to more than 9,000 by April of this year.  So yes, you know, phishing is unfortunately proving to be a constant source of ways for the bad guys to get in.



156,000 daily business email compromise attempts were observed between April 2022 and the same time in 2023.  156,000 daily business email compromise.  So, boy, that's another huge attack target.  41% of the threat notifications Microsoft sent to online services customers between July 2022 and June 2023, so basically summer to summer, one year, 41% threat notifications went to critical infrastructure organizations.  In other words, those that you don't want to have hacked because they're like in charge of the electric grid or power generation or who knows.  Hydroelectric dams. 



The first quarter of 2023 saw a dramatic surge in password-based attacks against cloud identities.  So again, not surprising, things are moving to the cloud.  That's where the passwords are.  Microsoft blocked on average 4,000 password attacks per second over the past year.  So this is just - it's a constant barrage of credential stuffing.  Approximately 6,000 multifactor authentication fatigue attempts were observed per day.  So multifactor authentication is under attack, as well.



The number of token replay attacks has doubled since last year, with an average of 11 detections per 100,000 active users in Azure Active Directory Identity Protection.  So doubled.  DDoS attacks are on the rise, with around 1,700 attacks taking place per day, cumulating at up to 90 terabits of data per second.  So huge attacks.  I mean, you're just - you're gone off the 'Net.  It's just, I mean, it takes, you know, these are just massive attacks, and 1,700 of them per day.



State-sponsored activity pivoted away from high-volume destructive attacks in favor of espionage campaigns.  50% of destructive Russian attacks observed against Ukrainian networks occurred in the first six weeks of the war.  In other words, there was an initial surge of Russian attacks against Ukraine, but that has since diminished.  Ghostwriter continues to conduct influence campaigns attempting to sow distrust between Ukrainian populations and Eastern partners who support Kyiv, both governmental and civilian.  And finally, Iranian operations have expanded from Israel to the U.S. to target Western democracies and NATO.



And stepping back from all of that, the thing that struck me most is, unfortunately, this is not an industry taken as such in which we would like to see such growth.  I mean, it's not like it's the same this year as it was last year.  Or like anywhere you would call this explosive growth.  Unfortunately, it's exploding in cybersecurity and, you know, and network attacks and intrusion attempts.  I mean, it's a massive increase over the course of just one year.  It would be, you know, way better if things were pretty much as bad this year as they were last year.  But unfortunately that's not what these numbers suggest.  We're seeing real growth in cybercrime activity, and unfortunately in its success, across the spectrum.  Wow.



It's unclear what, if anything, this news might mean for the future of the Brave browser, but Brave Software confirmed that it has just laid off 9% of its staff across departments.



LEO:  Yikes.  That's not good.



STEVE:  So, yeah.  And we don't have an absolute number.  We don't know how large the staff is that 9% of it was let go.  The company didn't indicate how many people were affected, but said its decision was driven by the tough economic climate, saying:  "Brave eliminated some positions as part of our cost management in this challenging economic environment.  Several departments were affected, amounting to 9% of our staff."



They had already been taking steps to bolster their revenue sources this year.  In April, Brave Search dropped Bing's index to start relying on its own indexing solution.  So, you know, it saved money there.  In May, the company released its own search API for clients, with plans starting from $3 per 1,000 queries, that is, uses of its API.  It also offers different plans for AI data model training, data with storage rights, spellcheck, and autosuggest.  Last month, Brave introduced image, news, and video results as part of its Search API.  So it's doing what it can.  And Leo, you'll be honored to know that Brave has named their forthcoming AI assistant after you.



LEO:  What, Leo?



STEVE:  It's called Leo.



LEO:  Well, it's better than TWiT, I guess.



STEVE:  It is.  While the plan is for Leo to be available to all users, Leo will also have a premium tier.  Leo, you're getting a premium tier.



LEO:  I think we need a premium tier, yes, indeed.



STEVE:  That'd be great.



LEO:  Yes.



STEVE:  To offer features like higher rate limits and access to more conversation models.  That paid premium tier will help pay for the cost of API access and hosting for everybody else.  So anyway, you know, Brave is making a, dare I say, brave attempt at cutting, you know, like forging their own path in an environment where that's not easy to do.



LEO:  I support them.



STEVE:  Yes.  We would like to not see a monoculture among browsers.



LEO:  Well, that's why I use Firefox because...



STEVE:  Yes, yes.  And I do, too.



LEO:  Brave is still Chromium; right?



STEVE:  Yup, exactly.  I do, too, as we know.



LEO:  Yeah.



STEVE:  Someone named Joe posted on Fosstodon.  He said:  "Today I found out that Google Docs infects HTML exports with spyware, no scripts, but links in your document are replaced with invisible Google tracking redirects."  He said:  "I was using their software because a friend wanted me to work with him on a Google Doc.  He's a pretty big fan of their software, but we were both absolutely shocked that they would go that far."  Okay, now, I was curious to see, since the show notes are written in Google Docs.



LEO:  We do, we use Google Docs, big-time.



STEVE:  Yeah.  So I was curious.  So I first exported these show notes as a PDF, which is what I normally do.  And as far as I could tell, their embedded links were clean.  But I wanted to see whether I could substantiate Joe's claim.  So I then exported the show notes as HTML.  And sure enough the embedded URLs all point to Google, with the original, visible URL as a parameter, along with a bunch of tracking gibberish.  I have a picture of it in the show notes.  So if you look at it, it's grc.sc/, you know, it's one of my shortcut URLs.  But what's actually there is www.google.com/url?q=, then my URL, followed by &sa=D&source=editors&ust= and some gibberish serial number, and then usg and another gibberish ASCII thing.  So, you know, that's nothing I did or wanted or created.  And what that means is, if that HTML, anyone clicks on links in that HTML, Google is tracking them.  I have no idea why they would care to, want to.  I guess it's just because they can.



LEO:  It probably has to do with editing permissions and things like that, too; right?  I mean, they want to know if you - because you can save something that they can't edit, or they can edit, or comment on and all that.  So I guess it...



STEVE:  So, yeah, I wouldn't go so far as to call this spyware.



LEO:  It's functionality.



STEVE:  I wouldn't call it spyware.  Well, it's not functionality because all it's doing is it's bouncing the person who clicks through Google and then to the destination URL.  So, I mean, it's counting or tracking or who knows what-ing.  Anyway, it's clearly deliberate, and it sure doesn't seem like it's any of Google's business what links are clicked in the future on an HTML export of something created by their Docs.  I mean, they don't do it where they can't.  It doesn't look like it's in docx for exports, or in PDFs.  Anyway, I just thought everyone should know, and that Joe was right in his post about what was going on over on Fosstodon.



Our podcast 928 of June 20th, so that was, what, a couple months ago, June 20th, was titled "The Massive MOVEit Maelstrom."  That's one that Jason co-hosted, Leo.



LEO:  What a mess this has been.  It's a gift that just keeps on giving.



STEVE:  Oh, my god.  And just all it was in this day and age, as our listeners know, was an SQL injection.  It was just a SQL injection.



LEO:  Yeah.



STEVE:  Anyway, it just came to light as a result of a disclosure that Sony filed with its U.S. authorities that Sony Interactive Entertainment was among the now more than 2,300 companies - 2,300 - who were impacted by the exploitation of that SQL injection vulnerability.  So anyway, Sony said that nearly 7,000 of their families and employees were affected as a consequence of that information leak.  So...



LEO:  And obviously they learned nothing from the Sony Pictures Entertainment leak of three years, I mean, this is just...



STEVE:  First thing that occurred to me, yeah.



LEO:  God.



STEVE:  And while we were talking about Encrypted ClientHello last week, this was a week ago today, on that day Mozilla, our browser, yours and mine, Leo, was busy announcing that in Firefox 118, which I just checked is what I was sitting in front of when I was writing this last night, is now supporting Encrypted ClientHello.  And I really like their page.  I have - I grabbed two pictures from their announcement page because it has such a very clean and simple graphic to highlight the difference between non-Encrypted ClientHello and Encrypted ClientHello.  It just shows, you know, here's your phone running Firefox.  And with standard SNI, Server Name Identification, example.com is flying through the air toward the web server, and then the TLS channel brings it back.  But with Encrypted ClientHello, it's encrypted in a channel, you know, in a little tunnel, going to the server, and encrypted coming back.  So very clean.



LEO:  Yes.



STEVE:  And they also summed up last week's deep dive, the one we did, quite succinctly.  They simply wrote:  "ECH uses a public key fetched over the Domain Name System (DNS) to encrypt the first message between a browser and a website, protecting the name of the visited website from prying eyes and dramatically improving user privacy."  And then later they explain that ECH must be paired with DNS over HTTPS (DoH) to secure and hide that initial public key fetch, as well, over DNS.



Anyway, as we know, Cloudflare's web proxy frontend has switched on ECH support for all of their free tier client sites, which is a gazillion.  Now we need the other standalone web servers to catch up and offer their support.  An experimental implementation for OpenSSL now exists, so that will help to get Unix and Linux-based servers, which are using the OpenSSL library for their encryption connectivity, onboard and running with it.  And in time it seems clear that all the rest will join, since user privacy is dependent upon the site they're connecting to, that is, once their browsers all support ECH, it's dependent upon the site they're connecting to, offering its support for the privacy of all incoming connections.  You know, the sites need to publish a public key in their DNS, which the browsers are then able to use to encrypt the first packet to the server.  So, you know, of the sites to do this, as soon as the software catches up, we'll be able to do that.



The Pixel 8 and the 8 Pro now offer two additional years of updates.  Last Wednesday, Google announced that its newest phones, the Pixel 8 and the Pixel 8 Pro, will now receive seven years of software and security updates, a two-year bump from the previous five-year support that they were providing for the Pixel devices.  And everyone listening understands the importance of long-term security updates for consumer devices.  Updates really do need to extend throughout the useful life of consumer devices.  You know, it doesn't do any good to say, oh, yeah, we'll support your phone for two years because we want you to buy another one.



Well, the phone still works, so you're going to give it to somebody who's going to have an unsupported phone.  And smartphones, as we know, are currently among those devices needing the most protection.  No other device is more prone to attack than smartphones, and few other devices are as complex or as exposed to the multiple channels of external material that smartphones are.  So props to Google for stepping up and going from five years to seven years.



Something known as the MACE Act Passed.  What's the MACE Act, you're likely wondering?  Well, MACE stands for Nancy Mace, who was coincidentally one of the bill's co-sponsors.  Nancy is a Republican representative from South Carolina.  But the acronym MACE (M-A-C-E), aside from being Nancy's last name, which I guess is coincidental, stands for "Modernizing the Acquisition of Cybersecurity Experts."  Actually she probably took her name and figured out how she could make it...



LEO:  Yeah.  I think she - unless she changed her name to match the act, which seems unlikely.  Yeah.



STEVE:  That would be going a little too far.



LEO:  Yeah.



STEVE:  So Modernizing the Acquisition of Cybersecurity Experts, MACE.  And the U.S. House of representatives, which is presently having a difficult time agreeing on the time of day, is apparently in wild agreement over this one, this idea, because the Act passed through the House on a vote of, get this, 394 to 1.



LEO:  Well, I think it's a good idea, then.  Who was the one?



STEVE:  I guess it must be.  It does make me curious, though, who the holdout was.



LEO:  Yeah.  Probably Joe Manchin.  He doesn't like anything.



STEVE:  Is Tommy Tuberville a senator or a representative?



LEO:  He's a senator.  Maybe it's Tommy.  Good old Tommy probably voted against it.



STEVE:  Well, but this is in the House, not the Senate.



LEO:  Oh, it's in the House, okay.  Well, could be anybody in the House.



STEVE:  Yeah, exactly.  The other cosponsor of this bipartisan bill was California Democrat Katie Porter.



LEO:  Oh, and she's good.  I like her.  Yeah.



STEVE:  Oh, yeah.  She's just about as far to the left as Nancy is to the right.



LEO:  Mace is to the right, yeah, yeah, yeah.  It's perfect.



STEVE:  But on this they agree.



LEO:  It's bipartisan.



STEVE:  So the MACE Act is aimed at addressing shortages in federal cybersecurity positions by expanding the pool of eligible applicants by lowering the education requirements.  Now, at first you might think, whoa, wait a minute, we don't want kindergartners setting up our firewalls.  No, no.  That's not what this does.  Under the legislation, agencies would be allowed to consider an applicant's education only if their education directly reflects the competencies required for the position.



So apparently before this, you know, you had to have a master's degree in social sciences or political science or something, and then also have your cybersecurity certificate.  Now, no.  Only the education that matters.  The bill would also require the Office of Personnel Management to publish annual reports detailing changes to minimum qualifications for cybersecurity positions and data on the education level of people in those positions now.  So anyway, the bill is now headed to the Senate, where it's expected to pass.  And that will give Tommy Tuberville an opportunity to say no.



LEO:  He can weigh in.



STEVE:  Yeah.



LEO:  It seems like a good idea, I guess; right?



STEVE:  I think so.



LEO:  It's an emergency.



STEVE:  Well, so the problem is, Leo, there is a serious shortage.  I mean, there are, like, we need more cybersecurity people.



LEO:  Yeah.  Everybody should go to ACI Learning and study so that you can get that job.



STEVE:  Yes.  Even if you didn't go to college, again, they're not going to care.



LEO:  It shouldn't matter.



STEVE:  As long as you've got your certification level, you're good to go.



LEO:  That's right.  The skills are what count.



STEVE:  And again, I think that's great.  So, dwell time has plummeted.



LEO:  Oh, no.



STEVE:  Who'd have thought that?



LEO:  What's that?



STEVE:  I know.  Who wonders what that is?  Those who track, monitor, and remediate the consequences of ransomware attacks use the term "dwell time" to reflect the time from the initial network intrusion until the ransomware encryption event is triggered.  So those monitoring this dwell time are reporting that attackers are deploying ransomware on breached networks faster than ever before.  In just 12 months, that is, the past year, the median dwell time of ransomware groups on hacked networks has fallen from 4.5 days to less than one day.  So which is to say a year ago your network would be breached, 4.5 days would go by until your network was encrypted.  No longer.  According to the security firm Secureworks, ransomware is now being deployed within one day of initial access in more than half of all engagements and in as few as five hours from initial network penetration in 10% of all cases.



Okay, now, one of the reasons for this is that the percentage - this is the other interesting thing.  The percentage of human-driven attacks has risen dramatically.  At the time, several years ago, we talked about how bad guys would have a pending inventory of victims that they would be getting around to when they had time, when they were able.  I mean, there were just many available networks.  This victim pool was coming into them from automated scanning and attack malware that would get into a host network, settle down, and then phone home, logging into its command and control network and saying, okay, boss, what do you want me to do now?



But that was then.  These days, the majority of attacks are human driven in real time.  It's not that there are fewer vulnerable systems, as we noted at the top of the podcast.  There's like more than ever, unfortunately.  It's that there's a lot more manpower available because this is where the money is.  This stuff is paying off.  So now a live human attacker is doing the penetrating.  They quickly get the lay of the land, look around, see what's up.  And if the network appears to be a useful victim, they will immediately set about exfiltrating all the data that they can in order to use it for blackmail purposes later, then find and prepare to encrypt all of the systems within reach.  And as we saw, in 10% of the cases, in as few as five hours of initial penetration.  So, you know, you just can't sit around on known vulnerabilities and think, yeah, I'll get to that after lunch.



LEO:  I wonder because, you know...



STEVE:  They may be eating your lunch.



LEO:  Dwell time was increasing also because they wanted to wander around, exfiltrate stuff.  There were other things they could do.



STEVE:  Yup.  Yup.



LEO:  But I'm wondering now if it's shrinking because of better defenses, like if somebody gets in, they think, yeah, we'd better take advantage of this before they catch us.



STEVE:  Yes, I think - yes, that's a very good point, awareness has certainly skyrocketed, as well.



LEO:  A lot of people with canaries in their enterprise; right?  You know?



STEVE:  Yes, yes.  And you can imagine that once upon a time the IT guy would hang his head low and walk into the CEO's office and say, "Uh, chief, we've been hit by ransomware."



LEO:  Right.



STEVE:  And a few years ago, the CEO would say, "Ransom what?"



LEO:  Uh-huh.



STEVE:  Now everybody knows what it is.



LEO:  Now it's more likely the CEO that's with his hair on fire running to the IT department saying, "Do something."



STEVE:  Yeah.  Exactly.  And speaking of doing something, Leo.



LEO:  Let's do something.



STEVE:  It's now your turn to do something.



LEO:  Let's do an ad.  I can do that.  I can do that.  Yeah, it's fascinating.  I don't know if it's in your story rundown, but MGM...



STEVE:  Yes.



LEO:  Is it in your rundown?  We'll talk about it later.



STEVE:  No, no, no.



LEO:  So they were, as we know, Caesars was hit by ransomware.  



STEVE:  Yup.



LEO:  Paid probably half the ransom, not the full amount.



STEVE:  Got themselves back online.



LEO:  And got online.  And MGM, they're saying they declined to pay.  And they suffered from at least a week of disruptions.



STEVE:  And they're now saying it's going to cost them a total of $100 million.



LEO:  Hundred million dollars.



STEVE:  For all the remediation.  And that 10 million of that was just, you know, hiring consultants and figuring out was going on and like dealing with it.  So, yeah.



LEO:  But good for them, I guess, for not paying because that's what the FBI wants you to do.  Because if you pay them it just encourages them.  Yeah.  I don't know.  It's probably cheaper to pay.



STEVE:  And Caesars did pay, and unfortunately it's yes.



LEO:  And they went to MGM.



STEVE:  There was a song once that I think the title was "It's Cheaper to Keep Her."



LEO:  Okay.  Just a little tip, don't sing that with Lorrie around.  I'm just saying.  Just saying.



STEVE:  So David Sherman, a listener, asked via Twitter:  "Which is better, Passkeys via Bitwarden or Passkeys via browser?"



LEO:  Or SQRL.



STEVE:  Well, there's a thorn.  "How to sync passkeys among different browsers?  Thanks."



LEO:  Passkeys - by the way, could I just say this?  We've been talking about this on the shows, not just this show - has really not turned out to be all things.



STEVE:  No.  I agree.  I think the best answer to his question is that Passkey management is still too new and too much in flux for any answer to necessarily hold for long.  But there's an essential point I don't think I've made clear enough in the past.  Sadly, Passkey management has been shrouded in hocus pocus because the purveyors of Passkeys have, from all appearances, desperately hoped to be able to use the mysticism surrounding this promising new technology to retain and corral their users into their own proprietary environments.  The universally missing feature of simple Passkey export and import is astonishing to me.



We all know what a password is.  We make them up, and we use password managers or our browsers to remember and regurgitate them on demand.  But no one really knows what a Passkey is because no one has ever actually seen one.  It's all just mysterious "Don't worry, we got this" hocus pocus behind the scenes, and we're told that it's all super wonderful and hacker proof and so forth.  Okay.  So here it is.  A Passkey is nothing more than a private key.  Period.  And as such it's not some impossible-to-represent mystical token.  It's just a relatively short blob of binary data.  So it could easily be turned into a QR code, or into a short and manageable Base64 encoded string of text.  And at that point it could be moved from place to place, printed out and stored somewhere for safety.  All of a user's current Passkeys could be exported as a simple CSV file for safekeeping.



But for some bizarre reason, end users are not allowed any of those freedoms today.  Expert users want it, and everyone could have it.  But no.  However, I don't expect this mystical barrier to survive in the long term because it is trivial to export and import Passkeys for backup and cross-platform sharing.  So someone will be the first to create a simple QR code, textual, or CSV file Passkey export and import.  Then everyone will want it, and it will become a required feature for any Passkey system.  But unfortunately we're certainly not there today because the system is still so new and Passkey support hasn't yet become the commodity that someday it presumably will.  And Leo, I agree with you.  I don't have a single one.  I mean, not even one.  It's just like...



LEO:  Yeah.  First of all, very few sites are using it.  I created one, you may remember I was on the air with you doing it for Best Buy.  And I only then found out, well, it's only on my iPhone.  It's not - and this is - and I think this is vendors wanting you to lock in because, I mean, I have a Passkey for Best Buy.  Why can't I use it on my Linux machine?  Why can't I use it on this Windows machine?  Well, you can't.  It's inside your iPhone.



STEVE:  Yup.  Yup.  And as I said, there's nothing to prevent your iPhone from putting up a QR code, and you show it to your Linux webcam.



LEO:  And then it would have it.



STEVE:  And now the Passkey is there.



LEO:  And this was what you did with SQRL.  You built this portability in.  But, see, you didn't have the same economic incentives that Apple and all these other companies have.



STEVE:  Right.  Right.



LEO:  They want to keep you on their platform.



STEVE:  Right.  And unfortunately what they've done is killing this thing.



LEO:  Yeah.  I think so.



STEVE:  I mean, basically nobody, it's like, I don't want this.  I know what a password is.  I don't know what a Passkey is.  Just show them.  I mean, again, it's like, it's not some, like, mystery.  It's a little bit of binary.  And you can put it in a QR code, print it out as text, I mean, and then you'd have way more flexibility and freedom.  And you'd lose none of the power.  It still uses public key crypto.  The site sends your client a challenge, which you have to sign using your private key, so it's not subject to replay attacks.  I mean, all the other good things that SQRL and Passkeys both have stays available.  I think, you know, your argument is, well, users cannot be trusted with this magic stuff.  So we're going to hide it from them and do the "just trust us."  But that just doesn't work.  And as a consequence, this thing's sort of DOA, unfortunately.



LEO:  Yeah.  Yeah.  I mean, when I go to Google, I can log in with my Passkey.  And then it puts up a QR code at Google.com.  And then I get it, I have to just aim my phone at it, and then the phone says okay.



STEVE:  And are you you.



LEO:  And I'm doing the Face ID.  And now I'm logging into Google.  And so it kind of works.



STEVE:  Oh, yeah, I mean, it works.  But, like, who wants it?



LEO:  That's the problem.  It's so much easier, especially if you're using a password manager, it's so much easier.



STEVE:  Yeah.



LEO:  All right.



STEVE:  Henrik Lexow.  He offers us a view from inside an ISP.  He said:  "Hi, Steve.  After your deep dive into encrypting the ClientHello," and I should preface this by saying this is a little chilling.  "After your deep dive into encrypting the ClientHello, an old internal dilemma has resurfaced.  I've spent years," he said, "at a European ISP, where ideas about selling content-filtered access control or insights would occasionally come up.  Some of the veteran business folks, who had experienced the unencrypted Internet era, demanded similar services here.  I had to explain that HTTPS Everywhere was happening, but we often circled back to the possibility of inspecting TLS packets for user insights."



LEO:  Oh, that's one way of putting it.  User insights, eh?



STEVE:  User insights.  That's what we're going to monetize.  He said:  "This usually boiled down to two 'customer value areas,' child content protection and enterprise content filtering, with the underlying goal of monetizing data from blanket Internet use inspection."  He said:  "(We eventually did not offer these services during my tenure.)"  He said:  "My ongoing struggle revolves around balancing parental concerns for children's online safety with the goal of Internet privacy.  ECH, despite its complexity, is a step in the right direction, as I'm aware of the ISP efforts to tap into this revenue stream while publicly championing privacy.  ECH can put an end to this hypocrisy.  But, as is often asked in privacy, 'What about the children?'



"My question to you, Steve, relates to those two 'customer value areas' in the context of ECH becoming a reality.  First, who should take responsibility for child protection services now that ECH is coming?  Not all children have parents who can actively monitor their online experiences.  And how could it be done correctly?  And second, are there valid reasons for enterprises to engage in content filtering, be it for security or other purposes?  I personally struggle to see the value in this.  In the case of spending time on non-work activities on the Internet, it seems more like a cultural issue within the company rather than an Internet access problem.  Best regards, Henrik."



Okay.  So in answer to his first question, "Who should take responsibility for children protection in a world where TLS packet filtering will finally be thwarted," the first thought that comes to mind is local DNS service filtering.  I think that's the best solution by far. If a family's home is using, for example, the free OpenDNS Family Shield Service, which is as easy as configuring the residential router to use a specific pair of IP addresses - for anyone who's curious, I have them in the show notes, 208.67.222.123 and .220.123 - then there's no need to see into any aspect of the TLS connections once they've left the family's router since unwanted domains will never have their IP address resolved in the first place.



And Henrik's second question:  "Are there valid reasons for enterprises to engage in content filtering, be it for security or other purposes?"  I doubt that the addition of Encrypted ClientHello will change the enterprise environment much.  As we've talked about in the past, an enterprise's network is owned by the enterprise, and all of its employees should be informed and aware and kept aware that the content of the enterprise's network is not private and that nothing they do over that network should be considered private.  We've talked about placing a strip of paper to constantly remind everyone of that fact along the top of every company-owned computer monitor, and of having the Human Resources Department remind every employee of that fact every year during their annual review.



So that's the enterprise's policy view.  The practical implementation of such technology within the enterprise is already well established.  All non-proxied TLS connections will be blocked from ever leaving the enterprise's network, and any system wishing to connect to the external Internet will need to have the enterprise's proxy server's root certificate added into its root store.  When that's been done, all outbound TLS connections will be intercepted and accepted by the enterprise proxy middlebox which will, in turn, connect on behalf of each user to the remote resource while being able to fully examine in detail and filter the contents, not only of where they're connecting to, but everything that goes on during that connection.  So for the enterprise, the addition of Encrypted ClientHello won't change anything.  The enterprise will need to update their middlebox's firmware to add support for ECH, but otherwise life goes on as it has before.



LEO:  That's great to hear it from inside an ISP.  And those silly rationalizations that they come up with.  Oh, it's for the kids.



STEVE:  Yes, yes.  Can't we, like, somehow make money from knowing what our clients are doing?  You know?



LEO:  Yeah, that's really it.



STEVE:  Is there some way we can do that?



LEO:  Yeah, that's it.



STEVE:  Yeah, that's it.  Matthew Cowgur, he tweeted:  "Does having a very good password," he says, "mine is about 140 bits of entropy, have any effect on encryption with a low number of iterations?"  Now, it's interesting that this is not a question that we've directly addressed during our focus upon the question of iteration count.  We've exclusively focused upon iteration count as the way to slow down any brute force password guessing attack.  But the only reason such an attack needs to be slowed down is because we're assuming that the user may not have chosen a super-strong password in the first place.  That means that given a sufficient number of guesses, the attacker will eventually obtain the not-very-strong password just by trying a whole bunch.



Wikipedia reminds us that in one study of half a million users, the average password entropy was estimated to be about 40, four zero, .54 bits.  So a little over 40.5 bits of entropy.  And the medium to strong password threshold is regarded as around 50 bits of entropy.  If you've got 50 bits, that's considered to be the low end of a strong password.  So 50 bits is 9.5 bits more than the study quoted as being typical.



So thanks to the power of powers of two, and we're using powers of two because they're binary bits, they can have two states, raising that, raising two to the power of 9.5, which is to say the number of more entropy bits for a strong password versus the average, so two raised to the 9.5 is 724.  That tells us that the difference in brute force resistance between that study-average password strength and one that's on the lower border of being strong is 724 times the strength, going from 40.5 bits of entropy to 50.



So now let's look at Matthew's question.  Matthew claims to be using what he calls a "very good password," having around 140 bits of entropy.  Now that we've created some context, it should be clear that, if Matthew is correct, and his password truly contains around 140 bits of entropy, it's not a very good password, it's an insanely good password.



LEO:  Ooh, you scared me.  Good.



STEVE:  Insane.



LEO:  It's great.



STEVE:  If 50 bits of entropy is the lower bound of a good password, then Matthew has added 90 bits to that 50 bits to get up to his 140.  Once again, turning to the power of powers of two, 90 additional bits of entropy results in a password that is 1.238 times 10^27 stronger.  Okay, now, to help us understand the size of that number, 27, that's the number of zeroes, that's 27 is three times nine.  And nine zeros is a billion, which we have that three times.  In other words, Matthew's 140-bit entropy password is 1.238 billion billion billion times stronger than a strong password.



LEO:  Oh, that seems pretty good.  That seems good.



STEVE:  And a password we'd considered to be [crosstalk].



LEO:  I'll take it, yeah.



STEVE:  Yeah, wow.  So Matthew's question was:  "Does having a very good password, mine," he says, "is about 140 bits of entropy, have any effect on encryption with a low number of iterations?"  And the answer is a resounding hell, yes.  If you have such a password, you would be fine with zero iterations.



LEO:  Aha.



STEVE:  Since the only reason we iterate is to protect weak passwords.  If you really have a strong password, and I'm talking to everybody now, you have nothing to worry about.  When we changed the iteration count, back in the LastPass days, from 5,000 to 100,100, or now to 600,000, we've increased the attacking difficulty only by a factor of 120.  But just adding seven binary bits of entropy, seven binary bits is 128 times because that's 2^7, 128.  So a password having just seven additional bits of entropy would provide more cracking resistance, 128 times more, than jumping the iteration count from 5,000 to 600,000, which only increases it by 120 times.



So the key takeaway here is that increasing iteration counts is a linear increase, whereas adding bits of entropy is exponential.  Every bit, every single bit of entropy added doubles the cracking difficulty.  So when you've doubled it seven times, that's 128 times stronger that you've made it.  Therefore anyone who has a really strong high-entropy password, even back then when their vaults were stolen from LastPass, even if it's not as ridiculously strong as Matthew's, has really nothing to worry about regarding PBKDF iterations.  Increasing iteration counts is far weaker protection than using a truly strong password.  In which case it doesn't even matter how many times you iterate.  It could be zero.



LEO:  So really these key derivative functions, whether it's PBKDF2 or Scrypt or Argon2, they're really kind of belt-and-suspenders to protect you if you don't have a great password.



STEVE:  Yes.  They linearly slow down the attack.  But adding bits of entropy exponentially slows down the attack.



LEO:  Yes.  So have a good, long, strong, truly random password, the kind the password manager generates for you.



STEVE:  Yup.



LEO:  I guess you probably shouldn't keep - well, you could; right?  You could keep your vault password in your password manager.  It just wouldn't be any use if you didn't have an access to it.  Okay.



STEVE:  Okay.  So a listener said, "Hi, Steve.  According to this TorrentFreak article, Cloudflare has enabled Encrypted ClientHello for all customers on free plans, which includes many pirate sites.  The new privacy feature makes it impossible for Internet providers to track which websites subscribers visit.  As a result, it also renders pirate site-blocking efforts useless, if both the site and the visitor have ECH enabled."  And he provided the link to this TorrentFreak.com page.



So, okay.  This is inevitable, and it's analogous to the encryption debate; right?  We want to enhance privacy, but we're unable to enhance only the good guy's privacy.  Everyone gets their privacy enhanced, even those who will criminally abuse that privacy.  The question the world has been struggling with for the last few years is whether our inability to restrict who gets more privacy means that no one should have any more.  But it's looking like the world is going to agree that giving more to everyone is the best solution.



The TorrentFreak article that our listener linked to was interesting, and it shed a different light onto the emerging presence of Encrypted ClientHello connections.  I've trimmed it down a bit to remove the stuff we already know, but here's what TorrentFreak observed.  They said:  "Cloudflare has enabled Encrypted ClientHello for all customers on free plans, which includes many pirate sites.  The new privacy feature makes it impossible for Internet providers to track which websites subscribers visit.  As a result, it also renders pirate site-blocking efforts useless, if both the site and the visitor have ECH enabled.



"Website blocking has become the go-to anti-piracy measure for the entertainment industries when tackling pirate sites on the Internet.  The practice has been around for well over 15 years and has gradually expanded to more than 40 countries around the world.  The actual blocking is done by Internet providers, often following a court order.  These measures can range from simple DNS blocks to more elaborate schemes involving Server Name Indication (SNI) eavesdropping, or a combination of both.  Thus far, the more thorough blocking efforts have worked relatively well.  However, as privacy concerns grew, new interfering technologies have emerged.  Encrypted DNS and SNI, for example, made blocking efforts much harder, though not impossible.



"A few days ago, Internet infrastructure company Cloudflare implemented widespread support for Encrypted ClientHello (ECH), a privacy technology that aims to render web traffic surveillance futile.  This means that site blocking implemented by ISPs will be rendered useless in most, if not all cases.  ECH is a newly proposed privacy standard that's been in the making for a few years.  The goal is to increase privacy for Internet users; and it has already gained support from Chrome, Firefox, Edge, and other browsers.  Users can enable it in the settings, which may still be experimental in some cases.



"The main barrier to widespread adoption is that this privacy technology requires support from both ends.  Websites have to support it, as well.  Cloudflare has made a huge leap forward on that front by enabling it by default on all free plans, which currently serve millions of sites.  Other subscribers can apply to have it enabled.  The push for increased privacy is well-intended, but for rights holders it represents a major drawback, too.  When correctly configured, ECH defeats site-blocking efforts.  Tests conducted by TorrentFreak show that ISP blocking measures in the UK, the Netherlands, and Spain were rendered ineffective.



"This doesn't automatically apply to all blocked sites, as the sites must have ECH enabled, too.  We've seen mixed results for the Pirate Bay, perhaps because it has a paid Cloudflare plan, but other pirate sites are easily unblocked.  This new privacy feature hasn't gone unnoticed by pirate site operators.  The people behind the Spanish torrent site DonTorrent, which had dozens of domains blocked locally, are encouraging users to try ECH."  Yeah, no kidding.



"DonTorrent notes:  'Before ECH, your online privacy was like a secret whispered in the wind, easily picked up by prying ears.  But now, with ECH by your side, your data is like hidden treasure on a remote island, inaccessible to anyone trying to get there without the right key.  This feature encrypts your data so that neither ISPs nor organizations like ACE and MPA can censor, persecute, and intimidate websites they consider illegal.'



"Cloudflare and other tech companies," writes TorrentFreak, "are not supporting ECH to make site-blocking efforts obsolete.  However, this privacy progress likely won't be welcomed by rights holders, who've repeatedly criticized Cloudflare for hiding the hosting locations of pirate sites.  TorrentFreak reached out to a major anti-piracy organization for a comment on these new developments, but we have yet to receive an on-the-record response.  It wouldn't be unthinkable, however, that we will see more blocking lawsuits against Cloudflare in the future."



So, and of course we touched upon briefly the content-filtering thwarting aspect of ECH last week, but we didn't explore the real-world consequence to those, like the Motion Pictures Association, who have been using legal means to force the blocking of sites containing pirated copyrighted content.  Looks like that ability to do so is going to be short-lived at this point.  As I said at the top of this, everyone getting their privacy increased means that the bad guys do, too.  And the world has decided that's the way we're going to go.  Which I think everyone agrees is the right direction.



Skynet tweeted:  "This ECH stuff is going to mess with my public WiFi content filtering; isn't it."



LEO:  Oh, yeah.



STEVE:  Yeah.  He said:  "I just got my vendor to figure out what was blocking patrons' eBooks from being downloaded onto their Kindles, and now this.  Thanks, Steve.  Thanks a lot."  Anyway, so...



LEO:  Think of the authors.



STEVE:  That's right.  Think of the authors.  This was, we all know, not my doing.  I'm just reporting the facts <grin>.  But as for it messing with someone's public WiFi content filtering, uh, yep, it's going to do that.  I presume that ECH is somehow protecting itself from protocol downgrade attacks.  We've talked a lot about them in the past because they can be tricky to prevent.  The original downgrades were HTTPS to HTTP.  On an initial HTTP connection that was attempting to switch itself to HTTPS, since the connection was not yet encrypted under HTTP, something like a script running would simply change all the HTTPS URLs into HTTP, keeping the connection and all of its components unencrypted, and leading each end to assume that the other end had a problem with encryption when that was not true.



So downgrade attacks have always been a problem.  In the case of ECH, for example, as we know, it only works when both ends support it.  And at this early point in time, support is probably more surprising than not.  That is, it's like, hey, I can get ECH?  Great.  Doesn't happen that often.  So if, for example, an outgoing or incoming initial TLS handshake packet were to be tweaked to show that ECH was not supported by that packet, when in fact it was, the other end would shrug and not be surprised.  Meanwhile, the domain name would be exposed to anyone watching the traffic because it wouldn't have the advantage of Encrypted ClientHello encryption.



Hopefully - and I don't know.  I did a little bit of poking around.  I couldn't come up with an answer quickly.  Hopefully ECH's designers were aware of this problem and did come up with some means of preventing middlemen from removing ECH support on the fly from connections before ECH has a chance to get started.  Otherwise, its attempted presence would just be a temporary inconvenience.  Anyway, in the case of our listener Skynet's public WiFi, any ECH-using clients would also be using DNS over HTTPS, and thus not his local access point's DNS.  So unfortunately, filtering DNS wouldn't work, either.  I mean, it actually is going to present a problem to local WiFi content filtering.  That's true.



For this next question, I anonymized it myself.  He didn't ask me to, but I feel as though this person's tweet should be anonymous.  He said:  "Hey, Steve.  You mentioned in a recent episode that the Linux kernel has fixed the epoch time issue in kernel build 5.10.  I feel like that might have put too many people at ease.  Not only are old devices still running out-of-date kernels, but modern stuff does, too.



"As I have mentioned before, I work for Check Point, a company with firewalls protecting a massive portion of the Internet.  We serve millions of businesses, including all of the Fortune 500 companies, which account for decent chunks of the packets moving across the Internet, all having to pass through one of our firewalls.  After many years of fighting with R&D, Check Point finally upgraded its OS to move away from Linux Kernel 2.6.18 to 3.10 as of 2020.  This is the latest anyone can run, 3.10, still with the epoch code issues.



"I shudder at the thought that we still have customers running code that has passed end of life over a decade ago.  Even if R&D begins working on a migration to kernel 5.10 today, it would take years to finish, and decades to move everyone off the older releases.  All of that on current, modern, state-of-the-art key infrastructure that the world relies on.  I think people should still expect a huge mess at the end of epoch time.  Thanks for the wonderful show.  Looking forward to more episodes to 999 and beyond, and not having to use Twitter anymore."  Well, maybe someday soon.  "All the best."



So anyway, that note speaks for itself, and I thought it was a valuable look inside the reality of the commercial use of Linux-based appliances.  There really is an "If it's not broken, let's not break it" mentality.  And really, after Check Point built a robust firewall architecture on top of an old OS kernel, if it's working, why change it?  If it's primarily being used to boot one's own code, and important things like the OpenSSL library can be kept current, then why mess with the boot loader?  You know?  Which is essentially what Unix or Linux has become there.  The problem is, though, the earlier Linux file system timestamps are all using the signed 32-bit time, so any reliance upon file times is going to go berserk in 2038.  On the other hand, that's still 15 years off.  So we're going to be okay.



LEO:  I guess.  I think we'll still be doing this.



STEVE:  And Leo, you may be driving, we may both be driving new cars by then.



LEO:  Maybe, yeah.



STEVE:  We'll probably be driving our last cars by then.



LEO:  Fifteen years from now you and I, I don't know if we'll be driving, but we'll see.



STEVE:  Ah.  Hopefully.



LEO:  It's coming pretty quick.  Hate to say it.  And I hope my car is not running a Linux kernel pre-3.10.  That's one thing, for sure.



STEVE:  Matt, tweeting as @Slater450413, said:  "Hey, Steve.  There's something that still bothers me about that recent Microsoft hack."  He's talking about the capture of the key in the crash dump.  He said:  "There were far too many coincidences where the attacker just 'happened to know' various flaws."  Okay, now, so just to interject, Matt's referring to the fact that not only was the secret key resident in RAM and then captured by a system crash dump, but then there was a chain of no fewer than every single one of five flaws were required for that crash dump image to migrate itself all the way to where it was able to be reached by an attacker.



So anyway, Matt continues, writing:  "Unlike regular flaws in a desktop OS where you can continually poke at it with a debugger attached to the OS, most of the surface of a cloud instance being attacked also happens to be blind to outcome observation, like the modern version of blind SQL injection.  Yet pinpoint accuracy was achieved regardless of that, multiple times.  It's almost as if the attacker had source code to examine, sort of like the access and download that was achieved during the SolarWinds attack two years ago."



He says:  "I don't like conspiracy theories, and I'm aware we will never know, but this seems far more likely than an attacker guessing their way through this much obscure and highly technical knowledge.  I remember back on Security Now! Episode 800 there was a passing comment Microsoft made publicly that they do not rely on source code being kept secret as a security feature.  But I wonder if this accidental visibility may now be coming home to roost."  And it is a good point.  As Matt says, we'll never know.  But it was, if nothing more, a surprisingly glaring sequence of successive failures all required in sequence that brought that key into the hands of someone who then also knew what to do about it.



LEO:  My memory is not complete, but it seemed like it wasn't necessarily triggered by the bad guy.



STEVE:  No.



LEO:  In other words, there was a key.  It was in RAM.  Due to a flaw in Microsoft, it was saved as a dump.  The bad guy didn't trigger that.



STEVE:  No.



LEO:  Then further mistakes were made kind of things where it was [crosstalk] public network.



STEVE:  Multiple stages where they had code in place that was supposed to scrub their crash dumps for keys.



LEO:  But it didn't work.  [Crosstalk] case Microsoft had [indiscernible].  But I don't think this was triggered by the bad guy.  I think it was a crime of opportunity where that dump kind of got its way into the public, and then the bad guys know, one thing they do know, you look at crash dumps for private keys, and that's when they found it.



STEVE:  I think you're making a very good point, yeah.



LEO:  It was really just bad coding on Microsoft's part.  It didn't need to be discovered by anybody.  It just happened.



STEVE:  Brings bad karma.  Brought a whole new meaning to bad karma.



LEO:  Yeah, yeah.



STEVE:  I do have one piece of errata, and then we're going to take our last break and then talk about, begin to talk about the Top 10 Cybersecurity Misconfigurations.



LEO:  Oh, I can't wait.  That's a great subject.



STEVE:  Oh, it is.  Last week I mis-numbered the shortcut of the week 842 instead of 942.



LEO:  Doh.



STEVE:  So, yeah, technically that would have made it a shortcut of the week from about two years ago, which was not what I intended.  Elaine, who transcribed the podcast, of course, and several of our listeners picked up on my mistake.  That shortcut, just to remind everyone, was to the very nice YouTube video explaining how to set up Syncthing.  So for anyone who may have thought, "Hey, that's great, Steve created a shortcut for this week's podcast 942," and then found that it didn't work, okay, the link I created was grc.sc/842.  Sorry for the confusion.



LEO:  Can't make one at 942 now?



STEVE:  No.  I figured I'd just tell everybody where it went.



LEO:  You could probably google it, too, "Syncthing tutorial," something like that.



STEVE:  Well, yeah.  When I was setting up this week's properly numbered shortcut of the week, I noticed that 555 of our listeners took me literally and followed the 842 link to learn more about Syncthing.



LEO:  Good.  They're smart.



STEVE:  So I'm glad for that.  Or they're used to me, like Steve really means, oh, let's guess.  Let's roll the dice.



LEO:  Take a chance.  All right, Steve.  Let's take a break, and then I am fascinated to learn...



STEVE:  This is very meaty, very good piece.



LEO:  Top 10.  We should do like a Letterman countdown from 10 to 1, cybersecurity misconfigurations.  Maybe we can get Tom Selleck or somebody in to read those.



STEVE:  Last Thursday, the U.S. National Security Agency, our NSA, and the awkwardly named Cybersecurity and Infrastructure Security Agency, our CISA, jointly published a cybersecurity advisory.  Of course, everything has initials, so it's the "CSA," right, the Cybersecurity Advisory, the CSA.  This advisory was the result of NSA and CISA red team and blue team activities, as well as the activities of both agencies' Hunt and Incident Response - that's of course HIR - teams.  The advisory identifies and highlights the most common cybersecurity misconfigurations which they continually uncover within organizations.  And the report details the tactics, techniques, and procedures - which of course is TTPs - which the bad guys use to exploit these misconfigurations.



Checklists, I think, such as these, and obviously you agree, Leo, can be very useful because over and over and over in this podcast we encounter the many consequences of the tyranny of the default.  You add multifactor to your authentication.  Great.  Good move.  But did you carefully read through all of the cautions that its publisher included?  Has this new facility been fully configured correctly?  Or did it take longer than you expected to get it going, and so now you're late for a meeting and had to run off leaving it - probably forever - the way it came out of the box?



Reading through this advisory, I feel as though this podcast has been serving its listeners well, since nothing here will actually surprise anyone who's been listening for long.  These are the topics that we often focus on.  In fact, number one on the list is "Default configurations of software and applications."



Okay.  So today we're going to hit the high points on this advisory.  But this work really drills down into very useful and actionable specifics.  In fact, there's so much here, this is really only going to be part one of talking about this because there's so much.  And their intention really was to get specific, not just to kind of produce a Top 10 list of - and you've got to figure out what that means.  But they dig in.  So that makes this thing too long, even for us to cover in two parts at the level of detail here that it offers.



But every listener who's responsible for their enterprise's network security really would do well to spend some time with it on their own.  So it's available as both a web page and as a very nicely formatted 44-page - like I said, it goes into depth - 44-page PDF.  I've made the PDF edition this week's carefully-numbered GRC shortcut of the week.  So grc.sc/943 will redirect you to the PDF which resides at defense.gov.  And I also have the link to the original web page up at the top of the topic of the show.



Okay, so without further ado, I'm going to quickly enumerate these Top 10 most common and most troublesome cybersecurity network-related misconfiguration issues.  Then we'll begin digging into each one a bit deeper.  So Top 10 are default configurations of software and applications.  No surprise there.  Improper separation of user/admin privileges.  Yup.  And one of our sponsors of TWiT will love this one:  Insufficient internal network monitoring.  That's clearly a biggie.  Number four, lack of network segmentation.  How many times have we talked about that, especially in a residential setting with untrustworthy IoT things.  Number five, poor patch management.  In other words, not keeping up with updates, or not prioritizing updates.



Number six, bypass of system access controls.  That'll be interesting to see what they have to say about that one.  Weak or misconfigured multifactor authentication methods.  Again, okay, not quite clear what they mean there.  Insufficient access control lists on network shares and services.  Okay.  Now there we're talking about it works for everyone if we don't make the ACLs too tight.  So let's just leave them the way they are.  Bad idea.  Number nine, poor credential hygiene.  And number 10, unrestricted code execution.  Again, that's one of those, well, if we start restricting things, things are going to break.  So that's not good.  Unfortunately, it can be very useful for the bad guys.



So the NSA and CISA elaborate, just before they get into the details on those Top 10.  They explain:  "These misconfigurations illustrate, first, a trend of systemic weaknesses in many large organizations, including those with otherwise mature cyber postures; and, two, the importance of software manufacturers embracing secure-by-design principles to reduce the burden on network defenders."  In other words, they're saying both ends of this are at fault.  And I think that's exactly right.



They said:  "Properly trained, staffed, and funded network security teams can implement the known mitigations for these weaknesses."  Meaning it is possible to secure things.  And secondly:  "Software manufacturers must reduce the prevalence of these misconfigurations, thus strengthening the security posture for all customers by incorporating secure-by-design and default principles and tactics into their software development practices."



They wrote:  "NSA and CISA encourage network defenders to implement the recommendations found within the Mitigations section of this advisory, including the following, to reduce the risk of malicious actors exploiting the identified misconfigurations.  So remove default credentials and harden configurations.  Disable unused services and implement access controls.  Update regularly and automate patching, prioritizing patching of known exploited vulnerabilities."  Of course that's a big CISA issue.  "Reduce, restrict, audit, and monitor administrative accounts and privileges."



They said:  "NSA and CISA urge software manufacturers to take ownership of improving security outcomes of their customers by embracing secure-by-design and default tactics, including embedding security controls into product architecture from the start of development and throughout the entire software development lifecycle.  Eliminating default passwords.  Providing high-quality audit logs to customers at no extra charge."  And we have seen a case where you had to pay more for the logs, and that didn't work very well.  And finally:  "Mandating multifactor authentication," that is, the software manufacturers should be mandating multifactor authentication.  They said:  "Ideally phishing-resistant for privileged users and making MFA a default rather than an opt-in feature."



In other words, both parties is what we're seeing here right off the bat.  The software manufacturer and the software user have responsibilities.  A perfect example that they mention is default passwords.  The users of any system MUST change the default passwords when they're first setting up their software, but the creators of that software should also absolutely come up with some way to avoid ever having a default password in the first place.  In other words, you know, yes, the people using it should know to change the default.  But there shouldn't be a default for them to have to know to change.



So just how serious is this simple-seeming problem of default credentials?  Actually, it's a bit shocking when you look at how many different ways these "defaults" can be abused.  The document explains a few.  They wrote:  "Many software manufacturers release commercial off-the-shelf network devices which provide user access via applications or web portals, containing predefined default credentials for of course the built-in administrative accounts.  Malicious actors and assessment teams regularly abuse default credentials."  So again, this is coming from real-life experience with red team and blue team work and post-incidence response.



They're finding:  "Malicious actors and assessment teams regularly abuse default credentials by finding credentials with a simple web search and using them to gain authenticated access to a device; resetting built-in administrative accounts via predictable forgotten passwords questions; leveraging default virtual private network credentials for internal network access; leveraging publicly available setup information to identify built-in administrative credentials for web applications and gaining access to the application and its underlying database; and leveraging default credentials on software deployment tools for code execution and lateral movement."



They said:  "In addition to devices that provide network access, printers, scanners, security cameras, conference room AV equipment, Voice over Internet Protocol phones, and Internet of Things devices commonly contain default credentials that can be used for easy unauthorized access to these devices, as well.  Further compounding this problem, printers and scanners may contain privileged domain accounts loaded so that users can easily scan documents and upload them to a shared drive, or email them.  Malicious actors who gain access to a printer or scanner using default credentials can use the loaded privileged domain accounts to move laterally from the device to compromise the entire domain."



So, okay.  My feeling is that the awareness of the danger posed by dangerous defaults of any kind has been very well known for decades.  So at this point, any manufacturer who's still shipping products with dangerous default settings which they expect their customers to know to change, and frankly which must be changed in order to have any security, such a manufacturer at this point is beyond lazy.  Many, if not most users, even obviously at the enterprise level, presume that the way things come from the factory are intended to be the way they should be, thus the tyranny of the default takes hold.  But, you know, if this was not the case, if it was the case that defaults were secure, we wouldn't have the tyranny of the default, it would be the blessing of the default because these things would be ready to go, secure out of the box.



And, you know, there is, I think we know, the occasional sighting of a manufacturer who gets it, who requires their user to invent their own admin password right off the bat, you can't go any further until you do that, during the initial setup and configuration of the device.  Then it'll make you log off and log back on using it to prove that you're able to, and then you'll be able to move forward, but not until then.  And, you know, even sometimes that's annoying when you're in a hurry and just want to get something, a test setup, up and going and running.



But you've run across a device which was - whose design was done correctly.  They've got no defaults, so they're inherently far more secure.  But again, even today, even though we know how to do that, everyone knows how to do that, these sightings are still the exception rather than the rule.  So the responsibility is still resting upon the rest of us, you know, those users who use these things.



The document notes also some interesting specifics.  They said:  "Certain services may have overly permissive access controls or vulnerable configurations by default.  Additionally, even if the providers do not enable these services by default, malicious actors can easily abuse these services if users or administrators enable them."  Or, I would argue, leave them enabled.  Again, not secure by default.  "Assessment teams regularly find the following:  insecure active directory certificate services; insecure legacy protocols and services; insecure server message block (SMB) service."



Looking more closely at legacy protocol and services and insecure server message block services, they note:  "Many vulnerable network services are enabled by default, and assessment teams have observed them enabled in production environments.  Specifically, assessment teams have observed Link-Local Multicast Name Resolution (LLMNR) and NetBIOS Name Service (NBT-NS), which are Microsoft Windows components that serve as alternate methods of host identification.  If these services are enabled in a network, actors can use spoofing, poisoning, and relay techniques to obtain domain hashes, system access, and potential administrative system sessions.



"Malicious actors frequently exploit these protocols to compromise entire Windows environments."  This is what's happening during that dwell time after someone gets in while they're busy digging deeper into the network.  They said:  "Malicious actors can spoof an authoritative source for name resolution on a target network by responding to passing traffic, effectively poisoning the service so that target computers will communicate with an actor-controlled system instead of the intended one.  If the requested system requires identification/ authentication, the target computer will send the user's username and hash to the actor-controlled system.  The actors then collect the hash and crack it offline to obtain the plaintext password.



"Server Message Block service is a Windows component primarily for file sharing.  Its default condition, including in the latest version of Windows, does not require signing network messages to ensure authenticity and integrity."  And we've touched on this not that long ago in the podcast.  They said:  "If SMB servers do not enforce SMB signing" - which again is not required by default because oh my god, oh heavens, it might break something, we'd have to go find out what and then fix it.  They said:  "Malicious actors can combine a lack of SMB signing with the name resolution poisoning issues above to gain access to remote systems without needing to capture and crack any hashes at all."



So as I'm reading that, another thing occurs to me.  There's an aspect of asymmetrical warfare that applies here.  These systems have grown to be insanely complex over time, and they're dragging along a growing encrustation of legacy protocol crap so that nothing from the past ever breaks, and everything that someone might have continues to work.  Even if no one has anything, you know, if they plug it in, oh, it needs to work.  And this is true, you know, even if the organization themselves doesn't have any of that stuff.  It's all still there because it was on by default to make sure everything just works out of the box.  But it is also horribly insecure.



So the beleaguered IT professional who just wants things to work doesn't mess with those things.  Again, assuming that, if it came that way, it's supposed to be on.  Sure, he or she also wants them to be secure.  But first they have to work.  But the bad guys have an entirely different agenda.  And I understand that this is obvious, but I think it's still critically important.  The bad guys are living off of this debris, off of all of this, you know, "What if maybe we'll need this someday" legacy stuff.



They've learned and know the ins and outs of how to abuse these retired or retiring systems that persist.  And this is the asymmetric aspect.  As we know, security is all about the weakest link.  So it literally does no good to have super security on the latest spiffy new network layers if the ancient networking protocols are still left lying around, active and enabled.  The enterprise may not be using them, but that doesn't mean the bad guys will not be abusing them.



Okay.  So we're at page 18 in the show notes, which usually means we're out of time.  And as I look at the clock, we're also at two hours.  But there is still so much really good meat here to discuss:  segmentation of user and admin privileges; lack of network monitoring and lack of network segmentation; poor patch management; bypassing of access controls; weak or misconfigured multifactor authentication; and more.  So next week I plan to continue digging into some of the remaining high points of this very important document.  So stay tuned.



LEO:  Nice.  It is, it's fascinating.  And at some point the CISA people wrote us and said they wanted to get on the show.  We should talk about the - maybe they wanted to talk about this.  I don't know.  We could ask them about it. 



STEVE:  That's good.  That's a good point.



LEO:  Yeah, yeah.



STEVE:  We will do a show with some CISA guys.



LEO:  I think it's a good idea, yeah.  But really all we need is one guy right here, this guy, Steve Gibson.



STEVE:  Bye.



Copyright (c) 2023 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#944

DATE:		October 17, 2023

TITLE:		Abusing HTTP/2 Rapid Reset

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-944.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  How have ValiDrive's first 10 days of life been going, and what more have we learned about the world of fraudulently fake USB thumb drives?  Should Passkeys be readily exportable, or are they better off being kept hidden and inaccessible?  Why can't a web browser be written from scratch?  Can Security Now! listeners have SpinRite v6.1 early?  Like, now?  What was that app for filling a drive with crypto noise, and what's my favorite iOS OTP app?  And couldn't Google Docs HTML exported links be being redirected for user privacy?  After we address those terrific questions posed by our listeners, we're going to take a look at the surprise emergence of a potent new HTTP/2-specific DDoS attack.  Is it exploiting a zero-day vulnerability as Cloudflare claims, or is that just deflection?



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here, and we have a very big show for you.  Lots of information.  But most importantly we're going to talk about the largest DDoS attack of all time, how it happened, why it happened, and what companies and, more importantly, server makers can do to stop it.  That's next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 944, recorded October 17th, 2023:  Abusing HTTP/2 Rapid Reset.



It's time for Security Now!, the show where we cover your security and safety and privacy and all that good stuff online, sometimes offline as well, with this man.  Now, I usually point over my left shoulder, but he's over to my right now.  No, my left.  Here he is, Steven Gibson of GRC.com.  For those that don't watch the video, I am at my mom's house doing the show from there instead of the usual studio.  So we're a double box, you and I.



STEVE GIBSON:  Yes, we are.  And for anyone who's wondering, wait, wait, wait, wait, wait, wait, wait, wait, wait, what happened to Part 2 of the Top 10 most significant cybersecurity misconfigurations that was promised for this week, well, I did say that unless something even bigger and more important and timely came along, that's what we would be doing.  And wouldn't you know it, that's what happened.  There is a serious new way of abusing the most - I was going to say the most recent HTTP protocol, but that's actually HTTP/3.  This is HTTP - which hasn't been well - has not been widely deployed.  HTTP/2 is, and it's been giving the big cloud providers a run for their money, and for a while now.  I mean, so it is a serious new form of DDoS attack against HTTP/2-capable servers.



LEO:  We had that story on Sunday on TWiT, that the Google had suffered the largest DDoS of all time.



STEVE:  By hugeness.  I mean, like...



LEO:  And I think that was HTTP/2; right? 



STEVE:  That was that story.  So we're going to cover a bunch of our listeners' feedback and questions, which brings us some interesting topics such as how have ValiDrive's first 10 days of life been going, and what more have we learned about the world of fraudulently fake USB thumb drives?  Should Passkeys be readily exportable, or are they better off being kept hidden and inaccessible?  Why can't a web browser be written from scratch?  Can Security Now! listeners have SpinRite 6.1 early, like now?



LEO:  Ooh, what?



STEVE:  Uh-huh.  What was that app for filling a drive with crypto noise, and what's my favorite iOS one-time password app?  And couldn't Google Docs HTML exported links being redirecting be a user privacy feature?  After we address those terrific questions and a few more posed by our listeners, we're going to take a look, as I said, at the surprise emergence of a potent new HTTP/2-specific DDoS attack.  Is it exploiting a zero-day vulnerability as Cloudflare claims, or is that just deflection?



LEO:  Oh, that's interesting.  Cloudflare, one of the biggest DDoS mitigation companies in the business.	



STEVE:  Right.



LEO:  So they would have some inside information.



STEVE:  They got caught with their DDoS down, baby.



LEO:  They might have some incentive for dissembling.



STEVE:  Uh-huh.



LEO:  Oh, interesting stuff.  And now our guy on the job for security, and I guess you're probably the number one place people send silly pictures these days.



STEVE:  I think we've pretty much cornered the market on that.  This one...



LEO:  So good.



STEVE:  So what we have is at least a four-story building because we can see four stories' worth of it, although the picture's cropped a little bit.  And you know how code requires that you have often two exits from like a domicile.  In case a fire broke out near the front door, you need to have like a back way out; right?  Like the traditional fire escape.  Well, that appears to be what we have here because we see a line of back doors.  You know, there's no numbers on the doors.  They don't look like front doors.  They look like, you know, backdoors.



LEO:  They're escape doors, yeah.



STEVE:  Yes.  Unfortunately, if you were to open the door and run out, that would be bad.



LEO:  Wiley Coyote off the cliff.



STEVE:  Because, like, for a reason that's difficult to explain, the stairs are shifted off to the side.  So there's like a spiral staircase where each of the landings of the spiral would correspond with the height at the bottom of the door, like you'd expect, except that they're about, what, six feet off to the side?  And, you know, Leo, what occurred to me is that they're lined up with the windows because there's also windows off to the side.  And anyway, I gave this picture the title "Slavish devotion to the specifications."  As if, like, maybe somebody was looking at the plans and got the windows and the doors exchanged.



LEO:  I can see the memo.



STEVE:  And they didn't stop to ask, why are we putting the stairs over here where there's no doors?



LEO:  I can see the memo to tenants saying, now, just a reminder, when you're at the fire escape, take a sharp right; okay?  Very, very important.  You know, somebody in the chat room said, and this might actually be right, that maybe they're moving - they're prefabbed, and they're moving those into place.  And some smart photographer got the shot before they actually got moved over.



STEVE:  Oh, that's good.



LEO:  That would make sense.  They do prefab those things, I know.



STEVE:  That would explain it.  So they, like, tilted it up, and it's against the wall.



LEO:  Yeah, and then slide it over.



STEVE:  And now they're going to move it over and then anchor it to something.



LEO:  At least I hope that's the case because, if they built it like that, somebody ought to get fired.



STEVE:  Yeah, I don't think that would pass the code inspection.  Let's see.  How's your exit?  Well, don't walk through it.



LEO:  Oh, that's easy.  When the inspector wants to see the exit, you just show them the door.



STEVE:  That's right. 



LEO:  See through there.



STEVE:  See you later, that's right.  Okay.  So I wanted to follow up a bit on ValiDrive.  We're 10 days out from its release, and we're currently seeing a seven-day moving average, which is what my server produces, of 2,443 downloads per day, with a total of 27,603, total when I looked this morning.



LEO:  That's amazing.



STEVE:  So that download rate currently places it at the top among GRC's freeware goodies.  It has managed to push GRC's always number one, which is the DNS Benchmark, that's currently being downloaded 1,867 times per day, it's pushed it to the number two spot.



LEO:  By the way, that's been out for years.  That's pretty impressive.



STEVE:  I was going to say, yeah, the DNS Benchmark has demonstrated somewhat astonishing endurance.  It's got more than 8.5 million downloads since its release.



LEO:  Wow.  Well, I still recommend it all the time.  I mean, it's really...



STEVE:  Well, there isn't really an alternative.  It is the right one.  And it does run under WINE, so Linux people are able to run it, too.



LEO:  Well, that's one problem I have with ValiDrive.  You really have to do it on Windows; right?



STEVE:  Yes, you do.  And that's one of the questions that we'll be addressing here in just a second.  But anyway, so I think it's fair to say that the DNS Benchmark has become a staple on the Internet.  And I expect that ValiDrive may have similar long-term legs, although I would expect at probably a lesser average rate once the news of its existence has settled down.  Anyway...



LEO:  I just hope that the people selling crappy hard drives on Amazon don't figure out where you live because you're kind of cutting into their business there, Steve.



STEVE:  Well, so, for example, I got some feedback from someone whose handle is qqq1.  I don't know why.  It's easy to type over there on the left-hand side of the keyboard.



LEO:  It is, actually.  That's smart.



STEVE:  Anyway, he said:  "I knew these would be trash," he said, "'free gifts' with stuff we order.  We have a large pile of them."  And then he sent me in his tweet a screenshot of the ValiDrive results.  So the drive's declared size is 32GB.  The valid size is 512MB.  And so you can see in the graphic there's like a little strip of green where there's actual storage, and then just a sea of red where there's nothing.



LEO:  And the point here that people who didn't hear your original discussion of this and why you did it, is it's really easy to have the firmware report a larger size than is actually available.



STEVE:  Exactly.  Exactly.



LEO:  So you can make a cheap 500MB drive and have the firmware say, oh, no, this is 32GB.  And the sad thing is a lot of software won't even question it and will just write to these non-existing sectors.



STEVE:  No software will.



LEO:  Oh, wow.



STEVE:  I mean, it will accept a format because formats no longer actually go out and read the data.  A long format does.  But even a long format won't catch it because the drive reports no error when you read from this nonexistent memory.



So, okay.  It pretends to be a 32GB drive, but it only contains half a gig of nonvolatile storage.  So what I was wondering was okay, it's a free gift drive.  Why lie about its size?



LEO:  Why lie?



STEVE:  It's free.



LEO:  Well, it's really a gift horse, and you should look it in the mouth.



STEVE:  Yes.  So as we know, the drive's electronics proactively lie, as you said, about the amount of storage that it contains.  It cannot be a mistake; right?  I mean, this is deliberate.



LEO:  Yeah.



STEVE:  It has to be deliberate fraud.  So then I had the question, you know, but a fraud perpetrated upon whom? 



LEO:  Right.



STEVE:  And in thinking about this further, in the case of such freebie giveaway drives...



LEO:  Oh.



STEVE:  ...it occurred to me that the first target of this fraud is not those of us who receive these freebies.



LEO:  Oh, yeah.



STEVE:  We're the victims, but we're the secondary victims.



LEO:  Right.



STEVE:  The primary victim is the entity, whomever it was, who purchased these as gifts.



LEO:  Right.



STEVE:  You know, in this instance above, they believed they were purchasing probably a huge quantity of 32GB drives, you know, doubtless at a bargain quantity discount, to be used as appealing 32GB giveaway drives.  So the fraud was actually perpetrated upon them by the people from whom they purchased these drives.  No one wants a free 512MB drive anymore.  It's like, what am I going to do with this?  You know?



LEO:  Thanks.  Thanks a lot.



STEVE:  I can only store, like, one photo on it, you know...



LEO:  Yeah, exactly.  



STEVE:  ...these days.  So anyway, our takeaway is to be very cautious about the actual use of any drives that you buy for a bargain at the checkout stand as you're moving your groceries along, and there's a little, you know, like a fishbowl of drives, and you can have one for a buck.  Or a drive that you may have ever received as a thoughtful thank you gift thrown in with something else.



LEO:  For a long time, I said drives are getting so cheap you're going to get them in boxes of Captain Crunch.  That actually probably is true, but it won't be the drive you think you're getting.



STEVE:  Exactly.



LEO:  Wow.



STEVE:  It'll be marked 32GB.  It'll format as 32GB.  And it'll look like it's storing your 32GB of files.  But they're not there when it comes time to read them off.



LEO:  So it writes without error, and then it's only when you try to read it and there's nothing there that you go, oh.



STEVE:  Right.



LEO:  And even then you might not suspect.  You might just think, oh, I made a mistake.



STEVE:  No.  It will be a - so in order for ValiDrive to show - because I have read error and write error colors in the map.  In order for it to show red, it had to - ValiDrive wrote and read with no error.  So no error was returned.  The drive accepted the data from the operating system, said thanks, I got it.  And then when the operating system said give it to me back, it said here you go.  Unfortunately...



LEO:  Well, how does ValiDrive know that it didn't work?



STEVE:  Because it uses a crypto random pattern in order to - and puts that out, and then verifies it when it asks for it back.



LEO:  So the drive will say here, but what you get is nothing.



STEVE:  You get zero.



LEO:  Or some random bits.



STEVE:  Yep.



LEO:  That don't match the bits you wrote.



STEVE:  It's typically all zeroes or all ones.



LEO:  Okay.  Okay.



STEVE:  And the problem is it'll give it - so the drive will give you the file back.



LEO:  You won't get an error.  You won't get an error.



STEVE:  You know, try opening that in your image viewer, and it's not going to be happy.  But this is not a valid file.  So anyway...



LEO:  It's amazing how widespread this is.  Last week you talked about many drives on Amazon you bought that almost all of them...



STEVE:  I bought a dozen.  They were all bad. 



LEO:  Yeah.



STEVE:  Every single one of them.



LEO:  This is terrifying.  Now, you said you're going to talk about other operating systems, so I'll let you go on because I would like - I don't have a PC running Windows.



STEVE:  Well, so Alain said, "Hi, Steve.  Thank you for your work on ValiDrive.  I'd really like to test a few USB sticks with it, but I only have access to Linux systems.  Is it possible to get this running on Ubuntu or some other distro?"  Okay, so Alain and our listeners, I understand.  The DNS Benchmark that I wrote was deliberately WINE compatible.  I went out of my way to make sure that I wasn't doing anything there that WINE did not support on its Windows compatibility layer.  So, and I spent some time on that.



But ValiDrive's UI, which uses dynamic USB insert and removal events to determine which drive the user wishes to test, uses features that are outside of WINE's Windows compatibility set.  So if it was crucial for ValiDrive to run under WINE, I could probably arrange to somehow make that happen.  But I've already spent more time on it than I planned to, and I'm chomping at the bit to get back to finishing SpinRite.  If it turns out to be seeing strong long-term use, I would be inclined, once SpinRite 6.1 is finished, to return to ValiDrive and give it a significant, you know, v2.0 update.



There are some other things I could do that, for the sake of expediency and not spending another few months on it, I said no, no, no, no, no, let's not go there.  There are many people in the newsgroup who are saying, hey, how about this, and how about this, and how about that?



LEO:  I'm sure somebody's asked this, but the one thing I would say would be useful, if you just wrote a paragraph on the methodology.  You don't have to say how to do it, but just what works to verify.  You know, we're going to write this script, you know, whatever it is.  And then maybe somebody can duplicate it on another platform.  You wouldn't mind that; right?  You're not making money on it.



STEVE:  No, I wouldn't mind that.



LEO:  I think that what we need to do is understand.  Because there are ways you could do this that would not be a valid test.  Like for instance saying, "Can I read it?  Oh, I can read it, it's good."



STEVE:  Oh, yeah.  Yeah, yeah, yeah.



LEO:  So you went the extra mile.  So what is it that we need to do to really have a valid test?  You've got all this data now, as well, and what works and what doesn't work.



STEVE:  One thing I should mention is that it does run, for example, under Parallels VM, even on an ARM Mac.



LEO:  Oh, that's good to know.



STEVE:  Yes.



LEO:  Oh, well, I would do that.



STEVE:  Yes.



LEO:  Oh, okay.



STEVE:  It absolutely will.



LEO:  Oh, well, then I'm set.



STEVE:  Yeah.  There is something known as Safe Emulation Mode which, at least on an ARM Mac....



LEO:  Which is what I have.



STEVE:  ...we have to turn on in order to get it to run.



LEO:  So you turn on Safe Emulation Mode.  I thought it was really masking the reads and writes.  But of course it can't be because you have to read and write data to a drive.  You have to get out of the way and let that happen.



STEVE:  Right.



LEO:  So that makes sense.  So if I run Windows on ARM in emulation mode on my Mac, using Parallels...



STEVE:  It'll work.



LEO:  It will still be able to run it.  Oh.



STEVE:  Yup.  We've got people who are running it all the time.



LEO:  That's good to know.  That's great.



STEVE:  Yup.  Okay.  So some feedback from our listeners.  Marko Simo said:  "Just visiting X to contact you."  Oh, and he's Marko Simo / @markos@twit.social, by the way.



LEO:  Yeah.  A good Mastodon user.  Yeah.



STEVE:  So he said:  "Regarding Passkeys and their lack of easy exportability, QR codes, et cetera, have you ever thought that those exact features would make Passkeys vulnerable for phishing and other social engineering attacks?  They're hidden from normal users for a very good reason.  Someone should now figure out what could be a phishing-resistant way to move them across ecosystems.  Or we could all start using SQRL."  Well.



LEO:  Because you solved this already.



STEVE:  I did.  So I completely agree with Marko's observation.  It is definitely a double-edged sword to make the Passkeys less mysterious.  And I have to say, Leo, your comment about the lackluster success, to put it, well, to phrase it oddly, of Passkeys heartened me a little bit.  You know, not because I don't want the world to have a public key-based network authentication system, but because the reason my SQRL system has gone nowhere wasn't just me.  Of course, that is the reason it's never going to go anywhere.  But at least Passkeys hasn't yet taken the world by storm.  Or if it had, I would have at least expected a little bit more from SQRL.  And I use the word "yet," you know, hasn't "yet" taken the world by storm since I really do fully expect that this FIDO2-based Passkeys public key system will someday supplant the use of secret passwords.  But probably not in my lifetime.  Seriously.  Not in my lifetime.



LEO:  Okay.



STEVE:  All of the evidence suggests - and I'm feeling great right now, so I'm planning to be around for a while.



LEO:  You know, I think [crosstalk] take off in the next 10 years, it's going to be something else; don't you think?  You think it's going to be around?



STEVE:  No.  I think they did it right.  I think, and my god, if it takes 10 years to get this thing going, we don't have another 10 years to go for Plan B.  So, you know, seriously, all the evidence suggests that implementing this sort of sweeping change really will be that slow.  But, you know, the reason I put all that time into SQRL was I said, well, no one had done it.  And if no one does, then it's never going to happen.  Well, my system's never going to happen anyway, but at least a public key system option now exists.  And the problem is it just, from the users' standpoint, there's nothing that's obviously better about it.  As you said, Leo, I mean, we already have password managers.  This problem has been solved.



LEO:  Yeah, yeah.



STEVE:  You know?  It's not solved well, you know, and we're kind of limping along with a few arrows in our back, some of us.  But at least it's obvious how it works.



LEO:  Yeah, everybody's already gone the extra mile to get that working.



STEVE:  Yup.



LEO:  And if they haven't, that's even going to be harder to get them to switch over because they're just entering their pet's name and their birthday on every site.  And they say, well, what's wrong, you know, there's no - they don't see a problem. 



STEVE:  Right.



LEO:  So what are you fixing?



STEVE:  Yeah.  Yeah, so Michael Moloney, he tweeted:  "Aren't Passkeys meant to be a device verification like SSH keys?  While you could copy them between devices, best security is to generate them on the device you need to authorize, so if that device gets stolen or hacked, you can revoke the key for that device.  Also, if the device is hacked, logs are way more useful if it shows 'Paul's iPhone' rather than 'Paul's key,' and then he has to redo every Passkey device."  He said:  "Aside from backups, I hope Passkeys stay nontransferable.  When you put your PIN or fingerprint in your phone, you're basically unlocking all the sites this Passkey has access to, same as when you sign into your PC with your password that has all your SSH keys."



Okay.  So that's true, and I think it's a good point.  This would be roughly equivalent to having multiple separate username/password pairs for logging into one's bank account with a different username/password pair stored in each device.  Then, if a device was compromised, the usernames and passwords that were being used by the compromised device could be independently disavowed and disabled.  Now, while that's good in theory, that really raises the level of management complexity to a whole new level.  You know?  Yes, there's a lot of power in that approach, but also a huge amount of responsibility for keeping track of who's on first.  And, you know, okay, yes.  A crazy power user could do it.  But we can't even get anybody to use a Passkey, let alone manage them all separately.



Okay.  Brian M. Franklin said:  "@SGgrc Maybe it IS still possible to write a new web browser, with even better features."  And then he quoted a thread in X, which I'm now calling it, where from someone named AKHIL, who tweeted:  "Here's the multiplayer browser engine I've been working on in action."  And I have to say I love the name because, because it's multiplayer, he named it the BRAID browser, as in, you know, B-R-A-I-D, like something braided, which I think is a very cool name for it.  Anyone who's curious can go to braidbrowser.com, B-R-A-I-D-B-R-O-W-S-E-R dot com.



LEO:  I wish it didn't sound so much like Brave.



STEVE:  Yes.



LEO:  So Braid like in hair.



STEVE:  There is a collision there.



LEO:  Yeah, yeah.



STEVE:  Okay.  So, okay.  So here we've got a guy who wrote a browser.  Well, there's browsers, and then there's browsers.  I love the idea of the creation of hobby browsers.  You know, it's a modern take on the idea of a hobby operating system.  And a bunch of hobby operating systems have been created.  In fact, I carefully considered basing SpinRite's future upon a few of them.  But lack of support for booting on UEFI-only systems ultimately led to me to the embedded RTOS-32 OS that I've talked about before.



Similarly, while I think it's possible to create a functional outline of a modern browser, the task, as I've said, has grown in size such that it's similar to that of creating a fully functional operating system.  You know, yeah, there's hobby browsers, and then there's production-scale Chromium, Firefox and Safari, and that's it.  So anyway, the guy, I don't know what this thing actually does.  I played the video that was there in X.  And there were two cursors moving around separately.



LEO:  Ooh.  What a cool feature.



STEVE:  I don't, what, if you're ambidextrous maybe?



LEO:  I don't know what...



STEVE:  Maybe you keep one cursor over on the left and one on the right?  Then you don't have to go so far?  I don't know.  Anyway, whatever it is.



LEO:  That's not a thing a hobbyist browser would do.



STEVE:  That's right.  Because that's what we really need.  We need more cursors on our browser.



LEO:  I'd be very careful because really the browser is the primary vector for malware into your machine.



STEVE:  I know.



LEO:  And it's really easy to make a mistake in a browser.  You know, maybe people are using Chromium as their base for all the rendering.



STEVE:  This guy claims it's a million lines of C++.  And you know, Leo, every one of them is perfect.  Every one of the million lines.



LEO:  That scares me.  That scares me.



STEVE:  Yeah.



LEO:  There's a new browser called Arc, A-R-C, from Arc the browser company.  But it's Chromium, and then they're doing the GUI on top of it, the chrome, as it were.  And that's a little more doable.  I don't know if I'd trust an individual to write a browser.  That's hard.



STEVE:  We've got Mozilla.  We've got Chromium.  We've got Safari.



LEO:  Yeah, yeah.



STEVE:  I think that's it.  Techgifthorse tweeted:  "Hi, Steve.  long-time listener, first-time DMer.  Regarding Episode 943" - the last week - "where you introduced that Google was embedding tracking links in Google Docs exports, I recently noticed that Google is also doing this in Calendar invites.  No privacy.  I'm not sure if they were trialing it in Google Docs HTML exports, and now it is expanding, but I thought it was noteworthy.  I also wanted to say that you and Leo have inspired me to start my master's in cybersecurity."



LEO:  Oh, that's great.  Oh, that's great.



STEVE:  "So please keep the pods coming and keep up the great work.  Can't wait for your forthcoming email option so I can delete this bogus Twitter account.  Sincerely, Sean."



LEO:  People are going over to Twitter just to talk to you.



STEVE:  Yeah, they are.  And, I mean, I've seen enough of that.  And that was why, when I said I'm leaving Twitter, what I meant really was I'm not going to make everyone use Twitter as a means of contacting me.



LEO:  Yeah, that's the problem, yeah.



STEVE:  So just as soon as I have SpinRite 6.1 ready for rollout, I'll let everyone here know, of course, to get the official final.  That's the super-easy communication path, Leo, that you and I have built with all of our listeners every week.  Then I'll begin work on bringing up a new email facility so that I'm able to get the word out to all of 6.0's current owners, and part of that work will be to create a new means for me to send announcements to my Twitter followers and also create a means for them to send me the equivalent of private DMs.



Brett Russell says:  "Hi, Steven."  Being formal.  Ant calls me Mr. Gibson.  So, yeah, this is different.  This is "Hi, Steven."



LEO:  Ant doesn't call me Mr. Laporte, he calls me Leo.  But everybody else is Mister, which is well-brought-up Southern boy.



STEVE:  Here we have "Hi, Steven."  It actually is my given name, S-T-E-V-E-N, so very formal.



LEO:  I didn't know that.



STEVE:  "I realize this is a request out of the blue, but my hard disk is failing on my main machine.  I own a copy of SpinRite," and then he provided in the tweet his code, which is his SpinRite serial number.  "But the disk is set up as GPT, which 6.0 does not support.  Any chance you can give me a pre-release of 6.1 to run, please.  There's nothing critical on the drive; but if it dies, it will take some time to bring it all back."



So of course, Brett.  And this goes for everyone.  If you go to grc.com/prerelease.htm, just P-R-E-R-E-L-E-A-S-E dot htm, what you'll get is a little form.  Enter your current SpinRite 6.0 serial number, which is presented whenever SpinRite is run.  And I just verified that Brett's code works perfectly for him.  So when you do that you'll receive a link to download your own personalized and licensed copy of the latest prerelease of v6.1.



What you'll get at this point is the DOS executable, since I haven't yet built it all into the Windows boot prep thing.  That's coming next.  So just place that DOS executable, I think it's 95K, onto your SpinRite boot drive and let 'er rip.  The current prerelease expires at the end of November, so that's six weeks from now, and I'm sure we'll have updated releases before then.  So you just come back and get an update.  Or just retard the machine's date, you know, if you want to buy yourself some more time.



At this point I'm 100% certain that SpinRite is safe to use.  The reason is I just checked, and we have exactly 800 registered SpinRite prerelease testers who have been using and pounding on all of the SpinRite prereleases.  I've taken great pains to be very careful along the way, and no one has ever experienced any data loss at any point.  So this thing's ready for the world.  I just need to tie up a bunch of loose ends which, you know, any three-year project of this complexity is going to have.  So, you know, that's where we are.  But, you know, absolutely, all of our listeners, anybody who has SpinRite, you're welcome to go grab 6.1.  It's all but done, just a few last things will need to get changed.



Ray Franklin tweeted:  "Steve, I've looked through the current show notes for the reference 'wipe drive with encryption.'  What's the program to effectively wipe a drive with random data?"  And that's VeraCrypt.  And I tweeted him the link.  It's at veracrypt.fr because it's French.  And you could use it as we've talked about, basically to fill your drive with cryptographically secure noise.  It's actually encrypting what you have already there.  But if you throw away the key, no one's ever going to get it.



Marcus, whose handle is @BadAssB, he said:  "Hi, Steve.  I just had a quick question.  I heard you say that you do not use Authy for two-factor authentication, and I was wanting to know what you do use.  Thanks."  So being an Apple ecosystem person for mobile, I use an app called OTP Auth, and I love it.  It's clean, simple, and is very widely praised.  And for anyone who can't find it easily, I've got a link in the show notes.



LEO:  I also, just I'll plug for something that's both iOS and Android, which I've been using, also free, also open source:  2FAS, 2FAS.  And what's nice about it is it will back up an encrypted blob with your secrets to either Google Drive or iCloud.  So it makes, you know, this is an issue for me.



STEVE:  Nice, nice.



LEO:  And one of the reasons we used Authy is so that we can move - because I get a new phone, many new phones all the time.  And so the ability to move is very, very important to me.



STEVE:  And especially be able to move cross-platform.



LEO:  Yes, exactly, yeah.



STEVE:  Nice.



LEO:  I just installed both a new Pixel 8 Pro and a new iPhone 14 Pro, and 2FAS works on both.  And I was able to import those secrets.  And it was really nice.  And it looks good.  Here, I'll show you, it's pretty.  It's a pretty - I don't know about yours, but this one's pretty.  And it uses Face ID to...



STEVE:  Oh, very nice.



LEO:  By the way, there's all my two factors.  They're useless in 30 seconds.  Use them quick, kids.



STEVE:  This is very pretty.



LEO:  It's very pretty.  You click it, and it will copy it to the clipboard.  It goes red when it's about to expire and then generates a new one.



STEVE:  Oh, I like that.  OAuth does not do that.



LEO:  And it can sort alphabetically or in other ways, which is also nice.  Yeah, and it's just the I use.  But 2FAS.  To each his own.  The nice thing is you don't have to use a page solution or a solution from Google or Microsoft.  You can, you know, this is a standard, TOTP is a standard, and there are lots of people that make software.



STEVE:  And finally, Henning said:  "Dear Steve.  When listening to SN-943" - again, last week - "I have a different explanation for why Google is rewriting links in Google Drive documents published as HTML pages:  Prevent the leakage of the document's URL through referer information.  Though the drive documents are public, they're hidden by their long, obscure URL.  But the referer info can reveal the URL in the server logs of the embedded links.  The additional step through a second Google service hides this information."



So first of all, Henning, that's a neat thought.  That hadn't occurred to me.  But these links were present in offline HTML exports of the doc.  So wherever the HTML was being served from would be appearing in the referer header.  Also, if this was a favor that Google was doing for us to protect our privacy, then their referring link URLs would not have been loaded down with unique tracking parameters in order to see exactly what link it is that someone has clicked on in the future.  So, much as I love the idea that they might be protecting us, that doesn't appear to be their intent here.



LEO:  There's been a lot of conversation about Google rewriting search queries, as well.



STEVE:  Drives me nuts.  I mean, and they even show you the true link, but then that's not what you get when you click on it.



LEO:  Yeah.  Yeah.  You know, I think that, let's face it, they're an advertising-based business, and the need to monetize through advertising kind of permeates everything, all these free services.  And as a result I think, you know, if you want privacy, maybe, I pay 25 bucks a month for advertising free search.



STEVE:  A month?



LEO:  Well, it doesn't have to be a month.  No, it is, I think you can get it for five bucks.  I pay a little more because they have a browser and stuff.  But Kaki is very good results, Google-quality results, but no ads, no targeting, none of that.  But they don't have Docs.  They don't have Google Drive.  They don't have all these other great features.  And that's the problem.  But all that stuff's free, and you've got to monetize it.  I understand.



STEVE:  Abusing HTTP/2 Rapid Reset.  As I said last week, we would be continuing with the second part of our look into the Top 10 Cybersecurity Misconfigurations if something bigger and more important didn't bump it to the following week.  Well, something did indeed.  So we'll do the second part of the Top 10 Cybersecurity Misconfigurations if something doesn't again bump it into the following week.  But today we're going to talk about an interesting surprise.



The headline about this which first caught my attention was Cloudflare's, was on Cloudflare's blog.  The headline read:  "HTTP/2 zero-day vulnerability results in record-breaking DDoS attacks."  So, you know, yeah, what?  The idea of there being a newly discovered zero-day vulnerability in a core Internet protocol such as HTTP/2 would be huge news, if it were true.  For the rest of the podcast we're going to explore what has happened and decide to what degree this is actually true.  And as always, the devil is in the details, and these details are both interesting and important.



So a few lines into their coverage, Cloudflare wrote:  "Cloudflare has mitigated a barrage of these attacks in recent months."  So my first reaction was to question their use of the term "zero-day" if they've been doing something for several months.  You know, but after wondering whether it might be an abuse or an overuse of the term, I suppose it's correct if they first learned of this unsuspected problem through an attack.



You know, we've settled upon the definition of "zero-day" as being the exploitation of any previously unknown vulnerability.  Or stated another way, the first indication we have of there being some specific vulnerability is not when we find it through examination or when it's reported by a responsible researcher, but when we're surprised by something happening that we didn't believe to be possible.  So, yeah, it would probably be correct for this to be considered a zero-day vulnerability in the v2 HTTP protocol, if indeed it was a vulnerability in the HTTP/2 protocol - which, surprisingly, is not the case.



So it is to Cloudflare's benefit, Leo, exactly as you said at the top of the show, since they are in the DDOS protection business, to characterize this as a surprising zero-day vulnerability in HTTP/2.  That deflects the blame to the protocol when it appears that the true culprit is implementation.  The HTTP/2 protocol is not to blame because there's a difference between a vulnerability and the abuse of a feature that's exacerbated by Cloudflare's internally decoupled serial processing multi-proxy architecture.



LEO:  Oh, this is interesting.  Ah.



STEVE:  That's what made it such a problem.  And it applies to the cloud providers because the way they distributed the workflow interacted with this feature.  So as I think the evidence will show, and Google was much more forthcoming, as we'll see, this is more about something that Cloudflare's behind-the-scenes request-processing architecture was ill-suited to handle than the exploitation of any previously unknown vulnerability in HTTP/2.



It's a clever and, in retrospect, probably foreseeable and maybe even inevitable leveraging, that is, the attack is, of a feature in HTTP/2.  So, you know, I don't mean to jump on Cloudflare too much about this.  But characterizing this as a zero-day vulnerability in HTTP/2 is really not accurate or fair.



Okay.  So let's back up a bit.  Let's start with a very quick background review of layered network design, and then a quick history of HTTP.  We refer to HTTP, the Hyper-Text Transfer Protocol, thus HTTP, as an application-level protocol - also known as a Layer 7 protocol because that's as high as the layers go -  because after we've finally finished building up layer upon layer of the various underlying enabling protocols, we finally get to HTTP, which was the whole point.  And it's there where something finally happens and gets done.



So what do we mean by "layer upon layer"?  At the bottom-most level or layer we have an agreement about wires and voltages and clock rates and, like, signal transmission times.  And those are used to signal the transmission of individual discrete zeroes and ones.  That's typically referred to as the physical layer.  You know, that's as physical as it can get.  Like what's the voltage on the wire, and what does it mean?  Then, with that agreed upon, we describe the groupings of those ones and zeroes to form packets of data which are addressed between physical hardware endpoints on a single local network.



This is then the Ethernet protocol.  And those Ethernet packets, in turn, contain IP (Internet Protocol) packets which carry the addresses of endpoints on the global Internet.  And inside those IP packets are TCP packets where the TCP protocol is used to manage the numbered, sequential byte-oriented reliable flow of data between those specific virtual ports on the devices specified by their IP addresses on the global network.



And finally, the data that flows back and forth under the watchful eye of the TCP protocol is formatted as specific queries and responses in accordance with the top-level application protocol, HTTP.  So that's the layering all the way up from the physical layer to layer 7, the application layer.  And over the decades since their initial definitions, all of these protocols have seen some tuning, some tweaking and evolution in their definition as they interacted with the real world.  And that's certainly been true for HTTP.



With a bit of editing by me for clarification, here's how Wikipedia briefly sums up HTTP's evolution.  They wrote:  "Development of HTTP was initiated by Tim Berners-Lee at CERN in 1989 and summarized in a simple document describing the behavior of a client and a server using the first HTTP version, which was 0.9.  That version was subsequently developed, eventually becoming the public 1.0.



"Development of early HTTP RFCs began a few years later in a coordinated effort by the IETF" - that's our Internet Engineering Task Force - "and the W3C" - W3 meaning World Wide Web, and C for Consortium - "with work later moving exclusively back to and now at the IETF.



"HTTP/1 - and it's shown as a /1 rather than a v1 because HTTP/1 is actually what's sent over the wire with each query to specify the format of everything that will follow, so you need to specify the version right upfront so that the recipient is able to know how to interpret everything that's going to come.  So v1.0 was finalized and fully documented in '96.  It quickly evolved into v1.1 a year later in 1997, after which its specifications were updated in '99, 2014, and even just last year in 2022.  Its secure variant, HTTPS, is now used" - as we know - "by more than 80% of websites," writes Wikipedia.



Now, HTTP/2, which is the topic for today, published first in 2015, so it's already eight years old, it "provides a more efficient expression" - and we'll be talking about this efficiency in a minute - "of HTTP's semantics 'on the wire.'  As of April 2023, HTTP/2 is used by 39% of websites."  So as of April this year, just shy of 40% of all websites offer HTTP/2.  "And it's supported by almost all web browsers, representing over 97% of web users."  So, you know, all the popular browsers do it now.  "It's also supported by major web servers over TLS using an Application-Layer Protocol Negotiation (ALPN) extension where TLS 1.2 or newer is required."



And just to round this out for the sake of completion, we actually drop TCP as the transport protocol for HTTP in favor of the faster to set up, yet less feature-rich UDP, when we're using HTTP/3, which is technically out there now, though not yet widely adopted.  "HTTP/3, of course, is the successor to HTTP/2.  That was published just last year.  It's now available from one quarter of all websites," says Wikipedia, "and it's at least partially supported by most web browsers.  As I mentioned, unlike all of its TCP-based predecessors, HTTP/3 does not run on top of TCP.  Instead it uses QUIC, which runs on top of UDP.  Support for HTTP/3 was added to Cloudflare and Google Chrome first, and is also enabled in Firefox.  HTTP/3 has lower latency for today's web pages which are pulling information together through many separate widespread sources.  If HTTP/3 is enabled on the server, such pages," says Wikipedia, "will load faster than with HTTP/2, which is, in turn, faster than HTTP/1.1, in some cases as much as three times faster than HTTP/1.1."



Okay.  So now that we have the lay of the land, let's look at what took Cloudflare and others by surprise a few months back. There's a bit of, I guess it's marketing/bragging going on here from Cloudflare, but mostly a lot of good information, and I'll insert a little bit of editorializing as we go.  So Cloudflare wrote:  "Earlier today" - and this was just posted exactly one week ago on the 10th of October.  They said:  "Cloudflare, along with Google and Amazon AWS, disclosed the existence of a novel zero-day vulnerability dubbed the 'HTTP/2 Rapid Reset' attack."



And they wrote, Cloudflare wrote:  "This attack exploits a weakness in the HTTP/2 protocol to generate enormous, hyper-volumetric Distributed Denial of Service (DDoS) attacks.  Cloudflare," they said, "has mitigated a barrage of these attacks in recent months, including an attack three times larger than any previous attack we've ever observed, which exceeded 201 million requests per second.  Since the end of August 2023, so that's just a couple months ago, Cloudflare has mitigated more than 1,100 other attacks with over 10 million requests per second, and 184 attacks that were greater than our previous DDoS record of 71 million RPS (requests per second)."



And again, now these new attacks, 201, so nearly three times those 184 attacks that were greater than their previous DDoS record of 71 million.  And I should note that a similar stat from Google about this put it at just shy of 400 million requests per second.  I think that's 390 something, as I recall.



So I'll pause here just to note that there are several different ways to measure or characterize DDoS attacks.  Traditionally, attacks leveraged bandwidth flooding and saturation.  Just pure bandwidth.  The incoming network links to a server would be so saturated with packet traffic that a site's upstream routers would be overwhelmed as they were aggregating the traffic being aimed at a destination server, so overwhelmed with bogus packets that legitimate packet traffic stood little chance to get through to the server.  It was just a matter of just, you know, a matter of numbers.  If you've got a hundred bogus packets for every good one, there's a 1% chance that the good one's going to get through.  So essentially the server's services are being denied, and thus the term Denial of Service, of course.



But as we know, some time ago web pages shifted from being simple static dumps of a previously written textual page.  For example, my own GRC site remains that way to this day.  It's just simple static HTML.  But most web services switched over to being the public-facing aspect of a content management system, a CMS of some sort.  Under this new website paradigm, any incoming HTTP query is fielded by some sort of code running on the server.  That code examines the query details and assembles the page which will be returned to the client on the fly.  And this, in turn, typically involves multiple queries to a backend relational database of some kind, typically a SQL server.



The point of this is that under this new content management paradigm, replying to a query for a simple page, what looks like a standard HTML page, consumes server-side compute resources in order to pull the various components of the page together for its return to the client.  The other thing is that in the earliest days of this new approach, these on-the-fly page assembly systems were often not very efficient.  So each query, especially deliberately complex queries, which, you know, might be requiring a lot of backend database work to pull a complex page together, could be quite costly to answer or to reply to.



Naturally, it didn't take the bad guys long to figure this out, after which they switched their attack tactics from flooding a server's raw bandwidth pipeline with what could well just be noise, it didn't really matter what the packets, you know, contained, to loading the server down with actual valid queries, each of which could be quite expensive for it to answer.  Thus we went from thinking of DDoS attacks in terms of bits per second to requests per second because it was the request that was expensive to handle, not just a bandwidth flood.



Now, I take issue with Cloudflare's characterization of this as an attack which "exploits a weakness in the HTTP/2 protocol."  That's not what has been happening.  As we'll see, these new attacks exploit some new features of HTTP/2 which were added to make the protocol much faster overall.  But it turns out that these new features are subject to abuse.  And if a server's query-processing chain is not really explicitly designed to handle these new features, it can be forced into collapse by attackers.  This is where we'd say "it's not a bug, it's a feature," and we'd mean it.



So Cloudflare continues.  They said:  "This zero-day" - which, okay, it was a surprise - "provided threat actors with a critical new tool in their Swiss Army knife of vulnerabilities to exploit and attack their victims, at a magnitude that has never been seen before.  While at times complex and challenging to combat, these attacks allowed Cloudflare the opportunity" - oh, it's an opportunity - "to develop purpose-built technology to mitigate the effects of the zero-day vulnerability."  In other words, whoops.  As I said, they got caught with their DDoS down and needed to, you know, up their technology in order to deal with this.  And again, we're going to see, and I'll shortly provide ample evidence to back it up, this is not really a vulnerability, and I am a little disappointed in Cloudflare's approach to this.  We all know that I'm a big fan of Cloudflare, but I believe they took the wrong tack on this one.



So they said:  "In late August of this year, our team at Cloudflare noticed a new zero-day vulnerability, developed by an unknown threat actor, that exploits the standard HTTP/2 protocol, a fundamental protocol that is critical to how the Internet and all websites work," meaning HTTP, and now 2 is a lot better.  "This novel zero-day vulnerability attack, dubbed Rapid Reset, leverages HTTP/2's stream cancellation feature by sending a request, then immediately canceling it, over and over.



"By automating this trivial 'request, cancel, request, cancel' pattern at scale, threat actors are able to create a denial of service and take down any server or application running the standard implementation of HTTP/2.  Furthermore, one crucial thing to note about the record-breaking attack is that it involved" - and pay attention here - "it involved" - that is, this attack that was bringing down Cloudflare's very strong preexisting DDoS defenses, they said - "consisted of roughly 20,000 machines in a modestly sized botnet."  They said:  "Cloudflare regularly detects botnets that are orders of magnitude larger than this, comprising hundreds of thousands and even millions of machines.  For a relatively small botnet to output such a large volume of requests, with the potential to incapacitate nearly any server or application supporting HTTP/2, underscores how menacing this vulnerability is for unprotected networks."



LEO:  So I had assumed that this was some sort of massive amplification attack because previous attacks have had to have massive botnets in order to do this.



STEVE:  Right.  Right.



LEO:  Why is it that this HTTP/2 cancel request, rapid reset request, can't be more easily blocked?  If it's only 20,000 IP addresses it's coming from, how come you can't just mitigate it?  I don't understand.



STEVE:  That actually is the strategy.  That's the only strategy.  And I actually, I wind up saying exactly that.



LEO:  Okay, good.



STEVE:  The only strategy that I think makes sense.



LEO:  I don't want to jump ahead.



STEVE:  You jumped ahead.



LEO:  Okay.



STEVE:  Because thank god TCP cannot have its source address spoofed; right?  You need to have a full communication link that is confirmed on each end so you do know the IP address of every bot that's attacking you.



LEO:  So previous attacks were much simpler.  There were often SYN/ACK floods where a bot would say Hello, and then your server would have to say Hello to start the conversation.  At which point that server, that client would go, never mind, Hello.



STEVE:  And that's actually a good analogy to this attack.  But it's up to level where it's not just a SYN which is initiating a TCP connection.  It's actually a request that is initiating backend work.



LEO:  So there's a lot of work because it's reset, you're asking it to reset the thread.  So it's more than just hello, hello, are you there.



STEVE:  It's more than just a single packet being received.



LEO:  Yeah.



STEVE:  Okay.  So I'm going to make three points:  First, I hope it still astonishes us when Cloudflare writes:  "Cloudflare regularly detects botnets that are orders of magnitude larger than 20,000, comprising hundreds of thousands and even millions of machines."  Just think about that.  This kind of power in the hands of miscreants is fundamentally destabilizing.  Because the Internet is so powerful, and because it mostly works so well, we've allowed ourselves to gradually grow to become utterly dependent upon its presence.  But last week we noted Microsoft was observing around 1700 DDoS attacks per day.  So there are websites that are occasionally denied the sort of reliability that the Internet has to offer because someone somewhere just says no, you know, we want you off the 'Net.  We're going to, you know, we're not happy with something you just did, so we're going to punish you.



LEO:  Well, people might remember, back in the day Steve had this happen to him.



STEVE:  Yup.



LEO:  It was going on during the show.



STEVE:  Yup.



LEO:  And it was just, you know, it's just vandalism of a kind.



STEVE:  Well, it turned out it was because I had those DDoS attack reports on my site.  And so they thought I was bragging in some way.  And in fact I was not.



LEO:  You were just doing the news.



STEVE:  And when I removed those pages, the attacks stopped.



LEO:  Oh, that's very interesting.



STEVE:  You know, it wasn't me saying nanny nanny nanny, you can't get me.  It was like, ow, you know?  I just got blasted off the Internet.



LEO:  Yeah.



STEVE:  So the second point I wanted to highlight was that Cloudflare witnessed by far the largest attack they have ever experienced, which was produced by what is unfortunately now considered to be a relatively modest-sized botnet, consisting of only around 20,000 individual clients.  Since this sort of attack rides on TCP, the attacker's source IPs cannot be spoofed since TCP requires confirmation of packet round trips before the query can be sent.  So Cloudflare has the specific IP of every bot that was attacking it.  That's how it knows how many, because it can count.



LEO:  Right.



STEVE:  And the last point is that, since this was a new form of abuse to which HTTP/2 is prone, every one of these bots needed to be updated with a specific module designed to implement this attack against its target.  So someone, somewhere, figured this out, wrote the code to do this, then either established a new botnet for this purpose or caused an existing modest-sized updatable botnet to be updated to add this new form of attack to its bag of tricks.



They said, Cloudflare said:  "Threat actors used botnets in tandem with the HTTP/2 vulnerability" - again, I'll take issue with that characterization, but we'll get there in a second - "to amplify requests at rates we have never seen before.  As a result, our team at Cloudflare experienced" - I love this phrase here - "some intermittent edge instability."  Uh-huh.  Intermittent edge instability.  Right.  In other words, we're all about DDoS protection, but this one got us because we had never seen anything like it, and our backend architecture was ill-equipped to handle it at the time.  And in fact they do, they did publish a chart where it was like up to 80% of their connections were having trouble because, I mean, this thing really clogged up their network. 



LEO:  Well, that makes sense since you said 80% of all servers use HTTP/2.  It was able to hit them all, basically.



STEVE:  So their entire customer base, you know, their whole frontend is HTTP/2 equipped.



LEO:  So if you're using Cloudflare, it is HTTP/2.



STEVE:  Yeah.  You have HTTP/2 front-facing.  



LEO:  Ah, interesting.  Ah.



STEVE:  So they said:  "While our systems were able to mitigate the overwhelming majority of incoming attacks, the volume overloaded some components of our network, impacting a small number of customers' performance with intermittent 4xx and 5xx errors, all of which were quickly resolved."  Right.



"Once we successfully mitigated these issues and halted potential attacks for all customers, our team immediately kicked off a responsible disclosure process."  Meaning for several months there has been stuff going on behind the scenes that no one knew about until Tuesday, when this was officially revealed in a coordinated disclosure.  They said:  "We entered into conversations with industry peers to see how we could work together to help move our mission forward and safeguard the large percentage of the Internet that relies on our network prior to releasing the news of this vulnerability to the general public."



They wrote:  "There's no such thing as a 'perfect disclosure.'  Thwarting attacks and responding to emerging incidents requires organizations and security teams to live by an 'assume breach' mindset because there will always be another zero-day, new evolving threat actor groups, and never-before-seen novel attacks and techniques.  This 'assume breach' mindset is a key foundation towards information sharing and ensuring in instances such as this that the Internet remains safe.  While Cloudflare was experiencing and mitigating these attacks, we were also working with industry partners to guarantee that the industry at large could withstand this attack.



"During the process of mitigating this attack, our Cloudflare team developed and purpose-built new technology to stop these attacks and further improve our own mitigations for this and other future attacks at massive scale."  In other words, they learned that there was something that was able to get by their DDoS  mitigations, and they needed to develop new technology to deal with it.



LEO:  Boy.  Wouldn't you love to be a fly on the wall in the war room, there when they get these, you know, emergencies?



STEVE:  Oh, yeah.



LEO:  And they all leap into action.  I have to say, I mean, whatever they did wrong here, you've got to credit them with really talented people doing a lot of...



STEVE:  I do.  I love Cloudflare.



LEO:  Yeah.



STEVE:  I just wish they weren't calling it a zero-day vulnerability in HTTP/2.  It's not.  It's a feature.  But anyway, we'll get there in a second.



LEO:  It is a vulnerability, however, a feature-rich vulnerability.



STEVE:  It is, yeah, an exploitable feature.



LEO:  Feature.  There, that's a better phrase.  Yeah, they should have said that, yeah.



STEVE:  An exploitable feature.  So they said:  "These efforts [theirs] have significantly increased our overall mitigation capabilities and resiliency."  So yes, we're now better than ever.  They said:  "If you're using Cloudflare, we're confident that you are now protected.  Our team also alerted web server software partners who are developing patches to ensure this vulnerability cannot be exploited."  And they said:  "Check their websites for more information.  Disclosures," they said, "are never one and done.  The lifeblood of Cloudflare is to ensure a better Internet, which stems from instances such as these.  When we have the opportunity to work with our industry partners and governments to ensure there are no widespread impacts on the Internet, we are doing our part in increasing the cyber resiliency of every organization, no matter the size or the vertical.



"It may seem odd that Cloudflare was one of the first companies to witness these attacks.  Why would threat actors attack a company that has some of the most robust defenses against DDoS attacks in the world?"  Well, they've answered their own question, and they're about to.  "The reality is that Cloudflare often sees attacks before they are turned on more vulnerable targets.  Threat actors need to develop and test their tools before they deploy them in the wild.  Threat actors who possess record-shattering attack methods can have an extremely difficult time testing and understanding how large and effective they are because they don't have the infrastructure to absorb the attacks they're launching.  Because of the transparency that we share on our network performance, and the measurements of attacks they could glean from our public performance charts, this threat actor was likely targeting us to understand the capabilities of this new exploit."  And I think that's exactly, exactly right.



LEO:  So we're just going to launch a small botnet against you, just to see what you can do. 



STEVE:  Yeah.  Remember in the days when I was under attack, I had two T1s which were each 1.54Mb.



LEO:  Right.



STEVE:  So a 3.4Mb flood would swamp me.  I mean, I was a gnat.  I wasn't even worth swatting.  So, you know, it took nothing to knock me off the 'Net.



LEO:  But on the other hand, Cloudflare has terabytes of bandwidth.



STEVE:  Yes.  Yes.  It is very difficult to push them down.  But this attack did it initially.  They said:  "But that testing" - meaning the bad guys' testing - "and the ability to see the attack early helps us develop mitigations for the attack that benefit both our customers and the industry as a whole."  That is, you know, they shared what they learned.  Even so...



LEO:  Both sides get information, the bad guys and the good guys, yeah.



STEVE:  Yup.  "Even so, whether it was Log4j, SolarWinds, EternalBlue, WannaCry/NotPetya, Heartbleed, or Shellshock, all of these security incidents have a commonality, a tremendous explosion that ripples across the world and creates an opportunity to completely disrupt any Internet-dependent organization, regardless of the industry or the size.  While we wish we could say that Rapid Reset may be different this time, it is not.  We're calling all CSOs  no matter if you've lived through the decades of security incidents or this is your first day on the job  this is the time to ensure you are protected and stand up your cyber incident response team.



"We've kept the information restricted until today to give as many security vendors as possible the opportunity to react.  However, at some point, the responsible thing becomes to publicly disclose zero-day threats like this.  Today is that day.  That means that, after today, threat actors will be largely be aware" - largely, you bet - "largely be aware of the HTTP/2 vulnerability."  Again, it's a feature that's exploitable.  "And it will inevitably become trivial to exploit and kick off the race between defenders and attackers  first to patch versus first to exploit.  Organizations should assume that systems will be tested, and take proactive measures to ensure protection."



The guy wrote:  "To me, this is reminiscent of a vulnerability like Log4j, due to the many variants that are emerging daily, and will continue to come to fruition in the weeks, months, and years to come.  As more researchers and threat actors experiment with the" - and they're calling it again a vulnerability - "we may find different variants with even shorter exploit cycles that contain even more advanced bypasses."  I would argue against that, but we'll get to that in a minute.



And they said:  "And just like Log4j, managing incidents isn't as simple as 'run the patch, now you're done.'  You need to turn incident management, patching, and evolving your security protections into an ongoing process because the patches for each variant of a vulnerability reduce your risk, but they don't eliminate it.  We don't mean to be alarmist, but we'll be direct:  You must take this seriously.  Treat this as a full active incident to ensure nothing happens to your organization."



Okay.  So that's a little bit self-serving since Cloudflare has just finished explaining that while they were initially caught off guard by this unanticipated and massive zero-day attack, they're now up to speed and are inviting everyone to come and hide behind their perimeter.  On the other hand, they're offering it at no charge.  They do offer the service for free.



So they wrap this up by writing:  "Cloudflare's mission is to help build a better Internet.  If you're concerned with your current state of DDoS protection" - and if you weren't before, you should be now - "we're more than happy to provide you with our DDoS capabilities and resilience for free to mitigate any attempts of a successful DDoS attack.  We know the stress that you're facing as we have fought off these attacks for the last 30 days and made our already best-in-class systems even better."  Okay.  So thank you very much.



Now, even though this first posting of theirs didn't give us any of the meaty technical details that this podcast never shies away from, I wanted to share it since it's what Cloudflare is telling the entire world.  And sharing it here is important because I believe, as I've said, that it mischaracterizes HTTP/2 as having surprising and unsuspected vulnerabilities, which is not the case.  We're going to get into the meaty details, Leo, after you tell us who's paying for them.



LEO:  This is really fascinating.  And, you know, the takeaway is, if this was a test with a small botnet...



STEVE:  Oh, boy.



LEO:  ...imagine what the traffic could be if they really targeted somebody.  And of course there's some events coming up that are very commonly DDoSed, including the Super Bowl, it's very common to DDoS betting groups, or at least to extort them.  That's the other reason.  It's not just a test, it's also a note to companies that are getting ready for Black Friday, Prime Day, the Super Bowl betting, that we have the capability to shut you down on those days.  And I think that that's very clear message, and I'm sure that the teams, the security teams of those companies are sitting up and taking notice.  It's hard to fix a feature.



STEVE:  Exactly.



LEO:  Easier to fix a bug.



STEVE:  Exactly.



LEO:  Wow.  All right.  I'm just - I am just blown away by this HTTP/2 vulnerability that you say is a feature, this rapid reset is a feature, not a bug.



STEVE:  So, yes.  Now we get to the meaty details.  It is described in CVE-2023-44487.  The CVE disclosure is a bit more fair, but even it contains a bit of spin and a little bit of misdirection.  The description on the CVE states:  "The HTTP/2 protocol allows a denial of service (server resource consumption) because request cancellation can reset many streams quickly, as exploited in the wild in August through October of 2023."



Okay, now, Google's summary of the problem is, I think, exactly correct.  So here's what Google wrote.  They said:  "Since late 2021" - so for the last two years almost - "the majority of Layer 7" - meaning HTTP, you know, high-level, application-level - "DDoS attacks we've observed across Google first-party services and Google Cloud projects protected by Cloud Armor have been based on HTTP/2" - meaning so HTTP/2 has become the preferred DDoS attack protocol - "both by number of attacks and by peak request rates.  A primary design goal of HTTP/2 was efficiency, and unfortunately" - and here it is - "the features that make HTTP/2 more efficient for legitimate clients can also be used to make DDoS attacks more efficient."



So that's what it is.  It's an abuse of the efficiency that HTTP/2 offers.  That is precisely the truth.  "The features that make HTTP/2 more efficient for legitimate clients can also be used to make DDoS attacks more efficient."  So not any surprise zero-day vulnerability, just the clever abuse of a new feature of HTTP/2 that anyone designing HTTP/2-capable servers will need to take into account.  Until now, Cloudflare hadn't.  They'd been relying upon their conventional DDoS protections, which are doubtless very strong; but this new form of attack was too much even for that.



But that said, I do agree with the creation of a CVE and the raising of alerts about this since, indeed, rapid HTTP/2 stream resets promise to cause some havoc with any HTTP/2-capable web server that's not capable of handling these resets efficiently.



To get some scope of the problem, switching back to Cloudflare for a minute, in their more technical write-up they wrote:  "Starting on August 25th, 2023, we started to notice some unusually big HTTP attacks hitting many of our customers.  These attacks were detected and mitigated by our automated DDoS system.  It was not long, however, before they started to reach record-breaking sizes, and eventually peaked just above 200 million - 200 million - requests per second.  This was nearly three times bigger than our previous biggest attack on record.  Concerning is the fact that the attacker was able to generate such an attack with a botnet of merely" - and unfortunately we're now using the word "merely" when we talk about 20,000, you know, consumer routers and other things that have been compromised.



LEO:  I'll never forget, and this was 20 years ago at TechTV, we had - I can't remember who it was, one of our famous [indiscernible] hackers come in and show us an IRC channel that was being used to control, for command and control of a botnet.  And we're sitting here watching this IRC channel.  I think this was right before they shut it down.  This was law enforcement.  But just one after another, a compromised system announcing itself and effectively saying, you know, what is thy will, my master?  You know, what do you want me to do?  And they just all sit there in this IRC - I don't know if they still use IRC for this.  But they just sit there waiting to be commanded.



STEVE:  Now, maybe you're remembering my write-up because I...



LEO:  Maybe it was you.  I don't know.  I saw this happening, though.



STEVE:  Yeah.



LEO:  I think it was on the set of the TV show.



STEVE:  Okay.



LEO:  And I believe law enforcement brought it in.



STEVE:  Yeah.  And I also saw the same thing because I infiltrated the attackers on their C channel, and I captured a screen of all these bots reporting for duty.



LEO:  Yeah.  This is 20 years ago.  So it's been going on for a long time.



STEVE:  Oh, yeah.



LEO:  Do they still use that kind of command and control?  Or do they have a more modern way?



STEVE:  I've no idea.  I've been out of that specific loop.



LEO:  Yeah.



STEVE:  So the Cloudflare guy said:  "There are botnets today that are made up of hundreds of thousands or millions" - and remember, this is a 20,000-machine attack.  There are botnets with millions of machines.  So imagine when those millions of botnets are updated with this attack.  I mean, that's the meltdown event.  I mean, it's, whoa.



LEO:  Yeah.  We could see a world [indiscernible] here.



STEVE:  Given that the entire web - oh, here's another really interesting metric.  So this was 201 million RPS, requests per second.  They wrote:  "Given that the entire web typically sees only between one and three billion requests per second, it's not inconceivable that using this method could focus the entire global web's worth of requests onto a small number of targets."



LEO:  Holy cow.



STEVE:  It's like focusing the sun.



LEO:  Oh, god.



STEVE:  You know, onto Manhattan.



LEO:  The magnifying glass with all that power into a pinpoint.



STEVE:  Yup.



LEO:  Your server's going to melt.



STEVE:  It's going to melt.  So just to be clear, since putting this into context is important, this attack by just 20,000 bots generated, on its own, a peak of 201 million requests per second.  And the Internet across its entire expansive whole sees between one and three billion requests per second.  And whereas this breathtaking attack was produced by only 20,000 bots working in concert, botnets consisting of hundreds of thousands to millions of individual devices are known to exist.  And as Cloudflare wrote in their less technical overview, as of last Tuesday October 10th, every bad guy bot operator who may not have been in the loop until now, now knows about it, and it's a trivial attack to create.



What's more, as Wikipedia told us, HTTP/2 is offered as an available protocol by around 39% of the Internet's web servers.  We don't currently know what percentage of those web services poorly handle HTTP/2 stream resets.  It could be that cloud providers inherently have a greater problem with this due to their more highly distributed architecture.  A single standalone HTTP/2 server might now be much more affected by this than by an HTTP/1.1 request flood.



Okay.  However, unless and until any vulnerable web servers are updated and/or moved behind some sort of perimeter protection, they're going to be at least in danger of exploitation from this novel server-side resource depletion attack.  And what's significant is that this is true even from the great many more lesser bot operators who run much smaller botnets.  Once the code to implement this attack has circulated throughout the underground, as I noted above, it's trivial to do this attack.  And given how things are likely...



LEO:  Out of curiosity, is it just a loop?  Is it just, like, a couple of lines of code that says send this reset request, rapid reset request, to this server, and then go to 10?



STEVE:  That's what it is.



LEO:  Is it basically that?



STEVE:  That's it.



LEO:  I just implemented it.



STEVE:  Yup.



LEO:  Good lord.



STEVE:  Probably in LISP, knowing you.



LEO:  Yeah, I'll do it in LISP.  What is the command for rapid reset?  Is it a straightforward command?



STEVE:  We're getting there.



LEO:  Okay.  All right.  Sorry.  I'm getting excited.



STEVE:  So also dated last Tuesday the 10th was Cloudflare's posting titled "HTTP/2 Rapid Reset:  Deconstructing the record-breaking attack."  Their description begins:  "This attack was made possible by abusing some features of the HTTP/2 protocol and server implementation details."  Now, see, and the techie guys are a little less hype-y; and, you know, they're getting it right.  So this is still - this is Cloudflare saying:  "This attack was made possible by abusing some features of the HTTP/2 protocol" - not calling it a vulnerability - "and server implementation details."



LEO:  Well, I mean, to be fair, it's a feature, but it is also a vulnerability.



STEVE:  It's not a goodie.



LEO:  If you had a soft spot on the top of your head, it's a feature, but I can also knock it.



STEVE:  It's still a soft spot, yeah.



LEO:  It's still a soft spot.



STEVE:  So they said:  "Because the attack abuses an underlying weakness in the HTTP/2 protocol, we believe any vendor that has implemented HTTP/2 will be subject to the attack.  This includes every modern web server.  We, along with Google and AWS, have disclosed the attack method to web server vendors who we expect will implement patches."  Mmm, we'll see.  I mean, there's not much you can do.



LEO:  That's my question, yeah, yeah.



STEVE:  In the meantime, yeah, the best defense is using, of course, here they're going to sell themselves, although it is free, a DDoS mitigation service like Cloudflare's in front of any web-facing web or API server.



Okay.  Again, what exactly now is an HTTP/2 stream reset?  I could write this up myself, but in the interest of saving some time so that I have time to assemble other interesting things this week because actually I did this first, and then I did the user feedback, I'm going to switch to Google's explanation.  I'm switching to Google because Cloudflare is pretty much unable to stop blaming this on HTTP/2 and calling it a zero-day vulnerability.



Google explains:  "HTTP/2 uses 'streams,' bidirectional abstractions used to transmit various messages, or 'frames,' between the endpoints.  'Stream multiplexing' is the core HTTP/2 feature which allows higher utilization of each TCP connection.  Streams are multiplexed in a way that can be tracked by both sides of the connection while only using one Layer 4" - meaning TCP - "connection.  Stream multiplexing enables clients to have multiple in-flight requests without managing multiple individual connections."



Okay, now, I'm going to switch back and note that we talked about this a million years ago on the podcast.  Well, okay, 2015.  With HTTP/1.1, a single TCP connection can be created to a server and a client.  It's able to send multiple queries to the server sequentially in a sort of pipeline.  But the server must receive them in order and reply to each query in order so that the sequence of replies matches the sequence of queries.  The big trouble with this is that many smaller and faster-to-answer queries could get held up behind an earlier query that takes a server more time to reply to.  Perhaps it needs to query something else to obtain an answer.  Meanwhile, some queries like for simple images, for example, you know, little icons and embellishments, would be held up waiting behind the big slow query to be returned.



When Google says that HTTP/2 uses "streams," they mean that ID tags are added to individual queries, and ID tags are returned with their matching replies.  This creates a sort of parallel abstraction where, even though there's still only one connection, a highly capable HTTP server could accept every incoming query the moment it arrives, assign an independent task or thread to begin assembling each query's reply individually, and then send back that query's reply with its associated ID tag as soon as its reply is ready.



LEO:  That makes sense because modern machines are multithreaded.



STEVE:  Yes.



LEO:  So they're not doing a single process anymore.  That's ancient history.



STEVE:  Right.



LEO:  So they could take advantage of that.



STEVE:  Yes.



LEO:  A lot of programming languages have streams built in for this very reason.



STEVE:  Right.  So Google continues.  They said:  "One of the main constraints when mounting a Layer 7" - meaning an HTTP-level, application-level - DoS attack is the number of concurrent transport connections.  Each connection carries a cost, including operating system memory for socket records and buffers, CPU time for the TLS handshake, as well as each connection needing a unique four-tuple, that is, the IP address and port pair for each side of the connection, which constrains the number of concurrent connections between two IP addresses.



"In HTTP/1.1, each request is processed serially.  The server will read a request, process it, write a response, and only then read and process the next request.  In practice, this means that the rate of requests that can be sent over a single connection is one request per round trip, where a round trip includes the network latency, the proxy processing time, if there's a proxy, you know, in front, and backend request processing times.  While HTTP/1.1 pipelining is available in some clients and servers to increase a connection's throughput, it is not prevalent among legitimate clients."



Now, I thought that was interesting.  What Google just said was that, while HTTP/1.1 does technically support allowing clients to "send ahead" queries, as I originally mentioned, in the process known as pipelining, in practice that has not ever been widely adopted, and now it won't be because HTTP/2 has come along now, and all the browsers do that.  And the reason was it's a little scary to like send requests blind and just hope that the answers come back in the right sequence, and you not get confused in counting them.  Much cleaner if you tag them with individual IDs so you're sure about which query matches which request.



So anyway, it wasn't widely adopted, so the clients wait until they receive the reply to their single outstanding query before they're comfortable to submit the next query, even though technically they could.



Google says:  "With HTTP/2, the client can open multiple concurrent streams" - so again, get this, that a stream is just an abstraction.  In the same way that packets are an abstraction of a connection on the Internet between two points, we say we're connected, but it's actually a stream of packets.  Similarly, streams are an abstraction of multiple concurrent connections over a single TCP connection.



So they said:  "Each stream corresponding to one HTTP request. The maximum number of concurrent open streams is, in theory, controllable by the server.  But in practice, clients" - here it is - "may open 100 streams per connection, and the servers process these requests in parallel."  Meaning that a client could connect using TCP, bring up a TLS connection to go HTTPS, then generate - then when the server has responded, it'll say that it is able to handle HTTP/2, so the client says great.  It then basically just pours as many queries for items on the server into this connection as possible, giving each one a unique ID tag, knowing that as the server is able to handle them, and in any order that the server may choose, it will start sending the replies back tagged with the same ID as the query that it generated.  And a hundred of them can be outstanding at once.  You can just pour a hundred in.



So Google said:  "For example, the client can open 100 streams and send a request on each of them in a single round trip.  The proxy" - which is the frontend to a cloud service - "will read and process each stream serially, but the requests to the backend servers can again be parallelized.  The client can then open new streams as it receives responses to the previous ones."  Meaning it can always have a hundred of them outstanding, a hundred pieces of work to be done.  So as streams complete by their reply coming back, that says, oh, there's room for another stream, and so it just immediately emits another query.  So Google says:  "This gives an effective throughput for a single connection of 100 requests per round trip, with similar round trip timing constants to HTTP/1.1 requests.  This will typically lead to almost 100 times higher utilization of each connection."



Okay.  So this is where, why, and how HTTP/2 offers potentially far greater performance over HTTP/1.1.  HTTP/2 solicits clients, our web browsers, to dump all their queries into a single connection at once.  Since it's just a single TCP connection, they cannot actually move across the wire simultaneously, but they can be queued up and sent so they're packed very tightly, and so that fewer small and less efficient packets are sent.  Remember we talked about this back then.  If you send small queries, then you're only using a part of a packet, yet you're still having to switch and route an entire packet.  If instead you're able to pack queries literally where the last byte of one starts with the first byte of the next, then all your packets are full, and so all of your switching and your packet processing rate also benefits.  I mean, it's really the way that this should have been done.  But, you know, it was simple back then.  As I said a couple weeks ago, nothing is simple anymore, unfortunately.  It's all getting complicated.



So anyway, assuming that the backend has sufficient parallel serving capability, all of the then-outstanding replies can be assembled in any order.  The moment a reply is ready it's queued up for return to the client with its ID tag identifying which query it's the reply for.  It is beautiful and elegant, and it is a future that is here.  Unfortunately, it can be abused like nothing else ever has been.



So note that the most modern preexisting high request rate attacks are already leveraging this parallel streaming capability.  As Google said, since 2021 the largest attacks we've been seeing are HTTP/2 because they're using these streams in order to make this happen.  So HTTP/2 already allowed much higher request rates than were possible under HTTP/1 or 1.1.  But now here's what you've been waiting for, the problem that's created by the new abuse of this fancy parallel architecture.



Google writes:  "The HTTP/2 protocol allows clients to indicate to the server that a previous stream should be canceled by sending a RST_STREAM frame.  It's all you need to do at the client end."



LEO:  What could go wrong with that?



STEVE:  Basically, never mind.  You send a never mind.



LEO:  Never mind.  Forget I asked.



STEVE:  The protocol does not require the client and server to coordinate the cancellation in any way, meaning there's no need for it to be confirmed.  TCP automatically gives us reliable transport.  So TCP's got that covered.  If the client sends a RST_STREAM frame, it can assume the server has, and it is then able - that frees up one of its 100 for another query. Thus the client can do this unilaterally.  The client may also assume that the cancellation will take effect immediately when the server receives the RST_STREAM frame before any other data from that TCP connection is processed.



Google says:  "This attack is called Rapid Reset because it relies upon the ability for an endpoint to send a RST_STREAM frame immediately after sending a request frame, which makes the other endpoint start working, and then rapidly resets the request.  The request is canceled, but leaves the HTTP/2 connection open for additional requests and cancellations.  Therefore, the HTTP/2 Rapid Reset attack which is enabled by this capability is simple," says Google.  "The client opens a large number of streams at once as in the standard HTTP/2 DDoS attack.  But rather than waiting for a response to each request stream from the server or the proxy, the client cancels each request immediately.  And since HTTP/2 specifies that the client may assume that any canceled stream is immediately available for another request, the HTTP abusing attacker may immediately follow a stream reset with another new request using the same stream.



"The ability to reset streams immediately allows each connection to have an indefinite number of requests in flight.  By explicitly canceling the requests, the attacker never exceeds the limit on the number of concurrent open streams, which is typically 100.  The number of in-flight requests is no longer dependent on the round-trip time, but only on the available network bandwidth.



"In a typical HTTP/2 server implementation, the server will still have to do significant amounts of work for canceled resets, such as allocating new stream data structures, parsing the query and doing header decompression, and mapping the URL to a resource.  Moreover, in reverse proxy implementations, the request may be proxied to the backend server before the RST_STREAM frame is processed.  This requires the proxy to then forward the stream reset to the appropriate back-end server.  By comparison, the attacking client has almost no cost for sending the requests.  This creates an exploitable cost asymmetry between the server and the client."



LEO:  Oh, I was just saying there's the problem.  Clearly.



STEVE:  Yup.  Exactly.



LEO:  Right?  It's from a spam.  It costs them nothing to send it.



STEVE:  Exactly.



LEO:  Yeah.  Yeah.  Wow.



STEVE:  And so another advantage the attacker gains is that the explicit cancellation requests immediately after creation means that a reverse proxy server, which is what Cloudflare has on their perimeter and all the big cloud providers do, won't send a response to any of the requests.  Canceling the requests before a response is returned thus reduces the returning bandwidth to the attacker.  Meaning the upstream back to the attacker won't get clogged up with much bigger replies since they've been canceled.  So all of the work is loaded onto the servers with very little traffic returning.  Although it's diabolical, it's not a flaw in HTTP/2.  It's just an abuse of a deliberate design feature.



So what does Google recommend?  They begin by explaining.  They said:  "We don't expect that simply blocking individual stream requests is a viable mitigation against this class of attacks.  Instead, the entire TCP connection needs to be closed when abuse is detected."  Get this.  "HTTP/2," they wrote, "provides built-in support for closing connections, using the GOAWAY frame."  You've got to love that.  The designers of the HTTP/2 protocol defined a "GOAWAY" frame that could be sent back up the link to the client telling it to, well, go away.



Google says:  "The RFC defines a process for gracefully closing a connection that involves first sending an informational GOAWAY that does not set a limit on opening new streams, and one round trip later sending another that forbids opening additional streams."  So there is a process basically for allowing the client to gracefully shut down rather than just terminate all of its pending queries.  "However," they said, "this graceful GOAWAY process is usually not implemented in a way which is robust against malicious clients."



LEO:  Well, we've got to work on that.



STEVE:  So that, maybe we're going to see about fixing that.



LEO:  We need a graceful GOAWAY, for sure.



STEVE:  "This form of mitigation leaves the connection vulnerable to rapid reset attacks for too long, and should not be used for building mitigations as it does not stop the inbound requests.  Instead, the GOAWAY should be set up to limit stream creation immediately."  So maybe we will see an HTTP 2.1 that will tweak the definition of this.



They wrote:  "This leaves the question of deciding which connections are abusive.  A client which cancels requests is not inherently abusive.  The feature exists in the HTTP/2 protocol to help better manage request processing.  Typical situations," says Google, "are when a browser no longer needs resources it had requested due to the user, for example, navigating away from the page, or applications using a long polling approach with a client-side timeout."  Mitigations or, you know, maybe when you've got multiple mouse pointers, and they're in disagreement about what should be done next.



LEO:  You see?  There's a good reason for that browser.



STEVE:  "Mitigations for this attack," they said, "can take multiple forms, but mostly center around tracking connection statistics" - basically detecting abuse - "and using various signals and business logic to determine how useful each connection is.  For example, if a connection has more than 100 requests with more than 50% of the given requests canceled, it could be a candidate for a mitigation response.  The magnitude and type of response depends on the risk to each platform, but responses can range from forceful GOAWAY frames as discussed before to closing the TCP connection immediately."  Okay, just hang it up.  Basically go away, then hang up.  "To mitigate against the non-canceling variant of this attack, we recommend that HTTP/2 servers should close connections that exceed the concurrent stream limit.  This can be either immediately or after some small number of repeat offenses."



LEO:  In other words, there's a fingerprint for this kind of attack.



STEVE:  Yeah.



LEO:  You can immediately detect.



STEVE:  Yeah, it's very obvious if like every request is immediately being canceled.  You know, I'd hang up on that jerk immediately.



LEO:  Yeah, yeah.  Well, that's good news.  I mean...



STEVE:  Oh, yeah.  So that's the story.  Exactly as Google initially characterized the problem, the deliberate design decision of HTTP/2, which can be used to significantly increase its efficiency, and does increase its efficiency, can also be used to significantly increase the potency, the potential and the potency of DDoS attacks.  Attackers figured out that, rather than using HTTP/2 simply to simultaneously ask for tons of resources and then be flooded by their return, they could instead immediately cancel their request and reissue another.  Against this threat, today's servers, especially those in the cloud which have distributed their request handling among multiple backend components, which might make canceling those issued requests much more tricky because those also have to be distributed, make it more time consuming, would be seriously taxed by this new attack strategy.



It will be interesting to see whether anything can be done to change HTTP/2 to prevent or limit this abuse.  At the moment, the various servers are testing themselves and modestly tweaking their request handling to do a "less bad" job of dealing with this abuse of this HTTP/2 feature.  Since, as I mentioned above, it's not possible to spoof the IP addresses of anything that's riding on top of TCP, the best solution might be to dynamically blacklist, or at least significantly throttle, any IP that is found to be abusing HTTP/2 Rapid Reset.  In that way, the bots would be recognized and quickly ignored at the perimeter of a large hosting provider like Cloudflare.



LEO:  We do something like that all the time with password mitigation; right?  You don't let somebody log in - my light just went out.  It's a sign, isn't it.  You don't let somebody log in an infinite number of times, one after the other.  In many cases you just say, oh, six times you're out, or 10 times you're out.



STEVE:  Yeah.



LEO:  So this would be a simple mitigation.  It doesn't change the protocol, though; right?  It changes the way the server operates.



STEVE:  Exactly.



LEO:  Yeah.  You would - see, I feel like this is a really good example of - and I'm sure these days when you're in an IETF meeting or a W3C meeting, and they propose these protocols, they hammer, you know, they try to think of ways people would take advantage of it.  And somebody just forgot to say, oh, you know, somebody could just keep sending rapid resets, and it would overwhelm us because we can - we'll start a million threads per user.  We should build into the protocol there's a limit.  Or, you know...



STEVE:  Right.



LEO:  And they just didn't.  They missed it.



STEVE:  Right.



LEO:  It's not that the feature's a bad feature; right?  I mean, the idea of these with a well-behaved client is fine.



STEVE:  Well, and consider that this is eight years old, that is, the protocol.



LEO:  Just took them a while.



STEVE:  It took them this long, I mean, it took the bad guys this long to go, hey, you know?



LEO:  So I'm not surprised that a bunch of engineers didn't figure it out and say...



STEVE:  Our attack, we're only able to have 100 concurrent outstanding requests.  So we're having to wait for the replies to come back to free up a stream.  So why don't we just cancel that request?  Then we don't have to wait for the answer.



LEO:  Yeah.  And it's pretty - I think it would be pretty easy to spot somebody behaving badly, that there would be - I guess the key is that there's enough distinction between a well-behaved user - this is the problem.  If it's not distinct, you don't want to hang up on, well, this is why SYN/ACKs work because you can't tell the difference between somebody who's sending you a reasonable SYN and somebody who doesn't care.  But if you could tell the difference between a bad actor and a normal user pretty readily, which it sounds like you could, I think mitigation might be doable.



STEVE:  And here's where the term "heuristic" comes to our aid.



LEO:  Oh, yeah.  That's a heuristic, isn't it.



STEVE:  We would be using a heuristic.  We would say, you know, we've done stats on, you know, for like the last month, on all of the HTTP/2 connections.  We've never seen more than 5%.



LEO:  Exactly.



STEVE:  Rapid resets.



LEO:  Yeah.



STEVE:  So if anybody does 25, you know, they don't deserve our attention today.



LEO:  Right.  You always want to have it so that legitimate - it's like spam [indiscernible].  Legitimate users, you don't want false positives.  So you want legitimate users who are doing things right, but maybe are demanding.



STEVE:  Yeah, I mean, how many times do we have - is someone required to say, if you didn't get our email, check your spam folder.  Because presumably you wanted that because you were talking to them, but it got routed into spam because unfortunately we've, you know, again, a heuristic that is not sharp enough.



LEO:  Yeah, it was too aggressive.  This one seems like it would be doable, but maybe, I don't know, maybe it's not.



STEVE:  Well, and that's I'm sure what the server vendors are scurrying around now doing in anticipation of this.  And I should mention that all of these servers have the ability to turn off this protocol.  So if you were your own HTTP/2 site, and you were being attacked, just tell your server to turn off support for HTTP/2, be a 1.1 server.  And yes, your performance will drop, but it's better than being held off the Internet by one pissed-off bot.



LEO:  It really drops when you get 20,000 people doing hundreds of requests.



STEVE:  I mean, I wouldn't be at all surprised if one attacker could not hold, I mean, not Cloudflare, but a standard website with no upfront protection.



LEO:  Well, and a static site like mine, for instance, I'm not worried about mine being DDoSed, but a static site like mine, for instance, I don't need HTTP/2.



STEVE:  No.



LEO:  HTTP/1.1 would be fine.



STEVE:  Yup.



LEO:  I'm using NGINX.  Is NGINX one of the server...



STEVE:  Yes, NGINX is great, and they are working on a mitigation.



LEO:  We use DOS mitigation on our site, but I won't tell you by  whom.  Not Cloudflare.  But there's a lot of companies that offer DOS mitigation, DDoS mitigation.



STEVE:  Anybody who's curious could follow your track [crosstalk].



LEO:  I guess it wouldn't be hard to figure out.



STEVE:  It would go through your DOS mitigator.



LEO:  All right, Steve.  What a great, great topic.  Maybe I was more engaged because they weren't feeding me and asking me questions and saying, Leo, can you come down the hall and check on this.  So maybe I was more engaged.  But what a good, I think a really good show, really good.



STEVE:  Yeah, well, and I think it's important for us all to note that there is now a new DDoS technology, a way of doing a DDoS that is effective against the latest HTTP/2 protocol, which nearly 40% of the web, 39% of the web is using.



Copyright (c) 2023 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#945

DATE:		October 24, 2023

TITLE:		The Power of Privilege

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-945.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  How do fake drives keep being sold by Amazon?  If you don't already know it, is VBScript worth learning today?  NTLM authentication is 30 years old; will it see 40?  What startling flaw was just found in cURL, and what should you do about it?  Vulnerabilities with a CVSS score of 10.0 are blessedly rare, but today the industry has another.  And also, asked by our listeners, how should "lib" be pronounced?  How is SpinRite's 6.1 pre-release run?  Is Passkey export on the horizon?  Doesn't a server's IP address make encrypting the client hello superfluous?  Is there such a thing as encryption preemption?  Are fraudulent higher-end drives possible?  What's Privacy Badger and why did I just install it?  And finally, within any enterprise, few things are more important than managing user and device access privileges.  As highlighted by the NSA's and CISA's experiences, we're going to examine the need for taking privilege management more seriously than ever during this week's Security Now! Episode #945:  "The Power of Privilege."



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  Coming up, the end of the line for VBScript.  Or is it?  Another massive flaw in cURL.  A CVSS of 10 for Cisco.  And the power of privilege and why it's not a good thing.  It's all coming up next on Security Now!.	



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 945, recorded Tuesday, October 24th, 2023:  The Power of Privilege.



It's time for Security Now!, the show where we cover the latest security news and of course recommendations and explanations from this guy right here, Steven Gibson.  Hello, Steve.



STEVE GIBSON:  Yo, Leo.  It's great to be with you.



LEO:  Let's get ready to secure.



STEVE:  Here we go.  Yes, it is the penultimate episode of October.  I learned several years ago that that does not mean the last one, that means the second to last one because we're going to be on Halloween a week from today.



LEO:  Oh.  Are you going to wear a costume for me?



STEVE:  No, but we do have something to hang in the background.



LEO:  Okay.



STEVE:  Lorrie said, "Oh, you've got to put this in the podcast."



LEO:  I will be wearing a costume.  I just want you to know.



STEVE:  I'm in mine.



LEO:  Don't be too surprised.  I might dress in a black T-shirt as Steve Gibson, maybe put on a moustache.



STEVE:  Hey, if you just put your wizard hat on, I think no one would find fault with that, Leo.



LEO:  Okay, good.



STEVE:  So today we're going to address some questions, as we often do, certainly recently.  We're going to start off with a bit of fun, even after our Picture of the Week, which is how is it that fake drives keep being sold by Amazon?  Thanks to one of our listeners and users of ValiDrive, we actually have some first-hand knowledge of that experience.  Also, if you don't already know it, is VBScript worth learning today?



LEO:  Oh.



STEVE:  NTLM authentication is 30 years old.  Will it see 40?  What startling flaw was just found in cURL, and what should you do about it?  Vulnerabilities with a CVSS score of 10.0 are blessedly rare, but today the industry has another.  And also, asked by our listeners, how should "lib" be pronounced?  Or is it "lib"?  How is SpinRite 6.1's pre-release actually run?  Is Passkey export on the horizon, as it may turn out?  Doesn't a server's IP address make encrypting the ClientHello superfluous?  Is there such a thing as encryption preemption?  Are fraudulent higher end drives also possible?  What's Privacy Badger, and why did I just install it?  And finally, within any enterprise, few things are more important than managing user and device access privileges.  As highlighted by the NSA's and CISA's experiences, we're going to end by examining the need for taking privilege management more seriously than ever during this week's Security Now! Episode 945, "The Power of Privilege."



LEO:  The Power of Privilege.



STEVE:  Don't abuse it.



LEO:  I can't wait.  Good episode, as always, coming up.  This is why you've got to tune in every Tuesday because, I mean, this show has the stuff you need to know.



STEVE:  We got some stuff here, that's right.



LEO:  I need a Picture of the Week, Mr. G.



STEVE:  So looking at this, it occurs to me that sometimes people, like the wisdom of the crowd is what ends up prevailing.  We have a picture where there was, in the foreground, at the bottom of the picture, we see the end of some pavers, like a paved walkway or something.  And there's a big - and then the rest of the picture is a big grassy field.  It's well-kept, lawn-mowed lawn.  Now, clearly the people running this facility didn't intend for the actual users of the facility to wear a dirt path through the lawn to some remote corner where presumably there's a way out.  The whole point being that this demonstrates that people want to go from Point A to Point B, even though that was not the original plan of those who were putting this together.  Or if it was, they didn't appreciate that over time there would be no grass growing along this path that everyone was walking.



So unfortunately, the proper course of action would have been to simply pave an explicit path where the people want to go, rather than putting up this lame, like, it looks like maybe three feet tall by six feet wide barrier at the edge of these pavers, like as if it's going to stop people from going around it because they clearly want to use this path to get to wherever the path goes.  So I gave this picture the caption "Not a well-thought-out plan" because I don't think this is going to turn out the way they're hoping.



LEO:  You know what you're going to see is a path around the fence connecting up to the other path so people can continue to use it.



STEVE:  Yes.  And already on the far side of this you can sort of see some diversion from the path where it looks like maybe people have already started to go around.



LEO:  Yeah.  I love it.



STEVE:  So over time this problem is going to get worse.  You know, there's going to be like multiple ways to merge with the original path after you go around this ridiculous little, like, nothing.



LEO:  It is possible that the people who set this up were geniuses and wanted a three-way path structure going on.  Maybe they...



STEVE:  Ah.  That is - that would - very impressive, Leo. 



LEO:  All right.  All right, Steve.  Yeah, yeah, yeah.



STEVE:  Anyway, another of our observations of the nature of human nature.



LEO:  Human nature.  Gotta love it.



STEVE:  Love it or leave it.  So speaking of human nature, as many of our listeners know, the homepage for my ValiDrive freeware shows a photo of the 12 one and two terabyte drives which I purchased from Amazon last month.  That page also includes links to the Amazon purchase listing for each of those 12 drives.  And with some surprise, many people have wondered how this fraud can exist on Amazon.  So I wanted to share the experience of one of ValiDrive's pre-release testers who took the trouble to purchase a drive, then to post her review of that just purchased drive.  Her name's Leila, and she wrote, and this was posted over in GRC's newsgroup, so I just wanted to share it with our listeners here.



She wrote:  "I posted the following review of a cheap 512GB USB drive, together with a screenshot of the ValiDrive results."  So the review she wrote read:  "This USB stick is a fake.  Although labeled and formatted to appear to have 512GB of storage, it in fact contains only 53GB.  Anything written beyond 53GB appears to write without error, but cannot be read back.  The attached picture shows the result of testing with ValiDrive, a recently published freeware program for testing USB-attached storage.  If you buy a high-capacity USB stick or microSD card for peanuts, don't be surprised if it's a fake.  Check out the prices charged by reputable vendors such as SanDisk and Kingston for reference.  In any event, it makes sense to test all such cards and drives before putting them in your camera, dash cam, et cetera.  Even those which appear to be from reputable manufacturers are sometimes forgeries."



Okay.  So that's what she posted on Amazon.  Not long after, she received Amazon's reply.  They wrote back to her:  "Thank you for submitting a review of doykob" - that's not a name brand, D-O-Y-K-O-B, the lesser known doykob - "Memory Stick 512GB USB Stick Fast Speed USB 3.0 Drive Mini USB Metallic Flash Drive Portable USB Memory Sticks for Data Storage and Transfer, with Keychain."  So that's the name of the product.  Then they continued:  "We're sorry you did not have a positive experience.  We investigated your concerns about product authenticity, and the information we have indicates that the product you received was authentic.  As a result, we removed the review you submitted.  This ensures that customer reviews remain as accurate as possible for the benefit of future customers."



So that answers that question.  Unwitting users who are taken in by this fraud post, you know, "Holy crap!  I just bought this 2TB thumb drive for $20!  I plugged it into my computer to verify its size and it looks great!  This is a super bargain for 2TB so I'm definitely giving it five stars!"  Of course, you know, they haven't tried storing much data on it yet.  They just got it, plugged it in, looks fine, off they go.  So those reviews are immediately posted on Amazon.  Whereas an actual analysis of the drive, oh, you know, we're sorry you didn't have a good experience, but what you said was not the case, so we're not going to let anybody see it.  We're just going to with all of the happy customers.



So anyway, that's the way this happens.  And the other people have said, well, why not complain about a specific drive?  Well, first of all, who are you going to complain to?  Amazon won't list it because they just say, no, look at all the other people who are happy.  There's something wrong with your computer.  Okay.  Anyway.



Microsoft's VBScript, which, you know, was the proprietary scripting language based upon Visual Basic, is officially headed for the dustbin of history, and not a moment too soon.



LEO:  Aw.



STEVE:  I know, Leo.  I know.  Believe it or not, VBScript has been with us since 1998, so now for 25 years.  It was originally Microsoft's attempt to combat JavaScript.  You know, they didn't want somebody else's scripting language to prevail.  So it's like, wait a minute, we've got one, too.  And it's like, okay.  And, you know, it kind of - it lived alongside JavaScript and the early .NET stuff and so forth for a while.



But since PowerShell is far more comprehensive than VBScript, and since Microsoft had essentially already abandoned VBScript - its last significant release was VBScript v5.8, 13 years ago in 2010 - so the only ones who are likely to mourn its passing are malware authors, among whom it has remained a popular tool.  Even though, even there, not as popular as it once was.



Now, that said, VBScript still has a large fan base among system administrators who have established a code base of their own, you know, it's their stuff that they've written, is working, and it's been proven.  The good news for those who do still need it is that it's not being, at least immediately, completely removed from Windows.  Instead what's happening is that it's being moved from the "you can assume it's always there by default" into the so-called FoD, that's Microsoft's term, "Feature on Demand," also known as optional features.



LEO:  FoD.



STEVE:  The FoD, yes.  We're FoDing it.



LEO:  Yeah.



STEVE:  Where those - so, you know, in those comparatively few environments that do still depend upon it, you'll be able to enable it and install it on demand.  But it won't just always be there ready to go.  They have said that they definitely plan to remove it entirely at some point.  So anyone who does still today depend upon VBScript would be advised to migrate their code over to PowerShell.  That's the future.  VBScript has no future.



And as for malware, its removal, VBScript's removal is expected to have some impact on malware in the sense that another piece of lowest hanging fruit, which has been VBScript, will at long last finally have been pruned from Windows.  But it's not as if malware will be hugely inconvenienced.  Just as when Microsoft dropped support for macros in Office applications, VBScript's new FoD status will trigger a more concerted move to PowerShell-based malware exploitation.  The larger gangs have already switched over to PowerShell because there's a lot more you can do with it, and unfortunately a lot more mischief that you can get up to.  But the older groups who are more into the copy-and-paste mentality, they've just continued using VBScript because it was there.  It's like, why not?  So those guys will need to up their game once VBScript stops being available on all Windows unless it's installed by demand.



So anyway, anyone who is listening to this podcast would be well advised to plan today for the eventual disappearance of VBScript.  And knowing Microsoft, it will likely survive over in the Optional Features category for quite some time yet.  I'm not  sure, you know, why they would ever remove it unless at some point Windows evolved to make it incompatible, in which case then they'd probably say, oh, well, we had to get rid of it because it no longer ran on Windows, you know, 14.  But still, I wouldn't be counting on it in the long term.



Speaking of things being discontinued, Microsoft has also announced their intention to eventually disable support for the ancient and never really secure NT, as in New Technology, LAN Man, you know, LAN Manager Authentication Protocol, which will occur, that is, the eventual disablement in a future version of Windows 11.  And of course labeling anything NT, standing for New Technology, after 30 years in Internet time is really a stretch anyway.



So NTLM Authentication Protocol was first introduced in '93, so yup, 30 years ago.  And that was with the release of Windows NT 3.1.  It did serve as the primary user authentication protocol for Windows Networks until Windows 2000, which first introduced the Kerberos authentication system originally developed at MIT.  Ever since then, NTLM has still been there in Windows as a backup authentication protocol for legacy purposes, and in actually the still many situations where Kerberos could not be used. 



The problem, as we well know, with keeping legacy protocols around is that the "weakest link" principle applies, especially when it's coupled with the "Well, let's just leave it enabled in case someone might need it" philosophy of network administration.  The problem is, of course, the bad guys might be the ones who need it.  And if it's enabled, they're happy to take advantage of it.



But superior as Kerberos is, even today it's not a slam-dunk drop-in replacement for NTLM.  Kerberos has known problems where NTLM works, in offline and local use or in segmented network topologies where there's no direct line of sight to a Domain Controller because Kerberos is a three-party authentication system, not a two-party system.  So you need access to that third party.  Microsoft has indicated that it's well aware of these issues - and let's hope it is - and that it's going to fix, it's going to be working on Kerberos's shortcomings before completely removing NTLM.  But they have signaled that NTLM is eventually going to be retired.



Microsoft says it's working on two new Kerberos features, namely IAKerb and local KDC that will allow Kerberos to work in more diverse network topologies and in offline and local scenarios, as I said, where there's no DC, no Domain Controller around.  And the good news is all of this will only affect Windows 11, not Windows 10, because it's probably still going to take a while to get there.  And believe it or not, Windows 10 now has fewer than two years of support left.



Windows 10's support is slated to end on October 14th.  We just last week passed the two-year cutoff, October 14th of 2025.  So that's 103 weeks from now.  And that will be coinciding with Security Now! podcast 1046.



LEO:  Which will exist, thanks to you.



STEVE:  That's exactly right, Leo.  It's a good thing that we're going to keep on going past 999 since I would not want to miss out on the opportunity to complain a lot about the premature ending of Windows 10 support.  Which by then I'll just be starting to use.



So in any event - actually, I'm already using Windows 10.  I'm fine with it.  To help enterprises to wind down their dependence upon NTLM, and here's the takeaway for our listeners who are in the enterprise profile, Microsoft will be adding to Windows 11 new tools and logging capabilities to help monitor NTLM usage along with providing finer-grained group policies to disable and block NTLM so that enterprises can get ready for its eventual disappearance.  And that also will be providing metrics back to Microsoft, as they see that NTLM usage is diminishing.  They've said they fully intend to announce a hard date when the protocol will be disabled in Windows 11 and then finally pull the plug.



Quoting Microsoft, they said:  "Reducing the use of NTLM will ultimately culminate in it being disabled in Windows 11.  We're taking a data-driven approach and monitoring reductions in NTLM usage to determine when it will be safe to disable."  So I suppose the other strategy would be keep using the crap out of it, and Microsoft will go, whoa, I guess it's not safe to disable it, and maybe everyone will keep having access to it.  The problem is NTLM has been a real problem.  I mean, it has been a source of many conversations during the 18-plus years of this podcast when it has had serious vulnerabilities.  So it absolutely would be good to go to an updated modern protocol.  I mean, legacy protocols are not those that you want to keep around, especially when they're in crucial positions like network authentication.



Vulnerabilities in the massively widely used cURL command and library are always a concern since the library is so often included inside other projects because the features and services that cURL offer are incredibly handy.  So it is significant when Daniel Stenberg, who is the cURL project's lead developer, calls one of two just-discovered vulnerabilities "probably the worst cURL security flaw in a long time."



LEO:  And they say it a lot, by the way.



STEVE:  Oh, boy.  Yes, it is.  The two flaws, which were found by JFrog, impact libcurl versions 7.69.0, which is where they first appeared, through 8.3.  Daniel wrote:  "We're cutting the release cycle short and will release cURL 8.4 on October 11th, including fixes for a severity HIGH CVE and one severity LOW.  The one rated HIGH is probably the worst cURL security flaw in a long time.  The new version and details about the two CVEs will be published around 06:00 UTC on the release day."  So there was CVE-2023-38545, that was the severity HIGH which affects both libcurl and the cURL tool, which is built from the lib.  And also 38546, which has LOW severity and affects libcurl only, not also the tool.



He said:  "There is no API nor ABI change in the upcoming cURL release."  He said:  "I cannot disclose any information about which version range is affected, as that would help identify the problem area with a very high accuracy, so I cannot do that ahead of time."  He said:  "The 'last several years' of cURL versions are as specific as I can get."



In other words, the last several years of cURL have had these problems, one which he characterizes as the worst cURL security flaw in a long time, meaning that, and this is the significant part, anything built with those libraries.  Anything that incorporated any cURL within the last few years is also potentially at risk, and thus the severity of this overall.  He said:  "We've notified the distros mailing list, allowing the member distributions to prepare patches.  Now you know.  Plan accordingly."



So the worst of the two problems sounds quite bad and is easy to exploit on its face, but fortunately its required preconditions will prevent the world as we know it from ending.  The cURL maintainers wrote in their follow-up advisory, they said:  "The flaw is a heap-based buffer overflow in the SOCKS5 proxy handshake.  When cURL is asked to pass along the hostname to the SOCKS5 proxy to allow that to resolve the address instead of it getting done by cURL itself, the maximum length the hostname can be is 255 bytes.  If the hostname is detected to be longer than 255 bytes, cURL switches to local name resolution and instead passes the resolved address to the proxy.  Due to this bug, the local variable that means 'let the host resolve the name' could get the wrong value during a slow SOCKS5 handshake and, contrary to the intention, copy the too-long hostname to the target buffer instead of copying just the resolved address there."



Okay.  So we have a timing-based race condition that could lead to remote code execution via a buffer overrun.  The cURL devs said that the overflow could be triggered by a malicious HTTPS server performing a redirect to a specially crafted URL.



The JFrog guys who found the problem said:  "Seeing that cURL is a ubiquitous project, it can be assumed with high confidence that this vulnerability will get exploited in the wild for remote code execution, with more sophisticated exploits being developed.  However, the set of pre-conditions needed for a machine to be vulnerable is more restrictive than initially believed."



So this feels exactly like another one of those vulnerabilities that, while it doesn't explode the Internet, will be latent.  It will be entered into those massive exploit databases I've theorized about in the past.  Something like that must be maintained by our NSA and, unfortunately, by all other sufficiently sophisticated malign actors on the global stage.  There's likely a huge dependency tree that knows which version ranges of every utility and library that uses cURL, was built using any of those vulnerable cURL libraries during the years that cURL was now known to have been vulnerable.



So whether it's next year or 10 years from now, when we or some hostile actor, some hostile foreign power wishes to penetrate a network, that database might serve to remind such an actor, you know, like for example, "Hey, you know, the set of circumstances you're facing just happen to perfectly fit the use of a long ago patched, but still present in the target system, flaw that we could take advantage of today."



You know, as I've often observed, given the Swiss cheese that is our industry's legacy of interdependent flawed libraries which continue to persist because they're embedded in other things that are still being used, and in this environment of slow-moving updates there's just no way that such a capability does not exist.  And it's a little creepy because that's a lot of information.  So you can just imagine that would be a perfect place to apply AI.  Some AI has ingested all of the past history of vulnerabilities and interdependencies so that you aim it at a given scenario where you want to penetrate, and it looks and finds a way in based on the current versions of the things that are detectable and knowing everything about still vulnerable interactions among them.  So we may be entering a whole next generation of ways of penetrating networks.



Sometimes you don't have to work very hard, Leo, to penetrate a network.



LEO:  Sometimes the network penetrates you.



STEVE:  Sometimes it just says, "C'mon, baby, what are you waiting for?"  



Cisco has recently been dealing with a problem that is anything but latent.  As we've often observed, really, really bad vulnerabilities, I mean, really, really bad ones are typically awarded a CVSS score of 9.8.  And that's generally as high as it goes.  And even that, even a 9.8 is quite difficult to earn.  And I respect the reticence to go any higher, since we would like to reserve any assignment of 9.9 or 10.0 for those really just too horrible to believe problems.



LEO:  The Nadia Comaneci of exploits.



STEVE:  Indeed.  So this means that Cisco's recent award of a CVSS of 10.0 really does mean something.  What does it mean?  It means that almost overnight, more than 42,000 of Cisco's high-end publicly-exposed Internet routers and switches were compromised and taken over.  Like I said, that's what you want to reserve those 10.0s for.



The first known instances of attacks against Cisco's IOS - and you know that's not Apple iOS.  For Cisco, IOS is Internet Operating System.  So Cisco's IOS XE-based routers and switches, which appear to have been, that is, the first instances of attacks appear to have been initial proof-of-concept probing incursions, occurred at the end of last month on September 28th.  Then more than three weeks passed before Cisco finally released the first fixes last Monday on October 16th, so eight days ago.



During those intervening three weeks, more than 42,000 - 42,000, not like a hundred critical on-the-edge Internet-connected switches and routers, 42,000 - of these IOS XE-based devices are known to be compromised.  And again, not were scanned by Shodan and found to be vulnerable.  No.  Were known to be compromised because the malware was detected in them.  And as I said, compromise means full and effectively devastating takeover of the device.  We know that it was 42,000 devices because scanners were quickly created by security firms who wanted to track incursions.  And in response to the visibility of their initial implants, the perpetrators of these attacks then updated their malware to make it less visible.



To all of this, the tech press responded as we'd expect.  Just yesterday, SC Media lead with "What the Cisco IOS XE zero-day reveals about potential risk management blind spots."  Help Net Security wrote:  "Disappearing implants, followed by first fixes for exploited Cisco IOS XE zero-day."  CRN wrote:  "Cisco Releases First in Series of Patches for IOS XE Vulnerabilities."  A day earlier on Sunday, BleepingComputer's founder, Lawrence Abrams, wrote:  "Hackers update Cisco IOS XE backdoor to hide infected devices."  Dark Reading wrote:  "Cyberattackers Alter Implant on 30,000" - and they meant 42,000, but okay - "Compromised Cisco IOS XE Devices."  And The Register did not disappoint with their headline:  "Cisco fixes critical IOS XE bug, but malware crew way ahead of them.  Initial fall in infected devices indicates evolution, not extinction, of attacking code."



So in today's show notes I have a chart which shows the initial infection rise, which looks like it took about two days from the 16th of October to the 18th of October, two days.  And we hit 42,000 infected devices.  Looks like the majority were in Asia.  They just, they barely outflanked North America, which was at number two, then came Europe, then Africa, then Oceania, and then others.  So it peaked two days after, on October 18th.  And then just around the 20th, we see it dropping, like right back down to zero.



We already know that there's no way 42,000 devices randomly scattered out on the Internet got themselves patched in like a day.  That was the malware deciding, whoops, we made ourselves too easy to spot.  We're going to go undercover.  In other words, what we now have is a very real first-class CVSS of 10.0 mess.  As I mentioned, the news of this broke publicly for the first time on the 18th.  And that chart in the show notes shows that this corresponded with the peak of the detectable infections.  



The problem that Cisco revealed was a zero-day, now known as CVE-2023-20198, which exists in the web - I can't even believe I'm saying this - the web admin panel of its IOS XE operating system.  I can't count the number of times I've said on this podcast that all of our experience teaches us that it is absolutely never safe to expose any web-based administration to the public Internet.  As an industry collectively, we've proven that we simply do not yet know how to do that safely.  We keep thinking we do, but all of our experience says no, you don't.



If a network's management environment absolutely requires remote management access to a publicly addressable router or switch, then use a VPN to connect to the private network behind the device, then access its web interface from the inside.  Never expose any management-level web interface to the public Internet.  There's just no reason to.  "Ease of access" obviously also applies to the bad guys.  You know, you're making their access easy.  That's not what you want.



And really, in this instance, Cisco is at fault here in the year 2023.  Not because they made a mistake.  As we know, that can happen to anyone.  But because it's even possible in this day and age to configure their devices to expose a web admin interface to any publicly routable network.  It should not be possible.  This is all enterprise-grade high-end equipment.  Anyone using it should have VPN technology falling out of their pockets.



So only exposing web admin to private networks would never impose a burden on their administration.  And if Cisco had implemented such a policy, 42,000-plus recently compromised and now much more difficult to find routers and switches would not be compromised, and Cisco could then safely make all the mistakes they might want with their web admin authentication without it ever creating what essentially is a global catastrophe.



And also note that this does not only apply to the enterprise, that is, this as strongly worded as I could possibly make it advice, does not only apply to the enterprise or to enterprise-class devices.  It applies right now, today, to every one of us.  Remember it was an exactly analogous flaw in a Plex media server web admin interface, which was located in the home of a LastPass developer, that was responsible for all of us having our LastPass vaults stolen and now being targeted for selective decryption.



Unlike the Cisco case, it's probably asking too much for Plex to disallow Internet-facing administration.  But that individual, that LastPass developer who was deep into the security space as a developer at LastPass, should have done so him or herself.  And everyone listening to this podcast should consider that, too.  VPNs and overlay networks are now so freely available that it's only a matter of taking the time to set them up.  Okay.  Enough preaching and ranting.



In today's case with Cisco, the zero-day allowed threat actors to create an admin account with the highest level of privilege - in Cisco land it's Level 15 - on devices that had their WebUI panel exposed to the Internet.  After some additional investigation, the presence of a second zero-day was also discovered being used by the attackers to inject commands into the IOS XE filesystem that would execute with root privilege.  Well, that's never good.  Cisco revealed that the exploit chain was being used to inject a Lua-based backdoor on devices across the world.



Cisco initially believed that the second zero-day was the exploitation of a previously fixed bug that was from 2021, CVE-2021-1435.  But in an update late last Friday, Cisco said the attackers had discovered and were using a new zero-day, now assigned CVE-2023-20273.  But believe it or not, both Cisco and CISA confirmed that the two-year-old patched flaw was also being exploited in the wild by a different threat actor in a different campaign.  In other words, there are still machines out there where flaws are being exploited, a zero-day flaw from two years ago, further demonstrating that those 43,000 known to be penetrated routers and switches didn't just get miraculously patched in a day.  We've still got them out there that haven't been patched for the last two years.



So like I said, there's never a good reason to expose all of this management, web interface management UI to the Internet.  Just turn it off.  Don't do it.  Manage from behind and use an overlay network or a little VPN server, trivial to set up these days, if you need remote access.



The other thing that was interesting was that the Lua backdoor that was originally placed on all 42,000-plus hacked Cisco IOS XE devices since late September started disappearing over this last weekend.  Estimates from both Censys and Shadowserver, who were scanning the Internet doing all of this, put the number of hacked devices at the number I've been quoting, 42,000-plus IOS XE routers and switches.  And over the course of a couple days, as we know, that number of exposed known compromised devices dropped down to between 500 and a thousand.



So who knows why it didn't go to zero.  There are apparently more than one actor at work here.  And consequently it's believed that this change was made by the threat actor itself.  The Lua backdoor was not originally a particularly strong method of persistence since it could be removed with a device reboot.  So it was only RAM resident at that point.  And its public visibility was attracting too much attention to the attackers' operations.



Several security researchers pointed out that the Lua backdoor would likely have been replaced by something much more insidious, very similar to the way Chinese hackers evaded Barracuda's patches earlier this year to entrench themselves so deeply into the compromised appliances that Barracuda wound up telling their own customers that they probably should just replace the gear to be safe.  There was no other way to really know for sure.



LEO:  Wow.  Wow.  Geez.  Wow.



STEVE:  Yeah, it's really, really bad.  So at this point we don't know that anything like that has happened.  But when vulnerabilities have been discovered which we know allows for the injection of commands into the device's file system with root privileges, given sufficient time - and these attackers had three weeks - it's clear that anything could have been done.



So today, the Internet might well currently have something on the order of 42,000 compromised Cisco routers and switches where their compromises have gone stealth after some threat actor somewhere, we don't know who, finished burrowing in.  And at this point, you know, we'll never know exactly which routers and switches are affected.  As we know, once a device or a network has been compromised, can we ever fully trust it again?  Today we have patches for both the known zero-days that have made this possible.  But applying the patches of course is only the first step.  Even if some of the horses have left the barn, it's still worth closing the door.  But you may still have some loose horses.  Any enterprise whose Cisco edge gear may have been compromised would be well served to, unfortunately, perform a full and deep security audit into what may have happened on that device over the last month.



At this point, no one in the infosec community has made even a tentative attribution.  No one is suggesting who the bad guys are.  There's no malicious actor who would not be interested in penetrating Cisco's IOS XE gear, from hacktivists to advanced persistent threat operators, from initial access brokers, you know, IABs, to botnet operators.  Could be anybody.



And we do need to give Cisco a little bit of credit where it's due.  It was they who discovered this new pair of zero-days during their investigation of a customer's support ticket.  Another vendor might not have bothered to engage a competent security team to really dig into the trouble's root cause.  Cisco did.  What they found was certainly troubling.  And I would argue it should not have happened in the first place because you just should not be able to put a management, a web management UI out on the public interface.



Anyway, the takeaway for each of us individually should be that our own networks should never expose anything like that publicly.  You should never have your own router's web management interface enabled on the Internet, regardless of how well chosen your username and password may be.  And the same goes for Synology, Plex, and everything else.  As I said, just take the time to set up a simple overlay network or your own VPN server to allow you to access those devices from the inside.



And I'll just note that this made me think of this.  One of the things I have long wanted to do was to expand GRC's ShieldsUP! service-port-only scan to check all 65,536 ports for its users.  Back when I first wrote all of that code, I was running GRC's server from here at home over a pair of 1.54 megabit T1 connections.  I was unable to offer to scan 64 times more ports in a reasonable amount of time.  Today I could do that easily from Level 3.  So, you know, on the back of my list, once I get to a place where I can turn my attention back to ShieldsUP! - which won't be until after SpinRite 7 exists and has been released - I think it would be fun to boost ShieldsUP! so that it's able to scan all of an individual's ports.



LEO:  Oh, good idea.



STEVE:  Because, you know, who knows what it would show you that you might not even be aware of has opened incoming traffic through your router.  We need to find out.



So our listener Stephen Lee, he said:  "Love your show.  I was wondering:  If you pronounce 'lib' for library as rhyming with 'vibe,' do you pronounce 'var' for variable as rhyming with 'air'?"



LEO:  I say "lib" and "var."  I mean, that's just me.



STEVE:  Yeah.  And I'm sure he's asking because everyone's heard me say "libe."



LEO:  You say "libe," yeah.



STEVE:  I do say "libe" as in library.  But I do say "var" for variable, as opposed to "vair."  But, you know...



LEO:  I don't think you can tie the pronunciation of the full word to the pronunciation of the shortened version.



STEVE:  I agree.  Yeah.



LEO:  So what's "lib" to you is "libe" to me.  And "var," you know, I should say "vair."  If I say "libe," I should say "vair," but I don't.  I say "var."



STEVE:  Yeah, yeah.



LEO:  So that's a fork.  How do you say that?



STEVE:  Tom Frillman, he said:  "Hi, Steve.  I got the SR 6.1 EXE, but Win10 won't run it.  How may I use it?  Thanks for all you do.  By the way, ValiDrive is great.  I must be lucky.  All 25 of my drives are fine except for one that won't even format.  Maybe SpinRite will help."



Okay.  So a bunch of people were confused by, and I apologize for this, for my mentioning that the pre-release was available because what's available is very different from what the full release will be.  The SpinRite 6 that everyone is used to is a hybrid Windows and DOS EXE.  It's kind of a cool trick that I came up with where that one EXE is both a signed Windows app and it's a DOS app.  Because what Windows did when Windows first began to happen was, you know, there was still a lot of DOS around.  So people would be running a Windows app in DOS.  And so Microsoft thought, okay, well, we have to have some way to fix that.



So what they did was they defined the EXE so that there's a - it's actually a DOS executable on the front, a little stub.  And all this, the little stub which is a DOS executable, says this program must be run in Windows.  And then when you run the same executable in Windows, Windows is smart enough to know, oh, well, we don't - we just ignore this little stub, and we jump over it to go to the Windows app.



So what I did was I take the SpinRite DOS app and stick it on the front as the stub so that, when you run the SpinRite Windows app in DOS, instead of saying, whoops, this is a Windows app that you need Windows for, it runs SpinRite.  Like in DOS.  And when you run the same thing in Windows, you get the setup utility which allows you to create the USB drive, or format a floppy and install SpinRite on a floppy, or burn an ISO to CD-ROM and so forth.  So anyway, that's all sort of hidden for most users.  So unfortunately I didn't make all of that clear last week.  The pre-release that I invited our listeners to download is a DOS app, which means that you need to stick it on some DOS boot drive.



Anyway, I spent the weekend updating GRC's forums, and I created an FAQ entry that has very clear instructions for how to do this.  So if anybody else was confused, first of all, the people in the forums would be glad to help you, you know, forums.grc.com.  And you can also go there and find much cleaner instructions.  And I actually have them also in the show notes.  I just sort of skipped over them by giving a more longer winded explanation.  But that's the back story for why it wasn't as easy as just running the thing you downloaded from Windows.  Windows would say, what?  You know, like this is a DOS app.  That's not what you're here for.  And there's, you know, there's no Windows app behind it as there is in actual SpinRite.



Doug Smith, a listener, said:  "Regarding exporting and importing Passkeys, I saw this from a 1Password employee on their forums:  'At this point in time'" - this is the 1Password employee speaking.  "'At this point in time, you cannot import or export Passkeys from any managers like iCloud Keychain or 1Password.  The good news is that we're working closely with platform vendors and other password managers through the FIDO Alliance to create a secure way to import and export Passkeys.  We believe it's your choice where to store and use your Passkeys.  Hopefully we'll have more to share soon.'"



So that is wonderful news.  That's exactly what we need.  The user's experience in my opinion is paramount.  And what users would do would be to provide their own strong password, under which they would be exporting one or more Passkeys.  That would be, you know, that strong password would serve to keep their Passkey safe while it's outside of whatever environment, password manager, whatever.  Then they'd need to again provide it when they wish to import that into something else.



So that's the way I designed things with SQRL, and the technology to do this safely and unspoofably and verifiably is widely available.  That's just not difficult.  It's just a matter of establishing it as a standard that everybody can use.  So it is very good news that the FIDO Alliance recognizes that this is something that has been missing and that will go a long way toward making Passkeys a lot more, you know, more transparent, I think, and more useful for most of our users.



LEO:  This is something we talked about in the original piece you did on Passkeys, which is that they had never put anything in the spec about portability.



STEVE:  Right.



LEO:  And I think the reason they didn't is because Passkeys relies on a secure enclave like the hardware enclave in your iPhone or your Google phone to store those Passkeys.  And I think they're worried that it's inherently insecure to let them out of the enclave.  So they've got some work to do.  I'm not convinced the FIDO Alliance will agree, will go along with it.  This is just...



STEVE:  It'll be interesting to see if that's going to happen.



LEO:  The fact that they left it out tells you surely they knew this was an issue.  But they explicitly, well, I guess it's implicitly left it out; right?



STEVE:  Right.  Well, and we've also suggested maybe that it was just an attempt by the initial providers of Passkeys to create walled gardens within their own environments.  Maybe they've already seen that, whoops, you know, the number one thing people are asking for is I want to be able to export these.  I want to be able to import them from somewhere else.  I don't like not having that ability.  So they may have said, okay, well, we tried not to do that.  But users are saying they're not going to use Passkeys unless and until they're able to have them be portable.



LEO:  I agree.  It's kind of a nonstarter; right?  I mean, I have to be thinking is my Passkeys all live in this phone.  What happens when I get a new phone?  Well, actually it does move over to the new phone because mine did.  But if I've got an Android phone and I have an iPhone, it doesn't.



STEVE:  Unh-unh.  Nope.  Nope.  And you could imagine that a password manager like 1Password or Bitwarden, they would...



LEO:  They'd love to be the host; right?



STEVE: They would want to, yes, yes.  They would like to be able to come along later and say, well, you may have started with all your Passkeys in your iPhone, but now you can put them here.  And because we're cross-platform, you'll have cross-platform access.



LEO:  The problem in general with Passkeys is that there's always the password fallback.  And people are going to always fall back to passwords.  At which point...



STEVE:  That is exactly right, Leo.



LEO:  They say why am I using Passkeys?



STEVE:  That is exactly right.  Show me a website that does not say "I forgot my password."



LEO:  Right.  Right.  Right.  They can't.  They're always going to do that.



STEVE:  Right.  Exactly.



LEO:  Same reason [crosstalk]...



STEVE:  There'll always be phishing scams and, you know, yup, yup.  Victor,  he said:  "I powered on a couple of years old desktop that had been unpowered for about a year.  It took ages before the desktop was loaded, no errors anywhere, but I decided to try your ReadSpeed.  And look at those SSD speeds.  Is it time to invest in SpinRite now?  If SpinRite fixes this, I will try to encourage my employer to get a site license.  Thank you, Mr. Gibson.  Victor, long-time SN listener, keep up the good work up to 999 and beyond."



So you have it on the screen, Leo.  And this is what we discovered, to our surprise.  Actually, nothing quite this horrible, but not good.  So what you have on the screen and what I have in the show notes is this system that has three drives.  There are two 4TB Western Digital spinners.  And the primary drive, the boot drive, is a 512GB Western Digital SATA SSD.  Well, we can see that its optimal performance, because remember that what ReadSpeed does, and this is what opened our eyes so much, is it does separate identical benchmarks at five locations:  the beginning, the end, the middle, and then the 25% and the 75% points.



This SSD speed is maximum at the 25% point, where it's getting 482.5MB per second.  It does not perform well elsewhere.  But it performs the worst at the front of the drive, where believe it or not ReadSpeed measured it at 2.2MB per second, down from 482.  So, and of course at the beginning of the drive is where the OS lives.  And as we know, what we've learned is that the phenomenon of read disturb is that since those files are being read over and over and over, the successive reading without ever rewriting causes the bits to become fuzzy.  And it means that the SSD takes much longer to read them and to correct them properly.



To answer Victor's question, yes.  Running SpinRite right now on that drive would fix it.  And the slight downside, although I wouldn't give it another thought in this case, is that rewriting the entire drive, which is what SpinRite does, is less desirable than exactly locating the spots on the drive which are slow and targeting them for selective rewrite.  That will be the feature that SpinRite 7 brings, along with what I think is going to be an extremely interesting map of the read performance across the entire drive surface.



So that's why I'm so excited about there being a 7, why there will be a 7, is I can't wait to develop the technology to do that based on what we've seen, which is what  ReadSpeed showed us.  And, you know, for these spinning drives we see exactly what we would expect.  The beginning of the drives are fast, 182MB per second.  Then 169, 151, 122, and 83.  Where toward the end of the drive, because the circumference of the inner tracks is shorter, you've got a lower data rate there.



So anyway, Victor, yes, I would love to hear what happens after you run one of SpinRite's read/write passes.  You need to refresh the data.  It'll be a little slow going there at the beginning, just as it was slow to get your system booted.  But afterwards a before-and-after ReadSpeed, and SpinRite 6.1, you can certainly use 6 or 6.1 because it's half a gig so even SpinRite 6 will deal with it pretty quickly.  Either one will work, but 6.1 offers before-and-after benchmarks, specifically for this reason, and they're built into the system.  So anyway, I'm excited about the future.



Andy Suarez said:  "Hi, Steve.  Great to hear you cover ECH."  That was Encrypted ClientHello from a couple episodes back.  He said:  "The whole time, though, I kept thinking that something was missing from your coverage, and that was that people should not be thinking that this is providing them cloaked Internet access at all.  Knowing what IPs are associated with what websites and services is definitely a thing.  For anything other than the smallest websites, there is absolutely a known correlation between IP addresses and the site behind them.  And that is not in any way being obfuscated here.  Services know the IPs for porn sites and political sites and messaging services, et cetera.  And even," he gives an example, http://www.grc.com, as 4.79.142.202.



He said:  "They may not be able to see the DNS query with encrypted DNS, or the header request with ECH now.  But they are certainly still seeing the IP of the web server that you are sending and receiving packets from; right?  So won't tracking just change to keep fresh lists of sites and what their associated IPs are?"



And Andy is right, of course.  IPs are still IPs.  But the more sites move behind large aggregators like Cloudflare, and thus share IPs, the less clear the user's destination becomes.  But as he says, those are more likely to be smaller sites.  Big sites typically have their own block of IPs themselves.  You know, but it's also unclear how this may change as the popularity of IPv6 grows.  At the moment, the lack of IPv4 address space is forcing some of the sharing of IPv4 address space because it's so limited.  But IPv6 promises to at least offer the opportunity of demultiplexing websites once again.



So I agree that Andy's reminder is a very good one.  Not only was the Internet not initially designed to provide the security and privacy that we increasingly need, it was also never designed to hide where we go.  That just wasn't on anyone's radar back then.  Tor's concept of so-called "onion routing" is the best solution we've come up with for obscuring our traffic explicitly.  A trusted VPN is another good solution, at least for keeping your ISP and anyone snooping along the way from seeing what you're doing.  So we would need no one to be able to know what domain IPs and ECH public keys we look up through DNS.  Then Encrypted ClientHello to hide the domain within our traffic.  And then something like Tor's routing or a trusted VPN provider to hide our traffic's destination.  Any one of those three missing, and it's still possible to know what we're doing on the Internet.



Someone using the handle "New EOL," he asked a question that sort of took me a bit by surprise.  He said:  "Will ransomware be able to encrypt an AES-256 encrypted 7-zip archive?"



LEO:  Well, duh.



STEVE:  And as I said, that one caught me a bit by surprise.  You know, it's sort of an interesting thought.  This listener is wondering whether encrypting something before ransomware comes along to encrypt it itself could essentially get there first to prevent any further encryption.  Unfortunately, it's definitely possible to encrypt something that's already been encrypted.  And in fact, speaking of Tor, that's something that Tor, this onion routing, relies upon.  With Tor, a destination URL is encrypted using the public key of the last router in the chain.  Then that one-time-encrypted URL is again encrypted using the second from the last router's public key.  Then that twice-encrypted URL is again encrypted using the third from the last router's public key, and so on.  Then that multiply-encrypted blob is sent to the first router.  It decrypts the outer layer of the onion to obtain the identity of the next router in the chain.



But since the destination URL is still deeply buried under multiple rounds of encryption, it doesn't know where you're ultimately going to go.  It can only forward it to the next router that decrypts the current now outer shell of the onion which then forwards it to the next one.  So anyway, multiple encryptions definitely do work, and their corresponding multiple rounds of decryption, and they do even have powerful uses in the case, for example, of onion routing.  So, yeah, unfortunately, encrypting something before ransomware encrypts it won't keep it from encrypting it again.



Daniel Hodgin, he said:  "Hi, Steve.  Thank you for the new ValiDrive product, and thank you for everything you guys do with Security Now!.  I decided to try it out this morning with a few USB keys and SD cards around our office.  I then realized I wonder if this can work with SSDs as well with a USB-connected cloning station.  Sure enough, I was able to test a 1TB Silicon Power brand SSD from Amazon which turned out to be exactly as advertised.  It made me wonder if the same kind of scam can be done with hard drives, SSDs, M.2 SSDs, et cetera, which we are placing in corporate devices and assume our corporate data is being stored just fine once Windows reports the size is as expected."



Well, it's funny he should ask.  I actually, I was curious about a high-end device that was available.  It was advertised as a USB-C style connectable NVME, so a high-end device.  And you can see, I'm holding it up to the camera.  It has that physical profile.  It looks, you know, it's got a beautiful metal case, you know, beveled edges that are chrome.  Well, I opened it up.  And what do you think I found, Leo?  I found, and I think you can see it...



LEO:  Oh, my god.



STEVE:  ...here, and I also have it in the show notes.  It's literally it's your favorite hot-glued SD card sitting in there.  Somebody took the trouble of creating a chip to interface a USB-C connector to an SD card to create an incredibly cheap drive.  It does not have nearly the advertised memory.  So not only is it not an NVME, it isn't even offering the amount of storage available.  So, yes, it is certainly the case that there are - that the Internet is sadly full of scams.  Wow.



LEO:  You've really become the king of this.  You've got a whole collection now.  Wow.  I'm so impressed.



STEVE:  Well, I wanted to make sure that my software would work.  And as I kept getting these things, I mean, even really nice-looking drives like with beautiful packaging and user manuals and stuff, it's like, wow, okay.  Just amazing.



So two last feedback-inspired things I wanted to share.  Marcus Hitchins, not Hutchins, Hitchins, he said:  "Privacy Badger un-rewrites Google's rewritten links in search results."  Well, that caught my attention.  Everyone knows how annoyed I am that Google's link-hacking, where the link they show you in the search results is deliberately not the link you get when you click it.  They are tracking my choices by bouncing my browser through their servers.  The one beneficial reason I can see for doing this would be to use human feedback to help tune their search results.  But we're all certain that they're also profiling everyone individually for their own profile building.  They make all this noise about their privacy sandbox and how they want to do away with third-party tracking.  But what about tracking every link I click on in the search results?



So Privacy Badger is a creation of the EFF.  They are people whose intentions we know we can trust.  But I've often thought that it was an unfortunate name.  You know, Privacy Badger?  Really?  It's like...



LEO:  I think it's a play on Honey Badger.  But okay, fine.



STEVE:  Okay.  It fell flat for me.  Nevertheless, the idea that I could have something I trust stripping Google's URL link mangling out of Google's search engine results was too good to pass up.  So I added it to Firefox and immediately confirmed that, sure enough, just as Marcus said, the links presented to me in Google's search results are exactly now as they appear visually.  No more subterfuge.



LEO:  Ah.  Interesting.



STEVE:  Yes.  It is very nice.  So I want to take this opportunity to pause for a moment and touch on Privacy Badger a bit more.  First of all, you can find it at PrivacyBadger.org, and it's P-R-I-V-A-C-Y-B-A-D-G-E-R dot org.  And it is also GRC's carefully numbered shortcut of the week, so grc.sc/945.  And that'll just bounce you over to PrivacyBadger.org.  And I'm glad that I'm taking a longer look at it since what it does sounds really cool and interesting and unique.



So in describing it, the EFF writes, they said:  "Privacy Badger is a browser extension that stops advertisers and other third-party trackers from secretly tracking where you go and what pages you look at on the web.  If an advertiser seems to be tracking you across multiple websites without your permission" - and it ends up that number is three websites without your permission, we'll get there in a second - "Privacy Badger automatically blocks that advertiser from loading any more content in your browser.  To the advertiser, it's like you suddenly disappeared.



"Privacy Badger was born out of our desire to be able to recommend" - you know, the EFF's desire - "to be able to recommend a single extension that would automatically analyze and block any tracker or ad that violated the principle of user consent; which could function well without any settings, knowledge, or configuration by the user; which is produced by an organization that is unambiguously working for its users rather than for advertisers; and which uses algorithmic methods to decide what is and is not tracking.



"As a result, Privacy Badger differs from traditional ad-blocking extensions in two key ways.  First, while most other blocking extensions prioritize blocking ads, Privacy Badger is purely a tracker-blocker.  The extension doesn't block ads unless they happen to be tracking you; in fact, one of our goals is to incentivize advertisers to adopt better privacy practices.  Second, most other blockers rely on a human-curated list of domains or URLs to block.  Privacy Badger is an algorithmic tracker blocker - we define tracking behavior - and then Privacy Badger blocks or restricts domains that it observes tracking in the wild.  What is and isn't considered a tracker is entirely based on how a specific domain acts, not on human judgment.



"When you view a webpage, that page will often be made up of content from many different sources.  For example, a news web page might load the actual article from the news company, ads from an ad company, and the comments section from a different company that's been contracted to provide that service.  Privacy Badger keeps track of all of this.  If, as you browse the web, the same source seems to be tracking your browser across different websites, then Privacy Badger springs into action" - you can just see the badger springing - "springs into action" - grabbing those URLs and ripping them out by its teeth - "preventing your browser from loading any more content from that source.  And when your browser stops loading content from a source, that source can no longer track you.  Voila," they wrote.



"At a more technical level, Privacy Badger keeps note of the third-party domains that embed content - images, scripts, and advertising - in the pages you visit.  Privacy Badger looks for tracking techniques like uniquely identifying cookies, local storage, supercookies, and canvas fingerprinting.  If it observes a single third-party host tracking you on three separate sites, Privacy Badger will automatically disallow content from that third-party tracker."



As for their browser support, it's as universal as it could be.  Their FAQ asks:  "Will you be supporting any other browsers besides Chrome, Firefox, Edge, and Opera?"  And they said:  "We're working toward Safari on macOS support.  Safari on iOS appears to lack certain extension capabilities required by Privacy Badger to function properly.  Chrome on Android does not support extensions.  To use Privacy Badger on Android, install Firefox for Android.  Privacy Badger does not work with Microsoft Edge Legacy.  Please switch to the new Microsoft Edge browser.  Note that Microsoft Edge does not support extensions on mobile devices."



And they also noted that:  "Privacy Badger sends the Global Privacy Control signal" - that was the GPC we did a podcast on a few months ago - "to opt you out of data sharing and selling, and the Do Not Track signal to tell companies not to track you.  If they ignore your wishes, Privacy Badger will learn to block them, whether they're advertisers or trackers of other kinds."



And also finally I thought this was very interesting and cool from their FAQ.  They had someone ask:  "I run a domain that uses cookies or other tracking.  How do I stop Privacy Badger from blocking me?"  So they reply:  "One way is to stop tracking users who have turned on Global Privacy Control or Do Not Track signals.  Privacy Badger will stop learning to block that domain.  The next version of Privacy Badger to ship with an updated pre-trained list will no longer include that domain in the list.  Most Privacy Badger users will then update to that list.



"You can also unblock yourself by promising to meaningfully respect the Do Not Track signal.  To do so, post a verbatim copy of EFF's Do Not Track policy to your URL, whatever your domain is, .well-known/dnt-policy.txt.  If your domain is compliant with EFF's DNT policy and declares this compliance, most Privacy Badgers will see this declaration the next time they encounter your domain.  Also, the next version of Privacy Badger to ship with an updated pre-trained list will probably include your declaration of compliance in the list.



"Note that the domain must support HTTPS, to protect against tampering by network attackers.  The path contains '.well-known' per RFC 5785.  Also note that you must post a copy of the policy at each compliant subdomain you control.  For example, if you wish to declare compliance with both sub1.example.com and sub2.example.com, you must post EFF's DNT policy on each subdomain."



So to me, this reads like a solid piece of mature technology.  I hate the name, but I'm glad that I added it to my browsing experience so I wanted everyone to know about it.  I have been looking at it, and so far I've been impressed with what it's been doing.  It has been, you know, there's a little badger-like icon in the upper right now of my toolbar.  And I see that it is blocking trackers.  So, cool.  And immediately it cleaned up all the links that I'm getting from Google's search results so they no longer know what I click.  And I would really mind them doing it, but the idea that they're showing me the URL that I think I'm clicking on, and they've deliberately changed that, you know, in the HTML behind the scenes, that's just not okay.



So finally, recall the guy from last week whose tweet asked about obtaining the pre-release of SpinRite.  That was the tweet that prompted me to mention that it was available to everyone who was interested.  This listener was Brett Russell, and as a reminder what he tweeted was:  "Hi, Steven.  I realize this is a request out of the blue, but my hard disk is failing on my main machine.  I own a copy of SpinRite" - and then he had posted the code, which I redacted - "but the disk is set up as GPT" - meaning GUID Partition Format as opposed to old-school MBR.  So he said:  "...which 6.0 does not support.  Any chance you can give me a pre-release of 6.1 to run, please?  There is nothing crucial on the drive; but if it dies, it will take some time to bring it all back."



So I replied with the instructions about obtaining the current SpinRite DOS executable pre-release.  And then yesterday, when I was catching up on my Twitter feed, I found three tweets from Brett.  Apparently he had sent them shortly after I replied.  The first said:  "Thanks, Steve.  Much appreciated.  Running it now."  Second one he said:  "Wow, it's fast.  Feels a lot faster than 6.0."  He said:  "I like the new drive identification screen, as well."  And then the third tweet finally he said:  "It worked.  Machine is up and running, faster than ever, although no errors were picked up, which I've seen before."  He said:  "Setting up a RAID mirror for when the drive does die.  You saved me a lot of time and effort.  Thank you."



So anyway, Brett's comment about "no errors were picked up" is a common and sometimes mysterious experience for SpinRite users.  After running SpinRite, whatever was wrong will have been fixed, but SpinRite might not explicitly show that anything was done.  The reason for this somewhat unsatisfying outcome is that SpinRite induces the drive to deal with marginal problems internally.  Before SpinRite's deep recovery is needed, the drive will remove a sector from use, and everything after that will be well again.  But drives typically don't report doing so.  So all SpinRite and its user knows is that things are better afterwards.  Anyway, just wanted to close the loop on that.  I guess that's sort of our first testimonial for the upcoming SpinRite 6.1.  So thank you, Brett, for that.



LEO:  Privilege.  Now, see, when I see this title, I think I'm not understanding what you're talking about.  You say the "power of privilege."  You mean the power of having money and power?  Is that what you mean?  But of course we who are privileged have power.  What the heck could possibly be wrong with that?



STEVE:  That's right.  So today we're going to continue our examination of the NSA's and CISA's top 10 cybersecurity misconfiguration mistakes by drilling down deeply into their number two misconfiguration which is Privilege.  What strikes me most about this publication is that each item is quite beefy and worthwhile.  Unlike some recommendation lists that we see that feel like they were written by someone pontificating from an armchair, these resulted from their active successful penetration of networks.



So here's what they had to say on the topic of "Improper Separation of User and Administrator Privilege."  And then I'm going to embellish a little bit on that afterward.  So they wrote:  "Administrators often assign multiple roles to one account.  These accounts have access to a wide range of devices and services, allowing malicious actors to move through a network quickly after obtaining one compromised account without triggering lateral movement and/or privilege escalation detection measures."  In other words, don't do that.  That's not good privilege management.  They said:  "Assessment teams have observed the following common account separation misconfigurations:  excessive account privileges, elevated service account permissions, and non-essential use of elevated accounts."



Under Excessive Account Privileges they elaborate:  "Account privileges are intended to control user access to host or application resources to limit access to sensitive information or enforce a least-privilege security model.  When account privileges are overly permissive, users can see and do things they should not be able to do, which becomes a security issue as it increases risk exposure and attack surface."  And I would add that bad guys that are able to get in as that user are then also able to do things that they should not be able to do.



They said:  "Expanding organizations can undergo numerous changes in account management, personnel, and access requirements.  These changes commonly lead to privilege creep, the granting of excessive access and unnecessary account privileges.  Through the analysis of topical and nested AD (Active Directory) groups, a malicious actor can find a user account that has been granted account privileges that exceed their need-to-know or least-privilege function.  Extraneous access can lead to easy avenues for unauthorized access to data and resources, and escalation of privileges in the targeted domain."



Under Elevated Service Account Permissions:  "Applications often operate using user accounts to access resources.  These user accounts, which are known as service accounts, often require elevated privileges.  When a malicious actor compromises an application or service using a service account, they'll have the same privileges and access as the service account.



"Malicious actors can exploit elevated service permissions within a domain to gain unauthorized access and control over critical systems.  Service accounts are enticing targets for malicious actors because such accounts are often granted elevated permissions within the domain due to the nature of the service, and because access to use the service can be requested by any valid domain user.  Due to these factors, 'kerberoasting,' as it's called, kerberoasting, a form of credential access achieved by cracking service account credentials, is a common technique used to gain control over service account targets."



And finally, Non-Essential Use of Elevated Accounts.  They said:  "IT personnel use domain administrator and other administrator accounts for system and network management due to their inherent elevated privileges.  When an administrator account is logged into a compromised host, a malicious actor can steal and use the account's credentials and an Active Directory-generated authentication token to move, using the elevated permissions, throughout the domain.  Using an elevated account for normal day-to-day non-administrative tasks increases the account's exposure and, therefore, its risk of compromise and its risk to the network."  In other words, don't do that.  Don't be lazy, admins, and use your fancy elevated accounts if you're not actually needing to do so at the time.  Drop back...



LEO:  I can't believe we still have to tell people this.



STEVE:  And what's interesting is that these are the ways they have penetrated - they, the NSA and CISA, their red and blue teams - have penetrated users' networks.  They said:  "Malicious actors prioritize" - well, and Leo, again, of course we believe it's true, right, because it's easier.



LEO:  Sure, yeah.



STEVE:  It's easier just to use your god login because then you're going to be able to do anything that you want to do.



LEO:  Right.



STEVE:  Their point is that, if you happen to contact a compromised machine under that persona, the malware there can automatically compromise your credentials.  Anyway, they said:  "Malicious actors prioritize obtaining valid domain credentials upon gaining access to a network.  Authentication using valid domain credentials allows the execution of secondary enumeration techniques to gain visibility into the target domain and Active Directory structure, including discovery of other elevated accounts and where the elevated accounts are used.  Targeting elevated accounts such as domain administrator or system administrators performing day-to-day activities provides the most direct path to achieve domain escalation.  Systems or applications accessed by the targeted elevated accounts significantly increase the attack surface available to adversaries, providing additional paths and escalation options."



And finally:  "After obtaining initial access via an account with administrative permissions, an assessment team run by these guys compromised a domain in under a business day.  The team first gained initial access to the system through phishing, by which they enticed the end user to download and execute malicious payloads.  The targeted end-user account had administrative permissions, enabling the team to quickly compromise the entire domain."



Okay.  So there are two primary things that I want to elaborate on and highlight.  The first is that idea of mission creep.  They called it "privilege creep," and anyone who has been in charge of managing any complex environment of privileges will have seen and been fighting this.  I've often observed that code generally does not evolve well.  By its nature, code is not designed to evolve.  There will generally be an initial grand organizing concept for a project.  And that vision will be then realized in code.  But that organizing vision influences the code at all levels.  The way the large project is decomposed into smaller manageable components.  The way those components talk to each other, and what they are able to say, and how.  The data structures those components use to capture and manage the project's state.  Every one of those things is individually generally static, and they're interdependent, and thus the whole system is quite resistant to change.



I've also observed that it's often the case that, once a project is finished, it really should be entirely discarded, then redesigned and reimplemented from scratch.  The reason for that is not necessarily that the project itself evolved during its implementation, but that those who were implementing it evolved.  In other words, they were changed by doing the work because the process of solving any sufficiently complex problem teaches those who are solving it how it should have originally been designed in the first place.  So for sufficiently complex projects, several iterations of that sort can be required.  Unfortunately, of course, outside of academic settings the luxury of multiple iterations is seldom available.



Okay.  So how does that apply to account privileges?  The primary message here is that the management of privileges provides a tremendous amount of power.  And as always, with power comes responsibility.  Any major enterprise's account privileges hierarchy needs to be carefully designed.  In a sufficiently complex environment, the concept of "least privilege" is much more easily espoused than it is implemented.  Mistakes in either direction of privilege result in problems.  Users need varying levels of access, and differing groups of users may have both disjoint and overlapping regions of access at the same level.  Autonomous devices also need their own access privileges, you know, like printers scattered around.  And it's difficult enough to get everything set up correctly given a static environment without any change.  But when did nothing ever change?



The real test is the test of time, brought about by an enterprise's evolving structures and needs.  What this means is that any initial design will be almost immediately obsoleted once it faces reality.  The asymmetry of "privilege" is another factor:  Too little privilege and things don't work or break, printers don't print, users are unable to do what they need to do, and the boss is not happy.  Too much privilege and, sure, everything works, but users can get up to mischief, and things are made much easier for the bad guys once they get in.  And again, the boss is not happy.  So assigning and managing privilege is a bit like walking a tightrope.  Stepping off in either direction does not produce a good result.



I think the best advice that can be given is to clearly recognize and accept that the proactive management of "privilege" is a truly important aspect of the enterprise's entire operation.  Designing and managing an enterprise's account and access privileges should never be a grudging afterthought.  Be sure to give it its due.  Fortunately, it's never too late to repair an organization's no longer strictly applicable privilege model.  It can be fixed.  If this is your responsibility, you likely already know what needs to be done.  Don't wait until it's too late.



Next week we'll continue moving through this excellent joint NSA/CISA document, as we keep up with the news of the week.



LEO:  You know, it strikes me that Linux users and Unix users in general are constantly warned not to use the root account.



STEVE:  Yup.



LEO:  And they're provided with a great little command, sudo, so that they can easily escalate for one command and one command only.  And then it reverts back to the normal user privilege.



STEVE:  And we know that even Microsoft finally came up, they invented that split token system where, you know...



LEO:  Well, but I was going to say I think Windows users are a little brain damaged because so many of them operate as administrator.  And, you know, that's why, you're right, they created UAC, but they created UAC because people were not operating as limited users.  So they had to create the User Access Control as an automated way of keeping you from getting in trouble.  But I think that's caused brain damage.  I think that's caused people to think it's okay to operate as administrator.



STEVE:  Well, and you point out that sudo was easy to use, and it was the ease of use that made it feasible to run with non-elevated privileges.



LEO:  Exactly.



STEVE:  Windows didn't have an easy way.



LEO:  No.



STEVE:  Exactly.



LEO:  Yeah.



STEVE:  And no one wanted to log off and then log back in as an admin in order to install a piece of software.



LEO:  And by the way, we should say Apple's done pretty much the same thing with macOS.  They have a limited user.  And in fact when you create a new user, by default that's a limited user, but everybody runs it in admin.  And so Apple does the same thing that Microsoft does with this kind of temporary escalation process.  But so much better to do it like Linux does, I think.



STEVE:  Yes.  And as I've said before, operating a truly secure OS is just no fun.



LEO:  Yeah, nobody wants that, yeah.



STEVE:  You can't get anything done.  You spend all your time saying yes, I'm sure; yes, it's me; hello, I'm still here; yeah, yeah, yeah, I know, blah blah blah.



LEO:  I think you could argue that sudo is a better way to do it than User Access Control, than the escalation prompts.  I mean, fairly frequently you'll run a command that says you don't have enough privilege to do that.  And many Linux distros, many terminals there's a simple command keystroke that adds sudo to your previous command, so you can easily escalate.  I don't know.  I just, maybe it's just me, but I prefer that way of doing things.  And nevertheless, run as an administrator both on Mac and Windows.  So there.  By the way, you cannot, without basically rooting it, run as an administrator on either iOS or Android.  Why do you think?  It's dangerous; right?  They don't even have a way to escalate privilege.  You know, you're a user, whether you like it or not.



STEVE:  Yeah.  And in those environments it really makes sense for the OS to be the sole arbiter of what runs and how.



LEO:  Right, right.  I love this show.  And I'm so glad you're going to keep doing it.  Here we are at Episode 945.  At this point I would have been, like, counting down the days.  Oh, there's only a year left, oh no.  But we're going to until one or the other of us or maybe both keels over, whether it's with exhaustion or the final journey, the final...



STEVE:  Well, I'm glad I'm going to be here for the end of support of Windows 10.



LEO:  Yes.  And I hope you'll be here for the end of the Unix epoch.



STEVE:  Oh, that'd be good.



LEO:  In 2038, baby.



STEVE:  Oh, wow.



LEO:  That's my goal.  Steve Gibson...



STEVE:  Yeah, we only have 15 - we only have 15 more years.



LEO:  That's not - we're more than halfway there, Steve.



STEVE:  Exactly.



LEO:  Easy-peasy.



STEVE:  Bye.



LEO:  Bye.



Copyright (c) 2023 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#946

DATE:		October 31, 2023

TITLE:		Citrix Bleed

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-946.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  What caused last week's connection interruption?  Is it possible to create and maintain an Internet whitelist?  What's the latest on LastPass vault decryptions?  How do you know of a remote correspondent adds a new device to their Apple account that it's really them?  Might there be more life left in Windows 10 than we thought?  What's foremost in the minds of today's bug bounty hunters?  What new free and open source utility has CISA released?  Could it be that SpinRite 6.1 is finished?  Is TLS 1.2 ready for retirement?  And what about IPv4?  How can open source projects get their code signed?  And then we're going to take a really interesting deep dive into the Internet's latest mass-casualty disaster.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  He's got some really interesting stuff, including a new idea for zero trust network architectures.  We'll also talk about more LastPass hacks, sad to say.  And then he's going to do one of my favorite things, which is look at a major security flaw, this time with Citrix, and actually examine the code that made it possible.  This is a real great learning episode for anybody who has to write code or just wants to know how these break-ins happen.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 946, recorded Halloween, that's Tuesday, October 31st, 2023:  Citrix Bleed.



It's time for Security Now!, the show where we cover the latest security news, your privacy, your security, your online health and welfare with this guy right here, Mr. Steven Gibson of GRC.com.  Happy Halloween, Steve.



STEVE GIBSON:  Happy Halloween to you, you know, you are my namesake.  You're the SQRL.  



LEO:  I thought I'd wear a SQRL outfit for you, yeah.



STEVE:  That's very appropriate.  I have a SQRL right here.



LEO:  Yes, you do.



STEVE:  In the camera on my whatever this thing is called that holds the microphone.  Maybe it's a microphone holder.



LEO:  Yeah, or "boom arm" we call it, yeah.  And that's called a "mic flag," just in case anybody asks.  You have a SQRL mic flag, yes.



STEVE:  I proudly do.  Yes, the SQRL that took seven years of my life and will not give it back.



LEO:  And now that I've shown you my SQRL head, please, children, do not fear, there is a human in here.



STEVE:  Oh.  There's Leo, okay.



LEO:  I won't take off the striped overalls, though.  That's for another time.



STEVE:  No, stay in your prison garb, that's good.



LEO:  What, my friend, are we discussing today?



STEVE:  Oh, have we got a good one.  It is not often that we get a horrible problem in a - well, actually it was last week.  Okay, but aside from that, we have another horrible problem...



LEO:  Wow.



STEVE:  ...in a globally distributed, highly used piece of network appliance.  But what is so cool about this is that we're going to get to do a deep dive into the exactly code mistake that was made in a way that all of our listeners, maybe not their significant others, but all of our listeners will be able to almost certainly understand...



LEO:  I love it when you do that, by the way.  That's so much fun.



STEVE:  ...and get something from it.



LEO:  Yeah.



STEVE:  This is just - this one, it wasn't my plan.  I was going to move forward with the NSA/CISA Top 10 misconfigurations for cybersecurity stuff.  But when I realized that a research firm had done the reverse engineering, and that we'd be able to walk through that process and look at the code, and it's understandable, like the mistake that was made, it's just like, oh, this is just too good.  So Security Now! Episode 946 for this Halloween, last day of October - and we're all going to be changing our clocks, those of us who do that, on Sunday - is titled Citrix Bleed, reminiscent of the Heartbleed flaw in servers of the past.  But we have a bunch of fun stuff to talk about.



We're going to look at, are going to answer the question, what caused last week's connection interruptions that we briefly suffered?  Is it possible to create and maintain an Internet whitelist?  What's the latest on LastPass vault decryptions?  How do you know, when a remote correspondent adds a new device to their Apple account, if it's really them, you know, the people you think it is or not?  Might there be more life left in Windows 10 than we have been led to believe?  What's foremost in the minds of today's bug bounty hunters?  What new free and open source utility has CISA just released?  Could it be that SpinRite 6.1 is finished?  Is TLS 1.2 ready for retirement?  And what about IPv4?  How can open source projects get their code signed?  And then as I said, we also have a great Picture of the Week.  But then we're going to take a really interesting deep dive into the Internet's latest mass casualty disaster.



LEO:  Good lord.  This is all in today's show.



STEVE:  Yes.  And that's in the first five minutes.



LEO:  Wow.  Wow.  You've jam-packed our Halloween episode with a lot of scary stuff.  And that's I guess as it should be.



STEVE:  That's our plan.



LEO:  I am ready.  I have queued up the Picture of the Week.  I don't understand this at all.



STEVE:  I captioned this picture "What's the plan here?" because what we have is some apparently new asphalt that has been paved around what looks like a phone pole and an electrical pole, which - okay.  So what I know from the person, from the listener of ours who actually pulled off the road...



LEO:  To take this picture?



STEVE:  ...in order to take this picture for us and send it because he thought, okay, we have to send this to Steve.



LEO:  Picture of the Week for sure.



STEVE:  Yes.  There's actually a new road that's been added off to the right.  You can sort of see it toward the bottom of the picture heading off the picture to the right.



LEO:  Ah, yes, yeah.



STEVE:  So the old, the original road is a - you can see it's got a yellow line down the middle, and it looks like the blacktop on it has turned gray because it's been around for a while.  So that's the original road.  Well, they decided to add a right turn lane onto the original road to complement turning to the right into this new road that's been added off to the right.  Unfortunately, there was an existing two telephone poles.  Actually a telephone...



LEO:  Yeah, you can see in the distance the same configuration, but it's on the grass.  It's not in the road.



STEVE:  Right, right.  And this didn't used to be on the road, Leo.



LEO:  No.



STEVE:  This used to also be on the grass.



LEO:  Just like this, yeah.



STEVE:  But they decided to retract the grass and add some road.



LEO:  Oh, boy.



STEVE:  But, you know, you can't drive through the phone poles.



LEO:  No, bad idea, yes.



STEVE:  Because that would not be - that would not turn out well. 



LEO:  Yes.



STEVE:  No.  So, now, I was thinking about this.  They clearly have to move the phone poles back over onto what will soon be grass after they reseed this region.  And moving them to the right would add slack to them because we can see that it's going to be - so that would kind of work, although there are some lines running off to the left.  They're going to have to disconnect those and then stretch them or lengthen them and then...



LEO:  What a mess.



STEVE:  You know, anyway, I just thought this was interesting because really how did this happen?  How was it, I mean, people came along, and they added new concrete curbing to the right of the poles.



LEO:  And expanded the shoulder.



STEVE:  Exactly.



LEO:  Where the poles are.



STEVE:  Pulled that back, got everything ready, and put down new blacktop carefully around the poles so that it looks like they sprouted from the blacktop.  



LEO:  I'm just grateful they have two bright orange security pylons there to prevent people from driving into those poles.



STEVE:  That's right.  You would not want someone to think, well, I'd better turn right here and get into the right turn land in order to do that.



LEO:  Good lord.



STEVE:  Oh, goodness.



LEO:  The number one reason our power goes out in this area is because people - it happens at least every few months - drive into a pole.  They knock the pole over, power goes out, the electric company comes out and restrings the wires.



STEVE:  Those pesky poles.  Now, you realize, Leo, that in more recent communities...



LEO:  They're underground, yes.



STEVE:  The power is underground.



LEO:  In fact, ours in our cul-de-sac, everything is underground, and I love it because there's no - you don't - this is so unsightly.  Why are there two different poles right next to each other, too?



STEVE:  Well, one is power.  The tall one is power.  And the lower one is phone lines because you can see that black junction box there going away from the lower one is the telephone cable.



LEO:  Ah, yes.



STEVE:  So, yes.  For what it's worth, Leo, we have underground power here.



LEO:  So much better.



STEVE:  And I can tell you that it is not always a lot better.



LEO:  Oh, really.  Go out a lot?



STEVE:  The reason I had my own generator was that the power here has been notoriously bad.  Lately I think they finally changed the thing that was bad.  It's a lot better.  Anyway, I have two pieces of miscellany, one quick one, and then sort of something startling that I want to share.  The first is why were we interrupted a couple times last week.  After the connection interruptions which occurred during last week's podcast, I got serious about tracking down the cause of what, you know, I'd sort of been seeing them for a while, but they weren't bad enough to get in the way of the podcast.  And I thought, okay, that's the last straw.



LEO:  Yeah.  We edited it out, but you froze onscreen.



STEVE:  Yeah.  And I could see, because I'm monitoring my bandwidth on a separate screen over here, I could see a big red band twice come up where there was a lack of connectivity completely.  So I decided to get focused on this.  And the problem is normally I'm busy coding.  And when I'm focused, I am a little OCD.  I just sort of let everything else happen around me.  I don't really pay attention.



Anyway, I was able to track down the cause of the trouble to occasional spontaneous crashes and reboots of my little Netgate SG-1100 router, which runs pfSense and sits just inboard of my cable modem.  So it enforces my network segmentation, creates separate WiFi domains and does a bunch more things that I've talked about in the past you know, static port mapping and all kinds of cool stuff.  I love pfSense.  There's never been any - oh, even now it runs - it is a DynDNS client, and its firewall rules are adaptable, that is, the firewall rules will track any changes reflected by Dynamic DNS.  So it allows my two locations that have public IPs to track each other for the sake of keeping them connected.  So, for example, my Synologies are able to talk to each other, even though there's no public presence of the Synology sync going on.  Anyway, it's very cool.



So what I wanted to mention was the first thing to check when something like this is happening is the device's power supply.  Those little wall warts, as they're often called, that ship with consumer-grade equipment, are often the source of trouble.  If the power supply is getting tired, it could be the reason for weird behavior.  So fortunately that little Netgate SG-1100 uses a standard 12-volt DC supply that has the standard DC barrel connector.  So I had plenty of swap-in replacements at hand.  Ever since I exchanged its power supply, which was two days ago, I've had zero reboots.  You know, I still have my fingers crossed.  But since it was previous happening, sometimes several times a day, it really looks like the problem is solved.



So anyway, just a little note, a reminder that oftentimes the cheapest piece of equipment in the system, which is those little cheesy transformers these days, the little power supplies, they can have like much lower quality guts than the device they're powering, and they can bring it to its knees.  So just a reminder about that.



Okay.  Now, Leo, you'll remember another of this podcast's very early sponsors, the Nerds On Site guys, who are based in Canada, with their Nerds spread far and wide to help non-computer-savvy users and enterprises with their computer problems.



LEO:  Yeah, I'll always have a soft spot in my heart for them.



STEVE:  I do, too.



LEO:  Yeah.



STEVE:  You know?  They were early fans of the podcast.  And in fact I keynoted...



LEO:  David Redekop, yeah, yeah.



STEVE:  Yes, Dave Redekop.  I keynoted their annual gathering a couple times.



LEO:  Oh, nice, nice.



STEVE:  And in fact I also met with them for the first time when I was in Toronto when I was serving as a guest on your Call For Help show which you were doing for Rogers Cable up in Toronto.



LEO:  Yes.



STEVE:  So we're talking 18 years ago in those very early audio-only days of the podcast, when we used to record the show in your hotel room.  So I remember you had a little recorder, and I would bring the microphones, and we'd set up and talk for like...



LEO:  We tried once on the rooftop restaurant of the hotel.  But that didn't work out so well.



STEVE:  No, no.



LEO:  So we retreated to the room, yes.



STEVE:  Okay.  Anyway, since then I've stayed loosely in touch with David Redekop, who was one of the principals back then, and I've sort of been vaguely aware of what he's been doing.  Well, he was passing through town last Friday, so we rendezvoused at Starbucks to catch up, he and two of his guys.  Okay, now, this was over my second five-shot latte of the morning, so my brain was fully charged up.  David took about an hour to walk me step-by-step through the careful and deliberate design of the network security solution he and his team of techies at ADAMnetworks have designed, and which is currently in use actively protecting around two million user and device endpoints around the world.  So this solution is not just theory, and I think a surprisingly good idea.  It's actually in place and working.



And once I'd heard what this group has managed to pull off, that didn't surprise me because it felt like the result of a decade or more of iterating on a concept in order to bring it to maturity.  You know, you start with an idea, you implement a first pass, and then stick it out there.  You remain involved and connected.  You learn what does and doesn't work.  Then you iterate and push out an improved solution.  You add new features to address new requirements.  You test them and adjust them until they're working correctly.



Now, I've often noted on this podcast that I cannot imagine the challenge of keeping an enterprise safe while it's connected to today's Internet.  And in fact, today's Citrix Bleed topic makes me shudder a bit because, even if everything is done right, it's still possible to be taken down.  I'm so glad that keeping a highly connected sprawling enterprise safe is not my job.  But it does still need to be done, and these ADAMnetworks guys have not only imagined the challenge of keeping an enterprise safe, from what was explained to me on Friday - and I think I pretty much understand the way it works, at least in broad strokes - they've not only imagined the challenge, they've risen to it.  And if I had an enterprise network which I needed to protect today, based upon what I understand now, I don't think I'd want to be without this solution.  I'm thinking of reaching out to Father Robert to make him aware of this because it would be so cool to have the Vatican this well protected.



Okay.  So what did they do?  Nobody today runs their firewall where by default everything is allowed except for traffic that's known to be malicious.  In the dim dark ages of the Internet, once upon a time we once blocked known exploitable ports.  We don't do that anymore.  But blocking known bad domains or scanning communications traffic for known malicious content is what's being done today, and it's the modern equivalent of a default "allow all" rule at the bottom of a list of things that you want to block.  We do it because we think there's no alternative.  Last Friday, the ADAMnetworks guys showed me a better way.



What these guys have done is to not worry about inspecting and interpreting the contents of enterprise user communications, or anyone's communications.  As I understand it, they do not do any deep packet inspection so no one's communications contents privacy is ever compromised.  What they've focused on and specialized in and made work is only allowing connections to known safe and benign entities.  Now, okay, if it were possible to do this perfectly, phishing attacks would be ineffective because a user clicking on a phishing link would be unable to obtain any malicious content.  And even if someone were to inadvertently bring some malware into the enterprise that infected it from the inside, if that malware was then unable to connect to its command-and-control servers, it would be effectively inert.



So imagine that you start with very strong DNS filtering and require all clients on the network to use this DNS.  They've also extended this through and enforced it for enterprise-controlled smartphone and other mobile clients so you get complete coverage.  The thing that sets this apart from other solutions is that this is not the typical blacklisting DNS, which selectively blocks known bad domains.  ADAMnetworks has managed to build a practical whitelisting DNS which by default blocks everything unknown and only permits DNS resolution for known safe domains.  DNS is resolved on premises, so privacy is preserved; but it's also informed by a shared central manager in the cloud so that as new domains come online and are cleared, the master whitelist can be updated.



If a user attempts to connect to an unlisted DNS domain, they receive an intercept page.  And the way this is done is quite clever.  I won't go into it, but it works.  And they must manually accept and approve that if they want to proceed.  But before that is allowed, 18 other public sources of DNS filtering from other providers are consulted to make sure the unknown domain is not known to be malicious by anyone.  And there's also technology in the pop-up intercept page which prevents malware from being able to simulate the human giving their permission.



And speaking of malware, it turns out that modern malware is hip to having its DNS blocked.  So there is malware out there that has static IP addresses burned-in.  They don't do any DNS.  It just connects directly to its command and control.  So this introduces the outbound firewall component of this solution which they call "Don't Talk to Strangers."  No outbound connection is allowed to any IP that wasn't first looked up with the system's local DNS resolver.  So by linking DNS resolution to a dynamic outbound firewall...



LEO:  Clever.



STEVE:  Yes.  DNS, which has already been made very strong, becomes the universal gatekeeper.  Okay.  But then it turns out that Facebook, for example, has some apps that also don't do DNS.  Facebook's apps are benign, and they know the fixed IP addresses they want to connect back to at the mothership.  So to make a solution work completely, which they have, it's necessary for the system to incorporate some whitelisting for known benign blocks of destination IPs.  So that's in there, too.



Okay, now, what I've just described is an overview which just scratches the surface of the enterprise security solution these guys have developed.  And I'm sure that our techie listeners are saying "but but but but but," to which all I can say is that they actually have this thing working.  They demonstrated it to me; and, as I said, more than two million user and device endpoints are currently under this solution's protection.



They have a full mature-looking management UI, with scheduling and per-user permissions and everything else needed to implement a practical enterprise-class solution.  I asked David how much trouble there was with unknown, yet benign, DNS; and he acknowledged that it was there, but that they had reduced the level to such a degree that it was effectively negligible.  I recall that he said 5%, but I don't remember what 5% was of.



So my reaction was that, if I were an IT guy, like with this kind of responsibility, and it was possible and practical to operate behind what is essentially a whitelisting Internet firewall, if that only caused a tiny bit of trouble in fringe cases which can then be worked around, that would be entirely acceptable for my peace of mind and in return for the enhanced security it would provide for everyone else, all of the time.  And what's cool is that it protects against the unknown.



It turns out that the widespread SolarWinds vulnerability which took the whole world by surprise would not have affected any of ADAMnetworks' users.  And the always evolving Pegasus smartphone spyware would not function if it were to infect any of the handsets protected by this system.  So as usual, my focus and interest was on the technology, which I think is clearly pretty cool, and which I felt an obligation to share because this could help a lot of our listeners.  Although I cannot vouch for this from firsthand experience because I have none, you know, we do know these guys.  And we've known them for nearly 20 years.  They're techies like us.



Just take a look at any of the many videos posted on their website and how excited they are about what they've created, and you'll see that immediately.  I was very impressed by what I saw last Friday.  I don't know anything about the business side of this, you know, like what it costs or what plans they have or any of that.  We didn't talk about that.  We were talking about the tech.  My sense is that the solution is scalable down to a single router in a small office all the way up to a global enterprise because there was - he did make some mention of, like, the massive amounts of traffic that they're able to handle.  But I don't know what the extreme points are for that.  As I said, I also have no idea what it costs.  You'll need to track that down for yourselves, if it sounds interesting.



So anyway, after what I heard and learned Friday, I wanted to put this on everyone's radar.  They are ADAMnetworks.  It's A-D-A-M-N-E-T dot works.  And I have some links in today's show notes for their site, although I got the links just by poking around a bit the other day.  So you'll find a page of videos.  Even their wide-open tech support portal so you can see everything that their customers are asking and, like, being confused by or needing help with.  So anyway, I just wanted to share this because I was really impressed that they could make this work.  And boy, you know having a default deny, allow trusted system that could protect something the size of an enterprise, to me that's a game changer.  So I will leave it up to our listeners to decide if it looks like something for them.



Just last week, the still unknown hackers who breached LastPass and exfiltrated everyone's encrypted vaults managed to siphon off another $4.4 million worth of crypto-assets from 25 of LastPass's previous and maybe still current customers, those who haven't run for the hills.  We know that most LastPass users will have remained with LastPass and won't be receiving the news that stolen vaults are being actively decrypted.  As we saw previously, the hackers are cracking the stolen LastPass password vaults, presumably those that were protected by weak passwords and probably also few PBKDF iterations, though I would argue by no fault of the user.  That was squarely on LastPass for not bringing users up to speed.  That is, their password iteration counts to protect against having weak passwords, which we know users are going to have.



Anyway, from those vaults that they decrypted, they recovered the crypto wallet seed phrases and then drained those customers' user accounts.  So with the latest round of thefts, which are being tracked because we know the wallets into which those stolen funds are being transferred, this brings the total to nearly $40 million after they had previously stolen around 35 from about 150 other users earlier this year.  So it's ongoing, and it's really sad and unfortunate.



LEO:  And if you had a LastPass vault with your keys in it, you might want to change your keys.



STEVE:  Yes, yes.  And in fact remember that we read a note from one of our listeners who had set up another wallet and was planning to transfer his currency to it, but didn't get around to it.



LEO:  Yikes.



STEVE:  And lost three something million dollars, I think it was.



LEO:  Yes, yes.  If you don't have around to it, you'd better get around to it.



STEVE:  That's right.



LEO:  All I can say.



STEVE:  Remember that cool feature of Threema that I always liked so much?  It was the ability to positively verify the other person's public key through some out-of-band communication channel.  You know, you can't verify it in-band because if you're not talking to the person you think you are, then you're still not talking to the person you think you are after you exchange key verification somehow.  So it's got to be like some other non-in-band communication app channel, like a phone call or, even better yet, a face-to-face meeting.



Okay.  So Apple hasn't gone quite that far since they continue to manage all of their users' keys on their behalf, which I'm sure is a good thing in the case of your typical Apple user.  But they have developed a slick new way to verify any new device that registers itself with a user's account.  They call it "iMessage Contact Key Verification," and it has just appeared in the developer previews of iOS 17.2, macOS 14.2, and watchOS 10.2.  It's designed to present an alert inside iMessage conversations whenever a new device is added to a participant's account.



Okay.  So say that I'm in an iMessage dialog with someone, and they add a new device to their account and, you know, talk to me from it.  How do I know that the new device was actually added by the person I think I'm messaging with?  If it was someone else holding the new device and impersonating this person,  that's not good.  So this new system displays a fixed eight-digit code in both my phone and their phone, that is, the newly added phone.  This allows us to arrange a telephone conversation or a FaceTime chat or something not just iMessage, where I have some absolute knowledge that the person I think I'm messaging with is this other person.  We compare eight-digit codes.  That is, what I would do is I would tell the person, you know, holding the new device, okay, what eight-digit code is your phone showing you?  And, you know, I know who I'm talking to; right?  I mean, so because it's a telephone conversation or a FaceTime call, they read the eight-digit code, I confirm that that's the code my phone is sharing with me, and then I click on Mark as Verified, which then verifies that this new - it tells iMessage that I am accepting my remote correspondent's new device for subsequent iMessage communications.



So anyway, Apple's onscreen instructions say:  "Verify who you are messaging with by comparing contact verification codes in person or over the phone."  And at first I thought, wait, you know, shouldn't I make them tell me?  And I guess that's the case; right?  Except that, you know, if it's - basically what this does is it forces you to have a non-iMessage, some non-iMessage contact with the person whose phone is also showing this code.  So you know that that's the device which you don't yet want to fully trust.  So I'm not sure if it matters if I tell them the code, or they tell me, or we alternate digits back and forth, or they tell me the first half, and I tell them the second half?  I don't - I'm not quite sure.



LEO:  We leave this as an exercise.  You figure it out.



STEVE:  But it does force you to have a face-to-face confirmation.  And then you say, okay, fine, you're holding the phone, you told me the code, that's good.  I'm now going to accept this as belonging to you.  So, yeah.  Anyway, cool that Apple continues to do this.  They're not perfect, they make mistakes, I get that.  But, boy, you know, every contact we have with the measures that they are taking seem really proactive.  And, you know, they've got people sitting around just thinking of how to make our stuff more secure.



LEO:  This solves the problem of somebody rekeying or getting a new phone in this case.  And Signal has done it, as you said, Threema does it.  How do you verify that this new communication with a new key is the same as the old person?  And it just shows that Apple's now doing strong encryption on messaging; right?



STEVE:  Right, right.  Well, stronger authentication.



LEO:  Actually it doesn't say anything about encryption, does it.  It says [crosstalk].



STEVE:  Right, yeah.  And we know that they always had really good encryption.  On the other hand, if you're not who's decrypting it at the other end, that's not good either.



LEO:  Right, right.  Authentication's a big part of this, yeah.



STEVE:  Right, exactly.  And of course that's the whole certificate thing; right?  I mean, anybody can have a certificate that brings up an encrypted connection.  What you want is to know who has that certificate and for their identity to have been proven.



LEO:  Right.



STEVE:  So and that's what Let's Encrypt does not try to do.  It says it's only saying the domain that asked for this certificate was granted one.  So now nobody can see what's going on in private, but it's not making any assertations, I mean, it's not making any assertions about who it was that it issued the certificate to.  That's why, you know, I'm still using a fancy certificate from DigiCert where I have to jump through hoops every so often in order to say, yup, still me.  Hi, Mom.



LEO:  Yeah.



STEVE:  Okay.  So perhaps there's hope for longer Windows 10 support.  Remember I just talked about last week how like in the week before we had just crossed the less than two years remaining for Windows 10 line, which to my mind seems really premature.



Okay.  It turns out that Windows 10, Leo, is good for the Earth.  Or more correctly, it's that Windows 11 is less so.  More than 20,000 members of the Public Interest Research Group (PIRG) have formally asked Microsoft to extend support for Windows 10, which, as I recently mourned, is scheduled to reach end-of-life status in two years, in October of 2025.



LEO:  It has been 10 years, or will have been.  I mean, that's a long time.



STEVE:  But it still feels new.  It still feels new to me.



LEO:  Yeah, as you used Windows 7 until last month.



STEVE:  I'm still in front of Windows 7; that's right.  And my browsers are not happy with me.  But okay.  The organization, this PIRG group, says that, get this, around 40% of the one billion devices that run Windows 10 cannot be upgraded to Windows 11.



LEO:  Wow.



STEVE:  And that will create an unnecessary e-waste problem.



LEO:  400 million, to be precise.



STEVE:  Yeah, uh-huh.  Of course, on the flipside, Microsoft and many other hardware vendors are looking forward to this event for just that very reason.  PIRG is the same group, the same industry group that had recently convinced Google to extend the warranty and support for older Chromebooks, using similar arguments.  It's like, don't put these out to pasture.  It's going to create a bunch of toxic landfill, and what's wrong with them?  They still work.  They're still just fine.  Just like Windows 10, Microsoft.



LEO:  Or it's just good for Linux.  I mean, honestly, just put Linux on the damn things, and you'll never have to worry about Microsoft again.



STEVE:  Right.  You will have to pay people to teach you how to use it, however.



LEO:  Oh, yeah.  [Indiscernible] it's the same as Windows.  Easy-peasy.



STEVE:  So it is certainly somewhat bracing to learn that 40% of current machines, a billion current machines, won't be compatible with Windows 11.  I mean, we've been talking about this from the beginning, and we know that it's just because they don't want it to be.  Right?



LEO:  Yeah, right, right.  They have to have 8th generation Intel processor, they have to have TPM 2.0, even though that that's not necessary for the operation of Windows 11.



STEVE:  Right.  It is legitimately...



LEO:  But honestly, Linux works great on that machine.  I'm just telling you.



STEVE:  It is, yeah, the problem is that users don't.  It's legitimately a huge deal.  So I really do wonder whether Microsoft, you know, is prepared for the user and the corporate ire that will follow from being told, sorry...



LEO:  They've been warned.  It happens to every version of Windows.  And 10 years is, as you point out, much longer than the Chromebook.  It's much longer than any phone you've got.  Google just got a lot of attention for saying we're going to support this phone for seven years.  So I don't think, I mean, look.  Look at your phone.  Most people get new phones every few years.  I mean, the landfills are jammed with stuff.  What about your new car?  How long does your car last?  And that's a lot of physical waste.  Many computers' worth.  We've got to do something about this.  You can't put Linux in your car, unfortunately.



STEVE:  Why force the hard work to be obsoleted for Windows 11, which does not require, we know it doesn't require that the hardware...



LEO:  No, I agree.  I agree.  They shouldn't say 8th generation and TPM 2.  But increasingly they've added features that do support, maybe even require that newer hardware.  But I think you nailed it when it's really designed to support the PC industry and people buying new PCs.  I mean, that's clear.  But the whole - our whole industry is geared around getting people to throw out their old stuff and buy new stuff.



STEVE:  So last Thursday, the HackerOne bug bounty folks...



LEO:  Oh, yeah.  We saw this.  Yeah, really good stuff.



STEVE:  Yeah, neat stuff.  They're neat people.  Took the occasion of their total bounty payouts crossing the $300 million mark to provide some insights into the current state of their operations.  I've edited it down a bit, but there's some interesting stuff in here.  So just to be clear in the following text, when they refer to their "customers," they mean organizations who pay bounties to have their bugs found in their own code.  And of course when they refer to "hackers," those are the freelance agents who are the finder of those bugs.



So HackerOne said:  "HackerOne today announced its ethical hacker community has surpassed 300 million in total all-time rewards on the HackerOne platform.  And 30 hackers have also earned more than $1 million on the platform, with one hacker surpassing $4 million in total earnings."  So, yeah, it's possible to have a career.



"And hackers are finding new opportunities to earn more by diversifying their skill sets as emerging technology reshapes the threat landscape.  55% of hackers surveyed" - and they surveyed 2,000 of their hackers - "expect that Generative AI will become a top target in the future.  Crypto and blockchain organizations continue to see strong program engagement, offering the highest average overall rewards for hackers, including the award of this year's top payout of $100,050.  HackerOne's customers have also expanded how they commission hackers outside of traditional bug bounties with pentesting engagements increasing by 54% on the platform this year in 2023."



So, and they have some bullet points.  Hackers continue to experiment with Generative AI, as 61% of hackers said they will use and develop hacking tools from GenAI to find more vulnerabilities, and another 62% of hackers plan to specialize in the OWASP Top 10 for Large Language Models.  Hackers also said they plan to use GenAI to write better reports, 66% said that, or code, 53% said that, and reduce language barriers.  That was a third of the people.



Hackers report insufficient in-house talent, meaning on the part of HackerOne's customers.  Hackers report insufficient in-house talent and expertise as the top challenge for organizations, which is a gap they are filling.  70% of customers stated that hacker efforts have helped them avoid a significant cyber event.  57% percent of HackerOne customers believe exploited vulnerabilities are the greatest threat to their organizations, over phishing at 22%, insider threats at 12%, and nation-state actors at 10%.



Customers are getting faster at fixing vulnerabilities, as the average platform-wide remediation time dropped by 10 days in 2022.  Okay, now, I didn't see the report because I had to go through a bunch of hoops to get it, and it didn't seem necessary.  But I would like to know dropped from what to what by 10 days?  You know, did it drop from 20 days to 10, in which case it was in half, or from 100 to 90?  To me it makes a difference.  But anyway, their summary didn't say.  They said, interestingly, automotive, media/entertainment, and government verticals saw the biggest decrease in time to remediation, with an over 50% improvement.  Okay, so those industry groups - automotive, media/entertainment, and government - were in this most recent update much faster to jump on trouble and fix them.



And finally, organizations are reducing costs by embracing human-centered security testing earlier in their software development lifecycles, with customers saving an estimated $18,000 from security experts reviewing their code before its released rather than afterwards.



So Chris Evans, HackerOne's CISO and Chief Hacking Officer, was quoted saying:  "Organizations are under pressure to adopt Generative AI to stay ahead of competitors, which in turn is transforming the threat landscape.  If you want to remain proactive about new threats, you need to learn from the experts in the trenches:  hackers."



So anyway, that annual Hacker-Powered Security Report is based on data from HackerOne's vulnerability database and gathered the views from both HackerOne customers and more than 2,000 hackers who were interviewed on the platform.  And to me, it seems that I guess it's not surprising, but maybe only in how quick it's happened, an interesting takeaway that mistakes are bound to be made in the rush to get new Generative AI solutions rolled out and in the world, which did seem to happen pretty quickly.  So getting up to speed on AI technology appears to be on everyone's radar.  No doubt the bad guys and also the white hat hacker good guys who are going to help companies to solve their problems before they get exploited.



Logging Made Easy, from your friends at CISA.  Last Friday, our own CISA released a tool called Logging Made Easy through their official GitHub account.  LME is a toolkit that enhances log management on Windows devices.  The tool was originally developed by the UK's NCSC agency way back in the late 2010s, and they retired it in March of this year.  So CISA picked the tool up, updated and re-wrote it to cover recent Windows versions and to enhance its logging capabilities.



Here's what CISA said.  They said:  "Today, CISA announces the launch of a new version of Logging Made Easy (LME), a straightforward log management solution for Windows-based devices that can be downloaded and self-installed for free.  CISA's version reimagines technology developed by the United Kingdom's National Cyber Security Centre (NCSC), making it available to a wider audience.  Log management makes systems more secure.  Until now, it's been a heavy lift for many targeted organizations, especially those with limited resources.  CISA's LME is a turnkey solution for public and private organizations seeking to strengthen their cybersecurity while reducing their log management burden.  LME builds upon the success of the NCSC's log management solution, and CISA urges organizations to secure their Windows-based devices today by downloading the free LME technical solution."



Over on their GitHub page, they said that Logging Made Easy can show where administrative commands are being run on enrolled devices, see who is using which machine.  In conjunction with threat reports, it's possible to query for the presence of an attacker in the form of Tactics, Techniques, and Procedures.  So anyway, all that seems pretty cool and useful.  The link to jump there is this week's GRC shortcut, so grc.sc/946.  But the full link is also not very difficult, it's github.com/cisagov/LME.  And that'll take you to a place where you can find out more and grab it for yourself.



And Leo, I have here in the show notes, under the heading of SpinRite, the issues page...



LEO:  I love this.  Yeah.



STEVE:  ...from GitHub, from our GitLab, as of Sunday.  And I checked a few minutes ago.  It is still holding.  It says:  "There are no open issues."  And then further up on the page it says, you know, Steve Gibson, I'm the project author, SpinRite v6.1 issues.  Open:  0.  Closed:  536.



LEO:  That's impressive.  That is really impressive.



STEVE:  And of course those are all the things that we tripped over and stumbled upon.  Some bugs in SpinRite.  Many bizarro behavior in weird things that caused me to go to eBay and buy some old motherboard, and for Lorrie to say, uh, we're going to put these away someday; right?  Uh, yes, dear.



LEO:  Are they on the dining room table?  I mean...



STEVE:  They are out in the open.



LEO:  Did you run out of room in your office?



STEVE:  For all and sundry to see.  So...



LEO:  Oh, lord.  Poor Lorrie.  Hey, she knew what she was getting into.



STEVE:  So on Sunday - she does and she's still supporting my work.  So on Sunday SpinRite's second and quite possibly final pre-release was published.  As far as I know at this point, the DOS executable code that's now available will be what finally ships.



LEO:  Woohoo!



STEVE:  Yeah.  SpinRite 6.1 includes a tiny DOS text editor for viewing and editing its log files from DOS, and to make editing any DOS config files easier.  That editor also appears to be completely ready.  I need to verify that all of the FreeDOS OS kernel customizations that I have made for SpinRite are complete.  You know, I needed to tweak the FreeDOS kernel so that it doesn't completely go crazy if it sees a GPT partition because all it knows about is MBR format.  And also so that it doesn't freak out if there's like errors on the disk that it's trying to log into in order to create DOS-style drive letters for all of the drives the way we used to in the old days.



So I've done all that.  I think it's done.  I'm just going to go back and check because it's been a while.  Then I will return to finish a bit of the work on the Windows side of SpinRite, which is the thing that creates the boot media for SpinRite.  And basically I'm going to incorporate the improvements in that technology that arose from the work that I did on ValiDrive.  So I'll port those back over into the Windows side of SpinRite.  I need to update GRC's servers to digitally sign individually licensed copies of SpinRite on the fly for customer download. And at that point, SpinRite 6.1 will be available as a finished product to all SpinRite 6 owners, and then I'm going to begin work on SpinRite's web pages to document all of its new operations and screens to show all of the many changes and improvements that have been made during the past three years of this project.



And then, after I get an email system in place to begin notifying SpinRite 6.0's 19 years' worth of existing users, I get to start in on SpinRite 7, which really excites me.  But in the meantime, we really do have a major step forward here.  This morning I saw a posting in GRC's web forums which said:  "Thanks to the pre-release of 6.1, I'm running SpinRite on drives that I either have not been able to run it on for years, or drives that were just too big for 6.0, so I was never able to run SpinRite on them before."  So, you know, promise made and promise kept.



LEO:  Whoo, boy, that's exciting.



STEVE:  And quite soon, promise delivered.



LEO:  Yay.



STEVE:  Yeah, I'm very happy.  So we have a bit of closing-the-loop feedback.  Shep Poor said:  "Hi, Steve.  You've trained us well.  When I saw a BleepingComputer article titled 'Windows 11 adds support for 11 file archive formats,' my first thought was, 'Oh, great, 11 new interpreters built into Windows, ready to be exploited.'"



Yes, Shep.  I appreciated his tweet because "thinking security" is probably the best thing our listeners could take away from the many lessons we learn here every week from specific events.  Those events will come and go.  But seeing the underlying issues and what connects them together, you know, what are the takeaways and the lessons, those will endure and remain viable and valuable long after the Unix epoch has shut down everything that we know and love.



LEO:  It's true.  It's in my head.  I think the same thing.  Oh, another interpreter.



STEVE:  Yup.



LEO:  And it's because of you.  I mean, I totally think of these security flaws because you've trained us all these years.



STEVE:  Because we've looked at so many.  And boy, are we going to look at a doozy here in a minute.  Someone named BP, whose handle is @Wildlandsguy, he said:  "Hi, Mr. Gibson.  I had a huge smile on my face when you announced you'll be doing SN past 999.  I would appreciate hearing your opinion on what it would take to get the world to move to IPv6.  What are the significant hurdles?"  And he said:  "FYI, ran ValiDrive on three PNY drives from Best Buy, and all checked out okay."  So, cool.



Okay.  So there are probably still some backwaters of the Internet which IPv6 has not reached.  But from everything I'm seeing, nearly everything appears to be IPv6 ready.  So I think that at this point the only thing that's holding things back is a concern over breakage.  What might break if IPv4 was withdrawn, and we were left only with IPv6?  We've seen examples where surprising things have broken with change.  We were just talking about the surprise that the designers of the upgraded TLS 1.3 protocol faced when it initially didn't work because too many boxes somewhere along the way in the traffic path broke it.  They shouldn't have broken it, but they did.  So it was necessary for the TLS 1.3 guys to make 1.3 appear to be much more like 1.2.  Again, shouldn't have had to do it, but they did.



And we've seen how incredibly cautious the browser designers are whenever they change anything.  They'll initially have some newfangled feature disabled and only enabled manually by intrepid developers.  Then they'll turn it on by default for 1% of the population while closely monitoring that 1% for any trouble.  Then they'll gradually start rolling it out, enabling it more and more, still kind of holding their breath and, like,  making sure that nothing really horrible happens.  I mean, so, you know, they learned to be super cautious from experience.  So I think that's where we are.  



In the case of IPv6, I think we're at the "if it's not broke, don't fix it" stage.  The good news is that everyone is probably ready for it when it's forced upon us.  But until it's truly necessary for someone not to have IPv4, until there just isn't any IPv4 left, and the only thing that newcomers can have is an IPv6 address, that's when I suspect IPv6 will finally begin in earnest, and really no sooner, until there's really no other choice because that's just the way all this stuff is, is like, you know, leave it alone, unless and until we have to finally give it up.



Someone tweeting as @NotDorion said:  "Hi, Steve.  I've recently seen the high cost of signing certificates, especially for open-source projects like ImageMagick," and he provides a link to ImageMagick's pages over on GitHub.  He said:  "On one hand, I see that having costly certificates makes it difficult for malware to access them.  But maybe a solution is needed for open-source projects.  Would be glad to hear your thoughts on this.  Thanks for all the great content all these years."



So that was interesting.  The posting on GitHub that our listener linked to was titled "The Windows installer for ImageMagick will no longer be signed."  And its developer wrote:  "Today, our code-signing certificate will expire.  For many years LeaderSSL sponsored us with a code-signing certificate, but they're no longer able to do so.  Since June of 2023 the CA/B Forum requires that OV code-signing private keys be stored on a FIPS 140-2 Level 2 or Common Criteria Level EAL4+ certified device."  Meaning some sort of HSL, some hardware security layer.  Some, you know, like we've talked about before, inside a truly secure enclave device.



He writes:  "This means we are no longer able to export our code-signing certificate with its private key and use this in GitHub actions.  We would now either need to have our own GitHub agent and hardware token or use a cloud solution," he says, "e.g., DigiCert.  Our preference would be to use a cloud solution that integrates with GitHub.  DigiCert seems to be our only option now, but a certificate there would cost $629, tax excluded, for a single year.  If your organization requires a signed installer, then please consider sponsoring us with a code-signing certificate.  Please reach out to @" and then his handle "for questions or in case of a sponsorship."



Now, what was neat was that in a terrific example of community action, this posting from three days ago generated a terrific thread which, based upon this developer's reaction, appeared to provide a number of useful alternative solutions, one being a free solution for a year, and others involving Azure Code Signing, which appears to be a good thing.  I didn't look into it any deeper.  But other open source projects are having similar difficulties, and they chimed into the thread, as well.



So I wanted to share the news of this in the event that the eventual solution found by ImageMagick's developer might be of some use to some of our listeners.  As we know, all of GRC's code is signed by a DigiCert EV certificate which works for me since I'm a commercial entity, and it's worthwhile.  And I was just a couple weeks ago talking about, you know, how it matters to have your code signed.  You're looking at downloading something.  You want to make sure that that EXE hasn't been modified since the time it was signed by the entity that claims to do so.  And that requires a signature.



The problem, of course, is that these signatures, the code signing, the private key in these code-signing certificates must be kept secure because you don't want bad guys signing stuff, claiming to be ImageMagick, and getting the trust that ImageMagick has been able to accrue over the years.  So, I mean, the problem is the way we're going, the nature of open source software historically is kind of coming into collision with the fact that we really want to know, we want to know who's behind it. 



Matt Davis tweeted and offered some great feedback about last week's look at privilege.  He said:  "Hi, Steve.  Thrilled to hear you're continuing past 999.  I've listened since the beginning, and I'm looking forward to another 999 or so afterwards."  Okay, well, let's hope.  He said:  "You may have overlooked the heart of the Privileged Accounts issue you discussed during last week's episode.  The problem is not lazy IT folks signing in as root in the morning.  It's that we are trying to create a shift away from 1-to-1 parity between Accounts and People.



"Many moons ago," he wrote, "shared logins were the norm for system management, which has obvious downsides and drawbacks we all understand.  Single-sign-on solutions like Okta aimed to solve this by promoting unique accounts per user.  Each person gets exactly enough privilege for their needed tasks, and staff turnover is easy to handle by revoking a single password.  However, this led to a new problem.  Administrators only have a single account, which they have no choice but to use, even when they're performing basic tasks, and it's all powerful.



"The obvious solution would be to create separate Standard and Admin accounts for every administrator; but since most software is now is charged per account per month, this tends to be cost prohibitive."  And he said, "We have one service at $300 per month per user."  Yikes.  He said:  "Ideas like Just Enough Access help, allowing admins to switch between regular and elevated access, akin to UAC in specific environments like Azure and AWS.  But slow adoption in userland means that I still need to use my superuser-level login to access my email every morning.



"Looking forward to having the option to submit via email.  It seems like the Feedback Form hasn't really been working over the past few years, and you are the only reason I've pulled my Twitter account out of the dustbin.  Love the show and SpinRite.  Thank you."



So anyway, Matt, thank you for some terrific perspective from the field.  And yikes, charging an organization $300 per user per month for authentication, that makes my blood boil a little bit.  What a rip-off.  I can't see what great deal of work they're having to do that justifies that much expense.  



Okay.  Finally, JP McNeal said:  "Hi, Steve.  I'm a huge fan of SN and so glad you'll keep going after Episode 999.  I'm wondering what your thoughts are on turning off TLS 1.2?  I did a little research and found it interesting that there's no official EOL [end of life] for TLS versions.  It's just a matter of when the big browser vendors decide to not support it.  Microsoft, Apple, and Google agreed to stop supporting TLS 1.0 in 2018, for example.  Thank you.  Signed JP."  And he said:  "PS:  I work for a hosting company."  So, yeah, he's got a vested interested in, like, when will 1.2 go away.



So I suppose that time flies, but my impression is that TLS is still a relatively recent arrival, and that 1.3 hasn't fully happened yet.  And I guess, you know, I think that about Windows 10 also.  So what do I know?  TLS 1.0 and 1.1 were both deprecated only two years ago in 2021.  TLS 1.2 has been in use since 2008, and TLS 1.3 only began 10 years later and five years ago in 2018.  But here's the stat that most matters:  As of last month, 99.9% of all websites support TLS 1.2, but only 64.8% support 1.3.



LEO:  There you go.  Yup.



STEVE:  Yup.  So not even two-thirds of the Internet's websites today are TLS 1.3 capable.  And given a good choice of cipher suites for TLS 1.2, there's really nothing wrong with it.



LEO:  That's an important point.



STEVE:  It can be made secure by choosing your cipher suites carefully.  And so browsers could certainly choose not to support cipher suites that they felt were a problem and leave the cipher suites for 1.2 and the 1.2 protocol which are perfectly strong, continuing to be used.  So the Internet as a whole is not going to be able to move beyond 1.2 for a while yet, not until adoption of 1.3 becomes far stronger than it currently is.  And as we know, that'll just take time.  New web servers will all be supporting 1.3, but older servers which only support 1.2 will need to either be upgraded or die, and then be replaced by new servers.  So for a while it'll be 1.2 and 1.3, moving toward 1.3 over time.



LEO:  Very closely related to the IPv6 question.  It's interesting.



STEVE:  Yes, exactly.



LEO:  People want us to move along, don't they.



STEVE:  Exactly.  So our final sponsor.  And then, oh boy, do I have some fun to share.



LEO:  Oh, good.  By the way, let me echo everybody's pleasure that you're sticking around past 999.  That's good news for all of us.  All right, Steve.  You have promised...



STEVE:  Oh, boy.



LEO:  ...to blow the socks, so to speak...



STEVE:  Leo, you're going to love it.



LEO:  All right.



STEVE:  So as I said at the top of the show, I decided to punt our continuing exploration of the joint NSA/CISA Top 10 cybersecurity misconfigurations until at least next week, due to another mass casualty cybersecurity event that gives us a really interesting opportunity to take our podcast listeners on a journey into the interior code guts of a widely used and hugely popular piece of networking hardware whose recent wide-scale compromise is believed to have negatively impacted the network security of more than 20,000 enterprises worldwide who have been depending upon the integrity of this device to keep them protected.  So this week we're going to take a deep dive into the specific code flaw that resulted in the coining of the term Citrix Bleed, which is also the title of today's podcast.



I've often spoken of the now common practice of jumping on and quickly reverse engineering vulnerability-closing patches to update code as a means of discovering the details of the defect that the patch corrects for the purpose of either, A, if you're a white hat security research firm, documenting an interesting piece of Internet history; or, B, if you're a black hat criminal organization using the information gleaned from such reverse engineering to begin compromising the hapless users of not-yet-patched devices.



As always, it's one thing to talk about this being done, and an entirely different thing to obtain a good sense of the details from someone who's actually doing it.  So I smiled when I encountered a write-up from a firm wearing a white hat of the process they underwent to reverse engineer a really, really bad vulnerability that is, unfortunately, being exploited and is damaging people right this minute.  Before I get into the nitty-gritty, I want to set the stage by sharing an overview of the situation written by security researcher Kevin Beaumont, whom we often encounter in the security space.  He tweets as "Gossi the Dog" is his handle.  I don't know why.



LEO:  Okay.  Very reliable source, yes.



STEVE:  That's right.  Here's how Kevin described the situation three days ago.  Under his headline "Mass exploitation of Citrix Bleed vulnerability, including a ransomware group," he wrote:  "Three days ago, Assetnote posted an excellent write-up about Citrix Bleed aka CVE-2023-4966 in Citrix NetScaler.  This vulnerability is now under mass exploitation.  A few weeks ago it was under limited targeted exploitation to allow network access.  It's not Assetnote's fault.  It was clear multiple groups had already obtained the technical details.  The patch became available on October 10th."  Okay, so that's exactly three weeks ago today.



He says:  "Even if you applied the patch and rebooted, you still have a problem as session tokens persist.  The vulnerability allows memory access.  Sounds boring; right?  The same memory contains session tokens, which an attacker can easily extract.  Those session tokens allow the bypass of needed login credentials and the bypass of all multi-factor authentication.  An attacker can just replay the session key, and they're in.  Who uses Citrix NetScaler anyway?" he asks.  "Many tens of thousands of businesses run it.  It is very, very common in enterprises and governments.  If you think nobody runs this stuff, you probably also think everybody uses Linux on their laptop."  That's what he said, Leo.  I didn't say that.



He said:  "For an additional perspective, the Risky Business Newsletter wrote the following.  They said:  'A Citrix vulnerability has entered the dangerous stage of mass exploitation as multiple threat actors are compromising unpatched devices all over the Internet in a race with each other to steal their session tokens.  Known as Citrix Bleed and tracked as CVE-2023-4966, the vulnerability impacts Citrix ADC and Citrix NetScaler, which are extremely complex networking devices used in large enterprise and government networks in multiple roles, such as gateways, proxies, caching, VPN servers, and a bunch of other stuff.



"The vulnerability allows threat actors to send junk data to the Citrix OpenID component that will crash and leak a part of the device's memory.  The bad part is that, in some cases, this memory may contain session tokens that attackers can collect and then bypass authentication to access the device and the network behind.  Citrix released patches to fix the Citrix Bleed memory leak earlier this month, on October 10th.



"The bug in itself is extremely bad as it is, but things took a turn for the worse a week later when Google Mandiant researchers came out to say they found evidence Citrix Bleed had been exploited in the wild since late August, making the vulnerability one of this year's tens of actively exploited zero-days.  Mandiant said a threat actor was using the bug to gain access to Citrix gateways and then pivot to internal systems.  The attacks were small in scale and targeted professional services, technology, and government organizations.



"Things then went from bad to disastrous last week, around October 23-25, when several proof-of-concept exploits started popping up on GitHub and vulnerability portals.  Within a day, mass exploitation was in full swing.  At the time of writing, GreyNoise is tracking more than 120 IP addresses that are probing the Internet and attempting to exploit Citrix Bleed."



And finally, Kevin Beaumont later posted an update over on Mastodon.  He said:  "Quick update on Citrix Bleed.  Tracking just over 20,000 exploited NetScaler servers so far today, where session tokens have been stolen.  How?  Have a honeypot running to gather data on attackers, then compare with Netflow via industry friends.  Two TCP connections, first one large, plus Shodan cross reference to validate NetScaler victim.  Also it turns out in March of this year somebody documented how to replay the session token to bypass multifactor authentication."  



Okay.  So on the back of last week's Cisco web portal disaster, we now have another web portal login authentication disaster from another manufacturer, Citrix, who has a similar long history of catastrophic security vulnerabilities.  And Kevin has arranged to confirm that more than 20,000 instances of actual login session token exfiltration have occurred.



So now let's look at the mixed blessing arising from the need to patch a horrible and widespread problem in full public view.  What steps do both curious security researchers and rapacious malicious criminals take to turn a patch into an exploit?  The security research firm Assetnote explains.  They wrote:  "It's time for another round of Citrix Patch Diffing," as in differencing.  "Earlier this month Citrix released a security bulletin which mentioned 'unauthenticated buffer-related vulnerabilities' and two CVEs.  These issues affected Citrix NetScaler ADC and NetScaler Gateway.



"We were interested in CVE-2023-4966, which was described as 'sensitive information disclosure' and had a CVSS score of 9.4.  The high score for an information disclosure vulnerability and the mention of 'buffer-related vulnerabilities' piqued our interest.  Our goal was to understand the vulnerability and develop a check for our Attack Surface Management platform."  So these guys wanted to reverse engineer it for their own security suite offering.



They wrote:  "For those unfamiliar with Citrix NetScaler, it is a network device providing load balancing, firewall, and VPN services.  NetScaler Gateway usually refers to the VPN and authentication components, whereas ADC refers to the load balancing and traffic management features.  We've covered issues in NetScaler several times before.  We began by installing and configuring the two versions we wanted to compare.  We chose 13.1-49.15 and 13.1-48.47.  From our previous work with NetScaler, we knew to look in the /netscaler/nsppe binary.  This is the NetScaler Packet Processing Engine, and it contains a full TCP/IP network stack as well as multiple HTTP services.  If there's a vulnerability in NetScaler, this is where we look first.



"We decompiled the two versions of nsppe" - that would be NetScaler Packet Processing Engine - "with Ghidra and used the BinExport extension to create a BinDiff file."  Okay.  I'll pause here to mention that four years ago we discussed Ghidra at some length.  It's a free and open source binary code reverse engineering tool which was initially developed by the U.S. National Security Agency, our NSA.  The NSA initially released the Ghidra binaries during the 2019 RSA Conference and then followed up by releasing its source code a month later on GitHub.  Before Ghidra, the proprietary and quite expensive IDA Pro was the only real game in town.  Now that's no longer the case.



Okay.  So resuming what Assetnote was explaining, they had said:  "We decompiled the two versions of nsppe with Ghidra and used the BinExport extension to create a BinDiff file."  You can probably guess, but "BinDiff" is short for "Binary Difference."  And this, of course, is the key to the attack.  When something is patched, some code somewhere is changed.  So the first challenge is to locate the regions that the patch changed, then to carefully examine the before and after code to understand what problem that change was intended to fix.



They wrote:  "This process takes a while as the binaries are quite large.  To ensure success we tweaked the decompiler settings under Edit > Tool Options > Decompiler," and then they list the things that they changed.  "After creating the BinDiff, we opened them up for comparison and found roughly 50, five zero, functions had changed.  We proceeded to check each one, often opening both versions in Ghidra and comparing the decompiled output with a text diffing tool.



"We found two functions that stood out:  ns_aaa_oauth_send_openid_config and ns_aaa_oauthrp_send_openid_config.  Both functions perform a similar operation.  They implement the OpenID Connect Discovery endpoint.  The functions are both accessible unauthenticated via the..." and then they list two specific URLs where they respond respectively.  Both functions also included the same patch, an additional bounds check before sending the response.  This can be seen in the snippets below showing the before and after for one of those two.



Now, I have the two code snippets.  I grabbed them and put them in the show notes because they're very clear, and anyone who codes will be able to see what's going on.  But I can describe what's going on for those who are listening.  I think everybody's going to get it.  In the original code, the amount of data that is returned in response to the query - so basically what's happening is we have this OpenID query that's made over HTTP.  So it's a standard HTTP style query that is asking for, that is addressing a specific URL known, when you're talking about web APIs as an endpoint, so it is making this query.  It's got a bunch of, you know, standard query headers that you would expect with any HTTP query.  And then it's going to get some response back.



So in the original code, the amount of data that's returned in response to the query is determined by the size of a string that results after substituting a whole bunch of variables into a string template.  After all of those substitutions are made, the result will be a much larger string, so a 128K character buffer is provided to contain this expanded string after these substitutions are made.  Basically there are small, you know, percent symbols in a template, and each of those percent symbols is expanded into a parameter from a parameter that is provided to this function.



The string substitution function is known as "snprintf," which will be quite familiar to C programmers.  Snprintf performs the string substitution up to the limit of the length of the buffer that it's been given.  Then it returns as its function return value the full length that the string would be after performing all of the substitutions.  It will not overflow the size of the buffer it's been provided.  But it will return a larger value than the size of the buffer it's been provided if the resulting substituted string will not fit into the provided buffer.



This is typically done to tell the programmer how large a buffer would be needed to hold the entire expanded string if the buffer that was initially provided was not large enough.  Code that cared about that would then typically allocate a new buffer that was large enough, then perform the string substitution again into the now large enough buffer.  But in any event, this is considered a safe string function that will not overflow the size of the buffer that has been provided.



So here comes the fatal flaw.  In the original code, immediately after the snprintf does its work and returns the size of the required buffer, that size is directly used to specify the length of the data that will be returned from the user's query, returned to the user as a result of the user's or in this case an attacker's query.  In other words, what the original coder who wrote this failed to do was to check that the size of the response was not larger than the buffer which had been set aside to contain it.



LEO:  So you could easily put a 5R3, which is the size returned, is greater than hex 20,000, then truncate it or don't use it, but certainly don't substitute that number in.



STEVE:  Right.



LEO:  Because you're going to have a buffer overflow.



STEVE:  And you will see exactly what you just said in the second snippet, Leo.  Failing to perform that simple and required check meant that if the remote attacker could somehow arrange to cause that snprintf function to assemble a huge long string, and thus specify a massive response size, even though snprintf would not itself overflow the much smaller 128K buffer it had been provided, the response that would be sent back would be massive, with - and here it is - the data in that response coming from whatever happened to be lying around in RAM after that 128K buffer.



And, as it turns out, that just as with the infamous Heartbleed vulnerability, where the web server's private key just might have happened to be in nearby RAM, in the case of this Citrix Bleed disaster, the RAM following that 128K output buffer is chockfull of valid logged-on authentication tokens which the server dutifully sends back in response to the attacker's query, thus allowing any otherwise unauthenticated remote attacker to become authenticated by using those tokens and thus being logged on.



LEO:  Wow.  Easy mistake to make, though.



STEVE:  Yeah.



LEO:  But you just have to get in the habit of checking these overflows.  Wow.  You could see the signing - you have to understand snprintf returns not the string, but the size of the string or what it would be if we hadn't truncated it.



STEVE:  Exactly.



LEO:  And now they're reusing that size here.



STEVE:  Yes.



LEO:  As the parameter for the VPN send response.



STEVE:  For the amount of data to send.



LEO:  Yeah.



STEVE:  Exactly.



LEO:  Mistake.  And a lot of the time it would work.



STEVE:  Yup.



LEO:  Till a bad guy gets a hold of it, yeah.



STEVE:  Yup.  So, and that's one of the points we've often made here is that having code that works is entirely different from having code that is secure.



LEO:  Sure, very important.



STEVE:  They are two completely different things.



LEO:  And just as they said here in the fix, they check the size before they do that.



STEVE:  And they just skip the output.  They just drop the response completely.



LEO:  Wow.



STEVE:  So what Assetnote discovered in the patched and updated code, the change in the code that drew them to that spot was the addition of a simple check before sending any reply, to verify that the value returned by the preceding snprintf function was less than 128K.  In other words, the result of formatting the reply to the user would fit within the 128K buffer that was provided.  And if not, the bogus query would be ignored, and no response would be returned to the user, or to any attacker.



And as for exploiting this, Assetnote explained, they said:  "To exploit this, all we needed to do was figure out how to get the response to exceed the buffer size of 128K bytes.  The application would then respond with the completely filled, meaning Citrix, would then respond with the completely filled buffer, plus whatever memory immediately followed the print_temp_rule buffer.



"Initially we thought the endpoint would probably not be exploitable.  The only data that was inserted was the hostname, which is something that needed administrator access to configure.  Luckily for us, we were wrong, and the value inserted into the payload did not come from the configured hostname."  Get this.  "It actually came from the HTTP host header."  Okay, now, the host header, as we know, we talked about just recently, is one of the several query headers in any HTTP query.  So it is entirely within the remote attacker's control.  In other words, whoopsie.



These guys wrote:  "We were also fortunate that NetScaler inserts the hostname into the payload six times, as this meant we could hit the buffer limit of 128K bytes without running into issues because either the host header or the whole request was too long.  In other words, there was also a coincidental buffer size amplifier that worked to make attacks based on this mistake practical."



LEO:  Wow.  Wow.



STEVE:  So we might assume that the coder of the original implementation knew that the snprintf function was string safe, in that it could not be induced to overflow the buffer that was provided because the size of the buffer was also provided, and it would stop before it hit the end.  But rather than using the actual size of the string that ended up being returned in the buffer, the coder made the mistake of using the returned value from the snprintf function, which is, as we now all know, not the same as the length of the string that it placed in the buffer.  If the string that it wanted to place into the limited-size buffer was larger than the buffer, it would return that size.  And unfortunately, the programmer used that value as the length of the reply to send back.



Echoing these thoughts, Assetnote concluded:  "Here we saw an interesting example of a vulnerability caused by not fully understanding snprintf.  Even though snprintf is recommended as the secure version of sprintf, it is still important to be careful.  A buffer overflow was avoided by using snprintf, but the subsequent buffer overread was still an issue."



LEO:  Yeah.



STEVE:  "Like previous issues with Citrix NetScaler, the issue was made worse by a lack of other defense-in-depth techniques and mitigations, not clearing sensitive data from what appear to be temporary buffers, and stricter validation on client-provided data being the two most obvious mitigations which could have been applied to minimize the damage."



So as a consequence of that small oversight which was apparently first discovered back in August, but also enabled by, as Assetnote observed, a more general lack of architectural care in a security appliance, the network integrity of more than 20,000 large and high-end enterprise and government users around the world who have these network appliances has now been compromised.  And not just found vulnerable, but actually known to have been exploited.  And what's really, you know, as I've said, anyone can make a mistake.  But it's possible to do this better.



For example, when I was working on SQRL's code, I had a window always up on the screen which continually and in real time, it was updated in the background, showed me the current RAM contents of all of SQRL's sensitive data.  I had deliberately grouped it all into a single region for easier monitoring and wiping, and anything sensitive was wiped from RAM the moment it was no longer needed, and I was always able to confirm that visually.  And, you know...



LEO:  It's a good habit, yeah.



STEVE:  That's part of just doing it right.  I was terrified that I would make a mistake.  And people coding secure applications should live in a constant state of terror that they are going to make a mistake.



LEO:  It's one of the reasons I like Lisp is because you can see the value as the program's running of all the variables at any given time.  We don't have direct access to memory as you do in assembly, but that's a very useful tool.  You know, as I think about it, this might have been a kind of an inexperienced coder that made the mistake because, if you think about it, the fact that snprintf returns the size of the data it received is a hint to the coder to pay attention.  There's a reason why it returns that.  It returns it so you can test it.  I mean, it's explicitly there.  And Lisp does that a lot, too, where if there's a side effect, you're going to get back a value that's going to help you, though, prevent bad effects.  



STEVE:  One of the common coding patterns is to first call it with a buffer size of zero.



LEO:  Yeah.



STEVE:  In which case you use it to tell you the size of the buffer it needs.



LEO:  Oh, clever, yeah.



STEVE:  Then you allocate that buffer and call it again with that buffer, and you're [crosstalk].



LEO:  That's probably what snprintf was designed to do.



STEVE:  That's exactly what it was designed to do.



LEO:  Call it first.  Don't do anything with the string.  Get the size of it and then allocate that buffer.  Oh, yeah.



STEVE:  Yeah, and in fact I was looking at this, thinking, well, this is one of the reasons everything has gotten so big and bloated.  They've got a 128K buffer sitting around.



LEO:  For no reason.



STEVE:  You know, just for, like, to have.



LEO:  It's the case.



STEVE:  It's like, how lazy is that?  Allocate it on the fly the size you need.



LEO:  That's a good point.  So maybe they didn't want to call snprintf twice.  You know?  But that's the whole point of it is it tells you what it's going to need.



STEVE:  Yes.  You're right.  You're right.  You know, Leo, it's much better to have the security of 20,000 of your customers sacrificed.



LEO:  Than run a function twice.  Whoa.  No, that makes a lot of sense.  You return that value so people can use it.



STEVE:  Yup.



LEO:  When a function returns something, you know, think about why it's returning that.  That's good.  That's really good.  I like that.  So a normal way to use snprintf would be call a test.



STEVE:  Call the null buffer first.



LEO:  Yeah.



STEVE:  Yup.



LEO:  Figure out what you're going to need.



STEVE:  It tells you how big a buffer it needs.



LEO:  And call it again with the appropriate buffer size.  And I would still test and say, what did you get back?  Was it the same?  Because if it wasn't I'd still want to, you know, make sure it wasn't overflowing.



STEVE:  Well, you are wearing suspenders, Leo, so...



LEO:  And a belt, and the belt.



STEVE:  And I assume you have a belt.



LEO:  Great stuff.  You know, this is my favorite thing that you do, which is look at the code and see where the error happened.  I know it's not always as easy as that.  But I think that's a great, very instructional example.



STEVE:  It's a perfect learning opportunity, yeah.



LEO:  Yeah, yeah, really great.  Thank you, Steve.  Have a great week.  We'll see you next time on Security Now!.



STEVE:  Thanks, buddy.  Bye.



Copyright (c) 2023 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.






GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#947

DATE:		November 7, 2023

TITLE:		Article 45

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-947.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  Where was Microsoft storing their Azure keys?  What four new zero-day flaws has Microsoft declined to repair?  And what happens next?  What's this week's latest mass casualty event for publicly-exposed Internet servers?  And do we have any news on last week's Citrix Bleed fiasco?  What comes after CVSS v3.1 and why?  What happened to Google's Web DRM proposal?  And what about the earlier Cisco IOS XE mass casualty mess?  And what's the new Security Now! podcast slogan to emerge from it?  Our favorite password manager just announced their support for Passkeys.  Now what?  That guy with the badly messed-up SSD shared the results of using SpinRite 6.1.  I'll share and explain what happened.  And then, after entertaining some great feedback from our listeners, we're going to look into the next big looming battle between conservative tech and rapacious governments.  All that and more during this week's Security Now! podcast #947 - and counting.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  Coming up, lots to talk about.  Microsoft has some more flaws in Exchange Server.  This time they say, yeah, we're not going to fix it.  Well, maybe they ought to.  We'll also talk about an attack on our favorite hardware store, oh, no.  An update on Citrix Bleed.  And then Steve's going to talk about something I hadn't heard anything about, but it's a real grab from the EU that will really destroy Internet security.  What is Section 45?  Stay tuned.  Security Now! is next.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 947, recorded Tuesday, November 7th, 2023:  Article 45.



It's time for Security Now!, the show where we cover the latest news in the world of security, safety, and good vibes online with this guy right here, Steve Gibson, of the government - what did we decide GRC stands for?  Government Regulations?



STEVE GIBSON:  Actually, one of our listeners knew.  It's French for the - it's the French version of the RCMP, Canada's Royal Canadian Mounted Police.



LEO:  Okay.



STEVE:  And we'll be getting to that later in the podcast.



LEO:  Okay.  Okay.  



STEVE:  But first.



LEO:  Yes.	



STEVE:  But first.  Today's podcast has the mysterious title "Article 45."  What is Article 45, and why do we care?  Well, we're going to explain that, and it's like a big deal.  But first we learn where Microsoft was storing their Azure keys?



LEO:  Oh, no.  Oh, no.



STEVE:  Also, oh, it's as bad as we thought.  What four new zero-day flaws has Microsoft declined to repair, and what's probably going to happen next?  What's this week's latest mass casualty event for publicly exposed Internet servers?  And do we have any news on last week's Citrix Bleed fiasco?  What comes after CVSS v3.1 and why?  What happened to Google's Web DRM proposal?  And what about the earlier Cisco IOS XE mass casualty mess?  Oh, and what's the latest Security Now! podcast slogan to emerge from that event?  We've got a new slogan for Security Now!.



Our favorite password manager just announced their support for Passkeys.  Now what?  That guy from last week, or no, actually it was several weeks ago, with the badly messed-up SSD, shared the results of using SpinRite 6.1 on it.  So I'll share what he reported and explain that.  And then, of course, after entertaining some great feedback from our listeners, we're going to look into the next big looming battle between conservative tech and rapacious governments.  All that and more...



LEO:  I can't wait.



STEVE:  ...during this week's Security Now! podcast 947 titled "Article 45."



LEO:  All right.  You're not getting political on us, are you?



STEVE:  No, no, never that.  But we do have a wonderful Picture of the Week.



LEO:  We do.



STEVE:  Which begs the question, to park or not to park?  



LEO:  This is a great picture.  Look.  I can see somebody's thumb in it, so it's obviously from a listener; yes?



STEVE:  Yup, yup.  They saw this and said, okay, we've got to send this to Steve.  So for those who are not looking at video or don't have the show notes yet, our Picture of the Week is a municipal sign that is trying to control the parking in this area.  And it's two signs stacked on top of each other.  The first one is, you know, the one everyone has always seen; right?  No Parking Any Time, it says, and then there's a red arrow pointing to the left, meaning, you know, obviously leftward of this sign.  No Parking Any Time.  And then the second sign, the one below it, this one in green because it's got better news, although it ends up being a little confusing, this sign says 60 Min Parking - you know, minutes parking - All Other Times.



LEO:  What?  That means what?



STEVE:  Wait, wait, yeah.



LEO:  It's not completely inconsistent.  I mean, basically no other time.  But I don't know if that's what they intended; right?



STEVE:  That's true.  That is true.  So, you know, if you were to park for less than an hour, probably at any time...



LEO:  You'd be still...



STEVE:  ...since the first sign says No Parking Any Time...



LEO:  Yeah, yeah.  



STEVE:  ...you'd have a hard time arguing with a judge, hey, I was following the second sign, which said 60 Minute Parking All Other Times.  The judge would say, yes, but it said No Parking Any Time.  Anyway...



LEO:  This is basically in the benefit of the meter maid.  They get to choose.



STEVE:  Well, and you know, Leo, I was thinking about this.  This sort of explains how corporations screw up; right?



LEO:  Yes, yes.



STEVE:  Because here we are in a government, which is, you know, an organization.  Some poor guy was told, you know, put up this 60 Minute Parking sign underneath the No Parking Any Time sign.



LEO:  Right, right.



STEVE:  And he's looking at it, the guy actually on the ground, as we say now, you know, feet on the ground, he's there thinking, okay, this is the most screwed up thing I've ever seen.  But on the other hand, he works for the municipality, so it's probably not the most screwed up thing he's ever seen.  It's probably typical.



LEO:  Okay.  I'm just going to put it up and not ask any questions.



STEVE:  I did my job here.



LEO:  I did my job.



STEVE:  Anyway, yes, we are enjoying these pictures.  Thank you, listeners.  Okay.  So last Thursday Microsoft posted - I love this, too - under the headline "Announcing Microsoft Secure Future Initiative" - you know, there was some drum roll and some horns blowing in the background - "to advance security engineering."  So, okay.  They're announcing their secure future initiative to advance the state of the art in security engineering.  And this was written by Charlie Bell, the Executive VP of Microsoft Security.  So he opened this posting with this introduction.



He said:  "Today Microsoft's Vice Chair and President Brad Smith shared insight" - because that's what you want, you know, you want from your VP and your president, you want some insight - "on the global cybersecurity landscape and introduced our Secure Future Initiative.  These engineering advances anticipate" - they're anticipating, Leo, they're not reactive, they're going to get there ahead of time - "future cyberthreats, such as increasing digital attacks on identity systems.  They also address how we will continue to secure security foundations necessary for the AI era and beyond.  In the spirit of transparency and to emphasize the importance of this moment, we are sharing the internal email sent earlier about our Secure Future Initiative's strategy and objectives."



So, wow.  We're getting a peek inside, under the covers, if you would.  So I have the link to the entire piece in the show notes for anyone who is interested.  Mostly I have to say it's a marketing piece, you know, blah blah blah, we're leading the way toward a more secure future, improving the lives of our customers in the face of rapidly growing cyberthreats, blah blah blah.



And I normally wouldn't have given this a second thought, nor even be mentioning it here, except about 20 - I did read it.  That's how I know I can accurately summarize it with, you know, as I just did.  But about 80% of the way down, something appeared that did seem worth sharing.  They wrote, as part of their "Secure Future Initiative," where they're going to get ahead of the bad guys, and they're anticipating future cyberthreats, they said:  "To stay ahead of bad actors, we are moving identity signing keys to an integrated, hardened Azure HSM" - you know, a Hardware Security Module - "and confidential computing infrastructure.  In this architecture, signing keys are not only encrypted at rest and in transit, but also during computational processes as well."  What a concept.  "Key rotation will also be automated, allowing high-frequency key replacement with no potential for human access whatsoever."



Okay.  So in short, Microsoft will be leading the way into a Secure Future - that's capital S, capital F - by working to catch up with what everyone else who cares about security has been doing all along for many years already.  As I observed a couple of months ago, the only possible way they could have lost control of that private signing key during a system crash and the subsequent RAM snapshot that it took, was if that private key was in RAM at the time of the crash.  And the only way it could ever have been in RAM was if signing was being done outside of an HSM.  Now we learn indeed it was because, wow, they're going to move to the future, Leo.  They're going to lead the way by, you know, I mean, I have them on my various computers.  But okay.



During last week's "Citrix Bleed" podcast, we examined, as we know, a crisp and clear example of a bug which allowed for the exfiltration of RAM.  And as it happened, that RAM contained active and valid authentication tokens.  We don't know whether they needed to be lying around in RAM.  It's quite possible that they did need to be there in order to remain valid.  But it's very common to make the mistake of leaving sensitive information lying around even after it's no longer serving a purpose.  The problem is that our current programming languages are still not secure by default or design.  So they must be made - so making our systems secure while using these insecure by default languages is a deliberate act.  It must be a deliberate act.



And I also noted last week that while I was coding the SQRL client I was in a more or less constant state of terror that I was going to make a mistake.  And I would submit that that's the state you want your coders of secure systems to be in.  You know, they should not just be distracted and worrying about when is lunchtime.  They should be terrified about the code that they're writing.



So a great and useful concept and phrase is the notion of multi-layer security.  The idea is that there is no single point of failure that would result in a security compromise.  In order for security to be compromised, many things would need to go wrong all at once.  In the case of the Citrix Bleed that we talked about last week, if the system's RAM could have been swept clean of valid tokens - and we don't know whether or not that would have been feasible.  But if it could have been, then even in the face of that very clear and clean coding error, valid tokens would not have been available for exfiltration.  In other words, wipe RAM, not because you know you need to, but because doing so would add an additional layer of security.  And additional layers of security, as long as they don't get in the way, are never a bad thing for a secure system to have.



So back to Microsoft's case.  In that almost hard to believe case of Microsoft's loss of their private signing key, they explained that they did indeed already have multiple layers of security in place.  I think it was, like, five of them.  Yet in a bizarre and quite unlikely seeming chain of failures, where it was necessary for every one of these layers to fail, every single one of them was actually needed.  Yet they all collapsed at once.  If they had had one additional layer of using a Hardware Security Module in their system, which now they're boasting about doing, then none of those high-value government email accounts would have been breached as a result of the failure of every other layer of security.



So our takeaway here is, it is truly not possible to have too many layers of security.  You never know which one of those layers will be the layer that stops the bad guys.  And, you know, I've often talked about it, actually I will a little bit here a little bit later, the asymmetric challenge that security faces because secure systems cannot afford to make a single mistake because any opening allows the bad guys in.  Whereas the bad guys, all they have to do is find one.  Well, that's offset, that asymmetry is offset if you're able to layer your security, if you can design the system to be multilayered.  In that case, you could have some mistakes, and the bad guys still can't get in.



Okay.  Unfortunately, not all mistakes can be qualified as mistakes.  Last Friday, the day following Microsoft's big "Secure Future Initiative to advance security engineering" announcement that I just talked about, we learned from BleepingComputer that Trend Micro's Zero-Day Initiative had informed Microsoft on September 7th and 8th of four new zero-day vulnerabilities they had discovered in Exchange Server, one of which allowed for remote code execution.  Microsoft acknowledged the reports, but decided that the flaws were not severe enough to warrant immediate attention and decided to put off the fixes until some later unspecified date.  In other words, "Thank you, now go away."



Since ZDI strongly disagreed with this response, they decided to publish the rough descriptions, and actually the rough locations, of the four vulnerabilities under their own tracking IDs in order to at least warn Exchange admins about the security risks, even though unfortunately there's not much for Exchange admins to do about them at this point except to worry more than they already are.



Now, if something about this overall scenario seems familiar, where security researchers inform Microsoft of flaws that they have found somewhere which they believe are important, and after presumably examining those reports Microsoft decides that the problem is not worthy of their attention, you would be correct.  We've been right here before.  And if history continues to repeat itself, we also know what lies ahead.



Microsoft will leave this unpatched, some bad guy somewhere will pick up on the possibility of an outstanding unpatched remote code execution vulnerability in Exchange Server, and they will go a-hunting.  Some time later, Exchange Servers will start being compromised in some mysterious new way that no one ever saw before.  Except that, whoops, Trend Micro and Microsoft both saw it in September of 2023, and one of the two of them who could have done something to prevent it chose not to.  Like I said, we've seen this whole thing play out before, and it's a shame.



So ZDI-23-1578 - that's their own tracking terminology - they said is a remote code execution flaw in the "ChainedSerializationBinder" class, where user data isn't adequately validated.  This allows attackers to deserialize untrusted data.  Successful exploitation enables an attacker to execute arbitrary code as "SYSTEM," the highest level of privilege on Windows.



So now our would-be bad guy also knows right where to look in Exchange Server.  And surprise, surprise, the problem is deserialization.  We've talked several times about the inherent difficulty of deserializing data securely.  The process of "serializing" data takes some sort of formatted data structure, often a JSON structure, and turns it into a "blob" for storage or transmission.  That's the thing known as serialization.  The reverse process of deserializing the blob requires, yes, the interpretation of the data that the serializer produced.  Interpretation.  So we have some flaw in an interpreter in the ChainedSerializationBinder.  Probably wouldn't be too difficult to find.



So just for the record, though they are not also remote code execution flaws, the remaining three are still some concern.  We've got 1579, located in the "DownloadDataFromUri" method.  This flaw is due to insufficient validation of a URI before resource access.  Attackers can exploit it to access sensitive information from Exchange servers.



Then there's 1580.  This vulnerability in the "DownloadDataFromOfficeMarketPlace" method also stems from improper URI validation, potentially leading to unauthorized information disclosure.  And finally, 1581 is present in the CreateAttachmentFromUri method.  This flaw resembles the previous bugs with inadequate URI validation, again, risking sensitive data exposure.  So they all allow, those three, for some sort of unspecified information disclosure.  While it's not running the attacker's code, which has been remotely supplied, which is what the first of these four can, information leakage can still be very valuable to attackers as part of a larger campaign.



The mitigating factor behind all four of these vulnerabilities is that they all require authentication.  You need to be able to sign in as a user to this Exchange Server.  So this may be the basis for Microsoft's dismissal of this as anything to worry about.  But we've seen cybercriminals have repeatedly shown that they have many ways to obtain Exchange credentials.  There's brute-forcing weak passwords, phishing attacks, purchasing them outright on the dark web, or acquiring them from info-stealer logs.  So once the bugs are found, the need for a credential for a specific Exchange Server might not pose an insurmountable problem.



Trend Micro's Zero-Day Initiative folks said that the only salient mitigation strategy would be to restrict interaction with Exchange Server.  But what are you going to do, unplug it?  Many businesses and organizations cannot operate without access to their Exchange Server.  So anyway, I'm just putting this out there.  We'll see here in the future whether Microsoft decides to slip some fixes into a forthcoming update, or whether the bad guys decide to do some reverse engineering of those now specified functions in Exchange Server, find the vulnerabilities, then arrange to get themselves an authentication onto Exchange Server and then get up to some mischief.



We will basically see whether we - and all of the history is going to repeat itself where Microsoft said, oh, no, nothing to see here, until there was.  This is what happened with that horrible print server nightmare that we went through a couple of years ago where the researcher that found the vulnerability kept trying to tell them, over and over and over, look, this is a problem.  You didn't fix it yet.  Then when they said they did, it turned out they didn't.  And then it ended up really coming back to bite them.  So we'll see.



LEO:  Deny, deny, fix.



STEVE:  Yes, exactly, yeah.  And, you know, what's the recourse?  None.  The licensing agreement says, you know, if it works, great.  If it doesn't, well, we tried.  And again, you can't go anywhere else because Microsoft, you know, no one could argue that they're not a monopoly and that they don't have that power today.



There is a problem, not in Microsoft's camp, with something known as Apache ActiveMQ servers.  They've been having some trouble recently, and unfortunately they are this week's mass casualty event, although because there's a much lower level of deployment of them...



LEO:  I like how you go "this week's mass casualty."



STEVE:  I know.



LEO:  So depressing.



STEVE:  This week's mass casualty event.  Okay.  So Apache ActiveMQ Server is a standalone message broker server which facilitates reliable high-availability messaging among clusters of computers.  It's written in Java, and it's been around and evolving since 2004.  So it's got some lineage there.  The flaw, being tracked as CVE-2023-46604, is a maximum severity bug in the ActiveMQ scalable open-source message broker, that is, this server, which enables unauthenticated attackers to execute arbitrary shell commands on vulnerable servers.  In other words, one of those as bad as she gets.



It's unclear when the Apache Foundation became aware of the attacks on ActiveMQ.  But two security firms, Arctic Wolf and Huntress Labs, found that threat actors had been exploiting the flaw as a zero-day to deploy a remote access trojan known as SparkRAT since at least the 10th of October.  Apache released security updates to fix the vulnerability 17 days later, on October 27th.  So 2.5 weeks of window, and we don't know how much earlier this was being done.  But we do know that a patch being made available and patches being applied are two very different things in today's world.



In addition to the deployment of the SparkRAT remote access Trojan, ActiveMQ servers exposed to the Internet are being targeted in "HelloKitty" and another ransomware known as "TellYouThePass" is the name of the ransomware.  So those two pieces of ransomware do share a common infrastructure, email addresses, cryptocurrency addresses, and so forth.  So they're probably just versions of the same thing, or at least being run from the same ransomware group.  And in terms of its spread, data from the threat monitoring service Shadowserver found that there are currently more than 9,200 Apache ActiveMQ servers exposed online, with over 4,770 - so more than half - currently vulnerable to exploitation.



So as I said, not as mass, as some of the masses that we've seen recently.  But still not good.  If the good guys can scan the public Internet to obtain a count of victims, bad guys can scan for potential victims to target just as easily.  Needless to say, if your organization is using an Apache ActiveMQ Server with Internet exposure, you'll want to update it immediately, and also look around for any indication that the bad guys might have already entered your network and have set up some sort of persistence because you don't want that.



Okay.  Given the sweeping scope of the mess with Citrix Bleed, which we spent some time looking at last week since, well, we talked in detail about the exploit, and that occurred on Halloween.  The team at Mandiant, which is now a Google property, appears to be more on the ball and current about this than anybody else, since everybody else is just citing Mandiant's research in their own updates.  Their last update was from last Thursday, which has added some interesting new pieces of information and helps to bring home, I think, the reality of the situation that is facing those 20,000-plus Citrix users whose network appliances have already been compromised.  Not vulnerable, but compromised.



So on Thursday, Mandiant wrote:  "Mandiant has identified zero-day exploitation of this vulnerability in the wild, beginning in late August 2023, as well as n-day exploitation" - meaning after it's been known, you know, and is larger than zero - "exploitation after Citrix's publication.  Mandiant is investigating multiple instances of successful exploitation of CVE-2023-4966 that resulted in takeover of legitimate user sessions on NetScaler ADC and Gateway appliances.  The session takeovers bypassed password and multifactor authentication.  In this blog post, we will discuss artifacts that can be used to identify exploitation activity and highlight some of the post exploitation techniques we observed during the incident response investigations."



Okay.  So they lay out what's already known about the vulnerability of the Citrix endpoints, the challenges of investigating vulnerable devices because the web server running on the appliance does not record requests, or errors, to the vulnerable endpoint.  So there's no log of these things being made, making tracking them down extra tricky.  And they note that they're not aware of any configuration change that can be made to force request logging for these particular endpoints.  So, you know, those remote HTTPS queries.  But what was most interesting, I thought, and the reason I wanted to share this update, was what they had to say about the post-exploitation activity they observed.  In other words, what are some of the things the bad guys do once they bypass the system's authentication and gain access by grabbing one of those pre-authenticated tokens and then just using it?



So Mandiant explained.  They said:  "Following the successful exploitation of 4966, Mandiant has observed a variety of post-exploitation tactics, techniques, and procedures (TTPs).  Once an actor was able to successfully achieve session hijacking, the threat actor performed actions including host and network reconnaissance of the victim's environment, credential harvesting, and lateral movement via RDP," you know, remote desktop protocol.  "Mandiant identified evidence of Active Directory reconnaissance using living-off-the-land binaries such as net.exe."  As we know, "living off the land" is now an increasingly popular approach, meaning you don't need to bring any stuff with you, you just use this rich environment of command executables that Windows now ships with, net.exe.



LEO:  It sure meant something different when I was a kid.



STEVE:  That's right.



LEO:  Wow, I love it.



STEVE:  Living off the land.  "Additionally, Mandiant has observed the use of the SoftPerfect network scanner (netscan.exe) to perform internal network enumeration.  In several cases, the threat actor used 7-zip to create an encrypted segmented archive to compress the reconnaissance results."  Because, you know, you don't want all your reconnaissance results to be too big.  So you use an archiver like 7-zip specifically, to shrink them down.  And boy, you know, those kinds of logs are typically going to shrink way down.  "The threat actor then used the built-in certutil utility to Base64 encode the segments."  So it's not just, you know, ASCII going out.  It's somewhat obfuscated.



"In one case, certutil was used to decode multiple files related to credential theft.  Mandiant observed the threat actor use e.exe to load d.dll into LSASS process memory.  When run, the utility creates a memory dump file located at temp\1.png."  It's sort of interesting.  You give a binary memory dump a .png extension so that it looks like a known extension, and of course PNGs are binary, so presumably they just kind of get passed without much concern.  That's interesting.  Anyway, and prints success to the console when done.  That's nice, so the bad guys know that everything worked fine.  The memory dump file can be processed offline by the threat actor to extract credentials, that is, credentials from the LSASS process memory.  Mandiant identified sh3.exe as a utility suspected to run the Mimikatz LSADUMP command.



In another instance, a threat actor used certutil to decode a file that Mandiant identified as a newly tracked backdoor that uses Slack as its command and control.  Tracked by Mandiant as FREEFIRE, it is a lightweight backdoor written for .NET.  FREEFIRE communicates to a hard-coded channel, a Slack channel, to retrieve commands and upload responses.  It supports loading arbitrary .NET assemblies encoded as Base64 sent to it via chat commands.  Mandiant observed FREEFIRE being deployed by a threat actor through the following certutil command.  And then they go into it in more detail.



They've also observed the deployment, they said, of various remote monitoring and management tools following the successful exploitation of 4966.  Currently, Mandiant has observed the deployment of Atera, AnyDesk, and SplashTop to establish and maintain a foothold following exploitation of 4966.  They said:  "Mandiant is investigating intrusions across multiple verticals, including legal and professional services, technology, and government organizations.  Given the widespread adoption of Citrix in enterprises globally," they wrote, "we suspect the number of impacted organizations is far greater and in several different sectors.



"Mandiant," they said, "is currently tracking four distinct uncategorized groups involved in exploiting this vulnerability.  We have observed some lower degrees of confidence overlaps in post-exploitation stages among these UNC (uncategorized) groups, like using the same recon commands and utilities available on Windows.  Two threat clusters used Mimikatz for dumping process memory.  Notably, there were no overlaps in infrastructure between these clusters of activity.  The exploits were sourced from different VPN provider IP addresses and previously compromised third-party devices."



Okay.  So even though the attack is low complexity, easy to pull off, easy to launch, all indications are that well versed and very competent threat actors are behind these.  They're using tried and true post-exploitation tactics to obtain a high degree of leverage in and persistence on their victims' networks.



So we are now inhabiting a world where the moment a patch to fix a remotely exploitable flaw is announced, powerful malignant forces jump on the patch, determine what was changed, design an exploit for any not-yet-patched devices, then race to take advantage of the newly discovered vulnerability, using it against anyone who did not instantly patch their devices the moment the trouble and its fix were announced.



Asymmetric warfare is notoriously difficult to fight.  And this currently broken security model, which is the only thing we can call it, it is a currently broken security model, has these asymmetric aspects.  Consider that a small group of miscreants only need to watch for security updates from the major appliance vendors.  Yet on the other side, on the receiving side, every single person who is independently responsible for the operation of every deployed instance of every one of those devices spread anywhere in the world must be just as vigilant as that small team of bad guys.  And on top of that, everyone everywhere must be ready to apply the fix at any time - night, day, weekend or holiday.



The only way I can see this evolving is for the high-end enterprise appliance world to make the same move that the small office/residential router and consumer desktop world has made of allowing these devices to be remotely autonomously updated without the need for the device's IT personnel to be involved.  This feature should be enabled by default with IT personnel having the option to disable it if they understand and accept the risks that accompany doing so.



UDP packets are small, they are connectionless, and inexpensive to send.  So every such device that has not been disabled could send a packet periodically to the device's manufacturer to check in for any updates.  One tiny packet every 10 minutes would be more than sufficient.  You could make it hourly if you wanted.  But, you know, 10 minutes to be on the safe side.  In the event of a critical update, an affirmative UDP reply would contain the URL of the update to download and apply, and the certificate of the remote web server could be pinned to prevent any forgery.  The appliance would bring up an HTTPS TLS connection to download the updated module, install it, and reboot itself.



And of course I'm aware of the many arguments against this sort of autonomous upgrading.  Its first appearance in Windows all those years ago caused quite a stir. You know, those old-timers among us were like, what?  What a minute.  Wait, we don't want this to be automatic.  We want control of which security things we install.  We want to look over the list before we say, okay,  yeah, fine, do it all.  You know, my own Unix servers send email to inform me of the packages that are in need of attention.  This information they're obtaining without any assistance from me, although they do stop short of performing those changes autonomously.



So while autonomous patching of enterprise-class appliances may pose some risk, more than 20,000 users of this one device just had their networks deeply compromised because, for whatever reason, they did not install the patch that the bad guys were reverse engineering before that reverse engineering was turned into an active exploit.  If they had, 20,000 individual network compromise disasters would have been averted.  It seems to me that given the world we live in now, it is time to move autonomous patch updating from the consumer desktop and router - where it's now been proven to be providing much more benefit than harm - to the enterprise's border equipment which are subject to swift attack, as we've all just been seeing with actually the past three, we're now at three recent mass casualty events.



And just for the sake of discussion, there are many possible compromise measures.  For example, the periodic UDP packet sent by the device back to its manufacturer could contain the device's current build version and the latest current email address and cell phone number for the organization's IT cybersecurity team.  That information could be configurable in the device's admin setup as an "in case of critical vulnerability, send email to and send a text to."  That way, every one of the manufacturer's devices is pinging home base with the information needed to alert its administrators the instant any new and sufficiently urgent problem is discovered.  It's difficult to believe that we're not already doing this as an industry.



And while we're talking here about any of this, since it's foreseeable that the first thing a compromised device might do is shut down that early warning update system, the device's manufacturer should have these periodic info pings continually updating a database, which would also prevent malicious changes to that information by retaining a history of previous contact information.  In that way, the moment a serious problem was discovered, every admin could be made aware that they'll need to prepare for an update.



So I suppose my point is, we are really truly being lame about the way things are being done today, and I can't see any excuse for it.  Like we have the technology to solve this problem and to prevent what are now becoming weekly multi-tens of thousands of networks being compromised in mass casualty events.  I would argue that that Cisco, which was a web auth problem, should have never happened by policy.  But again, the technology to fix this is at hand.



LEO:  On we go.



STEVE:  We've just seen, and we continually see, the burden being placed upon frontline IT personnel to keep their networks safe.



LEO:  Yes.  It's a canary; right?



STEVE:  Oh, my god, yes.  When I talked about ADAMnetworks, it's not a job I want.



LEO:  No, no.



STEVE:  And if someone were to say, given all the evidence we've seen, that it's basically an impossible task, it would be difficult to mount a convincing counterargument.  As always with security, the good guys must prevent intrusion everywhere, all the time, all at once - which I think was the title of a recent movie.



LEO:  "Everything Everywhere All At One," yes.



STEVE:  That's right.  But the bad guys only need to find one mistake, anywhere, one time.  You know?



LEO:  Deep respect for the people who do this. 



STEVE:  It's not fair.  It's not like a fair job.



LEO:  No.



STEVE:  We've heard that IT security guys are stressed, and it's not surprising.  I've mentioned this before, but I'll say it again.  The job might not be for everyone.  But if it sounds like it's a fit with your personality, the good news is the world is desperately looking for you.  I saw a statistic recently indicating that there's about a 50% shortfall in IT security staffing.  Something like four million empty job openings right now that need to be filled.  One of the many things I've learned from our listeners is that they credit their listening to this podcast with giving them the inspiration to learn more about this subject which subsequently allowed them to move into the cybersecurity job market.



And I found that study.  A quote from the study says:  "The global cybersecurity workforce is estimated to have reached more than 5.5 million professionals.  And even though that number is 9% higher than it was last year, four million experts are still needed worldwide to fill open positions across the industry."  Okay, now, what I found is the 2023 "Cyber Workforce Study" in which they surveyed a record 14,865 cybersecurity professionals to share their unique perspectives on the state of the workforce.



I'm tempted to share more since this report is chock full of interesting data and statistics.  But instead I have a link to its 84-page PDF in today's show notes, and it is this week's shortcut of the week, which I again numbered very carefully.  So it's grc.sc/947.  That will bounce you to this PDF of the 2023 Cyber Workforce Study.  And really, as I was scrolling through this, I thought, oh, this is just full of cool stuff.  So I really do commend our listeners take a look at it:  grc.sc/947.



Okay.  So where am I going with this?  The job of controlling our networks, keeping them secure, is chaotic.  The challenge, as I said, is highly asymmetric and arguably unfair.  So the overworked cybersecurity professional needs all the help they can get.  In a world of software vulnerabilities, how does one know how to start the day?  One of the blessings this industry has created in an attempt to bring some form of order from this chaos is the Common Vulnerability Scoring System, which we are constantly referring to on the podcast, you know, CVSS scores.



The initial version 1 of the CVSS was instituted in February of 2005, the same year when we later began talking about these interesting issues every week.  A little over two years later, in the summer of '07, it moved to version 2, where it sat until version 3 was introduced in 2015.  That version lasted until 2019, when it was tweaked a bit to give us version 3.1, which is what we've been using up until now.



The reason the CVSS system has needed periodic maintenance and updating is that we're not living, as we clearly know, in a static world.  The drama we're seeing playing out this instant, for example, with the Citrix Bleed vulnerability and the Apache message queuing problem, that didn't exist to nearly this extent before cryptocurrency because it was far less clear how attackers in Russia and North Korea could monetize their cyber intrusions.  Now everybody knows.  And vulnerable devices like Citrix's NetScaler technology hadn't yet been created.  Since today's cyber landscape has changed, so must the metrics we use to characterize today's threats.  To that end, work has been completed just now on the next generation of Common Vulnerability Scoring System, and we are therefore now at CVSS version 4.



So what's changed?  There are four primary highlights.  First, CVSS scoring metrics have been added and redefined to improve the granularity and clarity of CVSS scores.  With the previous standard, it turned out like against today's threats it was common to have different types of vulnerabilities winding up being clumped around the same score, even though it no longer accurately reflected each one's severity.  So more scoring metrics in CVSS v4 means a better spread across the entire scale.



Second, in keeping with today's world, we now have ICS, OT, you know, operational technology, and IoT-specific scoring metrics.  This includes scoring metrics such as "Safety," "Automatable," and "Recovery," to let critical infrastructure operators know whether a security flaw just looks bad on paper, or if it's actually exploitable and dangerous to their networks.



Third, we also have new scoring metrics such as "Value Density," "Vulnerability Response Effort," and "Provider Urgency."  Those have been added to help responders evaluate and prioritize vulnerabilities.  Those last two, for example, "Vulnerability Response Effort" and "Provider Urgency," are intended to allow vendors to tell customers that a vulnerability needs to be patched ASAP.  This is a capability that was not present in the current CVSS.  And obviously having a "patch this ASAP" and have that actually mean something is something we need.  So we now have it in version 4.



And finally, CVSS version 3's "Temporal" metrics group has been replaced with a new group called "Threat Metrics."  Although this replacement group is intended to reflect the same exploitability and proof-of-concept availability as its predecessor, its application is significantly clearer now under version 4.



So while we'll still be seeing and quoting the same single 1-to-10 CVSS composite score, that score will now more accurately track the urgency presented by its vulnerability, and the detailed breakdown of that single score will provide cyber security professionals much needed additional details and may help them to decide how to start their day.



And a perfect example of this, I think, is that we keep seeing all of these 9.8s.  Well, what?  Why?  How is it that everything is 9.8, except every so often, you know, a complete disaster meltdown is a 10.0.  You know, it did feel like there was some nonlinearity or something to the way the CVSS v3.1 that we've been using for the last four years had been operating.  I expect we're going to see a different scale of CVSSes.  And so we should be prepared also not to apply the numbers coming from version 4 against what we've been used to seeing in version 3 and 3.1.  They may look like they're less severe.  Probably what they're doing is doing a better job of, like, reflecting a non-clumpy, more uniform scale so that, you know, the bad ones are probably more rare, but what they signify is probably more significant.  So, yay.  That's all for the best.



LEO:  It's inherently, though, subjective; right?  There's no - or do they have some weird objective criteria?



STEVE:  No, they actually have a calculator where you go to a website, and you say yes, no, yes, no, yes, yes, no, no, yes, and so forth.  And then it gives you the score based on a rigorously pre-established formula.  Yeah.  So it's not just oh, my god, you know, this one made me nauseous.  There's actually math behind it.



LEO:  Oh, okay.  And the people, and the yes or no questions are concrete.  They're not, well, was this really bad?



STEVE:  Yes.  They are, they're concrete.



LEO:  Okay.



STEVE:  And very specific.  And there's actually, if you go to the central CVSS repository, there is a breakdown, a multidimensional breakdown of, like, you know, which are the ways in which this thing is bad that all worked together to create this composite score?  So, you know, there's actually a lot of science behind it.



LEO:  Okay, okay.



STEVE:  Okay.  So I have a soft spot in my heart for Ace Hardware, Leo.



LEO:  Me, too.  We love our Ace Hardware.



STEVE:  Yes.  We have several of Ace's 5,700 retail stores in our area, and I have to say they have a truly amazing array of random little hardware bits.



LEO:  Yeah, there's always some guy in suspenders with a walrus moustache.



STEVE:  Oh, he's wonderful.



LEO:  And you can say "My faucet's leaking," he's oh, yeah, come over here.  Yeah.



STEVE:  Yeah, he kind of shuffles along and brings you to...



LEO:  Yeah, you've got him, too.



STEVE:  Oh, yeah, yeah.  I think we're all sharing him.



LEO:  Yeah.



STEVE:  So anyway, many of my own projects were saved right, you know, like in the middle of the day when I needed a particular size bolt or washer.  You know, there are still some things that are difficult to do online.



LEO:  Yeah.



STEVE:  I would argue, you know, trying on clothes is difficult, and getting exactly the washer that you need to fit in a tight space, or how many of them stack up in order to create a shim of the required size.  So anyway, when you need to match a bolt to a nut, there's no substitute for being there.



Anyway, I bring this up because nearly 200 of their servers -  yes, we know where this is going - and 1,000 other systems were hit by a cyberattack the day before Halloween, so that was last Monday, October 30th.  The attack impacted their ability to pick up new customer orders.  And also other impacted systems included their warehouse management systems, reward points tracking, their tech support call center, and the company's mobile assistant.



Despite the attack, the company's 5,700 retail stores have remained open, although with somewhat reduced activity.  And I haven't needed any bolts recently, but it's good to know that I can still get them there.  And we did actually lose one Ace retailer, the one that was closest to me, I think it was as a consequence of COVID.  It never recovered from the real slowdown.  But we still have one that I know I can get to if I need a specific thing.  You know, your store probably has it, too, Leo.  It's - I can't remember the name.  It begins with H.  It's like Hildebrand or HY-Guard or something.  But they have like, in mine, they have multiple aisles of identical, like, slide-out trays.



LEO:  Yeah, yeah, bins, yeah, yeah.  With all the different stuff, yeah, yeah.



STEVE:  Yeah, yeah.  And so - yes.  And it is all provided by one company that, like, stocks all those things.



LEO:  They always have the one you need.



STEVE:  Yes.  It's amazing.  Even if it's some, like, backdoor spring collapser or something.  It's like, my god, there it is.



LEO:  The one we had burned down about 15 years ago, burned to the ground, and it was on a big deal, it was like on the Fourth of July.  But they've rebuilt it.  But it was really nice because it had creaky old floors.  But they kind of preserved the spirit of it.  I love old hardware stores.  I have a lot of stuff from that Ace.  They probably also have a lot of my stuff in their database.



STEVE:  Yeah.  The best extension cord I've ever found is like this supple, I don't know, it's got to be like highly braided.  It's just amazing.  It just feels so good.  Anyway.



LEO:  Yeah, it's fun.



STEVE:  Enough of that nonsense.  Remember that bizarre plan Google floated a few months ago, which would have given websites absolute control over the extensions and other features that could be used by anyone visiting a website that wished to impose such control and restrictions over their browser?



LEO:  Is that the Web Integrity API that they did?



STEVE:  Yes, exactly.  And it was dubbed the Web DRM because, you know, nobody likes digital rights management.



LEO:  Right, because that's what it was really.  I mean...



STEVE:  Yes, it really was.  The good news is that plan is dead.



LEO:  Oh, good.



STEVE:  And I'm impressed that Google didn't make a larger push for something that really didn't seem to be in the best interest of the end user.  We talked about it a bit briefly, but there wasn't enough known at that point to really take it very seriously.  The only upside, like the only reason I could see that it might be useful was that, as we know, users are not very judicious in their choices of browser extensions.  So you could imagine like a banking site, for example, wishing to enforce much tighter security when people visited some of its more secure services.  So I could imagine that.



But the downside was that, for example, sites might just decide to restrict the use of ad blockers by disallowing their use.  We know they don't like them.  So why not just make them not work on their site?  So anyway, my hope is that Google did not kill this because they have figured out some better way to do something similar because this really seems bad.  And if they did, I would hope that Mozilla would not choose to follow with Firefox.  So we'll see.  But for now, it's not going any further.



Okay.  We were just talking about the first of the three recent mass casualty events being the Cisco IOS XE router attack.  And that was the one which preceded the Citrix Bleed mess.  And as I mentioned before, it was the result of an easily preventable web authentication bypass which, as I ranted at the time, was entirely foreseeable and unnecessary because no web UI administration should ever be placed on the public Internet.  We all know better.  Everybody should know better.  Cisco should know better.  It shouldn't even be an option since there are now many far more secure ways to do the same thing.  And as I was putting this together for the podcast I came up with a new slogan because, you know, it's there because a web UI is so easy to use.  So "Ease of use is no excuse."  So, yeah. 



Anyway, this all popped back up because Cisco's own Talos group just published a full technical analysis of what they call "BadCandy."  It's the implant that's being deployed on those compromised Cisco routers thanks to those zero-day vulnerabilities.  In their report, Talos notes that the BadCandy malware has evolved, now in its third major version, demonstrating that the threat actors behind this are still actively modifying their attacks to maintain access to those compromised boxes.  And remember, there were thousands, tens of thousands of them.



They also noted that the latest version 3 modifications which have been made to BadCandy appear to have worked, since the Shadowserver Foundation who had been monitoring the attack's progression over time has stopped detecting any infected systems, although we know 100% for sure that they're not all patched.  Presumably, the Shadowserver Foundation and others, other security researchers, had long ago captured the IP addresses of the infected and vulnerable systems.  But no one coming along now would see that anything was amiss.  Although in fact I'm sure it's just a cornucopia of compromised networks.  Probably the big problem now is just sorting through them all and deciding which ones to go after first.



Now, I know that before this next bit of news I'm supposed to remind our listeners that Bitwarden is a sponsor of the TWiT network, as if we weren't all already aware of and pleased with and even grateful for that fact.  So what's the news?  With release 2023.10.0 of the Bitwarden browser extension, it now fully supports FIDO2-style Passkeys.  Bitwarden's mobile clients have not yet caught up, but this is acknowledged, and it's on their development roadmap.  Meanwhile, the browser extension appears to be ready for prime time. I have a link in the show notes to the full 2023.10.0 release notes and another link to the specific page they've got there now discussing Bitwarden's browser extension support for Passkeys.  I've not tried it myself, but from a quick scan of that page it appears that everything is there.



At the bottom of the "Storing Passkeys" page they have a short Q&A, I think it just had three FAQ points, where one of the questions, the middle one, asked:  "Are stored passkeys included in Bitwarden imports and exports?"  To which they reply: "Passkeys imports and exports will be included in a future release."  So that's not there yet.  But they clearly recognize the need.  And as we noted when we recently talked about this, apparently the slow-moving FIDO group are involving themselves in the creation of the import/export format.  Although that's making us wait, we definitely want all Passkeys clients everywhere, everyone's Passkey client, to support a single common, well-designed, unified cross-platform standard.  So I think we all should be quite happy to wait for that.



Okay.  Recall the tweet from a listener named Victor from a few weeks ago.  His Twitter DM was dated October 18th.  In it, he wrote, he said:  "I powered on a couple years old desktop that had been unpowered for about a year.  It took ages before the desktop was loaded, no errors anywhere, but I decided to try your ReadSpeed.  And look at those SSD speeds!" exclamation point, he wrote.  "Is it time to invest in SpinRite now?"  He said:  "If SpinRite fixes this, I will try to encourage my employer to get a site license," meaning just purchase four copies, and then you can use it on all the machines there.  He said:  "Thank you, Mr. Gibson.  Victor, long-time SN listener.  Keep up the good work.  To 999 and beyond."



Okay.  He attached a screenshot from ReadSpeed which I described at the time.  It showed and explained exactly what he was describing.  You know, ReadSpeed takes a benchmark, read performance benchmarks at five locations across the drive - the beginning, the one-quarter point, the middle, the three-quarter point, and the end.  And what you'd expect on any SSD, the reason I did these five snapshots was that we all know that spinning hard drives are slower at the end because that's the inner tracks where there's less data since the drive is spinning at a uniform speed.  If you've got less data around the circumference, your transfer rate is going to have to be much lower.  Typically it's like half the speed as the beginning of the drive, the outermost circumference.  So I designed ReadSpeed just to take, you know, in order to sample those five points.



To our stunned surprise, we discovered many people's SSDs were slower, much slower at the beginning of the drive, even though an SSD being solid-state you'd expect it would be uniform across all five snapshot points.  Not so.  And in Victor's case, he had an extreme case.  Remember that the beginning of his drive was 2.2MB per second.  The 25% point was 482.5.  The 50% point was down to 53.  The three-quarter point was also bad at 13.8.  And the very end was 323.7.  So this drive is like, across it is, like, bad.  And in fact it peaks at 482 at the 25% point for who knows why.



Anyway, yesterday, just on Monday, yesterday, when catching up with my Twitter feed for the podcast today, I found his follow-up.  He did purchase a copy of SpinRite 6.0, then used his 6.0 license to immediately grab the 6.1 release candidate.  Here's what he wrote.  He said:  "Now, Mr. Gibson, I have some results for you, and possibly the listeners.  I ran SpinRite 6.1 on Level 4.  It took 28 hours, reported 6,199 command timeouts, found and repaired 183 defective sectors."  Remember this is on an SSD.



He said:  "For comparison's sake, here is a new screenshot of ReadSpeed.  Now the PC behaves like one would expect from an 8-core Intel i9700 with 64GB of RAM."  And so we have that.  Where the beginning of the drive was at 2.2MB per second, it's now 430MB per second.  The one-quarter point didn't change.  The middle went from 53.2 up to 508MB per second, like almost 10 times faster.  The 75% point went from 13.8 to 504.  And the end of the drive went from 323 to 513.8.  So much more uniform and way faster.  And also this SSD had a ton of problems.



Okay, now, under SpinRite 6.1, a read scan of a half terabyte directly connected, meaning not USB, but directly connected, this is the drive his system boots from, would have taken less than an hour.  We can easily do half a terabyte an hour and even faster on an SSD.  But rescuing and resuscitating that very sick SSD required rewriting its recovered data.  And actually all of its data.  So that would slow things down.  But not by nearly that much, probably only, you know, it would take an hour or two normally to do a Level 4 pass on an SSD.  But that SSD was clearly in seriously bad shape, and it sounds like it made SpinRite work a lot to pull it all back from the brink.



So SpinRite 6.1 is obviously highly effective for today's solid-state storage, in addition to what it has always been able to do for electromagnetic spinning drives.  What we've learned is that it turns out that electrostatic storage is prone to long-term charge degradation through several different mechanisms.  And this only promises to become worse as engineers continue to succumb to pressure from their managers to squeeze ever more data into ever smaller and fewer storage cells.



The good news is SpinRite v6.1 can resolve those problems today.  It is not as optimal as version 7 will be, but it works now, and I'm not stopping once 6.1 is published.  It's a big step forward, but I've got much more on the way, as soon as I'm able to get away from DOS, which is like going to be a big treat for me.



Okay.  We've got some closing-the-loop feedback.  Sam Miorelli, he said:  "Hey @SGgrc, listening to SN-946" - that was last week - "on IPv6.  Spectrum, the main ISP for Central Florida for Tampa, Orlando, and the Space Coast, IPv6 is still notoriously problematic.  For example, Pixel 3 and later phones go into WiFi disconnect loops when you let IPv6 hit cheap and good routers.  For example, I have a Netgate USA SG-2100.  This is well documented on Reddit.  Outbound DNS queries also frequently have long periods of time when IPv6 is enabled that they simply black hole on Spectrum, then randomly fixed for a while, then bad again, for months." 



He says:  "ISP shrugs."  He said:  "I fear the day we're forced to switch to IPv6 given how terrible the backend tech is maintained for home ISPs."  So he's talking from the ISP side.  And as we know, he was following up on my somewhat pessimistic appraisal last week of what appears to be the true current state of IPv6.  It'll be there when we really need it.  Actually now I would say "hopefully" because it's looking worse than I thought.  But until really, we REALLY need it, is in all caps, bold italics, and underlined, all of the prevailing evidence points to everyone doing everything they can to hold onto IPv4 until there's really no other choice.  I have 18 IPv4 IPs, and I treasure them.



LEO:  Holy cow.  Yeah.



STEVE:  Yeah.  Also, Bob Grant provided some additional terrific feedback about IPv6.  He said:  "Hi, Steve.  As an IPv6 proponent for a number of years, I listened with interest to your answer to reader email last week where you said nearly everything appears to be IPv6 ready.  I thought I'd share my experience.  It is certainly the case that most everyone's routers and many networks are dual stack with both IPv4 and IPv6 support.  In the case of my ISP, I am able to request an IPv6 /48 prefix which is 256 separate /64 networks, so I can have as many as 256 separate networks, each having a full /64 IPv6 subnet.  My firewall only allows established connections back in, so there's no additional security issue over my IPv4 NAT."



Meaning he's observing that IPv6 does not NAT.  So individual systems get their own IP because there's just so many of them, you don't need NAT any longer.  But you do still want firewall functionality.  He said:  "I recently upgraded my network and WiFi access points so it's trivial to segment multiple SSIDs into separate VLANs going back to my OPNsense router."  Okay.  So I'd say that we have indeed at this point established Bob as an IPv6 proponent.



He says:  "As an experiment, I decided to set up an IPv6-only network where only IPv6 IPs and DNS would be used.  I was quite disappointed to discover how few websites worked with IPv6 only.  Huge sites like nfl.com, twitter.com, and many well-known universities whom I won't embarrass by naming, and others like bitwarden.com all fail to load with an IPv6-only connection."  And I'll just note that all of my own servers at GRC are among those, too, which are still IPv4-only.



He says:  "Many other sites work when using www.site.com but fail to redirect when using just http://site.com.  My three credit unions' landing pages load under IPv6, but the financial backends hosting the login process and displaying account balances fail because they're IPv4 only.  Even Microsoft fails after the landing page because the login.live.com is not IPv6 enabled.  I noticed many sites' web pages load only because they use a content delivery network like Cloudflare or Akamai that supports both IPv4 and IPv6 at their border, thus proxying for their clients' IPv4-only web servers.  Kudos," he finishes, "to Google, Amazon, Netflix, Facebook, Stanford, MIT, Harvard, and the federal government for fully implementing IPv6.  I hope some of this is useful.  Bob."



And yes, very useful and very interesting, Bob.  Thanks for sharing, giving us an update on our IPv6 reality check.  So we are not IPv6-ready today, that is, we cannot abandon IPv4, just as we cannot abandon TLS v1.2.  As we learned, only one-third of servers are able to do 1.3 connections, so we still need 1.2.



CLT Cyber Security tweeted:  "Hey, Steve.  As security pros we know what to do, but I'm having trouble explaining why this is the right way to my company.  We have DigiCert TLS certificates for our websites.  We need to keep those private keys secure.  But if our server was ever compromised by a random attacker who obtained our private keys, what could they really do?  Just trying to better articulate this to non-security pros.  Thanks, and to the nines and beyond."



Okay.  The danger presented by the compromise of a server's TLS private certificate is one of impersonation, since it's only the server's sole possession of that private key certificate that allows it to assert its identity.  So if someone else is able to assert that their server is your server, this paves the way to an impersonation attack.  Depending upon who you are, that might be either a big deal or not so much.  If this was a site that really mattered to its visitors in some way, then the consequences could be significant.



However, even though the theft of the certificate may pave the way to such an impersonation attack, there's still a lot of pavement to traverse to pull off a working impersonation attack.  The biggest roadblock to implementing the attack is that the web browsers or other connecting clients who would be spoofed by this need to believe that they are actually connecting to the authentic server.  In other words, they need to look up the IP address of the domain of the authentic server and then send TCP traffic back and forth to the IP that was looked up.



In practice, that means that either the DNS lookup needs to somehow be poisoned to return the attacker's server IP, or the victim's IP traffic needs to be intercepted and redirected on the fly to the attacker's IP.  One way or another, the domain name the client believes it's connecting to must match one of the domains that certificate authenticates.



So either DNS subversion or dynamic traffic interception must somehow be provided.  If that sounds like a high bar to reach, it can be; but it's entirely dependent upon the specifics of who, or what population, is targeted for spoofing.  At the beginning of today's podcast I was talking about layered security.  Here's another example of layering.  Losing control of a website's certificate is not immediately the end of the world, since other layers are still in place to provide some protection.  But having exclusive use of a website's certificate is not a layer you want to give up.



This is why the Internet world went nuts several times in the past during this podcast.  One of those times was Dan Kaminsky's famous discovery of DNS spoofability, which also would not have been the end of the world, though it would have been bad for HTTP without TLS.  Even so, the integrity of DNS wasn't a layer of security that anyone wanted to lose.  And similarly the Heartbleed flaw, that potentially allowed some server web certificates to escape, got everyone's attention big-time because, again, it would strip a layer of our multilayered protection.



And I think that perhaps this also helps to put the never-exploited-as-far-as-anyone-knows Spectre and Meltdown vulnerabilities into a useful light.  It might at first appear that the industry was way over-concerned about what was a purely theoretical vulnerability for which no known attack has ever succeeded.  But again, robust interprocess isolation, which Spectre and Meltdown both threatened, is another layer.  And in today's heterogeneous cloud computing landscape it's a particularly critical layer.  So I cannot think of an instance where having too many layers of security is a bad thing, as long as it's not way over the top and gets in the way.



Craig from Scotland tweeted:  "Was just listening to SN-941" - so he's a few weeks back - "and the part about public key crypto and factoring primes."  He said:  "It got me wondering, how likely is it that there could be collisions in the primes chosen by two different people?  Or would it be feasible to create a rainbow table of factored primes allowing the discovery of the private key using a quick lookup of a public key?"



So that's a brilliant observation, and in the past - that is, the notion of a collision of primes.  And in the past that was discovered to be happening with somewhat horrifying regularity.  There were problems with the quality of some of the early random number generators which tended to choose and then test for primality the same large prime.  So, whoops.  Two or more completely unrelated servers would coincidentally be sharing the same public/private key pairs.  Not due to any collusion between them, well, except for the collusion of them both using the same poor sources of entropy.



What was found to be happening was that the servers were booted and were immediately being asked to produce a certificate.  So the server hadn't yet had time to collect sufficient entropy from the environment, and it could happen that two completely separate servers would both wind up picking the same keys.



And then into this we add the birthday paradox.  That teaches how quickly the number of collisions between pairs of unrelated items increases as the number of possible interactions increases.  There's not a huge danger from sharing the same keys.  But it's certainly not zero.  First, you would need to compare your server's public key with the public key being sent by everyone else's server.  If you did find a collision, since you know your server's matching private key, you now also know the colliding party's private key.  Now, that's not good.  But we already observed that just having that only removes one of the multiple layers of protection needed to exploit any advantage.



The takeaway here is that we don't want to be inadvertently sharing our private key with anyone else.  So the best way to assure that is to be certain that the process which is picking keys is using the highest quality possible source of randomness for its key guesses.



And finally, Chad Cosby says:  "Hi, Steve.  I'm curious if you would share how and how often you run SpinRite on the drives in your Synology NAS."  He says:  "I, too, use a Synology, and it feels like an absurd oversight that I trust my most valued data to an occasional glance at the Drive Health meter within DSM."  Which is Synology's management console.



So Chad, I suppose the question is how much redundancy you're using.  I've never bothered to run SpinRite on any of my RAIDed drives.  I have four-drive RAID arrays everywhere.  Every one of GRC's servers is running four-drive RAID, as are both of my Synology NASes, and my one still-standing Drobo, although I think it has five drives in it.  In every instance I'm running RAID 6.  So that allows me to lose any two drives at the same time without any data loss.  And once not too long ago I was flying a bit nervously with no reserve on one server until I could get two replacement drives for it.  I actually had drives ready for it, but I learned that it would not allow me to mix SSDs and spinning drives in the same array.



So far, I have never lost any data, and I've actually had more trouble with my SSDs than with the spinners.  Some spinners just appear to run forever, and others seem to tire quickly.  They last long enough that I wouldn't really call it infant mortality.  It's more like teen angst.  Anyway, I'm replacing my SSDs now with spinning drives.  And with them being so ridiculously huge and inexpensive I will always be running with RAID 6.  And in that case I welcome any drive that gets tired and no longer wants to play, you know, just let me know, and I'll slip in a replacement.  But, yeah.  As the long-time publisher of SpinRite, I believe in redundancy.  And so I've got as much as I can afford in the available space.



Oh, and I do have two last pieces.  Real quickly, John Carling had a tip for our listeners.  He said:  "Hey, Steve.  Listening to 946 and hearing about the requests to extend Windows 10 EOL.  The group that did all the testing, are they aware of the recently revealed command line argument to Windows 11 setup?"  And then he reveals it:  Setup /product server.



He says:  "This will install Windows 11 on a Windows 10 box that previously failed TPM 2.0 requirements.  I've done it on two laptops and one desktop myself, and they work just fine."  So there is a way, apparently just documented and just discovered.  Setup /product server, you're telling setup that you're running a server rather than a desktop so don't bother me with all this TPM nonsense and hopefully processor generation and all that, and you just get it without any muss or fuss.



And lastly, Michael Foley, he said:  "Just watched the latest episode.  Now I have to check all my uses of snprintf for the last 30 years."  And he said:  "GRC is also the acronym for the French name of the RCMP [which is the] Royal Canadian Mounted Police."  In French it's...



LEO:  Gendarmerie Royale du Canada.



STEVE:  Du Canada, exactly, GRC.  So now we all know.



LEO:  It's many things, actually.



STEVE:  Yeah, it is.  There are many.



LEO:  It's an overloaded acronym or initials.



STEVE:  That would explain the offers I get for GRC.com.  I think the highest one was $50,000.



LEO:  Wow.



STEVE:  Somebody is willing to pay for GRC.com.



LEO:  Wow.



STEVE:  So of course I'm not selling it.  But, you know, when I'm 85 or 90 it's like, yeah, okay, fine.  It'll be about time around then.



LEO:  When I do a search for just GRC, I get IBM first.  That's their Government Risk Management and Compliance solution.



STEVE:  Ah.



LEO:  And then I get the home of Gibson Research Corporation.  So you're coming in okay.  You're doing all right.  And I don't see the Mounties here anywhere.  Article 45.  What is that?



STEVE:  Oh, boy.



LEO:  Oh, boy.



STEVE:  So there's a storm brewing again in the EU.



LEO:  Yes.



STEVE:  It's been brewing for some time, and it appears that we have another case of politicians mistakenly believing that they're able to simply dictate the terms and conditions under which tech companies will serve their populace regardless of the implications to that populace's security and privacy.



We all just saw something similar come to a head, of course, with the attempt to force backdoors into all encryption services.  How'd that turn out?  Uh-huh.  Every messaging provider simply said:  "No, thank you, we'll just leave, and you and your citizens can figure out what to do without us."  The result was the addition of a nebulously worded "if it's technically feasible to do without weakening security" clause, which was every strong encryption provider's get out of jail free card.



Now we're moving into a similar challenge where, believe it or not, the EU might very well find itself and its citizens without any web browsers, or at least needing to return to the good old days of HTTP.



LEO:  Wow.



STEVE:  Uh-huh.  The controversy revolves around a made-up thing known as, I guess you'd call it "QWACs."



LEO:  QWACs, yeah.



STEVE:  QWACs, Q-W-A-C-s, which stands for Qualified Website Authentication Certificates.  So these QWAC-y things are a specific EU form of website certificate defined back in 2014 with the EU's eIDAS regulation.  Okay, what?  eIDAS stands for electronic ID Authentication and trust Services. And actually, Leo, we talked about this a couple years ago when this nonsense surfaced once.  It was something about the EU wanted to be able to display more information to users of websites, so they were going to, like, add some additional something onto the connection, like a banner or something.



LEO:  Ugh.



STEVE:  I know.  Anyway, so eIDAS is an EU regulation.  It did pass nine years ago in 2014.  Its stated purpose is governing "electronic identification and trust services for electronic transactions."  After it passed in 2014, its various provisions gradually took effect over time between 2016 and 2018.  That regulation, which never actually did much and was largely ignored, and which by the way we did, as I said, we talked about it at the time, has been under review and, in an upcoming process for the past several years, looks like it's coming to a head.



It appeared to be in fact going off the rails last year, and the tech industry did what it could back then to say:  "Hey guys, this is not looking like something we're going to be willing to do for you."  But apparently the politicians just figured that they could enact any laws they wanted to, and those techie geeks who were always complaining about something would have no choice other than to comply.  Uh-huh.



So about a year and a half ago, back in March of 2022, a who's-who of global Internet security governance - in fact it's two pages of cosigner's names and affiliations - wrote an open letter addressed to "Dear Honourable Member of the European Parliament, Dear Member of TELE Working Party."  And that letter begins, here's just the first few lines.  They wrote:  "We the undersigned are cybersecurity researchers, advocates, and practitioners.  We write to you in our individual capacities to raise grave concerns regarding certain provisions of the legislative proposal for a European Digital Identity framework (the 'eIDAS revision'), and their impact on the security on the web.



"While we understand that the intent of these provisions is to improve authentication on the web, they would in practice have the opposite effect of dramatically weakening web security.  At a time when two-thirds of Europeans are concerned about being a victim of online identity theft, and over one-third believe they are not able to sufficiently protect themselves against cybercrime, weakening the website security ecosystem is an untenable risk.  We therefore urge you to amend the revised Article 45.2 to ensure that browsers can continue to undertake crucial security work to protect individuals from cybercrime on the web."



Okay, now, to say that this letter - it goes on at some length.  But to say that this letter appears to have fallen on deaf ears would be an understatement.  That was a year and a half ago.  The near-final text for eIDAS 2.0 has now been agreed upon by the EU's negotiators, and it appears to be even worse than the earlier draft.  So now there's a new letter which, as of two days ago, on Sunday, has been signed by 466 scientists and researchers across 36 countries, as well as numerous NGOs.  And Google also just added their name to the document.  In this day and age, what this document describes is somewhat astonishing, and I need to share the first few paragraphs so that you'll get a feeling for what now hangs in the balance.



This was addressed to Dear Members of the European Parliament; Dear Member States of the Council of the European Union.  "We the undersigned are cybersecurity experts, researchers, and civil society organizations from across the globe.  We have read the near-final text of the eIDAS digital identity reform which has been agreed upon on a technical level in the trilogue between representatives from the European Parliament, Council, and Commission.  We appreciate your efforts to improve the digital security of European citizens.



"It is of utmost importance that the global interactions of citizens with government institutions and industry can be secure while protecting citizens' privacy.  Indeed, having common technical standards and enabling secure cross-border electronic identity solutions is a solid step in this direction.  However, we are extremely concerned that, as proposed in its current form, this legislation will not result in adequate technological safeguards for citizens and businesses as intended.  In fact, it will very likely result in less security for all.



"Last year, many of us wrote to you to highlight some of the dangers in the European Commission's proposed eIDAS regulation.  After reading the near-final text, we are deeply concerned by the proposed text for Article 45.  The current proposal radically expands the ability of governments to surveil both their own citizens and residents across the EU by providing them with the technical means to intercept encrypted web traffic, as well as undermining the existing oversight mechanisms relied on by European citizens.  Concretely, the regulation enables each EU member state, and recognized third-party countries, to designate cryptographic keys for which trust is mandatory.  This trust can only be withdrawn with the government's permission.  See Article 45a(4).



"This means any EU member state or third-party country, acting alone, is capable of intercepting the web traffic of any EU citizen, and there is no effective recourse.  We ask that you urgently reconsider this text and make clear that Article 45 will not interfere with trust decisions around the cryptographic keys and certificates used to secure web traffic.



"Article 45 also bans security checks on EU web certificates unless expressly permitted by regulation when establishing encrypted web traffic connections, see Article 45(2a).  Instead of specifying a minimum security measure which must be enforced as a baseline, it effectively specifies an upper bound on the security measures which cannot be improved upon without the permission of ETSI."



Okay, then skipping ahead a few pages, here's some detail that's actually difficult to believe, but it's true.  Same group writing, little bit later in this letter.  "The current text of Article 45 mandates that browsers must accept any root certificates provided by any Member State, and any third-party" - I can hardly believe I'm reading this - "and any third-party country approved by the EU.  This will have severe consequences for the privacy of European citizens, and security of European commerce, and the Internet as a whole."



They explain:  "Root certificates, controlled by so-called certificate authorities, provide the authentication mechanisms for websites by assuring the user that the cryptographic keys used to authenticate the website content belong to that website.  The owner of a root certificate can intercept users' web traffic by replacing the website's cryptographic keys with substitutes he controls.  Such a substitution can occur even if the website has chosen to use a different certificate authority with a different root certificate.



"Any root certificate trusted by the browser can be used to compromise any website.  There are multiple documented cases of abuse because the security of some certificate authorities has been compromised."  And of course we covered all this in the early days of the podcast.  "To avoid this, there exists legislation that regulates certificate authorities, complemented by public processes and continuous vigilance by the security community to reveal suspicious activities.  The proposed eIDAS revision gives Member States the right to insert root certificates at will, with the aim to improve the digital security of European citizens by giving them new ways to obtain authentic information of who operates a website."  I know, Leo.  "In practice, this does exactly the opposite.



"Consider the situation in which one of the Member States, or any of the third-party states recognized now or in the future, were to add a new authority to the EU Trusted List.  The certificate would have to be immediately added to all browsers and distributed to all of their users across the EU as a trusted certificate.  By using the substitution techniques explained above, the government-controlled authority would then be able to intercept the web traffic of not only their own citizens, but all EU citizens, including banking information, legally privileged information, medical records, and family photos.



"This would be true even when visiting non-EU websites, as such an authority could issue certificates for any website that all browsers would have to accept.  Additionally, although much of eIDAS 2.0 regulation carefully gives citizens the capability to opt out from usage of new services and functionality, this is not the case for Article 45.  Every citizen would have to trust those certificates, and thus every citizen would see their online safety threatened."



And lastly:  "Even if this misbehavior was discovered, under the current proposal it would not be possible to remove this certificate without the ultimate approval of the country having introduced the certificate authority.  Neither eIDAS's article 45 nor any provisions in adjacent EU legislation such as the NIS2 Directive provide any independent checks and balances on these decisions.  Further, European citizens do not have an effective way to appeal these decisions.  This situation would be unacceptably damaging to online trust and safety in Europe and across the world.  We believe this legislation text must be urgently reworked to avoid these serious consequences by clarifying that eIDAS does not impose obligations to trust cryptographic keys used for encrypted web traffic."



Okay.  So this letter goes for seven pages before we get to the 14 pages of signatures by everyone in the world in a place of authority who knows anything about the way our Internet security and privacy ecosystem is put together.  Mozilla authored their own letter which was dated last Thursday, November 2nd.  It was cosigned by the Bytecode Alliance, Cloudflare, DNS0.eu, Fastly, the Internet Security Research Group (ISRG), the Linux Foundation, Mozilla, Mullvad, OpenSSF, and Sigstore.  I'll only share the first line.



It begins:  "Dear Members of the European Parliament; Dear Members of the Council of the European Union.  We represent companies that build and secure the Internet.  Our organizations are either based in Europe or offer products and services in Europe.  We write to express our concern with the proposed eIDAS legislation.  We appreciate efforts to use rulemaking to strengthen the security of the Internet and the leadership role that Europe has taken in fostering cross-border interoperability.  However, leadership comes with a greater responsibility to consider the broader implications of changes."  That's just the top of basically them saying the same thing.  It expresses the same concerns and issues as the previous open letter.



So the question now is, what happens next?  A full year and a half ago the legislators were warned about this and were given a heads-up, with a full, detailed, careful, and respectful explanation.  They were very clearly told, "Do not proceed down this path."  They clearly blew it off, ignored it completely, and since then the wording of Article 45 has only grown more intolerable.  We've observed that in high-level, high-stakes politics it's necessary to give the player who's holding the weaker hand a face-saving way to back down.



This happened with the encryption debate where the loser in that struggle created their own way to save face.  But that didn't happen until those holding the stronger hand - the encryption service providers - were finally forced to deliver the ultimatum:  "If you outlaw our use of unbreakable encryption, you will leave us with no option other than to withdraw our then-illegal services from your territories."



So is this going to come to that?  Is this going to get to ultimatums?  At this point, it appears so.  And this will be another important juncture in the evolution of our Internet.  Governments are going to learn, again, that they are smaller than the technology which they and their citizenry have grown to depend upon.  It's theirs to use, but not to control.



LEO:  This is terrible.



STEVE:  I have at the bottom of the show notes four links:  the original open letter from March of 2022, 18 months ago.  I've got today's 21-page, 14 that are signatures, updated letter to the EU.  I've got the link to Mozilla's open letter, and the entire text of the proposed and agreed-upon eIDAS 2.0 legislation for anyone who's interested.  But yes, Leo, I mean, it won't come to pass.  It can't.  I mean, it can't.  It is exactly analogous to what just happened with the encrypted messaging providers, you know, I mean, it's worse than that actually.  And, you know, and they're saying, oh, but we want to be able to intercept connections in order to add additional information banners to people's web pages.



LEO:  No.  No.



STEVE:  And it's like, sorry.  We're not letting you do that.



LEO:  By the way, I mean, I guess it's just EU if the browser guys make sure it's just EU.  But, you know, you start to add certificates into your browser, it could easily be global.  They're going to vote on this tomorrow behind closed doors.  So it could be approved, according to the EFF, as early as tomorrow, November 8th.



STEVE:  Well, it'll certainly have some rollout period, you know, a grace period, and there'll be some deadline.  And it's just it's, I mean, everyone's going to have to say no.



LEO:  Yeah.  I guess.  I mean, what do you do?  It's one thing if you're Mozilla to say no.  It's another thing if you're Google to say no.



STEVE:  I believe that Google has altered the security of the guts of their browser so that they now use the hosting operating system's root store.



LEO:  Ah.  That would be - that's a good way to pass the buck.  So then it's Microsoft or Apple or - yeah.



STEVE:  Uh-huh.



LEO:  That's interesting.  Yeah, instead of putting these CA roots into the browser, you put into the OS.  So Edge I'm sure does that.



STEVE:  Right.



LEO:  Chrome does that.  Safari I'm sure puts it in macOS.  



STEVE:  Right.  Yeah, I think that the only one who still maintains their own root store is Mozilla.



LEO:  Mozilla.



STEVE:  Because they had NSS, the Netscape Security Suite, and that was where all of their SSL and TLS stuff is contained.



LEO:  This is such an obviously horrific idea.



STEVE:  Yeah.



LEO:  I mean, oh, yeah, well, you can trust every - every country in the EU is great.  So obviously this would never be a problem.  Unbelievable.  My hair is on fire.



STEVE:  I know.



LEO:  I am stunned.  Tomorrow.  This could happen as soon as tomorrow.



STEVE:  Article 45.



LEO:  Holy moly.  eIDAS is the regulation.



STEVE:  EIEIO.



LEO:  EIEIO.  Old McDonald.



STEVE:  Yow.



LEO:  Watch with interest tomorrow.  They're meeting in Brussels behind closed doors.  So you don't know what, you know.  Golly.  How stupid.  And yet how predictable.



STEVE:  Yes.  The politicians don't understand that this is not theirs to mess with.



LEO:  I fear that they do understand, and that they want in.  They don't want you to have security.  They want security themselves.  But I don't think they want us to have security.



STEVE:  That's right.  They want to be able to monitor the conversations of all their citizens.



LEO:  Yeah, yeah.



STEVE:  And they can't right now.



LEO:  And insert arbitrary banners whenever the hell they feel like it.



STEVE:  That's right.



LEO:  Election ads?  You know, Turkey's in the EU.  Erdogan, I could easily see him doing that.  Geez.  And that's the thing, that certificate is EU-wide.  So, you know, you've got to ask our friends in France, do you want Turkey to start inserting banners in your browsing sessions?  Let alone see what you're up to.



STEVE:  I just don't think that anyone will do it.  I just - I can't, you know, I just - I don't think, I mean, this has all been curated and carefully managed.  It's got some problems, but they're not big.  We've followed them for years.  This cannot be allowed to happen.



LEO:  Yeah.  Well, there you go.  This is why you listen to Security Now!, to set your hair on fire.  We do this show - wow,  Steve.  Wow.  And you know this is not getting the coverage it really should.



STEVE:  I know.  It's because it's sort of just happening quietly in the background.



LEO:  Yeah.  We'll start yelling about it for sure on all of our shows.  Thank you for giving us an update.  EFF also has a piece concurring completely with what you say.  Wow. 



STEVE:  I bet they do.



LEO:  Yeah.



STEVE:  This would just cause them to, you know, lose their lunch.



LEO:  The problem is that, you know, if you go to the EFF front page, there's plenty of other things to get upset about, too.



STEVE:  Yeah.



LEO:  They say Article 45 will roll back web security by 12 years.  Oh, worse than that, really.  Have a great week, Steve.  We'll see you next time on Security Now!.



STEVE:  Okay, buddy.  Bye.



Copyright (c) 2023 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.






GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#948

DATE:		November 14, 2023

TITLE:		What if a Bit Flipped?

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-948.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  Is your lack of privacy badgering you?  And if so, what can you do about it?  What's the latest on last week's bombshell news of the EU's Article 45 in eIDAS 2.0?  Who's lost how much money in online cryptocurrency?  And is using seed phrases for your wallet that you get from a seed phrase suggestion site a good idea?  Has there finally been a truly devastating and effective speculative execution flaw discovered in Intel's processors?  Could it be their Downfall?  What country has decided to ban all VPNs?  And how bad are the two flaws found in OpenVPN?  Why have I stopped working on SpinRite?  What's the best backup for a large NAS?  Should vulnerability researchers learn the assembly language of their target processors?  If quantum computers threaten asymmetric crypto, why not return to symmetric crypto?  Could someone explain exactly why Article 45 is a bad thing?  What in the world is a Windshield Barnacle and why don't you want one?  What's my latest sci-fi book series discovery?  And just how bad could it be if a cosmic ray flipped a bit at just the wrong time? 



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  You remember Spectre and Meltdown?  Nobody ever exploited those.  Well, there's a new speculative execution flaw called Downfall, and it's out there in the wild.  Steve has all the details.  We'll also talk about a brand new sci-fi novel Steve loves, is telling you all about it.  Why you would never want a Windshield Barnacle.  And finally, why cosmic rays could be a real bane of cryptography.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 948, recorded Tuesday, November 14th, 2023:  What if a Bit Flipped?



It's time for Security Now!, the show where we cover the latest in security, the Internet, everything you need to know about keeping yourself safe online with this guy right here, Mr. Steven Tiberius Gibson.  Hello, Steve.  There, he's doing the Spock thing.  You know, I made that Steven "Tiberius" Gibson thing up, but it's actually in your Wikipedia now.



STEVE GIBSON:  Oh, yeah.  It stuck.



LEO:  It stuck.



STEVE:  And I don't know whether they realize that's, you know, my middle name begins with M, or not T.  But I don't know.  Anyway...



LEO:  I love messing with things like that.



STEVE:  The nature of - my wife Lorrie has started doing crossword puzzles as a way of keeping her brain sharp.  Since she doesn't code, you know, she wants some exercise.  And one of the questions that she encountered last week was an opinion that spreads quickly on the Internet.  And it was four letters.  And I said "meme."  And she said, "How did you get that?"  I was like, well, you know.



LEO:  That's pretty obvious.  I thought of that immediately.



STEVE:  I know.  So those of us who are...  



LEO:  We live on the Internet.



STEVE:  ...in the meme-spreading business.



LEO:  It is, that's our business.



STEVE:  So, okay.  We've got to answer, as we like to lately, a bunch of questions.  Is your privacy badgering you?  And if so, what can you do about it?  What's the latest on last week's bombshell news of the EU's Article 45 in eIDAS 2.0?  Who's lost how much money in online cryptocurrency recently?  And is using seed phrases for your wallet which you get from a seed phrase suggestion site a good idea?  What could possibly go wrong?



LEO:  I've often wondered that.  I'm glad you're asking that.



STEVE:  I thought you had.  We're going to answer that question right here today, Leo.  We have actual evidence.  Also, has there finally been a truly devastating and effective speculative execution flaw discovered in Intel's processors?  And could it be their Downfall?  What country has decided to ban all VPNs?  And how bad are the two flaws which were recently found in OpenVPN, and how quickly should you update?  Why have I stopped working on SpinRite?



LEO:  What?



STEVE:  What's the best backup for a large NAS?  Should vulnerability researchers learn the assembly language of their target processors?  If quantum computers threaten asymmetric crypto, as they do, why not return to symmetric crypto?  Could someone explain exactly why Article 45 is a bad thing?  What in the world, Leo, and oh, are we going to have fun here, is a Windshield Barnacle, and why don't you want one?  What's my latest sci-fi book series discovery?  And just how bad could it be if a cosmic ray flipped a bit at just the wrong time?



LEO:  Oh.



STEVE:  All those questions and more will be answered during today's Security Now! podcast 948:  "What if a Bit Flipped?"



LEO:  What if?  You know, people have been mocking me for years because I would go on the radio show and say that your computer could be crashing because of a cosmic ray.



STEVE:  Yes.



LEO:  And nobody believed me.  But this is going to be my proof.



STEVE:  We have scientific evidentiary fact now.



LEO:  Science.



STEVE:  Coming soon to a podcast near you.  This one was a retweet from one of our listeners who thought I would get a kick out of it, and he was certainly right.  I titled this "A pessimistic view of multifactor authentication," where the three authentication factors are something you forgot, something you left in the taxi, and something that can be chopped off.



LEO:  Actually, that's a really good point, I've got to say.



STEVE:  Yeah.



LEO:  VessOnSecurity, that's a really good point, yeah.  Wow.



STEVE:  Yeah.  So, yeah.  Instead of something you know, something you have, and something you are, it's something you forgot, something that the taxi just drove away with, and whoops.



LEO:  Whoops.



STEVE:  Something that could be chopped off if someone really needed it.



LEO:  Let's hope it's your hand, not your head.  By the way, Robert got this from Mastodon, I notice, because VessOnSecurity is on infosec.exchange, which is the same Mastodon server you're on, I think, yeah?



STEVE:  Yeah, that is where I am, yup.  But don't send me anything there because I'll never see it.



LEO:  He's a read-only guy.  Or not even a read-only.  He doesn't read at all.



STEVE:  Not even that, no.



LEO:  Not even that.  He's just there.



STEVE:  I'm a placeholder guy.



LEO:  Placeholder guy.



STEVE:  Just in case it happens someday.  Okay.  So a bit of a follow-up on the annoyingly named, but nicely functioning, Privacy Badger from EFF.  While assembling today's podcast, I was greeted with this in the middle of a page at a news aggregation site.  And in the show notes I'm showing this window that says "Privacy Badger has replaced this TikTok widget."  And then I can click the big orange button, "Allow once," or "Always allow on this site."  My first thought upon seeing that was thank you.  I have no interest in having my browser going to TikTok to retrieve whatever it was that this website wanted my browser to have, and in the process of course expose itself to TikTok as a first party.  Thanks anyway.  I don't know what would have been shown in that location where Privacy Badger posted a fill-in, and I don't care.



While catching up yesterday with my Twitter feed, I ran across someone who was passing along the advice he'd heard from somewhere else that - and I'm talking about one of our listeners who said, "Hey, Steve, though you would like to know."  He said that uBlock Origin and Privacy Badger should not both be installed because they interfered with one another and because uBlock Origin was a superset of Privacy Badger.



So just for the record, I have seen zero evidence that any of that is true.  Looking at the Privacy Badger icon for that web page I was on, I saw that it had prevented the loading of nine things on that page.  It blocked blah blah blah .apple.com, apipodcast.apple.com, embed.podcast.apple.com, www.apple.com.  So the four apple.com properties.  Also it kept me from going to static.cloudflareinsights.com, googletagmanager.com, fonts.gstatic.com, open.spotify.com, and tiktok.com.  So that seemed very useful to me.



For its part, uBlock Origin blocked 10 fetches, preventing my browser from exposing itself to four of the page's 19 offered domains, including sentry-cdn.com and whatever datadoghq-browser-agent.com is.  I was morbidly curious about datadoghq, which describes itself:  "The Datadog Agent is software that runs on your hosts.  It collects events and metrics from hosts and sends them to Datadog, where you can analyze your monitoring and performance data."



That's right. Just as I have no interest in having my browser snuggle up to TikTok, I don't need Datadog to be collecting any metrics about me, thank you very much.  Needless to say, I've always been happy having uBlock Origin watching my back.  And I'm equally happy now having Privacy Badger as part of that team.  And Datadog be gone.  Wow.



And you know, Leo, it also occurred to me, if we didn't have tools like this, as most users don't...



LEO:  We'd never know.



STEVE:  We would have no idea that Datadog was hounding you.  You just, you know...



LEO:  You know what puzzles me, why doesn't uBlock Origin stop this stuff?  Maybe there's some, you know, there are a lot of optional filters on uBlock Origin.



STEVE:  Well, have you met EFF?



LEO:  Yeah.



STEVE:  You know, they don't think that a sunny day is a good thing if there's some way to monitor whether you're looking outside.



LEO:  Right, right.



STEVE:  So you've got, you know, the Badger, it's not going to let anything happen.  So, I mean, I'm wondering why fonts.gstatic.com, but apparently it appeared on three different sites, and the Badger said no, no more tracking for you.



LEO:  So what site was this, did you say, that you got all this crap on?



STEVE:  I didn't.  And I don't want to finger these people.  They're actually a really good news aggregator site, and...



LEO:  They're trying to make a living is what they're trying to do, unfortunately.



STEVE:  Exactly.



LEO:  It's what you have to do in this day and age.



STEVE:  Yeah, yeah, exactly, yeah.  They're over on substack, so it's hosted there.  Okay.  So because of last week's bombshell news, I looked around for anything happening further on the eIDAS 2.0 front, since it was such big news.  Now, as you would expect, The Register weighed in the following day, on Wednesday, in their inimitable snarky style with the headline "Bad eIDAS."  Uh-huh.  You know, eIDAS.  "Europe ready to intercept, spy on your encrypted HTTPS connections."  But in their reporting they didn't have anything new to add since there's nothing new to add.



As I noted last week, we haven't quite reached the ultimatum stage where the OS and browser vendors will simply refuse to turn over the management of global trust to political interests.  But given what we just witnessed a few weeks before that, where UK politicians refused to back down until every encrypted messaging provider, including Apple, made very clear that they would withdraw their services rather than comply with a bad law, the next thing we see may be a repetition of that same drama.



You know, I think the best way to view this is the way I concluded last week, noting that:  "The management of today's Internet and encryption technology is bigger than any single, or group, of governments."  That lack of absolute control over their populace doesn't make any governments happy, but it does appear to be the way the world is shaking out.  And I think it's clear that, you know, that's the way it should shake out.  So it's been interesting watching this all happen.  And we're still watching it happen.



So I have two quick news items to share in the world of online cryptocurrency, where you are constantly posing the question, what could possibly go wrong?  The first is the cryptocurrency exchange Poloniex, P-O-L-O-N-I-E-X, Poloniex I guess, you know, because the good names were taken, lost $130 million USD worth of assets after hackers drained its hot wallet.  And I guess the wallet was hot, and now its cryptocurrency is hot.  Poloniex confirmed the hack, paused its transactions, and promised to reimburse users' losses.  And this is the exchange's second heist after it was also hacked back in 2014.  This 130 million loss ranks as the 14th-largest cryptocurrency hack ever.  And the fact that we're ranking them should tell you that there's a problem somewhere.



And second, the decentralized finance platform Raft - again, not a great name - has lost 3.3 million worth of cryptocurrency after a hacker exploited a vulnerability in its platform.  The company confirmed the hack on social media and paused the minting of its R, as in Raft, stablecoin to investigate the incident.



So the one observation I wanted to make was to once again, as I said, pose our rhetorical question about what could possibly go wrong.  You know, when the integrity of the storage of shockingly large amounts of real-world actual money depends upon software that very few people even understand, you know, how could that be bad?  Yeah.  You know, you don't have a $130 million heist being ranked as the 14th largest in any industry that has any idea what it's doing.



So I'll just reiterate,  you know, to please be very careful and heed the ago-old advice to never invest - just as in never gambling - more than you can afford to lose.  Because sooner or later.  Again, and this points to the online side of cryptocurrency.  There are people with cryptocurrency in a wallet, and they're very happy with it.  You know, I wish I had those 50 bitcoins I once had my PC mint for me in a wallet.  Unfortunately, that was the most expensive reinstallation of Windows I've ever made.  Leo, it's currently worth $1.8 million.



LEO:  Oh, my god, Steve.  I'm so sorry.



STEVE:  I know.  But boy, you know...



LEO:  Oh, that burns.



STEVE:  I upgraded XP.  What could possibly be bad about that?



LEO:  Oh, this burns.  Oh.



STEVE:  Yeah, yeah.



LEO:  I mean, in our defense, we didn't know it was worth anything at the time.



STEVE:  Leo, in our defense, there was a bitcoin faucet that was dripping money.



LEO:  Right, right.



STEVE:  I mean, you could go get some.



LEO:  You got that 50 bitcoin overnight, one night.



STEVE:  Yes.  Yes.  I solved one hash puzzle because nobody else was doing it.  And so it's like, I got the answer.  Does anyone care?  We didn't care.



LEO:  Yup.  Yup.



STEVE:  But I do care now.  Anyway.  Okay.  So before we run away screaming from the topic of "What Could Possibly Go Wrong with Cryptocurrency," get a load of this one:  "UK police are currently in the process of returning around 1.9 million worth of cryptocurrency that was stolen back in January of 2018 by a Dutch hacker.  This individual operated a website called: iotaseed.io."  And you're going to love this.  As its name suggests, the iotaseed.io site was created to generate seed phrases for the IOTA cryptocurrency wallets, which the wallets' users then apparently used without any change.  So who can guess what happened next?



LEO:  Uh-huh.  What could possibly go wrong?



STEVE:  That's right. The site's criminal hacker/owner was recording all of the seed phrases his site was suggesting that its visitors should use to protect their wallets.



LEO:  That's brilliant.



STEVE:  To no one's surprise, other than all of those visitors whose money this guy then stole, the crook broke into their wallets and did just that.  He was caught and, following a Europol investigation, was sentenced to four and a half years in prison.  What's that expression?  "A fool and his money are soon parted."  Wow.  So, yeah, I'm going to go to a website, and I'm going to get the secret key to use for my wallet because, you know, that's what the site does, it gives secret keys to people.  You know, they're not secret to the site that gave them.  Oh, boy.



Okay.  "Downfall" is the name of yet another information disclosure vulnerability that was recently rediscovered in Intel's chips.  And unlike Spectre and Meltdown, this one really has some teeth.  Here's what it's rediscoverer wrote.  And I'll explain in a minute why I'm using that term.



He wrote:  "Downfall attacks target a critical weakness found in billions of modern processors used in personal and cloud computing.  This vulnerability, identified as CVE-2022-40982, enables a user to access and steal data from other users who share the same computer.  For instance, a malicious app obtained from an app store could use the Downfall attack to steal sensitive information like passwords, encryption keys, and private data such as banking details, personal emails, and messages.  Similarly, in cloud computing environments, a malicious customer could exploit the Downfall vulnerability to steal data and credentials from other customers who share the same cloud computer.



"The vulnerability is caused by memory optimization features in Intel processors that unintentionally reveal internal hardware registers to software.  This allows untrusted software to access data stored by other programs, which should not normally be accessible."  He wrote:  "I discovered that the Gather instruction, meant to speed up accessed scattered data in memory, leaks the content of the internal vector register file during speculative execution.  To exploit this vulnerability, I introduced Gather Data Sampling (GDS) and Gather Value Injection (GVI) techniques."



Okay.  So in the past, whereas we never had any demonstrations for the Spectre and Meltdown attacks because they remained theoretical, the Downfall site, downfall.page is the URL, .page, shows videos of 128- and 256-bit AES keys being stolen from another user sharing the same computer.  Also, the theft of arbitrary data from the Linux kernel and more.  All of Intel's Core processor architectures from the 6th-generation Skylake to and through the 11th-generation Tiger Lake are affected.  So this is nine years' worth of Intel's processors, since the 6th-gen Skylake chips appeared back in 2014.



And the attacks using this are practical.  Its author wrote:  "GDS (Gather Data Sampling) is practical."  He said:  "It took me two weeks to develop an end-to-end attack stealing encryption keys from OpenSSL.  It only requires the attacker and victim to share the same physical processor core, which frequently happens on modern-day computers, implementing preemptive multitasking and simultaneous multithreading."



So Intel has released a microcode update which blocks transient results of Gather instructions and prevents attacker code from observing speculative data from Gather, or gathered by Gather.  That's the good news.  The bad news is that, depending upon how much benefit the processor may have been obtaining from its speculative execution optimization, the impact on performance, Intel acknowledges, might be as much as 50%.  



There's one thing all of these vulnerabilities that we've talked about have in common:  They are all about speculative execution.  In other words, Intel back in, oh, probably 2011, given that it takes about three years to get a chip from original concept out, and the Skylake occurred in 2014.  So back around 2011 Intel engineers had this terrific idea that since the laws of physics were making it impossible to just keep increasing the processor's clock, a different way to speed up code was to create a massive overabundance of processor power by having more execution resources all running as fast as they could go since they couldn't go any faster.  So if you can't make them go faster, just have more of them going as fast as you can, and allow that excess, apply that excess to the task of executing code speculatively.



The idea is that the processor would be allowed to run ahead by pre-fetching from memory the data instructions and looking at them ahead of its actual execution.  And if that prefetching system encountered a branch instruction, it would follow both code paths from the branch, the one that branched and the one that didn't branch, by continuing to fetch and examine those instructions.  Then, at some point in the future, once the processor's actual execution had caught up to the branch instruction, then it would know which path to follow, and all the work down the other path would be discarded.



The side-channel edge case that Intel failed to pay sufficient attention to was that all of that extra work that ended up being discarded left some traces behind.  Memory would have been fetched that didn't end up ever being needed or used.  But the cache would have been filled with that, and other contents that would have been in the cache would have been evicted to make room.  Remnants were left behind in the branch prediction logic that's used to make decisions when insufficient information is available at the time, and on and on and on.  This has been biting Intel and, to a somewhat lesser degree AMD, for years now.



In the author's Q&A on the page, he posted the question:  "Why is this called Downfall?"  And the author replies:  "Downfall defeats fundamental security boundaries in most computers and is a successor to previous data-leaking vulnerabilities in CPUs including Meltdown and Fallout.  In this trilogy, Downfall defeats all previous mitigations once again."



At the start of this I referred to this author as the "rediscoverer" of this.  Wow.  The reason is that Intel was informed of this very vulnerability twice before, back in 2018, four years after Skylake began this.  They chose, for whatever reason at the time, to ignore the problem.  Perhaps it was due to the damaging effect it might have on their public relations, or maybe because of the massive performance hit any patched microcode would force.  Either way, its discovery, complete with ample proofs of concept - and Leo, you're playing one now into the video on the podcast - ample proofs of concept and source code on GitHub cannot be swept under the rug.



LEO:  Oh, dear.  Oh.  So my sense of Spectre and Meltdown was it was so hard to implement that nobody had.  There hadn't been any exploits. 



STEVE:  Exactly.



LEO:  This one's easier.



STEVE:  Now we have one that is practical.



LEO:  Oh, crap.



STEVE:  And the source code is on GitHub.



LEO:  Now we've got problems.



STEVE:  Now we've got problems.  And predictably, a class-action lawsuit has been filed by five aggrieved so-called victims and  their ambulance-chasing attorney.  It's necessary to show actual concrete damage rather than to simply be put out by Intel's past behavior.  So it's unlikely that this is anything more than a ploy, probably to get Intel to pay out some modest settlement to make this most recent lawsuit just go away.  You know, Intel certainly has bigger problems because they have had to patch all the microcode on their processors because this is really bad.  This really does break protection boundaries in the cloud.  And, I mean, this is the real deal finally.



LEO:  That's the main threat model here is shared processor; right?  So servers are particularly vulnerable because most websites don't own the computer or the processor.  They're sharing it.



STEVE:  Right.  So end-user workstations, not a problem because you're unlikely, I mean...



LEO:  Nobody's sharing my workstation.



STEVE:  It has been noted, though, that this exploit could probably be used through a browser.  So you could go to a website that loads malicious code and then, while you're not paying attention, it's busy using this exploit to try to get the secret keys that you've got stored in your computer.  So enterprise users need to be aware of this, and cautious.  The good news is Intel is on this, finally; and that, as we know, updates from Microsoft will bring microcode patches with them.  So this will end up getting fixed.



LEO:  And I guess Intel did fix it in every processor after the 11th generation.  So they know how to fix it.



STEVE:  Yes.



LEO:  Without killing performance.



STEVE:  No, it's a performance hit.  I mean, they're not happy.



LEO:  Just turn off speculative execution.



STEVE:  Yeah.  This was a real win for them, and by their own admission they're saying a 50% drop in performance, depending upon how much Gather was speeding things up.  So basically they were - you could say that they were cheating to get this performance benefit.  And certainly they were cheating after 2018 when they were told that this could happen.  And they said, oh, we don't think it's going to be a problem. 



LEO:  Is there no way to do speculative execution without this vulnerability, I guess would be the question.  I mean, Apple's processors use speculative execution.  This new Apple silicon, everything does.  That's a major technique used by modern microprocessors.



STEVE:  Yes.  And in fact they have a fix for it which obscures the data that this Gather instruction was being used to get.



LEO:  So block the side channel.  Don't turn off the speculative execution, yeah.



STEVE:  So exactly, yeah, yeah.



LEO:  That's interesting.



STEVE:  So I think they were insufficiently cautious with these edge cases which do allow some information leakage.  And, you know, this is the beauty, the good part of the way the security industry is working now.  We would not have any of this if it were not for the academic researchers who were just poking at this and exploring what can be done.  You know, there's no money in revealing this.  No one's getting paid.  It's just researchers who want to...



LEO:  [Crosstalk] UCSD.  He's a researcher.  He's not, yeah, he's not a hacker.



STEVE:  Nope.



LEO:  Why did he feel it necessary to put out exploit code, I wonder?



STEVE:  Probably out of just an upset over the fact that Intel had been so negligent.



LEO:  Yeah, yeah.



STEVE:  Wow.



LEO:  Well, we've been kind of waiting for this other shoe to drop.  In fact, I had kind of thought, oh, it's not going to happen.  We're okay.



STEVE:  I know.  And it took quite a while.  And, you know, Bruce Schneier's famous reminder keeps coming back, you know, attacks never get worse, they only always get better.



LEO:  Yeah.



STEVE:  And, you know, here we have something that's been sitting around for nine years in these chips that actually does allow cross-process information theft.



We've touched on this before, but I wanted to note that Russian officials are preparing to formally ban the use of all VPN services in the country.  And I got to use my favorite word again, Roskomnadzor.



LEO:  Roskomnadzor.



STEVE:  They have been testing blocks for various VPN protocols and services over the past year in preparation for creating a formal blockage.  Officials in Russia say that a formal VPN block is needed for the safety of the Russian Internet.  Because, you know, they believe they have their own.  And I think that's just fine.  If they want to disconnect from the rest of us, and we would lose some listeners, that's unfortunate.  But the listeners are not the government, so I recognize the difference.



And speaking of VPNs, if you're using OpenVPN, check for two security updates.  Neither one is super critical, but one of the two could potentially provide access to memory.  And as we know, attacks never get worse, they only ever get better.  So the point is OpenVPN has recently been updated.  If you're using OpenVPN, it's probably worth going and checking that out and bringing yourself current because one of the two is worth patching.  Again, not house on fire, but still, good to be current.



And as I mentioned at the top of the show - and Leo, you said "What?" - I've stopped working on SpinRite.  And not because I've been distracted by anything else, but because every indication is that both its DOS and its Windows code is well and truly finally finished.



LEO:  Woohoo!  Let me get some champagne.  Oh, they're cheering out in the hall.



STEVE:  It took five release candidates of each one of those to get us there.  But there is nothing left for me to do.  So I've immediately turned my attention, and this was like I posted that on Sunday and then began working, like immediately, on the next piece of the work, which is updating GRC's servers' Windows executable code delivery system to perform code signing on the fly.  Until now, code signing has always just been a manual process.  And this was fine when I was signing the DNS Benchmark, or Never10, or InControl, where that one download file would live on for years.  But now I need to automate the process so that it can be performed on GRC's server - and of course the key lives in a Hardware Security Module, since each purchased and licensed copy of SpinRite is unique to its user.



So anyway, I'm working on getting code-signing automation in place.  I think that should happen in the next day or two.  I just started on Sunday, and I solved another problem yesterday morning that was a bit of a puzzle because it turns out lots of people have wanted to do this, but no one has figured out how to do it in the right way, where you're signing a blob in RAM rather than signing a file because you don't want to have to write the file out to the file system just for the sake of signing it and then send it.  The right way to do it is to do it all in RAM.  So I think I've figured out how to do that.



And I've also been thinking, it's occurred to me a couple times, when I've solved really hard problems, and there were a couple that I've run across, that I ought to have some way to share the source of that.  So I may start doing that, so I'm leaving a little more legacy of solutions.  Because I know I'm often going to Source Forge or to - there's a code something, I can't remember the name of it.  Anyway, I'm often finding little bits of help here and there online, and I'd like to be giving some back.



LEO:  That's nice.



STEVE:  So I'm going to start doing that, too.



LEO:  Good.



STEVE:  Yeah.  Anyway, so I'm going to get that going.  Once we have code signing automation, then the existing pre-release page at GRC, which I've talked about before, which has only been able to download the DOS component of SpinRite, will be downloading, will be offering the hybrid, basically finished code, which is what I expect that we'll be shipping since it's been now quite a while since there's been anything that's come along that was serious.



This is kind of interesting.  The problem between - the thing that was in SpinRite itself that moved me from RC4 to RC5 was I've got one really intrepid tester whose handle is MillQ who set up a constrained RAM environment on a drive that just did nothing but produce errors.  And he discovered that something in SpinRite was causing the FAQ page, we have an online, in the program itself, is a scrolling FAQ page of like a description of the commands and a bunch of frequently asked questions, obviously.  When the drive was producing an incredible number of errors, the log of those would end up appearing in the FAQ.  So he took a screenshot of it and said this doesn't seem very serious, but just wanted you to know.  Well, it's serious to me.



So it turns out that I allocated 16MB of space for the log, figuring that nobody would ever overflow that.  That was 113,359 log lines.  It turns out MillQ overflowed that.  And I probably meant to get back to dealing with the overflow case, and I just forgot.  So anyway, what RC5 has that RC4 didn't is that if you actually do have a log - oh, and this is the scroll back.  You're able to log to the file till the cows come home, until you run out of storage space on your logging drive.  This is the scroll back buffer that allows you, while you're in SpinRite, to scroll back and look at prior log entries.  Well, when you get to 113,359 log entries, now the log starts wrapping instead of just continuing to grow without limit, which obviously it should have done.  Thus there was no other problem that we ever found in RC4, and now RC5 doesn't have that either.  And MillQ tested it.  It works perfectly.



So anyway, everyone will be able to download the final SpinRite code.  Then I work on the documentation on the website.  I want to put, you know, pictures of the various SpinRite screens and explain what they do and how it works so that there's online documentation.  Then I launch the email facility so that I have a means of notifying all of SpinRite's owners for the last 19 years.  And then I get to start on SpinRite 7.  So that's where we are.  We're getting there.



Some closing-the-loop feedback from our great listeners.  I have one from EdgeIT, who said:  "Great show.  I've been using Sophos XG/XGS firewalls for my customers for a while.  They have had this automatic hot fix feature which is enabled by default for many years.  And a few years ago, when there was a vulnerability, it was patched for every customer while I slept."



So Sophos, they're a real security company, and they make a good firewall, and they've already figured out how to do this.  So come on, everybody else.  All the other big appliance vendors need to get going because the disasters that we've been talking about the last few weeks are ridiculous to be having at this point where, literally, multiple tens of thousands of victims are suffering as a consequence of the bad guys beating the good guys to patching their systems.



Jim Harbin tweeted:  "Hey, Steve.  Long-time listener and so glad I can continue to be for a long time more.  Question around NAS backups.  Both you and Leo mention Synologies and Syncthing.  I went with a QNAP and have 30TB of RAID storage in a four-bay unit."



LEO:  Nice.



STEVE:  "But outside of this NAS this data is not replicated."



LEO:  Yeah.



STEVE:  "What would be the best, most cost-efficient way to create some redundancy, other than my single NAS?"  He said:  "Long ago are the old tape backup days."



LEO:  You can still get tape.  It's still out there.  It's not gone.



STEVE:  Wow.  So this really is a problem.  Spinning magnetic storage has become astonishingly inexpensive.  So I doubt there's any superior solution than simply using more spinning storage.  As I noted last week, I run RAID 6 exclusively.  So each array has two drives' worth of redundancy.  So what you might do, if you just want another layer of redundancy and access in the event that the entire NAS goes down, which would not be unheard of, would be to run just, you know, another single spinning drive on some always-on machine and keep it synchronized with the NAS.  But the bottom line is, I doubt that it's possible to do better than spinning drives, and then make up for the possibility of their failure with some additional redundancy.



LEO:  What I do, which gives you offsite and full redundancy, is I have a second Synology at work.  And I bet you QNAP does this, too, or has this available.  But Synology has a couple of ways you can synchronize two Synologies over the Internet.  I use Hyper Backup, which is one of the free Synology apps.  There's also Drive ShareSync.  And so the Synology here is always talking to the Synology at home, and they're making sure that they are mirrored duplicates of each other.  And that gives me real peace of mind.  I've got offsite, I've got, I mean, it would be hard to lose data at this point.  Not that I couldn't do it.



STEVE:  Yup.  Yup.  I do the exact same thing.  I have a pair of Synologies at different locations.



LEO:  Well, there you go.



STEVE:  And they are able to see each other through the pfSense Firewall, and then I use Syncthing at each of my locations to keep the directories I care about on my work stations synchronized to the local Synology NAS, which then clones it over to the other Synology.



LEO:  Perfect.



STEVE:  So that when I'm over on my other workstation at the other location, the Syncthing running there automatically synchronizes it to the NAS in that location.  And it works great.  I've got lots of redundancy.



LEO:  Yeah, because every machine I have, because I have a lot, like you, and I'm in different locations, has the same documents folder, I mean, there's some basic folders, shared folders, source folder, a sync folder, documents folder, pictures folder, and audio.  Those all get shared everywhere.  So I have copies on every computer, plus the backups locally, and the backups offsite, yeah.  I mean, Syncthing is really a great solution.  And both these solutions don't require the cloud at all.  Which is why you spend a lot of money on a NAS.  I mean, these aren't cheap, as your emailer knows.



STEVE:  Right.  And again, my feeling is, if you need more redundancy, just set up a system with a spinning drive and use something like Syncthing, just to make another copy.



LEO:  Get it offsite, though, if you can.



STEVE:  If you can; right.



LEO:  That's really the best.  You used to mail CDs to your mom, I remember.



STEVE:  You're right, back in the old days.



LEO:  That was it.  That was offsite.



STEVE:  Wow.  That's really true.  Yeah.  She thought it was another AOL CD coming.



LEO:  I don't know why I keep getting these.



STEVE:  So Jorge Moran said:  "Hi, Steve.  Knowing how skilled you are at actually producing ASM code, I was wondering if you've ever tried to do some malware analysis or exploit development?  After some time studying this, I've come to the conclusion that it might be better to step back and actually learn to write ASM code from scratch first.  What do you think?"



I guess I'm of two minds about that.  On the one hand, everything is ultimately assembly code since that's what our chips still run.  Various attempts to create higher-level chips have been undertaken.  The "Forth" processor comes to mind.  You know, there was actually Forth Inc. actually designed silicon that ran the Forth language, that interpreted the Forth language directly.



LEO:  And Symbolics made a Lisp machine.



STEVE:  Yup.



LEO:  And I presume it had a custom CPU.  I don't know.



STEVE:  Yup.



LEO:  Yeah.



STEVE:  So the problem is we appear to be moving in the other direction, right, from CISC architectures to RISC instruction sets.  So being able to understand code at its lowest level can certainly come in handy.  But on the other hand, we just saw two weeks ago that the code reverse engineering tool Ghidra was producing high-level decompiled code.  So it just wasn't serving as a disassembler, but also as a decompiler, taking the disassembled code up another level to create much more context for what the binary bits underneath were actually doing.



The code that the guys who were tracking down the Citrix Bleed flaws received from Ghidra was beautiful higher-level code.  It didn't know the names of the arguments that function calls were using, but it named them, you know, arg1, arg2, arg3.  And then a human, looking at the way those arguments were used, could then create the context to understand more about what was going on.



But, you know, I guess my feeling is, if you're looking at disassembled assembly language, you can learn a lot of assembly language by studying the disassembly.  I don't know that you actually need to learn to code that yourself from scratch.  These days, we do seem to be seeing things, you know, moving up to a higher level.  And I think you can still be very effective, even without learning assembly language.



LEO:  I mean, speaking of moving up, I think we're on the cusp of using AI to generate code.



STEVE:  Yes.



LEO:  A lot of people think that people won't be writing even high-level code for much longer, that they'll be writing...



STEVE:  Well, and reverse engineering.



LEO:  That's going to be tough.



STEVE:  You know, figuring out what those various variables mean.  



LEO:  Because if an LLM creates code, I don't know where it came from.



STEVE:  Well, Alex is a huge user of this generative AI, and he's talked about how he just sort of says what it is that he wants code to do.  And in fact, remember that our listener had AI generate the LastPass vault decryptor as like a first start.  And then he went in and tweaked it in order to fix it further.  So we've already seen that happening.



LEO:  I should show you, I was playing with these new OpenAI ChatGPTs.  I'll show it more tomorrow.  But one of the things you can do with their GPTs is give them a corpus of information, and then have the AI - you could tell it, only give me an answer that comes from this body of information I gave you.  So I gave a GPT all of my Lisp books, the greatest Lisp books, the beginning books, the advanced stuff from Peter Norvig, and I created a Lisp expert that can write code beautifully, and I can ask questions of.



I can say, oh, remind me how that for-loop works, and it will tell me and write sample code.  You can even execute the code in the ChatGPT.  So this is nice because it's not hallucinating; right?  If you had a bunch of PDFs, you know, you could do it with 8086 assembly.  You probably have all of those manuals from Intel and so forth.  Put those in there, and now you can say, write me a sort in assembler, and it would do it.  It's kind of incredible.



STEVE:  I know.



LEO:  I'm a skeptic, an AI skeptic.  But in this one regard, when they're given a corpus so they can't hallucinate, can't deviate from, it does a really amazing job.



STEVE:  It's a little more than spellcheck.



LEO:  It's a lot more, yeah.  I'll show you at some point, if you want.  I also did an Emacs expert.  I took all the Emacs reference manuals and put them into it.  And you can ask it any question, how do I set a font, and it - it's easy, here's the code.



STEVE:  Wow.



LEO:  Yeah, I mean, I think we're really - we're in an interesting world, let's put it that way.



STEVE:  Yup, we are.  So Trevor from infosec.exchange, he said:  "Hi, Steve.  Insert often repeated 'logging back into Twitter for the first time in a long time just to send this spiel.'"



LEO:  [Crosstalk] do this.



STEVE:  Well, and again, as soon as I get SpinRite 6.1 done, I'll be launching an email facility that will provide another means of communication.  So that'll be good.



LEO:  Actually, you could just take all the transcripts of this show and put them into a GPT.  Well, I don't want to say.



STEVE:  I know.



LEO:  It was my plan for 999 and after.



STEVE:  Here's Stevebot.



LEO:  Yeah, Stevebot's here, yeah.



STEVE:  So he said:  "I've been thinking about crypto and prime factorization lately and how to apply the 'assume breach' way of thinking to this problem.  My thinking is, assume that a bad actor has or will have access to a sufficiently capable quantum computer.  How can you protect against this?  Something in the context of a messaging app.



"Public key crypto and Diffie-Hellman are both in the crosshairs of quantum computing since Shor's algorithm can effectively factorize the large primes that RSA depends upon and can efficiently solve the discrete logarithm problem on which Diffie-Hellman is based.



"Post-quantum algorithms are also still in their infancy; and as we have seen before, new crypto systems need sufficient and constant beatings until they're deemed acceptably secure.  My first thought was find a way to stop using public key crypto or reliance on Diffie-Hellman for your communications with another person, maybe find a way to get them a symmetric key.



"Well, how can you communicate a shared secret securely without having an already pre-shared key?  You would need to use public key crypto/Diffie-Hellman to share the key over the wire, but that transmission could be broken and your secret revealed.  So I was thinking about you saying 'if you really want to communicate privately, strip naked and sit in the middle of a field with a blanket covering both of you while you whisper your secret.'



"Why can't we just exchange symmetric keys with people we interact with physically?  We can now share our contact information with another party by touching our iPhones together.  How come we can't at the same time just exchange a list of (n) number of keys and use those for iMessage or another messaging platform, and then we do not run the risk of intercepted communications being broken.  We could then use a single key until there's reason to believe it needs to be changed for any reason and cycle down the list.



"Anyway, those were just my thoughts as someone who is interested in, but not in the business of, secure communications.  Would be happy to hear why I'm thinking about this wrong or why are there better ideas.  Anyway, hope you're doing well.  Big fan of the show.  Thanks, Trevor."



Okay.  So Trevor is thinking about all of this correctly.  Though today doing what he suggests feels like throwing the baby out with the bathwater.  The largest leap forward we ever made in cryptography was in switching from the early proprietary and secret non-keyed one-off encryption schemes over to open and public ciphers whose specific behavior was specific and set with a key.  This, of course, was the invention of symmetric key crypto.



The next huge step forward was the invention of asymmetric crypto, which gave us the ability to have public keying negotiations without fear of eavesdropping and with the incredible power that flows from the use of having separate public and private keys, each with their own roles.  Given the tremendous power of asymmetric keying, it is difficult to imagine returning to the limitations of a purely symmetric keying only world.



For example, one of the other features made possible by asymmetric keying is the ability to have perfect forward secrecy, since asymmetric keying allows us to encrypt under a different symmetric key for every new connection or conversation.  This means that if a symmetric key were ever to somehow escape, only that single conversation would be at risk.  Neither any other past conversations nor any in the future would be subject to disclosure.  This is a powerful feature that we lose if we're unable to safely negotiate new symmetric keys.



So Trevor is right that it's only the asymmetric keying that's put at risk by quantum computing.  And he's right that the new quantum safe replacement algorithms need time to mature and be tested.  But since virtually all of today's applications are based upon asymmetric keying, I cannot imagine, like, not having that and going backward.



The good news is that the industry's inertia, which we spend so much time lamenting on this podcast, will not pose a problem in this regard for this industry.  Closed systems like Apple's and Signal's and others will be able to move seamlessly to quantum safe crypto overnight.  And open systems like the web can be enforced from the end with the fewest players, namely the browsers.  Once Google announces that Chrome and Chromium's will have a drop-dead date for non-quantum safe algorithms, at some point well in the future, web servers will have no choice other than to support the new safe standards or find their visitors receiving scary-looking warnings meant to put pressure on those server operators to get their act together.  And at some point Chrome and other browsers will stop being willing to connect altogether.



We saw, for example, that there was a minuscule support now for TLS 1.0 and 1.1.  Everything is 1.2 and 1.3.  That happened pretty quickly, and that happened because support for 1.0 and 1.1 was withdrawn.  And if your browser won't use it, you know, your server can't serve it.  So I suspect that the wisdom we're already seeing by the industry moving forward on next-generation crypto, apparently well before it's actually needed, will serve us well and will likely keep us ahead of the day when quantum computing is first able to crack any asymmetric crypto.  So I think that in this instance it makes a lot more sense for us to move forward than backward.  Especially when, you know, what was back there was so limited compared to what we have today.



LEO:  On the other hand, quantum computing is not necessarily around the corner.



STEVE:  I completely agree.  You know, they had a big party when they managed to factor 37.



LEO:  I know.



STEVE:  So, wow.  Five bits.  Woohoo.  Or six, rather.  Six bits, wow, that's great.  What used to be four, now it's six.



LEO:  We've got a ways to go.



STEVE:  Yeah.  We're okay because they've got to get out to 1024 or 2048 or 4096.  So, you know...



LEO:  Is it the case that if you made a quantum-safe, you know, if you're forced to ever use quantum-safe crypto, it would be safer in a non-quantum world, too?



STEVE:  Well, we think.  Because the problem is it's still attackable, and there was one of those algorithms that was believed to be ready for post-quantum.  They found a flaw in it.



LEO:  It was weak, yeah.



STEVE:  Yeah.  And it got withdrawn.  So, you know, it's not the case that there's some new special magic because it's quantum safe that then means it is automatically safe for non-quantum attack.  That's not the case.



LEO:  So you'd have to test them both, yeah.



STEVE:  Yeah, yeah.  So for example, Leo, we are absolutely relying on the fact that nobody is going to somehow figure out how to factor large numbers.  If that happens, it's game over.



LEO:  Yeah, right.  And you mean, like, without a quantum computer, into some algorithmically, say, oh, you know, you can do that, it's pretty easy.  And then, yeah, we'd have to do everything over.  We'd be - public key crypto would die.  Yeah, but let's not go - so far, by the way, it has resisted all attempts to do that.



STEVE:  Amazingly so.



LEO:  Yeah.



STEVE:  And I mean, believe me, the prize, as I was lamenting, there's no Nobel Prize for math.  But, boy...



LEO:  The Hodge Prize is there, yeah.



STEVE:  You'd be tempted to make one.  You'd make one, if someone could come up with a way to factor huge numbers.  Because a lot of time has been spent.



LEO:  You'd get a Fields medal for sure.  You'd get something, yeah.



STEVE:  Yeah.  Okay.  So someone in GRC's newsgroup asked, modestly, he said:  "I feel really embarrassed about this, but I've never really understood how the root store in the OS and the keys in the browser give the protection, or as in last week's podcast mentioned, how having a key from a bad source, like a state government, allows intercepted traffic to be decrypted, even if I was logging into, for example, my local supermarket.  Is there a nice concise explanation anywhere to help me, and a follow-up with the more complex aspects?  I'm looking for some context to start my understanding on a sound footing.  Thanks."



And so here's what I wrote.  I said, well, I should first preface this by saying that I do occasionally receive tweets from people who say that they don't understand all of what's said on this podcast, but they get enough from it that it's still worth their while.  Somebody made a comment about sipping from a fire hose.  So I thought that this person's honest question might be appreciated by others who might also feel sometimes a little bit left behind.  So, very clearly and concisely, here's how all that works.  And we haven't actually talked about it for many, many years.  So it's worth a little refresher. 



When a browser connects to a web server, during that so-called "connection handshake," the remote web server sends the browser a claim of its identity in the form of a digital certificate.  A certificate is nothing more than a collection of information fields which, after having all been set correctly, are then signed.  After the certificate has been signed, it becomes tamper proof.  Any change in its data will break its signature.



In the case of web servers, the certificate that the web server sends to the browser will have been signed by a Certificate Authority.  And a certificate authority is nothing more than its name suggests.  It's an organization that the world and the world's browsers have all agreed to trust.  These certificate authorities go to varying degrees of effort to independently verify the identity of the organizations whose certificate they sign.  So if the certificate authority says that the entity presenting this certificate is, for example, Joe, and if we trust the certificate authority to have done its business properly, then we trust that this is indeed Joe, if the certificate that Joe is presenting us, signed by the Certificate Authority, is valid, and we trust the certificate authority.  So it's sort of transitive trust.



Okay.  So now the question is, which certificate authorities should we trust?  And that's where the root store comes in.  Certificate authorities also have their own certificates which attest to their identities.  And since they trust themselves to be who they say they are, they each sign their own certificate, which sounds a little weird at first, but that's the way it works.  So what we refer to as the "root store" is the collection of certificates belonging to all the certificate authorities whose signatures of website identity certificates we have decided to trust.  So now, when a website presents the browser with a claim of its identity, which will have been signed by some certificate authority, the browser looks in its root store for the matching certificate of the certificate authority that signed that identity claim.  If it's found, that authority's certificate contains the public key that's then used to verify the signature of the website's certificate.



Okay.  So that's part one.  That's how the system works.  Part two is how traffic interception can occur.  We've often talked about network traffic interception.  The traditional term for it is "man in the middle," meaning that some entity arranges to place their equipment in the path of communication so that they're able to monitor everything that flows through them.  Normally, the use of HTTPS for web traffic, with its TLS for encryption, would render this ineffective since all of the traffic moving past would be encrypted.  But if the man in the middle themselves was also a trusted certificate authority, the man in the middle could pretend to be the website at the far end of the connection.



So when the user was attempting to connect to, say, a racy dating site, that "appliance in the middle" would create and sign a web certificate for that racy dating site on the fly and return it to the user's browser.  If that appliance's certificate was trusted by the browser, no red flags would be raised, the padlock would be present, the connection would appear to be secure, and the user would 100% believe that they had connected directly to the racy dating site.



But they would have, instead, actually connected to the appliance that has interposed itself in the middle where all of their traffic, their username, their password, and their cookies, and everything they did while at that site would be decrypted and made available to that appliance.  In order to complete the illusion, the appliance itself would have made the connection to the dating site, pretending to be the user's browser.  So it would pass along everything that transpired in both directions, while being privy to the entire dialogue without any encryption to get in the way.



Now, this is already exactly what's being done in corporations that deeply monitor all of their employees' communications.  To do this, every web browser in the enterprise must receive and accept that "middlebox" appliance's certificate.  But that's only within the enterprise.  This allows for full transparent "deep packet inspection" of all of the enterprise's network communications.  And this is precisely the power that the EU is saying it wants for itself and for all of its member countries. Article 45 of the eIDAS v2 insists that without any oversight or governance, any member state can provide a certificate that all web browsers must, as a matter of European law, blindly and fully accept and trust without question.  Meaning anything any of those certificates have signed would then be trusted.



And there's only one possible reason for them wanting this, which is to allow for on-the-fly Internet web traffic interception, exactly as happens inside of corporations.  And that's acknowledged.  Their claim, and I believe their hearts are probably in the right place, is that they want to have the ability to provide additional identity verification of websites for their citizenry.  They want to be able to inject top-of-page identity assertion banners into EU website properties to provide an additional assertion of a site's trustworthiness.



It must be, because, you know, these lawmakers didn't come up with this, it must be that some misguided techie, sometime in the past, cooked up this idea and sold it to the EU's legislators, you know, sold them on the idea.  This techie explained that if they could get their own certificates installed into the world's few web browsers and operating systems - and after all, there are not many of those - then they would have the ability to inject whatever they might wish to, like national emergency warnings or other public interest messages, into every one of their citizens' downloaded web pages.



The trouble is that the EU and its member nations are very different from the employees of a private organization.  Any time an employee doesn't want to be spied upon, they can use their own smartphone to circumvent their employer's network.  And of course an employer's private network is just that, a private network.  The EU wants to do this for the entire public Internet from which there would be no escape.  So it seems quite clear that this is not going to happen.  But it's the exact way that it doesn't happen that should prove quite interesting.  And we will certainly be following along on the podcast.



And Leo, we now have the much-anticipated Windshield Barnacle.  It's in the show notes.



LEO:  Thank you, Simon Zerafa.  Where would we be without you?



STEVE:  That is exactly right.  Our frequent tweeter, Simon Zerafa, sent me a photo which raised more questions than it answered.



LEO:  This must be the UK.  I don't - this couldn't be in the U.S.



STEVE:  Nope.  It is all over the U.S., and it is a big hit.  We're going to get to the Cheyenne City Police and what they think.



LEO:  Okay.



STEVE:  For those who are not watching the video, what we see is a huge bright yellow slab which is covering most of a car's windshield.  It's about an inch thick and is hinged in the center.  In the upper right corner we see a membrane-style keypad and a display.  And along with the red warning "DO NOT MOVE THIS VEHICLE," in the lower right we see what is apparently the name of this thing, which is just too wonderful all by itself since it's called "The Barnacle."  And I love everything about this.



The reason it's "The Barnacle" is that underneath, two large full-contact suction cups, one on each half - and if you scroll down, Leo, you'll see it, I have a picture from the driver's side - two large full-contact suction cups, one on each half, are adhering this bright yellow eyesore to the windshield.  And like any good barnacle, it will not let go.  Its creator claims that it generates in excess of 1,000 pounds of resistance to being removed.  So you return to your car to find that this bright yellow barnacle has attached itself to your presumably wrongly parked car.  Either the meter ran out, you parked in an unauthorized location, you're taking up two parking slots, or perhaps you have several years' worth of unpaid parking tickets, and "The Barnacle" has just caught up with you.



LEO:  See, now, we have the boot.  You've seen the boot.



STEVE:  This is the replacement.  This is the evolution of the boot, Leo.



LEO:  It's the new boots.  What's wrong with the boot?  The boot is a thing you clamp onto the tire so a person can't drive away.



STEVE:  Just wait.  So The Barnacle has caught up with you.  Now what?



LEO:  Okay.  Now what?



STEVE:  This large yellow thing is stuck to your windshield, totally obstructing your view from the driver's seat and making it impossible to drive.



LEO:  Yes, clearly you can't drive, yeah.



STEVE:  No way.  So you are reduced to visiting the website that's shown on The Barnacle, or calling the toll-free number printed there.  You identify the device instance to the website or the operator on the other end, or maybe it's a bot, by its very clearly posted serial number.



LEO:  Ah.  Now I'm getting the advantage.  Okay.



STEVE:  And you're told how much you owe, which must be paid, in full, on the spot, by credit card.  Once you have entered your credit card payment information to pay the parking fine, you're told that your account has also been charged a separate refundable fee for The Barnacle itself, whose return to a nearby receptacle you will now be responsible for, once the device has released itself from your windshield.  After agreeing to this secondary contract, you are given the device's current one-time release code, which you enter into the membrane keyboard.  And then presumably, with a sigh of releasing suction, The Barnacle releases its grip on your windshield.  You then refold it in half and slip it into the conveniently located nearby parking lot return stand.



LEO:  Oh, there's one nearby.  That's nice at least, yes.



STEVE:  Yes, yes.  They have them conveniently located.  Note that The Barnacle is equipped with GPS and will sound an excruciatingly loud and piercing siren if it detects that the car is moved after having been "Barnacled."



LEO:  Ah.  You could drive with your cameras.  I know I could in my car.



STEVE:  Right, right.  The traditional way of dealing with scofflaws who ignore their parking tickets or other parking-related violations is to "boot" a wheel of their car to prevent it from being driven.  The trouble is, this requires two visits from the "booter," one to affix the boot and another to remove it.  So The Barnacle creates what is essentially "self-serve unbooting."  A parking ticket slipped under a windshield wiper is easy to ignore.  But affixing a Barnacle to a windshield can be done quickly and easily, it's going to generate far more revenue, and has a side effect.  Everyone passing by can see it, too.



LEO:  Quite visibly barnacled, yes.



STEVE:  The UC Davis Medical Center said:  "Our Parking and Transportation department has been using The Barnacle for over a year."



LEO:  Oh, lord.  This is actually almost offensive to me, but okay.



STEVE:  "It's lightweight, easy to maintain and store."



LEO:  Oh, god.



STEVE:  "Our parking enforcement team loves the fact that this device can be deployed in under five minutes.  It has certainly captured the attention of scofflaws and onlookers.  The Barnacle enabled our department to recoup over $40,000 in outstanding parking citations."



The Cheyenne Police said:  "Thanks to The Barnacle, we increased collections by $30,000.  In addition, people see The Barnacle on a vehicle, which prompts many to pay their own outstanding tickets."



LEO:  Yeah, they don't want to get Barnacled.  Yeah.



STEVE:  So they don't get Barnacled.  They said:  "The presence of The Barnacle is also causing more to abide by city parking rules in the first place, and we've written 1,000 fewer tickets."



LEO:  Oh, interesting.



STEVE:  "We collect more parking fines and write fewer tickets.  It's a win-win for the City of Cheyenne."



LEO:  Mm-hmm.



STEVE:  So anyway, it's barnacleparking.com.  YouTube has a lot more for anyone who's interested.  You can imagine that annoyed parkers have come back to their cars to find them Barnacled and have taken pictures and made videos and such.  Anyway, I thought it was a very clever use of technology.  The device itself does not need any cellular radio.  It just needs an inexpensive GPS receiver to detect whether the driver is somehow moving their car.  Otherwise, it just uses a combination unlock on a keypad, and the driver who removes the device is responsible for its return and recycling in the facility where it was attached.  I think it is genius.



LEO:  We're going to see a lot more of these, I think.



STEVE:  Yup.  They are, they're really taking off.  Apparently they have become incredibly popular on university campuses, where students are just, you know, have a reputation.



LEO:  Scofflaws.



STEVE:  Yeah, they're just scofflaws.  They say, "I'm going to park wherever I want to."



LEO:  It's $250 per Barnacle per month.  But how many Barnacles will you need?  Fewer and fewer as you use it.



STEVE:  Boy, they're making some money on those Barnacles.



LEO:  Yeah.  It's good money.



STEVE:  Unfortunately, I'm sure they have a patent on it.  Otherwise there would be Barnacle Bill and other offshoots.



LEO:  Okay, okay.



STEVE:  So last week I mentioned having finally landed upon a series of military hard science fiction that I was really liking.  A day later I decided that I needed to share my find with John Slanina, a.k.a. TWiT's JammerB.  Here's what I wrote to John, because of course he's an inveterate sci-fi reader.



LEO:  Oh, he loves your recommendations, yeah.



STEVE:  So I said:  "John, after having real trouble putting the fourth book in the long series down in order to sleep, or do anything else other than consume it, I'll be telling the podcast audience about it next week."  Which is what I'm doing right now.  "But I wanted to tell you what I've found.  Being the voracious sci-fi reader that you are, you may have already run across the author M.D. Cooper (Malorie Cooper), who has created an extensive future universe for humankind under the banner of 'Aeon 14.'"



In terms of the overall timeline, where I began with "Outsystem" was not the earliest in the timeline, but it is the first book Malorie wrote, so she and her co-authors went back later and filled in the earlier timeline.  Here's what she explained.  She said:  "Whether you're a new reader, or you have a hundred titles under your belt, one thing is for sure, Aeon 14 is huge.  Hi, my name is Malorie Cooper (M. D. Cooper), and I created Aeon 14 as an imagining of a possible future history for humanity.  The story covers hundreds of characters, sweeping across over 10,000 years."



If you're just getting started, I recommend beginning where I did, with "Outsystem."  You can also check out the primer on the 42nd Century to get yourself acquainted with the setting.  So, okay.  So it's Aeon14.com.  Everything is Kindle Unlimited, and Audible is available.



LEO:  Oh, good.



STEVE:  It's possible to grab a single, what she called an "omnibus" that contains the first four books in a single download.  Oddly, this "Intrepid Saga" includes a fourth book.  It's the next book in the next series, and this is the book that really hooked me, Book No. 4.  I mean, 1 through 3 were great; but 4, I just could not put it down.  Things have really started to come together.



LEO:  They look a little pulpy.  Is it pulpy?



STEVE:  So here's what I wrote.  So I finished Book No. 4, and I'm into the next one.  And it has not disappointed.  John replied to my first email that the timing of my recommendation was perfect for him because he had just finished the book for Stacey's book club and needed something next.



LEO:  The John Scalzi book, "Kaiju Preservation Society," which he loved.



STEVE:  So I've always shared my sci-fi reading discoveries; and, boy, we have had some wonderful reading through the years of this podcast.  So I wanted to share this one, too.  I just an hour ago asked John if he had any feedback.  I cannot share all of what he wrote since I don't want any spoilers, but part of what he replied was:  "They've made it to Ceres, and they're planning their visit to Callisto.  Quite enjoying it!  Love the AIs some of them have in their heads.  And so much cool tech."  So it's still early for him, and a lot more ultra-cool tech is in store.  But I would say that it's been a hit for John, too.



So the series may not be for everyone.  It is more pulpy, campy, space operatic, and in some ways a lot more fun and frolic-y than any of Peter Hamilton's, for example, wonderful works.  And it does offer a lot of action.  Lots of stuff gets "blowed up."  And there's a never-ending stream of very cool tech.  It is by no means high brow, so I won't attempt to defend it on that basis; but I've read enough to be able to heartily recommend it, and I am having a lot of fun with it.



LEO:  I can't wait.  By the way, it's also available for free on Audible, if you're an Audible subscriber.  So she obviously made a deal with Amazon, a deal with the devil.  But that's good for us.  We can see it.



STEVE:  So, yeah.  And of course Kindle Unlimited for me.  We now know that Kindle Unlimited plan authors are remunerated based upon the pages that are actually read.



LEO:  Oh, interesting.



STEVE:  And it's too bad that re-reading pages probably doesn't create a bonus since there are many passages I have enjoyed more than once.  I mean, it is really good.  And something happened, I have to say, at the beginning of Book 4, where the writing seemed a little bit less great.  But maybe it was just a bad time.  I don't know.  But it got fixed.  So I just, you know, if anyone gets into the beginning of Book 4 and says "What happened," just hang in there a little bit longer because it fixes, and then it really takes off.



LEO:  I'm downloading it now.  I'm going to listen on the way home.  You had me...



STEVE:  I hope it works for you.



LEO:  Yeah.  You know, the cover is so pulpy, I get the idea, you know.



STEVE:  Oh.  And, well, so it's easy to find.  Aeon14.com I wrote in my notes.  And speaking of 14.



LEO:  Yes?



STEVE:  If that's your age, you'll also enjoy all of her books' cover art.



LEO:  Yeah, they're sexy covers, yeah, yeah.  Well, that's okay.  I mean, not super.  You know, she's just - it's a fairly skintight spacesuit, that's all.



STEVE:  That's what we want in spacesuits, Leo.



LEO:  You don't want a lot of excess fabric.



STEVE:  Unh-unh, no.



LEO:  Just gets in the way.



STEVE:  That can just cause trouble.



LEO:  Yeah.  And it looks like the hero is a woman, so that's good.



STEVE:  Actually, yes.  All of the main characters, I mean, we have guys around, but they're just sort of to keep the women busy.



LEO:  That's as it should be, I believe, yeah.  Let the women do the work.  We'll just - we'll be their toys.  That's good.  I like it.



STEVE:  Yup.



LEO:  Okay.  Thank you for the recommendation.  And this came from - did this come from John Slanina or...



STEVE:  No, from me.



LEO:  You found it.



STEVE:  I found it somehow.



LEO:  And John loved it.  That's good.



STEVE:  I had read the first book, or maybe the first three, at some point.  But then, you know, something shiny came along and distracted me.



LEO:  Yeah.



STEVE:  Now I'm back to it.  I read them all again and really liked them again.  I thought, okay.  And, boy, it's - now I can't wait to be done with the podcast.



LEO:  Oh, wow, that good.  Well, me neither, then.



STEVE:  So we have a classic case of attacks never getting weaker, only ever getting stronger.  In this case, a theoretical weakness that was first posited 27 years ago, back in 1996, has been found to be practical.  It's blessedly rare, but still practical.  What this means for us today is that the private keys protecting some SSH servers which use RSA keys can leak to an entirely passive observer if, for any reason, a mistake is made during the generation of the RSA signature.



So the first of two papers from back then was titled "Memo on RSA signature generation in the presence of faults."  A ways into this paper the author writes:  "This implies that, even if there is the tiniest probability that an error occurs during RSA signature generation, the generator of an RSA-signature must make sure that each signature generated is indeed correct."



The following year, in 1997, another paper was published titled  "On the Importance of Eliminating Errors in Cryptographic Computations."  The Abstract of this paper paints the picture very clearly, and a little starkly.  They say:  "We present a model for attacking various cryptographic schemes by taking advantage of random hardware faults.  The model consists of a black box containing some cryptographic secret.  The box interacts with the outside world by following a cryptographic protocol.  The model supposes that from time to time the box is affected by a random hardware fault causing it to output incorrect values.



"For example, the hardware fault flips an internal register bit at some point during the computation.  We show that for many digital signature and identification schemes, these incorrect outputs completely expose the secrets stored in the box.  For example, the secret signing key used in an implementation of RSA is completely exposed from a single erroneous RSA signature."



Okay.  So the point is, this fundamental brittleness of RSA has been known for 27 years.  Our primary takeaway at this point is the somewhat surprising news that, terrific as the RSA public key cryptosystem is, it is also quite fragile to information disclosure in the event of a computational error.  If any mistake is made for any reason during the computation of an RSA signature, that mistaken signature, along with any valid signature made under the same private key, can be used to completely expose that server's private key.  From there, this allows the server to be transparently impersonated by anyone who can arrange to place themselves in the information flow.



Red Hat considered the implication of this back in September of 2015 with regard to TLS connections.  Their piece was titled "Factoring RSA Keys With TLS Perfect Forward Secrecy," and this is what they found.  They wrote:  "Back in 1996," referring to the first paper, Lenstra described an attack such that, if a fault occurred during the computation of a signature, an attacker might be able to recover the private key from the signature.  At the time, the use of cryptography on the Internet was uncommon, and even 10 years later, most TLS or HTTPS connections were immune to this problem by design because they did not use RSA signatures.  This changed gradually, when forward secrecy for TLS was recommended and introduced by many websites.  Whoops.



We evaluated the source code of several free software TLS implementations to see if they implement hardening against this particular side-channel attack, and discovered that it is missing in some of these implementations.  In addition, we used a TLS crawler to perform TLS handshakes with servers on the Internet, and collected evidence that this kind of hardening is still needed, and missing in some of the server implementations.  We saw several RSA key leaks where we should not have observed any at all.



So this issue has been quietly lurking in the backs of the minds of cryptographers for decades, which brings us to today and the recently discovered and confirmed vulnerability of many, I should say several, SSH servers.  A group of four researchers from UC San Diego at La Jolla have just published their research under the title "Passive SSH Key Compromise via Lattices."  It will be presented, their research will be presented two weeks from now during the upcoming ACM SIGSAC Conference on Computer and Communications Security, which is CCS 2023.  It's being held this November 26-30 in Copenhagen, Denmark.



They wrote:  "We demonstrate that a passive network attacker can opportunistically obtain private RSA host keys from an SSH server that experiences a naturally arising fault during signature computation.  In prior work, this was not believed to be possible for the SSH protocol because the signature included information like the shared Diffie-Hellman secret that would not be available to a passive network observer.  We show that for the signature parameters commonly used for SSH, there is an efficient lattice attack to recover the private key in case of a signature fault.  We provide a security analysis of the SSH, IKEv1, and IKEv2 protocols in this scenario, and use our attack to discover hundreds of compromised keys in the wild from several independently vulnerable implementations."



So the best way to get a better feeling for what's going on here is to look at how they summarized their overall findings.  They wrote:  "RSA digital signatures can reveal a signer's secret key if a computational or hardware fault occurs during signing with an unprotected implementation using the Chinese Remainder Theorem and a deterministic padding scheme.  This attack requires only a single faulty signature, the public key, and a single GCD (Greatest Common Divisor) computation, and it has been exploited extensively in the cryptographic side channel literature on active fault attacks.



"In a 2015 report" - and this is the Red Hat report - "Weimer observed that this same vulnerability could be exploited in the context of TLS by an attacker without physical access to a machine, simply by connecting to machines and waiting for a fault to occur during computation.  He traced several of the failures he observed to failing hardware.



"In 2022, Sullivan et al. observed that this flaw remained exploitable on the open Internet, and used passive network countermeasures to compute TLS private keys from vulnerable implementations that appeared to experience hardware failure.



"However, this vulnerability was not believed to be realistically exploitable in the context of other popular network protocols like IPSec" - which of course is extensively used for VPNs or SSH - "because the signature hash includes a Diffie-Hellman shared secret that a passive eavesdropper would be unable to compute, thus ruling out the single-signature Greatest Common Divisor attack.  Because a passive adversary can typically collect significantly more data than an active adversary who must participate in every Diffie-Hellman exchange, this belief represented a significant underestimate of the cryptanalytic capabilities of such passive adversaries against SSH and IPSec compared to TLS v1.2.



"In this paper, we show that passive RSA key recovery from a single faulty signature is possible in the SSH and IPSec protocols using a lattice attack.  In this context, a passive adversary can quietly monitor legitimate connections without risking detection until they observe a faulty signature that exposes the private key.  The attacker can then actively and undetectably impersonate the compromised host to intercept sensitive data.  We cast the key recovery problem as a variant of the partial approximate common divisor problem, and we show that this problem is efficient to solve for the key sizes and hash functions used for SSH and IPSec."



And finally:  "We then carry out Internet-wide scans for SSH and IPSec to measure the prevalence of vulnerable signatures in the wild.  We find multiple vulnerable implementations that appear to be due to different classes of hardware flaws.  We also carry out a retrospective analysis of historical SSH data collected over the course of seven years, and find that these invalid signatures and vulnerable devices are surprisingly common over time.  Our combined dataset of around 5.2 billion SSH records contained more than 590,000 invalid RSA signatures.



"We used our lattice attack to find that more than 4,900 revealed the factorization of the corresponding RSA public key, giving us the private keys to 189 unique RSA public keys, and thus the servers that were using those.  We also analyze passively collected SSH network data.  In addition to the signature vulnerabilities we were searching for, our analysis gives us a window into the state of SSH, IKEv1, and v2 deployment landscape.  We observed a number of vulnerable and non-conformant behavior among IPSec hosts in particular."



Okay.  So by passively examining SSH handshake traffic on the Internet, and even recordings of previous handshake traffic, they were able to obtain the secret private keys belonging to 189 different devices on the Internet.  The other thing they had to say that was interesting was what they learned about the SSH devices which were occasionally generating invalid and thus vulnerable signatures.



They said:  "Our research identified four manufacturers of devices susceptible to this key recovery attack.  We disclosed the issue to Cisco on February 7th of this year, 2023, and to Zyxel on March 1st, 2023.  Both teams investigated promptly, although limitations in the historical scan data made it challenging to identify the software versions that first generated the vulnerable signatures and reproduce the issue.  Cisco concluded that Cisco ASA and FTD Software had introduced a stable mitigation in 2022, and was investigating mitigations for Cisco IOS and IOS XE Software.  Zyxel concluded that the issue had affected ZLD firmware version 3.30 and earlier, which had been end-of-life for years.  By the time of our disclosure, the ZLD firmware had begun using OpenSSL, which mitigates this issue.



"Our attempts to contact Hillstone Networks and Mocana were unsuccessful, and we submitted the issue to the CERT Coordination Center on May 18th, 2023 for assistance with disclosure.  We received no additional information from CERT during the 45-day disclosure period.  We considered notifying operators of affected devices whose keys had been recovered, but we determined this would be infeasible.  Even after combining publicly available data with the historical scanning data, we were unable to determine which organization was responsible for a particular device, whether the device was still in use today, or up-to-date contact information for the device's current operator.  We also lacked practical and actionable advice for owners of affected devices until the manufacturers completed their investigation."



So now we knew that communication protocols using RSA signature generation which have not been explicitly hardened against the possibility of mistaken signature computation are prone to having their private keys stolen if anyone is watching and paying attention.  And in today's world it would be difficult to imagine that with papers about this dating back to 1996, 1997, 2015, and 2020, that the NSA might not have already done the math and may have been keeping quiet about this in case it might come in handy.  We don't know what we don't know, which is why I think that this sort of academic poking at the protocols we have in use can be so very important.



I titled this podcast "What if a Bit Flipped?"  And now we know what if a bit flipped.  What we still don't know is why that bit may have flipped.  The literature doesn't have a lot to say about it, but it boils down to cosmic rays just happening to hit a microscopic transistor at the right moment to cause an unrepeatable event, or a system's power supply at the beginning of its death spiral.



As we know, I had this happen to me recently with my Netgate SG-1100 router.  On the surface, when it was working, it was working.  But was it really?  What if I'd been using it as an SSH endpoint?  Might it have glitched at just the right moment and caused a signature mistake?  I'm sure that pfSense was using OpenSSL, which had already been hardened against this a long time ago.  But my point is some hardware out there hasn't been.  And if the hardware is having a problem, then that can result in a completely disclosure of that system's private keys.



I think that the best takeaway is that we all sometimes see our computers do something weird and inexplicable.  Since software bugs are clearly by far the most common cause, we would rarely think that we just experienced a cosmic ray event.  But who knows?  And the cause of a mistaken signature computation doesn't have to be hardware.  A device driver might not have properly restored the machine's registers following a hardware interrupt that happened to occur right in the middle of that critical signature computation.  Not long ago we tracked down some weird one-off problems in SpinRite to exactly that cause.  After I wrapped SpinRite in its own protection, we've never had another such error occur.



The best news is that the crypto industry has learned that some cryptographic processes are highly sensitive to mistakes occurring from any source, even cosmic background radiation. And the systems that were late to protecting themselves have been identified.  Getting them fixed in the field may never happen.  But new systems won't be shipping with those cosmic ray vulnerabilities.



LEO:  Interesting.  Or you could just wrap your computer in tinfoil.



STEVE:  Yeah.  You know?  Now, then you have an overheating problem, and that might cause different sorts of errors.



LEO:  Other kinds of bit flips.



STEVE:  That's right.



LEO:  You know, Linus Torvalds said very famously a few years ago that it's insane not to have error-correcting RAM in all machines, not just servers; that general purpose PCs should have ECC.  He said it's undoubtedly, he knows as a developer of Linux, the source of many blue screens and crashes are bit flips.



STEVE:  Yup.  



LEO:  But the crypto implantations are even more significant, of course, yeah.



STEVE:  Yes, yes.  And we know that Rowhammer, that is, the deliberate hammering on RAM, can cause spontaneous flips.  But so can chance RAM events.



LEO:  Right.  And cosmic rays.



STEVE:  Yup.



LEO:  But ECC I think was [crosstalk]; right?  I mean, you know, just says have ECC in your hard drive.  And as you have often said, hard drives are incredibly unreliable.  You know, they're constantly making errors.  But ECC fixes it without your even knowing it, yeah.



STEVE:  Yup.



LEO:  I think that's a very interesting arena.  Someday I want you to talk about the technologies of ECC.  If you haven't already.  You might have.  We have done 948 freaking episodes.



STEVE:  Bye.



Copyright (c) 2023 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.






GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#949

DATE:		November 21, 2023

TITLE:		Ethernet Turned 50

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-949.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  Is there any such thing as truly free privacy?  What has Elon done now?  What's the latest new tactic in post-breach cyber-extortion?  Has Europe finally come to their senses over old and creaky proprietary radio encryption?  What new forthcoming iPhone communications feature took everyone by surprise?  What discovery did I make for super-secure code signing?  Just how sticky are those barnacles?  What's a good way to measure USB drive speed?  Is the EU's proposed eIDAS 2.0 QWACs system as bad as it seems?  And if it passes into law as-is, CAN companies realistically say no?  What's my favorite little PC platform for building security gateways?  Why couldn't we just use the good part of a fake drive?  What should ex-LastPass users watch out for in their credit card statements?  And, finally, we recognize the 50th birthday of Ethernet and look back at the history of its creation.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here with some theories on how to release the Windshield Barnacle; good ways to measure USB drive speed; his struggles with code signing.  One last speed bump on the way to the latest version of SpinRite 6.1.  And, finally, a look at Ethernet, 50 years old this year.  Steve's got some great stories.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 949, recorded Tuesday, November 21st, 2023:  Ethernet Turned 50.



It's time for Security Now!, the show where we cover the latest security news, explain how things work in the real world, occasionally recommend some good sci-fi, and it's all thanks to this guy right here, Mr. Steve Gibson.  Hello, Steve.



STEVE GIBSON:  Yo, Leo.



LEO:  How you doing?



STEVE:  Good to be with you once again.



LEO:  Yeah.



STEVE:  For this pre-Thanksgiving podcast.



LEO:  That's right.  I won't be here next week.



STEVE:  And as I understand it, we're losing you for a couple weeks after this.



LEO:  Just one week.  One week.



STEVE:  Oh, okay.  Oh, okay.



LEO:  I'll be back two weeks from today.  Next week I'm missing the whole week.  Not a vacation.  It's some sort of weird psycho retreat.



STEVE:  Well, and if it's what Lisa did, you're not allowed to take anything with you. 



LEO:  Nothing.  I had to get a...



STEVE:  They strip you naked.



LEO:  I had to get a Timex.  No, no, you can wear clothes.  But I had to get a Timex to replace the Apple Watch.  When you check in they say, "Give me your cell phone."  And so it'll be interesting to be completely disconnected.  I think that's probably the main benefit, frankly.



STEVE:  I think it'll be nice; won't it?



LEO:  Yeah, I'm looking forward to it.



STEVE:  I mean, like just to say, like, you can't do anything except, you know, think.



LEO:  I'm just hoping that Sam Altman doesn't get fired again because then I'll really be out of the loop.  A lot can happen in a few days.



STEVE:  That's a really good point.  I mean, you spend so much time now staying current.  You were just showing us your massive feed that you spend eight hours a day going through.  Imagine a week going by.  I mean, anything could happen.



LEO:  Well, it was very hard for me over the weekend, because we were in Las Vegas for the Formula One race, to be reading these stories and saying I'm not going to get to talk about this on TWiT.  It was a challenge.  It was hard.  I ended up doing a little mini-TWiT for Lisa, "Let me tell you what happened today."  Poor Lisa, she had to put - she was very kind about it.  She put up with that.  So what's happening in your world this week?



STEVE:  So we've got a lot of fun stuff to talk about this week.  We're going to answer the question, is there any such thing as truly free privacy?  What's the latest new tactic in post-breach cyber extortion?  Has Europe finally come to their senses over old and creaky proprietary radio encryption?  What new forthcoming iPhone communications feature took everyone by surprise last week?  What discovery did I make for super-secure code signing?  Just how sticky are those barnacles?  What's a good way to measure USB drive speed?  Is the EU's proposed eIDAS 2.0 QWACs system really as bad as it seems?  And if it passes into law as-is, can companies realistically say no?  What's my favorite little PC platform for building security gateways?  Why couldn't we just use the good part of a fake drive?  What should ex-LastPass users watch out for in their credit card statements?  And, finally, because this podcast is titled "Ethernet Turned 50," we recognize the 50th birthday of Ethernet and look back at the history of its creation.



LEO:  Wow.  I'm feeling old, Steve.  Holy cow.  I don't know, do you remember when Ethernet first appeared in your life?



STEVE:  Yes.  I was actually onsite at the time.  I will mention that.  I was there with Bob Metcalfe...



LEO:  What?  What?



STEVE:  ...and the connection of the first Xerox graphics printer and...



LEO:  Oh, so you had some firsthand experience of this.



STEVE:  '73 was the year I graduated high school, and the summer that I worked for SAIL, the Stanford University AI Lab...



LEO:  Oh, my god.



STEVE:  ...and they received the first Xerox graphics printer from PARC.  



LEO:  Wow.



STEVE:  Which was, you know, nearby in Palo Alto.



LEO:  Wow.  Okay.  Okay.



STEVE:  You would not believe, Leo, that it is now - well, you would - that it's now impossible for me to type sequel, SQL...



LEO:  Right.



STEVE:  ...without putting an R in, SQRL.



LEO:  Oh.



STEVE:  My hand just for so many years...



LEO:  Yeah.



STEVE:  ...I was typing SQRL...



LEO:  How funny.



STEVE:  ...that it's embedded in my autonomic nervous system.  It just, I go, oh, darn it, and I have to go back and take the R out.



LEO:  I just wish it were embedded into our Internet nervous system, frankly.



STEVE:  Yeah, yeah.



LEO:  Well, that's how the cookies crumble.



STEVE:  So our Picture of the Week is a sort of a rendition, a rendering of a famous coffee-stained napkin on which Bob Metcalfe first sketched out his concept for the Ethernet's physical layer connection.  This was in 1973 when PARC needed to connect all of the different things.  They had created this Alto work station, which is also famously what Steve Jobs saw when he was taken on a tour through PARC.  Of course Palo Alto's, Xerox's PARC, Palo Alto Research Center, where he first saw a GUI interface and immediately recognized what it was.  So this was the beginning of hooking everything together.  So we'll have much more to say about that at the end of the podcast.  But it is a cool picture, and it is, like, it's what, you know...



LEO:  It was a napkin.  There really was a real napkin with that drawing.



STEVE:  There actually was a real physical napkin.



LEO:  Is it around, or did it get destroyed?



STEVE:  That's a good question.  I was wondering that myself.  There are several pictures that pretend to be it, but they're not.  They're fakes.  So, I mean, and, you know, what are the chances that some random sketch on the back of a napkin would, like, actually have value in the future?  Leo, what are the chances that some measly little bitcoins that you created overnight could actually...



LEO:  Stop it, stop it.



STEVE:  Yeah, yeah, that's right.



LEO:  By the way, I love it...



STEVE:  That's right.



LEO:  ...that the name Ethernet comes from, and he's got it on the napkin, "the ether."



STEVE:  Yes, yes.  Which is to say his conductive medium would be medium through which the messages travel.



LEO:  And the medium's unimportant.  It's the message protocol that matters.  So he just writes "The Ether."  Love it.  Love it.



STEVE:  Yeah.  So Last Thursday, Meredith Whittaker, whom we've spoken of before, she's Signal's CEO President, and of course whom we saw first stand up to the UK with their absolute refusal to compromise on Signal's encryption, along with a lead developer, Joshua Lund, authored a lengthy fundraising wake-up call in the form of a breakdown of their expenses, of Signal's expenses.  Now, while they're a registered 501(c)(3) nonprofit organization, they maintain a staff of 50 busy people.  And actually, when we put this in context as I will a little bit later here, it's shocking that it's only 50 for when you consider how much these 50 people are doing.  And they also describe their significant infrastructure costs.  And there are another few surprises that I'll share because I'm going to kind of pick and choose among this very long posting.  But I'll share the beginning of it to give everyone a sense for what they're aiming for here.  It starts out under the headline "Privacy is Priceless, but Signal is Expensive."



And they write:  "Signal is the world's most widely used truly private messaging app, and our cryptographic technologies provide extra layers of privacy beyond the Signal app itself.  Since launching in 2013, so, okay, we're now 10 years out, the Signal Protocol  our end-to-end encryption technology  has become the de facto standard for private communication, protecting the contents of billions of conversations in WhatsApp, Google Messages, and many others.



"Signal also continues to invest in research and development in the pursuit of extending communications privacy.  This commitment underlies our recent work to add a layer of quantum resistance to the Signal Protocol, which we talked about a few weeks ago, and our previous work on metadata protection technologies that help keep personal details like your contact list, group membership, profile name, and other intimate information secure.  This singular focus on preserving the ability to communicate privately is one reason that we work in the open, documenting our thinking and making our code open source and open to scrutiny so you don't have to take our word for it.



"Signal is also a nonprofit, unlike almost every other consumer tech company.  This provides an essential structural safeguard ensuring that we stay true to our privacy-focused mission.  To put it bluntly, as a nonprofit we don't have investors or profit-minded board members knocking during hard times, urging us to 'sacrifice a little privacy' in the name of hitting growth and monetary targets."  They don't have any growth and monetary targets.



"This is important," they said, "in an industry where 'free'" - and they have that in air quotes - "'free' consumer tech is almost always underwritten by monetizing surveillance and invading privacy.  Such practices are often accompanied by 'growth hacking' and engagement maximization techniques that leverage dark patterns to keep people glued to feeds and notifications.  While Signal is also free to use, we reject this kind of manipulation, focusing instead on creating a straightforward interpersonal communications app.  We also reject business models that incentivize such practices.



"Instead of monetizing surveillance, we're supported by donations, including a generous initial loan from Brian Acton.  Our goal is to move as close as possible to becoming fully supported by small donors, relying on a large number of modest contributions from people who care about Signal.  We believe this is the safest form of funding in terms of sustainability: ensuring that we remain accountable to the people who use Signal, avoiding any single point of funding failure, and rejecting the widespread practice of monetizing surveillance."



And finally:  "But our nonprofit structure doesn't mean it costs less for Signal to produce a globally distributed communications app.  Signal is a nonprofit, but we're playing in a lane dominated by multi-billion-dollar corporations that have defined the norms and established the tech ecosystem, and whose business models directly contravene our privacy mission.  So in order to provide a genuinely useful alternative, Signal spends tens of millions of dollars every year."  And they finished with this intro:  "We estimate that by 2025, Signal will require approximately $50 [five zero] million a year to operate.  And this is very lean compared to other popular messaging apps that do not respect your privacy."



Okay.  So then their blog post proceeds from there to substantiate that claim and to further detail the nature of their current cash outlay for all facets of what it takes to make Signal go.  I've got a link in the show notes for anyone who's interested:  signal.org/blog/signal-is-expensive.



LEO:  You know, it's interesting because this ties in - she wrote this before the OpenAI kerfuffle of the past weekend.  But it ties in very much to that because OpenAI was founded to be a nonprofit like Signal.  But it got so expensive they had to create a for-profit arm.  And really what you're seeing is the tension between the for-profit, very, very successful OpenAI and the not-for-profit AI should be not run by a big profit-making company arm.  And look who won.  You know, it's amazing that Signal can survive at that run rate.



STEVE:  And we're about to hear some gloom and doom from me.



LEO:  Yeah.



STEVE:  So I said I'm not a big Signal user, though I do have it installed on my phone and desktop because back when I was bringing up GRC's XenForo web forums, one of our listeners, Rasmus Vind, who knew XenForo and PHP, came to my rescue and implemented the forum's interface to SQRL, which by the way remains in heavy use today and is the way most of our forum users logon, I've been told.  So Rasmus suggested that we use Signal to converse in real time, and it worked perfectly.  But I have to admit that I never even stopped to consider the fact that Signal was both free and devoid of any annoying advertisements or other nonsense.  And in thinking about it, I realized that we've become very accustomed to receiving "free" things in our lives.



LEO:  Yeah, that's the problem, yep.



STEVE:  Yes.  And I have "free" in air quotes because something pays for the things we receive.  You know, we grew up, well, for those of us with thinning or gray hair, listening to free AM and FM radio and watching free television, all sprinkled with ads.  And today it's still possible to get lots of high-quality free stuff if you don't mind ads, like this podcast.  GRC's various services like ShieldsUP!, and our growing library of ad-free freeware on a 100% ad-free website, is entirely financed by the sales of SpinRite.



LEO:  See, that's interesting because, I mean, it's one thing for you to write a program and then distribute it.  But you've got ongoing costs.  With ShieldsUP! you've got to run a server all the time.



STEVE:  Yup.



LEO:  Yeah.  But you don't show ads.



STEVE:  No, it's 100% financed by SpinRite sales.



LEO:  But I would say, by the way, that that is not really the case.  It's 100% financed by Steve Gibson, who happens to have an income thanks to SpinRite.



STEVE:  That's a good point.



LEO:  You're really paying for it.  It's out of your pocket,  yeah.



STEVE:  Yeah, that is the case.  You know, so we have 8,661,248 people as of this morning who have downloaded and used our DNS Benchmark.  ShieldsUP! is beyond 101 million uses.  68,000 people have downloaded ValiDrive since it was released.  All these people received something of value for free, but only because a sufficient number of other people have been interested in having access to SpinRite and were willing to pay for it.



So Google exists because of advertisements and extensive tracking.  The TWiT network and this podcast are here thanks to advertisements and the direct support of Club TWiT members.  And all of the stuff I've been able to do, create, and give away is thanks to people purchasing SpinRite.



But what of Signal? What we're living in now is an "attention economy" where commercial interests are vying for, and are willing to pay dearly for, people's attention.  But not Signal.  It occurs to me, and I really hope I'm wrong, that this is going to be a heavy lift and that they're probably in a very tough spot.  For all the best and right reasons, they've put themselves in the spot of needing to ask for voluntary donations from a global community that's grown accustomed to getting everything for free, or at least apparently for free.  In other words, not needing to directly part with any of their hard-earned cash.



So how will Signal support itself?  Can it support itself?  It'll be up to those who use it and who depend upon its integrity, or perhaps other wealthy donors who want to see it survive, to keep it going.  Do most people actually care - and this is the question.  Do most people actually care about the integrity of their messaging service?  I have come to doubt it.  It's fine if it's there for free.  But will they pay for it?  Will its users even understand the value that they're receiving in return?  Or is it just a convenient messaging app that they're using without really understanding what's behind it and what it takes to offer such a service with absolutely no strings attached?  And that's what differentiates it from everything else.  Signal is 100% absolutely no strings attached.



Elsewhere in their blog they write:  "Data is profitable, and we're a nonprofit, focused on collecting as little data as possible.  Most tech companies collect and create as much data as they can.  They build large data warehouses, and then later invent new terms like 'data lake' when their unquenchable thirst for more of your private information can no longer fit within the confines of a single warehouse.  Their default move is to store everything for as long as they can in an easily accessible and unencrypted format, suffering data breach after data breach after data breach, hoping to monetize this data by indirectly, or directly, selling it to advertisers or using it to train AI models.	Again, data is profitable."



They noted that their data storage costs are far less than other messaging companies specifically because, unlike everyone else, they only retain the barest minimum needed for the system to function.  And it's been designed from front to back not to store anything that it doesn't need to.



One of the surprising line items they highlighted was the cost of registration.  Get a load of this.  They said:  "Signal incurs expenses when people download Signal and sign up for an account, or when they re-register on a new device.  We use third-party services to send a registration code via SMS or voice call in order to verify that the person in possession of a given phone number actually intended to sign up for a Signal account.  This is a critical step in helping to prevent spam accounts from signing up for the service and rendering it completely unusable, a non-trivial problem for any popular messaging app.



"Signal's registration service routes registration codes over multiple telephony providers to optimize delivery across the globe, and the fees we pay to third-party vendors for every verification code we send can be very high.  This is in part, we believe, because legacy telecom operators have realized that SMS messages are now used primarily for app registration and two-factor authentication in many places, as people switch to calling and texting services that rely on non-telecom Internet data.  In response to increased verification traffic from apps like Signal, and decreased SMS revenue from their own customers, these service providers have significantly raised their SMS rates in many locations, assuming correctly that tech companies will have to pay anyway.  The cost of these registration services for verifying phone numbers when people first install Signal, or when they re-register on a new device, currently averages around $6 million dollars per year."



LEO:  Wow.



STEVE:  Leo, that's like an eighth of their budget, or a seventh of their total budget, or a sixth.  It's astonishing.  And just think of all the people who might download and then set up the app, then wind up not using it that much.  My phone is chock full of such apps.  Why would I pay anything for anything for something that I never use?  Yet they're footing the cost of my sign-up verification whether I ever use it again or not.  Another thing I appreciated is how lean they're running.  They have 50 employees total.  How does this compare to other similar services?



They wrote:  "Signal is not a collection of privacy-preserving services that route end-to-end encrypted messages and calls around the world."  That is, it isn't just that.  "It's also a set of cross-platform apps and modular development components (libraries) that make this type of private communication possible in the first place.  Because the norm is surveillance, we're often required to create or modify our own libraries from scratch, swapping in privacy instead of using more common frameworks that assume surveillant defaults.  Swimming against the tide of an ecosystem whose incentives and infrastructure promote surveillance and privacy invasions is, of course, more time-intensive and more expensive, and requires dedicated and experienced people.



"First, we have three distinct client teams, one for each platform:  Android, Desktop, and iOS.  These teams are constantly working, adjusting to operating system updates, building new features, and making sure the app works on a wide variety of devices and hardware configurations.  We also have dedicated engineering teams that handle the development and maintenance of the Signal Server and all its infrastructure, our calling libraries like RingRTC, and core libraries like LibSignal.  These also need constant development and monitoring.



"Product and design teams help shape the future of the app and determine how it will look and function, while our localization team coordinates translation efforts across more than 60 [six zero] languages.  We even have a full-time in-house support group that interfaces with people who use Signal and provides detailed technical feedback and real-time troubleshooting information to every other team.  This is an essential function, particularly at Signal, because we don't collect analytics or telemetry data about how people are using Signal.  So otherwise we would never know.



"This is a lot of work, and we do it with a small and mighty team.  In total, around 50 full-time employees currently work on Signal, a number that is shockingly small by industry standards.  For example, LINE Corporation, the developers of the LINE messaging app popular in Japan, has around 3,100 employees; while the division of Kakao Corp. that develops KakaoTalk, a messaging app popular in Korea, has around 4,000 employees.  Employee counts at bigger corporations like Apple, Meta, and Google's parent company Alphabet are much, much higher."  Of course they're also doing lots of other things.



And the last bit that I wanted to share is this:  They wrote:  "Growth in Signal translates into increased infrastructure costs, and having more infrastructure requires more labor.  As of November 2023" - today, this month - "Signal's server network is regularly responding to around 100,000 requests per second, and we routinely break our previous records.  A funny thing happens when a globally accessible service starts handling billions of requests every day.  Suddenly, one-in-a-million possibilities are no longer unique or rare, and unlikely situations become more and more common as Signal grows.



"It's not unusual for our engineers to do things like write custom code to reproduce an esoteric and complicated IPv6 connectivity issue that's affecting people running an arcane operating system configuration in specific regions, but only when connected via a certain set of Internet service providers."  Whew.  "Troubleshooting such infrastructure issues can be very expensive because isolating a problem and developing a fix can take a lot of time and expertise."



So, okay.  When I opened the app just now, I found a request for a donation.  Clicking the "Donate" button took me to a screen which read:  "Privacy over profit.  Private messaging, funded by you.  No ads, no tracking, no compromise.  Donate now to support Signal."  And then there was an option to read more.  Having read everything I just had, I decided I would give them some money.  So I selected an amount and was taken to a screen that said "Donate to Signal.  Get the Signal Boost badge."  And the payment options were Apple Pay, Debit or Credit Card, or PayPal.



It'll be interesting to see whether a user-supported secure messaging system is an oxymoron.  Can such a system support itself?  I have no idea whether a sufficient number of users will step up and pay, and on the ongoing basis that Signal requires, to use a miraculously secure and clean service that's truly free in every sense of the word.  I'm not inclined to pay continuously because I don't use it on a regular basis.  I am a continuing supporter of Wikipedia because I do often use it.



My fear, based upon everything I've seen, is that people do not appreciate and probably cannot appreciate what Signal is, and that they use it because among all of the other free messaging services, they can see that it's the cleanest and totally free of debris.  But they have no idea why.  The problem is that being totally debris free is an expensive service for Signal to deliver.  I'm glad they took the time to explain their financial reality.  Those of us here get it.  But most Signal users will never see their posting.  And without that posting, that "Donate" button at the bottom of Signal's screen doesn't pack nearly the same punch.



And Leo, I think that your immediate reaction is probably the right one.  It's like, it's been nice knowing you.



LEO:  I don't know.  You know, we should point out you mentioned Brian Acton is the CEO.  He was the founder of WhatsApp, sold it to Facebook for 19 billion.  He's worth it's estimated 2.5 billion.  I suspect it's Brian and a few other well-situated donors that have been keeping it going.  It'll be interesting to see if, I mean, obviously no one wants to pay $50 million a year forever, even if you're a 2.5 billionaire.  It'll be interesting to see if they can get people to donate.  I do think, and we've talked about this on our other shows, we are coming to the end of the free Internet.  We all know it couldn't hold forever.  Right?  



STEVE:  Well, we were all wondering why it, like, held at all.



LEO:  How could it be free, yeah.



STEVE:  Like for how many years were we hearing that Twitter's losing money, but they're going to make it up in volume.



LEO:  Right.



STEVE:  It's like, what?



LEO:  Yeah.  And I think, you know, the crunch has come.  And I don't think, you know, look.  We are in the same position.  Ad sales have dropped way off.  Podcasting, which was the fair-haired boy, no longer is with advertisers.  They quite understandably want to go where they can get more information about who's watching.  So they buy YouTube, and they buy Google, and they buy Facebook, because they like that information about you.  And so because we have very limited ability to track you, podcasts in general are much less attractive.



That's why we make a big deal about Club TWiT and invite you to join and support us and stuff.  And we have to date 8,322 paid members, which is great.  Not enough to keep us on the air.  We're doing some sort of hybrid thing.  But it's a big crunch.  And so, you know, I just don't know if people have, you know, people talk about subscription fatigue.  I understand nobody wants to pay $7 here, $5 here.  But donations, I've got to tell you, we used to do donations.  We started, I thought the whole thing will be donation-based.  You remember that because you were there.



STEVE:  Yeah.



LEO:  And we'd get about 8,000 a month.  It wasn't enough to pay a salary, let alone a whole team.



STEVE:  Yeah, yeah.  And one thing I did note that I didn't put in the show notes is that Tor remains more than half supported by the U.S. government.



LEO:  Yeah, isn't that interesting.



STEVE:  Yeah. 



LEO:  Why would they support that?



STEVE:  It is curious, although the half of support is way less expensive, it's like $1.3 million or something.  So the Tor Project is not the same as Signal by any means in terms of scale.



LEO:  Well, I imagine most Tor servers are self-funded; right?  They volunteer.  So it's a really interesting - we are in interesting times.



STEVE:  I sort of wanted to - yeah.  We are.  I wanted to put that on everyone's map just, you know, to make sure they knew about Signal, where it stands, that it is, you know, unique in the industry.  And I wish them luck.  You know?  I just don't have an occasion to need Signal.  Rasmus and I haven't talked in a long time, and iMessage is fine.  And I buy an iPhone, and that apparently underwrites iMessage.



LEO:  Yeah.  I mean, honestly the people who most need it, the dissidents and so forth that do need it, probably have no means to pay for it.



STEVE:  Right.



LEO:  So really the donations are to support that, guess.



STEVE:  Yeah.  And it's why I think the more reasonable solution is the benefactor.  You know, they would like to have, as they said, a stable income source from many small donations.  But I just, you know, I don't know.  And as I said, Leo, I don't know why people chose it except that it's the best one.  You know, it's clean and no ads.  But does anyone really know about Signal, or care?  If they tried to make it a subscription, people would leave.  They'd just go to one of the other free things that is not actually as free as Signal truly is.



LEO:  Mm-hmm.



STEVE:  So, okay.  Here's one that just makes your head shake.  It was tied for the most listener news mentions with feedback about last week's discussion with The Barnacle, which we'll be getting to a bit later.



We've talked before about how ransomware has evolved.  In the good old days, ransomware attackers would just encrypt a business's various servers, you know, their workstations and hopefully their backups, if they could get it, and leave it at that.  But when that wasn't always delivering the results they wanted, meaning getting paid in lots of cryptocurrency in return for providing the decryption keys, some of the gangs would then launch a powerful DDoS attack against the company just to increase their pressure on it.



And when even that didn't work, the gangs began exfiltrating the enterprise's data before encrypting it so that they could also hold it for ransom, threatening to release it to the public or sell it on the dark web unless the enterprise met their extortion demands.  And in true escalation, when even that wasn't producing the results they sought, they further threatened to use the exfiltrated data to contact the victim's clients and customers directly, informing them that the company had lost control of their personal and presumably private data.  So this brings us current, where today, believe it or not, we have yet another step upward in this seemingly endless escalation.



The ransomware group known as ALPHV, who uses the BlackCat ransomware, claims to have breached the systems of MeridianLink, a company based in California that provides digital lending solutions for financial institutions and data verification solutions for consumers.  So seemingly quite sensitive data.  This cybercrime gang claims to have stolen a significant amount of both customer data and operational information belonging to MeridianLink, and they've been threatening to leak it unless a ransom is paid.



But apparently negotiations were not going as well as they'd hoped so they decided to escalate.  What could they do to make matters worse for MeridianLink?  In an effort to increase their chances of getting paid - and this doesn't quite make sense to me either - the malicious hackers claim, and have presented documentary evidence, to have filed - I love this - a complaint with the U.S. Securities and Exchange Commission, our SEC, against MeridianLink, accusing the company of failing to disclose the breach - which they themselves perpetrated.



LEO:  Wow.  That is chutzpah.  Wow.  Holy cow.



STEVE:  Within four business days, as required by the new rules announced by the SEC last July.  So the group published screenshots on its leak website last Wednesday, November 15th, showing that the complaint has been filed and received by the SEC.  I have a screen shot in the show notes.



From their multiple-choice selection, they chose "Material misstatement or omission in a company's public filings or financial statements, or a failure to file."  Under the category that best describes the complaint they chose, they selected "Failure to file reports."  And in the fill-in field for "In your own words, describe the conduct or situation you are complaining about," these criminals wrote:  "We want to bring to your attention..."



LEO:  Wow.



STEVE:  I know.  It's wonderful.  "We want to bring to your attention a concerning issue regarding MeridianLink's compliance with the recently adopted cybersecurity incident disclosure rules.  It has come to our attention that MeridianLink, in light of a significant breach compromising customer data and operational information, has failed to file the requisite disclosure under Item 1.05 of Form 8-K within the stipulated four business days, as mandated by the new SEC rules."



Okay, now, I want to pause here for a moment to take note that the grammar used in this posting is uncharacteristically excellent.



LEO:  Native English speaker, I'd say.



STEVE:  Uh-huh, well, since this is so rarely seen from non-native English speakers, and the ALPHV gang operates out of Russia, my first thought was that ChatGPT, or one of its ilk...



LEO:  Oh, yeah, sure, sure.



STEVE:  ...may have had a hand, or a register, in producing this prose.  You know; right?



LEO:  Of course, yeah.



STEVE:  So however, nice as that bit was, the group screwed up a few other substantive and material details that directly bear on the success of this.  First, there's some confusion over the date of the breach.  The hackers told the site "DataBreaches.net" that the attack against MeridianLink  which only involved data theft, not file encryption  was conducted two Tuesdays ago, on November 7th, and that it was discovered by MeridianLink the same day.



However, MeridianLink has told the tech press that the intrusion occurred three days later, on Friday, November 10th.  They said: "Upon discovery on the same day, we acted immediately to contain the threat and engaged a team of third-party experts to investigate the incident.  Based on our investigation to date, we have identified no evidence of unauthorized access to our production platforms, and the incident has caused minimal business interruption."  Okay, now, of course, no evidence is not proof that it didn't happen, just that they're able to legally say, assuming that it's true and they were using a third party, so like they can't obfuscate there, that there's no evidence.



So, okay.  It is data exfiltration, not encryption, so no business interruption would have been expected to have occurred.  If MeridianLink is able to assert formally, as they apparently have in reporting, that they quickly detected the breach, engaged a third-party forensics team, and have so far identified no evidence of unauthorized access to the system containing the sensitive customer data, then they would have had no cause to inform the SEC under the new four-day rule.



Even so, though such disclosure would be prudent for any publicly traded company, that new four-day rule doesn't actually go into effect until the middle of next month on December 15th.  The way that new and still-pending rule is written, companies will be required to notify the SEC within four business days of determining that a cybersecurity incident is material to investors, which - because, you know, it's the SEC - based on MeridianLink's statement, that does not appear to be the case.



So I think it's probably significant that the one thing that appears to be conspicuously absent, even today, is a shred of actual proof of this breach from the attackers.  Yeah, they got in.  Did they get anything?  They say yes.  MeridianLink says no.  The attackers have not managed to prove otherwise, and that's trivial to do if they actually have any data.



LEO:  Sure, sure.  [Crosstalk] to do, yeah.



STEVE:  You know?  Yes.  Uncharacteristically, following any true data breach, nothing has been posted to the dark web to confirm and prove that the attackers actually did obtain any sensitive data.  So it appears that MeridianLink is correct in their statement that nothing material was obtained by the hackers, and that the entire extortion incident with the SEC may have been an empty threat.



LEO:  Or the point; right?



STEVE:  Yes.  Well, it's weird, too, because on their site they showed the evidence that they had submitted this, and the automated reply.  And they said, "Now they have 24 hours to respond."



LEO:  It's blackmail, yeah.



STEVE:  What, you just shot your load, what are you talking about, 24 hours?  What do you have to threaten them with now?



LEO:  Wow.



STEVE:  Anyway, you know, despite all the fact that this may be empty, it did make headlines in the tech press because, although threats have been made in the past by many ransomware and extortion gangs, alleging or saying, threatening that they would follow up their theft of data with reports to the SEC because it's occurred to people in the past that companies would be vulnerable to the need to report this, this does appear to be the first time a ransomware group has actually followed through and filed an SEC complaint against one of its victims.



LEO:  That's hysterical.  That's real chutzpah.



STEVE:  It really is.  I just love the "It has come to our attention."  Yeah, right.



LEO:  Because we did it.



STEVE:  Because we did it to you.



LEO:  Very nice.



STEVE:  Leo, let's take a break.  We're at a good point.  And then we're going to talk about Europe and their faulty radio encryption, basically circling back to something that we talked about previously.



LEO:  Okay.  Yeah.  Actually, I see that the NYPD wants to spend $30 million to encrypt their radio traffic. 



STEVE:  Yup.



LEO:  Sometimes encryption could be used against you, not in your favor.  But that's a story for another day.



STEVE:  Yup.



LEO:  Now back to Mr. G.



STEVE:  So there's some welcome news in the world of open versus closed encryption standards.  Last summer we covered the story of serious flaws being found in yet another closed and proprietary radio encryption system.  The fastest way to bring everyone back up to speed is for me to share just the beginning of some coverage in WIRED about this discovery at the time.  They said:  "For more than 25 years, a technology used for critical data and voice radio communications around the world has been shrouded in secrecy to prevent anyone from closely scrutinizing its security properties for vulnerabilities.  But now it's finally getting a public airing thanks to a small group of researchers in the Netherlands who got their hands on its viscera and found serious flaws, including a deliberate backdoor.



"The backdoor, known for years by vendors that sold the technology, but not necessarily by customers, exists in an encryption algorithm baked into radios sold for commercial use in critical infrastructure.  It's used to transmit encrypted data and commands in pipelines, railways, the electric grid, mass transit, and freight trains.  It would allow someone to snoop on communications to learn how a system works, then potentially send commands to the radios that could trigger blackouts, halt gas pipeline flows, or reroute trains.



"Researchers found a second vulnerability in a different part of the same radio technology that is used in more specialized systems sold exclusively to police forces, prison personnel, military, intelligence agencies, and emergency services, such as the C2000 communication system used by Dutch police, fire brigades, ambulance services, and Ministry of Defense for mission-critical voice and data communications. The flaw would let someone decrypt encrypted voice and data communications and send fraudulent messages to spread misinformation or redirect personnel and forces during critical times.



"Three Dutch security analysts discovered the vulnerabilities, five in total, in a European radio standard called TETRA, short for Terrestrial Trunked Radio, which is used in radios made by Motorola, Damm, Hytera, and others.  The standard has been used in radios since the '90s, but the flaws remained unknown because encryption algorithms used in TETRA were kept secret until now.  The technology is not widely used in the U.S., where other radio standards are more commonly deployed.



"But Caleb Mathis, a consultant with Ampere Industrial Security, conducted open source research for WIRED and uncovered contracts, press releases, and other documentation showing TETRA-based radios are used in at least two dozen critical infrastructures in the U.S.  Because TETRA is embedded in radios supplied through resellers and system integrators like PowerTrunk, it's difficult to identify who exactly might be using them and for what.  But Mathis helped WIRED identify several electric utilities, a state border control agency, an oil refinery, chemical plants, a major mass transit system on the East Coast, three international airports that use them for communications among security and ground crew personnel, and a U.S. Army training base."



Okay.  So back when this all broke we dug into this more deeply.  But that explains the breadth and importance of this system.  It's far more widely used throughout Europe than in the U.S., but that doesn't make its security any less important.  The fact that the system is so old, and that it's been in place since the 1990s, likely explains part of the problem.  Once again we have inertia.  We think it's encrypted.  The colorful glossy brochure says it uses super-fancy "Air Interface Encryption," whatever that is.  And we already have a huge investment in these radios in the field, so we really don't want to be told that it's all crap, thank you very much.



So here's the good news.  Although the gears are turning as slowly as ever, after being bombarded with well-deserved criticism for keeping its crappy encryption algorithms secret for the past 25 years, after they were exposed as such, and not until, the European standards body ETSI, behind the TETRA algorithms, has finally decided to open them to the public for scrutiny.  Although one could argue that those three Dutch security analysts have already done that.



At the time, Matthew Green, the Johns Hopkins University cryptographer and professor whom we often quote, called the technology in use old-fashioned and behind the times for continuing a practice of secrecy that had long been abandoned by the security world.  When asked about the recent decision to open the encryption now to the world, he said:  "This whole idea of secret encryption algorithms is very 1960s and 1970s and quaint.  It's nice to see them joining us here in the 21st Century."  And now having made the decision to go public, they are owning it.



ETSI's statement said:  "Keeping cryptographic algorithms secret was common practice in the early 1990s when the original TETRA algorithms were designed.  Public domain algorithms are now widely used to protect government and critical infrastructure networks, for example AES, the Advanced Encryption Standard, standardized by the U.S. government.  Effective scrutiny of public-domain algorithms allows for any flaws to be uncovered and mitigated before widespread deployment occurs."  So, yeah, right.  As Matthew said:  "Welcome to the 21st Century."



But better late than never.  And what will be interesting is to see whether they did better.  Remember we talked about it, there was like the TEA, they called it T-E-A, TEA1, TEA2, TEA3, TEA4.  Some of them were deliberately crippled, and those were like the ones they sold to governments that they trusted less and presumably didn't want them to have strong encryption and maybe be able to break into it if they needed to.  And then now they've kept going to 6, 7, and 8.  So we're going to be able to see if the new stuff is any good.  So we don't know.



Okay.  Meanwhile, late next year, those green bubbles which appear in iMessage whenever the message's recipient is "out of system," which typically means an Android user, will be getting many more features.  In a move that few saw coming, last Thursday Apple announced that it will be adopting the RCS - which stands for the Rich Communication Services - messaging standard.  This will finally bring, for the first time, a wide range of iMessage-style features to messaging between iPhone and Android users.  This has been actually made possible as RCS has continued to develop and become a more mature platform than it was when it first, you know, came out of the gate.  So it's to the point now where Apple has said, yeah, okay, we understand we should be doing more for cross-platform messaging.  RCS is what we'll use.



So RCS on iPhone will bring many iMessage-style features to this new cross-platform messaging, you know, read receipts, typing indicators, high-quality images and videos, and much more.  Their implementation will also give users the ability to share their location with other people inside text threads.  And unlike regular SMS, RCS can work over mobile data or WiFi, as well.



At the same time, iMessage isn't going anywhere.  That's Apple's.  It will continue to be the messaging platform used for all "in-system" communication between iPhone users.  So RCS will simply replace SMS and MMS and exist separately from iMessage when it's available.  And SMS and MMS will also continue to be the fallback when they're needed.  So anyway, as an iPhone user myself, it will be nice to retain some of those goodies when messaging with other people who have the green bubbles in messaging.



On my end, I'm still working on the dynamic server-side code-signing technology I'll be using for all future SpinRite and other commercial software downloads.  My favorite discovery last week was finding that Yubico, you know, Yubico now makes their own HSM, I mean, a commercial product in HSM, which is, as we know, needed for secretly storing keys and securely storing keys and then using them internally to perform crypto operations inside the HSM so that those keys are never exposed to any possible hacking.



And since their famous key is called the YubiKey, it won't surprise anyone to learn that their HSM is the YubiHSM.  There's a YubiHSM 2 and a YubiHSM 2 FIPS which provides additional certification assurances.  Since EV code-signing certificates can only be signed by a FIPS-certified HSM, Yubico has that covered for me.  Aside from the fact that it's by Yubico, which would be reason enough for me, it is vastly more capable than that Gemalto/Thales SafeNet 5110 dongle that I've been using for manual code signing so far.  It's able to hold many more keys, it can work with much longer keys, and it supports many more recent crypto algorithms.



The Gemalto/Thales device that I'd been using, the SafeNet, it has virtually zero developer support.  I wrote to them last week requesting access to their developer SDK, and I received the reply, basically:  "You didn't purchase it from us, so go away."  By comparison, Yubico's developer support is astonishing, and it's all published out in the open for anyone to have and to hold.  The product has been around for a while, so it's only that I hadn't been paying attention that I wasn't aware of the YubiHSM 1, which has now been replaced by the YubiHSM 2.  And needless to say, for anyone who may have been thinking, with all the attention we've recently been giving to the need for an HSM, that switching to one might make sense, now you know that a favorite company of ours, Yubi, also offers a terrific-looking solution.  So I will be spending more time with it after today's podcast.



Okay.  Let's close some loops.  PhobicCarrot tweeted to me:  "@SGgrc:  Is this a joke?"  He's referring to the Barnacle, which we had a lot of fun with last week.  He said:  "Someone could easily slide something between the suction cup and the windshield to remove it.  Haven't you ever removed a suction cup before?  I'm guessing a playing card or credit card would do the trick."  Now, one would also think that the company behind this had arranged some means for making that difficult to do.



Fredrik tweeted:  "@SGgrc:  How do you beat Windshield Barnacle?  As it turns out, to take off the Barnacle, all you need to do is run your vehicle's windshield defroster for 15 minutes, and then use a credit card or similar thin piece of plastic to release the suction cup around the edge."



Okay, that was interesting.  My guess is - and there are articles on the 'Net that confirm this.  My guess is that the defroster heats the windshield, which causes the air trapped underneath the Barnacle's two massive suction cups to expand, right, as it gets hotter.  That in turn lifts the whole system away from the windshield, which then allows a shim of any sort to be slid underneath to break the suction cup's seal.  So apparently students have been having a lot of fun in the universities where this has been deployed, to the point that some universities have decided to back away from using it because it's causing more trouble than it's worth.  



Art Nilson tweeted:  "Hi, Steve.  I have a need to determine if a USB drive is fast or slow at writing.  ReadSpeed and ValiDrive are very close to giving me this, but not quite there.  Any suggestions?"  



Okay, well, ValiDrive is not a useful test for real world speed because it continuously alternates between tiny reads and tiny writes as part of its job.  Not only is that not the way such drives are typically used, but my theory, which will be confirmed or repudiated before long, is that many thumb drives may have an especially difficult time getting ready to write after reading.  Since ValiDrive is not the way drives are typically used, that's not something that many have been optimized to do very well, that is, to switch from reading to writing rapidly.  They just don't need to, typically.



ReadSpeed fails to meet your needs for two reasons.  First, it won't run on USB.  It was designed during the early SpinRite work as a test of SpinRite's new native drivers.  So only on AHCI or ATA-connected drives.  Secondly, you want a writing speed benchmark, not just reading.  And ReadSpeed, as its name suggests, only benchmarks read speeds.



Now, SpinRite 6.1 has a very nice new drive benchmarking system which will benchmark any drive, including USB drives, in three places:  front, middle, and end.  But like ReadSpeed, SpinRite is only benchmarking drive read performance.  However, If you are a SpinRite owner, you could grab the latest pre-release code for SpinRite 6.1.  Level 1 is a read-only test, and Level 3 writes back what Level 1 reads.  So Level 3 is both reading and writing, but in large typical blocks that a drive would normally be used with.  Until we get to SpinRite 7, SpinRite is still forced to access drives through the machine's BIOS.  And that limits us to 127 512-byte sectors, but that's more than 65,000 bytes at a time.  So far better than ValiDrive.



SpinRite 6.1 also allows you to set precise starting and stopping points, down to the single physical sector number.  So when it stops it shows and logs the exact elapsed time required to do whatever you told it to.  So you could first run SpinRite at Level 1 over a specific region and specifically sized region of the drive, and note the time required for that pure reading operation.  Then do exactly the same thing, but at Level 3.  Since Level 3 only adds writing of what was read, the difference in the timing of the two levels will entirely be due to the drive's true writing speed for that amount of writing.  And since SpinRite 6.1 also now logs not only the percentage of the drive, but also the exact physical sectors, you get the exact number of bytes by  multiplying the sector span by 512 bytes per sector, and then calculate the drive's write bandwidth.



Okay.  Now, having just written all that, I realized that with a tiny change, ValiDrive could become a very nice freeware USB drive read and write performance benchmark.  I never really knew why I was posting all of that read and write performance statistics with min, max, median, standard deviation, and variance, but now I know.  All that would need to be changed for ValiDrive to become a very useful and precise benchmark for USB drives, would be to have it transfer much larger blocks of data at once.  Then its measured performance, which ValiDrive already does beautifully, would echo a drive's speed in the real world.  I have been planning to revisit ValiDrive once SpinRite 6.1 is packaged and finalized.  So now I have another reason to create a ValiDrive 2.0.  And that'll make it quite useful for more than just checking drives for their validity.



Actually, I sent that reply back to Art via Twitter before I had hit on the idea of using SpinRite in this read and read/write mode.  His reply tweet was:  "I have been running it regularly during development, but my systems are all just boring.  SpinRite always just worked."  So he wasn't providing a lot of feedback in the newsgroup.  You know, and the fact that we have 805 testers the last time I looked, and not that many people that are actively reporting problems, suggests that he's like most of the others.



T A Hofer tweeted something interesting.  He said:  "Dear Steve.  Thank you for your relentless coverage of the sometimes too exciting cybersecurity world, and for providing an insightful and entertaining way to earn CPEs.  Listening to your coverage of Article 45 and QWACs" - you know, QWACs, which is the idea that certificates are going to be produced by EU countries - "or qualified website authentication certificates, it piqued my interest.  And after some research, I thought I might be able to add perspectives to understand the motivations that I've observed having worked in financial services," he said, "the previous globally connected network before the Internet, in strategic, operational, and audit roles."  He says:  "Well, I am not for or against QWACs.  I'm not even in the EU anymore.  Nevertheless, these are a few thoughts about this from a compliance perspective.



"The EU would definitely propose that technology is too big for any single country, and that's why they legislate for the territory of the EU.  At the moment, practically all major providers are U.S.-based, operating under U.S. law.  For example, the AWS CDN CloudFront operates in the Virginia region of AWS, where data passes through the territory of the U.S.  Although you, residing within the U.S., enjoy the protections of the Constitution and other laws enforcing, for instance, your right to free speech and restricting unreasonable search and seizure, these protections do not apply to me and my data passing over there, as I reside in the UK.



"The kerfuffle about GDPR, privacy, and safe harbor agreements with the U.S. were somewhat about companies, but more painfully about protections against government intrusion.  Although it's true that companies terminate TLS sessions on the edge and do filtering for internal networks, so do CDNs covering a vast part of Internet traffic.  And if routed through U.S. territory, the providers have obligations under U.S. law.  That's why the EU would want to assert legal primacy over their own citizens and territories.



"The financial services industry has already gone through this process about a decade ago, and U.S. international banks established local subsidiaries operating under the EU directives.  Value-added tax or VAT is a similar example.  It used to be possible to buy digital content from U.S. companies without VAT.  Now it's not.  They're all registered for VAT.  Indeed, if you are a European client of Google, you would be contracting with Google Ireland, showing that large U.S. tech providers have already adapted to increasing local laws and regulations on technology.



"Based on the materials available about QWACs, they appear to serve the exact opposite of eavesdropping.  They appear to be THE banner.  The idea is that if a QWAC has signed the site" - that is, you know, one of these certificates - "has signed the site certificate, then this would show in the address bar as OV and EV once did.  Documentation available on the European Parliament website show that QWACs are considered super-EVs and would be used to provide assurance that users, and particularly payments providers, are connecting to legitimate counterparties.  This way, QWACs would actually defeat eavesdropping and transparent TLS-terminating proxies because Cloudflare or AWS might be able to issue a valid certificate for the Swedish government website on the fly, but cannot issue one signed with a QWAC.



"By the way, that would also be the simplest way to expose eavesdropping, if any.  Just show a banner that the site used a QWAC-signed certificate.  This also answers the challenge of technical feasibility.  There is no need to insert a banner into web traffic, as the CA certificate being a QWAC is the banner.  For the countries in question, the governments are the elected representatives providing the checks and balances over the technology companies, not the other way around.  From Germany's or France's perspective, the Mozilla Foundation might not look like an organization that has to police them.  Rather, they might demand that Mozilla put the countries' CAs in the trusted store if Mozilla wants to do business there.  That's really the crux of the issue.  The QWACs are already long there, just not added to the trust stores.



"Mozilla might choose not to add them, but it is less unlikely that Apple, Google, Microsoft, and others would leave a market as big as the EU, just as large U.S. banks chose to operate under local laws there, too.  And yes, that's a more fragmented world, for better or worse.  Apologies, long message, hopefully somewhat useful.  Wishing all the best and looking forward for 999," he said, "perhaps 0xAAA and beyond.  Kind regards, Thomas."



Okay.  So I think Thomas's counterpoint brings up a lot of useful issues.  It's certainly useful to see both sides of this.  There are a couple of corrections, though, that I need to make.  For one thing, CDNs like Cloudflare actually do terminate the HTTPS TLS connections coming into their proxies with their customer's actual certificate.  I looked into this at one point and learned that they had several ways to do this.  A customer could provide their certificate or make it available on the fly for Cloudflare's use without losing control over it.  The system is very slick.



But it also means that even fully proxied and decrypted communications would still bear that QWACs seal.  So that would mean it was a false positive.  If the presence of a QWACs certificate was somehow surfaced on the browser's UI chrome, where in this instance "chrome" is the generic term for all of the browser's non-webpage content window dressing, it would be providing a false assurance that eavesdropping had not occurred and was not possible.



I have not spent too much time digging around into this myself because that time would be wasted if QWACs never materializes, as I hope it won't.  And so many others feel similarly.  But if it appears as though it's going to, then I'll definitely be coming up to speed on the details.



The question is, is QWACs a countersigning of a certificate that was also signed by an existing Certificate Authority for browser trust, or can it be used standalone?  It would seem that browsers could be designed to require that any QWACs-signed web certificate also be signed by a traditional CA.  But then, even so, Thomas describes the effect of this as a sort of super-EV cert, which is likely accurate and exactly what the EU wants.



The trouble with that is that the web industry already had "EV" which was shown proudly on the browser's chrome.  And it collectively decided over time that it was a bad idea to show anything that could be interpreted as "extra trust and encryption goodness."  You know, browsers' users would think that meant something more than it did.  It could be abused, for example, by any site that got an EV cert, then used that cert to do bad things to its visitors under the misunderstood banner of "extra trust and encryption goodness."



So if this eIDAS 2.0 QWACs business looks like something that will actually come to pass, you can count on receiving a 100% fully detailed description of exactly what it winds up being right here because I'm going to want to find out about it and share it.



I received another note, which is a fitting follow-up to this, from a listener who asked me not to share his name.  He wrote:  "Hi, Steve.  Thank you for your podcast.  It's been a great source of information, laughs, and concerns.  Your explanation of how root certificates work and how Article 45 could be misused has been a factor of stress."  I guess for him.  "I listened to the episodes 983 and 984, and then went on to read the show notes, just to be sure that it was as bad as I had understood.  I was curious about the number of CAs trusted by my Firefox browser, and there it was:  I counted 165 root certificates."



LEO:  Wow.



STEVE:  Yikes, yeah.  "After some manual cleaning, I was able to reduce this number to 84 entries.  Some of these are from European countries, many from the United States and China, and then we have Slovakia, South Africa, Tunisia, Ecuador, and many more.  From what I understand, any of these 165 root certificate counterparts could be used to intercept my communications.  I have to trust that none of these 84 CAs" - which apparently survived his pruning - "will lose their signing private keys or be compromised by one of the powerful states that have the capability to intercept communications worldwide.  So there's no way to ensure real privacy, even if I remove all but the root certificates that I know are needed for my communication, and the CA of the cert used in the TLS will be able to intercept it?"  He ended that sentence with a question mark.  So I'll say that, yes, that's 100% correct.



Then he finishes:  "Is it just me, or is this system still largely based on faith?  It seems to rely on the expectation that all participants will all behave ethically all the time, and be highly competent in protecting their precious keys."



Okay, so, yes, just to be clear, our current system of global trust is fragile.  This is largely why so many well-placed parties have spoken out against the idea of having the EU mucking around in it in any way.  All that can do is destabilize an already somewhat precarious system of trust.  The reason the system we have today works at all is that the incentives strongly favor rigorously correct behavior from every single one of those trusted CAs.  We've talked about this in the past, but it's worth refreshing briefly.  Being able to charge money for encrypting the hash of a certificate after verifying its statements is a profitable privilege, not a right.  And it's a privilege that those running the world's few root stores can and will and have rescinded after sufficient evidence of misconduct has been found.  No CA wants to lose their privilege of charging money for encrypting a hash.



So as I said, the incentives which are built into this system strongly favor rigorous compliance.  We've talked about mistakes and deliberate misconduct here in the past.  But overall, the whole trust-based system has been functioning amazingly well.  And everyone wants it to stay that way, which is why allowing EU countries to unilaterally mandate that their individual countries' own certificates must be present in OS and web browser root stores, and that there can be absolutely no oversight or recourse or consideration by any browser, you know, that's just politicians with a bad idea running amok.  The entire idea absolutely runs contrary to the historical communal spirit which has carefully evolved through the years and which, perhaps against all odds, has been working.  And we just can't allow this to happen.  Bad idea.



Someone who tweeted with the handle "M" said:  "Hey there, Steve.  Long-time listener.  I just wanted to pass along an observation about your musings on EU Article 45.  It is in no way similar to the UK messaging demand.  If Apple chooses to remove iMessage in the UK, there is absolutely zero impact to their bottom line.  People in the UK still buy iPhones and Macs.  But if the entire EU, an enormous market for any tech vendor, decides browsers and operating systems will conform, Apple, Microsoft, and every other publicly traded stock company will absolutely comply.



"Remember, these companies are owned by stockholders.  If Apple executives decided to drop such a major market on principle, their board would fire them before you could say 'fiduciary duty.'  If their board refused to do that, the board would be replaced by the shareholders, many of them financial activists,  sooner than you could say 'shareholder lawsuit.'  Opportunities to exchange potential income for principled stands disappeared when they no longer owned a majority of their own stock.  Thanks for the show."



Okay.  So I don't know how to reply to that, but I hope it doesn't turn out to be true.  At the same time, M's logic seems sound.  We did once have our browsers' crypto deliberately compromised by having strong crypto, any symmetric crypto with a key longer than 40 bits, classified in the U.S. as a munition and therefore against the law to export.  Even crypto ideas, conversations, and the publication of papers about strong crypto was banned.  It was a sad state of affairs at the time.  The cryptographer Dan Bernstein sued the U.S. State Department using First Amendment free speech argument.  Anyway, as I've been saying often more recently, we're all living through some very interesting times.



Bob Van Valzah, he said:  "I love having a personalized copy of SpinRite 6.0.  But if you add code signing to personalization for 6.1, how will it ever develop a trusted reputation with AV tools?  No two binaries will ever be the same."



And yes, Bob, that is absolutely a problem that's been haunting me.  But there were two points.  I've confirmed that code signing with an EV certificate, as opposed to a non-EV cert, does convey special meaning to Microsoft's SmartScreen filter.  It cares if the signer qualified to receive an EV certificate.  And given the hoops that DigiCert jumps its certificate users through in order to qualify, I'm not surprised.



Also, there's apparently some level of heuristics at play with the most modern AV.  All of the test releases of SpinRite's Windows boot prep component have been signed, and they're all different.  But for whatever reason, they have not been setting off alarms for SpinRite's testers in Windows, and even when I send them to VirusTotal.  I've looked carefully at what VirusTotal has to say about them, and there does appear to be significant behavior and proximity matching going on for the specific purpose of reducing false positives for "near relatives" that have long been known to be benign.  So I think we're going to be okay.



LEO:  Well, I think it's a misunderstanding of how AVs work.  They don't do a - they're not comparing binary to binary, hash to hash.  They're looking at signatures.  They're looking at strings.  So they're not saying, oh, this binary is different from that binary, and this one's okay.  They're looking for - aren't they looking for fingerprints of viruses via strings?



STEVE:  There's definitely that going on.  But there's also reputation.  Reputation counts a lot.



LEO:  Sure.  There's other Signals, and that's why an EV cert's good.  And, I mean, obviously heuristics help because you're looking at the behavior of the app.  But I don't think just because every binary is slightly different, that's going to impact it one way or the other.  Right?  They're not looking at the whole binary.



STEVE:  And it doesn't seem to be the case.



LEO:  Yeah.



STEVE:  So his point...



LEO:  That would be too costly and time-consuming.



STEVE:  Well, and his point was that, for example, the DNS Benchmark that's been downloaded more than eight million times, its signature has acquired a reputation, that is, its hash, the hash of that is what VirusTotal looks at.  So if someone drops a copy of DNS Benchmark on VirusTotal...



LEO:  Oh, I see, right.



STEVE:  It doesn't scan it.  It just it already knows.



LEO:  They don't bother.  I get it, yeah.



STEVE:  That that is perfect.  And so his concern is that every copy of SpinRite 6.1 that is downloaded will have a unique hash because it is personalized for each user.  And he's right about that, but the EV cert matters, and there does seem to be some heuristics that are softening, exactly as you say, Leo.  They've kind of gotten comfortable with that class of software.



LEO:  Right.



STEVE:  And so it doesn't send up red flags.



LEO:  Right.



STEVE:  Nathan, whose last name I'm not even going to try to pronounce, R-Z-E-P-E-C-K-I, it sounds like an eye test.  Anyway, he said:  "You mentioned you use a pfSense router.  I'm interested in switching my router from a Ubiquiti Edge router to pfSense, mainly because I want to use multiple VLANs and WireGuard/Tailscale networks and so forth.  Have you shared any details of what you use hardware-wise for pfSense, or could this be a topic for an episode?  I know it's not directly security related for the show."  Yeah, it kind of is.  "But I would be interested in knowing some of what you're willing to share for your personal home/work/office setup.  Thanks for reading this far, #securitynow."



Okay.  So I have been completely happy with the Netgate SG-1100 that I have here.  It's a perfect starter router with three fully independent NIC interfaces, one for the WAN, and two for LANs that can be configured and filtered completely separately.  As we know, that little router's "wall wart" power supply began causing it to reboot a few weeks ago.  But after switching to another beefier 12V supply, it's returned to absolute stability.



But when I was setting things up in the new home I was establishing with my wife, who at the time was my girlfriend, I chose differently.  I went with Protectli, P-R-O-T-E-C-T-L-I, Protectli.  And it's Protectli.com.  Their website's home banner says:  "#1 Appliance for Open-Source Software."  I was recently actually totally by coincidence discussing them with Alex Neihaus, recall from Astaro, one of this podcast's original commercial supporters.  He had independently settled upon Protectli, as well, and he is also 100% happy with his choice, as am I.  So that's the direction I would go without question.



I purchased a 4-port device, but they have 2, 4, and 6 ports in a very wide array of configurations and speeds.  I have a link to their product comparisons page in the show notes, or you can just click on it on their website.  And they even offer 4G cellular LTE failover in the case of a WAN outage to keep the network alive, you know, presumably at somewhat slower speed.  The routers, you know, these things, basically they're just small PCs.  They're user-configurable for storage, RAM, and processor.  You can get it bare bones or with whatever software you want preinstalled by them, and it runs everything since it's essentially a generic fanless PC with either a Core Boot or AMI BIOS.  You can purchase directly from them to get exactly the configuration you want, and Amazon also sells them off the shelf if you're in a hurry.  Unless and until something better comes along, that's what I'll be doing and recommending in the future:  P-R-O-T-E-C-T-L-I.



Anon John wrote:  "Hi, Steve.  Something worth sharing:  We've rolled out uBlock Origin, specifically to combat malvertising.  Thus far it's only deployed in IT.  We aim to deploy enterprise-wide, 25,000 users."  He said:  "We recently saw Redline stealer malware, where uBlock was not deployed, delivered via malvertising.  It slid past EDR; but luckily, C2 comms were stopped by a cloud proxy.  Blocking advertising categories at the proxy would be a choice, but that results in user browsing issues, and so far uBlock has generated barely a peep of user feedback.  Perhaps this tale might encourage others to implement security controls for malvertising."



And anyway, that's, you know, it's a great point.  I just read, while I was putting things together, that ALPHV ransomware group is using malvertising to spread their malware.  So, you know, and it's clicking on a link in an ad to receive some enterprise-level tool that was actually, you know, had a backdoor bundled into it, and that's how people were hurting themselves.  So don't do that.



And John's note caused me to also realize that we've only been talking about minimizing the annoyance of ads and tracking aspects.  But again, I just wanted to reassert that malvertising is a big deal.  And of course when he talked about proxies and blocking this stuff, my mind jumped to those ADAMnetworks guys I talked about a few weeks ago whose DNS-based trust and the use of trust-no-one whitelisting is also designed to perform the same sort of blocking.  It would block the browser from ever loading the malvertising content in the first place.  And like you wouldn't get the ad, you wouldn't get where the ad was linking to, and you wouldn't get any link back to command-and-control if someone were to, like, bring it into your enterprise mistakenly and allow it to get loose.  So great thoughts and solutions.



LEO:  It's also nice to know that uBlock works.



STEVE:  Yes.



LEO:  Because we all recommend uBlock Origin and use it.  Now, I don't know if you've seen this.  This is I think recent news.  Google has announced starting in June adblockers like uBlock Origin will be disabled in Chrome 127.  They're rolling out Manifest v3.  So it's unknown whether Edge is going to adopt Manifest v3.  I think Opera said they wouldn't.  Vivaldi said they wouldn't.  And of course Firefox won't.  



STEVE:  And are they both Chromium based?



LEO:  Yeah.



STEVE:  So, okay.  That's interesting.  So that doesn't mean that the Chromium core determines what Manifest level a browser's running.



LEO:  One hopes, yeah.



STEVE:  Well, otherwise, my point is if it does, then the other browsers can't...



LEO:  [Crosstalk] Firefox.  Yes.



STEVE:  Yeah.



LEO:  You know, you can't use a Chromium browser.  So there you go.  That malware blocking you're doing with uBlock Origin, that may not work in Chrome starting in June.



STEVE:  Yikes.



LEO:  Yikes.



STEVE:  And I wonder about things like Privacy Badger.  It's probably [crosstalk].



LEO:  Probably not, yeah.  A lot of extensions are blocked.  And I understand, Google's point is that, well, extensions are inherently dangerous, and we don't control the security model of your extension, and all sorts of bad things can happen.  It's true.  You can have malver-extensions, just like malvertising.



STEVE:  Yup.



LEO:  But I don't know if this is the right way to do it.  I don't know.



STEVE:  Yikes.



LEO:  Yeah.



STEVE:  Okay.  So two last pieces.  Mike asks:  "So, can I change the declared size of my mostly fake 2TB stick that is actually 32GB, and can I then use that 32GB of storage?  Thank you."



Okay.  From Mike's message it appears that he used ValiDrive on a 2TB drive and discovered that it only contained 32GB of actual memory.  There's been some discussion of the prudence of reformatting a fake drive to its actual size.  For example, GRC's InitDisk utility could be updated to incorporate a true drive-sizing feature.  It would exactly locate the physical end of the drive, then intelligently give it a file system that matches the drive's true size, not its declared size.



But the question was raised about the wisdom of ever trusting the memory of a drive which you receive from people who are clearly crooks and who are clearly in the "crook business" of selling deliberately bogus drives.  For all we know, the chips they're using were swept up off the floor at the end of the shift because they failed other critical reliability tests.  They got them cheap because no one else wanted them.  They don't seem like a good place to place anything that anyone ever wants to retain.



Since a true and authentic 32GB drive is very inexpensive  I just checked, a SanDisk brand 32GB Ultra Flair USB 3.0 Flash Drive is $7.69 from Amazon.  So why would you not use that?  If one cares at all about the data they're storing on a drive, I think that would make much more sense than trying to take a crooked drive and straighten it out.  You know, good luck.  But I have to say I considered that before the discussion that we had over in GRC's news groups said, are you nuts?  And I thought, yeah, okay, that would not probably be a good idea.



Finally, Shoeless Scoop	wrote:  "I'm glad I used a Privacy card for LastPass because I canceled my account back when you aired the LastPass breach, and they have not stopped trying to charge my family membership."



LEO:  Oh, wow.



STEVE:  Yeah.



LEO:  But Privacy's great.  So it's a win.  One sponsor wins, another loses.  Former sponsor LastPass, yeah, that's a loser.  But at least Privacy worked, and that was a sponsor, too, yeah.



STEVE:  Yeah.  He said:  "Clearly the days of them being a good company are gone."  And I didn't put it in the show notes, but he included, because it was private by DM, a picture of his statement from Privacy card showing the repeated attempts...



LEO:  Declined, yeah.



STEVE:  ...from LastPass to bill a card on an account that he closed.  So I just wanted to say to our listeners, I know that not everyone scrutinizes their credit card statements closely, especially for smaller charges, you know, the ones that have more digits and stick out, you know, you want to make sure that you know what those are.  But it would be really creepy and really extra sad if LastPass was continuing to do this.



LEO:  Yeah, no kidding.  And they wouldn't be the only ones who do this, either.  I mean, this is very common.



STEVE:  So ChaCha was designed by the individual I referred to earlier.



LEO:  Brian Acton?



STEVE:  Dan Bernstein.



LEO:  Oh, Dan Bernstein, yeah, yeah.



STEVE:  Dan Bernstein.  And it is the follow-on from Salsa20, which is why ChaCha.



LEO:  Is the next one going to be Merengue20 or Waltz20?  Foxtrot.



STEVE:  I think maybe Dan is done.  But...



LEO:  I love that.  ChaCha20.  All right.



STEVE:  So the question:  What do you get when you subtract 1973 from 2023?  You get 50.  And since the design for the Ethernet was first famously sketched out on the back of a coffee-stained napkin in 1973, earlier this year the Ethernet turned 50.  Many podcast moons ago we thoroughly and deeply covered both the electrical and logical operation of the Ethernet.  And its official birthday was widely celebrated in May.  But just last Thursday, on the 16th, the IEEE Spectrum magazine posted a birthday retrospective that I thought everyone here would enjoy.  With a bit of editing and my editorializing added, here's what they had to say under their title "Ethernet Is Still Going Strong After 50 Years.  The technology," they wrote, "has become the standard LAN worldwide."  And when you think about that, I mean, it's really something.



LEO:  Really, it's everywhere.  It's what I use; what you use.  Everywhere.



STEVE:  Yes, yes.  I mean, yes.  That's just astonishing.



LEO:  Because it's remarkable that something designed 50 years ago is still, you know, you can now get 10Gb, I mean, I bet you can go more.  I mean, it's amazing.



STEVE:  Yup.  Yup.  So they wrote:  "The Xerox Palo Alto Research Center in California has spawned many pioneering computer technologies including the Alto - the first personal computer to use a graphical user interface - and the first laser printer."  And I'll stop there to mention, and I've said this before, I've talked about it in the past, but that printer, the first laser printer, was known as the XGP, you know, stood for Xerox Graphics Printer.  And by pure coincidence I was there at the time.  I clearly recall the silver serial number plaque on the back of the printer which had a whole bunch of zeroes and a single one.  It was XGP-1.



LEO:  Wow.  Wow.



STEVE:  And I had the privilege of designing and building the first interface for that printer to the DEC PDP-10 at the nearby Stanford University Artificial Intelligence Lab which had employed me in the summer of 1973 while I was between high school and college.  And there was an IMP, an Interface Message Processor, standing there and that I used to sort of look and think, I wonder what that is?  Well, of course we know that was the birth of the Internet.



LEO:  Wow.



STEVE:  So they wrote:  "The PARC facility is also known for the invention of Ethernet, a networking technology that allows high-speed data transmission over coaxial cables.  Ethernet has become the standard wired local area network around the world, and it is widely used in businesses and homes.  It was honored this year as an IEEE Milestone, a half century after it was born.  Ethernet's development began in 1973, when Charles P. Thacker, who was working on the design of the Alto computer,  envisioned a network that would allow Altos to communicate with each other, as well as with laser printers and with PARC's gateway to the ARPANET.  PARC researcher Robert M. Metcalfe, an IEEE Fellow, took on the challenge of creating the technology. Metcalfe soon was joined by computer scientist David Boggs."



And actually what's not widely known is that the reason he was joined by David is that Bob cut himself badly cutting coax, and so Boggs stood in and like did the coax cutting because Metcalfe wasn't able to.  And they were called, I think, the Boggsey Twins because they were spending so much time together.



LEO:  Here is a picture of those days.  And that is - was it this big?  It looks like it's...



STEVE:  That's it.  That is the printer.



LEO:  It's a refrigerator.  It's huge.



STEVE:  Yes.  That is, that's the XGP.  Holy crap.  I haven't seen one forever.



LEO:  Holy crap is right.



STEVE:  Yeah.  It was - it stood that tall.  And Leo, it had a single continuous roll of paper.



LEO:  Wow.  And a cutter then.



STEVE:  It was about a foot in diameter.  And so you'd like put this sheet of paper, you know, like in it, and then it would just spool it and then cut it.  And it had a bug, and the paper just came rolling out and never got cut.  That was not good.



LEO:  There it is.



STEVE:  Ah.



LEO:  There is the paper, not getting cut.



STEVE:  Very cool.



LEO:  Wow.  Look at that.  Holy cow.  Put your hand in there, you're taking your life in your hands.



STEVE:  Oh, not good.



LEO:  Wow.



STEVE:  Yeah, very cool.



LEO:  Yeah.



STEVE:  So anyway, "Metcalfe and Boggs," they wrote, "had two criteria.  The network had to be fast enough to support their laser printer."



LEO:  Oh, wait a minute.  Wait a minute.  Here is a picture of SAIL with the XGP in the background.



STEVE:  Yes.  Yes, yes, yes.



LEO:  That's where you were.  That's the lab.



STEVE:  Yes.  I was there.  Behind them was something known as the hand-eye table, which is what transfixed me because it was robotic cameras and robotic arms.  And we're talking in 1973.



LEO:  Wow.



STEVE:  And when you drove up the driveway to SAIL, there was a sign that said "Caution:  Robot Vehicle in Operation."



LEO:  Here's the XGP, lurking in the corner there.



STEVE:  Yup, that's exactly where it was located.



LEO:  And there's young Steve Gibson, high school student.



STEVE:  I can't quite remember that guy's name in the background but he was - yeah.



LEO:  That's so cool.



STEVE:  So that was a PDP-10 on that side and a PDP-6 is behind them.



LEO:  So this is what you wrote the interface on.  Wow.



STEVE:  Yup.  Yup.



LEO:  Okay, I'm impressed.



STEVE:  And a DECwriter.  Very cool.  Anyway.



LEO:  Yeah, yeah.  I'll send you the links.



STEVE:  I was there then, yeah.



LEO:  Yup.



STEVE:  Okay.  So they wrote:  "Metcalfe and Boggs had two criteria:  The network had to be fast enough to support their laser printer, and it had to connect hundreds of computers within the same building.  The Ethernet design was inspired by the Additive Links On-line Hawaii Area network."  Okay, now, this is one of those where you come up with a name afterwards.  Additive Links On-line Hawaii Area network.



LEO:  Oh, ALOHA.



STEVE:  ALOHAnet.  That's right.  It was a radio-based system at the University of Hawaii.  Now, again, we're talking in the early '70s.  Computers transmitted packets, prefaced by the addresses of the recipients, over a shared channel as soon as they had information to send.  If two messages collided, the computers that had sent them would wait a random interval and try again.



And as we know, this is exactly the way today's Ethernet operates.  The brilliance of Bob's original conception was its simplicity.  Essentially, an Ethernet network is a big party line to which everyone is attached.  Anyone who wishes to talk on the line first listens to be sure the line is clear, that no one is currently talking.  If so, the party who wishes to talk just does so.  But since two waiting parties might start talking at exactly the same time, anyone who is talking is also listening to the same communal party line to detect any collision of their message with someone else's.



The abbreviation that's been given to this little bit of brilliance is CSMA/CD which stands for carrier sense multiple access with collision detection.  As the IEEE article noted, when two or more talkers detect that their message had collided with someone else's, they would each pick a back-off interval at random and wait again before trying.



Now, this is somewhat similar when you think about it to the way the Internet itself operates, where - and this is the key - where the guarantees for delivery were almost counter-intuitively softened by deliberate design, like the Internet's deliberate lack of any guarantee that a packet sent would ever reach its destination.  But the somewhat surprising result of these approaches was that the creation of quite robust networking systems occurred because the assumption of failure was deliberately designed into the system as part of the solution.



LEO:  Isn't that fascinating, yeah.  We can make it more robust, as long as you don't guarantee success.



STEVE:  Well, by designing failure into it.  That is, you know, it was so cool.



LEO:  Yeah, brilliant.



STEVE:  Okay.  Now at the same time one of the obvious problems with this freewheeling system is congestion.  The collision and random retry solution is very clever.  And it works terrifically so long as the overhead from collisions and retries does not become too great, since none of that represents a successful use of the bandwidth.  As a consequence, the Ethernet does suffer from one fatal flaw known as "congestion collapse."  Ethernet does not degrade gracefully under too great a load, it collapses as everyone is trying and failing to get their message through without collision.  There is, however, one simple solution to this, simply provide more bandwidth, which is what we've been doing ever since.



Continuing with what the IEEE wrote, they said:  "Metcalfe outlined his proposal, then called the Alto Aloha Network, in a now-famous memo to his colleagues.  Using coaxial cables rather than radio waves would allow faster transmission of data and limit interference.  The cables also meant that users could join or exit the network without having to shut off the entire system.  In a 2004 oral history conducted by the IEEE History Center, Bob Metcalfe said:  'There was something called a cable television tap, which allows one to tap into a coax without cutting it.  Therefore, Boggs and I chose coax as our means of communication.  In the memo, I described the principles of operation:  very distributed, no central control, and a single piece of 'ether.'



"Metcalfe and Boggs designed the first system," they wrote, "of what is now known as the Ethernet in '73.  It sent data at 2.94 Mb/s and was 'fast enough to feed the laser printer and easy to send through the coax,' Metcalfe told the IEEE History Center.  A 9.5-millimeter thick and stiff coaxial cable was laid down the middle of a hall in the PARC building.  The 500-meter cable had 100 transceiver nodes attached to it with N connectors, known as 'vampire taps' because each of the taps had small boxes with a hard shell and two probes that when you closed the box over the cable, they 'bit' through the cable's outer insulation to contact its copper core.  Thus new nodes could be added while the existing connections were live literally on the fly.  You could be tapping into virgin cable.



"Each vampire tap had a D-type connector socket in it, consisting of a plug with nine pins that matched a socket with nine jacks.  The sockets allowed Alto computers, printers, and file servers to attach to the network."  Boy, those were exciting times.  "To enable the devices to communicate, Metcalfe and Boggs created the first high-speed network interface card (NIC), a circuit board that is connected to a computer's motherboard.  It included what is now known as an Ethernet port.



"The researchers changed the name from the original Alto Aloha Network to Ethernet to make it clearer that the system could support any computer.  PARC researcher Alan Kay recalled a comment Thacker had made early on, that 'coaxial cable is nothing but captive ether.'"



LEO:  And the joke of course is scientists for a long time thought that - they couldn't figure out how light transmitted through a vacuum, so they invented a magical...



STEVE:  Ether.



LEO:  ...ether that it was going through until Michelson and Morley proved...



STEVE:  A substance.



LEO:  Yeah.  So there's no - light does not need ether to travel through to transmit.  



STEVE:  Yeah.



LEO:  Yeah.  So it's kind of fun they're using this word "ether."



STEVE:  Right.



LEO:  This antiquated scientific word.



STEVE:  Yeah.  So Metcalfe, Boggs, Thacker, and Butler Lampson were granted a U.S. patent in '78 for their invention.  And I'm sure it was assigned to Xerox.  Or actually I'm not sure because Intel and DEC both got involved.  These guys continued to develop the technology.  And seven years later, in 1980, PARC released Ethernet that ran at 10 Mb/s.  That was the first commercial Ethernet that we used.  The update was done in collaboration with researchers at Intel and Digital Equipment Corporation to create a version of Ethernet for broad industrial use.



"Ethernet first became commercially available that year, in 1980, and quickly grew into the industry's Local Area Network (LAN) standard.  To provide computer companies with a framework for the technology, in June of '83 Ethernet was adopted as a standard by the IEEE 802 Local Area Network Standards Committee.  Currently," they finish, "the IEEE 802 family consists of 67 published standards, with 49 projects under development.  The committee works with standards agencies worldwide to publish certain IEEE 802 standards as international guidelines."



So of course today's Ethernet protocol is carried over twisted-pair, which is the cabling that we all use when we're using wired Ethernet; optical; and of course radio, which is the ubiquitous WiFi that has mostly replaced wires anywhere ad hoc networking is needed.  The interconnection technology that began 50 years ago with a single long run down the center of the hallway is the technology that is now literally everywhere.



LEO:  Truly amazing.  And if you'd ever used Token Ring or any of the pre-Ethernet solutions, you realize what an amazing invention Ethernet was.  I remember crawling around not even that long ago at a radio station that was - all the machines were networked, and but it was a serial network - and disconnecting my machine, bringing the entire network down.  It  had to all be connected in a ring.  Ethernet is a miracle, for anybody who worked with pre-Ethernet technologies.  Really incredible.  Fifty years.



STEVE:  Yeah, it is.  And, you know, the Internet is a similar miracle that we've talked about often.  Again, both of them were designed to assume failure.  



LEO:  Brilliant.  Yeah.



STEVE:  And that by building that in, they made an incredibly robust system.



LEO:  And incidentally, that's how WiFi works.  It's still a collision-based system.



STEVE:  Yup.



LEO:  And it just works.  It's just brilliant.  Do you think that Metcalfe came up with that out of nowhere?  Or was there some precedent for this?



STEVE:  It was the ALOHAnet that was using the same...



LEO:  ALOHA was the first.  Okay.



STEVE:  It was using that collision technology for radio.



LEO:  It's really cool.  It's so cool.



STEVE:  And then he grabbed it and further refined it.



LEO:  I think really important, too, for younger people, younger than us, which is almost everybody, that they know about this because the history really gives you some reason to appreciate what we've got.  It didn't just all spring out of the, you know, full flown out of the minds of the founders.



STEVE:  Yeah.



LEO:  It's really been an amazing trip.  Ethernet turned 50.  And 50, by the way,  ain't that old.  It ain't that old; right, Steve?



STEVE:  That's right.  I had products when I was 20 years old, when it was being born.



LEO:  Exactly.  Steve Gibson.  GRC.com.  I love doing this show, and I'm so glad that you bring us each week these nuggets, this information, stuff we can use today, stuff to think about from days gone by.  You'll find Steve at his website, which does not stand for governance and compliance and any of that stuff.  GRC.com, it stands for Gibson Research Corporation.  Bye.



STEVE:  Thanks, buddy.  Bye.



Copyright (c) 2023 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.






GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#950

DATE:		November 28, 2023

TITLE:		Leo Turns 67

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-950.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  Since last week's podcast was titled "Ethernet Turned 50," it only seemed right to title this one "Leo Turns 67."  I'll have more to say about that at the end.  Until then, Ant and I will examine the answers to various interesting questions, including:  How many of us still have Adobe Flash Player lurking in our machines?  What can you do if you lose your VeraCrypt password?  Firefox is now at release 120; what did it add?  What just happened to give Do Not Track new hope?  Why might you need to rename your "ownCloud" to "PwnCloud"?  How might using the CrushFTP enterprise suite crush your spirits?  Just how safe is biometric fingerprint authentication?  How's that going with Apache's MQ vulnerability, and have you locked your credit bureau access yet?



Should Passkeys be stored alongside regular passwords?  What's the best way to prevent techie youngsters from accessing the Internet, and is that even possible?  What could possibly go wrong with a camera that digitally authenticates and signs its photos?  Could we just remove the EU's unwanted country certificates if that happens?  What's the best domain registrar, and what was Apple's true motivation for announcing RCS messaging for their iProducts?



SHOW TEASE:  Hey, it's time for Security Now!.  I am Ant Pruitt.  I'm not Leo Laporte.  He is out and about, having some interesting birthday celebrations.  We'll leave it at that.  But I'm joined this week by Mr. Security, Steve Gibson himself.  And today we're going to talk about what's going on with CrushFTP, the enterprise FTP package.  There's also some interesting stuff going on with the fingerprint scanners and biometrics on your laptops and devices that are out there.  Are you really secure?  Hmm.  Some think that they are.  Maybe they're not.  Then also there's some big news happening again with all of these credit agencies out there.  TransUnion and Experian hacked yet again.  Good grief.  All of that and more is coming up here on Security Now!.  Y'all stay tuned.



ANT PRUITT:  This is Security Now!, Episode 950, recorded Tuesday, November 28th, 2023:  Leo Turns 67.



Hey, what's going on, everybody?  I am Ant Pruitt, and this is Security Now! here on TWiT.tv.  Yes, I said I am Ant Pruitt.  No, I am not Leo Laporte.  That man is somewhere way off in the distance with no telephones, no computers, no tech at all.  So he's probably pretty frustrated right now.  I'm kidding.  No, it's all for his own good, having himself a nice little retreat this week.  So I am sitting in for him and going to sit down with Mr. Steve Gibson as we get into a great episode of Security Now! and get into all of the ins and outs and questionable things happening in the world of cybersecurity.  How you doing, Mr. Gibson?



STEVE GIBSON:  Hey, Ant.  It's great to be with you this week.  There's one other thing that Leo may be without, and that would be a birthday party.  Since last week's podcast was titled "Ethernet Turned 50," and since no other major topic grabbed the headline for today, I thought it only seemed right to title this one "Leo Turns 67."



ANT:  I dig it.



STEVE:  Since that will be his age tomorrow.  Now, wherever it is, whatever cave he's in somewhere, maybe they'll have a little birthday party for him there.  But apparently, you know, if he really is off sequestered somewhere, it won't be happening with his normal family.  They'll have to defer, I presume, until he gets back. 



ANT:  Right.



STEVE:  But you and I are going to examine the various answers to some interesting questions, including how many of us still have Adobe Flash Player lurking in our machines?  What can you do if you lose your VeraCrypt password?  Firefox is now at release 120.  What did it just add?  What just happened to give Do Not Track new hope?  Why might you need to rename your ownCloud to PwnCloud?  How might using the CrushFTP enterprise suite crush your spirits?  Just how safe is biometric fingerprint authentication?  How is that going with Apache's MQ vulnerability?  And have you locked your credit bureau access yet?



Should Passkeys be stored alongside regular passwords, or kept somewhere else?  What's the best way to prevent techie youngsters from accessing the Internet, and is that even possible?  What could possibly go wrong with a camera that digitally authenticates and signs its photos?  Could we just remove the EU's unwanted country certificates if that happens?  What's the best domain registrar?  And what was Apple's true motivation for announcing RCS messaging for their iProducts?  We're going to cover a lot of ground today.



ANT:  Oh, my.  I mean, you have a lot of questions there, sir.



STEVE:  I am so full of questions, I don't know what's going on.



ANT:  You've got a lot of questions.  I hope you have a lot of answers because that's some pretty interesting topics there.  And some of it's probably going to provoke some people and anger some people because I'm looking at one of those issues, especially regarding youngsters.



STEVE:  That I want to talk to you about because you're a dad, yup.



ANT:  Yeah.  I have thoughts.  And, yeah.  We'll get into that.  So Mr. Gibson, I love how you always start the show out here each and every week with some funny little things that we get from our awesome listeners.  We have a bunch of them hanging out here in our Club TWiT Discord.  Thank you all for being here, you members.  But what's the first thing you've got to share with us today, sir?  



STEVE:  Our Pictures of the Week, the original concept was to do something security related, you know, something, you know, techie and security related.  What's happened, however, is that we've, I guess mostly because I was so fascinated by the idea of these gates, like a locked gate out in the middle of a field somewhere.  It's like, what?



ANT:  Why?



STEVE:  So, you know, then we sort of wandered far afield.  Anyway, this one was a great one, and I gave it the caption "Sometimes ya gotta love humanity."  So this is at Euston, E-U-S-T-O-N, Railway Station in London.  And we have a yellow Do Not Enter tape stretched across an escalator which is not functioning.  What's so beautiful is the sign that they put up in front of this.  It reads:  "This escalator is refusing to escalate."  Okay, that's wonderful.  And it says:  "This has been escalated to the engineer who is on their way up (or down) to check it out.  Please use the lift."  So anyway, just props to people.  "This escalator is refusing to escalate."  Indeed.



ANT:  It's better than just saying "Out of Order."  Got to give them that.



STEVE:  Yeah, exactly.  You know, it's perfect.  Okay.  So speaking of out of order, or maybe in order, I'm just - this came about because I was still stuck last week on the task of performing unattended server-side Microsoft Authenticode code signing.  I am managing to inch forward with that challenge, and I've already made one very useful breakthrough, which was to figure out how to programmatically unlock a PIN-protected hardware token whose key is stored in one of the new style, they call it a KSP, a key storage provider, you know, HSM hardware dongle.  And I do look forward, since nobody's been able to do this, I look forward to sharing that with the open source community as soon as I come up for air.



Okay.  So while working on this, last week I discovered an amazing piece of free technology that I would have gladly paid hundreds of dollars for.  It's simply called "API Monitor."  I have a link to it here at the top, here in the show notes.  It was once a commercial product; but it went free about 10 years ago, likely because, incredible as this thing is, it's only going to appeal to a relatively small audience.  Ant, I love you, but I doubt that you need an API monitor to track down the intermodule API linkage calls in Windows apps.



ANT:  You might be correct, sir.



STEVE:  Okay.  However, when you need one, oh, boy.  So it's going to have a relatively small audience.  They probably didn't sell many copies.  But if this thing is what you need, there's nothing else like it.  I would send the guy a donation, I've tried to write to him, I don't know what happened to him, you know, he's like nominally around, but he hasn't replied.  There's not even a donation button on his site.  Due to the incredible lack of documentation on Microsoft's next-generation cryptography APIs - they literally call it CNG for Cryptography Next-Generation - I've been reduced to doing a bit of reverse engineering.  This API Monitor facilitates the creation and exploration of a detailed interactive log of all Windows module API calls.



Now, being an OCD perfectionist myself, I'll admit it, I'm not really impressed - I'm sorry, I meant to say I'm not readily impressed with other things that I see.  But this thing is truly incredible.  I won't spend any more of everyone's time raving about it.  You know, this is the sort of thing that, if you might find it useful, you already know enough to go grab it.  It is one of the most utterly stunning pieces of work I've encountered in years.  Okay.  Here's why I'm talking about it.



ANT:  Okay.



STEVE:  Other than just to give its gifted author more well-deserved public praise.  One of its process capture modes allows it to be triggered upon any new Windows process that starts up, like in the background.  Or in the foreground for that matter.  But in this case it was the background.  While I was coming up to speed on it on how this thing works -  because it's covered with buttons and options and stuff.  It also has tutorials.  I had not disabled, because it's enabled by default, the "trace starting processes" option, which I did not need.  So I kept seeing pop-ups which I would dismiss.  And it's not unusual for Windows, which is very busy in the background, to be just sort of autonomously doing stuff for you.



But one in particular caught my eye.  It kept popping up, Adobe's Flash Player Updater.  So while this thing was running, this API monitor, it would pop up a dialog saying, "Adobe's Flash Player Updater," you know, "has launched.  Would you like to trace it?"  And it's like, well, first of all, no.  But, like, why is Adobe's Flash Player Updater trying to launch?  So the first few times, because I was busy doing what I was doing, I just dismissed it.  I said no, I don't want that.  But finally I thought, okay, wait a minute.  What is going on?



Okay.  So in my machine Adobe Flash Player Updater was still alive.  The problem with leaving something like this attempting to run in the background is I don't know what URL this thing was constantly querying.  Hopefully it was a subdomain of Adobe, and not some separate domain from Flash Player's legacy Macromedia days that might not be renewed.  But if whatever domain the updater was querying were ever to become available for any reason, a bazillion PCs around the world, apparently like mine, would be querying it for an update.  Now, hopefully, Adobe also did the right thing and had any updates digitally signed with a "pinned" certificate, so that the updater would only accept updated code that had been signed with an absolutely specific Adobe certificate.



ANT:  Okay.



STEVE:  That would, on its face, prevent a malicious actor from injecting their code into these bazillion systems around the world that are all still apparently attempting to update their long-since-retired copy of Adobe Flash.



Okay.  So here's what Adobe had to say about their retirement of Flash Player.  They said:  "Since Adobe no longer supports Flash Player after December 31st, 2020, and blocked Flash content from running in Flash Player beginning January 12, 2021, Adobe strongly recommends all users immediately uninstall Flash Player to help protect their systems."



Okay.  Well, as we all know, Flash Player was nothing short of a catastrophic security disaster from the moment it appeared.  And of course, I mean, like, how many episodes in this podcast's past were we saying, well, once again, exploits of a Flash Player, blah blah blah.  So, now, Adobe was never seen to be using language like "Adobe strongly recommends all users immediately uninstall Flash Player to help protect their systems."



ANT:  To help protect their systems.



STEVE:  While it was a going concern; right?  It was only after they'd decided to finally give up that they said, okay, now get rid of it because it's a disaster.  On their Flash Player End-Of-Life FAQ page they ask themselves:  "Why should I uninstall Flash Player from my system?"  Then they provide the answer:  "Flash Player," they said, "may remain on your system unless you uninstall it.  Uninstalling Flash Player will help secure your system since Adobe will not issue Flash Player updates or security patches after the End of Life date.  Adobe blocked Flash content from running in Flash Player beginning January 12th, 2021, and the major browser vendors have disabled and will continue to disable Flash Player from running after the EOL date."



Okay.  So first of all, I have no idea why my computer still had it installed.  As we know, at one time, millions of websites and many standalone enterprise applications were dependent upon Flash Player for their operation.  I had it installed for research purposes, and I had been blocking its operation through browsers since early in this podcast because it was clearly a disaster.  But had I received any proactive Adobe reminder or suggestion that Flash Player had gone EOL - I mean, we talked about it at the time on the podcast - I would have clicked their "remove Flash Player" option.  I doubt that ever happened.  I certainly would have clicked it if it had.  This suggests to me that Adobe may not have been as proactive in promoting Flash Player's removal as they might have been.  And even now, when for the past several years their Flash Player Updater code has been running every hour in my system...



ANT:  Oh, my.



STEVE:  ...probing to see if there's been an update, why could they not have provided one final update, at any time, which would have caused Flash Player's Updater to remove itself, either immediately or the next time my system restarted?  This could have been done any time in the last three years.



ANT:  Sounds like the call back to home, if you will, is just another way for Adobe to do some data extraction potentially?  Say, hey, who is calling the home?



STEVE:  It certainly has that ability; right?  They have a server somewhere, presumably still answering these calls.  Maybe not.  Maybe there's nothing there any longer.  Or they're answering the calls, and they've got this massive library of all the IPs of all the systems.  And who knows what else, what other information it's sending back?  I don't want to disparage them anymore, but still.  I guess my point is why haven't they shut this down themselves because they could?



Okay.  So anyway, I first examined my system's registered system services.  And sure enough, right up there at the top when sorted in alphabetical order, was A-D-O-B-E; right?  Adobe Flash Player Update Service.  The service's run state was set to "manual."  So next I went over to my system's Task Scheduler app; and, once again, along with scheduled tasks to keep Google Chrome and Microsoft Edge and a few other odds and ends updated, was the task to run Adobe Flash Update Service hourly around the clock.  My next stop was Windows Programs and Features, where Adobe Flash Player was, once again, at the head of the class.  I highlighted it and clicked Uninstall, and to its credit it did indeed remove every trace of itself from my system.  And good riddance.



Okay.  So this leaves us with two questions:  First, how many of this podcast's security-minded listeners might also still have Flash Player and its very persistent updater present in their systems?  We don't know.  I did.



ANT:  We don't know, but I think it's, again, being someone that listens to this show, they're probably really security-conscious and, like you, potentially had it on the system just for research purposes.  You know?



STEVE:  Could have been.  Or one of those I'll get around to doing it and never did.



ANT:  Right.



STEVE:  Anyway, since it doesn't show unless you peek into the proper corners of your system, it might be worth taking a look at your various Windows machines under Programs and Features.  Just make sure that it's not still represented there.  And while you're at it, why not scan through that list and remove any of the other cruft that most systems tend to accumulate over time?  You know, I bet you that there's a bunch of stuff there that you're never going to use again.



Okay.  The second question is, why hasn't Adobe at least been proactive in shutting down the probable millions of Adobe Flash Player Updater instances that must still be running around the world?  The idea of them still having their hooks, literally, into all of these systems is more than a bit disturbing.  Nearly three years ago they formally stopped further updating of Flash Player.  So if they were unwilling to proactively remove Flash Player from everyone's machines, at least they could use everyone's hourly query to remotely shut down all future queries by removing the Task Scheduler entry and the Update service from everyone's machine.



You know, Adobe never did seem to be highly responsible with their shepherding of Flash Player after they acquired it from Macromedia.  But, you know, at this point it's everyone's individual responsibility to protect themselves.  So I just wanted to give a heads-up.  This happened last week.  I didn't know, even suspect that it was still there doing this.  If anybody finds it in their machine, clearly it's long since time to get rid of it.



ANT:  Yeah, well, like I said, it's probably a data grab.  I put nothing behind these companies, these big tech companies.  It's all data, and it's all cash, and they're just going to figure out a way to continue to capitalize off of all the data that they can get.



Mr. Gibson, one more thing I wanted to point out.  One of our members here in the Discord mentioned that the API Monitor URL, we must mention that it is not an HTTPS URL.



STEVE:  Ah.



ANT:  So just...



STEVE:  That's a very good point.



ANT:  Take it with a grain of salt.  And thank you.  [Crosstalk].



STEVE:  Yeah.  And actually that flows from the point I made a while ago which got me in trouble with our listeners when I said, you know, there's a lot of still very useful stuff on the Internet which is not HTTPS.  So it may be, I mean, I'm wondering who's paying for this and where this is hosted.  It is at his own domain.  But that's a very good point.  I just scrolled back and looked.  It is not HTTPS, just HTTP.  So that's a good point.  And I don't think I checked the signature.  I normally do.  I don't think I did.  So bad on me for not seeing if it was digitally signed.  But still, wow, beautiful, beautiful piece of work.



So over the weekend I received a note from a desperate person. Now, I don't know if he's a listener.  It's unclear why he wrote to me.  Perhaps GRC came up in a Google search for the term VeraCrypt, because it does, because we've talked about VeraCrypt a lot in the past.



ANT:  Yup.



STEVE:  But in any event, this is what he wrote.  He said:  "Hi.  VeraCrypt password lost.  How can I get into my device?  VeraCrypt site says it's impossible."  And then he provides a link.  He says:  "So everything on this device is lost?  Please, if you can help, appreciate any/all help."  And he just signed off with his initials, CGS.  He later sent a second email inquiring whether my consulting services were available for hire.  They're not.  But, you know, I would have told him the same thing.  And I did write back to him.



So sorry as we might be for this hapless person, the entire reason he, or someone, presumably chose to encrypt his device with VeraCrypt is because, assuming the use of a good password, just as VeraCrypt's FAQ correctly stated, no help is possible or available by deliberate design.



ANT:  Right.



STEVE:  And I said "he or someone" just now because we only have his statement to lead us to believe that he has any actual legal or ethical right to the data that has been encrypted on that device.  Right?  Knowing nothing more, it could just as easily be that a thief has stolen someone else's drive, knowing that it contains the password information for that person's cryptocurrency, which could be worth millions of dollars, where the only thing protecting that crypto from the theft is the device's VeraCrypt encryption, and that right now at this very minute the original true owner of this device is thanking his lucky stars.



ANT:  Right?



STEVE:  That first, he chose VeraCrypt; and second, he also locked that drive up with a password that no one will ever be able to brute force.  However, in the meantime, out of an abundance of caution, this person whose jewels have been stolen has had plenty of time to relocate his crypto to some other wallet where it will now, again, be safe.



In any event, the lesson here is, (a), use VeraCrypt.  You know, it remains the go-to solution for open source, audited, whole drive or partition encryption; (b), no matter what, always use a really strong and difficult to brute force password; and (c), be very careful to create ample backups of the password you assigned.  Again, there is - I know our listeners know this.  But just as a reminder, there is no way to get your VeraCrypt data back on purpose.  You don't want anybody else who were to steal your drive to be able to get your data back.  So make a copy of your impossible-to-remember password.  You have to.



ANT:  You know that phrase right there, make a copy of your password, could sometimes be problematic, depending on the person you're saying that to.  What type of suggestions or recommendations do you have for someone that's going to make a copy of their "master password" far as what should it be in, where should it be stored, that kind of thing.  Because we don't - we're not necessarily saying write your password down, put it on a sticky note, and stick it up under the seat cushion.



STEVE:  Right.  I would not post it behind you when you're doing web conferences.  That would not be good.



ANT:  Yeah, or that.



STEVE:  Bruce Schneier, the security guru that we refer to frequently on the podcast, made a comment once that has stuck with me when asked about that.  He said:  "Write it down."  He said:  "We are very good at managing little bits of paper."  He said:  "We are not good at storing things securely."  So the point being I would print it out on a piece of paper and put it somewhere.  I mean, and consider the risk profile.  It is often the case that you're protecting yourself from someone in Russia or China.



ANT:  Mm-hmm, mm-hmm.



STEVE:  Some hostile foreign nation.  They're not going to come to your home from their country and get into the shoebox you have tucked away under the bed.  That's not going to happen.  So that would be a safe place to store your password.  Now, it would not be safe to store it from your kids because they would get into the shoebox that you have.  So you have to keep the threat model in mind.



ANT:  Right.



STEVE:  If you keep a safety deposit box, or if you have an attorney you trust.  And also remember you don't have to actually store the correct password.  You could store the password with one change being made to it, and nobody would ever know what that change was.  Or drop off a chunk of text that you know you always add to the end of your passwords.  That kind of thing.  So there are lots of things you can do in order to arrange to have it available for sure when you need it.  But it's worth taking those kinds of measures.  It's sort of a variation on the theme of what happens if you die.  And, like, you know, now what?  What about your spouse and people you care about and their need to access your stuff in order to help manage the existence that you had up until then.  These sorts of things really do need to be given some consideration.



ANT:  Good point.  Good point.



STEVE:  So exactly one week ago, last Tuesday, Firefox released to their so-called "release channel" their version 120.  And while this is going on all the time with all of our browsers, this one is worth taking a moment to examine.  As we noted last week, with the impending end of Chrome's support for Manifest 2.0, which will disrupt the operation of some of Chrome's more popular advertising and tracking controlling extensions, we may soon be seeing a welcome resurgence in Firefox's popularity because presumably they will continue to support uBlock Origin and Privacy Badger and then Adblocker and the other things that the people really like to use.



So Firefox release 120, as I said, brings us a few new features that are worth noting.  For one thing, its right-click pop-up context menu when you right-click on a link now adds a new feature down toward the bottom named Copy Link Without Site Tracking, which Mozilla says ensures that any copied links will no longer contain tracking information.  And I think that's a great new feature.



ANT:  Yeah.



STEVE:  Why would you not do that?  As we know, one of the ways that we are being tracked is that links are being embellished with unique identifiers.  And if you just copy the whole link, you're going to copy that identifier.  And so, you know, wherever that link goes, whoever clicks on it in the future, is going to be sending an identifier to the link's target unwittingly.  So copy link without site tracking, cool new feature.



Firefox now also supports the welcome setting, it's under Preferences > Privacy & Security, to enable GPC, which we did a whole podcast on a while back.  That's Global Privacy Control, which is a newly defined beacon which is receiving some legal support in order to enforce and encourage its adoption by websites.  So although this is opt-in during normal browsing, it is enabled in private browsing mode by default.  Also in that same Privacy & Security region you'll find the Do Not Track request which can also and should be enabled.  As I mentioned a few weeks ago after rediscovering the EFF's Privacy Badger, Privacy Badger also adds those beacons to every user's web requests.  But there's no harm in adding a belt to go with those suspenders.  So I also have some very encouraging news about the future of DNT to share in a minute.  But a few more Firefox features first.



Firefox's private windows and its ETP strict privacy configuration now also enhances its Canvas APIs with Fingerprinting Protection, essentially continuing to protect the users' online privacy.  As we've discussed in the past, allowing websites to probe various subtle details of how a given browser renders specific pixel illumination to the user's viewing canvas is just one more trick that the trackers have developed to follow us around the Internet.  So Firefox has taken measures to obfuscate that.



ANT:  Outstanding.  Outstanding.



STEVE:  Yes.  And listen to this one.  This is a cool one.  Firefox is rolling out Cookie Banner Blocking in the browser by default in private windows, but only for users in Germany, during the coming weeks.



ANT:  Aww.



STEVE:  Well, but first.  Firefox will now auto-refuse cookies and dismiss annoying cookie banners for supported sites.  Furthermore, also only for all users in Germany for the time being, Firefox has enabled URL Tracking Protection by default in private windows.  Firefox will remove non-essential URL query parameters, just like we were talking happens if you right-click on a link and select that option now, which are often used to track users across the web.  Again, I'll have more to say about Germany in a minute.



Firefox now imports TLS trust anchors, you know, web certificate authority certificates, from the operating system's root store.  Now, that's a little controversial, for reasons we'll explain.  This will be enabled by default on Windows, macOS, and Android; and, if needed, can be turned off in Settings under Preferences > Privacy & Security > Certificates.  On my Firefox 120 this checkbox is labeled; and, yes, it was enabled by default.  It says:  Allow Firefox to automatically trust third-party root certificates you install.



Okay, now, my problem with this wording is that it's misleading.  It sounds as though users, you know, "you" is the word he uses, as though users would be the ones to install those third-party certificates.  But that's almost never the case.  Presumably, Mozilla is attempting to be more compatible with the third-party TLS proxying middle boxes increasingly employed by enterprises to filter their network traffic.  The use of any of those requires that the browser trusts the certificates that those boxes mint on the fly.  Those third-party root certs are typically installed directly into the operating system over the network through active directory and group policies.  Firefox has been unique in that it has always used its own root store which it has brought along and has not been dependent upon the hosting operating system's root store.  So it must be that this recent move just now, with Firefox 120, is intended to make the use of Firefox easier in such enterprise settings.



Now, that's great for the enterprise.  We'd like to see more use of Firefox there.  The worry is that, if the EU gets its way with this misdirected eIDAS 2.0 QWACs certificates mess, and is able thereby to force browsers and operating systems to install their member countries' web certificates into their root stores, then this mechanism, which has just now been added to Firefox 120, would automatically place it into compliance with that EU effort.  Okay, now, given Mozilla's clearly and quite strongly publicly stated position on the EU's eIDAS 2.0 QWACs certificates, it seems unlikely to me that pre-compliance with something to which Microsoft and Mozilla and others quite strongly disagrees is likely.



Remember that rather than signing onto that large open letter that most others co-signed, Mozilla chose to write one of their own, which the likes of Cloudflare, Fastly, the ISRG, and the Linux Foundation and others, all co-signed.  So my guess is that it's that smoother functioning within the enterprise which was the sole motivation.  And note that we do have a simple checkbox that any of us can uncheck if we do not want to have Firefox's root store augmented or polluted by its use of the underlying host OS's root store.



Okay.  So what about Firefox and this Germany business?



ANT:  Yes, please.



STEVE:  There were several interesting changes in Firefox, which I just mentioned, which only benefit German users.  What's up with that?  Turns out the German courts have been weighing several issues, and that their decisions have come down on the side of user privacy and choice.  TechRadar pulled together a nice piece, providing this recent news and also some back story about the DNT and GPC beacons.  So with a bit of editing by me, here's what TechRadar explained.



They said:  "Germany is perhaps the most proactive country when it comes to protecting its citizens' privacy, something that privacy advocates and enthusiasts have been aware of for a while now, and the country recently reiterated its stance against Microsoft-owned LinkedIn.  A Berlin Court found in favor of the Federation of German Consumer Organizations, which filed a lawsuit against LinkedIn for ignoring users who turned on the Do Not Track function in their browsers.  According to the German judge, companies must respect these settings under the GDPR."



So that's big news.  It's a small victory for privacy.  But this Do Not Track ruling might end up reshaping how websites and other online platforms have to handle our data more broadly.  Adoption and support of DNT has been in sharp decline from its initial introduction back in 2009.  Now, adblocker and VPN service provider AdGuard was asked by TechRadar.  AdGuard believes this is a potentially game-changing court decision which could, as they put it, could exhume the once-abandoned - and apparently buried - privacy initiative for good.



So a little bit more back story here.  Do Not Track headers, as we know, are beacons sent by browsers to proactively inform a website not to collect or track that visitor's browsing.  DNT was first proposed by security researchers Christopher Soghoian, Sid Stamm, and Dan Kaminsky well back in 2009 to limit web tracking.  And I just jumped on this thing full on because it was such a clean, beautiful, simple idea.  Just simply turn on a header that says thank you very much, I'm a person who does not want to be tracked.  A year later, after 2009, the U.S. FTC, our Federal Trade Commission, gave its approval and called for the creation of a universal mechanism to give users more agency over their data.



The first web browser to support the new initiative was Mozilla Firefox, which added the feature in March of 2011.  Other services followed suit, including Microsoft's Internet Explorer.  And that, back then, IE was the big browser at the time.  So them adding it was a big deal.  Apple's Safari and Opera were included. Google Chrome embraced the industry trend back in 2012.



Now, as our longtime listeners may remember, the problem was IE from Microsoft turned this on by default.  The fact that it was on by default allowed those who were fighting against it to take the position that, well, wait a minute, if it's on by default, then it doesn't necessarily reflect what the user wants.



ANT:  Choice, no choice there.



STEVE:  So because it's always on, we're going to ignore it, thank you very much.  Anyway, so AdGuard said:  "The early 2010s was perhaps the time when the enthusiasm for the DNT and its potential to improve privacy was at its peak."



Okay.  So unfortunately, after initial success with browsers, the DNT wave seemed destined to dwindle.  The problems started from the lack of similar support among websites and advertisers.  You know, they didn't want to do it.  I mean, they didn't want to obey it.  And nobody was making them obey it.  So why would they?  That's where GPC stands to benefit, and why this recent decision in Germany is significant.  The German court is saying, you know, you have to pay attention to this.



And, wow, even Google, that implemented the DNT feature on its browser, refused to "change its behavior" on its own websites and web services.  In other words, naturally, they declined to honor DNT requests themselves, even from users on their own browser who had turned it on.



ANT:  Shocking, yeah.



STEVE:  The final nail in the coffin came in 2019, when the group working on standardizing DNT was finally dismantled just due to lack of anything happening.  And we talked about this at the time on the podcast.



So privacy advocacy groups did not want to renounce giving users a better way to help protect their personal data and browsing activities.  They wanted to hold on.  AdGuard explained, they said:  "While DNT failed to gain much support, the need for a mechanism that would allow people to opt out of having their personal information shared or sold was still strong."  Privacy focused experts believe organizations should allow their customers to decide whether to have their information shared or sold in the first place.  It was from this need for an alternative that the Global Privacy Control (GPC) was born a couple years ago in 2020.



Like DNT, the GPC is a signal sent with every web request over HTTP and HTTPS to opt-out having browsing data collected or sold.  Supporters of this new initiative include many privacy-first browsers and search engines like DuckDuckGo, Brave, and Firefox; and browser extensions such as Abine's Blur, Disconnect, OptMeowt, and EFF's Privacy Badger, as I mentioned before.  GPC seems to have gained more traction than DNT was ever capable of, until now at least, where DNT may be catching up.  And I think it's just that back in 2009 it was still ahead of its time.  The idea was correct.



But, you know, pressure hadn't built up enough against all of this going on as it has now.  I mean, we even have Google designing their own non-tracking system in order to perform some user profiling in order to deliver more relevant ads.  So, you know, clearly tracking is in trouble at this point in our entire industry.  So in August of last year, GPC won its first legal battle in California against the commercial retail brand Sephora.  And now, on October 30th, 2023, that date may be remembered as a milestone for the DNT initiative because that was the date that the Berlin Regional Court ruled that LinkedIn can no longer ignore its users' DNT requests.  Now, LinkedIn was not silent on this.  We'll get to that in a second.



But Rosemarie Rodden, a legal officer with the German consumer rights group who brought the lawsuit, said:  "When consumers activate the 'do-not-track' function of their browser, it sends a clear message:  They do not want their surfing behavior to be spied on for advertising and other purposes.  Now, website operators must respect this signal."



It turned out that the judge agreed with Rosemarie, ruling that LinkedIn is no longer allowed to warn its users that it will be ignoring their DNT signals.  That's because, under the GDPR, the right to opt out of web tracking and data collection can also be exercised using automated procedures, period.  In other words, the court found that a DNT signal is legally binding.  This sets a precedent and revives the all-but-abandoned idea of Do Not Track.



Now, as I said, not everyone is happy with this decision.  The LinkedIn spokesperson told CyberNews:  "We disagree with the court's decision which relates to an outdated version of our platform, and intend to appeal the ruling."  Now, outdated?  What's outdated, the LinkedIn platform?  If the ruling only applies to an outdated version of the LinkedIn platform, then why appeal it?  And surely you can see with the growing support for the closely related GPC signal, and with Google, as I said, developing their own non-tracking means to obtain interest categories for web users, this is a tide that is finally beginning to shift.



So anyway, more enforcement is likely coming on the horizon.  One decision in one country doesn't change the world overnight.  But it is sure a step in the right direction.  And there are times when having some ambulance-chasing attorneys around can come in handy.  Let's let them, you know, sic them on some other large websites that are not abiding by this court order, and we'll begin to get things turned around.



And Ant, I think we should take our second break so I can catch my breath.  And then we will proceed to talk about PwnCloud. 



ANT:  I wanted to ask you something in following up on this.



STEVE:  Yeah, yeah.



ANT:  With the DNT.  Because I think about all of the websites that I've gone through, you know, throughout the years, that are just random websites, whether it be a photography site or some online shopping or what have you.  Some of them they put the banner up about tracking cookies.  And it gives you the option to reject, so on and so forth.  And then there's others that say, hey, our site uses tracking cookies.  Period.  You can close this window and carry on, or you can get the heck off our website.  Period.  So this is only going to apply for German sites?  Because I guess we're still going to see a bit of the wild, wild west everywhere else, and still have those banners pop up and say, you know what, you don't have a choice.  We're going to track you anyway.  Is that what you're saying?



STEVE:  Right.  So the popup banners and the asking the user for permission or acknowledging that, that was something that the GDPR instigated and required that sites do, that they proactively get users' permission.  The DNT and the GPC - and, you know, it's kind of dumb to have two, be nice if we just amalgamated them into a single beacon.  But those are settings that the user can set in their browser so that the browser is itself saying, "I don't want to be tracked.  I don't want to have my information aggregated and sold or shared with others."



So I think where we are at the moment is in this confusing set of changing times.  So, you know, the GDPR said to websites, if you're going to be using tracking, you need to inform the user and obtain their permission.  That's why we got all these ridiculous banners that you have to click in order to make go away.  Now the court is saying, you know, even that is obsolete.  If the user has DNT and GPC beacons on, period, you are never allowed to track them, no reason to put up a cookie banner because you can't be using cookies that are going to be tracking them.



ANT:  Right.  And that's if they use an automated beacon, is what you're saying.



STEVE:  Right.  And all the browsers, I mean, that's the thing that Firefox just added and other browsers are beginning to return to.  And so I think this is what's going to happen.  Basically, browsers are simply going to be saying this, and websites are going to start having to conform.  They're going to, you know, they're going to be kicking and screaming.  On the other hand, Google has now formally rolled out their interest category system.  That's what everyone's going to have to switch to.  And remember, it's way weaker than tracking.  I mean, tracking allows the aggregation of all kinds of crap, instead of just like what are the interests that your recent web behavior has suggested that you might have, which is the Google replacement system.  So I have a feeling tracking's not long for the world.  As we know, things don't change soon, but they do eventually change.



ANT:  Right.  This is a start.  This is a start.  So ownCloud, PwnCloud, tomato, tomahto, is it one of those things?  What's the deal here?



STEVE:  Well, it appears that anyone running any instance of the very popular open source "ownCloud" file sharing system needs to take immediate action - as in, stop listening to this podcast right now, and immediately unplug anything running ownCloud to get it off the Internet.  Unfortunately, due to today's ultra-swift nature of the exploitation of any publicly announced new vulnerabilities - in this case it's a remotely exploitable CVSS 10 out of 10, which is difficult to get - it may already be too late.  But even if so, at least closing the unlocked front door and working to clean up any damage still needs to be done.



Okay.  So here's the story.  GreyNoise reported the following in their coverage of CVE-2023-49103.  Yesterday they wrote, under the title "ownCloud Critical Vulnerability Quickly Exploited in the Wild," they said:  "On November 21st, 2023" - so that's exactly one week ago today - "ownCloud publicly disclosed a critical vulnerability with a CVSS severity of 10 out of 10.  This vulnerability, tracked as CVE-2023-49103, affects the 'graphapi' app used in ownCloud.  ownCloud," they wrote, "is a file server and collaboration platform that enables secure storage, sharing, and synchronization of commonly sensitive files.  The vulnerability allows attackers to access admin passwords, mail server credentials, and license keys.  GreyNoise has observed mass exploitation of this vulnerability in the wild as early as November 25th, 2023."  Okay?  So it took four days from the announcement for mass exploitation to take off.



They said:  "The vulnerability arises from a flaw in the 'graphapi' app, present in ownCloud versions 0.2.0 to 0.3.0. This app utilizes a third-party library that will reveal sensitive PHP environment configurations, including passwords and keys.  Disabling the app does not entirely resolve the issue, and even non-containerized ownCloud instances remain at risk.  Docker containers before February 2023 are not affected.  Mitigation information listed in the vendor's disclosure includes manual efforts such as deleting a directory and changing any secrets that may have been accessed.  In addition to this vulnerability 49103, ownCloud has also disclosed other critical vulnerabilities, including an authentication bypass flaw (49105) and a critical flaw related to the OAuth2 app (49104).  Organizations using ownCloud should address these vulnerabilities immediately."  Okay.



ANT:  Boy.



STEVE:  That's bad.  So for ownCloud users, we have a potential four-alarm fire situation.  There are three newly disclosed CVEs with ratings of the difficult-to-obtain 10.0, a highly critical 9.8, and a still very bad 9.0.  That 49103 CVE with a CVSS of 10.0 allows for disclosure of sensitive credentials and configuration in both containerized and non-containerized deployments.  The 49105 CVE is the second worst, with a CVSS of 9.8.  It's a WebDAV API authentication bypass using pre-signed URLs which impacts core versions from 10.6.0 to 10.13.0.  And the third 49104 CVE with the CVSS of 9.0 is a subdomain validation bypass impacting OAuth2 prior to version 0.6.1.



So in the case of this first worst mistake - and really a mistake is what it is.  It's not some fancy Log4j tricky-to-exploit vulnerability.  Anyone who has any experience with PHP knows that you never want to expose PHP's phpinfo applet to the public Internet, yet that's exactly what this 'graphapi' has done.  Located down the path:  "owncloud/apps/graphapi/vendor/ microsoft/microsoft-graph/tests/" is a "GetPhpInfo" PHP file - and it can be accessed remotely to disgorge all of the system's internal sensitive data, including all the environment variables of the web server.  And in containerized deployments this includes the ownCloud admin password, mail server credentials, and license keys.  ownCloud recommends deleting that file and administratively disabling the very dangerous phpinfo function.  This can be done by simply adding "phpinfo" to the "disable_functions" list in the system's php.ini file.  And sadly, that list is empty by default, meaning PHP ships with phpinfo enabled.  Not on my PHP instances, but by default.



After doing this, however, do not make the mistake of not also immediately rotating all of the system's credentials - the admin password, the mail server and database credentials, as well as Object-Store S3 access keys if the ownCloud instance was hosted by an S3 cloud provider.  Again, multiple, I think it was 12 independent, probably malicious, we don't know, maybe they were some security firms.  But there were 12 unique IPs identified scanning the Internet, looking for instances of ownCloud.  So in this instance you have to presume your instance was infected by something if it was exposed to the Internet, since essentially they all were.



The second problem makes it possible, that is, the other CVE at 9.8, to access, modify, or delete any file without any authentication if only the username of the target is known, and if they have no signing key configured on their account, which is the default behavior, that is, not to have one.  That's also obviously quite a potentially serious vulnerability, to be able to access, modify, or delete any file without any authentication, knowing only the username of the target.  



And the third 9.0 flaw relates to a case of improper access control that allows an attacker to pass a specially crafted redirect-url which bypasses the validation code and thus allows the attacker to redirect callbacks to a TLD controlled by the attacker, a top-level domain controlled by the attacker.  Anyway, bottom line, everyone using ownCloud should update to the latest builds and make sure that everything else is still okay because I did say 12 unique IPs were found to be scanning the Internet looking for instances of ownCloud and were carrying through with exploits.  So update.  Change all of your sensitive credentials.  It's a must-do, unfortunately.



While we're on the topic of critical vulnerabilities that will wreck your day or your week or maybe even your month, anyone using the sadly named "CrushFTP enterprise suite," and there are currently somewhere around 10,000 publicly exposed instances of it on the Internet, must immediately update to v10.5.2 or later.  Back in August, the security firm Converge Technology Solutions responsibly disclosed a critical unauthenticated zero-day vulnerability, meaning you don't need authentication to use it, to exploit it, which affects the CrushFTP enterprise suite.



Having 10,000 of these instances publicly exposed is bad enough, but a great many more are known to be residing behind corporate firewalls which malware might manage to crawl behind.  The exploit permits an unauthenticated attacker to access all CrushFTP sites, run arbitrary programs on the host server, and acquire plaintext passwords.  The vulnerability was fixed in CrushFTP version, as I said, 10.5.2, and it affects software in the default configuration on all operating systems.



What's more, Converge's threat intelligence has discovered that the security patch which resolved this problem has been reverse engineered, and adversaries have developed proofs of concept.  So forthcoming exploitation can be presumed.  Update immediately.  The attack chain hinges upon an unauthenticated query when CrushFTP parses request headers for a data transfer protocol called AS2.  By exploiting the AS2 header parsing logic, which obviously has a flaw, the attacker gains partial control over user information Java Properties.



This Properties object can then be leveraged to establish an arbitrary file read-and-delete primitive on the host system.  Using that capability, the attacker can escalate to full system compromise, including root-level remote code execution.  So it's really bad.  10,000 instances are public.  The patch has been reverse engineered.  Those 10,000 servers are going to get attacked.  Update to 10.5.2 ASAP if your enterprise is using the CrushFTP because, as I noted at the top, you don't want to have your spirits crushed.



ANT:  Wow.  With something like this, because I just looked up this CrushFTP, are you saying this is 10.5 or greater where we need to step up to?  They're already up to, like, 10.9 with this application.  Is there any reason why someone would use this?  Because there are plenty of other FTP options out there, including some open source options that are out there.  Why would an enterprise not even look at the open source side of things?



STEVE:  Well, I did not look to see what CrushFTP enterprise suite means.  And, for example, this AS2 protocol is something I've never encountered by FTP.  So it may be that it's got a  bunch of extra special features that specifically target it to the enterprise.  And who knows, a suite implies there are other components to it.  So it may be a big package that has a whole bunch of other stuff that is targeted at the enterprise, and this CrushFTP server is just one of the modules.  Unfortunately, it only takes one to be bad in order to give the whole thing, make the whole problematic.



ANT:  True, true.



STEVE:  So a very interesting set of flaws has been found in the fingerprint sensors manufactured by Goodix (G-O-O-D-I-X); Synaptics, they're a very popular supplier, I ran across Synaptics, they make the touch pads in most laptops; and ELAN (E-L-A-N).  The OEMS who purchase and integrate those sensors and whose equipment is therefore vulnerable to fingerprint sensor-based authentication bypasses include the Dell Inspiron 15, the Lenovo ThinkPad T14, and Microsoft Surface Pro X laptops, just to name a few which are known to contain these popular sensors.



So this is what happens when a hardware-savvy security firm takes a close look at what's going on inside of fingerprint sensors.  And as is often the case, the result is frightening.  All three of the fingerprint sensors are the "good kind," which perform something known as MoC verification, which stands for "match on chip."  That's what you want since it integrates the matching and other biometric management functions directly onto the sensor's integrated circuit.  But the researchers said:  "While MoC prevents replaying stored fingerprint data to the host for matching, it does not, in itself, prevent a malicious sensor from spoofing a legitimate sensor's communication with the host and thus falsely claiming that an authorized user has successfully authenticated."



Okay.  So to thwart this problem in general, Microsoft created something known as the Secure Device Connection Protocol (SDCP).  It's designed to eliminate this problem by establishing an end-to-end secure channel between the sensor and the machine's motherboard.  And we know how this can be done in practice.  You know, TLS that is the protocol that HTTP uses to establish a secure connection between endpoints in full public view, it works.  So SDCP can theoretically work.  You're able to share secret keys, establish a symmetric key, and as long as you have authentication of the endpoints, that can potentially be secure.  But the researchers designed a novel technique that can successfully circumvent these SDCP protections to create adversary-in-the-middle attacks, as they call them.



ANT:  Of course they did.



STEVE:  Now, that's - uh-huh.  That's if this is turned on.  And get a load of this.  The ELAN sensor which interfaces over USB doesn't even offer SDCP.  So it's easily spoofed simply by sending cleartext security identifiers.



ANT:  Oh, boy.



STEVE:  This allows any device, any USB device, to masquerade as a fingerprint sensor and claim that an authorized user is logging in.  In the case of the Synaptics fingerprint sensors, not only was SDCP found to be turned off by default, the implementation used a known-flawed custom TLS stack to secure its USB communications between the host driver and the sensor. So once again it was possible to defeat the biometric authentication.



The exploitation of the Goodix sensors leverages a fundamental difference in enrollment operations carried out on a machine that's using Windows versus Linux, or a machine that transiently boots a copy of Linux.  This takes advantage of the fact that Linux does not support SDCP.  So we have what is essentially a protocol downgrade attack.  This truly lovely hack is done as follows:  Boot Linux.  Enumerate valid IDs.  Enroll the attacker's fingerprint using the same ID as a legitimate Windows user, and again you can do this because SDCP is not supported by Linux.  Intercept the connection between the host and the sensor by leveraging the cleartext USB communication.  Then boot to Windows.  Intercept and rewrite the configuration packet to point to the Linux database.  And finally, log in as the legitimate Windows user with the attacker's fingerprint.  Essentially, this allows for the installation of an attackers fingerprint and association to the legitimate user's fingerprint.



It's also worth noting that, although the Goodix sensor design anticipated this bait-and-switch weakness and therefore uses separate fingerprint template databases for Windows and for non-Windows systems, the attack is still possible thanks to the fact that the host driver sends an unauthenticated configuration packet to the sensor to specify what database to use during sensor initialization.  So you simply change that, point it to the Linux database, and now you log in with a fingerprint that you set up when the system had been booted under Linux.



To mitigate such attacks, the researchers have recommended that OEMs enable SDCP and ensure that the fingerprint sensor implementation is audited by independent qualified experts; you know?  These guys did that.  So we insert our standard refrain there:  Have the security audited by somebody who wants to find flaws, not by your own people who just finished writing it themselves and assume it works.



Just for the record, this is not the first time Windows Hello biometrics-based authentication has been successfully defeated.  In July of 2021, Microsoft issued patches for a medium-severity security flaw, 2021-34466 that had a CVSS of only 6.1.  But it could still permit an adversary to spoof a target's face and get around the login screen.  The researchers said that:  "Microsoft did a good job designing SDCP to provide a secure channel between the host and biometric devices, but unfortunately device manufacturers seem to misunderstand some of the objectives."  Like having it turned off by default.  I would call that a misunderstanding.  "Additionally, SDCP only covers a very narrow scope of a typical device's operation, while most devices have a sizable attack surface exposed that is not covered by SDCP at all."



Okay.  So I think our takeaway from this should be to not over rely upon the convenience offered by biometric authentication.  Yes, it's convenient to be able to hold your phone up and have it look at you and say, oh, that's Steve, or that's Ant, or whomever.  But this is why Apple, whose biometric authentication has been very tightly designed by security-crazed engineers, will require the "something you know" to be provided initially when you're unlocking your device following any restart.  You know, if I had any device whose security was truly critical to me, I'd encrypt its drive, and I would supply the key with an outboard USB dongle, not something biometric.  In fact, that's what I did when I was in Europe traveling with a laptop during the SQRL tour years ago.  It was deeply encrypted, and I used a physical dongle in order to supply the unlock key.



ANT:  And Mr. Gibson, looking at this story, I think about there's an old adage of, you know, if there's a problem, typically you can throw some money at it to fix it.  And I know that may not apply for everything, especially when it comes to hardware, computer hardware and whatnot.  But I'm looking at these manufacturers, these OEMs that provide this.  What was that one touchpad, hold on, I've got to scroll up, Synaptics.



STEVE:  Yeah.



ANT:  They're in everything.



STEVE:  Yup.



ANT:  That touchpad is in everything.  And typically it's in the less expensive laptops that are out there.



STEVE:  Yup.



ANT:  Microsoft.  Is this something, is this on Microsoft?  Or is this on Synaptics?  Because Microsoft clearly got a deal for that licensing, to put it in all of these devices.  But yet Synaptics dropped the ball by cutting that off, by cutting off SDCP by default. 



STEVE:  So it's probably the case that Synaptics sensors offer SDCP, but I would imagine - so we don't know which of these sensors goes with which of the OEMs that I mentioned.



ANT:  Okay.



STEVE:  I would imagine that Microsoft would require SDCP be enabled.  The only one of those three of ELAN, Synaptics, and Goodix, where the researchers found SDCP actually enabled and running, was on the Microsoft Surface tablets.



ANT:  Oh, okay.  All right.



STEVE:  So that would be my guess.  It would be surprising if Microsoft supported this protocol and Synaptics was allowed to leave it off.  It would probably have to be turned on.



ANT:  Yeah.  I was thinking about it, and I was like, and then you brought up Apple.  And I was like, yup, that's exactly my thought because Apple spends a lot of money on this stuff from a security standpoint, and those relationships, and making sure things are done in a particular way.  Sounds like is this just where people are cheaping out on things and should just spend a little bit more money with these relationships and these licenses?



STEVE:  Well, and that's the problem is that, as we know, Apple did a really strong job of implementing a fingerprint sensor.  You know, they really, we covered it in detail when it happened on the podcast.  They nailed this technology.  The problem is other people come along with a fingerprint sensor, and the consumer thinks, oh, you know, it's a fingerprint.  Mine's unique, and it's not going to be like anybody else's.  They're only looking at, literally, at the surface of their skin.  That says nothing about the technology that implements what happens when that skin hits the road and actually has the fingerprint read.  So just the fact that it requires some biometrics, it implies nothing about the security behind that.  And that ELAN sensor, it has no security at all.  It's just wide open.  Crazy.



ANT:  Yikes, yikes.  Unbelievable.  So what's going on with Apache ActiveMQ, sir?



STEVE:  Well, so we talked about this a few months ago.  They had a horrible problem with something known as Apache ActiveMQ, a message queuing vulnerability.  It was another of those 10.0s.  I just wanted to mention that it remains under very active exploitation.  A proof of concept exploit was initially posted on GitHub.  It was later updated to add an English language translation, and then two weeks ago it was further improved to change and basically bolster its TLS support.  So by now, pretty much any Apache ActiveMQ server that has been left unattended will be spinning its fans as fast as possible because cryptominers have been observed being installed into any still-vulnerable servers.  They've just been turned into cryptominers for as long as they can run, you know, generating cryptocurrency for the bad guys.



Okay.  So a public service announcement by way of our major credit reporting bureaus.  Two of them, TransUnion and Experian, were both just hacked, with their super-sensitive consumer data exfiltrated.  The hacking group named "N4ughtySecTU," so that's Naughty Sectu, is asking - I know - is asking for $30 million from each firm, threatening to release its customers' data online.  And this is the second time the N4ughtySecTU group has hacked TransUnion, having previously done so back in March of last year, 2022.



So let me just take this opportunity to once again remind everyone that all four of the major credit reporting bureaus support credit locking, and that everyone - everyone - should be taking advantage of this feature.  Given today's cybercrime environment, and the fact that those who are holding and aggregating our private information without our permission - no one ever gave these credit reporting agencies the right to collect all this stuff on us.  They just do it, and they resell it for money, and they track us, so without our permission.  They have been proven unable to keep it private.  So we need to minimize the chance that our private information, if it escapes, will then be leveraged against us for identity theft.  Identity theft is one of the most debilitating, and difficult to recover from, things that can happen to an individual.



A few years ago I decided that since I was such a large customer of Amazon, I would start routing my purchases through their card to obtain an additional several percent of savings.  To apply for their card, I needed to briefly drop my credit reporting agency shields to allow Amazon's credit folks to verify my credit worthiness.  What I learned at the time was that it is now possible to ask the bureaus to temporarily drop our shields for a specified duration, you know, like seven days, after which time they will automatically snap back up.  So that really removes the last barrier of inconvenience from having one's credit reporting blocked by default.  Everyone listening and everyone you care about should be running with their shields up full-time.  There's just no reason not to.



ANT:  Yeah, I agree 100%.



STEVE:  And Ant, as we start taking some listener questions, why don't we take our last break, and then we will see what Christian has asked.



ANT:  Oh, sure, yeah.  I'm looking forward to some of the feedback from our fine listeners.  Thank you again, Mr. Gibson.  Another great episode so far here on Security Now!, and thank you to everybody hanging out watching this live, whether it be in IRC or here in our Club TWiT Discord.  Really do appreciate y'all being here.  Mr. Gibson, about time to close the loop and check out what our awesome listeners have sent in; right?



STEVE:  Yup.  So Christian, I think his name, I would pronounce it Rutrecht, R-U-T-R-E-C-H-T.  He said:  "Hi, Steve."



ANT:  I'm not going to try to pronounce it.



STEVE:  "Not sure" - yeah - "if you have managed to catch up to Passkey support in Bitwarden.  I have not heard it mentioned lately in Security Now!.  I've just started testing it on selective services, and it works flawlessly across my various devices.  I am very impressed, I must say."  And we need to mention that Bitwarden is a sponsor of the TWiT network.



He said:  "What I would like to know is the view you have on adding all your Passkeys to a combined password vault?  I know that you have a standpoint that MFA verification apps or physical token devices should be separated.  But what about Passkeys?  Personally, I prefer combining everything for the sake of convenience.  I have family members and colleagues that I try to nudge towards using a password manager.  For them to be able to use it, it must be easy going.  Even the concept of having 'some thing' remembering their password for them is complicated to comprehend for some.



"In my research I've found that the best way of keeping my password security posture up to date and ready available for access is to have a single vault/app for everything.  I chose Bitwarden for that purpose at the conclusion of my research two years ago, as it was the best hardened platform available, including the support for authentication tokens, and now Passkeys.  Keep up the good work."



So Christian, I would classify Passkeys exactly as I would passwords.  Passkeys are just superior passwords because, by using public key asymmetric crypto instead of secret key symmetric crypto, Passkeys are inherently immune to a great many of the attacks and failure modes that have always beset passwords.  In other words, I think it's entirely acceptable to keep Passkeys in the same vault, managed right alongside your traditional passwords.



And as you noted, I do feel strongly that the entire point of multifactor authentication is to create a clean and clear security boundary for use when remotely authenticating to a higher than usual security facility.  For that reason, the idea of having a password manager also able to fill in the time-varying six-digit MFA token makes me shake my head.  Why bother at all, if that's what you're going to do?  It's true that some benefit will be derived from the inherent time-varying nature of the token.  So simple replay attacks will be thwarted.  But if you're going to go to the trouble of using some form of multifactor authentication, why not get as much benefit from it as you can?  And that means keeping it separate from your web browser that is able to fill in all the rest of your information.  Don't have it also fill in your multifactor authentication.



ANT:  But sir, we want convenience.  Users want convenience and stuff to be done like right now.



STEVE:  In fact, if you want convenience, I would say don't bother adding multifactor authentication to any accounts, if you're going to have your browser automatically fill them in.  That's nutty.



Okay.  So here's the one that's I think interesting, Ant, that I asked you if you had kids about earlier.  I'll read his question, I'll share my reply to him, and then let's talk about it.  So a listener named Victor wrote.  He said:  "Howdy, Steve.  Longtime IT guy here, but recent, about a year, listener as I didn't really do podcasts until Security Now!.



"I have a question for you that may be something of a rabbit hole, but I am seeking opinions on parental controls.  I consider myself a well enough accomplished IT guy, but I'm facing a problem that one of my kids is about step into the realm of getting a phone.  We, my wife and I, have held off until now, citing COPPA laws, but we are out of excuses at this point.  The issue is that, for all of my IT experience, this kid, my third, is exceedingly more tech savvy than any of my other kids..."



ANT:  Imagine that.



STEVE:  Uh-huh, "...having already proven their ability to circumvent restrictions on school-operated technology and continuing to do so without any repercussion as the school can't seem to collect any evidence of their wrongdoing.



"I've done my best to protect the home front - Pi-hole, pfSense router with static routes to nowhere for undesired sites, et cetera.  But once on the phone, the kid will be able to connect when/wherever they please.  And I have yet to find a truly secure parental control app which will do all the standard watchdog things AND self-protect from deletion on Apple and/or Android.  Any advice is welcome. Here's to 999 and beyond.  Signed, Victor."



Okay.  So I wrote to him.  Though I never had my own kids, during my late 20s, 30s, and 40s I participated in raising several long-term girlfriends' kids from pre-teen through their teens.  And although I was never more than "Mom's boyfriend," I was around during some important years, so we bonded.  And I've remained in touch with several of them who are now married and with their own kids.  So I'm not a total newbie on this front.



Victor didn't share the age of his youngest and most technically savvy of the three, so this might not apply if this individual is too young.  But if this person is this tech savvy, then perhaps they're not that young.  What occurs to me is to wonder whether this particular problem has a technical solution.  I think that perhaps the solution lies in parenting rather than in technology.  I am horrified by what is now available - I was distracted, I should explain to our listeners, by Ant, who just did a big "yay, congratulations" by my saying that the solutions lie in parenting rather than technology.



ANT:  My bad.  My bad.



STEVE:  No, that's okay.  I'm horrified, thank you for that, I'm horrified by what is now available on the Internet.  And I completely get it that age appropriateness is a real thing.  There are many salacious adult depravities that young minds should not be exposed to until they have obtained sufficient context and maturity to understand them for what they are.



But at the same time, "blocking" feels like a losing uphill battle.  The Internet is truly pervasive.  If this youngster wants access to the Internet, he or she is going to obtain that access.  If not at home where IT security is strict, then over at a friend's home whose parents never considered this to be a problem, or by breaking through the school's security.  And erecting technical blockades might just present a challenge to make what's hiding behind them seem all the more intriguing.



Given what's out there, I understand the dilemma that today's parents face.  And I would not want to be in that position today.  But I also believe that there's a very real limit to a parent's ability to control what a free-ranging young person is exposed to.  I think that if I were in this place, in Victor's place, I would sit down with all of my kids as a group and talk to them honestly and openly about what's on the Internet, and why.  About how a great deal of what's there does not represent what most people think and feel.  About how it's often deliberately extreme.  About how behind a lot of it is a profit motive, trying to separate people from their money one way or the other.



And I would also take some time to explain about predation on the Internet.  About how there are truly dangerous people hiding behind fake names, photos, and identities.  That these people are often not who they claim to be.  They may well be in a far off country and not be at all nice people.  And that the only thing that you ever really know are the people you've met in the real physical world.  I would not pull any punches.  I'd tell them that I'm terrified by the idea of them being exposed to what's out there on the Internet.  And that the only thing that will keep them safe is their own common sense and keeping lines of communication open with their true friends in the real world, and with their parents.



ANT:  And look here.  First off, let me just give you an applause because everything you just said, we're pretty much in agreement here.  I echo you.  This stuff with the Internet, and I think back to my time with my kids - granted, they're older now.



STEVE:  Right.



ANT:  I have one in college, he's a junior in college.  I have a high school senior.  And my oldest boy, my stepson, he is like, I think he's 25 now.  You know, so they're out and about.  But when phones began to be a big thing in their lives, I was tough.  I was hard on them.  I was like, no, you're not getting a phone.  Period.  You're not getting a phone.  And it sucked.  I hate to say it that say.  It was hard to do that.  But I knew that having a phone wouldn't necessarily help them, far as getting their work done and being good students and whatnot.  But I also thought that it was going to lead to some social issues, with kids being kids, because kids will make fun of you if you don't wear the right type of T-shirt for whatever reason.  



STEVE:  Yup.  And if you don't have a phone, oh, your mommy won't let you have a phone.



ANT:  Exactly.  So I knew I was going to be in it for that.  But I thought in the long run they were going to be better off.  But just as you mentioned, I explained to them why.  I explained to them the realities of the Internet.  There's a lot of things out there that are just, whoo, wow.  But then there's also some good stuff on the Internet, great information.  And I had to teach them, you know, there's going to need to be a balance.  And right now at this particular age you're not ready for a phone.  I did the same thing with particular video games.  Grand Theft Auto wasn't allowed in my house for a little while.  You know.



Now, when they got older, sure.  Now you can play it because you have a little bit more common sense and know that's not the reality.  But mentioned here in our Discord, I believe it was Berserk, says 100% this kid would see the parents as a "challenge."  I agree.  That right there.  I did not want to use tech to get in the way and put up firewalls and things like that because all they were going to do is just try to figure out a way to get around it and put a lot of effort in that that they didn't need to when they should have been putting that same amount of effort into stuff that matters, like homework.  You know?



So I didn't want to just throw up a brick wall for everything.  I just tried to be upfront and say no when I needed to say no.  And when it was time for them to be able to get phones and access to the Internet and stuff like that, I gave it to them with a couple caveats and just sort of eased them into it.



STEVE:  Well, and if nothing else, your being that strong, even if they did go to a friend's house in order to have their curiosity satisfied, the fact that you made such an issue of it that, I mean, someone who their dad was saying, this is really bad, even that would tend to create a barrier of caution that would serve to help them. 



ANT:  It planted a seed because there were instances where they would come back to the house after visiting, you know, so-and-so relative, you know, this cousin, this friend or what have you, and they would pretty much tell me everything they saw the other kids doing.  And they were not comfortable.  You know?  And I had to tell them, okay, yes, thank you for letting me know.  It's all right that they did this and that or what have you.  But that's in their own home, under their own parents' jurisdiction, not my jurisdiction.  I appreciate you honoring our relationship and our agreement.  



STEVE:  Well, it's very cool, too, that those lines of communication are kept open because that's the crucial thing you want is for them not to be sneaking around and keeping secrets and thinking that they can't share with their parents what's going on.



ANT:  And don't get me wrong.  My kids are known as "hard heads," I call them #hardheads.



STEVE:  Wait.  Your kids?  I can't imagine that.



ANT:  There's a reason for that.  But at the same time I do appreciate them being able to come to me and their mother when it comes to stuff about technology and just information security and so forth, or social media.  And we still have those talks.  And like I said, they're a lot older now.  And every now and then we still have some conversations about things we see on TikTok or things we see on Instagram or what have you because it's not...



STEVE:  It's a crazy world out there.



ANT:  It's not just the fact that something could be pornographic or whatever.  Some things are just flat out lies sold as truth.



STEVE:  Right.



ANT:  And they need to say, hey, I need to check my sources here because that doesn't look right, you know.  But, yeah, kudos to you, and applause to you, sir.



STEVE:  So Alphageek asked, he said:  "I'm interested in your take of this from a security standpoint."  And Ant, this is about cameras so you're going to like this, too.  From a security standpoint, he said:  "Thanks for the years of helping keep my brain sharp.  As an EE, your podcast has helped me look smart at important times."  So, great.



So what Alphageek was curious about, he provided me a link, was an interesting solution to the problem with deep fake photos, talking about things being fake on the Internet just now, Ant.  The IEEE Spectrum Magazine carried an interesting story about a new Leica camera that binds authenticating metadata into the photos it takes, then digitally signs them as they are taken.  And there's more.  So here's what the article explained.



Article said:  "Is that photo real?  There's a new way to answer that question.  Leica's M11-P, announced in late October, is the world's first camera with support for content credentials, an encryption technology that protects the authenticity of photos taken by the camera.  The metadata system can track a photo from shutter snap to publication, logging every change made along the way.  



"Award-winning photographer David Butow said:  'In the last few years it's become easier to manipulate pictures digitally.  Photographers can do it; and when the photos are out on the web, other people can do it.  I think that puts in jeopardy the strength of photography, the sense that it's a true representation of what someone saw.'



"In November of 2019, Adobe, The New York Times, and Twitter partnered to solve this problem by founding the Content Authority Initiative (CAI).  Twitter left the CAI after Elon Musk purchased the company.  But CAI now boasts over 200 partners, gave itself the difficult task of finding a 'long-term holistic solution' for verifying the authenticity of photos.  In 2021 it joined with another initiative called Project Origin to form the Coalition for Content Provenance and Authenticity (C2PA).



"Leica's M11-P is the first hardware embodiment of its solution.  The camera has a toggle to flip on content credentials, which is based on the C2PA's open technical standard.  The M11-P then embeds identifying metadata such as the camera, lens, date, time, and location in an encrypted C2PA 'manifest.'  The M11-P digitally signs the manifest with a secure chipset that has a stored private key.  The manifest is attached to the image and can be edited only by C2PA-compatible software which, in turn, leaves its own signature in the manifest.  Once published, the image can display a small interactive icon that reveals details about the photo, including the device used to take the photo, the programs used to edit it, and whether the image is wholly or partially AI-generated.



"It's still early days for content credentials, however, so support is slim.  Adobe's software is the only popular image-editing suite to support the standard so far.  The presentation of the data is also an issue.  The interactive icon isn't visible unless an application or program is programmed to present it.



"David Butow said:  'The way this technology is integrated in Photoshop and Lightroom, which is what I use, is still a bit beta-ish.'  David used the Leica M11-P for several weeks prior to its release, but he says these early problems are countered by one key win:  The standard is easy for photographers to use.  'You shoot normally; right?  There's nothing that you see, nothing that you're aware of when you're taking the picture.'



"The Leica M11-P's support for content credentials wasn't the only reason it made headlines.  It arrived with an intimidating price tag of $9,195."



ANT:  Oh, that's just typical for Leica.



STEVE:  The article said:  "That's a high price for authenticity, but Leica says" - exactly as you said, Ant - "says the camera's cost has more to do with Leica's heritage.  Kiran Karnani, Leica's vice president of marketing, said:  'If you look at the price points for our M series cameras, there's absolutely no added cost to have the content credentials feature in the M11-P.'  And the M11-P is just the tip of the iceberg.



"Canon and Nikon already have prototype cameras with content credentialing support.  Smartphones will also get in on the action.  Truepic, a startup that builds 'authenticity infrastructure,' has partnered with Qualcomm to make Qualcomm's Snapdragon 8 Gen 3 chips support content credentials.  Those chips will power flagship Android smartphones next year.



"No news organization currently requires photographers to use content credentials, but the C2PA standard's influence is beginning to be felt.  Karnani points out that The New York Times and BBC are members of the CAI, as are The Wall Street Journal, The Washington Post, the Associated Press, Reuters, and Gannett.  Karnani notes that 'Adoption is certainly a goal.'"



So back to Alphageek's question.  This all sounds great on the surface.  A digital camera contains a digital representation of an image which can be digitally signed by the camera itself.  The way this would be done is that metadata would be added to the image.  Then a cryptographic hash would be taken of the combined file.  That hash would then be encrypted using the camera's private key.  Then at a later time it would be possible to verify that not a single pixel of the original image had been tampered with by rehashing the image, and using Leica's published public key to decrypt and verify that the hash bound to the image matches the one that was just made.  But from everything we know of crypto there would appear to be one glaring problem with this entire concept.



ANT:  Do tell.



STEVE:  A web server's private key is secure only because no unauthorized people are able to obtain its key.  If that key is in a hardware HSM, then that key won't even exist in the machine's memory, making it even less accessible.  Although asymmetric encryption offers many cool features and powers, it does still rely upon a secret being kept.  Its private key must remain private.  And that's the Achilles heel that I fear any digitally signing camera will face.



A web server's private keys are safe only because no one has unauthorized physical access to its hardware.  If you can get to the hardware, all bets are off.  Just ask the folks that thought that encrypting DVD discs was a great idea.  They thought, hey, no problem.  We'll just embed all of the decryption keys into every consumer DVD player so that they'll be able to decrypt the discs.  Right.  Back in the day, my copy of "DVD Decryptor" was one of my favorite tools.



ANT:  I have no comment on that.



STEVE:  It was, oh, it was and still is entirely legal to decrypt one's own DVDs, and I appreciated the freedom that that afforded.



In order for this Leica, or any other camera, to digitally sign anything, it must carry a secret.  It's the camera's secret that makes its signature mean something.  But the camera is obviously not locked up in some data center somewhere.  Just like a DVD player, it must be out in the open to do its job.  And everything history has taught us is that these secrets cannot be kept, not under these conditions.  And if that's true, it creates another new problem that we never had before, digitally verified deep fakes.  Once a camera's secret signing key escapes, deep fakes will be signed and digitally authenticated, making the problem worse than it was before.  So it'll be interesting to see how this all turns out.  Mark me down as skeptical and a bit worried.



ANT:  Okay.  So yes, this story got my attention.  And I spoke about this back in October on Tech News Weekly with our hosts Mr. Mikah Sargent and Mr. Jason Howell, October 12th, 2023.  This was right after the Adobe Max Creative Conference.  And a big discussion was of course AI.  But it was also about content authenticity.  Adobe, as the emailer mentioned, has been working with the C2PA for quite a while now, a handful of years now.  And so we've had some developments on this.  And it was all good news.  But just as you say, there are a couple caveats.  First of all, the presentation.  I could shoot something with my camera that is certified and has all of the proper encryption to put that badge on the image.  But what if I put that on Instagram?  Instagram's not going to show that badge.  It's not going to mean a hill of beans.  You know.  And then I never thought about the aspect that you brought up of that key being out in the public and available to anyone.  They could take it and make some totally ridiculous images that are fake.  But they're properly signed. 



STEVE:  Right.



ANT:  So they're still considered official.



STEVE:  Right.



ANT:  So, yeah, this is - I'm glad that this is in place.  I'm glad that there's some headway being made here with the different partners with the C2PA because it's Microsoft, I'm looking at the page now, Microsoft, BBC, Intel, Sony.  So people are talking about this, and they have good intentions, especially right now with this being November here in the U.S. and the official start of the U.S. election season.  I think this is really important to figure out a way to get our hands on some of this content that's being put out there as mis- and disinformation.  But again, this is very early.  It's not perfect.  But it's a start.



STEVE:  So Andrew Drapper said:  "If the EU demand their certificates are in our root store, could we not just remove them or have a script or extension that does?"



Okay.  So great question.  Many questions remain about this whole unresolved mess.  For example, would the EU's certs be countersigning traditional certificate authority certs?  Based on the behavior that the EU wants, that could be a requirement.  But if not, then removing those trust roots would prevent access to those EU web services that had only been signed by the EU certificates and had not been countersigned.  Would these EU certs be trusted all by themselves, if they're standing alone?  If not, then we really don't have anything to worry about.  So long as a traditional CA also needs to sign a website's certificate, the EU's signing would simply be adding additional information.  But if this were the case, everyone would not be all up in arms over this, and everyone is.  So it sounds like the EU certs are going to be able to stand alone, and that's a problem.  



So it appears that the EU wants their certs that way, to be able to stand alone.  Would these EU certs carry some distinguishing mark that would allow an automated cert sweeper to uniquely identify and remove them?  I suspect that the CA Browser forum would require some form of clear designation, and the good news is that certs have all manner of means for carrying such markings.  This would make a cert cleaner entirely safe.  It would be able to identify those certs which were based on this EU eIDAS law.



One potential problem is that users of affected machines, such as in the enterprise, may have limited access to their machine's certificate root stores.  But the biggest problem is that while those listening to this podcast and other in-the-know techies might know enough to clean their root stores, most of the world would not.  So, yeah, even if some of us were to keep our machines clean, that doesn't help everyone else in the world.



ANT:  Right.



STEVE:  Mike asked:  "Hey, Steve.  What company do you recommend for a domain registrar?  I currently have all my domains with Google Domains, and they are moving to Square Space.  I only need a place to store the domains, as my name servers are with various other providers.  Thanks, Mike."



So without any hesitation I would and do always recommend Hover, and I cannot imagine why I would ever move.  I did move once, and that was away from Network Solutions.  They were the original primary registrar of domains for the Internet, but let's just say they did not age well.  I became so tired of Network Solutions' constant upselling attempts.  When I did anything there, I would be forced to decline one "special limited time offer" after another, endlessly, just to renew a domain.  I'm inherently loyal, so I did stick with them as long as I could.  But finally it was too much.



So I went looking for an alternative.  A good ultra-techie friend of mine, Mark Thompson at AnalogX, has all of his domains with GoDaddy.  But GoDaddy's style doesn't appeal to me either.  They just don't seem serious.  And the one thing you want in a domain registrar is seriousness.  They've also had security problems in the past with some of their services, though I don't think with their domain registrar business.



By comparison, Hover is just a clean and simple domain registrar.  They do offer some other services, but they are never pushed.  For a long while they were advertisers here on TWiT, but it was one of those situations where I had switched to Hover and was already singing their praises every chance I got long before they began advertising here.  And I still am, for the same reason, singing their praises.  So anyone who's looking for a clean and simple, no-frills, no annoying upselling, domain registrar I think will find that in Hover.  And I know that Leo feels the same way.



ANT:  As do I.



STEVE:  Cool.  And our last question from Glenn F.  He said:  "Hi, Steve.  I was just listening to SN-949 and wanted to let you know you may have been a bit overly charitable when describing Apple's motives around RCS.  Looks to me like Google got creative and used the EU as a cudgel to 'encourage' Apple to adopt RCS.  From what I can tell, Apple's RCS announcement appears to coincide with the deadline for their response to the EU.  Love the show and just recently joined Club TWIT due to all the great content.  Glenn."



ANT:  Woohoo.



STEVE:  Okay.  So, yeah.  Glenn's tweet linked to an interesting article at The Verge.  The Verge's headline reads:  "Google turns to regulators to make Apple open up iMessage."  And their tag line is:  "In addition to shaming Apple for not supporting RCS, the search giant has reportedly co-signed a letter arguing that iMessage should be designated a core platform service under the EU's Digital Markets Act."



Okay.  So I read the entire piece, and I agree with Glenn's assessment.  What Google really appears to want is to force Apple to open iMessage, since today's green bubbles are lame by comparison to the blue ones.  I have a text messaging group where one of its five members is an Android user.  As a consequence, the entire group is forced out of iMessage into SMS and thus reduced to this lowest common denominator due to the presence of this one individual in our group who is not an iPhone user.



So if Apple were to upgrade the rest of us iPhone people to RCS, then the green bubbles would be at parity with iMessage blue bubbles.  But as for opening up iMessage?  From a technical standpoint I cannot see how that's really possible due to the closed security ecosystem iMessage lives within.  I bet that's the last thing that Apple would consider doing.  But the addition of RCS does seem like a clever countermeasure designed to take the pressure off Apple in this regard.  And I think that Google should be happy with it.  I know I would be.



ANT:  Again, what more do they need to do?



STEVE:  Right, right.  Basically giving iMessage features to the Android user community and allowing that to cross over into the iOS ecosystem.  I think that sounds right.



ANT:  Yeah.



STEVE:  And I'll just wrap up by saying, reminding everyone the title of today's podcast is "Leo Turns 67."  And as I mentioned, last week's podcast was titled "Ethernet Turned 50."  So I decided to go with "Leo Turns 67" since that's happening tomorrow, on November 29th.  And even though he's currently sequestered in some far off cave somewhere with no Internet and no other technology, doubtless contemplating the nature of life, the universe, and everything, you might want to send him birthday wishes, which I'm sure he'll discover once he emerges and rejoins the rest of humanity, much as he'll be rejoining us this time next week.  And Ant, thank you for being my host this week.  Bye.



Copyright (c) 2023 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#951

DATE:		December 5, 2023

TITLE:		Revisiting Browser Trust

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-951.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  How can masked domain owners be unmasked?  What new and very useful feature has WhatsApp just added?  How did Iranian hackers compromise multiple U.S. water facilities across multiple states?  Did Montana successfully ban all use of TikTok statewide, and is that even possible?  How many Android devices are RCS-equipped?  What's the EU's Cyber Resilience Act, and is it good or bad?  Is ransomware finally beginning to lose steam?  What's the deal with all of these new top level DNS domains?  Do they make any sense?  Has CISA been listening to this podcast, or have they just been paying attention to the same things we have?  What's up with France's ban on all "foreign" messaging apps, and did the Prime Minister's nephew come up with an alternative?  And I want to share two final insights from independent industry veterans regarding the EU's proposal to forcibly require our browsers and operating systems to trust any certificates signed by their member countries.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  What happened when Montana tried to ban TikTok?  What's EU's Cyber Resilience Act?  And how good, how lucky are we to have CISA?  And then the way the EU is about to break browser security for everyone worldwide.  Steve explains it all next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 951, recorded Tuesday, December 5th, 2023:  Revisiting Browser Trust.



It's time for Security Now!, the show where we cover the latest news about security, what's going on in the Internet with this guy right here, who knows all, sees all, tells all:  Mr. Live Long and Prosper, Steve Gibson.  Hi, Steve.



STEVE GIBSON:  Yo, Leo.  Welcome back from your weeklong retreat.  We missed you last week.  Ant held down the fort.



LEO:  Thank you for the tribute you did.  That was very sweet.



STEVE:  Well, it's funny because no major topic occurred.  And when I looked at the previous week's episode, that was Ethernet Turned 50.  And I had received a notice from my phone saying, hey, you know, Leo's about to have an event.



LEO:  It was my birthday was the 29th, so yeah.



STEVE:  Yeah.  And so what was funny, too, was that my calendar didn't have your year of birth, so I didn't know which number this was.  So I went to Wikipedia.  And of course it knew.  And it said 66.



LEO:  It's wrong.



STEVE:  So I put all the - I know.  Well, no.  It said 66 on Monday or Tuesday.



LEO:  The day before, yes, right.



STEVE:  And so I forgot to add one.  So I produced the show notes, I had the title was Leo Turns 66.  Woohoo!.



LEO:  I wish.



STEVE:  And sent email out to everyone.  And I quickly got a note from John saying, "Steve, Leo's been 66 all year."



LEO:  A whole year.



STEVE:  I said, "Ooh, crap, that's right, I need to add one."  Anyway.



LEO:  I really appreciated [crosstalk].



STEVE:  So I quickly fixed everything.  And anyway, it was - we had a great show.  We had one piece that Ant was great for, was one of our listeners asked about my technical suggestions for how to block his third very most technical savvy of all three kids from any access to the Internet.  And my answer was...



LEO:  Fuggedaboutit.



STEVE:  Don't try.



LEO:  Yeah.



STEVE:  You know, this is a problem that requires parenting, not firewalls.  And anyway, so we had a good time talking about that.  I'm going to drag our listeners one final time, I hope it's final, through the issue of what the EU is planning to do because two notable industry people weighed in last week.  And actually the second of these two had shared some statistics about the distribution of certificate authorities which is sort of astonishing.  So today's title is "Revisiting Browser Trust."  I think everyone's going to find it interesting.  If not, by the time you get there, you'll be exhausted anyway.  So you could, just say, okay, you know, I don't want to hear Gibson talk about this anymore.  But there are a couple important new pieces of information I think are going to be useful.



But before we get to that, we're going to answer some questions.  How can masked domain owners be unmasked?  What new and very useful feature has WhatsApp just added?  How did Iranian hackers compromise multiple U.S. water facilities across multiple states recently?  How did Montana successfully - oh, I'm sorry, did Montana successfully ban all use of TikTok statewide?  And is that even possible?  How many Android devices are RCS-equipped now?  What's the EU's Cyber Resilience Act all about, and is it good or bad?  Is ransomware finally beginning to lose steam?  What's the deal with all these new top-level DNS domains?  Do they make any sense?  Has CISA been listening to this podcast, or have they just been paying attention to the same things we have?  What's up with France's ban on all "foreign," literally, in quotes, "foreign" messaging apps, and did the Prime Minister's nephew come up with an alternative?



And as I said, I want to share two final insights from independent industry veterans regarding the EU's, like, I mean, this is like all but happened at this point, signed behind closed doors, new legislation to forcibly require our browsers and operating systems to trust any certificates signed by their member countries.  So, and of course - I know.  It's just, I have a lot of commentary at the end about how I just, well, we'll get there.  So, and of course we have a pretty funny picture of the week.  So I think another great podcast for our listeners.



LEO:  I would expect nothing less.  Wow.  You know, I think I can see the next free GRC program as a program that rips those certificates out at the roots from your browser.



STEVE:  Yeah.



LEO:  Because, I mean, seriously, we're going to need that, Steve.  Boy, just unbelievable.  Let's get jiggy with our Picture of the Week.  What are you laughing at?  Can I show it?



STEVE:  I'm looking at the picture.



LEO:  Oh, you're waiting - okay.  Let me put the camera back on me, and I am going to show the Picture of the Week.  Should I show the world at the same time?



STEVE:  Oh, sure.  It's just so good.



LEO:  Okay, here we go [laughing].  Okay, that is funny.  Will you describe this, Steve?



STEVE:  So what Leo is laughing at, and I have to say when I looked at it again this morning when I was selecting from among the archive, I did, I burst out laughing because it's just so perfect.  It is a sign that says - and it's a real authentic sign that in a yellow rectangle says "Warning Low Flying Aircraft."  And it's got like then a diamond above it showing a picture, you know, sort of an iconic picture of an aircraft.  What makes it so funny...



LEO:  Yeah, this is obviously near an airport.  I mean, this, you know, you see this.



STEVE:  Yeah, and I was thinking that.  When, where would you encounter a sign that's warning you of low-flying aircraft?



LEO:  A small airport, small plane airport, you'd definitely see it.



STEVE:  Yeah.  Anyway, what's so funny about this is the sign has been knocked over.  So you can see it's broken off at the base, and it's lying on the grass because the presumption is...



LEO:  Low-flying aircraft.



STEVE:  ...the low-flying aircraft got it.



LEO:  Anyway, that's as low as you can go.  It's on the grass, yeah.



STEVE:  It is just - it's just so good.



LEO:  Wow.  That's hysterical.



STEVE:  So I just, you know, it makes me love humanity even more.  Last week we had one where there was a sign in front of an escalator that had some yellow, you know, warning tape across it.  And it said:  "This escalator is refusing to escalate."



LEO:  I love people with a sense of humor.  That's awesome.  That's great.



STEVE:  And actually someone tweeted me, and he said, okay, that's dumb.  An escalator is the one piece of equipment which is still useful even when it's broken.



LEO:  Well, that's a good point.



STEVE:  As a set of stairs.



LEO:  Yeah.  Not quite as good, but it still works.



STEVE:  And of course, you know, liability and all that.  You couldn't do that.  Anyway, okay, so one of the things that was always chafing while I was with Network Solutions, which, you know, I'm very loyal, I started with them in the beginning.



LEO:  They were your registrar.



STEVE:  Yes, GRC.com, on day one, my domain was at Network Solutions because they were the guys.  They were the Big Kahuna.  They may have been the only Kahuna back in those days.  But what really chafed was the idea of my - and this of course came along later - the idea of my needing to pay them additional money, annually even, to redact the domain registration listings they themselves had created for ICANN's Internet's WHOIS database queries.  The original idea behind domain registration was for it to be public.  Right?  I mean, this whole thing was we're all one big happy globe, worldwide network, and this is all going to be wonderful.  And so the people who register domain names should be public.



But it wasn't long before spammers and scammers were scraping the public domain registration WHOIS database for information and abusing it in every way imaginable.  So it soon became prudent to have that data masked, and masking services appeared.  Then the domain registrars themselves began offering this extra service, with many seeing the provision of this masking as of course another revenue opportunity.  So just one of the many reasons I'm so glad that I left Network Solutions and moved over to Hover, who has been, if they're not still, a sponsor of the TWiT Network.



LEO:  They're not, but they're good, and they still do free WHOIS privacy; right?



STEVE:  Exactly.  I went back to make sure that that was the case.  So, you know, that's the way it should be.



Okay.  Now, in the EU with GDPR, things are somewhat different.  Now, as we know, the GDPR has had its pluses and minuses.  One of the minuses we all now experience every day is the pervasive annoyance of every website being forced to wave its cookie policies in our faces and obtain our acknowledgement and consent.  On the flipside, one of the pluses is that the GDPR includes a stringent data protection law that has forced domain registrars to redact information on owners from their publicly available WHOIS databases.  This information is still present in the private databases of domain registrars, which, you know, they have to have that in order to maintain the domain.  And it has historically been made available to some organizations, but usually only in a very limited fashion, you know, like under court order, or responding to subpoenas, or following intelligence-sharing arrangements and agreements of some sort.



Okay.  I'm bringing this all up today because last Tuesday ICANN announced a new facility to improve the current situation for those, such as in law enforcement, who have a legitimate need to obtain access to otherwise redacted domain ownership information.  So with a bit of editing, here's what ICANN said.  They wrote:  "The Internet Corporation for Assigned Names and Numbers" - that's what ICANN stands for, I-C-A-N-N - has launched the Registration Data Request Service (RDRS).  The RDRS is a new service that introduces a more consistent and standardized format to handle requests for access to nonpublic registration data related to generic top-level domains (gTLDs).



"Personal data protection laws now require many ICANN-accredited registrars to redact the personal data from public records that was previously available in their WHOIS databases.  With no one way to request or access such data, it can be difficult for interested parties to get the information they need.  The RDRS helps by providing a simple and standardized process to make these types of requests.  The RDRS can be an important resource for ICANN-accredited registrars and those who have a legitimate interest in nonpublic data, like law enforcement, intellectual property professionals, consumer protection advocates, cybersecurity professionals, and government officials.



"The RDRS is a free, global, one-stop-shop ticketing system that handles nonpublic TLD registration data requests.  The RDRS connects requestors of nonpublic data with the relevant ICANN-accredited registrars for TLD domain names that are participating in the service.  The system will streamline and standardize the process for submitting and receiving requests through a single platform.  The service does not guarantee access to requested registration data.  All communication and data disclosure between the registrars and requestors takes place outside of the system.



"By utilizing a single platform and request form, RDRS provides a consistent and standardized format for handling nonpublic TLD registration data requests.  This simplifies the process for requestors by automatically identifying the correct registrar for a domain name and preventing the need to complete multiple forms with varying sets of required information managed by different registrars.  The service also provides a centralized platform where requestors can conveniently access pending and past requests.  They also have the ability to create new requests, develop request templates, and cancel requests when needed."



Finally, "Registrars can benefit from using the service as it provides a mechanism to manage and track all nonpublic data requests in a single location.  Registrars can receive automated alerts anytime a request is submitted to them.  The use of a standardized request form also makes it easier for the correct information and supporting documents to be provided to evaluate a request."



So to me, this seems like it's been a long time coming, and it makes so much sense.  You know, there are, today, there are so many shenanigans going on with Internet domain names that abusers of the system need to know that their ability to hide is being reduced.  And legitimate domain owners should have a reasonable expectation of privacy.  So the idea of having, you know, the WHOIS databases not all public, yet still creating a uniform, less hassle-full means of obtaining that nonpublic data across registrars who all have their own ways of doing things to me, you know, the idea of standardizing this process for obtaining the information seems like makes a lot of sense and seems like a long-missing piece that's finally being provided.  So, you know, props to ICANN for this, you know, yay.  I think that just - that works.



Due to the strength of Facebook, Meta's WhatsApp, as we know, is the world's number one most used, most popular messaging app.  And last Thursday WhatsApp announced a significant new feature which was missing when they announced something earlier in May known as "Chat Lock" last May.  So, okay, first, here's what they announced on May 15th under the headline "Chat Lock:  Making your most intimate conversations even more private."  They said:  "Our passion is to find new ways to help keep your messages private and secure.  Today we're excited to bring to you a new feature we're calling Chat Lock, which lets you protect your most intimate conversations behind one more layer of security."  Now, right off, I think that sounds like a great idea.  And we'll look at why they think so.



They said:  "Locking a chat takes that thread out of the inbox and puts it behind its own folder that can only be accessed with your device password or biometric, like a fingerprint. It also automatically hides the contents of that chat in notifications, too.  We think this feature will be great for people who have reason to share their phones from time to time with a family member, or those moments where someone else is holding your phone at the exact moment an extra special chat arrives.  You can lock a chat by tapping the name of a one-to-one or group and selecting the lock option.  To reveal these chats, slowly pull down on your inbox and enter your phone password or biometric.  Over the next few months we're going to be adding more options for Chat Lock, including locking for companion devices and creating a custom password for your chats so that you can use a unique password different from the one you use for your phone."



And it is that last feature that I had on my mind the whole time I was reading the foregoing.  It was like, well, that's nice that you're going to move this out of the inbox, and you're going to give it its own place to live.  But then you're going to allow the same phone password or biometric to unlock it.  That's not optimal to me, or I should say maximal.  And for this, the whole point is to obtain something maximal.



So what they announced last week was, they said:  "Earlier this year we rolled out Chat Lock to help people protect their more sensitive conversations.  Today we're launching Secret Code, an additional way to protect those chats and make them harder to find if someone has access to your phone or you share your phone with someone else.  With Secret Code you'll now be able to set a unique password different from what you use to unlock your phone to give your locked chats an extra layer of privacy. You'll have the option to hide the Locked Chats folder from your chat list so that they can only be discovered by typing your secret code in the search bar."  They've just done this whole thing exactly right.



"If that doesn't suit your needs, you can still choose to have them appear in your chat list.  Whenever there's a new chat which you want to lock, you can now long press to lock it rather than visiting the chat's settings.  We're so happy our community has been loving Chat Lock, and hope that Secret Code makes it even more useful to them.  Secret Code starts rolling out today, and in the coming months will be available globally.  We're excited to keep bringing more function to Chat Lock to help people protect their privacy.  Let us know what you think."



Anyway, as I've been saying, I think it makes total sense, and I predict that it will become a heavily used feature.  From a privacy and security standpoint, it makes sense for our devices to have multiple layers and levels of protection.  We need to have more than just a device being locked or unlocked.  That's no longer sufficient, or at least certainly not for all possible use cases, and that's what this is allowing WhatsApp to be extended to.  And I don't think that using the same password or biometric, as I said, makes sense for an "inner" level of protection.  Locking enhanced layers of privacy behind "something you know" makes I think the most sense.  So bravo to Meta for doing this.



Okay.  I said recently that one of the broad changes to the way we've always done things must somehow be the elimination of any initial default password from our devices.  My first thought was to require the user to set a password themselves, while preventing them from setting it to "password" or "Monkey123," by also embedding some minimal complexity requirements.  But I don't think that's the right solution.  I think the right answer is to have the device randomly assign a strong password when it's initially set up, and that's it.  The user needs to write it down.  Period.  We've been talking for years about the need to be using strong passwords that we cannot recall.  That needs to apply to equipment as well as websites.



So here's the news that brought me back to this train of thought.  Get a load of this:  The U.S. government has confirmed that an Iranian hacking group named Cyber Av3ngers, where the A-V-E of Avengers is actually a numeral 3.  So, you know...



LEO:  Okay.  We know they're 12-year-old boys now.  Go ahead, yeah.



STEVE:  Yes, Cyber Av3ngers; right.



LEO:  Okay.



STEVE:  And actually, Leo, you're right because a 12-year-old could do this.  They gained access to the equipment at water facilities across multiple U.S. states.  CISA, the FBI, the NSA, and other agencies say the attacks began around November 22nd.



LEO:  Jesus.



STEVE:  I know, and exploited PLCs, you know, Programmable Logic Controllers that we've spoken of many times in the past,  manufactured by the Israeli company Unitronics.  The group targeted Unitronics PLCs that were still using the default password "1111."



LEO:  Oh.  My.  Good.  Ness.  That's absurd.  And that's a water supply.



STEVE:  I know.



LEO:  Oh, god.



STEVE:  It's un-frigging-believable.  So last week CISA asked U.S. organizations to please change the default password, enable multifactor authentication, and remove the devices from the Internet.  Gee, what a concept.  U.S. officials say the Cyber Av3ngers group is affiliated with the IRGC, an Iranian military and intelligence organization.  Maybe they're their kids.  According to the Shadowserver Foundation, between 500 and 800 Unitronics PLCs are currently exposed to the Internet.  And let me just say that you don't, you almost, you probably have absolutely no actual business purpose for connecting a PLC to the Internet.  You know, it runs equipment in factories.  Well, and obviously water systems.  But who needs to hook it to the Internet?



Okay.  66 were identified in Australia, 52 in Singapore, 42 in Switzerland.  Those are the top three:  Australia, Singapore, Switzerland.  37 are known to be in the United States, and apparently they've all been hacked, and they're all controlling our water supply.  Then following up is Estonia and Spain, both with 31, and then it continues to dwindle down the list.  Like pretty much everybody has one, you know, every country.



Unlike web servers, PLC systems, as I said, typically have no need to be exposed to the Internet.  Doing so, if you like actually had to do that, it should require jumping through some real hoops.  Under no circumstances should a device be produced where it both has a well-known default password, all set to 1111 in the factory, and is also exposing any interface protected by that default password to the Internet.  In today's world, designing and selling such systems is really nothing short of irresponsible.



We've talked in the past about countries becoming proactive in scanning their own Internet address space with an eye toward getting ahead of attackers and cleaning up some of these issues.  This is the sort of thing that CISA in the U.S. ought to be considering because in CISA, I'm increasingly impressed by them, we finally have a proactively useful cybersecurity entity.



LEO:  Yeah, yeah.  Chris Krebs was great when he ran it.  He's a smart guy.



STEVE:  Yeah.  They're on the ball.



LEO:  Yeah.  I'm not surprised.  So I think Retcon5 in our chatroom said it's simple, you just change it to 2222.  And next year, 3333.



STEVE:  That's good.  I mean, at least it would - at least the kiddies...



LEO:  Rotating.



STEVE:  The script kiddies would not be able to get to you.  My god.  Unbelievable.



LEO:  Oh, my god.



STEVE:  Okay.  So a while back we covered the news that a bunch of states were enacting legislation to block the use of TikTok on government devices within their jurisdictions.  Doing that was likely within their power.  But the state of Montana wanted to go further and - get this, Leo - outright ban all use of the TikTok service statewide.  Okay, now, from a purely technical standpoint this would be somewhat tricky, since network boundaries and state borders are not currently aligned since there's never been any need to align them.  But now it appears that it might not matter about that after a recent federal ruling which occurred just last Thursday.  NPR's coverage of this also provides some interesting background.  So I want to share it.



They wrote:  "A federal judge has blocked a law in Montana that sought to ban TikTok across the state, delivering a blow to an unprecedented attempt to completely restrict a single app within a state's borders.  The ruling, which came on Thursday, means that Montana's TikTok ban, which was set to go into effect on January 1st, has now been temporarily halted.  U.S. District Judge Donald Molloy said Montana's TikTok ban 'oversteps state power' and 'likely violates the First Amendment.'



"Molloy wrote that though officials in Montana have defended the law as an attempt to protect consumers in the state, there is 'little doubt that Montana's legislature and Attorney General were more interested in targeting China's ostensible role in TikTok than with protecting Montana consumers.'  Montana as a state does not have authority over foreign affairs, Molloy said, but even still, he found the national security case presented against TikTok unconvincing, writing that, if anything, the Montana law had a 'pervasive undertone of anti-Chinese sentiment.'



"The ruling is preliminary with a final determination to be made following a trial expected sometime next year.  TikTok, which has more than 150 million American users, has for years been under intense scrutiny over fears that its Beijing-based parent company, ByteDance, would hand over sensitive user data to Chinese authorities, or that Beijing would use the app as a propaganda tool, even though there is no public proof that either has ever happened.



"Although several states and federal government have prohibited the app from being downloaded on government devices, Montana was the first state to pass an outright ban of the app.  Some critics have accused it of government overreach.  In May, TikTok sued the state over the law, arguing that it amounts to an illegal suppression of free speech.  Lawyers for TikTok argued that the national security threat raised by officials in Montana was never supported by any evidence.  Molloy, the judge overseeing the case, was skeptical of the ban in an October hearing on the lawsuit.  He pointed out that TikTok users voluntarily provide their personal data, despite state officials suggesting the app was stealing the data of users.  He said state officials justified the Montana ban under a 'paternalistic argument.'



"As Washington continues to debate TikTok's future, states have been acting faster, and the law in Montana was considered an important test case of whether a state-level ban of an app would survive court challenges.  Backing the Montana law were 18 primarily Republican-led states that were eyeing similar bans of TikTok.  Aside from legal hurdles to implementing such laws, cybersecurity experts have raised questions of how, from a technical standpoint, such a ban would even be possible."  Right.  Count me in that group.  Those pesky technical details which keep tripping up the legislators who believe that they can simply have any magical technology that they demand.



And then, anyway, NPR concludes:  "President Trump clamped down on TikTok and attempted to outlaw the app, but his efforts were twice struck down in the courts.  National security experts say TikTok is caught in the middle of escalating geopolitical tensions between the U.S. and China, as Washington grows ever more concerned about the advancement of Chinese tech, like semiconductors, and the country's investments in artificial intelligence.



"Supporters of restricting or banning TikTok in the U.S. point to Chinese national security laws that compel private companies to turn information over to Beijing authorities.  They also point to ByteDance, TikTok's corporate owner.  It admitted in December that it had fired four employees, two of whom worked in China, who had improperly accessed data on two journalists in an attempt to identify a company employee who leaked a damaging internal report."



Now, I'll just say that by no means am I defending TikTok.  But let's not forget that many domestic companies, as well as many of our own U.S. law enforcement agents - we've covered these issues in the past - have also been caught with their hands in the cookie jar.  It appears that access to personal and private data is quite tempting.  So it's not just Chinese misbehavior.



Oh, and finally, this is significant, too.  TikTok says China-based employees no longer have access to U.S. user data under a new firewall it has put in place with the help of Oracle.  With this change, dubbed "Project Texas" after Oracle moved its corporate headquarters to Austin, all Americans' data will be stored on servers owned and maintained by Oracle, with additional oversight from independent auditors.  So it seems clear that TikTok is obviously, well, we know that TikTok is obviously an extremely successful and valuable service.  It seems to me that they're making every effort to legitimately assuage concerns of secret Chinese influence.  And of course today's social media is all about influence.  But such influence is as pervasive over with Facebook and X as it is anywhere else.



LEO:  That's the problem.  It's very selective enforcement.  I mean, it's all crap.  How do you pick one out?  You know, it's all propaganda.  It's all lies.  And as you and I know very well, if the Chinese government wants information about U.S. citizens, they just go to a data broker.  It's cheap.  So...



STEVE:  Right, right.  And in fact there are some, oh, it's Senator Ron Wyden is in the...



LEO:  Wyden's smart.  He's good.



STEVE:  Yes.  He's smart.  He's threatened to stop the appointment of somebody, I don't remember whom, until the NSA answers questions...



LEO:  Oh, yeah.



STEVE:  ...about whether it's been purchasing this private data about U.S. citizens.



LEO:  They've been really hedging their responses to that question, which tells me of course they are.



STEVE:  Right.  Otherwise they'd just say no.



LEO:  And by the way, that's why Congress will never pass a law against data brokers.  Because law enforcement and our three-letter agencies are saying you can't do that, we need that information.



STEVE:  Yeah, yeah.



LEO:  Sad.



STEVE:  Okay.  So just a quick note.  RCS is now enabled on more than one billion Android devices.  We recently noted Apple's announcement that they would be upgrading their non-iMessage messaging, which is currently using SMS and MMS to RCS.  So it was noteworthy that last Thursday Google announced that its RCS messaging system is now enabled on more than one billion Android devices.  So it appears that Android users will be ready once Apple joins them with RCS next year.  And as I said, I will be quite happy to have a better messaging experience when any members of my little otherwise iOS group who have an Android force the whole group down to SMS.  You know, be really nice to have the RCS features.  Which looks like it's pretty much at parity, largely, with iMessage.



LEO:  Well, and this will cross your desk later today, and I'm sure you'll want to talk about it next week, but Beeper has just announced this week, maybe you heard us talking about it on MacBreak Weekly, this new program Beeper Mini, that basically they jailbreak an iPhone and reverse engineered the protocols to use to log into the iPhone servers.  So you are able now on an Android device to use Messages legally and be a blue bubble and all of that.



STEVE:  Now, what do you mean "legally"?



LEO:  There is a carve-out in the DMCA apparently, I didn't know this, Jason Snell explained it, that allows you to reverse engineer this particular kind of thing.  So it wasn't illegal to reverse engineer it.  Furthermore, and this is what I wonder, and I would love to hear your thoughts on, it is speculated that Apple can't stop this without breaking their own authentication servers.  So they're kind of over a barrel because if they attempt somehow to prevent this login - now, I don't know that - I wouldn't be surprised if Apple had some secret way of doing this.



STEVE:  Yeah, I would think you'd have to have like a - you'd have to have a pseudo-iPhone in order to connect to their servers; right?  I mean, like, yeah.



LEO:  I don't know.  It works.



STEVE:  You're right.



LEO:  It's two bucks a month.



STEVE:  I'm curious.



LEO:  What we said is I wouldn't subscribe for a year.



STEVE:  And did it just happen?



LEO:  This morning.  Beeper Mini.



STEVE:  Wow.



LEO:  It goes on Androids, gives you full parity.  It's basically using iMessage on an Android.  Logging in with an Apple account.  And it's open source, and they, well, at least the Python part is.



STEVE:  Oh, logging in with an Apple account.



LEO:  Yeah, yeah, yeah.  Yeah, you're using your Apple account and going through the servers.  The question is, I've got to think Apple has some sort of fingerprint.



STEVE:  How can they not know it's not an iPhone?



LEO:  Right.



STEVE:  Yeah.



LEO:  But I don't know.



STEVE:  Maybe they just never needed to worry about it because they figured no one can break in.  It's proprietary protocol.



LEO:  Right.  And then furthermore, there's no man in the middle.  It is effectively you are using this software like you would use Messages on an iPhone.  It's then encrypted, direct to, you know, I mean, it's very interesting.  And I, you know, Apple may or may not have a technical ability to defeat it.  But if they do, then there's the secondary question of would they, given that it would certainly raise the ire of regulators all over the place because that is certainly an anti-competitive move, to say no, no, you can't...



STEVE:  Especially if there's a carve-out that says it's possible to reverse engineer the protocol.



LEO:  Exactly.  So if it's legal, even if Apple could block it, would they is the question.  It's a fascinating subject.  You'll be talking about this, I'm sure.



STEVE:  Yeah.  So I guess their concern would be that having a non-iPhone endpoint running iMessage protocol is inherently insecure.



LEO:  That's right.  So that will be, if they do break it, they will say, no, no, we're protecting your security because Android devices are inherently insecure and should not be allowed on our network in this way.  That's probably not the case, but that would be their - I guarantee you that'll be the verbiage.  Oh, no, we're just protecting the network.  It's the same verbiage AT&T used in Carterfone.  They said they can't put any non-AT&T devices on our phone network.  That would break it.



STEVE:  Boy, I remember those days.  Wow.



LEO:  Yeah.  You had to rent a phone from Ma Bell.  And of course the Carterfone decision, the Supreme Court decision overturned that and changed the world.



STEVE:  Yeah.



LEO:  As always in this business we're in interesting times.



STEVE:  It also did lower the quality of telephones.



LEO:  Well, that's true.  It did, didn't it.



STEVE:  Remember those old AT&T sets, you can run over them with a truck.



LEO:  The Western Electric, yeah, they were made like of hard rubber.  They were tough.



STEVE:  Yeah.  I think it was Bakelite.



LEO:  I think it might have been Bakelite in the early days, for sure.



STEVE:  Yeah, and then a steel base plate, I mean, they were really built.  It was the 501 was the model number, that classic phone.



LEO:  Right.  And the mic pickups were carbon.



STEVE:  Yes, yes.



LEO:  And sometimes they get clumpy, so you bang it.



STEVE:  Yup.



LEO:  We sound like two old men.



STEVE:  Yes, children, back in the day.



LEO:  You would bang, if it started to sound bad, you'd bang your phone, and it would fix it.  Dad, you're making that up.  No, it's true.



STEVE:  Break loose the carbon granules.



LEO:  Right.



STEVE:  Oh, god.  Okay.  So much as I'm becoming, as we're going to hear later, increasingly annoyed with the EU over their move to commandeer our web browsers' well-established system of trust, it appears that the EU's European Council and Parliament have reached a useful agreement known as the Cyber Resilience Act.  This is a piece of legislation designed to improve the security of smart devices sold within the European Union.  The new regulation, it will take three years to come into effect; but god bless 'em, this is a good thing.  This new regulation applies to products ranging from baby monitors and smartwatches to firewalls and routers.



Under the new rules, vendors must establish processes to receive reports about vulnerabilities, and must support products for at least five years.  Moreover, products will be required to come with free and automatic security updates as the default option.  They must ensure confidentiality using encryption, and vendors must inform authorities of any attacks.



This won't be happening immediately, as I said.  The requirement set by the new rules will come into effect three years after the Cyber Resilience Act is formally voted on on the EU Parliament floor.  Given the requirements, which will likely require some redesign and new infrastructure to support them, that seems reasonable to me.  At least in this regard, the EU is finally leading in the direction we need to be heading, and not backwards, as they are unfortunately with browser security.  But so this is really cool.  This says that connected consumer devices, three years from the time they sign this into law, which is imminent, will be required to auto-update by default.  Which means all of the consumers who have heretofore not been protected and are running routers and firewalls and everything else, baby monitors, with extremely obsolete firmware, will get five years of support, including automatic updates to them.  So, yay.



The forensics industry is getting better at tracking cryptocurrency flows, and cyber insurance firms are being more forthcoming about what they're seeing.  So we know more now than we have previously.  For example, one of the newer upper echelon ransomware groups we've referred to before is known as Black Basta.  This gang is believed to have netted more than $107 million in ransom payments since it first appeared and began operations early last year.  Those who watched this have believed that it was, like, emerged from the ashes of Conti after Conti shut down.



Okay.  Since we're closing out 2023, and it emerged for the first time early last year, that's $107 million in less than two years' time.  That $107 million represents payments made by more than 90, nine zero, victims of the, get this, 329 organizations known to have been hit by the gang.  Okay.  So there's 365 and a quarter days in the year.  Yet in less than two years, 329 individual organizations were breached by this group.  So on average that's about one every other day, a breach every other day.  The largest payment observed was $9 million, while the average ransom payment is $1.2 million times 90.  So this is according to joint research published by the blockchain tracking company Elliptic, and the cyber insurance provider Corvus Insurance.



Now, unfortunately, what this shows is that there is a great deal of money to be made through cyber extortion, which is really what this all boils down to.  And the hostile governments - or in this case government, since this group is known to be operating out of Russia - harboring these criminals are more than happy to turn a blind eye.  This means that a great deal of pressure will continue to be placed on the security of our networks and systems.



And unfortunately, as the last few months of many very serious large weaknesses and compromises have continued to show, our networks and systems are not up to the challenge.  Years of laxity in the design, operation, configuration, and administration of these systems is catching up with us.  We know that thanks to the inherent inertia which works against change, we're not going to fix these endemic problems all at once.  But they're never going to get fixed at all if we don't apply constant effort in that direction.  And I have some good news to that effect here in a minute.



I did want to mention that Google is offering a new ".meme" top-level domain for anyone who wants to play with meme-related Internet properties, and to observe that it's difficult to keep up with all of the new TLDs which are appearing.  And it does feel as though this aspect of the Internet's original design, which is to say the original concept of a hierarchy of DNS domains, anchored by just a few major classifications, is not evolving that well.  There are companies that attempt to snatch up their existing dotcom second level domain names in every one of the other top level domains, presumably to preserve their brand and their trademark.  But that's certainly not in keeping with the spirit of creating additional DNS hierarchies for future growth.  You know, I have no interest in grc.meme, and grc.zip would have caused all kinds of confusion.  You know, what is that?  GRC's entire website in a ZIP archive?  Who knows?



LEO:  I think grc.meme might be kind of fun.



STEVE:  Well, while you were telling us about canary.tools I thought, well, there's a perfect example...



LEO:  Yeah, there's a good use, yeah.



STEVE:  ...of a good use of one of the newer TLDs.  I'm sure that canary.com was taken, you know, decades ago, so that wasn't available.



LEO:  I use a dot email domain, TLD, for my email.



STEVE:  Yeah.



LEO:  I have a variety of domains that I use for email that are nontraditional TLDs.



STEVE:  Right.



LEO:  You can't get a good dotcom anymore.  They're gone.



STEVE:  It's true.



LEO:  Yeah.



STEVE:  It's true.  Okay.  Speaking of CISA, last Wednesday they  introduced a new series of publications called - oh, be still my heart - "Secure by Design" with its first alert titled "How Software Manufacturers Can Shield Web Management Interfaces From Malicious Cyber Activity."  And if I didn't know - as I do - that anyone who's focused on security would naturally come up with the same thoughts, I would think that they'd been listening to this podcast.  Get a load of what's in this first document.  And it's short.



So CISA writes:  "Malicious cyber actors continue to find and exploit vulnerabilities in web management interfaces."  Newsflash, right.  "In response, software manufacturers continue to ask why customers did not harden their products to avoid such incidents."  Like, what do you mean, you left the default set to 1111?  That's crazy.  Uh-huh.



LEO:  Mm-hmm.  Who would do that?



STEVE:  CISA says:  "'Secure by design' means that software manufacturers build their products in a way that reasonably protects against malicious cyber actors successfully exploiting vulnerabilities in their products.  Baking in this risk mitigation, in turn, reduces the burden of cybersecurity on customers.  Exploitation of vulnerabilities in web management interfaces continues to cause significant harm to organizations around the world, but can be avoided at scale.  CISA urges software manufacturers to learn from ongoing malicious cyber activity against web management interfaces by reviewing the principles below."  And again, they're quoting this podcast recently. 



"Principle 1:  Take Ownership of Customer Security Outcomes."  And actually there's only one principle, that one.  "Take Ownership of Customer Security Outcomes."  They said:  "This principle focuses on key areas where software manufacturers should invest in security:  application hardening, application features, and default settings.  When designing these areas, software manufacturers should examine the default settings of their products.  For instance, if it is a known best practice to shield a system from the public Internet, do not rely on customers to do so."  Again, oh, thank you, thank you.



LEO:  Yeah, because we know customers aren't going to do it.



STEVE:  They're not.  Apparently they've plugged it in and left it set to 1111.



LEO:  Exactly.



STEVE:  God.  They said:  "Rather, have the product itself enforce security best practices."



LEO:  Yes, yes.



STEVE:  Examples include, and we have three bullet points:  "Disabling the product's web interface by default" - oh, yes, thank god this is in print - "and including a 'loosening guide' that lists the risks, in both technical and non-technical language" - right, make it very simple for Johnny - "that come with making changes to the default configurations.  Two, configuring the product so that it does not operate while in a" - oh, look - "does not operate while in a vulnerable state, such as when the product is directly exposed to the Internet.  Third, warning the administrator that changing the default behavior may introduce significant risk to the organization."  Okay, now, look.  Think about that.  This is a complete reconception of the way everything is done today.  You know, yay for CISA.



LEO:  I love this principle.  Take Ownership of Customer Security Outcomes.  That's exactly right.  You know, don't let - it's your job.  It's not their job.



STEVE:  Right.  Right.  Which is a complete turnabout.



LEO:  Yes.



STEVE:  They said:  "Additionally, software manufacturers should conduct field tests to understand how their customers deploy products in their unique environments and whether customers are deploying products in unsafe ways.  This practice will help bridge the gap between developer expectations and actual customer usage of the product.  Field tests will help identify ways to build the product so customers will securely use it."  And finally:  "Furthermore, software manufacturers should consistently enforce authentication throughout their product, especially on critical interfaces such as administrator portals."



So, wow.  Amen to all of that.  Now, as I said, none of these concepts will come as news to the listeners of this podcast, but it would be great if those manufacturers to whom CISA is addressing this alert would immediately take heed.  We know it's going to take time for any such change to work their way through the entire supply chain, from drawing board into final deployment.  It would have been nice if we could have started that "Secure by Design" process 10 years ago, but we haven't even fully started it today.  The fact that this alert has been published, with what it says, is a very good sign.  I suspect that this may be the first step toward beginning to hold the designers of these systems accountable for their default security.



Unfortunately, due to the "hold harmless" nature of software and equipment licensing agreements, accountability, as we've discussed before, is difficult to create.  I intensely dislike the idea of having government criminalize insecure design.  That's a slippery slope that's not far from what the EU is planning to do with their eIDAS 2.0 web certificate overreach.  Legislation and technology rarely make great bedfellows.  But one of the ways we've seen government influence things for the better is by using its own purchasing power to create voluntary incentives.



LEO:  Mm-hmm.



STEVE:  With CISA, the U.S. government finally has a highly effective and worthwhile cybersecurity agency.  Based upon what CISA just published last Wednesday, that thing I just read, it would not be a stretch to imagine adding exactly those default network behavioral requirements to any future software and equipment purchasing made by state and federal government agencies.  That would affect voluntary change overnight.  Vendors would be required to legally attest that their equipment abides by this new set of requirements; and if it was later found not to be true, then they could be held liable for damages resulting from the functional out-of-spec behavior of their equipment.  And just to be clear, not for bugs in their systems, but for the deliberate design of those systems.  As I've repeatedly observed, anybody can make a mistake, but vendors can and should be held responsible for their policies.  And design is a policy.  So yay to CISA.



Okay.  Last one before we take our break and then get into our topic.  This one is really interesting.  And it led me down a path I didn't expect.  While we're on the subject of things governments do, also last Wednesday, France's government announced a near immediate ban - as in 10 days from last Wednesday - on the use of what they called "foreign end-to-end encrypted messaging apps."



LEO:  That's so French.  That's France.  That's France for you.



STEVE:  So they've banned government officials from using foreign encrypted messaging services including specifically Telegram, Signal, and WhatsApp.  Uh-huh.  The government is notifying its ministers and their cabinet staff...



LEO:  What do they have?  What have they got?



STEVE:  ...that they must uninstall any such applications from their devices by this coming Friday, December 8th.  French officials have been told to use the French-developed alternative messenger known as Olvid, O-L-V-I-D.  Uh-huh.



LEO:  Is no good.  If it's not French, it's no good.  Must be French.



STEVE:  Officials cited privacy risks and a need to "advance towards greater French technological sovereignty."



LEO:  Mais oui.



STEVE:  Okay.  So what the heck is Olvid?  Even though we've never talked about it here, I have to say that it looks pretty good.



LEO:  Good.



STEVE:  It's both open source, well, it is open source for both Android, iOS, macOS and Windows.



LEO:  Okay.



STEVE:  And it's living over on GitHub.



LEO:  Oh, well, that's fine.



STEVE:  Here's how it describes itself:  "Olvid is a private and secure end-to-end encrypted messenger.  Contrary to most other messaging applications, Olvid does not rely on a central directory to connect users.  And there is no user directory.  Olvid does not require access to your contacts and can function without any personal information.  The absence of directory also prevents unsolicited messages and spam.  Because of this, from a security standpoint, Olvid is not 'yet another secure messenger.'"



LEO:  It's la French Tech.  Wow.



STEVE:  "Olvid guarantees the total and definitive confidentiality of exchanges, relying solely on the mutual trust of interlocutors.  This implies that your privacy does not depend on the integrity of some server."



LEO:  Okay.



STEVE:  "This makes Olvid very different from other messengers that typically rely on some trusted third party, like a centralized database of users or a public blockchain.  Note that this doesn't mean that Olvid uses no servers.  It does.  It means that you do not have to trust them.  Your privacy is ensured by cryptographic protocols running on the client-side,  on your device.  And these protocols assume that the servers were compromised from day one.  Even then, your privacy is ensured."



Okay.  So this is less looney than it might seem at first, though it does have some feeling of nationalism and protectionism with the French government labeling everything else "foreign" and talking about the need to increase France's technological sovereignty.  But that said, Olvid is not some random homegrown messaging app designed by the Prime Minister's nephew.



LEO:  That's exactly what it sounds like; right?  No, this is safe.  My nephew said so.



STEVE:  So I've not had time to look at it closely, but it looks like the real deal.  And the more I look at it, the more I like it.  Over on Olvid's website, which is olvid.io, they proudly note that:  "Olvid does not require any personal data:  no phone number, no email, no name, no surname, no address, no date of birth.  No nothing."



LEO:  Yeah, that's nice.  That's one of the things that bugs me about Signal.  I don't like that.



STEVE:  Yes.  I completely agree.  "Unlike your previous messenger, Olvid will never request access to your address book."  Okay, so those are some compelling features.  And under the headline "Compatible with what you already have" they say: "Olvid is available for your macOS and Windows computers, as well as your iPhones, iPads, Android smartphones and tablets.  No SIM?  No problem.  No SIM card required.  WiFi is all you need.  Since Olvid needs no phone number to work, you can use any of your devices.  And they'll stay in sync.  Olvid even works in an emulator.  Geeks will love it."



Okay.  So Olvid uses something known as SAS-based authentication.  Of course, Leo, you'd expect them to be sassy, them being French.



LEO:  They are French.



STEVE:  SAS stands for "Short Authenticated Strings."  The concept of SAS was produced and formalized in a 311-page PhD thesis by, of course, a French cryptographer, Sylvain Pasini, back in 2009.  So here's what Pasini explained in the first two paragraphs of his PhD thesis.  He said:



"Our main motivation is to design more user-friendly security protocols.  Indeed, if the use of the protocol is tedious, most users will not behave correctly; and, consequently, security issues occur.  As an example..."



LEO:  You are not behaving correctly.  You must go back to the beginning.



STEVE:  "An example is the actual behavior of a user in front of an SSH certificate validation.  While this task is of utmost importance, about 99% of SSH users accept the received certificate without checking it.  Designing more user-friendly protocols may be difficult since the security should not decrease at the same time.  Interestingly, insecure channels coexist with channels ensuring authentication.  In practice, these latters may be used for a string comparison or a string copy, for example, by voice-over-IP spelling.  The shorter the authenticated string is, the less human interaction the protocol requires, and the more user-friendly the protocol is.  This leads to the notion of SAS-based cryptography, where SAS stands for Short Authenticated String."



Finally:  "In the first part of this thesis, we analyze and propose optimal SAS-based message authentication protocols.  By using these protocols, we show how to construct optimal SAS-based authenticated key agreements.  Such a protocol enables any group of users to agree on a shared secret key.  SAS-based cryptography requires no pre-shared key, no trusted third party, and no public-key infrastructure. However,  it requires the user to exchange a short SAS, for example, just five decimal digits. By using the just agreed secret key, the group can now achieve a secure communication based on symmetric cryptography."  And yes, Leo, five digits is all it takes.



"Since 2009 this SAS proposal first outlined by this guy's PhD, has received a great deal of further scrutiny within the security community, and it has held up 100%.  So this works by having the users at each end initially discover each other by sharing the short tokens being displayed on each other's devices."



LEO:  Ah.  That's a little hardship because I'd have to tell you what that pre-shared key is somehow.



STEVE:  Yes.



LEO:  Over a secondary channel; right?



STEVE:  Yes, yes.  I think that's exactly the case.  So for that some form of already authenticated out-of-band channel is used, like an audio or a video call, to exchange the information that each user's device presents.  And this simple process has been proven, as I said, to be cryptographically sound.  But notice also this eliminates spam completely.



LEO:  Right.  Good.



STEVE:  It's over.



LEO:  Yeah.



STEVE:  So I also really like the fact that its integration with the desktop.  That's something I've been missing, you know, as a cross-platform iOS and Windows user.



LEO:  Right.



STEVE:  And, as you said, Leo, Signal is annoying with its required tie to a phone number.  Signal claims that's needed to prevent spam, but with Olvid there's no possibility of being spammed.



LEO:  Yeah.  People know my phone number.  That does not prevent spam at all.



STEVE:  Right, right.  And we also, as we covered - I think it was you, maybe it was Ant - when we talked about the breakdown of Signals' cost structure.



LEO:  Yeah, yeah, I was here, yeah.



STEVE:  It was you before you left.



LEO:  Yeah.



STEVE:  That telephone authentication is a huge percentage of Signals' total annual outlay because verifying those phone numbers is very expensive.  So a real-world out-of-band interaction is required to establish a channel between two participants or among the participants in a group.  After that, the devices remain linked for further communication.



Okay.  So what pays for this?  The system runs on a "freemium" model.  All bidirectional text messaging and incoming audio, encrypted audio, is free.  You get unlimited messages, unlimited attachments, secure group discussions, unsend and edit messages, remote deletion, ephemeral messages, multiple profiles, user mention, markdown, Olvid Web, whatever that is, and inbound secure audio calls.  The system is financially supported at a 5-euros-per-month level.



LEO:  That's steep.



STEVE:  Yeah, by those who want to be able to initiate secure voice calls, as well as use Olvid on multiple devices that receive all messages and keep themselves cross-synchronized.  So, yeah.  So it's only free if you limit yourself to text messages.  But for there, it really is.  There are also more powerful enterprise plans that have much more features.  It's interesting that the French government is telling their ministers and cabinet staff that they must switch to Olvid.  Since only text messaging is completely free, so one wonders who's going to pay for that.



LEO:  They bill it to the French public.  So congratulations.



STEVE:  Well, there is an enterprise classification.  So presumably France government would act as an enterprise and then would make all of their individual government employees subscribers underneath that one umbrella policy.



LEO:  If I'm Mr. Olvid, I'm going to give it to them free.  This is the best publicity you could ask for.



STEVE:  I know.



LEO:  Right?  I mean...



STEVE:  Yes.



LEO:  Yeah.



STEVE:  Yes.  Anyway, so I wanted to make sure that all of our listeners were aware that it existed.



LEO:  I'm installing it right now.



STEVE:  It might suit many peoples' needs, yes.  Cross-platform, desktop, for Mac and Windows.  I'm sure it runs under WINE, and there's probably a way to get it running under Linux easily.



LEO:  Well, if there's a web version, which there is, you just do it in the web.  And that's, yeah, that's straightforward.



STEVE:  Oh, okay.  Olvid.io.



LEO:  Yeah.  Yeah, I'm installing it right now.  Looks good.  The one negative is that they give you a backup key because it will do encrypted backups.  But you can't cut and paste it, so I have to type in this very long, 32 letter and number backup key.  But I'm typing it in right now.



STEVE:  Or just take a picture of it with your other phone.



LEO:  Oh.  Aren't you smart.  You must be Steve Gibson.  I know you.  No, this is cool.  You know what?  My name is Leo Laporte.  I should be using Olvid.  The problem is, as with all these messaging systems, you have to get other people to use it or, I mean, that's the problem.



STEVE:  Yup.  Yup.



LEO:  Yeah.  And I don't know anybody who uses Olvid, so so much for that.  Mr. Gibson?



STEVE:  Okay.  So we've been covering the news of the now-impending EU eIDAS 2.0 legislation...



LEO:  Bleah.  Bleah.



STEVE:  I know, mostly from the standpoint of the two open letters that those in the industry and academia have authored and co-signed.  And by "those in the industry and academia" I mean a number now totaling more than 500 individuals who are truly concerned about what the EU is about to unilaterally place into law.  Four weeks ago this podcast was titled "Article 45," so I understand that we've already talked about this.  But I just encountered, as I mentioned earlier, two new pieces of commentary from two well-placed technologists.  So I decided to share their appraisals to create some "what it would really mean to the world" perspective.  And there are a couple surprises.



The first person's name is Ivan Ristic.  I was immediately curious when I saw that Ivan had chosen to weigh-in and address this issue because I know his name well.  If Ivan's name doesn't immediately jump out and mean anything to you, you may know his  well-known website and service, SSL Labs.  For as long as I can remember, Ivan's SSL Labs (ssllabs.com) has been the go-to site for checking the security at both the server and browser ends of secured connections.



Ivan is also the author of two books:  "Bulletproof TLS and PKI:  Understanding and Deploying SSL/TLS and PKI to Secure Servers and Web Applications."  Its first edition was published nearly 10 years ago, in 2014, and the book is now in its second print edition with added coverage of TLS 1.3.  It's also available as an eBook.  Ivan's second book is the "OpenSSL Cookbook:  The Definitive Guide to the Most Useful Command-Line Features."  And anybody who's ever looked at OpenSSL knows that a command-line reference would be a good thing to have.



Anyway, that one's in its third edition, also available for free.  He and his wife Jelena are based in London.  His piece, which he wrote last Thursday, and if all of that preceding didn't give you the idea, this guy understands authentication and certificates and SSL and TLS, and he's been at this for a long time.  His piece is titled "European Union Presses Ahead with Article 45."



So Ivan wrote:  "The European Union continues on its path to eIDAS 2.0, which includes the controversial Article 45 that basically tells browsers which certificate authorities to trust.  eIDAS, which stands for Electronic Identification and Trust Services, is a framework aimed at regulating electronic transactions.  As part of this proposal, the EU wants to support embedding identities in website certificates.  In essence, the goal is to bring back Extended Validation certificates.  Browsers, of course, don't want that.



"But the real problem is the fact that, with the legal text as it is at the moment, in its near-final form" - and I'll just mention that this is what was signed behind closed doors - "the EU gets the final say in which Certificate Authorities are trusted."  I mean, that's the crux of this.  And we have a lot more to say about that.  But, he says:  "The global security community has been fighting against Article 45 for more than two years now.  We wrote about it on a couple of occasions.  As of November 2023, the European Council and Parliament have reached a provisional agreement.  The next step is for the law to be put to the vote, which is usually a formality.



"In November, ahead of the crucial vote, the campaign intensified, with browser providers (Google and Mozilla), civil society groups (EFF) and other companies, and more than 500 security experts voicing their concerns.  In the end, it did not help.  The bureaucrats drafted the text and voted behind closed doors with little acknowledgement of the protests.



"And therein lies the main problem.  The EU doesn't understand the global technical community.  Internet standards are developed collaboratively and organically, with careful deliberation of the details.  The EU, on the other hand, prefers a top-down approach that ignores the details and apparently involves no debate.  They expect everyone to trust that the details will turn out all right.  The text voted on was published only after the fact.



"The EU might have the right to govern its territory, but when it comes to these global matters, it also has a duty to respect and compromise with the rest of the world.  Above all, care must be taken to separate technology and politics as much as possible.  After all, it took the world a very long time to achieve reasonable security of global website authentication.  A decade ago, we were witnessing hackers breaking into CAs and government agencies issuing certificates for Google properties.  Today, we have much stricter issuance and security standards, and we also have certificate transparency, which provides visibility and auditing.  No one knows what's going to happen with that, and the EU doesn't engage.



"Where are we now?  The EU wants browsers to display legal identities embedded in the qualified certificates, but it also wants to control who issues them.  It so happens that the same certificates are used to store the identities and authenticate websites.  It's not at all clear if the EU cares about the latter part.  In fact, the following statement appears in the recitals in the provisional agreement:  'The obligation of recognition, interoperability, and support of QWACs is not to affect the freedom of web-browser providers to ensure web security, domain authentication, and the encryption of web traffic in the manner with which the technology they consider most appropriate."



So he writes:  "Can browsers recognize and show legal identities from the EU-approved CAs, but continue to require full compliance with current technical standards in order to fully trust qualified certificates?  Or can browsers require two certificates, one for TLS and the other for identities, like Mozilla proposed a year ago?  We'll need to wait and see."  So that's what Ivan wrote, who is way, you know, been around the block and paved a bunch of the block.



LEO:  Yeah.



STEVE:  The second piece, which Ryan Hurst wrote a little over two weeks ago, is titled "eIDAS 2.0 Provisional Agreement: Implications for Web Browsers and Digital Certificate Trust."  And here's where some really interesting numbers come up.  What Ryan wrote goes further than anything I've seen so far to provide an assortment of interesting facts to clarify the way things are today, and to examine what the EU's proposed changes would mean to the industry and to the world.  So he leads with a summary, writing:  "This document contains my notes on the problematic elements of the provisional agreement on the EU eIDAS 2.0 legislation reached by EU legislators on November 8th."



So six main points.  "Mandatory Trust in EU-Approved Certificate Authorities:  Browsers will be required to trust certificate authorities approved by each European member state.  This could lead to scenarios where the government forces the trust of CAs that put global users at risk.  Two, Lower Standards for EU-Approved Certificate Authorities:  Establishes a lower standard for European CAs, limiting the browser's ability to protect users from underperforming EU certificate authorities.  Third, EU to Override Browser CA Trust Decisions:  In cases where an EU investigation does not lead to the withdrawal of a certificate's qualified status, the EU can request browsers to end precautionary measures, forcing them to trust the associated CA."  And, like, why would that be in there?  I mean, that's just, like, asking for a fight.



"Number four establishes global precedent for further undermining encryption on the web.  When a liberal democracy establishes this kind of control over technology on the web, despite its consequences, it lays the groundwork for more authoritarian governments to follow suit with impunity.  Next, browsers are forced to promote legal identity for authentication of websites.  Browsers will be required to have a user interface to support the display of legal identity associated with a website, potentially reversing previous design choices made based on user behavior and research."  And I'll come back to this point later, but what gives the EU any authority over the design of third-party browsers over which, you know, they have no say?



And lastly, "The Inconsistencies of Recitals with the Substantive Legal Text.  The recitals in the legal text have ambiguities and contradictions which will cause long-term negative consequences for the web."  In other words, the recitals were put in in order to try to soften what the legal tech says.  But of course the legal text is what's binding.



Okay.  So Ryan explains:  "The text says browsers must either directly or indirectly take a dependency on the EU Trust List to determine if a CA is trusted for website authentication.  This is a list of CAs as determined by each member state to be in conformance with the legal obligations under eIDAS."  He says:  "To put this into context, based on the currently authorized organizations on this list, we can expect to see 43 new organizations added to both the Mozilla and Chrome Root Stores.  This is just a number, though.  Let's give it a little color."



And Leo, there's a chart here at the bottom of conveniently numbered page 13.  He says:  "Today there are seven organizations in the Web PKI that are responsible for 99% of all certificate issuance."  That's astonishing.  Once again, let me say that:  seven organizations, seven certificate authorities, seven certificate signers.  Those seven are collectively responsible for 99, actually I think it's 99.36, if I recall, percent of all certificate issuance.  So this chart has this big, huge, blue region.



LEO:  Yeah.  Who's Internet Security Group?  That's almost half.  Who is that?



STEVE:  Uh-huh.  And that's Let's Encrypt.



LEO:  Oh, I love you, Let's Encrypt.  Good for you.  Wow.  That's great.



STEVE:  Isn't that astonishing?



LEO:  Oh, my gosh.



STEVE:  Let's Encrypt has 46.52% of all currently non-expired web certificates in circulation.



LEO:  That's really awesome.  That's what I use for my website, yeah.  Love it.



STEVE:  Well, I'm still with number two, but number two has about half of that, and that's of course DigiCert.



LEO:  Yeah.  They're very good, yeah.



STEVE:  They're my favorite.  You know, they are still my CA.



LEO:  They're expensive.  They're not cheap.  They're more expensive than others, yeah.



STEVE:  That's true.  Though what you get in turn is a higher level of assurance. 



LEO:  Right.



STEVE:  Inherently, Let's Encrypt is only a domain validator.  That's all it's able to do.



LEO:  Right.



STEVE:  Although it is able to do that for free.  And, as we can see from this pie chart, that's what half of the Internet is using today.



LEO:  Wow.



STEVE:  46.52%.  So what astonishes me, though, is that - so we have DigiCert at 22.19, Sectigo at half of that, that is, half of DigiCert's at 11.89.  Google Trust Services is a 8.88.



LEO:  Hmm.  Surprising, actually.



STEVE:  Followed by GoDaddy...



LEO:  Yeah, that's who we use.



STEVE:  ...at 5.77.



LEO:  Yeah.



STEVE:  Yeah.  Microsoft Corp. has 3.45, and then IdenTrust commercial root CA is down at 0.63.  So if you sum all of those, those are the top seven, and you can almost argue that you don't need that last 0.63.  But if you include that in order to get to 99.32%, those seven CAs alone give you coverage of 99.32%, which says you're only missing a bare 0.63% in all the others.  The hundreds of others, Leo, only account total for 0.63%.  So that, you know, this should bring everyone up short.  This means that the industry, in the guise of the CA/Browser forum, has been incredibly permissive about extending our global browser trust to organizations whom we really have very little actual need to trust.  Yet today we're inherently trusting the signatures of certificates that most of us are never going to see.



And of course the great controversy is that any of them could sign the certificate for any domain they wanted to, and a browser would trust it because we trust anything that any of them sign.  So you know, that suggests to me that we're going in the wrong direction here.  Even the idea of adding any more, it's like, what?  No.  We can survive with seven.



LEO:  Yeah.  Who's IdenTrust?  I don't...



STEVE:  That's a good question.  I don't know whose certificates they're signing.



LEO:  They're small.



STEVE:  Yeah.



LEO:  And why isn't VeriSign on this list?



STEVE:  Yeah.



LEO:  That's surprising.



STEVE:  Good question.



LEO:  Yeah.  I love it that Let's Encrypt is so dominant.  I mean, is that okay?  People who come to our site do not look at the certificate and say, oh, it's GoDaddy, not DigiCert.  And by the way, that saved us hundreds of dollars.  I mean, so, yeah, DigiCert is the gold standard.  But I don't think our users really care.



STEVE:  Yeah.  And frankly, if Let's Encrypt had existed 10 years ago, the pie chart would not look like this.



LEO:  I agree.  It's going to be 99% soon enough.



STEVE:  And in 10 years it won't look like this, yes.



LEO:  Right, right.



STEVE:  So, okay.  Here's what Ryan has to say about this.  He writes:  "There are between 75 and 85 organizations in the various root programs constituting the entire Web PKI that can issue certificates for the entire web.  If we use the higher estimate of 85" - okay, now, so that's like all the rest which includes these seven, so those other 82 have just microscopic shares; right?  He says:  "If we use the higher estimate of 85" - and the reason it's an estimate is like he says between 75 and 85, it's like, those last 10, it's like, you know, they signed, you know, monkeymoose, and no one, you know, and their certificate expired.  You know, I mean, it's just like...



LEO:  Monkeymoose.  I want that one.  Good one.



STEVE:  So, and by the time this podcast is over somebody will have registered it.  So he says:  "If we use the higher estimate of 85, the addition of the EU's 43 member countries represents an increase of over 50% in the number of organizations that are trusted for doing what they're doing."  And they don't have to abide by anybody's rules.  The EU says you must trust these.



Ryan says:  "Why is all this significant?  While it's true that there are numerous CAs in the Web PKI beyond the seven mission-critical ones, each additional CA represents an increased surface area for all users of the web.  The 'long-tail' CAs, those lesser-relied-upon entities, are part of the Web PKI because they ostensibly meet the same objective technical and procedural standards as their more prominent counterparts."



Okay.  So in other words, we all trust all of those essentially unneeded CAs because the way the system has evolved, it would be considered rude not to give anyone the benefit of the doubt and trust their work signing certificates until and unless they give the world reasonable cause not to.  However, it's also likely that with the ISRG's Let's Encrypt having changed the rules, no one in their right mind today would attempt to establish a new commercial Certificate Authority.  That would be nuts.  Given today's startling distribution of signed web certificates, my feeling is that we ought to be running in the exact opposite direction than what the EU proposes.  If IdenTrust at 0.63% was also eliminated, presumably also with minimal impact, we could reduce CA trust to just six well-proven certificate authorities.  That sure seems like the future as opposed to adding 43 new and highly political trust roots.



So Ryan continues, writing:  "Web browsers set these standards to participate in their programs, striving for objectivity, openness, and consistency.  This approach not only keeps the web open and fosters the development of sovereign digital capabilities in various countries, but also involves a balancing act, mitigating the risks associated with the expanded attack surface that each new CA introduces.  What the EU proposes tips the scales of this system by lowering the bar for European CAs, allowing them to meet a lower standard while at the same time putting these governments in charge of which CAs meet the bar."



LEO:  Ugh.



STEVE:  I know.



LEO:  Such a terrible idea.



STEVE:  It is.  I just can't - we can't allow this to happen.  He says:  "To put this in context, consider this case in 2013 where a French agency that was allowed into the Web PKI was caught minting SSL certificates that impersonated major sites like Google.  Putting 27 governments in a position to add more CAs that are trusted by the world means they can do this at scale if they decide to do so."



LEO:  Oh, my god.



STEVE:  "Or an attacker uses these governments' ability to do so for their own benefit."



LEO:  That's a good point.  That's a good point.  If an attacker gets in and gets the root certificate, all hell breaks loose.



STEVE:  Yup.  Overnight.  It's worth noting that the browsers distrusted this CA when it did this.  In this new world that won't be possible.



LEO:  Wow.



STEVE:  "Now consider for a moment that there are 195 sovereign nations in this world, for now.  The 27 member states of the EU will be the only countries in the world with the ability to force browsers to trust arbitrary CAs like this or to add their pet features.  How long," Ryan asks, "do we think that will last if browsers become compliant with this new legislation?"



He linked to an article which appeared in Ars Technica 10 years ago, in 2013, and what I read above.  I was pleased to see that this podcast had covered every one of those incidents cited.  And I'll just skim it over here because, for example, there was the trust wave was discussed, and also remember this, the Netherlands-based Diginotar.



LEO:  Diginotar.  I do.



STEVE:  I know you'll remember Diginotar.



LEO:  Yeah, yeah.



STEVE:  So there have been several other instances in addition to this Cyber Defense Agency that have been caught in the past.  And the good news is browsers are immediately able to blacklist the hashes of those known fraudulent certs and then yank the trust from the root.  The French legislation prevents that.  I mean, it's hard to believe it does.



He says, okay, so - and of course now we know that we only really need to trust six or seven CAs to obtain trust coverage of 99.32% of the entire web.  So next Ryan makes the point in the EU's legislation, says requires browsers to have a user interface to support the display of legal identity associated with a website.  He writes:  "Extended Validation certificates, once used by about 9 to 10% of websites, now represent only about 3.8% of all certificates on the web."  And one wonders once they expire if they'll be renewed as EV because as we know browsers no longer show anything special.  "Web PKI CAs originally marketed these tools for increasing conversion rates, among other supposed benefits," meaning more consumer belief in the value of the site.  "But there was never any data supporting these claims.  Over time, it became apparent that they provided little to no value, and in some cases even harmed users.  A notable example of the confusion arising from this paradigm is a case where a security researcher demonstrated the ability to quickly and inexpensively create a legitimate company with a name conflicting with a well-known organization, without needing to reveal the identity."



Okay.  So for those interested, the researcher was a guy named Ian Carroll.  Ian filed the necessary paperwork to incorporate a business called Stripe, Inc.



LEO:  Oh.



STEVE:  And he did this in a different state than where the actual Stripe, Inc. was incorporated, which is perfectly legal.



LEO:  Is legal, legal, yeah.



STEVE:  He then used the legal entity to apply for and receive an EV certificate to authenticate the website stripe.ian.sh.  But of course Ian was unable to get stripe.com because the real Stripe owned that domain.  But creating a stripe.com subdomain, or a stripe subdomain under his own ian.sh domain was sufficient.  This was because, at the height of EV certificate usage, the domain's EV certificate details would be shown instead of that messy HTTP URL.



LEO:  Oh, my god.



STEVE:  So what visitors to Ian's demo site saw was simply "Stripe, Inc."



LEO:  Oh, boy.



STEVE:  Yep.  And I'll note that this followed three months after a different researcher named James Burton established a valid business entity, "Verified Identity," to demonstrate how the resulting EV certificate might be used to add an air of authenticity to a scam site.



LEO:  So you go to the scam site, and it says "Identity Verified."



STEVE:  Oh, that's right.



LEO:  Must be legit.



STEVE:  That's right.



LEO:  That's clever.



STEVE:  Yes.  The bottom line was that since typical users don't actually have any idea what's going on, all of this extra specialness was abandoned.  Or, as Ryan puts it:  "This incident, along with several others and research based on large-scale analysis of user reliance on browser trust indicators, led to the de-emphasis of all these affordances in the browser UI.  The previous UI, which highlighted this information, was redesigned and demoted in the visual hierarchy, setting it on a path for a likely eventual removal as a result."



Which is where we are today.  But the legislation that is poised to become law in the EU after several years of the industry warning against all of this in the strongest possible terms, requires browsers, literally is enforcing the design of browsers to bring this back, and for the EU to be able to add their own identity assertions to the browser's location bar display.



LEO:  Ugh.  This is horrible.



STEVE:  Leo, it's unbelievable.



LEO:  It's breaking security.



STEVE:  Yes, it is.  Another point of serious concern is that the EU's forthcoming legislation explicitly and deliberately limits the ability of browsers to protect users from poor-performing EU certificate authorities.



LEO:  You mean they're forced to accept the certificate, even if they know these guys are bozos.



STEVE:  Correct.  Correct.  It is no exaggeration to say that we depend upon our browsers to have our backs in countless ways.  Ryan writes this:  "Today, CAs are removed as trusted for a vast range of reasons.  For example, last year" - and I checked this out, it was in 2022 - "a Turkish Certificate Authority, e-Tugra, demonstrated they lacked the most basic security practices and could not effectively respond to a security incident and were distrusted as a result.  Not due to having made any mistake, but because their service was clearly shown to be unconscionably insecure.  When I dug into this, Leo, their web portal had never changed the default admin login credentials."



LEO:  Oh, admin/admin.



STEVE:  Of a certificate authority.



LEO:  But that's why the system works because the browsers then say, yeah, we're revoking that CA.



STEVE:  Yes.  Yes.  Ryan writes:  "Under this new legislation, browsers will no longer have the ability to distrust European CAs that are trusted for QWACs except for 'breaches' and 'loss of integrity of an identified certificate,' whatever that means.  Each of the CAs trusted within the Web PKI represents a risk to users.  This is why it is so important that browsers, acting as the agents of their users, are empowered to establish uniform criteria to ensure all the CAs meet minimum best practices and have the ability to remove them when those minimum best practices are not met.  The text reduces the cases substantially in which they may do that."  And he says:  "Unfortunately, it gets worse."



He then cites some legislation that will take effect.  And so the legislation reads:  "Web browsers may take precautionary measures related to a certificate or set of certificates in case of substantiated concerns related to breaches of security or loss of integrity of an identified certificate.  When such measures are taken, the browsers must notify their concerns in writing without undue delay."



LEO:  With a quill pen.



STEVE:  Oh, my god, yes.



LEO:  On a piece of parchment.



STEVE:  "Along with a description of the measures taken to mitigate those concerns.  This notification should be made to the Commission, the competent supervisory authority, the entity to whom the certificate was issued, and the qualified trust service provider that issued the certificate or set of certificates.  Upon receipt of such notification, the competent supervisory authority is expected to issue an acknowledgement of receipt to the web browser in question."



LEO:  We have received your missive and shall respond...



STEVE:  Oh, my gosh.  Did anyone ever see, what was it, "Brazil"?



LEO:  Yes.



STEVE:  I think it was the movie...



LEO:  Yes, it's totally "Brazil," yes.



STEVE:  Oh, my god.



LEO:  The bureaucracy.  Submit your form in triplicate.  B/7935.  Wow.



STEVE:  So anyway, the legislation contains language such as "shall not be subject to any mandatory requirements other than the requirements laid down earlier and shall not take any measure contrary to their obligations set out in Article 45,"  referring to the browsers.  Again, the EU is flatly asserting absolute authority over the trust that browsers will place in any certificates issued by their member states.  And the text also says that even in the cases where there are "breaches of security or loss of integrity of an identified certificate," the EU can override the browsers and force them to trust the associated CA anyway.



"When the outcome of an investigation does not result in the withdrawal of the qualified status of the certificate or certificates, the supervisory authority shall inform the web browser accordingly and request it to put an end to the precautionary measures referred to."  In other words, there is no other way to look at this, Leo, other than that they are absolutely getting into business they have no business getting into.



LEO:  Shocking.



STEVE:  The more time I've spent looking into this, the worse it seems.  The world has spent a great deal of time slowly and carefully evolving an equitable system of trust.  And now, for essentially commercial, like ego reasons, to force the display of website digital identity through the equivalent of their own system of EV certs, this legislation would force all web browsers to accept root certificates from every EU member state, which would then use them to assert the identity of anything they choose.  And there's nothing any browser can do about it.



What I'm most wondering now is what gives the EU the right to dictate the operation of our web browsers?  To me, this seems like uncharted waters.  Users currently have some say over the certificates which populate their root stores.  If they wish to remove trust from some certificate authority, nothing prevents them (us) from doing so.  But the EU is stating that browsers will be required to honor these new, unproven, and untested certificate authorities and thus any certificates they issue, without exception and without recourse.  Does that mean that my instance of Firefox will be legally bound to refuse my attempt to remove those certificates?



If the EU wants to create their own "EU Browser" based upon a fork of Chromium, embellish it with their own certificates and a user interface display of whatever those certificates wish to assert, then require that their own citizens use it, the only people who would have any problem with that would be their own citizens, who could then decide whether they want to keep those legislators in office.  To me, that appears to be the only feasible course of action.



What's completely unclear, and what I haven't encountered anywhere, is an explanation of the authority by which the EU imagines it's able to dictate the design of other organization's software.  Because that's what this comes down to.  The UK tried to do this, as we know, with end-to-end encryption.  Every last publisher of that technology said no, and the UK blinked.



Edge and Chrome on Windows obtain their root stores from Windows. So the EU is telling Microsoft that they must add and unilaterally trust 43 new root certificates to their operating system's root?  And what about Linux?  Who's going to make Linux do this?  Good luck sneaking this past Linus!  That's never going to happen.



LEO:  Yeah, because the OS stores, in fact most of the time your OS is the root.



STEVE:  Firefox is the only exception.



LEO:  Yeah.



STEVE:  And in a weird thing that you missed last week, Leo, Firefox 120 just added a new option to include the Windows root certificates into its own root store.



LEO:  But that's optional; right?



STEVE:  And that's optional, yes.  It could be turned off.  And of course, and really, there's the very real specter of what other doors this opens.  If the EU shows the rest of the world that it can successfully dictate the terms of trust for the independent web browsers used by its citizens, what other countries will follow with similar laws?  Now everyone gets to simply require that their own country's certificates get added? This takes us in exactly the wrong direction.  That's crazy.  None of this is good.



And it's not even as if there's some actual problem that needs to be solved here, Leo.  Like someone just invented this over in the EU.  It's just crazy.  The more I think about this, the more I like the idea of disabling Firefox's newly added "trust the certs in the underlying operating system's root store" option, then pruning all but six of Firefox's current root certificates.  That trusts 99% of the Internet's certificates, and likely 100% of any certificates that I would choose to trust.



LEO:  Well, when are we going to get the Steve Gibson Prune Your CAs app?  Because is it - how easy would it be to prune out all the CAs?



STEVE:  Definitely doable.



LEO:  You could list them and have checkboxes next to them. 



STEVE:  Yes.



LEO:  You could have your recommended six.



STEVE:  Yup.



LEO:  Press a button.



STEVE:  The good news is this is generic enough that somebody will do it by the next podcast.



LEO:  Yeah.  If they weren't all doing Advent of Code right now, maybe they would.  Interesting.



STEVE:  Wow.  Did you miss - is that the week you missed?



LEO:  I missed it.



STEVE:  Aww.



LEO:  I mean, it's not over.  It's all month.  But if you don't start on day one...



STEVE:  Yeah, it ramps up.



LEO:  Yeah.



STEVE:  So you're just going to skip it this year.



LEO:  Well, I'm not going to do it in real time.  I'll probably get around to doing it.  I'm just - I'm not, honestly, I'm not really in the mood to sit down and...



STEVE:  You're altered.  Yes, you have an altered state of...



LEO:  Altered state, yeah.



STEVE:  Yeah.



LEO:  Basically I want to go out and hug a tree, and that's it.  So, wow.  This is hair on fire bad.  I'm very glad you covered it.  



STEVE:  Yeah, it's astonishing.



LEO:  But the funny thing is, it's trivial to prune these out.



STEVE:  Yes.



LEO:  It's not illegal even in the EU; right?



STEVE:  Probably not.  As we know, the problem is all the people who won't prune it.  So, I mean, and that's who the browsers are worried about is, you know, they're wanting to protect everybody.



LEO:  Normal people won't.  Only the listeners of this show will know what to do.



STEVE:  Yeah.  I mean, so can the EU outlaw the use of a browser unless it complies?  I mean, like, by what authority could they dictate the operation of software they don't control?



LEO:  That's interesting.  So if Firefox says, and I hope they do, if the Mozilla group says no, we're not going to do it, then what?  Can they ban it EU wide?  No.  Maybe they could.  They could fine them, I guess.  You know, the EU's done some good things, I have to say. 



STEVE:  Yeah.



LEO:  Absent a U.S. government that's willing to protect our privacy, it's good that the EU is.



STEVE:  Yup.  Yup.  GDPR does have some privacy-centric things.



LEO:  They've done some dopey stuff.  This cookie thing just drives me crazy.



STEVE:  Oh, my god.



LEO:  It's so...



STEVE:  I know.  I know.



LEO:  It's so meaningless.  And how many billions of human hours are lost clicking that thing?



STEVE:  Yup.



LEO:  And now this.  I don't know.  I mean, what's worse?  Somebody who doesn't, who is just like laissez-faire, like the U.S. government is, do whatever you want?  Or somebody who does some good things and some bad things?  Thank you, Steve.  Have a wonderful week.  We'll see you all next time.



STEVE:  Okay, buddy.



LEO:  On Security Now!.



STEVE:  Bye.



Copyright (c) 2023 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.






GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#952

DATE:		December 12, 2023

TITLE:		Quantum Computing Breakthrough

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-952.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  Why is metadata such a problem?  What massive new audience just got end-to-end encryption by default?  What's the latest on Iran's Cyber Av3ngers?  What were the most exploited vulnerabilities of 2023?  How are things looking two years after the discovery of the Log4j flaw?  Whatever happened with Sony's attempt to force Quad9 to block a music pirate's domain? What exactly is the Dark Web, anyway?  And where is it?  And after closing the loop with some of our listeners, we're going to examine last week's surprising news of a significant breakthrough in quantum computing.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  Coming up, Ron Wyden's fight to protect our metadata.  Why is metadata a risk?  We'll find out.  We'll also talk about Sony.  They lost that big case.  Yeah.  And then Steve explains why he thinks quantum computing just had a breakthrough moment.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 952, recorded Tuesday, December 12th, 2023:  Quantum Computing Breakthrough.



It's going to be good because it's Steve Gibson.  That's what I hear everywhere because Steve is the hero of the hour, the host of the show you're now listening to, Security Now!, where we cover security and privacy and how computers work, how technology works.  This one's going to be a big one, I think.  Hi, Steve.



STEVE GIBSON:  Hey, Leo.  Great to be with you.  Let's see.  Oh, yeah.  This is airing before anyone will have seen what we did on Thursday.



LEO:  Oh, yeah, our holiday show.  We should mention...	



STEVE:  Yeah, yeah, the old prostate show. 



LEO:  The good thing is because we're all old we had to make it a short show so everybody could get to the bathroom.  It's our Christmas Eve TWiT, which Steve is a part of.



STEVE:  It was a lot of fun.



LEO:  With Doc Searls and Jeff Jarvis and Rod Pile.  And it was, it was great.  So if you're around Christmas Eve, make sure you get TWiT.  It'll be up on the feed December 24th.



STEVE:  So here we are, and I'm more conscious than ever that we're moving past 999 because we've just crossed the 950.  We're into 952 now.



LEO:  Yeah, yeah.  And it would be shaking my boots right now.  I'd be...



STEVE:  It would be a little sad if we were like...



LEO:  Yeah, thank you.



STEVE:  ...heading to the end.  But we're not.



LEO:  Thank you.



STEVE:  Well, the end of the podcast at least.  We may be nearing the end of traditional asymmetric crypto.  And don't anyone get upset.  We still have plenty of time.  But today's podcast is titled "Quantum Computing Breakthrough" because there has been one.  And it appears to be significant.  I appreciate, and our listeners will hear this when I'm quoting the scientists who are involved, everybody around it is, you know, they're adding the caveat of, you know, if this indeed works the way it appears to.  You know, that's what scientists say.  I mean, that's science, instead of, you know, the marketing hype people.



But I really believe everybody's going to understand what happened because it's under, well, it's understandable.  And in fact it's also this week's Picture of the Week, not yet something that people are going to have on their desktops.  But the first computers weren't, either; right?  I mean, they were - they had, you know, power generation stations all dedicated to them in order to light them up.



LEO:  Well, given that these have to operate at absolute zero, I don't think there's going to be a desktop quantum computer.



STEVE:  Not soon.



LEO:  No.



STEVE:  So we're going to get there.  But first we're going to answer the question, why is metadata such a problem?  What massive new audience just got end-to-end encryption by default?  What's the latest on Iran's Cyber Av3ngers?  What are the most exploited vulnerabilities of 2023, which we can now examine because we're nearing the end of 2023.  How are things looking two years after the discovery of the Log4j flaw?  Whatever happened with Sony's attempt to force Quad9 to block a pirate domain that was offering some of Sony's music?  Whatever  happened with that lawsuit?  We talked about it a couple years ago.



LEO:  That's right.  I forgot about that.



STEVE:  Yup.  What exactly is the Dark Web, and where is it?  And after closing the loop with some of our listeners, we're going to examine last week's surprising news of a significant breakthrough in quantum computing.



LEO:  Wow.  That's almost scary, to be honest with you.



STEVE:  It definitely has advanced the timeline significantly.  



LEO:  Holy cow.



STEVE:  So I'll be - we'll talk about that and everything that it means here by the end of the podcast.



LEO:  Yeah, I want to know more.



STEVE:  A great podcast for our listeners.



LEO:  I really want to know more because honestly I've been skeptical, to be honest, about what quantum computing, whether it would even happen.  It's like fusion energy; you know?



STEVE:  Well, when they're bragging that they were able to factor the number 37 or something, then it's like, uh, okay.



LEO:  Yeah.  Nevertheless, I trust you.  And if you say there are big things ahead, I'm thinking, uh-oh.



STEVE:  Everyone will get it by the end of this podcast.



LEO:  Good, good.  Good, good, good.  All right.  Let me get - I should stall a little bit because I don't have the ad in front of me.  But I shall get it soon.



STEVE:  I heard that Wix is now an advertiser.



LEO:  Yeah.  Do you use Wix?



STEVE:  No, but Lorrie does.  



LEO:  Oh, yeah.



STEVE:  She's a Wix maven.



LEO:  It's good stuff, yeah.



STEVE:  Leo, I use a text editor.  Excuse me.



LEO:  Of course, HTML.  Nothing wrong with HTML.  No sirree.  Steve, I have queued up the Picture of the Week.  Do you want to introduce it?



STEVE:  Yeah.  This is an actual picture from the lab at Harvard University that we'll be discussing at the end of the podcast.  One of Michael McCollum's sci-fi novels is titled "The Sails of" - Sails as S-A-I-L-S, as in light sails - "The Sails of Tau Ceti."  And light sails is an actual thing.  You can use a large, very gossamer-thin fabric which is aimed at a star in order to get propulsion.  Which is to say that photons, light coming from the star, actually do create pressure which can be used, in the case of Michael's book, to power a starship, to propel a starship through the universe.  Well, through the galaxy because, you know, between galaxies you've got a problem with insufficient starlight.



But in this picture, that is being done.  Somewhere buried in this insane-looking machine, and this is one of those where, like, you know, you don't let the janitor come in to clean.  This is, you know, painstakingly assembled.  And you could sort of see that it's like a high-end LEGO set; right?  You know, these things are screwed down.  They all have slots which allow them to be adjusted before the screw is tightened on the base in order to hold whatever that is in place.  But in here, and you'll hear me talking about it later, are laser tweezers which use lasers to manually move molecules of...



LEO:  What?



STEVE:  ...super-cooled rubidium atoms around.  



LEO:  What?



STEVE:  Yeah.



LEO:  What?



STEVE:  So we're talking serious, I mean, you know, this was science fiction 20 years ago.  Now here it is, and it actually works.



LEO:  This would make a great jigsaw puzzle.



STEVE:  Oh, it really would, yeah.



LEO:  Wow.



STEVE:  Yeah.



LEO:  Find the tweezers, everybody, find the tweezers.



STEVE:  By the end of the show, I'm not saying that this will make any more sense than it does now, but at least we'll have an overview of what this contraption has accomplished.



Last Wednesday, December 6th, one of our favorite privacy rights crusading senators, Oregon's Senator Ron Wyden...



LEO:  Ron Wyden.  Love this guy.  He's been so good.



STEVE:  Yup.  Well, we need somebody like this to be doing this, so I'm sure glad he is.  He addressed a letter to "The Honorable Merrick B. Garland, Attorney General, U.S. Department of Justice."  Here's what Wyden and his team wrote.  They said:  "Dear Attorney General Garland:  I write to urge the Department of Justice (DOJ) to permit Apple and Google to inform their customers and the general public about demands for smartphone app notification records.  In the spring of 2022, my office received a tip that government agencies in foreign countries were demanding smartphone 'push' notification records..."



LEO:  Oh, boy.



STEVE:  "...from Google and Apple.  My staff have been investigating this tip for the past year, which included contacting Apple and Google.  In response to that query, the companies told my staff that information about this practice is restricted from public release by the government."  So your democracy's tax dollars hard at work tying the hands and gagging companies to prevent them from disclosing what they're being forced to do in what, like the name of national security?



Anyway, Ron's letter continues with Ron explaining:  "Push notifications are the instant alerts delivered to smartphone users by apps, such as notification about a new text message or a news update.  They aren't sent directly from the app provider to users' smartphones.  Instead, they pass through a kind of digital post office run by the phone's operating system provider.  For iPhones, this service is provided by Apple's Push Notification Service; for Android phones, it's Google's Firebase Cloud Messaging.  These services ensure timely and efficient delivery of notifications, but this also means that Apple and Google serve as intermediaries in the transmission process.



"As with all of the other information these companies store for or about their users, because Apple and Google deliver push notification data, they can be secretly compelled by governments to hand over this information.  Importantly, app developers don't have any options.  If they want their apps to reliably deliver push notifications on these platforms, they must use the service provided by Apple or Google, respectively.  Consequently, Apple and Google are in a unique position to facilitate government surveillance of how users are using particular apps.



"The data that these two companies receive includes metadata, detailing which app received a notification and when, as well as the phone and associated Apple or Google account to which that notification was intended to be delivered.  In certain instances, they also might also receive unencrypted content, which could range from backend directives for the app to the actual text displayed to a user in an app notification.



"Apple and Google should be permitted to be transparent about the legal demands they receive, particularly from foreign governments, just as the companies regularly notify users about other types of government demands for data.  These companies should be permitted to generally reveal whether they have been compelled to facilitate this surveillance practice, to publish aggregate statistics about the number of demands they receive, and unless temporarily gagged by a court, to notify specific customers about demands for their data.  I would ask that the DOJ repeal or modify any policies that impede this transparency.



"Thank you for your attention to this pressing matter.  If you have any questions or require clarification, please contact Chris Soghoian in my office."



LEO:  Right on, Ron Wyden.  And by the way, I didn't know Chris Soghoian was working for Ron Wyden.  Just yet another reason to respect Ron.  Chris Soghoian's one of the best security guys out there.



STEVE:  Yup, he absolutely is.



LEO:  Yeah.  Wow.



STEVE:  So Reuters picked up on this and did some additional reporting.  They wrote:  "In a statement, Apple said that Wyden's letter gave them the opening they needed to share more details with the public about how governments monitored push notifications.  Apple wrote:  'In this case, the federal government prohibited us from sharing any information.  Now that this method has become public, we are updating our transparency reporting to detail these kinds of requests.'  And for their part, Google said that it 'shared Wyden's commitment to keeping users informed about these requests.'  The Department of Justice declined to comment on the push notification surveillance or whether it had prevented Google or Apple from talking about it.



"Wyden's letter cited a 'tip' as the source of the information about the surveillance.  His staff did not elaborate on the tip, but a source familiar with the matter confirmed that both foreign and U.S. government agencies have been asking Apple and Google for metadata related to push notifications to, for example, help tie anonymous users of messaging apps to specific Apple or Google accounts."  In other words, deanonymizing them.



"The source declined to identify the foreign governments involved in making the requests, but described them as democracies allied to the United States.  The source said they did not know how long such information had been gathered in that way.  Most users give push notifications little thought, but they have occasionally attracted attention from technologists because of the difficulty of deploying them without sending data to Google or Apple.  Earlier this year, French developer David Libeau said users and developers were often unaware of how their apps emitted data to the U.S. tech giants via push notifications, calling them 'a privacy nightmare.'"



So we've talked before about the power and value of metadata.  While it may be that Apple and Google's messaging encryption technology deliberately prevents them from being able to comply with lawful government subpoenas for the content - we know quite well that Apple doesn't want to - the fact that messages are flowing, and between whom they are flowing, is not something that is nearly as easy for Apple to have no way of knowing.  Right?  They've designed their technology so that they cannot respond to the subpoenas.  They don't want to.  But this is not something for them to - like for them not to know.  These subpoenas doubtless require Apple and Google to provide all information they have of any kind, including specifically metadata, for the individuals targeted by the subpoenas and relating to the apps and other users those targets are interacting with.



So metadata itself is a pervasive problem.  We've talked about how even though today's ISPs who are unable to see into today's web connections, the fact of those connections is not hidden without the use of a VPN to tunnel everything past an ISP's prying eyes.  We also talked recently about the move to encrypt the initial TLS handshake packets since they were also carrying useful metadata.  And even the Tor network, whose entire existence is about obscuring endpoint connection metadata, has a difficult time doing so.  We've seen that a correlation attack mounted by examining packets entering and exiting the Tor network, assuming that there's some way to obtain that sort of near-global visibility, can be surprisingly effective.



So it's significant that Wyden and his team are only, at least in this instance, asking about Apple and Google.  It's significant because this is very likely the tip of the iceberg.  It would be safe to say that any and all social media enterprises operating inside the U.S., such as Meta with WhatsApp, and of course Facebook and Facebook Messenger, are subjected and being subjected to the same sort of metadata requests accompanied by the same sort of gag orders.



Although law enforcement and other government agencies may be thwarted in their examination of messaging content, the social graph networks through which their targets move remains a significant source of valuable and, in fact, vital information.  And it's not at all clear how Apple, Google, WhatsApp and the rest can eliminate that loophole.  I mean, even Tor has technically a difficult problem doing so because these service providers are maintaining explicit accounts with the users of those devices.  In all of this worry over encryption, backdoors, and privacy, this ubiquitous presence of notification, connection, and messaging metadata is almost always overlooked.  So again, as you said, Leo, bravo to Wyden for pointing at this and probably eventually hopefully prying this open as a much bigger problem than it's been given.  [Crosstalk] we've got everything in...



LEO:  Very few people know how useful metadata is.  And obviously law enforcement does.  And I'm glad that he recognizes that.  And I think credit to Chris Soghoian because he's got a good cybersecurity advisor.



STEVE:  Yes.  And you could also argue that third-party cookies are nothing but metadata.



LEO:  Yeah.



STEVE:  That's, you know, it isn't - all it is, is literally a token which follows you around, and it's by - it is your movement around the Internet metadata which is then all aggregated and pulled together to mean something.



LEO:  Right, yeah.  And people are wondering, yes, Chris Soghoian is a relative of Sal.  It's Sal's nephew.  Sal Soghoian of course a long-term contributor on the network, the creator of - a big guy at Apple, scripted Apple for a long time, the creator of Automator, and a regular on our shows in the early days.  So, yeah, comes from a good family, too.



STEVE:  Yup.



LEO:  Yeah, good on you, Ron.  Good on you.



STEVE:  Yeah, yeah.  Thank goodness we have him, you know, jumping up and down about this stuff.  And while we're on the topic of end-to-end encrypted conversations, also last Wednesday Facebook announced that their gradual rollout of encryption for Facebook Messenger, which we talked about when it first appeared as an encrypted conversations option in Messenger, is now switching to always on by default.  This took a long time since it meant that all endpoints would need to have been upgraded.  That has happened.



So now Facebook is flipping the switch.  Since I was curious about what underlying technology they were using, I did a bit of digging and learned that they had the wisdom to not roll their own encryption system.  Facebook Messenger is based on the Signal protocol.  Even to the extent of borrowing from Signal's open source libraries over at GitHub.  Facebook needed to tweak Signal here and there since their platform models are different.  But it's nice to know that Messenger's billion-plus users will be using a stable and well-proven encryption technology by default moving forward.



Last week we noted that widespread attacks were being seen targeting PLC systems made by the Israeli company Unitronics. These were of particular concern since the systems in question were controlling water management systems in the U.S. and throughout the world.  The shocking piece of this was that the systems being "attacked" - and you really need to put that word "attacked" in air quotes here, since given that these systems had never bothered to change their manufacturer's original, default established password of "1111," connecting remotely to any of the between 500 and 800 systems now known to be vulnerable, really, you know, it amounts to more of a visit than an attack.  "Yeah, we're just visiting your..."



LEO:  Just dropping in.



STEVE:  We're just visiting your system from Iran.  You know? What's up?  So in any event, these Iranians are reportedly continuing to busily scan the Internet for the appearance of any of these PLC systems, and we now have the six Iranian IP addresses they're using.  I've added them to the show notes for anyone who might be interested in watching their honeypots or their public WAN interfaces for probes from any of these six IPs.  You know, 88.135.36.82, and there are five others.  I won't recite them all here.  They're at the very bottom of page 3 of the show notes for anyone who's interested.  Thank you, Leo, for putting it on the video there.  So yeah.  If any of those IPs visit you, they're coming from Iran.  And they're just coming over to say hi and to see whether you might be, you know, have your sprinkler system connected to the Internet through an Israeli Unitronics PLC.



LEO:  They're probably not geolocated in Iran, either; right?  They're probably, you know, come - they look like they're coming from Indiana or somewhere, I would imagine.



STEVE:  I did note that there's a five-dot, which is interesting.



LEO:  Yeah, that's interesting.



STEVE:  Because remember that Hamachi used the five-dot network extensively because not a single IP had ever been allocated from five-dot anything.  And of course we gave - we had to give up on that a while ago as we began to run shy of IP4 address space.



LEO:  I'll add these to my Ubiquiti blocklist.  I block nations, you know, I block China.  I block Russia.  But like I said, I  bet it doesn't all - they're not all Iranian.  I'll block Iran.



STEVE:  Probably not.  Probably specifically in order to get around these sorts of blocks.



LEO:  Sure, yup, yup.



STEVE:  So Cisco's Talos group have just published their annual cybersecurity Year in Review.  And the thing that caught my eye there was their breakdown of the most targeted vulnerabilities.  Before we look at the top 10, here's what the Talos group said about their findings.  They said:  "In 2023, cyber threat actors exploited older software vulnerabilities in common applications.  In many cases, the vulnerabilities were more" - wow, it's hard to even read this.



"The vulnerabilities were more than 10 years old, consistent with CISA's finding that adversaries have targeted old security flaws more than newly disclosed recent ones in recent years.  In fact, four of the top five most-targeted vulnerabilities we observed were also cited by CISA as being frequently exploited in prior years, further highlighting this point.  This underscores the need for entities to regularly install software updates, as many of these systems were likely unpatched given the age of the targeted vulnerabilities."  Meaning they're so old they obviously never received a patch since then.  Which, right, okay.



So they said:  "The top targeted vulnerabilities were found in common applications like Microsoft Office.  This finding is also substantiated by CISA, which noted that actors in 2022 prioritize CVEs that are more prevalent in their targets' networks."  In other words, you know, they're going to attack the ones that are there; right?



"Adversaries likely prioritize targeting widespread vulnerabilities because the exploits developed for such CVEs can have long-term use and high impact."  That's a good point.  If you're targeting, if you're able to target an exploit, a very old vulnerability, that says the system has never been updated since then, which further says it probably won't ever be again.  So if you can get in using that, you can probably get to stay a long time.



They said:  "Lastly, most of the vulnerabilities on our list would cause substantial impact if exploited" - right, they're all like in the high nines in the CVSS score, and a lot of them are remote code execution.  So ouch.  They said:  "With six of these top 10 receiving a maximum vulnerability risk score of 100 from Cisco Kenna and seven receiving the highest 'critical' score from the Common Vulnerability Scoring System (CVSS).  Most of the CVEs are also listed in CISA's Known Exploited Vulnerabilities catalog" - their KEV, K-E-V - "which is meant to inform users on the security flaws for which they should prioritize remediation.  The high frequency of targeting attempts against these CVEs, paired with their severity, underscores the risk to unpatched systems."



So I have in the show notes the chart from Cisco's Talos Group.  What's astonishing is that this year, okay, 2023 we're talking about, the number one most often exploited vulnerability, with a high CVSS of 9.3, is CVE-2017-0199.  That's right, 2017.  This is a vulnerability that...



LEO:  So it's six years old.



STEVE:  Yes.  Six years ago this thing was found.  And we talked about it on the podcast extensively.  This affects Microsoft Office and WordPad.  It's a remote code execution vulnerability that leverages RTF files and a flaw in Microsoft's equation editor which is exploited through Microsoft Word.



LEO:  Oh my god.  Oh my god.



STEVE:  I know.  For six years.  Leo, it's remote code execution.  It's a CVSS of 9.3, the maximum vulnerability score of 100.  And it is the, today, the number one most exploited vulnerability.



LEO:  Yeah, but I have to point out it's not the oldest on this list.



STEVE:  No, no, no.



LEO:  By a long shot, in fact.



STEVE:  No.  It's not.



LEO:  Holy cow.



STEVE:  You know, and what's absent from these discussions - and from my ability to imagine - are any details about the machines which are, today, still not only running Microsoft's Office software from 2017, unpatched even once since then, but also have no other AV defense of any kind in place, even Microsoft's free solution.  And this is not just one lonely abandoned system somewhere forgotten in a closet.  The exploit enters through an email attachment.  So the systems being compromised not only have no defenses of any kind and haven't received a single update in six years, but they are in active use.  Somebody is receiving email on them and clicking a link to get themselves exposed to this.  And it's the number one most often thing that happens.



LEO:  Oh, my god.



STEVE:  Where are these systems?  Whose are they?  Okay.  And want to take a guess at the number two problem among the top 10?  Believe it or not, it also has the same high CVSS of 9.3, and its CVE was also issued back in 2017.



LEO:  Oh, my god.



STEVE:  CVE-2017-11882.  And that beauty is brought to us by Microsoft's Exchange Server.  Now, obviously the same thing applies as above, although an unattended machine running Exchange Server might well be in some dusty old forgotten back closet somewhere.



And lately I've been pushing the idea that automatic updates would solve many of the problems we're seeing in our industry today.  But Microsoft pioneered the idea of automatic updates.  They're enabled by default and have been for a very long time.  Yet somehow a huge number of machines, enough to put their exploit at number one in the first place in the ranking, are still not being patched.  They've remained unpatched.  So it would really be interesting to look at case studies of the specific machines and the environments that surround them that allow six-year-old Microsoft flaws to remain present today.  Who has these things?  Where are they?  We don't know.  But Leo, we are going to know who's helping to support this podcast and the TWiT Network.



LEO:  Very important.  We would not want to forget such important people.  You didn't mention that there is a CVE from 2010 from Apple Safari.



STEVE:  Yes, Safari, yeah.



LEO:  But oh, don't worry, it's only a 9.3.  Nothing to worry about.



STEVE:  Yow.



LEO:  There's a CVE from 2012 for Microsoft Office, GZIP 2012.  I mean, some of these are pretty dang old.  It's kind of incredible.



STEVE:  It does demonstrate what a problem we have with software that isn't getting updated.



LEO:  Legacy, yeah.



STEVE:  And that will - we'll be talking about the Log4j library next.  But first a word from our sponsor.



LEO:  I can't imagine anybody's still using a 2010 version of Safari.  I don't even know if that would work.  But if you are, stop; okay?  Just stop.  Stop the insanity.



STEVE:  So while we're on the topic of long-lasting vulnerabilities, it's not only the deep past that seems to be behaving irresponsibly.  Two years ago, 2022, we kicked it off here with the news that a potentially devastating flaw in the Log4j library might rock the Internet.  Fortunately, that never happened because the flaw was not readily exploited except in the one instance where it was found in a zero-day attack.  So it was never the lowest hanging fruit.  And as we've seen, the bad guys use the easiest route in, which Log4j turned out not to be.  But that didn't mean that the flawed library didn't still need to be fixed and replaced because as long as this known-to-be-vulnerable library is present, a resourceful hacker might find a way to exploit it in its particular use case.



Last Sunday, on the second anniversary of the discovery of this flaw, BleepingComputer reported on the findings of the application security company Veracode, who took a look at the status of Log4j today.  Here's what BleepingComputer reported.  They said:  "Roughly 38% of applications using the Apache Log4j library are using a version vulnerable to security issues, including Log4Shell, a critical vulnerability identified as CVE-2021" - because it was at the very end of 2021, and we began talking about it at the beginning of 2022 - "44228, that carries the maximum severity rating, despite patches being available for more than two years."  Okay?  So 38% of applications that have a version of Log4j bound into them include a critical vulnerability.



They said:  "Log4Shell is an unauthenticated remote code execution (RCE) flaw that allows taking complete control over systems using Log4j 2.0-beta9 through 2.15.0.  The flaw was discovered as an actively exploited zero-day on December 10th, 2021; and its widespread impact, ease of exploitation, and massive security implications acted as an open invitation to threat actors.



"The circumstance prompted an extensive campaign to notify affected project maintainers and system administrators; but despite numerous warnings, a significant number of organizations continued to use vulnerable versions long after patches became available.  Two years after the vulnerability was disclosed and fixes were released, there are plenty of targets still vulnerable to Log4Shell.  A report from application security company Veracode, based on data collected between August 15th and November 15th, highlights that old problems can persist for extended periods.



"Veracode gathered data for 90 days from 3,866 organizations that use 38,278 applications relying on Log4j with versions between 1.1 and 3.0.0-alpha1.  Of those apps" - so those 38,278 apps - "2.8% use Log4j variants 2.0-beta9 through 2.15.0," that was the original Log4j disaster, "which are directly vulnerable to Log4Shell.  Another 3.8% use Log4j 2.17.0, which, although not vulnerable to Log4Shell, is susceptible to the same CVE, a remote code execution flaw that was fixed in version 2.17.1.  Finally, 32%" - get this - "are using Log4j earlier, Log4j version 1.2.x, which reached its end-of-life support in August of 2015, and all of those versions are vulnerable to multiple severe vulnerabilities published until 2022" - CVE-2022-23307, 305, and 302.



"In total, Veracode found that about 38% of the apps within its visibility use an insecure Log4j version.  This is close to what software supply chain management experts at Sonatype report on their Log4j dashboard, where they show 25% of the library's downloads in the past week include known vulnerable versions.  The continual use of outdated library versions indicates an ongoing problem, which Veracode attributes to developers wanting to avoid unnecessary complications.  Veracode found that 79% of developers chose to never update third-party libraries after their initial inclusion in their code base to avoid breaking functionality."  In other words, this is deliberate in 80%, 79% of developers.  "And this is true even if 65% of open-source library updates contain minor changes and fixes unlikely to cause functional problems.



"Moreover, the study showed that it takes half of all projects over 65 days to address high-severity flaws.  It takes 13.7 times longer than usual to fix half of what's in their backlog when understaffed, and over seven months to handle 50% of it when lacking sufficient information.  Overall, Veracode's data shows that Log4Shell was not the wake-up call many in the security industry hoped it might be.  Instead, Log4j alone continues to be a source of risk in one out of three cases, and may be one of the multiple ways attackers can leverage to compromise a given target.  The recommendation for companies is to scan their environment, find the versions of open-source libraries in use, and then develop an emergency upgrade plan for all of them."



Okay, now, there are many places where our industry is limping along and hoping for the best, such as with the use of very useful open source libraries.  One of the problems is that developers could truly spend all of their time just keeping their code current with upstream library version changes.  You know, I love, and I've mentioned this before, the Notepad++ application for Windows.  It's really terrific.  But its developer refuses to leave it the "F" alone.  It's fine.  It works.  Just stop messing with it.  It is constantly wanting to update itself.



And since it does everything I want, I finally disabled its checking for updates because it was just so annoying.  Every time I launched it, oh, look, a new version.  Stop everything.  Hold the presses.  You know, upgrade.  But now, with updates disabled just for the sake of sanity, now I have a real problem because I wouldn't know if a serious vulnerability turned up in it because it was just too noisy, because the developer abused the automatic update system.



So I can imagine just how much chaos there would be if a project was using a large assortment of open source libraries which were themselves dependent upon other open source libraries, and so on up the dependency tree.  It likely would be truly a full-time job just keeping everything current.  And I totally sympathize with the idea that you update some library, and it breaks a bunch of things because the functionality changed.  It wasn't just fixing a bug.  It was, oh, look, we realize the whatever it is, you know, you don't want new, you just want secure.



So it's not difficult to imagine a developer thinking, look, everything's working right now.  I don't want to rock the boat and risk breaking things that are working just for the sake of having the latest and greatest version of everything.  Maybe they're more secure, or maybe a bug was introduced when the change was made.  Right?  I mean, it's not like when we're adding new features we're not also bringing new bugs along.  Otherwise, you know, Windows would have been perfect a long time ago.  But it sure isn't.



Okay.  Two years ago we reported on the despicable tactic Sony was undertaking of blaming the DNS resolving service Quad9, just singled them out because, you know, they're non-profit, so they ought to be easy to squash and step on and set a precedent.  So they sued Quad9 for the fact that they were providing random people on the Internet who have their browsers pointing at Quad9 the IP address of a pirate domain whose server was making some of Sony's copyrighted recordings available.  Sony said that the domain owners didn't respond and couldn't be located.  And I don't know about the hosting provider.  So they decided to sue Quad9 because, you know, they could find them, and they would respond to email.



LEO:  Notice they didn't sue Google.



STEVE:  Exactly.  Exactly.  And at the time, Leo, you and I were both livid at the idea that Sony might prevail in this since this might set a precedent where anyone with a grievance could sue someone who was providing some of the Internet's infrastructure, regardless of how incidental to the underlying complaint.



So here we are two years later with the news that the German court where Sony brought this case has ruled in favor of the defendant Quad9.



LEO:  Hallelujah.



STEVE:  Thank goodness.  Denying any appeal and instructing Sony to pay all of the defendant's defense costs, which certainly is the way it should go.  Here's what Quad9 had to say about all of this.  They wrote:  "Today marks a bright moment in the efforts to keep the Internet a neutral and trusted resource for everyone.  Quad9 has received word from the courts in Dresden, Germany in the appeal of our case versus Sony Entertainment (Germany).  The court has ruled in favor of Quad9, clearly and unequivocally.  Needless to say, we are elated at the news.



"Sony Entertainment (Germany) started a legal proceeding against Quad9 more than two years ago to force Quad9 to stop resolving certain domain names which they claimed were involved in copyright infringement behavior.  We believe this lawsuit was an attempt to set a precedent, such that commercial rights holders could demand that sites on the Internet be made unreachable by forcing recursive resolvers to block content.  We contended that recursive resolvers have no commercial or even remotely indirect relationship to any of the infringing parties, and that Sony's demand for blocking was ineffective, inappropriately specified, and not related to Quad9.



"What made this case more problematic, in our view, was that the servers in question in this case were not located in Germany, and the links they pointed to were on servers also not in Germany.  The domain name (canna.to) was not registered in Germany and was under the top-level domain operated by the nation of Tonga.  Sony Entertainment further asserted that we block the domains globally, not just in Germany, as GeoIP does not block for users based in Germany with certainty.  For that matter, Quad9 has no office or standing in Germany (we are a Swiss entity), but due to the Lugano Convention treaty it was possible for Sony to serve an injunction in Switzerland and drag Quad9 into legal proceedings there.



"The appeal with the Higher Regional Court in Dresden follows a decision by the Regional Court in Leipzig, in which Sony prevailed, and Quad9 was convicted as a wrongdoer.  Before that, Sony successfully obtained a preliminary injunction against Quad9 with the Regional Court in Hamburg.  The objection against the preliminary injunction by Quad9 was unsuccessful, and the appeal with the Higher Regional Court in Hamburg was withdrawn by Quad9 since a decision in the main proceeding was expected to be made earlier than the conclusion of the appeal in the preliminary proceedings."  And you have to read that paragraph five times to understand what they said.  But basically they appealed their initial loss, and they won on appeal.



They said:  "The court has also ruled that the case cannot be taken to a higher court, and their decision is the final word in this particular case.  Sony may appeal the appeal closure via a complaint against the denial of leave of appeal and then would have to appeal the case itself with the German Federal Court. So while there is still a possibility that this case could continue, Sony would have to win twice to turn the decision around again."



They said:  "We would like to clarify that even though Quad9 benefits from the liability privileges as a mere conduit, it is possible that a DNS resolver operator can be required to block as a matter of last resort if the claiming party has taken appropriate means to go after the wrongdoer and the hosting company unsuccessfully."  Which is interesting because those - two years ago we said that, like, it's so wrong that Sony didn't do those things.  Turns out that the way the language is written, and here Quad9 is saying, if a complainant did first go after the wrongdoer, and the hosting company, and do everything, you know, all their due diligence, which Sony didn't, then it might be ruled that a DNS resolver operator could be required to block the domain.  



Quad9 said:  "Such measures could be legal action by applying for a preliminary injunction against a hosting company within the EU."  Meaning that would be regarded as sufficient.  "These uncertainties still linger, and we expect that this ongoing question of what circumstances require what actions, by what parties, will continue to be argued in court and in policy circles over the next few years."



Now, I'll just pause here to say on a broader level this cannot be allowed to happen.  Recognize what this would mean.  This would mean that anybody, any big bully with money could sue DNS resolvers to force them to block domains that they're not happy with for any reason.  It also means countries could do it.



LEO:  And worse than that, it's worst than that because it wouldn't just be DNS resolvers.  It'd be any infrastructure, any [crosstalk] infrastructure.  Because it's all using that infrastructure.  I mean, that's ridiculous.



STEVE:  I know.



LEO:  They're not aligned, I mean, that's like suing the phone company because somebody phoned in a bomb threat.  It doesn't make sense, and it's just untenable.  It can't - you can't continue that way.



STEVE:  Yes.  Please don't bring this up to that judge in Texas.



LEO:  Oh, lord.  Oh, lord. 



STEVE:  Because that would be the end of life on the Internet.



LEO:  And I always get nervous when stuff goes to courts because you don't know what you're going to get.  You know?



STEVE:  Right.  Right.  So Quad9 says:  "We remain committed to the concept that resolving a domain name is not an action that should be prohibited for commercial goals.  The DNS does not contain content.  It is a system designed for delivery only of pointers, not for data transport.  The courts in Cologne also recently ruled in favor of Cloudflare in a similar case involving DNS recursive resolution, although that case also includes a separate consideration of issues relating to CDN and proxying services.  And we are pleased to have consistent and clear statements from both courts in this matter of DNS recursive resolution.



"Today was a significant win in Germany, but there is some disappointment, as well.  We received a notice from a consortium of Italian rights holders - Sony Music Italy, Universal Music Italy, Warner Music Italy, and Federation of Italian Music Industry - who have demanded that Quad9 block domains in Italy, and there is potentially another court process ahead of us."  Wow.



Okay.  So on the one hand this is good news, but it's also clear that this is not over yet.  Sony is at it again, attempting to use their muscle to force Quad9, a non-profit DNS resolving company, to bend to their will.  Here's the status of this second new lawsuit.  Quad9 wrote:  "While our case in Germany has been found in favor of Quad9, we've been served with another demand from commercial interests in an EU nation to block domain names, again based on alleged copyright violations.  Italian legal representatives have presented us with a list of domains and a demand for blocking those domains.  Now we must again determine the path to take forward fighting this legal battle, in another nation in which we are neither headquartered nor have any offices or corporate presence.



"Unsurprisingly, the group of media companies that are represented by the plaintiff" - and it's an Italian legal firm,  LGV Avvocati, Milan, Italy on behalf of the Federation against Music and Multimedia Piracy - "are Sony Music Entertainment Italy, Universal Music Italy, Warner Music Italy, as well as the Federation of the Italian Music Industry.



"The short answer from us is that we're going to fight this demand for censorship, but it may take us time to be certain of a full victory and completion of our case in Germany before we are able to take on another court process.  Quad9 can only have a few legal fronts open at once.  We are nearly entirely dedicated to operational challenges of running a free, non-profit recursive resolver platform that protects end users against malware and phishing.  We are not a for-profit company with lawyers on retainer.



"We've complied with the request, and these names are now blocked on Quad9 systems.  Since the courts have provided again no guidance on how we determine if a request is made by someone under Italian jurisdiction, we have applied this block globally.  The German courts entirely disregarded our use of GeoIP lookups on queries, and asserted that since tests via a VPN were able to resolve the domain, we were in breach of court orders.  We wish to avoid that same distraction again for the time being.



"We've created an attributed blocking list with these domains on it, and they are searchable on the front page of our website in the search bar, and the results will contain information about the status and origin of the blocking requirements.  When Quad9 decides to take action on this issue, we will post updates on this blog and modify our blocklists accordingly."  Well, you know, Leo, saying again, this is so wrong.



LEO:  Yeah.



STEVE:  I am so glad that the court in Dresden made Sony responsible for all the costs of Quad9's defense, and I hope Quad9 may be able to use their success in Germany to help with this next case.  What Sony, Universal, and Warner are doing cannot be allowed to happen.  And, I mean, what we really need is some higher level intervention somewhere.  I mean, somehow.  This just can't, as you said, Leo, this is just one handy part of the infrastructure.  But, you know, the separation has to be made.



LEO:  It makes, I mean, it just makes you not want to do good things.  It's like, you know, they're doing this great service.



STEVE:  I know, it's sad.



LEO:  And thank goodness that they have the fortitude.  I would just say, well, screw you, bye.  But good for them that they stuck it out.  I mean, it's got to cost them a ton to do this.



STEVE:  Yeah.  Well, and the good news is they're getting their expenses reimbursed.



LEO:  Sony, yeah.



STEVE:  And so I hope so.  So the problem is, Sony is so huge.



LEO:  They don't - it doesn't hurt them.



STEVE:  And they do have, I mean, they have attorneys on retainer who they're paying a salary to, so it doesn't cost them anything.



LEO:  It's not a fair fight.



STEVE:  No.  I hate bullies.  That just - I have a problem with that.  So a group known as Searchlight Cyber posted an analysis about the state of the DDoS for Hire business which exists on the so-called Dark Web.  That made me realize that while I've referred to the Dark Web often in the past, we've really never stopped to talk about it.  For anyone who's unsure, the Dark Web is actually a thing, or rather a place.  It's not just an expression.  And you cannot get there from here.  It's not just a matter of using some secret URL to bring up a Dark Web site.  There are three terms used within the cyber security community to refer to three classes of the web.  There's the Clear Web, the Deep Web, and the Dark Web.



The web that we all use every day is more formally known as the Clear Web.  It's what Google indexes and where anyone can easily wander with links that are shared with us either by a public search engine or by other web pages.  This is the traditional web that Tim Berners-Lee first conceived of when he was at CERN.  And interestingly, this Clear Web only contains roughly 4% of the entire web content.  It's like the tip of an iceberg.



So where's the rest?  Most of the rest resides in the so-called Deep Web.  This is the web content that is not indexed publicly because it requires authenticated access through a portal of some kind.  So this includes things like our credit reports, IRS tax records and medical histories, fee-based content, membership websites and confidential corporate web pages.  Those are all considered to exist on the Deep Web.  And estimates place this Deep Web content at roughly 96% of the total browser-accessible web.



And finally, a small subset of this Deep Web is known as the Dark Web.  The dark web's servers are deliberately hidden and are accessible only through the Tor browser on the Tor network.  It's necessary to use Tor because those servers and the services they offer wish to remain hidden and quite difficult, if not nearly impossible to locate.  And it's worth noting that despite its ominous-sounding name, not all of the dark web is used for illicit purposes, though this is another place where the emergence of cryptocurrency has transformed the place from a backwater hacker curiosity into a significant collection of criminal enterprises.



My advice would be to stay as far away from the Dark Web as possible.  So I won't be offering any guide or tips to accessing this dark underbelly of the Internet.  But if you're really curious and want to go poking around down there, you'll find that the Clear Web contains many step-by-step how-to guides into setting up a secure sandboxed OS, obtaining the Tor browser, putting on your hazmat suit, holding your breath, and taking the plunge.  Good luck.



And as for what the Searchlight Cyber guys found out about DDoS for Hire services being offered on the Dark Web, there was nothing really noteworthy.  It's pretty much what we would expect.  Portals exist where people can create an account and transfer some of their cryptocurrency to it to create a positive balance.  Then they select the type of DDoS attack they wish to launch, either at OSI stack Level 4, which is down at the transport layer, so using either UDP or TCP flooding packets, or up at Level 7, which is the application layer, so an HTTP connection and query flood.  Then the target IP or URL is provided, and the size and duration of the attack is specified, and the site takes care of conducting the attack from there.



One thing that did surprise me was that one of these services, named the "Nightmare Stresser," claimed to have 566,109 registered users.  That number cannot be verified, of course.  



But it does perhaps support the view from up here out in the Clear Web, you know, up in the light, we see more or less constant DDoS attacks, so much so that they're just prevalent.  Talk to any Tier 1 Internet provider, and it's like, oh, yeah, you know, that's just happening all the time.  So 566,109 registered users of this one service where if you want someone to be attacked, it's not hard to do.  And actually it doesn't even cost that much, which is another reason we're seeing so many of them.



Okay.  Let's close the loop with a few listeners.  Mike Ward, via DM, said:  "Hi, Steve.  I just discovered a major privacy and security flaw in the messaging app Telegram.  Telegram," he wrote, "automatically informs users you've joined if that user has your phone number, and has previously uploaded it to Telegram as part of their contact list sharing, even if you deny Telegram access to your contacts."  He said:  "I discovered this firsthand just now.  After downloading and signing up for Telegram, and specifically denying it access to my contacts, a friend sitting across the table from me received a notification on his phone that said I had joined Telegram."



Mike wrote:  "This is a major privacy and security flaw, as a stalker or domestic abuser could upload a victim's number and be automatically notified if that person ever joins Telegram.  For that matter, so could any law enforcement or government agency.  It may also explain why spam and scams are so prevalent on the services, as mal-actors are able to upload your phone number acquired from somewhere else, then inundate you with messages the moment you join."



So that was an interesting observation, Mike.  And I agree that this appears to be a significant privacy failing of Telegram.  The system ought to require mutual sharing of intersecting contact information in order to provide any notifications, not just one-way, unilateral contact use.  They likely use the less private approach as a means of promoting the further use and adoption of Telegram.  But that's not what anyone wants from a secure messaging solution.



LEO:  Yeah, it's actually very annoying because I get so many messages from Telegram saying your friend has just joined Telegram.  Like, and I never give - by the way, let's emphasize this.  Never give your contacts to any third party.  You're basically turning over information about your friends to a third party without their consent.  Never do that.



STEVE:  Right.



LEO:  Not for the very mild convenience of getting notified when your friends join Telegram.  But you know that's not why they want that information.



STEVE:  Right.



LEO:  Yeah.



STEVE:  Yeah, I'm still using an old and long-since-retired landline as the phone number that I give out most places.



LEO:  Right, that's a good idea.



STEVE:  I just won't let them have even my phone number because, you know, I look at other people whose phones are just exploding, you know, with nonsense, and I'm glad I don't have that.



Justin Ekis, he said:  "Have you ever seen something like this?  Looks like my cell carrier is forcing me to trust an additional root CA.  This is unacceptable.  I'll be switching at the earliest opportunity.  Thought my fellow listeners would like to know."  And he included a link to a Twitter media snapshot which was of his Xfinity cellular provider asking him to accept a root certificate and give it full trust on his phone.



LEO:  Oh, sure.



STEVE:  I know.



LEO:  Why the hell wouldn't I want to do that?



STEVE:  Obviously we agree with Justin completely.



LEO:  Wow.



STEVE:  There's only one possible reason for this, which is that Xfinity wishes to insert a transparent proxy into his Internet connections.



LEO:  Right.  So why?  So they could put [crosstalk] in there; right?



STEVE:  Well, and it will decrypt everything he does with his phone.



LEO:  Yeah.



STEVE:  All of his use of the Internet browsing will be decrypted by certificates which Xfinity is able to build on the fly, which will be trusted because he accepted their root cert.  I mean, it is full-on spying.  And so yes, that should certainly raise warning flags for anybody who understands what's going on.  Unfortunately...



LEO:  I should mention, Steve, that this link that you put in here is actually a link to direct messages.  So I can't see it, and no one could see it because it's a message to you.



STEVE:  Ah, okay.



LEO:  So don't get all excited, folks.



STEVE:  Thank you, Leo.



LEO:  I would love to see that screenshot.  That's why I clicked it.  But geez, Louise.



STEVE:  Ah, right, right.



LEO:  Yeah, yeah.



STEVE:  So, yes.  Anyway, do not accept any requests for, like, from your cell provider saying, hey, you know, in order to continue our relationship with you, you're going to have to add this root certificate to your phone.



LEO:  Holy cow.



STEVE:  No.  No.



LEO:  Is that a condition of using Xfinity cellular?  If it is...



STEVE:  Apparently it's [crosstalk].



LEO:  Don't, yeah.



STEVE:  Right, right.



LEO:  Ugh.



STEVE:  Justin was very right to bring this up.  Thank you.



Elliot Alderson said:  "Instead of pulling all but six CAs," meaning six CA roots, "couldn't someone make an extension that just shows green for one of the main six, yellow for another CA, red for HTTP, and a duck for a QWAC cert?  Maybe need blue, orange, and pink or something for color blindness."



So that's an interesting thought.  But my concern would be that this places the burden on the user to observe some visual flag.  If our goal in removing the ridiculous number of barely, if ever, needed certificate authority root certs is to improve our security in the event that one of the fringe CAs mints a malicious cert, then I still think that the cost of running with only the top six is likely very minimal and represents the best solution.



Michael Smithers says:  "Hi, Steve.  I'm a longtime listener and SpinRite customer.  Thank you for all your work.  Can you recommend a hardware VPN solution for home that's not complicated to set up?  I need to access my home server from around the world, mainly for version control access, and I presently have some ports Internet-facing with port forwarding.  Yes, I know this is not good.  I can see from the logs that bots are continuously attempting to guess login credentials and are causing user lockouts for some more common usernames.  Many thanks for your help.  Here's to 1000 and beyond.  Michael."



Okay.  VPNs are definitely useful when you want to protect your use of the Internet, for example, from your prying ISP, who's like trying to scavenge all the metadata they can from you.  But setting up your own VPN server to provide incoming access is probably no longer the optimal solution.  For all of those needs you really want to look at an overlay network.  Think Tailscale or ZeroTier.  Overlay networks is the newer, better, more secure and much more powerful and flexible way to solve this sort of problem.  We've talked about these before, and I've received a bunch of feedback from our listeners who have said that they have been astonished by how easily the system was installed, set up, and running.



And another advantage is that they run through NAT routers without needing any static port forwarding, so no bots are going to be probing for a connection.  Tailscale has a companion page where they compare themselves with ZeroTier, and I've looked at that page in the past.  It appears to be quite evenhanded.  Since nothing else has claimed GRC's shortcut of the week so far, grc.sc/952 will take you directly to that page.  It's at tailscale.com/compare/zerotier, Z-E-R-O-T-I-E-R.  I'm very glad that Michael asked this question since today's new overlay network solutions really do represent a useful advance and a much better way to solve the need for roaming access to a remote network.



Once upon a time it was Hamachi and their five-dot networks.  You know, we talked, we covered this a long time ago.  Now these things have gone mainstream.  They're open source.  They're free.  They have been audited.  They are bulletproof.  And they really work.  Tailscale or ZeroTier.  And Tailscale, by the way, runs on top of WireGuard, which is the rewrite of a state-of-the-art secure tunneling technology.  It just really works, and it is very fast.  So that's what I would look at for any of that sort of problem.



And finally, Chris Hatch says:  "Steve, very thankful you're continuing SN past 999.  Longtime listener.  Just heard you mention API Monitor.  When I put the URL in Firefox, Malwarebytes blocked going there with the warning 'Website blocked due to Trojan.'"  He said:  "I checked with Virus Total, and it had two AV's that thought it was malicious.  What do you do to check if a site is safe to visit?  Looking forward to upgrading SpinRite when 6.1 is released.  Thanks for the great show.  Chris."



Okay.  Unfortunately, that amazing API Monitor tool that I talked about before and impressed me so much when I had to do some reverse engineering of some Microsoft undocumented stuff,  it's about 10 years old.  And it has not been touched in all that time.  It is also not digitally signed since, 10 years ago, few things were.  And I recognize that many people feel that any software that has apparently been abandoned is automatically worthless.  That's not my feeling at all.



I strongly prefer software that has become stable because all of its bugs have been found and eliminated, making that software complete and finished.  I'm fully aware that's not the way our industry has evolved where many programs are full of bugs or where commercial interests require that software remains forever in flux with people continually paying in the hope that it will finally someday be finished.



For what it's worth, only triggering two out of all of VirusTotal's AV scanners, generally about 70, means that there's nothing whatsoever wrong with that code.  If the screen had lit up with 20 or more out of 70, then, yeah, I would never have let it get near my machine.  Just now, like two hours ago, I dropped my copy of API-Monitor-Setup.exe, which I downloaded and then tested against VirusTotal.  I dropped it on VirusTotal.  I just now received a 0 out of 71 score.  So VirusTotal agrees that the code being served by that site, old and unsigned as it is, is clean and safe to use.



And Leo, this brings us to our third sponsor, and then I think what is going to be a really illuminating and interesting discussion of a recent quantum computing breakthrough.



LEO:  I am, you know, fascinated by this whole thing.  And as I said, skeptical.  So I'm looking forward to hearing this.  Okay.  Now we've got to find out what's all this about quantum computing, Steve.



STEVE:  So the podcast actually has an extended title.  It's "Quantum Computing Breakthrough:  A Breakthrough for the Turbo Encabulator," and "All Your Qubits Are Belong to Us."



LEO:  Yeah, we couldn't fit that in the lower third, Steve.



STEVE:  No.



LEO:  Do people know what the Turbo Encabulator is?



STEVE:  I think maybe now would be a good time to introduce them, Leo.



LEO:  I'll play a little short - this is just a short audio-only clip.



STEVE:  Just a piece of the beginning.



LEO:  Just so you understand what we're talking about here.



STEVE:  It will prepare them for what is to come.



CLIP:  For a number of years now, work has been proceeding in order to bring perfection to the crudely conceived idea of a transmission that would not only supply inverse reactive current for use in unilateral phase detractors, but would also be capable of automatically synchronizing cardinal grammeters.  Such an instrument is the Turbo Encabulator.



LEO:  There you have it, ladies and gentlemen.  You've wanted it.  You've needed it.



STEVE:  Yes, and that lingo will sound familiar in a minute.



LEO:  By the way, that's doublespeak nonsense.  But what you're going to tell us is real.



STEVE:  Eh, we've got some of the doublespeak in there.  It's  not nonsense.



LEO:  Okay.



STEVE:  So DARPA-funded quantum computing research conducted at Harvard made headlines last Thursday with what those in the know are pretty certain represents a significant step forward in the quest for a practical quantum computer.  One of the big problems, which is currently and has been limiting the practical application of quantum computers, is the need for extensive error correction.  One way to think of this is that quantum bits, the so-called qubits, spelled Q-U-B-I-T-S, qubits, are powerful but imprecise.



LEO:  Yes.



STEVE:  So when you need precision for things like, you know, cryptographic computations, where quantum's fuzziness is a bug, not a feature, it's necessary to use a whole bunch of fuzzy qubits in an error-correcting system.



LEO:  Is it using - let me just ask, prefabulated aluminite, surrounded by a malleable logarithmic casing?



STEVE:  It's a foundation of prefabulated aluminite.  Yeah, yeah.  Yeah.



LEO:  Okay.  Just checking.



STEVE:  The individual fuzzy qubits are referred to as "physical" qubits.  But what we want and need for many of the types of calculations that quantum computers promise are what's known as "logical" qubits.  And as I said, traditionally, the number of physical to logical qubits has been around 1000:1.  Which is to say it takes 1000 fuzzy physical quantum bits in an error-correcting mode to reliably obtain a single solid logical bit.  And it's only the logical bits that are able to do most of the useful work.



Okay, now, in a minute I'll share a useful explanation of what was recently accomplished and just published.  And that will come from a non-physicist who speaks English as his first language, rather than quantum.



LEO:  Good.



STEVE:  Reading the actual paper, published in Nature last Wednesday, written by those who do speak quantum fluently, I was reminded of our favorite turbo encabulator which made good use of those flambulating differential reverse trunions.



LEO:  Oh, yes.



STEVE:  To give everyone - you ought to have those, and they have to be differential.



LEO:  Oh, yeah.



STEVE:  To give everyone a flavor of just how far out this science has wandered, this is what they wrote for the abstract of their paper, which was titled "Logical quantum processor based on reconfigurable atom arrays."  They wrote:  "Suppressing errors is the central challenge for useful quantum computing, requiring quantum error correction for large-scale processing.  However, the overhead in the realization of error-corrected 'logical' qubits, where information is encoded across many physical qubits for redundancy, poses significant challenges to large-scale logical quantum computing."



LEO:  Makes sense so far.  Yeah, yeah.



STEVE:  "Here we report the realization of a programmable quantum processor based on encoded logical qubits operating with up to 280 physical qubits.  Utilizing logic level control and a zoned architecture in reconfigurable neutral atom arrays, our system combines high two-qubit gate fidelities, arbitrary connectivity, as well as fully programmable single-qubit rotations and mid-circuit readout."



LEO:  Of course.



STEVE:  But you have to have that, mid-circuit readout.  "Operating this logical processor with various types of encodings, we demonstrate improvement of a two-qubit logic gate by scaling surface code distance from d = 3 to d = 7."



LEO:  Oh.



STEVE:  Yeah, that was - uh-huh, huh.



LEO:  Yeah.



STEVE:  "Preparation of color code qubits with breakeven fidelities, fault-tolerant creation of logical gigahertz states, and feed-forward entanglement teleportation, as well as operation of 40 color-code qubits.  Finally, using three-dimensional [8x3x2] code blocks, we realize computationally complex sampling circuits with up to 48 logical qubits entangled with hypercube connectivity with 228 logical two-qubit gates and 48 logical CCZ gates."



LEO:  Ah.



STEVE:  Yeah.  "We find that this logical encoding substantially improves algorithmic performance with error detection, outperforming physical qubit fidelities at both cross-entropy benchmarking and quantum simulations of fast scrambling.  These results herald the advent of early error-corrected quantum computation and chart a path toward large-scale logical processors."



LEO:  Well, of course,



STEVE:  Well, naturally.



LEO:  Oh, thank god.



STEVE:  What you're looking for is high-fidelity feedforward entanglement teleportation.



LEO:  Uh-huh.



STEVE:  Wow.  And that was not nonsense.  That was actually the abstract from the paper published in Nature last week.



LEO:  I bet there's only 100 people in the whole world who could understand that.  Do you understand that?



STEVE:  No.



LEO:  No.



STEVE:  No.



LEO:  Okay.  I'm relieved.



STEVE:  But we're going to get to the gist of what it means.  Everyone agrees that if this turns out to work as it appears to, it would represent a bonafide breakthrough in quantum computing by dramatically reducing that previous 1000:1 physical-to-logical qubit ratio thanks to a breakthrough in quantum error correction technology.  And when I saw this, it immediately became this week's podcast topic.



Okay.  So now I want to share English.  I want to share something that was written by a non-physicist where there will be no mention at any point of feedforward entanglement teleportation.



LEO:  Oh, well, okay.



STEVE:  It begins with this powerful claim:  "Widespread quantum computing may now come years sooner than widely expected." 



LEO:  Wow.



STEVE:  Yes.  "Thanks to a Pentagon-funded project, as in DARPA which, as we know, also brought the world the Internet, with implications for everything from rapid vaccine development and weather forecasting to cyberwarfare and codebreaking.  If the Harvard-led experiment can be replicated and scaled up, it would still take years to make quantum computers widely available to run new forms of artificial intelligence for medical research, scientific experimentation, and military command-and-control."  Thus DARPA's interest.  "But early adopters would almost certainly include intelligence agencies eager to crack encryption protocols widely used by governments and businesses alike.  That makes it all the more urgent to implement the new quantum-resistant encryption algorithms the National Institute of Standards & Technology (NIST) aims to finalize next year in 2024.



"On Wednesday afternoon" - that's last Wednesday - "the Defense Advanced Research Projects Agency and a paper in Nature announced results from a team of almost two dozen scientists, most of them from Harvard, funded by a DARPA program known as ONISQ (Optimization with Noisy Intermediate-State Quantum devices).  By manipulating individual atoms with precise, low-powered laser beams, known in the trade as 'laser tweezers,' the team was able to create 'quantum circuits' that correct for errors much more efficiently than alternative techniques, potentially overcoming the biggest barrier to practical quantum computers.



"DARPA's program manager for ONISQ said 'Quantum error correction is fundamentally challenging.'  Mikhail Lukin, one of the Harvard scientists" - actually it's Lukin's lab where this was done - "said:  'You cannot copy quantum information, and you also cannot measure the quantum state without destroying it.'  Different corporate, government and academic teams have tried various approaches to error correction, but they all waste an exorbitant amount of the quantum computer's power.  But the Harvard team has found a radically more efficient way to guard against errors.  DARPA's program manager said:  'This is truly revolutionary.  Having been demonstrated and even validated in this paper, we are off to the races.'



"Back-of-the-envelope calculation suggests that the Harvard team's experimental quantum computer is potentially four times as powerful as the most advanced quantum chip available for purchase, IBM's Condor.  Unveiled on December 4th, Condor boasts 1,121 qubits, which is almost a threefold increase over last year's record-breaking IBM Osprey.  So what's a qubit?  The term turns out to have multiple meanings.  While a normal 'classical' computer uses bits that can represent either zero or one, a qubit exploits the fuzzy nature of quantum phenomena to let it represent all the infinite possible values in between.  That's a nifty trick that could shortcut previously impossible calculations.  But because quantum phenomena are so strange, it's also much harder to figure out whether a qubit is working properly or glitching.



"So all quantum computers to date have had to devote most of their qubits to double-checking each other.  That means the number of usable 'logical qubits' that can actually do reliable calculations is orders of magnitude smaller than the number of a machine's physical qubits.  Using error-correcting methods, it takes more than a thousand physical fuzzy qubits acting together to form one logical qubit.  IBM is now exploring a new, more efficient error-correcting technique it says should allow a mere hundred physical qubits to form one logical qubit.  So depending on whether the technique is used, a high-end chip with a thousand physical qubits, like Condor, could generate as little as one usable logical qubit or as many as 10.



"The Harvard team, however, used a radical new approach to error correction that turns a mere 280 physical qubits into 48 logical qubits.  That's about 20 times better than what IBM is hoping it may be able to achieve in its next-generation chip and 200 times more efficient than the 1000:1 ratio that current techniques are trying to reach.



"One of the Harvard physicists said:  'Up till now, all the state-of-the-art people have realized was one logical qubit or maybe two at most.  We have done 48.  For the first time, we've actually built a processor which operates on these logical qubits.  We demonstrated the key basic elements of this processor, namely the ability to encode, decode, and perform logical gate operations.  For the first time ever, we executed algorithms with logical qubits."



"Skip Sanzeri, a co-founder of QuSecure, which builds software to defend against quantum-powered hacking, said logical qubits are the Holy Grail for quantum computing.  He explained that physical qubits are very ethereal.  They can be disturbed easily, which is what leads to errors.  Sanzeri said:  'With the Harvard team's approach you don't need thousands, hundreds of thousands, or millions of physical qubits to error-correct.  If it works, it's a huge speed-up.'"



Okay.  So I want to flesh this out one bit more with some additional background coverage and quotes from other researchers quoting about this breakthrough:  "The system is the first demonstration of large-scale algorithm execution on an error-corrected quantum computer, heralding the advent of early fault-tolerant, or reliably uninterrupted, quantum computation.



"Professor Lukin, who runs the lab at Harvard, described the achievement as a possible inflection point akin to the early days in the field of artificial intelligence.  The ideas of quantum error correction and fault tolerance, long theorized, are starting to bear fruit.  He said:  'I think this is one of the moments in which it is clear that something very special is coming.  Although there are still challenges ahead, we expect that this advance will greatly accelerate the progress toward large-scale, useful quantum computers.



"The National Science Foundation's Denise Caldwell, who is the acting assistant director of the Mathematical and Physical Sciences Directorate which supported the research through the NSF's Physics Frontiers Centers and Quantum Leap Challenge Institutes programs, agrees."  She said:  "This breakthrough is a tour de force of quantum engineering and design.  The team has not only accelerated the development of quantum information processing by using neutral atoms, but opened a new door to explorations of large-scale logical qubit devices, which could enable transformative benefits for science and society as a whole.



"In quantum computing, a quantum bit or qubit is one unit of information, just like a binary bit in classical computing.  For more than two decades, physicists and engineers have shown the world that quantum computing is, in principle, possible by manipulating quantum particles  be they atoms, ions, or photons  to create physical qubits.  But successfully exploiting the weirdness of quantum mechanics for computation is more complicated than simply amassing a large number of qubits, which are inherently unstable and prone to collapse out of their quantum states.



"The real coins of the realm are the so-called logical qubits formed from bundles of redundant, error-corrected physical qubits.  These can store information for use in a quantum algorithm.  Creating logical qubits as controllable units, like classical bits, has been a fundamental obstacle for the field; and it's generally accepted that until quantum computers can run reliably on logical qubits, the technology cannot really take off."  In other words, that's what just happened, folks.  "To date, the best computing systems have demonstrated one or two logical qubits, and one quantum gate operation, akin to just one unit of code, between them.



"The Harvard team's breakthrough builds on several years of work on a quantum computing architecture known as the neutral atom array.  This was also pioneered in Lukin's lab.  It is now being commercialized by QuEra, which recently entered into a licensing agreement with Harvard's Office of Technology Development for a patent portfolio based on innovations developed by Lukin's group.



"The key component of the current system is a block of ultra-cold, suspended rubidium atoms, in which the atoms, the system's physical qubits, can move about and be connected to pairs, or 'entangled,' mid-computation.  Entangled pairs of atoms form gates, which are units of computing power.  Previously, the team had demonstrated low error rates in their entangling operations, proving the reliability of their neutral atom array system.  With their logical quantum processor, the researchers now demonstrate parallel, multiplexed control of an entire patch of logical qubits, using lasers.  This result is more efficient and scalable than having to control individual physical qubits.



"The paper's first named author Dolev Bluvstein, a Griffin School of Arts and Sciences Ph.D. student in Lukin's lab, said:  'We're trying to mark a transition in the field, toward starting to test algorithms with error-corrected qubits instead of physical ones, and thus enabling a path toward larger devices."



So everyone agrees that this is almost certainly a benchmark on the road to moving quantum computing from a promising curiosity in the lab to a place where it can be commercialized for use in solving previously intractable problems in the real world.  Cryptography is not in trouble yet.  But that trouble just jumped significantly closer.  Remember that it's not symmetric crypto that's endangered by quantum computing.  So algorithms such as the AES's Rijndael cipher and hashing-based solution they've always been and will likely continue to be quantum safe.  But asymmetric crypto such as RSA, based on the previously intractable problem of factoring a huge number composed of two half-as-huge prime numbers, and elliptic curve crypto are both on the chopping block.



So it was prescient that the academic crypto community began worrying about this years ago and has now generated a series of quantum-safe replacement algorithms that are rapidly being moved toward adoption.  The challenge is that almost everything that's currently being done in the world today is still using traditional quantum unsafe asymmetric cryptography.  I say almost everything because remember that three months ago, in September, Signal announced their adoption of quantum-safe crypto running in parallel with traditional time-proven, but now potentially unsafe, crypto.  



The other entity that has proven itself to be prescient is the U.S. National Security Agency, our NSA.  As we know, having watched them build that massive data center facility out in Utah, they've taken advantage of the astonishing reduction in the cost of electromagnetic storage - or "spinners" as we call them over in GRC's newsgroups and forums - to suck up and store mass quantities of Internet and other communications that they cannot decrypt today, but which they will likely be among the first to be decrypting once they receive their standing order for serial number 1 of a sufficiently powerful quantum computer.  I'll bet that facility already has a room set aside just waiting for something to be installed there.



What this means for the industry is that we're not going to be able to enjoy the traditional luxury of upgrading our communications encryption when we eventually get around to it. It will be necessary for the finally approved standards to be moved into preliminary implementations with more speed than normal.  The good news is that this is not our first retirement of creaky older crypto in favor of shiny new crypto, so we've gained experience with doing this and can expect it to go smoothly.  Once sufficient testing has been done, these next generation algorithms will need to be deployed.  And again, not eventually, but as soon as possible.



The wisdom Signal demonstrated of running any new and inherently less well-proven algorithm alongside and in parallel with a traditional well-proven asymmetric crypto system, may be the best in the near term.  Requiring that the results from both systems are needed for encrypting and decrypting an ephemeral symmetric key provides protection against the possibility of an unexpected weakness being found in the newer post-quantum system, which would break everything.  Then later, once the new system has been shown to be entirely safe, the secondary parallel backup system could be dropped to obtain a boost in performance.



The surprise this week is yet another reason for taking this podcast past 999.  We definitely want to be here to chronicle the adventure and the arrival of post-quantum cryptography.



LEO:  Well, and I know your focus is crypto, of course.  But you mentioned it, you know, in glancing.  We are at a kind of already at an inflection for AI.



STEVE:  Yeah.



LEO:  And it's throwing this kind of massively advanced computing against the software we've developed.  I mean, right now it's a software thing running on existing hardware.  We're getting better at the hardware.



STEVE:  Well, running on the fastest available, monstrously crazy GPU arrays.  Now you just drop one rubidium atom on it, and it says, "Well, hello.  How are you feeling?"



LEO:  Well, and I also think that there's a kind of a nice nexus between what is not exactly determined, these are not deterministic computers, these quantum computers.  And AI shouldn't be deterministic either.  We're not deterministic.  And so there's a certain - the fuzziness of it kind of lends itself, I think.



STEVE:  There's a synergy; right.  



LEO:  Yeah.  So honestly, we may think, oh, yeah, crypto, fine.  But we had no idea Hal 9000 was just around the corner, or Skynet, or worse.  And of course if it is security and privacy you're concerned about, remember that big data facility that the NSA built in the Midwest, storing every electronic communication for the last two decades, much of which they couldn't decrypt.  Well, now, thanks to stuff like this, maybe they will be able to decrypt it unless it used perfect forward secrecy.  And with modern analysis techniques, AI-style analysis techniques, extract the juice out of it in new and terrifying ways.



STEVE:  Yeah. 



LEO:  So we are at - we could well be, this could be a watershed.  I think you're right.  We want to go past 999 because I think right now we are in technology watershed that is the equivalent of the invention of the personal computer or the Internet.  Maybe even more significant.  I don't know, is that overstating it?



STEVE:  I don't think that's overstating it, my friend.  No.



LEO:  We live in interesting times, Steve.  And it gives us something to talk about, which is good.  And it gives you something to think about, which is really good.  Listeners, that's, you know, I mean, that's why we think what we do here at TWiT is so important because staying on top of these developments and understanding them, it is why we've always been here, going back, you know, 15 years, 18 years.  But now more than ever I think that it's important that we as citizens kind of stay on top of this.  We can't let Ron Wyden do all the work.  And be able to act rationally around it.



Thank you, Mr. Gibson.  You'll find Steve's work at GRC.com, the Gibson Research Corporation.  That's where SpinRite lives, his bread and butter, the world's best mass storage maintenance and recovery utility available.  Right now it's 6.0.  You did say you were going to have an announcement today.  Okay.  I'm not going to hold you to it.



STEVE:  I did.  I slipped.  Everything last week fought against me.  I did a big posting in my newsgroup to explain what happened.



LEO:  Don't worry.



STEVE:  Definitely next week I'll have an announcement.



LEO:  We don't hold it against you, Steve.  We love you, and we know you're working as hard as you can.



STEVE:  I'd rather get the bugs out now than later.



LEO:  Do it right.  Absolutely.  That's one of the reasons we totally respect you.  Take care.



STEVE:  Bye.



Copyright (c) 2023 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#953

DATE:		December 19, 2023

TITLE:		Active Listening

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-953.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  Is the U.S. ever going to be able to introduce new child protection legislation, or are we going to continue punting to the U.S. Constitution?  2024 means the beginning of the end of traditional third-party cookies in Chrome.  What's the plan for that?  How much did the Internet grow during 2023, and why?  What's the most used browser-based query language?  What's the updated ranking of sites by popularity?  What percentage of total Internet traffic is generated by automation?  Those and many other interesting stats have been shared by Cloudflare.  Then, after catching up with a bit of SpinRite news and some feedback from our listeners, we're going to examine the content of some very disturbing web pages that Cox Media Group originally posted, then quickly removed.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  Can you believe that the FTC wants to fine Facebook $200 billion?  Why Facebook's fighting it, and why some people think maybe they ought to.  We'll also talk about the percentage of Internet traffic generated by automation, and how much the Internet traffic has increased in this year alone.  And then an amazing offer from Cox Media Group to sell advertisers based on the things you say around the house.  Is that possible?  Steve will talk about it and more, next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 953 for Tuesday, December 19th, 2023:  Active Listening.



It's time for Security Now!, the show where we cover your security, privacy online, and we talk about how the Internet works, and we also share some fun stuff about sci-fi and other things with this guy right here, Steve Gibson of GRC.com.  Hi, Steve.



STEVE GIBSON:  Leo, this is the last podcast we will be recording in 2023.



LEO:  End of the year.



STEVE:  And the good news is the podcast itself will not be ending in 2024.



LEO:  Woohoo.



STEVE:  As it was previously planned to be.  But no, by popular demand, literally, we're going to continue.  We're going to answer some questions and look at something I think interesting and kind of horrifying and come away with an action plan for next year.



So is the U.S. ever going to be able to introduce new child protection legislation, or are we going to continue punting to the U.S. Constitution?  2024 means the beginning of the end of traditional third-party cookies in Chrome, finally.  What's the plan for that, exactly?  How much did the Internet grow during 2023, and why?  What's the most used browser-based query language currently?  What's the updated ranking of sites, top 10, by popularity?  What percentage of total Internet traffic is generated by automation?  Those and many other interesting stats have been shared by Cloudflare, and we'll dig into those.



Then, after catching up with a bit of SpinRite news which I had promised last week, I have it this week, and some feedback from our listeners, we're going to examine the content of some very disturbing web pages that Cox Media Group originally posted, then quickly removed from the Internet.  But not before the Web Archive snatched them up, so we still get to see them.  And as always, we have a fun Picture of the Week to share with our listeners.



LEO:  And I will not look, as always.  All right.  I have a picture.  Well, you have a picture.  I'm ready to look at it.  Shall I look at it now?	



STEVE:  Look at it now.



LEO:  Okay.  Schrodinger's Bowls.  I've seen this picture before.  This is a nightmare scenario.  Describe it for us, Mr. Gibson.



STEVE:  Yeah.  So what we have is a cabinet with four panes of glass, so we're able to see what's inside.  And what's happened is, through some catastrophe, maybe an earthquake, we don't know what, a large number of bowls that were once stacked have fallen over inside the cabinet and fallen toward the glass so that, by some miracle, a whole bunch of them have not actually dropped to the bottom because each one in sort of an almost accordion-like fashion is holding onto the lip, the bottom edge of the one below it.  And so here's the point.  This says "Schrodinger's Bowls currently exist in a state of being both broken and unbroken, until the cupboard is opened to determine their fate."  Because it's very clear you're not opening this cupboard, as long as there's gravity... 



LEO:  You might try.



STEVE:  ...without breaking some bowls.



LEO:  You could try.  You might slide a little paper in there.  Yeah, you're right, they're just going to - something's got to...



STEVE:  Yeah, I mean, maybe like if you had some fancy expandable foam, chemical foam.



LEO:  There you go, that you put on the floor.



STEVE:  That you just squirt in there and then, like, you know.  But basically these bowls, the point is these bowls are already broken.  I mean, they are as good as broken.  But it just hasn't happened yet.  So, yeah.  Anyway, that was a great one.  And Lorrie's son Robert was over yesterday, and he, too, had seen it before.  So it was new to me, but obviously it's been a meme that's been around a while.  So anyway, a little bit of fun.



Okay.  So here's something that's not fun.  Meta is suing the U.S. Federal Trade Commission on the grounds that it, the FTC, does not have the constitutional authority to hold Meta accountable, and the FTC is attempting to fine Meta - get this -  $200 billion.



LEO:  What?  What?



STEVE:  $200 billion over Meta's deliberate and flagrant violation, they say, the FTC says, of the Children's Online Privacy Protection Act - we've talked about COPPA, C-O-P-P-A, before - which requires parents to sign off before websites gather and use personal information from children younger than 13.  The FTC claims that - and yes, Leo, I like did a double-take and went back and verified it was billion with a "B" because $200 billion.  Anyway, the FTC claims that the recent lawsuit filed by 41 states' attorneys general documents that Meta had knowledge that millions of users younger than 13 use its services without having obtained parental permission.



Okay, now, the thing that caught my eye about this is that unbiased observers believe that Meta's argument likely has merit.  Even ex-FTC guys are saying, oh, the FTC's probably going to lose this one; and also that the U.S. Supreme Court may side, is likely to side with Meta if the dispute reaches our highest court.  So this is another of those issues - and yes, they're piling up - where what's really needed is for U.S. lawmakers in Congress to make some laws, in lieu of continuing to over-leverage and over-interpret the U.S. Constitution.  Not surprisingly, the U.S. Constitution, largely written in 1787, which was before two paper cups connected by a string was invented, offers little guidance on the issue of age-based IP-driven website content filtering.  What we need are some laws here.



But that turns out to be a problem, too.  We've spent some time looking at the UK's attempt to force some means of monitoring all encrypted messaging.  And the future of the EU's legislation to force browsers to accept their countries' individual certificate authorities without question remains unclear.  The clear pattern here is that technology rarely seems to line up with what lawmakers want.  Unfortunately, that never appears to deter them or to make them want it less.  They just become more insistent that they ought to be able to have anything they want.



So along those lines, we have a mess brewing here now in the U.S. that Meta's pushback against the FTC brings into sharp focus.  It promises to create another challenge that technology may not be able to deliver, and this happens every time we ask the Internet's amazing array of technologies to do something it was never designed to do.  Legislators are talking about wanting Internet content providers to protect young people in the U.S. by blocking content based on age.  And even if that was all they wanted, it's unclear how technology could provide that.  But there are other legislators who believe that some young people must continue to have unfettered access to content that's inherently controversial.  So we can't even agree on what we want, even if the Internet technology could provide it.



Suzanne Smalley is a reporter for The Record.  She's been following this developing story.  Her most recent installment last Monday covered this Meta pushback against the FTC, which may actually, as I say, the FTC may actually lack the legal grounds for their attempted regulation of Meta.  But back in July, Suzanne posted a piece in The Record which captured the heart of the issue.  Under the headline "Sweeping and controversial children's digital privacy bills" - plural, bills - "head to the full Senate."  After my light editing for the podcast, this is what Suzanne wrote.



She said:  "Two bills designed to bolster children's privacy and safety online advanced in the Senate on Thursday" - again, that's a Thursday back toward the end of July - "after months of infighting between children's advocacy organizations and technology civil rights groups over what the latter see as problematic freedom of speech and privacy concerns in the legislation.  Despite the mixed views, Commerce, Science and Transportation Committee voted to advance the bills, known as the Kids Online Safety Act" - okay, so this is a new one we haven't spoken of before, KOSA - "and the Children and Teens Online Privacy Protection Act."  That's COPPA 2.0, so we're going to update that, hasn't been touched since 1998.  She wrote:  "The latter updates an original COPPA bill passed in 1998, which is considered the first and only major federal privacy legislation.



"Committee Chair Senator Maria Cantwell cheered both bills' success" - that is, you know, in getting to the Senate, at least - "saying of COPPA 2.0 that children and teens can be 'overwhelmed with the complexities of online content that is manipulated and targeted at them.'  She said the bill strengthens protections and closes loopholes while ensuring data of children under age 17 is protected more rigorously.  President Joe Biden, who discussed the need for stricter children's privacy laws in his State of the Union address, urged the committee to approve the bills earlier this week, but there are questions about their potential to become law, particularly since there are no House versions.



"COPPA 2.0 changes the existing law to require online services to stop collecting data from kids under age 17," which raises it from the underage 13 of COPPA 1.0.  "KOSA [that K-O-S-A law] is far more sweeping and requires platforms to filter content directed to users under age 17 in the name of preventing, for example, suicide and anorexia.  While celebrating the progress of KOSA, Cantwell acknowledged the profound concerns in the free speech and technology civil rights communities about how the bill would block vital LGBTQ content from older teens.  Acknowledging the advocates' concerns, Cantwell said, 'we will continue to work with them.'  KOSA, the more politically charged of the two bills, is supported by a range of children's privacy groups and larger organizations devoted to children's mental health, including Common Sense Media, the American Psychological Association, Fairplay and the American Academy of Pediatrics.



"A letter to senators signed by more than 200 groups pointed to troubling statistics that advocates say are directly tied to the broad freedom of access children have to online content and the uncontrolled and often profit-driven behavior of companies pumping it out to them.  The letter highlighted that depression rates in teens doubled from 2009 to 2019 and cited a similar doubling of eating disorder emergency room admissions for teen girls from 2019 to now."  So just in four years that doubled.



"More than 90,000" - nine zero zero zero zero - "90,000 pro-eating disorder accounts with 20 million followers appear on Instagram, the letter said, with Meta earning an estimated $230 million annually from such accounts.  The letter signed by more than 200 groups said:  'After numerous hearings and abundant research findings, the evidence is clear of the potential harms social media platforms can have on the brain development and mental health of our nation's youth, including hazardous substance use, eating disorders, and self-harm.'



"The founder and CEO of Common Sense Media, which focuses on children's privacy and safety online, echoed the letter's assertions Thursday and highlighted the outdatedness of current laws governing children's use of technology, saying the bipartisan group of sponsors were 'doing their part to bring tech policy into the 21st Century.'  A large number of freedom of expression and data privacy groups, including the Electronic Frontier Foundation, the Center for Democracy and Technology, and the ACLU, have lobbied hard against KOSA in particular, saying the costs it imposes to address children's online safety are too high.  KOSA would mandate parental consent when children under age 13 create online accounts and require providers to give the parents of these children the ability to change privacy settings.  As a result, advocates say, children will be forced to tell their parents which sites they visit.



"They point to the bill's inclusion of a so-called 'duty of care' provision, which they say creates an obligation for online service providers to prevent harm to minors under age 17.  But in doing so, the bill's broad language will effectively block a wide range of important information, including about mental and reproductive health, LGBTQ issues, and substance use dependency support, they say.  The requirement to 'prevent' harm is extreme, advocates say, and will lead to extensive and often ineffective content filtering.



"The bill will also likely trigger an overreaction from online content providers who Emma Llanso, director of the Free Expression Project at CDT, said will block far more content than necessary over liability concerns.  Llanso also criticized how the bill would give civil enforcement power to uphold the law to states' attorneys general, many of whom she said have extreme views on reproductive care and LGBTQ rights.  At a time when many states are already seeking to block information about gender affirming and reproductive health care, she said, the bill 'puts the most vulnerable young people at a serious disadvantage, facing harassment and consistent targeting of their speech or the speech of people who might be resources or lifelines for them.'



"KOSA inserts itself into the parent-child relationship while ignoring minors' privacy, constitutional and human rights access to information, according to Cody Venzke, who is senior policy counsel for Surveillance, Privacy, and Technology at the ACLU.  Venzke said:  'KOSA has created a blunt technological veto over minors' right to learn, explore, and speak.'  Advocates also have argued that age verification requirements in both bills would undermine adult and children's privacy.



"In a recent blog post, a CDT policy analyst wrote that because KOSA proposes having online services 'limit by default' minors' ability to communicate with other users  a provision that can't realistically be applied to adults  it will be impossible to separate adults and children without asking for identification, which could include birth certificates or even facial scans.



"The large number of children's health organizations pushing KOSA say years of failure by social media companies to protect children and adolescents from harmful effects is what prompted the bill's 'duty of care.' The bill's provision for substantial parental controls will create a far safer digital environment, they say.  Citing the 90,000 Instagram accounts promoting eating disorders, the Common Sense Media's Technology Policy Counsel said currently many platforms are sitting idly by, continuing to profit off a bubble like that.



"The Counsel acknowledged that content filtering isn't perfect, but said KOSA can be refined over time.  In the meantime, she said, under the new law, policymakers will learn more about how online providers' algorithms work, which they can leverage to better protect kids.



"Meanwhile, over on the COPPA 2.0 side, many advocates worry the COPPA 2.0 bill would undermine privacy for substantially more people because it will be less clear who is a child when data collection bans apply to users as old as 16."  Previously it was 13.  "As a result, age verification will be required from a larger number of people, they say.  With users under age 13, content filtering is easier to do.  But 16-year-olds use most of the Internet, making age verification much more sweeping and problematic."



And finally, "Eric Null, who is co-director of the privacy and data project at CDT, said, as with KOSA, COPPA 2.0's imposition of an implicit society-wide need for identification could quite possibly lead to the platforms requiring photos of all users' faces.  The CDT blog said the bill is poorly designed, pointing to how the bill's 'verifiable parental consent mechanisms' in some cases allow any adult to provide consent, which would make the law easy to circumvent and meaningless.  Null said, 'A big issue from our perspective is that when you raise that age limit, the number of websites that have to verify age of all their users skyrockets."



Okay.  That was long, but I think it was important to, like, lay out the fact that this is not going anywhere.  The UK and the EU are certainly not alone in facing challenges created by the Internet.  Here in the U.S., as I said, what I just shared clarifies why we don't have legislation around this.  It's not for any lack of recognition that problems exist.  The problem is that there's zero consensus about what the problem is.  Half of our legislators and action groups want to protect children from content they consider to be harmful, while the other half feels just as strongly that those same children need protected private access to exactly that same controversial information for their benefit.



The way things are currently balanced in Congress, I don't think we need to worry about anything happening in the way of new legislation anytime soon.  It should be clear to everyone why Congress is deadlocked over this legislation.  And as I noted earlier, Suzanne's updated reporting suggests that the FTC may have broadly overreached and overstepped and that Meta, whose size certainly enables it to defend itself, may prevail in the FTC's attempts to rein in, you know, various parties, including Meta's behavior, with lawsuits and stunning monetary fines.  $200 billion is money worth fighting over.



LEO:  Yeah, I mean, they're only worth 900 billion.  So that's a quarter of their CapEx.  That's crazy.



STEVE:  It's crazy, yes.



LEO:  So I think the real issue is that it's a new medium, and legislators or people in the EU are trying to regulate it like broadcasting.  But it doesn't work that way.  And you're trying to attack at the wrong end of the funnel, at the big fat end.  Ultimately it's got to be parents.  They're the only people who can - once it gets out of the house, there's nothing you can do.  And trying to regulate the Internet like television is nuts.  It's just not going to - it doesn't work.



STEVE:  Well, and so the good news is, I completely agree, the good news is it's very clear this country today is not going to pass this legislation.  



LEO:  Yeah, but it's sad when we have to rely on the dysfunction of our elected leaders to protect us.  That's, you know, that's not ideal, shall we say.  And what happens if we get an effective Congress; you know?



STEVE:  Well, and so if we had that happen, then we have the problem of implementing the technology.



LEO:  Yeah, you just can't do it.  That's the problem.



STEVE:  No.  It's just not like, again, just as in the UK and in the EU, if some law was passed that said, you know, you must verify the age of anyone 16 or younger, okay, how would you like us to do that?



LEO:  Yeah.  States have passed laws like that for pornography.  And Pornhub just pulled out, said look, anything that we could do to verify ages would be such an invasion of privacy that we're not willing to do it, and you probably wouldn't want us to do it.  It's just, but legislators are - I don't think they're dumb.  I think they're just angling for votes.  And this is a thing that can get votes.  Protect the children always gets votes.



STEVE:  And I would argue, too, that they often aren't concerning themselves with the how of doing things; right?  



LEO:  Yeah, that's true, yeah.



STEVE:  You know, it's just like, well, you know, you geniuses in Silicon Valley...



LEO:  You figure it out, yeah.



STEVE:  You seem to be able to do anything.  So just, you know, solve that problem.



LEO:  We can regulate TV.  We can keep stuff that minors shouldn't see off television because it's a small group of people, and you can do that.  But you just can't do it with the Internet.



STEVE:  Or the time that it's broadcast during the day, once upon a time.



LEO:  Right.  You make laws, and you can regulate that.  But the Internet isn't like that.  And it's not - they think Meta is like NBC.  And that's not how it works.  And they've just got to get some sense in their heads.  I mean, but it's true.  That means you can't protect children from stuff they're going to see online.  So that's why ultimately the only people who can are parents.  



STEVE:  Yeah.  Yeah.  The recent developer blog discussing the release of Chrome 120 included a little blurb that reminded me that it's about to be 2024.  The blog wrote:  "And a reminder that Chrome is working toward deprecating third-party cookies.  In January" - meaning two weeks from now - "an experiment begins that could affect your website, so it's important that you check."  And they provided a link to an article titled "Preparing for the end of third-party cookies for auditing and mitigating steps."  On that page they wrote:  "If your site uses third-party cookies, it's time to take action."



Actually it would have been time a while ago because, you know, two weeks.  "It's time to take action as we approach their deprecation.  Chrome plans to disable third-party cookies for 1% of users starting in Q1 of 2024 to facilitate testing, and then ramp up to 100% of users by the third quarter of 2024.  The ramp up to 100% of users is subject to addressing any remaining competition concerns of the UK's Competition and Markets Authority (CMA)."



Okay.  So what Google is referring to here is that it appears that the UK's government Competition and Markets Authority has expressed some concern on behalf of UK advertisers that they might be materially damaged by Google's removal of third-party tracking cookies from Chrome.  Oh, gee.  So this appears to be the sort of nonsense that any global technology behemoth such as Google just needs to put up with as part of doing business.  Anyway, nothing will deter Google from doing this, and that's good news.



Their posting continues:  "Our goal with the Privacy Sandbox is to reduce cross-site tracking while still enabling the functionality that keeps online content and services freely accessible by everyone.  Deprecating and removing third-party cookies encapsulates the challenge, as they enable critical functionality across sign-in, fraud protection, advertising, and generally the ability to embed rich, third-party content in your sites.  But at the same time they're also the key enablers of cross-site tracking.  In our previous major milestone, we launched a range of APIs providing a privacy-focused alternative to today's status quo for use cases like identity, advertising, and fraud detection.  With alternatives in place, we can now move on to begin phasing out third-party cookies."



Okay.  So as we know, Google's replacement which will allow advertisers to obtain some weak "interest" categories about visitors is called TOPICS, T-O-P-I-C-S.  We've talked about it here several times, and it's a terrific solution.  So 2024 will finally be the year when third-party cookie behavior is changed for the better.  It won't be that a third-party site cannot still place a cookie into a user's browser.  They can.  But that same third-party site will not be able to retrieve that same cookie when that visitor is at any other site.  And that's a huge change in behavior.  Firefox led the way with this more than two years ago when with Firefox 86 in February of 2021 they introduced Total Cookie Protection.  Back then it was present, but not enabled by default.



Two years later, in April of this year, it went live and was enabled by default.  And, you know, the world as we know it did not end.  Everything kept working.  All that happened was the addition of cookie storage partitioning.  Historically, all web browsers maintained one single global cookie jar, which held all the cookies being stored by the browser.  This was the single fact which made tracking possible, since any advertiser offering content to multiple websites would receive their same tracking cookie, no matter where the user traveled.



But with the adoption of Firefox's Total Cookie Protection, each website effectively gets its own private cookie jar which stores any cookies that anyone wants to set while the user is at that site.  But once the user changes to any other site, that site's cookie jar then becomes current.  So while advertisers are still welcome to set any cookies they want at every site, all cookie linkage between sites is then broken.



Google certainly already knows that catching up with Firefox in this regard won't end the Internet.  They understand that turning this on for 1% of users next month is going to be just fine.  But at the same time there's no arguing that this really does represent a significant change to the way browsers have ever worked by default.  Firefox did it a couple years ago.  Since April it's been on by default.  Everything kept working. So it's reasonable, I think, for Chrome, you know, the elephant in the room browser-wise, to be sticking a toe in the water at 1% before jumping in headlong, which they're going to do by the time we get halfway through 2024.



So anyway, we know that change often needs to be forced.  If anyone is still, like holding onto some need for third-party cookies to be global across browsers, you know, that's got to end.  Chrome is saying, hey, we're not kidding about this.  This change is coming.  You need to make sure that this isn't going to be breaking anything weird that you might be doing.  So yay for that.  And it's going to end up, you know, once Google has TOPICS and third-party cookies are sequestered within their own individual cookie jars, Chrome is going to then be able to continue blocking, tracking, and stopping it wherever they can.



And as we've also just seen with GPC and Do Not Track, you know, Global Privacy Control and Do Not Track, we're beginning to have legislation that's going to be enforcing this, too.  So I think in the not-too-distant future we are truly going to be seeing a different world where browser-based tracking is no longer happening the way it has historically.



The last piece of news, because there actually wasn't a lot that happened this week, that I want to share before we get to my update on SpinRite and some feedback is Cloudflare's summary of interesting statistics which they gathered over the course of 2023.  This is their fourth annual review of Internet trends and patterns which they've observed throughout the year at both the global and also at the country and regional level.  So probably the metric which most surprised me was that global Internet traffic grew 25% in 2023.



LEO:  Wow.



STEVE:  I know.



LEO:  That's a shock.



STEVE:  It is astonishing.  They noted that major holidays, severe weather, and intentional shutdowns clearly impacted Internet traffic.  We'll talk of intentional shutdowns in a second.  But, you know, remember all that dark fiber we once had during that Internet, that initial Internet overbuild?  I would bet that there's far less excess today than there once was.  But Leo, I agree with you.  Think about that for a minute.  25% growth in Internet traffic in one year.



LEO:  What?



STEVE:  That is a massive increase in something that's already as mature as the Internet.  The only thing I can imagine that might account for that is the continuing increase in the use of streaming media for content delivery.



LEO:  Oh, yes, of course, yeah.



STEVE:  You know?  I had been a happy TiVo user for years, switching from analog TiVo to digital TiVo, but remaining with traditional cable TV.  But then for me, six years ago when I was setting up my new home with Lorrie, I tried an experiment.  We never asked Cox for cable TV, only Internet service.  And I've never looked back.  And so, Leo, that seems plausible to you, too?



LEO:  Yeah, that's - the widespread access to broadband, I mean, really fast broadband, also probably encourages people to do things like stream more content.



STEVE:  Right, that they just weren't doing before.



LEO:  Yeah.  I saw a stat, oh, wonder if I can remember it, that Netflix, how many petabytes of data Netflix sends over a month.  And it's many hundreds of petabytes.  And that's just Netflix.



STEVE:  I have a buddy whose father does nothing but sit on the couch with his phone in landscape orientation watching Turner Movie Classics.



LEO:  Yeah.



STEVE:  You know, that's how he spends his day.  So like, people are watching movies on their phones now.



LEO:  Yeah.



STEVE:  That didn't used to happen.



LEO:  Yeah.  Yeah, exactly.  And, I mean, people have always watched TV all day, but now they're streaming, and streaming a lot, all day.



STEVE:  Right.



LEO:  So, yeah, I guess that must be it; right?  Netflix in 2019 took up 11% of the global downstream traffic on the Internet.  You know.  So I'm sure that's at least that much.  And then add on top of that, you know, HBO MAX, or MAX I guess it's called, Disney Plus, all of this stuff.



STEVE:  Yeah.



LEO:  Yeah, I'm not surprised.  I think it's got to be.  I think you nailed it.



STEVE:  Okay.  Not surprisingly, Google was again the most popular Internet service.  But TikTok, which was the leader two years ago in 2021...



LEO:  Oh, yeah, we're watching [indiscernible] videos, yeah.



STEVE:  Yup.  TikTok fell to fourth place.  The ranking among the top 10, from number one to number 10, is Google, Facebook, Apple, TikTok, Microsoft, YouTube, AWS, Instagram, Amazon, and iCloud.  And I have to say I'm a bit surprised that Apple's domain is in number three position, above TikTok in number four and YouTube in number six and Instagram in number eight.  You know, like what's Apple doing?  Is it just Apple TV?  I'm really surprised by the...



LEO:  No, because think about it, I mean, all the downloads of apps, you know, the App Store all comes from Apple.  The music, Apple's the number one streaming music service.



STEVE:  Ah, okay.



LEO:  All that music.  I mean, Apple's doing a lot of streaming, actually, if you think about it.  And I guess when you buy a movie on iTunes the stream comes from Apple, too.  So there, you know, yeah.  I'm not surprised.



STEVE:  Yeah, yeah.  Wow.  Wow.  As we know, OpenAI was the most popular service in the emerging Generative AI category, and Binance remained the most popular cryptocurrency service.  On the mobile front, also no surprise, over two-thirds of all mobile device traffic was consumed by Android devices, with Android commanding a greater than 90% share of mobile device traffic in over 25 countries or regions.  So there were places where Android was two-thirds and iOS was one-third.  And there were countries where it was almost all Android.  So, I mean, like, you know, Bangladesh, for example, was like, you know, all - there was hardly any iOS activity there.  



And in the skies above us, the global traffic from Starlink nearly tripled in 2023.  After initiating service in Brazil in mid-2022, Starlink traffic from that country jumped by more than a factor of 17 in 2023.  So I guess anywhere where you're connectivity challenged, and Starlink is offering service, they're going to see a big jump in Starlink traffic in that region.  So, but again, Starlink up by a factor of 3.



On websites, Google Analytics, React, and HubSpot were among the most popular technologies.  And worldwide, nearly half of web requests are now using HTTP/2, with 20% using HTTP/3, and the balance, around 30%, still back at /1 or 1.1.



NodeJS was the most popular programming language used for making automated API requests by browsers to backend servers.  Cloudflare noted that as developers increasingly use automated API calls to power dynamic websites and applications, they're able to use their visibility, that is, Cloudflare is able to use its visibility into web traffic, so they're able to see what's going on.  You know, they're often serving as a proxy in front of web services.  So they're able to identify the languages and the APIs that the clients are written in.  So beyond NodeJS, which holds the number one spot at 14.6%, the ranking in descending order behind NodeJS is Go at 8.4%, Java at 7%, Python at 6.8% and .NET at 4.3%.



And during 2023, Googlebot was responsible for the highest volume of request traffic to Cloudflare's hosted and their proxied sites.  Not surprisingly, Google's busy sucking down web content in order to keep its indexes current.  



As for Internet connectivity & speed, Cloudflare saw over 180 Internet outages globally during 2023, with many they said deliberately created by government-directed regional and national shutdowns of their own Internet connectivity.  And we've talked about this happening before where for various reasons, like on voting day, various countries will say, okay, pull the plug, we don't want anybody on the Internet while we're doing whatever it is we're doing.  So 180 times.  So basically, what, one every other day on average.



Only one third of IPv6-capable requests worldwide were made over IPv6.  So even though IPv6-capable servers are still relatively rare, among those services that do support IPv6, two thirds of the queries they received were to their IPv4 addresses, not taking advantage of IPv6.  And that's not really that surprising at this point.  It's going to be a while.



The top 10 countries all had measured average download speeds above 200 Mbps on average, that is, the average user among the top 10 countries had speeds over 200 Mb, with, interestingly,  Iceland showing the best results across all four measured Internet quality metrics.  You ask why?  Well, the reason for Iceland's outstanding performance is that over 85% of all Internet connections there are over fiber.  So they've got just a base of really good Internet connectivity in Iceland.



Over 40% of all global traffic is exchanged with mobile devices.  40% is now mobile.  And in more than 80 countries and regions, the majority of all traffic is exchanged with mobile devices.  And I guess that's no surprise, again, in countries that are basically just mobile users instead of desktop connectivity.



And finally, on the security front, just under 6% of global traffic was mitigated by Cloudflare's systems as being potentially malicious or for customer-defined reasons.  So 6% of all global traffic was blocked.  In the United States, 3.65% of traffic was mitigated; while in South Korea, it was 8.36%.



A third of global bot traffic comes from the United States, and over 11% of global bot traffic comes from Amazon Web Services.  And just to remind everybody, they don't mean "bots" as in malicious bots.  Cloudflare means anything that's automated.  So any and all legitimate Internet-indexing bots would doubtless be a hefty part of that total of 11%.



But on the malicious front globally, finance was the most attacked industry, but the timing of spikes in mitigated traffic and the target industries did vary widely throughout the year and around the world.  So it's not only finance that is being attacked.



Even though the two-year-old Log4j vulnerability remained a top target for attacks during 2023, the HTTP/2 Rapid Reset attacks, which we covered a few months back, emerged as a significant new vulnerability, beginning with a flurry of record-breaking attacks.  And as we know, they did cause a little bit of stumble on Cloudflare's systems until they were able to update them in order to mitigate the attack.



And get this:  1.7% of TLS 1.3 traffic is already today using post-quantum encryption.  So we're beginning to get there.  1.7% won't get us there; but still, TLS 1.3 does offer some of those protocols, and they're beginning to get used.



In malicious email messages, deceptive links - in other words, phishing - and extortion attempts were the two most common types of threats people received.  



And finally, the good news is that PGP-style, I mean, why can't I say this, BGP, whew, BGP, yes, Border Gateway Protocol everybody, BGP-style Internet routing security, measured as the share of valid routes, improved globally during 2023.  Significant improvement in routing security was observed in countries including Saudi Arabia, the UAE, and Vietnam.



So stepping back, overall, no big surprises.  And overall, when viewed from 20,000 feet, the Internet remains largely stable.  It's cool to see the emergence of some post-quantum crypto protocol usage.  And I'm still surprised that there could be year-over-year growth of 25% in overall traffic.  That just - that's a big bump.



Okay.  The announcement I expected two weeks ago to be able to make last week, which Leo, you noted I missed, it was Don Adams on "Get Smart" who used to say "Missed it by that much."



LEO:  "Missed it by that much."



STEVE:  The announcement was that I finally accomplished the surprisingly challenging task of on-the-fly remote server-side EV code signing using a hardware security module.  It's a capability I've wanted to have for years.  And although it took far more time than I expected, since every aspect of the project fought back, that technology now exists, is now in place, and it appears to be working reliably.  I brought it online last Saturday morning for cautious testing by the gang in GRC's development newsgroup.  And I checked an hour ago.  More than 165 instances of SpinRite's current release candidate number five were successfully built, on-the-fly signed, downloaded, and tested.



I decided to obtain a fresh three-year, which is the maximum you can get, an EV code-signing certificate from DigiCert for the HSM, since the one I had was a year old, and I wanted to get as long a run from the appearance of this new certificate as possible.  As we know, reputation is now the way the world works.  Not surprisingly, a few people reported that Windows Defender was upset, quarantined, and deleted their download.  But most people said that Defender didn't make a peep, and that everything worked for them without a hitch.



My hope is that by the time SpinRite 6.1 is being heavily downloaded, Microsoft will have had time to decide that all is well with it.  I just grabbed a fresh copy and dropped it on VirusTotal.  It triggered zero out of 68 tests.  Not one AV engine had any problem with it.  But during my testing I tried that with unsigned code, and at least one-third of them freaked out.  So I conclude that this was time well spent to get this thing signed on the fly.



Where I am today is that SpinRite is all but finished.  I'm actually a bit happy that the code-signing project took four weeks because during that time many more people have had the chance to use the current release candidate.  This has surfaced some remaining edge cases, not really in SpinRite.  Well, I'll give you an example.  And I would like to resolve those before I formally declare SpinRite 6.1 finished.  I could ship it now.  But on really troubled drives we've seen that it can still stumble a bit.  And since really troubled drives is definitely one of SpinRite's targets, I want to at least know that there's nothing more that can be done.



For example, there have been some reports of very troubled drives dropping offline when SpinRite touches a particularly sensitive spot on the drive.  This is something we encountered many times during our early testing.  SpinRite pops up a scary red dialog to explain that after being reset, the drive never reported that it was ready for more.  SpinRite waits for up to 10 seconds while checking the drive's status every 10 milliseconds before it makes that determination.  And if you watch the seconds tick by on a clock, 10 seconds is a long time.  But over this past weekend I verified that for really troubled drives, 10 seconds might not be long enough.  In this instance, SpinRite needs to be even more patient.



So starting with the next release, SpinRite will give drives a full 60 seconds to get themselves back up and ready to proceed.  And since SpinRite's user may wonder what's going on while nothing appears to be happening, after waiting a few seconds, SpinRite will display a "waiting for drive" and then a countdown, so that its users know why nothing appears to be happening.  Anyway, so it's those sorts of things.  Really, I mean, really just the last little bits.



But, you know, since we've so close to the end of '23, and I'd like any recent improvements to have a bit of time protesting, I figure I'll fuss with SpinRite for the rest of the year, and make it available at the start of 2024.  I recognize that even after its official release, I may still be tempted to deal with even edgier edge cases.  And, frankly, any improvements I'm making at this point all feed forward into SpinRite 7's design anyway.  So it'll be time well invested.  I have the feeling that I may be continuing to nudge it along for a while, but also in this day and age it's very easy for any SpinRite owner to update themselves and get the latest and greatest.  And of course I'll let everybody know here when that happens.  So at the moment there's no reason for me not to improve what I can, and it'll be an early '24 release.



LEO:  Perfect.  And that's no very far from now.  That's just a week or so.



STEVE:  No, it's two weeks.



LEO:  Two weeks.



STEVE:  And that'll give me some time to add those little finishing bits of polish.



LEO:  Me, too.  That'll give me time to answer Day Five of Advent of Code.



STEVE:  That's right.



LEO:  Well, it all comes down to Allen's interval algebra, actually, Steve.  You would have understood it immediately, but it was a little beyond my tiny brain.  We have some very good programmers in - there's an Advent of Code section in our Club.  And I don't know, there may be a dozen people solving it in the Club, which is kind of cool because then you can say, hey, what the heck's going on here, and people are really great.  It's really - it's a lot of fun.



STEVE:  That is neat.



LEO:  Yeah. 



STEVE:  So we have some good feedback, as always, from our listeners.  Someone who asked for anonymity said:  "I'm not going to pretend to understand the details of this, but @SGgrc and other authorities have deemed this a major step forward in quantum computing."  And apparently he aimed this at @bitcoincoreorg.  This was a public tweet.  So he asked @bitcoincoreorg, "What is the plan for post-quantum implementation?  Current asym crypto is threatened."



So this listener was clearly referring to last week's "Quantum Computing Breakthrough" topic.  And he's correct that asymmetric crypto using the algorithms in use today will not be in use in the future.  The good news is that things like iterative PBKDF hashing of passwords to obtain a fixed-size token are not asymmetric, so they will remain safe.  I'm mentioning this,  first because it's something just by itself that's important to appreciate.  The world's password-accepting websites will not all need to revamp their password hashing systems.



But also, and blessedly, neither will the operation - he's referring to @bitcoincoreorg - neither will the operation of the Bitcoin Blockchain need to be changed.  Recall that the way bitcoins are earned is by guessing what needs to be appended to the most recent blockchain update in order to yield a hash result that ends in some number of trailing zeroes.  While GPUs have proven to be quite facile at performing the hash function needed to guess that at very high rates, choosing a random value and hashing the result is not something that is suited to tomorrow's quantum computers.



And thank goodness for the fact that symmetric crypto will not be affected by quantum computing because otherwise we would be in much more serious trouble than we are now.  You know, as it is, we've got to abandon all of our asymmetric crypto.  We have time to do that, but we really do have to get it done.  Still, underlying all of that is symmetric crypto.  And as I said, it does not need to be changed.  It gets to stand unmodified.



Philip Griffiths tweeted:  "Hey, Steve.  In 952 you mentioned ZeroTier and Tailscale as open source.  Well, 'sort of' is my opinion.  ZeroTier is BSL, so open source to many, but not everyone.  Tailscale is largely open source; but core parts, for example, the coordination server, are not.  NetBird or Headscale would be better examples of overlays which are open source software and allow circumvention of NAT.  You could also include OpenZiti, though you could also argue, while it can be better used as a VPN, its true design goal is to make it easier to build secure-by-default distributed apps."



Okay.  So although he's right, much of Tailscale is open source, Tailscale have retained some pieces.  The GUI clients for Windows, macOS, and iOS are not open, and that rendezvous server, the so-called coordination or control server.  So Headscale, which he mentioned, implements a self-hosted, fully open source alternative to Tailscale's control server.  Headscale's goal is to provide self-hosters and hobbyists with an open-source server they can use for their projects and labs. It implements a narrow scope, essentially a small Tailscale overlay network suitable for personal use, or small open-source organizations.



NetBird looks like another interesting, fully open source overlay network, which does also use Wireguard for its transport security and encryption.  OpenZiti is not a general purpose overlay network.  It's a system of technology, language APIs, and SDKs that allow developers to incorporate secure overlay network technology into their apps.  So it's a completely different bear than Tailscale, ZeroTier, or NetBird.



Now, as for how those three compare, I'm unable to offer any comparison or recommendation from experience.  I haven't yet had the need to deploy any of them, so I haven't given them a close look.  But when I do I will definitely share a full review of what I find.



Comm Tech Engineer sent a question.  He said:  "A quick question on SpinRite.  Do the hardware specs (better CPU or more RAM) of the PC running SpinRite affect the speed of SpinRite's operations?"



And I've seen this question several times before, so I thought I'd answer it for everyone.  The answer is almost always no, the hardware does not matter.  SpinRite will detect and alert its user when a SATA III, which is capable of - it has an interface rate of 6 Gbps - when a SATA III drive is connected to a SATA II interface, which is only capable of running at 3 Gbps.  However, even though 3 Gbps is sufficient for any spinning drive, there are some solid-state drives which can benefit from SATA III speeds.  So the idea there, and actually it happened to a number of our testers who were caught off guard, they did not realize they had a SATA III drive plugged into a SATA II interface.  So SpinRite pointed that out for them, and they were able to move them over to SATA III interfaces and increase their performance.



As for CPU and RAM, SpinRite 6.1 does use about 50MB of RAM, but it's rare to find a machine today that has fewer than 4GB, and even 1GB would be fine.  Not one, as far as I know, of our more than 800 testers have encountered anywhere in their testing a machine with insufficient RAM for SpinRite 6.1.  So I just don't think that's going to be a problem.  And the same is true for the CPU under 6.1.  What that means essentially is that almost any older hand-me-down PC can be used as a dedicated SpinRite test machine, and many of SpinRite's owners do just that.



Even though 6.1 is way faster than 6.0, and 6.1 is able to run a directly connected drive as fast as that drive is capable of going, drives have become ridiculously huge, and it does still take time to move all that data back and forth.  If you've ever tried to do an actual physical format on a drive today, as opposed to a quick format, you quickly learn that that's not something you want to do, at least not often.  Anyway, so almost certainly the speed of the system and the amount of RAM it has doesn't matter.



I should mention there was one instance where somebody was considering purchasing a machine and was wondering about SpinRite.  7.0 will be more aggressive.  Among other things, it'll be fully multitasking, meaning you'll be able to run it on all the drives you have in the system at once, and the technology it's going to be using for spotting slow spots is going to be much more CPU intensive.  So there the answer will probably differ, but not for 6.1 that we have today.



Another listener said:  "I'm listening now to SN-950 and wanted to share my experience with trying Passkeys on GitHub.  Before Passkeys, I had username, password, and multifactor authentication."  And by that he means an external authenticator app.  He said:  "After enabling Passkeys and using them as the authentication method, GitHub no longer prompts for another factor, so it seems the security of multifactor authentication has disappeared."  He said:  "I store my MFA seeds outside my password manager.  If, however, I were to store my Passkeys within my password manager, I would then reduce my security for any site that skips MFA when using Passkeys.



"I understand," he said, "that Passkeys are better than passwords as the site no longer has a secret to protect.  But all the necessary 'eggs' would still be in my password manager's basket.  I think Passkey plus MFA would provide the highest level of security, but I don't know which sites will allow or offer that option, and which will drop multifactor authentication while using Passkey authentication under the assumption that multiple factors are no longer necessary, as GitHub appears to assume.  For those with the strictest threat model, I think I would not recommend storing Passkeys in the same password manager as everything else.  Am I missing something here?



"Also, I never use Twitter myself, so I'd be one of those looking forward to your new email list.  I had to dust off my login from years ago just to write this to you."



Okay.  So like this listener, I'm a bit distressed to learn that the use of Passkeys automatically disables the use of additional authentication factors, especially for a site such as GitHub where improving authentication integrity has been a recent need and issue.  As we know, Passkeys are stronger than usernames and passwords for a number of reasons.  But this listener is correct about the vulnerability inherent in allowing any single device to have authority to authenticate us, no matter how strong its authentication mechanism might be.



The right solution would be to offer users who are adopting Passkeys the option to disable their additional authentication factors in favor of Passkeys since Passkeys are certainly stronger than passwords alone; or to keep additional authentication factors in place and enabled under the more proper understanding that any single factor of authentication, regardless of how strong it may be by itself, can still be made more strong by the requirement for an additional factor, especially one that relies upon another device and uses an entirely different technology such as time-varying six-digit tokens.



SKYNET said:  "Hi, Steve.  Regarding SN-952 and upstream library dependencies, what is this notion that developers have that fixing a bug will automatically introduce something new that will break?  How is it not possible to fix a flaw without introducing a new feature that will break something else?  It seems that everyone, including Microsoft, cannot fix a flaw or vulnerability without breaking something else.  Why are they so connected, and how?  How is it that a bug or flaw that needs to be fixed will just automatically break something totally different, totally unrelated to the bug?



"I know it is most certainly possible for companies to fix flaws without breaking something new in their products, so I find it a poor excuse for developers to claim that 'It's all working right, so I don't want to rock the boat and possibly break something else.'  To me," he writes, "that just screams that either everything in their code is connected" - that's bad - "or they're bad coders."



Okay.  So last week's discussion more specifically related to a fear of updating upstream library dependencies, which are inherently black boxes, in a situation where the successful functioning of a coder's own code is entirely dependent upon the exact behavior of those black boxes.  The point being, everything is working now.  Let's not rock the boat.



In highly complex projects, mistakes do happen.  Say that a group of people decide to entirely recode some library because its codebase has grown old and creaky over years of tweaking.  That does happen, and recoding can be a really good thing to do.  But try as they might, it could be that this new code, which is intended to behave just the same as the original code did, nevertheless exhibits some slightly different behavior around the edges; and that this change, even if it's subtle, might cause some other code that uses the newly recoded library to break.



Yes, it's a mess; but it's a mess we've created.  And the motivations behind the "don't fix it if it's not broken" is, I think, understandable.  As a programmer, I understand it, and I guess I'm somewhat surprised and thankful that Microsoft has somehow managed to keep really old programs that were running on very different operating systems still running reliably today.  It really is an achievement on their part.



Blaine Trimmell wrote:  "I just listened to the Security Now!  podcast and want to pass on that Chromium browser does not use OS root store anymore by default."  And he provided a link.  Okay.  So Blaine's Twitter DM included a link to Google's announcement about three months ago, which was made on September 19th, under the headline "Announcing the Launch of the Chrome Root Program."  And there Google wrote:  "In 2020, we announced that we were in the early phases of establishing the Chrome Root Program and launching the Chrome Root Store.  The Chrome Root Program ultimately determines which website certificates are trusted by default in Chrome, and enables more consistent and reliable website certificate validation across platforms.  This post shares an update on our progress and how these changes help us to better protect Chrome's users."  Since we all know most of this, I'm not going to share it all.  But they make a couple points that are salient.



They said:  "Chrome uses digital certificates (often referred to as 'certificates,' 'HTTPS certificates,' or 'server authentication certificates') to ensure the connections it makes on behalf of its users are secure and private.  Certificates are responsible for binding a domain name to a public key, which Chrome uses to encrypt data sent to and from the corresponding website.  As part of establishing a secure connection to a website, Chrome verifies that a recognized entity known as a Certificate Authority issued its certificate.  Certificates issued by a CA not recognized by Chrome or a user's local settings can cause users to see warnings and error pages.



"Root stores, sometimes called 'trust stores,' tell operating systems and applications what certificate authorities to trust.  The Chrome Root Store contains the set of root CA certificates Chrome trusts by default.  A root program is a governance structure that establishes the requirements and security review functions needed to manage the corresponding root store.  Members of the Chrome Security Team are responsible for the Chrome Root Program.  Our program policy, which establishes the minimum requirements for CAs to be included in the Chrome Root Store, is publicly available.



"Historically, Chrome integrated with the root store and certificate verification process provided by the platform on which it was running.  Standardizing the set of CAs trusted by Chrome across platforms through the transition to the Chrome root store, coupled with a consistent certificate verification experience through the use of the Chrome Certificate Verifier, will result in more consistent user and developer experiences."



Okay.  So what they're saying here is they used to just use the store that the OS provided.  But because those could vary from OS to OS, that meant that Chrome's behavior varied from OS to OS.  They decided to change that.  So they decided two years ago, actually three, in 2020, to work on incorporating the root store into Chrome and no longer use the underlying OS.



So they finished:  "Launching the Chrome Root Program also represents our ongoing commitment to participating in and improving the Web PKI ecosystem.  Innovations like ACME have made it easier than ever for website owners to obtain HTTPS certificates.  Technologies like Certificate Transparency promote increased accountability and transparency, further improving security for Chrome's users.  These enhancements are only made possible through community collaboration to make the web a safer place.  However, there's still more work to be done.



"We want to work alongside Certificate Authority owners to define and operationalize the next generation of the Web PKI.  Our vision for the future includes modern, reliable, highly agile, purpose-driven PKIs that promote automation, simplicity, and security.  And we formed the Root Program and corresponding policy to achieve these goals."



So again, that was in 2020.  Ninety days ago they announced this went live.  So thank you, Blaine, for this.  I had announced Google's announcement that this initiative was ready and was now being rolled out.  This of course means now that both Google and, well, Google with Chrome, all the various Chromium browsers, and Firefox will be running with their own local root stores.  And given that, as we've seen, just six or seven Certificate Authority root certificates are all that most users will ever need, that doesn't seem like such a big deal.  But all of this is relevant, of course, because of the EU's Article 45, which brings into question, what is going to happen in 2024?  I have a feeling whatever it is, it'll be next year that we see something happening.  So it's going to be extremely interesting.



Okay.  And finally, Active Listening.  A story blew up in the news last week that currently lacks solid evidence.  Well, okay.  Solid evidence only inasmuch as it's so bad that it's hard to believe it's true.



LEO:  And it could very well be that Cox Media Group is overselling their capabilities.



STEVE:  Right.



LEO:  At least I hope they are.



STEVE:  On the other hand, we have documentary evidence, thanks to the Web Archive.  Anyway, whether or not it's true, it has certainly worried and upset everyone who has heard it.  And Leo, I know you talked about it on TWiT on Sunday.  You know, definitely an important topic.  I suspect that this is the sort of thing that investigative reporters will be digging into further.  The short version of the news is that the massive media giant CMG, which is the Cox Media Group, claims in its marketing materials to advertisers and in actual discussions with prospective clients, that it currently has and is using the capability, which their marketing materials term "Active Listening," to listen into the ambient conversations of consumers through microphones embedded in their smartphones, smart TVs, and other similar devices; and that through this means they're able to gather data which they then use to target advertising.



Last Thursday's headline in 404 Media - I need to stop every so often to take a drink, Leo, and I...



LEO:  Take a drink.  I didn't do an ad for you.  Take a drink.  Pause for a little bit.



STEVE:  Last Thursday's headline in 404 Media, which broke the story, was titled "Marketing Company Claims That It Actually Is Listening to Your Phone and Smart Speakers to Target Ads."  404 Media wrote:  "The news signals what a huge swath of the public has believed for years, that smartphones are listening to people in order to deliver ads, may finally be a reality in certain situations.  Until now, there was no evidence that such a capability actually existed; but its myth permeated due to how sophisticated other ad tracking methods have become."



Okay.  So exactly three weeks ago, on November 28th, the CMG website posted a blog page titled "Active Listening:  An Overview."  They also had a permanent page linked off their domain's root with the URL, you know, blah blah blah domain, /cmg-active-listening.  So at that time, three weeks ago, it was all right out there for the world to see.  And this is not some fly-by-night sketchy operation.  This is the Cox Media Group.  To no one's surprise, all of those pages have since disappeared, though it's really worth noting that they were initially completely public, and no one at Cox thought that they nor their Active Listening was a problem.



Fortunately, those pages were up long enough to have been crawled by the Internet's historian, the Web Archive.  And this week's shortcut of the week, grc.sc/953, will take you and your browser directly to a faithful copy of the archived blog posting which was made three weeks ago.



So what do we learn directly from this once publicly posted page?  It does not disappoint.  The page shows a photo of four young hip consumers in their late '20s or early '30s gathered around a table, smiling and chatting with a Mac and a tablet. And the page says:  "Imagine a world where you can read minds.  One where you know the second someone in your area is concerned about mold in their closet, where you have access to a list of leads who are unhappy with their current contractor, or know who is struggling to pick the perfect fine dining restaurant to propose to their discerning future fiance.  This is a world where no pre-purchase murmurs go unanalyzed, and the whispers of consumers become a tool for you to target, retarget, and conquer your local market.



"It's not a far-off fantasy.  It's Active Listening technology, and it enables you to unlock unmatched advertising efficiency today so you can boast a bigger bottom line tomorrow.  Do we need a bigger vehicle?  I feel that my lawyer is screwing me.  It's time for us to get serious about buying a house.  No matter what they're saying, now you can know and act.  A marketing technique fit for the future, available today.



"Machine learning algorithms are improving and introducing a new era for advertising.  Our Active Listening tech gives you a weekly roster of qualified customers who have voiced their need for your service or product.  We will then upload the list to your preferred advertising platforms so you can target ads to the right people at the right time.  Reactive advertising is no longer enough to get ahead.  Embracing predictive and proactive strategies is the key to growth.



"Active Listening gives organizations clarity into the most effective channels and timing for their advertising efforts.  By incorporating and analyzing customer data gleaned from conversations happening around smart devices, we can pinpoint where and when customers are most likely to engage with ads.  When you have this information in reach, you have the power to deploy targeted campaigns at opportune moments on the platforms where your audience spends their time.  The results?  Maximized visibility and impact.  Whether you're a scrappy startup or a Fortune 500, Active Listening makes the unreachable in reach.



"How does it all work?  Advertise to the exact people who need your services.  CMG can customize your campaign to listen for any keywords or targets relevant to your business.  Here's how we do it.  Create personas:  We flesh out comprehensive buyer personas by uploading past client data into the platform.  Identify keywords:  We identify top-performing keywords relative to the type of customers you are looking for.  Transparent tracking:  We set up tracking via pixels placed on your site so we can track your ROI in real-time.



"Leverage AI:  AI lets us know when and what to tune into.  Our technology detects relevant conversations via smartphones, smart TVs, and other devices.  Analyze consumer behavior:  As qualified consumers are detected, a 360 analysis via AI on past behaviors of each potential customer occurs.  Create a list:  With the audience information gathered, an encrypted evergreen audience list is created.  Target, retarget, and transcend:  We use the list to target your advertising via many different platforms and tactics, including streaming TV, OTT, streaming audio, display ads, paid social media, YouTube, and Google and Bing search.  Don't leave money on the table.  Claim your territory.



"Our technology provides a process that makes it possible to know exactly when someone is in the market for your services in real-time, giving you significant advantage over your competitors.  Territories will be available in 10- or 20-mile radiuses, but customizations can be made for regional, state, and national coverage."  And then there's a link with the phrase "Claim your territory now."



Then they provide a handy FAQ where the first question they ask themselves is "Is Active Listening Legal?"  To which they reply:  "We know what you're thinking.  Is this even legal?  The short answer is yes, it is legal for phones and devices to listen to you.  When a new app download or update prompts consumers with a multipage terms of use agreement somewhere in the fine print, Active Listening is often included."  Uh-huh.  So why, exactly, then, has CMG website been totally scrubbed of all mention of Active Listening?  Perhaps strict legality is not the problem here.



The next question they ask themselves:  How Does Active Listening Technology work?  And they answer:  "Our technology is on the cutting edge of voice data processing.  We can identify buyers based on casual conversations in real time.  It may seem like black magic, but it's not.  It's AI.  The growing ability to access microphone data on devices like smartphones and tablets enables our technology partner to aggregate and analyze voice data during pre-purchase conversations.  The result?  Advertising efficiency and timing taken to a new level.  We set specific keywords relevant to your product and service so we know who needs you, why they do, and where we can target them.  With this unprecedented understanding of consumer behavior, we can deliver personalized ads that make your target audience think, wow, they must be a mind reader."  Right.  And that's not creepy at all.



Anyway, 404 Media, the organization that spotted this and broke the story, also found a representative of the company, CMG, on LinkedIn who was explicitly asking interested parties to contact them about the product.  One marketing professional pitched by CMG on the tech said a CMG representative explained the prices of the service to them.  So it certainly appears to have been available.  CMG's website says:  "What would it mean for your business if you could target potential clients who are actively discussing their need for your services in their day-to-day conversations?  No, it's not a 'Black Mirror' episode.  It's Voice Data, and CMG has the capabilities to use it to your business advantage.



"The part of CMG advertising the capability is CMG Local Solutions.  CMG itself is owned by Apollo Global Management, a hedge fund, and Cox Enterprises, which includes everyone's favorite residential cable provider, the ISP Cox Communications.  CMG operates a wide array of local news television and radio stations."



So, wow.  I mean, everybody gets this; right?  They talk about overseeing, you know, on this page they say imagine this, what could it do for your business if you were able to target potential clients or customers who are using terms like this in their day-to-day conversations.  And they give three, or they give six examples.  The car lease ends in a month; we need a plan.  Or overheard, a minivan would be perfect for us.  Or do I see mold on the ceiling?  Or we need to get serious about planning for retirement.  Or this AC is on its last leg.  Or we need a better mortgage rate.



So according to CMG's now-removed web pages, the way this works is that clients can "claim" a territory where they want to use CMG's services, which are available in a 10- or 20-mile radius.  After being set up:  "Active Listening begins and is analyzed via AI to detect pertinent conversations via smartphones, smart TVs, and other devices."  CMG also claims it installs a tracking pixel on its client's website to monitor the return on investment.  With an audience created, CMG then delivers advertisements to these people through streaming TV, audio, display ads, YouTube, and search.



The marketing professional who was pitched by CMG told 404 Media that after a call with the company, they disabled microphone access on much of their own technology.  The guy was quoted saying:  "I immediately removed all my Amazon Echo devices and locked down microphone permissions on things like my phone.  Receiving confirmation that they are doing things like this have confirmed my worst fears; and I, for one, will take no part in it."



For its part, while CMG was busily removing all traces of this from their website, they told 404 Media the following.  They said:  "CMG Local Solutions markets a wide range of advertising tools.  Like other advertising companies, some of those tools include third-party vendor products powered by data sets sourced from users by various social media and other applications, then packaged and resold to data servicers.



"Advertising data based on voice and other data is collected by these platforms and devices under the terms and conditions provided by those apps and accepted by their users, and can then be sold to third-party companies and converted into anonymized information for advertisers.  This anonymized data is then resold by numerous advertising companies.  CMG businesses do not listen to any conversations or have access to anything beyond a third-party aggregated, anonymized, and fully encrypted data set that can then be used for ad placement.  We regret any confusion, and we are committed to ensuring our marketing is clear and transparent."



Okay, now, here's something to think about.  Why is Cox doing this?  They would not seem to be an obvious entity to create such a service.  If, as they claim, all of the data is being aggregated by third parties, and they're just the middlemen who are not doing any direct data gathering themselves, then there's nothing Cox is bringing to the table.  Any random organization could do the same thing.  But they certainly do appear to have been all gung ho into it when they were public about it.  If I were a betting man, I'd put my money on this being an adjunct to all of the massive amount of data that Cox Communications - the Internet ISP - is almost certainly already obtaining from monitoring all of their Internet consumers' available Internet traffic, meaning all DNS query and TLS handshake metadata.



Cox, like any ISP, is sitting on a treasure trove of extremely valuable personal data.  Everywhere everyone in everyone's family goes on the Internet is available to an ISP, unless consumers take extreme measures to prevent it from happening.  And as we know, almost no one does.  And Cox consumers are not anonymous to Cox.  Cox knows exactly who and where every household is.  They pay their bill every month.  And now we know something we didn't directly know before.  Now we know who Cox is and what they're really thinking.  They're quite willing to hide in the fine print of, in their words, "multipage terms of use agreements."



I'd be much less worried about microphones, which at least for our very secure smartphones seems like a red herring, than about the fact that an organization such as Cox has just shown itself to be is the conduit through which all of its subscribers' residential Internet traffic flows.  I think the time has come to think seriously about bringing up encrypted DNS for residential Internet users.



We do not know for sure whether Active Listening applies to audio, or where it applies.  But given what we've seen, it seems very unlikely that Cox and its ISP ilk are leaving any money on the table by not "actively listening" to everything they are able to obtain by monitoring all of their subscribers' use of the Internet through our PCs, our smartphones, and any other user-directed Internet devices.  Given the evidence it seems clear that, if they can get it, they will sell it and use it.  As a result of what was found on Cox's website, the threat of that sort of monitoring being done by a major residential ISP just became far less theoretical and far more likely.



Now, at this point I'm sure many of our listeners are thinking, that seems like a good idea.  How do we go about making that happen?  Which I think will be a great topic for us to look at closely in 2024, which after all is only a few weeks away.  And speaking of which, I want to personally wish, and I know Leo and all of the TWiT network joins me, in wishing all of our listeners a happy and safe holiday season.  2024 promises to be at least as interesting as recent years, and we'll be right here to watch and examine all of it as those events unfold.  See you then.



LEO:  You bet, yeah.  You know, I think there's a mixture of CMG overselling their capabilities because, you know, honestly, we would, if Amazon's Echo, for example, were sending that data back, we would know.  The processor on it is not sufficient to do that kind of AI extraction.  They would have to send the content back.  And you'd see a huge amount of traffic coming out of that thing.  And we just don't see that traffic.  Same thing with your iPhone.  And most hardware microphones there is a light that comes on.



STEVE:  It's a little orange dot on iPhones.



LEO:  And you know when that mic is listening; right?



STEVE:  Yeah.



LEO:  I mean, I guess Apple could be lying to us.



STEVE:  Well, or users are not - they don't know what the little dot is.



LEO:  Yeah, but you and I know, and we don't see that dot coming on at random.  In fact, if it comes on, that's a cause for concern.  We go, okay, something's listening.



STEVE:  Right.



LEO:  But I would also point out you're already giving them all that information.  How good is the signal if I talk to my wife, "I think my lease is running out, what should I do," versus me going online and searching for new cars?



STEVE:  Exactly.



LEO:  They're getting much better signals all the time from your TikTok usage, from your Facebook usage.



STEVE:  Exactly.



LEO:  From your web surfing.  And you're absolutely right, Cox sees all of that.  So I think that this is, you know, oh, marketers would never lie; right?  I think this is marketers overhyping their capabilities, perhaps out of ignorance.  What's sad is I think a lot of people are going to throw out their Amazon Echoes, even if they find them useful, because they're worried about that.  And I don't think there's any evidence that Echoes or Google device, you know, hubs listen.  We would know if they did; right?



STEVE:  Yup.



LEO:  We would know if your phone is listening.  Now, your car probably is listening.  Your smart TV probably is listening.  There are some of the things they mention in here that are, certainly.



STEVE:  Yeah, I do, I absolutely agree with you.  We know much less about some generic smart TV and what it might be doing.



LEO:  Yeah, I mean, there's no reason Samsung put a camera and a microphone on my TV.  I mean, they may say, oh, it's so you can Zoom.  No, that's not.  I know exactly why you guys put a camera and microphone on my TV.  So certainly that's a reasonable thing to be concerned about.  But I think there's also - I hope people don't set their hair on fire on this.  You can punish Cox all you want.  But, you know, you use Cox.  My mom uses Cox.



STEVE:  Yeah.



LEO:  It's not like you have a choice in most markets.



STEVE:  No choice at all.  There's only one broadband provider in our market.



LEO:  See?  So cutting off your nose to spite your face, it just doesn't make sense.  I think they oversold the capabilities.  But you're already giving them all that information all the time.  I mean, you really are.  So, you know, there's no secrets.  When you start searching for mold abatement on the Internet, they know what's going on.



STEVE:  There's a reason; right.  Exactly.



LEO:  Really good stuff, though.  And I'm really glad you brought it up and found the details and everything because it's a topic that we will undoubtedly be covering more in the future.  And the use of AI in it is nontrivial.  AI makes it - see, the reason it wasn't such a thing to worry about before is because there was such a volume of stuff.  Wait a minute.  You're listening to everything I say every day all the time?  No one can listen to all that.  AI could.



STEVE:  No, in fact we know that the processor in the Echo, it's listening for its trigger word.



LEO:  It can barely do that.



STEVE:  Which then, yeah, exactly, which then wakes it up, and it streams the audio for analysis during, you know, while it's awake.



LEO:  People are right to be concerned about privacy.  The other thing to watch with interest is who's going to write the letter?  Is it going to be Ron Wyden to Cox saying, hey.



STEVE:  I know.



LEO:  What is this?  But remember, if such capabilities exist, you know who the number one customer would be?  Government.  Law enforcement.  And that's something really to be concerned about.  I don't care if some car dealer contacts me because my lease is running out.  I'm sure Ford's already sold that information to every car dealer in town.



I am very concerned if the government thinks I'm an agitator and decides to, you know, listen to everything I'm doing.  That's something really much more serious.  So I'm interested to see what government's response to this is.  I expect a lot of saber rattling and absolutely no action because they use these tools.  They don't want data brokers to go away.  They're buying the information.



Thank you, Steve.  Have a wonderful holiday.  We'll see you next time on Security Now!.



STEVE:  See you next year, my friend.



Copyright (c) 2023 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.






GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#955

DATE:		January 2, 2024

TITLE:		The Mystery of CVE-2023-38606

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-955.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  After everyone is updated with the state of my still-continuing work on SpinRite 6.1, and after I've shared a bit of feedback from our listeners, the entire balance of this first podcast of 2024 will be invested in the close and careful examination of the technical details surrounding something that has never before been found in Apple's custom proprietary silicon.  As we will all see and understand by the time we're finished here today, it is something that can only be characterized as a deliberately designed, implemented, and protected backdoor that was intended to be, and was, let loose and present in the wild.  After we all understand what Apple has done through five successive generations of their silicon, today's podcast ends, as it must, by posing a single one-word question:  Why?



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here, and we have perhaps the most interesting story.  What a way to kick off 2024, our first episode of the new year.  And Steve breaks down what looks to be a massive conspiracy around the security of the last five generations of iPhone.  The details coming up on Security Now!.	



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 955 for Tuesday, January 2nd, 2024:  The Mystery of CVE-2023-38606.



It's time for Security Now!, the show where we cover your privacy, your security, your safety online, how computers work, and a whole lot more with this guy right here, Steve Gibson, the autodidact who knows all, tells all, and is back again for 2024.  Happy New Year, Steve.  



STEVE GIBSON:  Happy New Year to you, Leo.  And I was pleased to see that we've got continuing sponsors in 2024 that'll be familiar to our audience.



LEO:  Yes, yes.  Your show will never be hard to sell.  People want to be on Security Now!, that's for sure.



STEVE:  Okay.



LEO:  But by the way, we're always looking for new advertisers.  I don't want to discourage anybody.



STEVE:  No.



LEO:  We always want - if you work for a company, and you say, boy, they really - I hear this a lot - they really ought to advertise on Security Now!, by all means contact us, advertising@twit.tv.



STEVE:  I can definitively state, Leo, that Apple will not choose to be an advertiser on this podcast.



LEO:  Well, I can think of many reasons for that.  But what do you mean?



STEVE:  Well, we know they have a very thin skin; right?  Like, you know, you haven't been allowed back after you said one comment once, or looked at your phone during a presentation or some nonsense.  I don't remember.



LEO:  Yeah, no, I'm on a blacklist for sure, yeah.



STEVE:  Okay.  This podcast has a very dry title, "The Mystery of CVE-2023-38606.



LEO:  Hmm, whatever could that mean?



STEVE:  Uh-huh.  After everyone is updated with the state of my still-continuing work on SpinRite 6.1, and after I've shared a bit of feedback from our listeners, the entire balance of this first podcast of 2024 will be invested in the close and careful examination of the technical details surrounding something that has never before been found in Apple's custom proprietary silicon.  As we will all see and understand by the time we're finished here today, it is something that can only be characterized as a deliberately designed, implemented, and protected backdoor that was intended to be, and was, let loose and present in the wild.  After we all understand what Apple has done through five successive generations of their silicon, today's podcast ends, as it must, by posing a single one-word question:  Why?



LEO:  Well, well.  I think you're probably right, but I can't wait to - we talked a little bit about it on MacBreak Weekly.  And I even said, "I bet you Steve will have something to say about this."  So I'm very, very interested in the deep details, the breakdown.



STEVE:  We've got the details.  Everybody is going to understand it.  We're left with some questions.  Not just one, like how did this happen?  Because there are things unfortunately we're never going to know.  But there are definitely things we do know.  And it really takes a little bit of the polish off the Apple.



LEO:  Oh, my.



STEVE:  Yeah.



LEO:  Oh, my.  Well, I'll be very interested to hear more about this.  All right.



STEVE:  And we do have, oh, my goodness, a great Picture of the Week.  I gave it a caption that really...



LEO:  I actually like the caption, "Can you say the word 'ad hoc' in a sentence."  I don't know what it means, but we will get to the Picture of the Week.  All right.  Picture of the Week.



STEVE:  So this picture was of an insane person, something that a crazy person did.  Apparently they were traveling somewhere in Europe, where the plugs are two round pins, offset from the center.



LEO:  Yeah.



STEVE:  But they had a U.S. standard two-prong, straight-prong power plug.  And they couldn't, I guess, go downstairs to the hotel gift shop and buy themselves an adapter.  So they managed to put one together.



LEO:  Oh, lord.  This is insane.  Oh, my god.



STEVE:  So I looked at this, and I...



LEO:  Would this work?



STEVE:  Oh, yeah.  I gave this - although, you know, you want to keep your distance.  I gave this the caption "Please use the term 'ad hoc' in a sentence."  And the sentence is "This is a photo of an ad hoc EU to US power adapter."



LEO:  It looks like he's got forceps and a key and a padlock.



STEVE:  There's forceps stuck in one of the round European outlet holes.  And then on the U.S. plug the two prongs have holes through them, as many of ours do, so the plug was - one of the plug's holes was threaded through the handle of the forceps, giving it an electrical attachment to one side of the European power.



LEO:  It's kind of clever because it's taking advantage of the hole in the U.S. plug.



STEVE:  Yes.  That was clever.  Well, then, so does the padlock, which has been stuck, the hasp of the padlock has been stuck through the hole in the other prong of the U.S. plug, the padlock then closed.  And of course it's got a key, linking it with a key ring to another key, and that second key is plugged into the opposing hole in the adjacent EU power outlet, thus completing the circuit.



LEO:  I mean...



STEVE:  If you use the term "circuit" very loosely.



LEO:  I'd be a little concerned about the impedance of the key ring.



STEVE:  Yeah.  Yes.  You're right, that little key ring loop, if you're drawing too much power from the circuit, it's going to get...  



LEO:  It's going to start to glow.



STEVE:  It's going to get a little warm.  Oh, god.  Anyway...



LEO:  This is hysterical.



STEVE:  A great photo.



LEO:  Don't ever try this at home, kids.  This is not recommended by any means.



STEVE:  Yes.  We're not suggesting this.  We're just saying, if you are really desperate...



LEO:  No, no, no, no.  Nobody's that desperate.



STEVE:  Then even don't do it.



LEO:  No, even then, no.



STEVE:  Okay.  So as I mentioned two weeks ago, I was hoping to be able to announce that SpinRite 6.1 was finished.  And really it is all but finished.  If I were not still making some incremental progress that matters, I would have declared it finished.  I've been wrestling with the behavior of some really troubled drives, like finally someone Fedexed me - I paid the FedEx shipping because there was no way I was going to have someone who's been testing SpinRite pay shipping.  He Fedexed me his drive Friday that arrived three days ago, on Saturday, because SpinRite was getting stuck at 99.9953% of the drive.  Well, stuck is no good.



Turns out it was - that drive, it was a Western Digital, was returning a bogus status which was tripping up SpinRite, which wasn't prepared for like a ridiculous status byte coming back from the drive.  So SpinRite's better now, and three other people's drives that were also getting stuck for the same reason no longer do.  So it's like the edgiest of edge cases.  But I want this thing to be perfect.  And the things I'm doing will carry forward into the future.  So I would have to do it sooner or later.  I might as well do it now.



Anyway, I need to give it a little more time to settle.  But what I know is that, as a consequence of this intense scrutiny at the finish line, this SpinRite really does much more than any version of SpinRite ever has before.  So anyway, I'm just days away.



Okay.  So I have a couple bits of Closing the Loop feedback.  This is interesting.  This is from Rotterdam in the Netherlands.  The subject was "Which root certificates should you trust?"  Remy wrote:  "I made the tool requested in 951."  He wrote:  "Hi, Steve.  Happy SpinRite customer here.  Can't wait for 6.1.  In Episode 951 you discussed root certificates, and Leo asked for a 'Prune your CA' app.  You said:  'The good news is that this is generic enough that somebody will do it by the next podcast.'"  He said:  "Well, fun challenge, and I happen to know a bit of C++, so here is my attempt."  And we have a link in the show notes.  It is also this week's Shortcut of the Week.



Anyway, he wrote:  "It parses the Chrome or Firefox browser history and fetches all certificates.  It's a reporting-only tool that does not prune your root store, since I suspect," he says, "that could break stuff.  So if people want that, they can figure out how to do that themselves and make a backup."  He says:  "I've compiled it as a 32-bit Qt 5.15 C++ application, specifically for you, so it runs on Windows 7."



LEO:  You've got a reputation, Steve.



STEVE:  "And up."  He says:  "Qt 6, the current version of the C++ framework, does not support Windows 7.  Cheers, Remy."  So anyway, since I found this as I was pulling together today's podcast, I've not yet had the chance to examine it closely.  But I quickly went over to Remy's page, and it looks like he has done a very nice piece of work.



LEO:  Yeah.  It's very cool,  yeah.



STEVE:  Yeah.  And I don't know if you noticed, but there are counts on the number of instances of each of those certificates that he found.  And he's also supporting by, I mean, he's sorting by the most hits on a given cert.  And not surprisingly, Let's Encrypt is up there at the top.



LEO:  Right.



STEVE:  Anyway, so he provides an installer for Windows users with the full source code.  So for Linux users he suggests installing QT and compiling his what he calls "Certutil" from the provided source code.  He's deliberately removed all Google ads, Google and other tracking from his site.  So he suggests that, if you find his work useful, you might perhaps buy him a cup of coffee.  The link to Remy's page, as I said, is this week's GRC shortcut, so grc.sc/955.  And that'll just bounce you directly to Remy's page.  And so Remy, thanks for this.



LEO:  This is read-only, and I understand his reluctance to put a surgical tool in there.  But how hard, if one's seeing these, how hard would it be to remove them?  Is there a mechanism for that manually?



STEVE:  Yes.  Well, under Windows, you down in the search box put MMC and hit ENTER.  That brings up their desktop.  MMC ENTER.  And there it is.  And so that brings up - and yes, you want to run it.  Yup.  And so then you go to File up in the File Menu and add snap-in.  Add/remove snap-in.  Now find Cert and add that.  You have to move it into the box on the right.



LEO:  Oh, you can't just click it, you have to drag it.



STEVE:  Yeah.



LEO:  Okay.



STEVE:  Add/remove snap-in.



LEO:  So there's the certificates snap-in.



STEVE:  And then click on that little thing that...



LEO:  Oh, I see, add it here, I get it.  Oh, yeah.  I haven't used this in a while.  All right.



STEVE:  And my account.



LEO:  There it is.  Now certs are in there.  And now I can see my certificates.



STEVE:  Now you've created a certificate viewer which allows you to see all the certs you've got on your machine.



LEO:  And the one you care about is Trusted Root; right?



STEVE:  Yes.  And if you click there...



LEO:  Oh, there you go.



STEVE:  ...there they all are.



LEO:  Now, let's say I don't want Comodo.  Can I right-click on it and delete?  Yeah.



STEVE:  Yup.



LEO:  So you totally can do that manually, one by one.



STEVE:  You absolutely can.



LEO:  Okay.



STEVE:  Yup.  And we'll see how this goes because this also could be automated.  All the certs are in the registry.  This is basically a...



LEO:  It's a regedit, yeah.



STEVE:  ...convenient viewer into the registry that does a lot of interpretation for you, too.



LEO:  This is good.  I mean, honestly, this is the tool that's kind of sufficient.  It's just that you have to do it onesie-twosie.



STEVE:  Well, and you've got to know which ones.  So his certutil shows you...



LEO:  That's useful to report.  Yeah, yeah, yeah.



STEVE:  ...the entire, given your entire current browser history, what certificates has your browser ever touched?



LEO:  And what hasn't it touched because there's a whole listing of unused CAs.



STEVE:  Exactly.



LEO:  Yeah.  And those are the ones you might consider removing.



STEVE:  Yeah.  Actalis.  Who's ever heard of IT Milan Actalis?



LEO:  It's Italian.  It's an Italian...



STEVE:  I don't think we need them.



LEO:  Yeah, we don't need them.



STEVE:  You know, we don't - and of course what this means is anything they ever sign our browser absolutely trusts.  You know, that's the great danger of this whole system.



LEO:  Right.



STEVE:  Is that it's carte blanche trust for every single certificate in our root.



LEO:  And as you pointed out when we first started talking about this, there's only a handful of certs that do 90% of the traffic into your computer.



STEVE:  Seven certs gives you 99.96% of all non-expired browser certificates.  



LEO:  Wow.  Okay.  I could probably remove the China Financial Certification Authority from my EV roots.



STEVE:  I think so.



LEO:  I have a feeling I won't be needing that.



STEVE:  Yeah.  Now your light switch might not work anymore.  But still.



LEO:  So that's the question.  What does the - you would get - it would tell you this is an untrusted site.



STEVE:  Correct.



LEO:  But you still could go to it.  You could bypass it.



STEVE:  Absolutely.  You could push past it.  And, you know, maybe that's what you want is something that says whoa, wait a minute, why am I going to the Chinese laundry?  I don't really need that.



LEO:  Yeah, I don't need that one, yeah.  Yeah, okay.  Very handy.  Thank you.  Good job, Remy.  Yeah.



STEVE:  Thank you.  Martin Bergek was referring to last week's Schrodinger's Bowls Picture of the Week, or rather two weeks ago.  This was great.  He said:  "Showed the Picture of the Week to my son, age 13.  He immediately gave the obvious answer:  'Break the top glass.  Obviously that is cheaper to replace than the plates.'"



LEO:  Ah.  Very clever.



STEVE:  And he said - yes.  He said:  "Slightly annoyed I didn't come up with it myself."  Well, none of us did, Martin.  "But happy that the future is looking bright."



LEO:  That's great.



STEVE:  So anyway, yeah.  The upper glass was in front of an empty shelf.  So you could break the glass, then lift the shelf and have full access to all the bowls that are precariously trying to fall but are being held in place by the pane of glass below that.  So very, very clever.



@sammysammy222 tweeted @SGgrc:  "Steve, I planned to use DNS Benchmark today.  When I ran the download by VirusTotal, three security vendors flagged this file as malicious.  While I trust you, I wanted to check in, in case there is a problem with the file.  Thanks."



Okay.  So I just dropped the file on VirusTotal, and at the  moment I did, four of the AV tools at VirusTotal dislike it.  Now, I'll note, however, that they're all rather very obscure AV, so it's not Google or Microsoft or anybody.  And since GRC's DNS Benchmark is approaching 8.8 million downloads at a rate of 4,000 per day, I suppose it's a good thing that not everyone is as cautious as sammysammy222 or there would be a lot more concern than is being expressed.  But seriously, I poked around a bit in VirusTotal because it, like, allows you to see what things are setting off alarm bells.  And I looked at the behavior that is concerning to VirusTotal, and I can't really say that I blame it.  The program's behavior, if you didn't know any better, would peg it as some sort of bizarre DNS DOS zombie.  You know, it scans network adapters.  It's chock full of every DNS server you can imagine.  There's 505 of them or something.



LEO:  It does look kind of suspicious, come to think of it, yes.



STEVE:  Yeah.



LEO:  Good point.



STEVE:  And, you know, it's got all sorts of mysterious networking code.  Unfortunately, this is the world we're living in today.  It doesn't matter that that program which has been around since, what, 2007 or something I think I wrote it?  You know, it's never hurt a single living soul, and it never would.  You know, it's even triple SHA-256 signed, with various of GRC's old and new certificates, all of them valid.  Do the signatures matter?  Apparently not.  It still looks terrifying.



So anyway, to answer your question, SammySammy, the file's code has not changed in nearly five years, since I remember I tweaked it, I think to add 1.1.1.1 or something, you know, or 9.9.9.9, some updated server I wanted to put in there, and that was five years ago.  It's never actually harmed anyone.  Yet I can understand why VirusTotal would go ugh, you know.  So yeah, anyway, I'm pretty sure you can rely on it being safe.



Christian Nilsson said:  "Hello, Steve.  I'm one of all those who is so glad you decided to go past 3E7 hex."  So we know that's 999 in decimal.  He said:  "Thank you.  Speaking of Tailscale, et cetera, I strongly recommend Nebula Mesh.  Open source all the way, and very easy to manage.  I know it's been on your radar before.  What's the reason for not advocating it in SN-953?"



Christian, no reason at all.  I just forgot to mention it among all the others.  And I agree 100% that Nebula Mesh, which was created by the guys at Slack, is a very nice-looking overlay network.  We did talk about it before.  Slack wrote it for their own use because nothing else that existed at the time, this was back now five years ago in 2019, did what they needed out of the box.  So they rolled their own.



Nebula Mesh's home on GitHub is under SlackHQ's directory tree, and I have the link in the show notes.  But you can just search "nebula mesh" to locate it.  It's a fully mutually authenticating peer-to-peer overlay network that can scale from just a few to tens of thousands of endpoints.  It's portable, written in GO, and has clients for Linux, MacOS, Windows, iOS, Android, and even my favorite Unix, FreeBSD.  So absolutely thank you for the reminder.



And for those who are looking for a good, 100% open, unlike Tailscale, where they are keeping a couple of the clients closed source and the rendezvous server, which those nodes need to find each other, closed, Nebula's 100% open.  So again, Christian, thanks for the reminder.



Ethan Stone said:  "Hi, Steve.  I've noticed something recently that you might be interested in.  First on my Windows 10 machine, then on my Windows 11 machine, the Edge browser has kept itself running in the background after I close it.  The only way to actually shut it down is to go into Task Manager and manually stop the many processes that keep running after it's closed.  I noticed this because, as one of your most paranoid listeners" - he says, parens, "(I know that's a high bar) - "I have my browsers set to delete all history and cookies when they shut down."  Boy, he must like logging into sites.  "And I have CCleaner doing the same thing if they don't.  CCleaner seems to have caught on to Microsoft's little scheme and now notes that Edge is refusing to shut down and asking me if I want to force it.  Anyway, it seems like it's just another little scheme to keep all of my data and activity accessible to their grubby little data-selling schemes."



And actually I think it's probably just, you know, who knows what.  They figure just get more RAM if you don't want Edge continually running.  Actually what it probably is, is it allows it to launch onto your screen much more quickly when you start it again; right?  Because it never really stopped.  So it's already, it's like still there.  It just turned off the UI.  So you click on the icon and, like, bang, it's like whoa, okay.  So anyway, I understand you want things that you don't need and that you stop to actually stop.  That's a good thing. 



He finally says:  "I love the show, and I'm looking forward to new ones in 2024 and the advent of SpinRite 6.1, although I really, really, really need native USB support.  So I hope SpinRite 7 isn't far behind.  Also, I'm looking forward to not having to log into Twitter to send you messages, although please give priority to SpinRite 7."



Okay.  So I have been gratified with the feedback from Twitter users that they're looking forward to abandoning Twitter to send these short feedback tidbits.  My plan is to first get SpinRite 6.1 finished.  As I said, that's just like moments away.  Okay.  But then I need an email system, which I don't currently have, in order to notify all v6.0 owners through the last 20, what, 21 years - 20 years because it was in 2004 I began offering 6.0.  So I need, you know, an email system to let them know that it's available and basically of what has turned out to be a massive free upgrade to 6.0.  So that need will create GRC's email system, and we'll experiment then with my idea for preventing all incoming spam without using any sort of heuristic false-positive tendency filter.



And then I plan to immediately begin working on SpinRite 7.  I don't want to wait for several reasons.  For one thing, I currently AM SpinRite.  I have the entire thing in my head right now.  My dreaming at night is about SpinRite.  And I know from prior experience that I will lose it if I don't use it.  You know, I barely remember how SQRL works now.  So I want to immediately dump what's in my head into the next iteration of SpinRite code.  Another reason that Ethan referred to is about USB.  Unfortunately, USB is still SpinRite 6.1's Achilles heel.  All of SpinRite has historically run through the BIOS, but I've become so spoiled now by 6.1's direct access to drive hardware that the BIOS now feels like a real compromise.



Yet even under 6.1, all USB-connected devices are still being accessed through the BIOS.  Since SpinRite's next platform, which I've talked about, RTOS-32, already has USB support, although not the level of support required for data recovery - that's what I'll be adding - still, it made no sense for me to spend more time developing USB drivers from scratch for DOS when at least something I can start with is waiting for me right now under RTOS-32.  So yes, I'm a hurry to get going on SpinRite 7.  It'll be happening immediately.



Oh, and then I had already replied to Ethan.  And when I checked back into Twitter, he had replied to me.  And he wrote again, "Okay, but please spend less time on the email thing than you did on SQRL."  And so yes, I am not rolling my own email system from scratch this time.  I've already purchased a very nice PHP-based system that I've just been waiting to deploy.  So I'll be making some customizations to part of it, but that's it, and then we'll have email.



Okay, Leo.  I'm going to catch my breath, you tell us why we're here, and then, oh boy, we're going to...



LEO:  So the whole show's going to be this Apple thing?



STEVE:  Yes.



LEO:  Holy cow.  There is a lot to talk about.



STEVE:  Some other things happened, but they pale in comparison.



LEO:  Yeah.



STEVE:  To what we now absolutely know that Apple has done.



LEO:  Well, and they said in the information that it was ARM devices, other ARM devices might also be susceptible.



STEVE:  All iDevices.



LEO:  All iDevices.  But ARM makes stuff, like my Pixel phone, no, not my Pixel, my Samsung phone is ARM based.  So that, well, we'll find out.



STEVE:  Yup.



LEO:  We'll find out.  And as far as I know, those CVEs have been patched on Apple devices.  But we'll find out about that, as well.



STEVE:  Well, we're going to find out why this was here for five years.



LEO:  Yeah.  Whoof.  Wow.  I used your name in vain on MacBreak Weekly.  I said if there's one thing I've learned from listening to Steve for almost 20 years is interpreters are a problem.



STEVE:  I heard you, yes.



LEO:  Yeah, yeah.  



STEVE:  And you're absolutely correct.  



LEO:  We know this.  Okay.  Let's get the CVEs here.



STEVE:  I have in the past suggested that you buckle up.  And this is no exception.



LEO:  I don't have a seatbelt on this bicycle seat that's attached to a stick.



STEVE:  Oh, that's right.  You're not on the ball anymore.  That's good.  That's good.



LEO:  I will plant my feet firmly on terra firma.



STEVE:  Plant your feet, yeah, firmly apart.  Okay.  Our long-time listeners may recall that during the last year or so we mentioned an infamous and long-running campaign known as "Operation Triangulation," which is an attack against the security of Apple's iOS-based products.  This breed of malware gets its name from the fact that it employs canvas fingerprinting to obtain clues about the user's operating environment.  It uses WebGL, the Web Graphics Library, to draw a specific yellow triangle against a pink background.  It then inspects the exact high-depth colors of specific pixels that were drawn because it turns out that different rounding errors used in differing environments will cause the precise colors to vary ever so slightly.  We can't see the difference, but code can.  And since it uses a triangle, this long-running campaign has been named "Operation Triangulation."



Okay.  In any event, the last time we talked about Operation Triangulation was in connection with Kaspersky - Kaspersky.  I heard you putting the accent on the second syllable, and I went and checked, Leo, and you're exactly right.  I have been always saying Kaspersky, but it's Kaspersky.



LEO:  I just put myself in kind of the mind of Boris Badenov.



STEVE:  The Russian, the Russian...



LEO:  And I say, how would Boris Badenov pronounce it?  Kaspersky.  And then that's how I say it, yeah.



STEVE:  That's right.  That's right.  Well, you got it right.  Okay.  And we talked about it with Kaspersky Labs because someone had used it, that is, this Operation Triangulation attack, to attack a number of the iPhones used by Kaspersky's security researchers.  In hindsight, that was probably a dumb thing to do since nothing is going to motivate security researchers more than for them to find something unknown crawling around on their own devices.  Recall that it was when I found that my own PC was phoning home to some unknown server that I dug down, discovered the Aureate Spyware, as far as I know coining that term in the process since these were the very early days, and then I wrote "OptOut," the world's first spyware removal tool.  My point is, if you want your malware to remain secret and thus useful in the long-term, you need to be careful about whose devices you infect.



Although we haven't talked about Kaspersky and the infection of their devices for some time, it turns out they never stopped working to get to the bottom of what was going on, and they finally have.  What they found is somewhat astonishing.  And even though it leaves us with some very troubling and annoying unanswered questions, which conspiracy theorists are already having a field day with, what has been learned needs to be shared and understood because, thanks to Kaspersky's dogged research, we now know everything about the "What," even if "How" and "Why" will forever probably remain unknown.  And there's even the chance that parts of this will forever remain unknown to Apple themselves.  We just don't know.



Kaspersky's researchers affirmatively and without question found a deliberately concealed, never documented, deliberately locked but unlockable with a secret hash, hardware backdoor which was designed into all Apple devices starting with the A12 chip, the A13, the A14, the A15, and the A16.



This now publicly known backdoor has been given the CVE which is today's podcast title, thus CVE-2023-38606, though it's really not clear to me that it should be a CVE since it's not a bug.  It's a deliberately designed-in and protected feature.  Regardless, if we call it a zero-day, then it's one of four zero-days which, when used together in a sophisticated attack chain, along with three other zero-days, is being described as the most sophisticated attack ever discovered against Apple devices, and that's a characterization I would concur with.



Okay.  So let's back up a bit and look at what we know thanks to Kaspersky's work.  I can't wait to tell everyone about 38606, but to understand its place in the overall attack we need to put it into context.  The world at large first learned of all this just last Wednesday, on December 27th, when a team from Kaspersky presented and detailed their findings during the 37th four-day Chaos Communication Congress held at the Congress Center in Hamburg, Germany.  The title they gave their presentation was "Operation Triangulation:  What You Get When Attack iPhones of Researchers," which, I think, is a perfect title because yeah.  On the same day last Wednesday they also posted a non-presentation description of their research on their own blog, titled "Operation Triangulation:  The Last Hardware Mystery."



I've edited their lengthy posting for the podcast, and I'm not going to go through all of it.  But I wanted to retain the spirit of their disclosure since I think they got it all just right, and they did not venture into conspiracies.  So after some editing for clarity and length, here's what they explained.  They said:  "Today, on December 27th, 2023, we delivered a presentation titled "Operation Triangulation:  What You Get When Attack iPhones of Researchers" at the 37th Chaos Communication Congress held at the Congress Center, Hamburg.  The presentation summarized the results of our long-term" - it is multiyear - "research into Operation Triangulation, conducted with our colleagues.



"This presentation was also the first time we had publicly disclosed the details of all exploits and vulnerabilities that were used in the attack.  We discover and analyze new exploits and attacks such as these on a daily basis.  We've discovered and reported more than 30 in-the-wild zero-days in Adobe, Apple, Google, and Microsoft products.  But this is definitely the most sophisticated attack chain we have ever seen.  Here's a quick rundown of this zero-click iMessage attack, which used four zero-days and was designed to work on iOS versions up to iOS 16.2."



Okay.  So "Attackers send a malicious iMessage attachment, which the application processes without showing any signs to the user.  This attachment exploits the remote code execution vulnerability" - and these are all CVE-2023, so I'm going to skip that.  So it's "the remote code execution vulnerability 41990 in the undocumented, Apple-only ADJUST TrueType font instruction.  This instruction had existed since the early '90s until a patch removed it.



"It uses return/jump-oriented programming and multiple stages written in the NSExpression/NSPredicate query language, patching the JavaScriptCore library environment to execute a privilege escalation exploit written in JavaScript.  This JavaScript exploit is obfuscated to make it completely unreadable and to minimize its size.  Still, it has around 11,000 lines of code, which are mainly dedicated to JavaScriptCore and kernel memory parsing and manipulation.  It exploits the JavaScriptCore debugging feature DollarVM to gain the ability to manipulate JavaScriptCore's memory from the script and execute native API functions.



"It was designed to support both old and new iPhones and include a Pointer Authentication Code bypass for exploitation of recent models.  It uses the integer overflow vulnerability 32434 in the OS's memory mapping syscalls to obtain read/write access to the entire physical memory of the device.  It uses hardware memory-mapped I/O (MMIO) registers to bypass the Page Protection Layer known as PPL by Apple."  This was mitigated as that CVE that is this podcast's title, 38606.  A lot more on that in a minute.



"After exploiting all the vulnerabilities, the JavaScript exploit can do whatever it wants to the device running spyware.  Which is to say it is at this point the iDevice, because this is in all the A12 through A16 chips, in all of Apple's devices.  The device is cracked wide open.  At this point they've achieved absolute dominance.  So they say after exploiting all the vulnerabilities, the JavaScript exploit can do whatever it wants to the device, including running spyware.  But the attackers chose to, A, launch the IMAgent process and inject a payload that clears the exploitation artifacts from the device; and B, run a Safari process in invisible mode and forward it to a web page with the next stage.



"The web page has a script that verifies the victim and, if the checks pass, receives the next stage, the Safari exploit.  The Safari exploit uses 32435 to execute shellcode.  The shellcode executes another kernel exploit in the form of a Mach object file.  The shellcode reuses the previously used vulnerabilities 32434 and 38606.  It is also massive in terms of size and functionality, but completely different from the kernel exploit written in JavaScript.  Certain parts related to exploitation of the above-mentioned vulnerabilities are all that the two share.



"Still, most of its code is also dedicated to parsing and manipulation of the kernel memory.  It contains various post-exploitation utilities, which are mostly unused.  And finally, the exploit obtains root privileges and proceeds to execute other stages, which load spyware.  And those subsequent spyware stages have been the subject of previous postings of Kaspersky.



Okay.  So now I'm going to summarize this, and then we're going to zero in.  The view we have from 10,000 feet is of an extremely potent and powerful attack chain which, unbeknownst to any targeted iPhone user, arranges to load, in sequence, a pair of extremely powerful and flexible attack kits.  The first of the kits works to immediately remove all artifacts of its own presence to erase any trace of what it is and how it got there.  It also triggers the execution of the second extensive attack kit, which obtains root privileges on the device and then loads whatever subsequent spyware the attackers have selected.  And all of that spyware has full rein of the device because that subsequently loaded spyware is under, you know, obtains the permission of these kits.



So it uses CVE 41990, a remote code execution vulnerability in the undocumented, Apple-only ADJUST TrueType font instruction.  That's where it begins.  With that still somewhat limited but useful capability, it uses "living off the land" return/jump oriented programming, meaning that since it cannot really bring much of its own code along with it at this early stage, instead it jumps to the ends of existing Apple-supplied code subroutines, threading that together to patch the JavaScriptCore library just enough to obtain privilege escalation.



LEO:  So they call that "living off the land."



STEVE:  Right.



LEO:  Because it's not using - you can't put much of your own code in there, so you have to use existing code.



STEVE:  Right.  Right.  And it just jumps to just before other subroutines occur.



LEO:  So there might be a little memory there somewhere that you can...



STEVE:  Yeah.  There are a few little instructions just before a subroutine finishes and returns.  So all they can do is like jump to a sequence of the ends of existing code in subroutines as like little bits of worker code, threading that together to achieve what they want. 



LEO:  Now, to do that you have to know exactly where that stuff lives; right?  You have to have the actual memory event.  That's why address randomization works; right?



STEVE:  Yes.



LEO:  Yeah.



STEVE:  Address space layout...



LEO:  Apple doesn't do ASLR, though, I guess, with this code?



STEVE:  Well, remember that they said the bulk of that code is analyzing the kernel memory.  So it's looking for all of these things in order to figure out where everything is.



LEO:  Oh.  So it may be moving around, but they look for a chunk, and they recognize it and say, oh, it'd be right after this, wherever that is.



STEVE:  Yup.  Yup.



LEO:  Holy cow.  This is so sophisticated.



STEVE:  Or they find like an API jump table and then know that the fourth API in that table will be taking them to the subroutine, the end of which they need.



LEO:  So that's why ASLR doesn't always work, even if you use  it, because...



STEVE:  Right.  Right.  It definitely makes the task far more difficult.  But here they're using it.  And they've managed.  So, I mean, it really raises the bar.  But it can still be worked around.



LEO:  Right.



STEVE:  So after that, they have obtained privilege escalation.  By using the library's, the JavaScriptCore library's debugging features, then they're able to execute native Apple API calls with privilege.



LEO:  Oh, my god.



STEVE:  It then uses another vulnerability, 32434, which is an overflow vulnerability in the OS's memory mapping system API, to obtain read/write access to the entire physical memory of the device.



LEO:  Oh, my goodness.



STEVE:  Yeah, that's different than the normal access because normally you are constrained, like a program is constrained, to what you have access to.  So they use an integer overflow, another zero-day to get read/write access to the entire physical memory, and read/write access to physical memory is required for the exploitation of the title of this podcast, CVE-38606, which is used to disable the entire system's write protection.  Normally you can't change anything.  You can't write anywhere.  And so they figured out how to turn off write protection. 



LEO:  That's amazing.  I mean, this is what the most sophisticated hacks often do is chaining vulnerabilities.



STEVE:  Yes.



LEO:  And you slowly escalate your capabilities till you get what you want.



STEVE:  Yup.



LEO:  Wow.



STEVE:  Okay.  So, so far we've used three vulnerabilities:  41990, 32434, and 38606.  The big deal is that by arranging to get 38606 to work, which requires read/write access to physical memory because that's how it's controlled, this exploit arranges to completely disable Apple's Page Protection Layer, which Apple calls PPL.  It is what provides the great deal of the modern lockdown of iOS devices.  Here's how Apple themselves describes  PPL in their security documents.



They said:  "Page Protection Layer (PPL) in iOS, iPadOS, and watchOS is designed to prevent user space code from being modified after code signature verification is complete.  Building on Kernel Integrity Protection and Fast Permission Restrictions, PPL manages the page table permission overrides to make sure only the PPL can alter protected pages containing user code and page tables.  The system provides a massive - this is Apple - a massive reduction in attack surface by supporting system-wide code integrity enforcement, even in the face of a compromised kernel.  This protection is not offered in macOS because PPL is only applicable on systems where all executed code must be signed.



LEO:  So what's interesting is this comes after system integrity is complete.



STEVE:  Right.



LEO:  So you've verified.  You go, okay, we've got to make sure everything's good.  Everything's good.



STEVE:  Lock it down.



LEO:  Lock it down.  And you've gotten in after that.



STEVE:  Yeah.  The key sentence here was:  The system, PPL, the system provides a massive reduction in attack surface by supporting system-wide code integrity enforcement, even in the face of a compromised kernel.  So that means that defeating the PPL protections results in a massive increase in attack surface.



LEO:  Sure, because it's assumed you're good.



STEVE:  Right.  Once those first three chained and cleverly deployed vulnerabilities have been leveraged, as Kaspersky puts it, "the JavaScript exploit can do whatever it wants on the device, including running spyware."  In other words, the targeted iDevice has been torn wide open.







Okay, now, the next thing Kaspersky does is to take a closer look at the subject of their posting and of this podcast.  But before we look at that I need to explain a bit about the concept of memory-mapped I/O.  From the beginning, computers used separate input and output instructions to read and write to and from their peripheral devices, and read and write instructions, you know, read and write, to read from and write to and from their main memory.  This idea of having separate instructions for communicating with peripheral devices versus loading and storing to and from memory seemed so intuitive that it probably outlasted its usefulness.



What this really meant was that I/O and memory occupied separate address spaces.  Processors would place the address their code was interested in on the system's address bus.  Then, if the IO_READ signal was strobed, that address would be taken as the address of some peripheral device's register.  But if the MEMORY_READ signal was strobed, the same address bus would be used to address and read from main memory.  To this day, largely for the sake of backward compatibility, Intel processors have separate input and output instructions that operate like this, but they are rarely used any longer.



LEO:  As with a lot of stuff, it was for efficiency; right?  So peripheral devices could just sit there, and then every once in a while instead of you - it was like a spooler to the printer.  Instead of having to wait for the printer to ACK it and all that, you just stick it there, and the printer gets it when it wants it, basically.



STEVE:  Well, yes.  And, for example, behind me we have my PDP-8 working boxes.



LEO:  I'm sure they used it, yeah.



STEVE:  So when you only had 12 bits of address space, which is 4K, you don't want to give any of that up to peripheral I/O.  You need every single byte to be actual memory.  So the idea was that the peripheral devices occupied their own memory space, or  I'm sorry, their own, not memory space, their own I/O space.  I/O space was separate.  It was completely disjoint from memory space.  And you access the I/O space with I/O instructions, and  memory space with standard read and write instructions.



Okay.  So to be practical, one of the requirements is that the system, if you want to - okay.  I jumped ahead.  What happened was that someone along the way realized that, if a peripheral's device hardware registers were directly addressable just like regular memory, meaning in the system's main memory address space, right alongside the rest of actual memory, then the processor's design could be simplified by eliminating separate input and output instructions, and all of the already existing instructions for reading and writing to and from memory could perform double duty as I/O instructions.



LEO:  Because it's all in the same spot now.  It's all in the [crosstalk].



STEVE:  Right.



LEO:  Yeah.



STEVE:  But to be practical, one of the requirements is that the system needs to have plenty of spare memory address bits.  But even a 32-bit system, which can uniquely address 4.3 billion bytes of RAM, can spare a little bit of space for its I/O.



LEO:  Probably not a lot of space; right?  I mean...



STEVE:  Right.  And that's exactly what transpired.  So, for example, when today's SpinRite wishes to check on the status of one of the system's hard drives, it reads that drive's status from a memory address near the top of the system's 32-bit address space.  Even though it's reading a memory address, the data that's read is actually coming from the drive's hardware.  This is known as memory-mapped I/O because the system's I/O is mapped into the processor's memory address space, and discrete input and output instructions are no longer used.  And for example, in the case of ARM, they don't even exist.  You know, ARM has always been just direct memory-mapped I/O.



LEO:  Interesting.  So just by polling that address, you trigger a transfer from the I/O device.



STEVE:  Exactly.



LEO:  You're not really reading RAM.  You're reading the I/O device.



STEVE:  Exactly.  And, you know, if you've got 64 bits of address [crosstalk] have anything in most of that space.



LEO:  Yeah, right, right.



STEVE:  So today's massive systems  and by "massive" I mean an iPhone because it has grown into a massive and complex system  they use 64-bit chips with ridiculously large memory spaces.  So all of the great many various functions of the system, you know, fingerprint reader, camera, screen, crypto stuff, enclave, all of that stuff are reflected in a huge and sparsely populated memory map.  Okay, now, in the past we've talked about the limited size of the 32-bit IPv4 address space and how it's no longer possible to hide in IPv4 because it's just not big enough.  The entire IPv4 address space is routinely being scanned.



This is not true for IPv6 with its massive 128-bit address space.  Unlike IPv4, it's not possible to exhaustively scan IPv6 space.  There's just too much of it.  So here's my point.  When there is a ton of unused space, it's possible to leave undocumented some of the functions of memory-mapped peripherals, and there's really no way for code to know whether anything might be listening at a given address or not.  And not only that, there may be too many addresses to reasonably check.



LEO:  There could be millions.



STEVE:  Billions.  Trillions.  Gazillions.



LEO:  Yeah.



STEVE:  Yeah.  I think...



LEO:  Oh, this is interesting.



STEVE:  Yeah.



LEO:  So this reminds me of the old peek and poke commands.  Right?



STEVE:  Yeah.



LEO:  On older machines like Commodores and Ataris, you'd peek an address, or you'd poke an address.  And sometimes that would be an I/O, like if you wanted to write to the hard drive, you'd poke it to an address, and it wouldn't be a memory address, it would poke it to the hard drive.  So that's still built in to modern processors.  In fact, it sounds like more so because...



STEVE:  Yes.



LEO:  There are many, many more devices attached.



STEVE:  It is now the way - yes.  Or things, yes.  And it is so convenient to just be able to use the existing architecture of reading and writing to and from actual memory to do the same with your peripherals.



LEO:  It's a clever hack.  And you know what that means.



STEVE:  What Kaspersky discovered was that Apple's hardware chips, from A12 through A16, all incorporated exactly this sort of hardware backdoor.  And get this.  This deliberately designed-in backdoor even incorporates a secret hash.  In order to use it, the software must run a custom, not very secure but still secret, I mean, it's not crypto quality, but it doesn't need to be, hash function to essentially sign the request that it's submitting to this hardware backdoor.



Here's some of what Kaspersky's guys wrote about their discovery.  And note that even they did not discover it, because it's explicitly not discoverable.  They discovered its use by malware which they were reverse-engineering.  The question that has the conspiracy folks all wound up is how did non-Apple bad guys discover it?



LEO:  Yeah, where did they get the information of where to peek and poke?



STEVE:  And we're going to be spending some more time on that.



LEO:  Because presumably they couldn't brute force it.  They couldn't go through every address and see what happened.



STEVE:  It is not brute forcible.



LEO:  Yeah.



STEVE:  As we'll see.  In the section of their paper titled "The mystery of the CVE-2023-38606 vulnerability," Kaspersky wrote:  "What we want to discuss is related to the vulnerability that has been mitigated as 2023-38606.  Recent iPhone models have additional hardware-based security protection for sensitive regions of the kernel memory."  This is the PPL they're talking about.  "This protection prevents attackers from obtaining full control over the device if they can read and write, that is to say, even if they can read and write kernel memory, as achieved in this attack by exploiting CVE-32434."



So the point being even if you have read and write to kernel memory, you still can't do anything with it.  And that's what PPL, Apple said, protects from; right?  This thing is so locked down that you still can't get into user space.  They said:  "We discovered that to bypass this hardware-based security protection, the attackers used another hardware feature of Apple-designed Systems on a Chip.



"If we try to describe this feature and how the attackers took advantage of it, it all comes down to this:  They are able to write data to a certain physical address while bypassing the hardware-based memory protection by writing the data, destination address, and data hash to unknown hardware registers of the chip unused by any firmware.  Our guess is that this unknown hardware feature was most likely intended to be used for debugging or testing purposes by Apple engineers or the factory, or that it was included by mistake.  Because this feature is not used by the firmware, we have no idea how attackers would know how to use it.  We're publishing the technical details so that other iOS security researchers can confirm our findings and come up with possible explanations of how the attackers learned about this hardware feature.



"Various peripheral devices available in the System on Chip may provide special hardware registers that can be used by the CPU to operate these devices.  For this to work, these hardware registers are mapped to the memory accessible by the CPU and are known as "memory-mapped I/O."  Address ranges for MMIOs of peripheral devices in Apple products (iPhones, Macs, and others) are stored in a special file format called the DeviceTree.  Device tree files can be extracted from the firmware, and their contents can be viewed with the help of the DT (stands for device tree) utility.



"While analyzing the exploit used in the Operation Triangulation attack I," wrote the Kaspersky researcher, "discovered that most of the MMIOs used by the attackers to bypass the hardware-based kernel memory protection do not belong to any MMIO ranges defined in the device tree."  In other words, they're deliberately secret.  "The exploit targets Apple A12 through A16 Bionic Systems on a Chip, targeting unknown MMIO blocks of registers which we have documented."



He said:  "This prompted me to try something.  I checked different device tree files for different devices and different firmware files.  No luck.  I checked publicly available source code.  No luck.  I checked the kernel images, kernel extensions, iBoot, and coprocessor firmware in search of a direct reference to these addresses.  Nothing.  How could it be that the exploit used MMIOs that were not used by the firmware?  How did the attackers find out about them?  What peripheral devices do these MMIO addresses belong to?



"It occurred to me that I should check what other known MMIOs were located in the area close to these unknown MMIO blocks.  That approach was successful, and I discovered that the memory ranges used by the exploit surrounded the system's GPU coprocessor.  This suggested that all these MMIO registers most likely belonged to the GPU coprocessor.  After that, I looked closer at the exploit and found one more thing that confirmed my theory.  The first thing the exploit does during initialization is write to some other MMIO register, which is located at a different address for each version of Apple's System on Chip."



In the show notes I have a picture of the pseudocode that Kaspersky provided.  It shows an "if cpuid =" and then a big 32-bit blob.  And the comment is CPUFAMILY_ARM_EVEREST_SAWTOOTH (A16).  And if it's equal, it loads two variables, base and command, with two specific values.  Then "else if cpuid =" and another 32-bit blob.  That refers to CPUFAMILY_ARM_AVALANCHE_BLIZZARD (A15).  And if that cpuid equal is met, it loads two different base and commands into those values.  And so forth for A14, A13, A12.  In other words, all five CPU families from Bionic A12, 13, 14, 15, 16, require their own something.  And what we learn is that it's their own special unlock in order to enable the rest of this.  It is different for each processor family.



LEO:  Wow.  And it seems like Apple did a good job with this.  That's what's interesting; right?



STEVE:  Oh, Leo.  Wait, wait.  They did a great job with this.  There's more.  Kaspersky's posting shows some pseudocode for what they found was going on.  Each of the five different Apple Bionic A12 through A16 is identified by a unique CPU ID which is readily available to code.  So the malware code looks up the CPU ID, then chooses a custom per-processor address and command, which is different for each chip generation, which it then uses to unlock this undocumented feature of Apple's recent chips.



Here's what they said:  "With the help of the device tree and Siguza's utility, PMGR [power manager]," he said, "I was able to discover that all these addresses corresponded to the GFX register in the power manager MMIO range.  Finally, I obtained a third confirmation when I decided to try to access the registers located in these unknown regions.  Almost instantly, the GPU coprocessor panicked with a message of "GFX ERROR Exception class" and then some error details.  This way, I was able to confirm that all these unknown MMIO registers used for the exploitation belonged to the GPU coprocessor.  This motivated me to take a deeper look at its firmware, which is written in ARM code and unencrypted, but I could find not anything related to these registers in there."



LEO:  Hmm.



STEVE:  "I decided to take a closer look at how the exploit operated these unknown MMIO registers. One register, located at hex" - and I put this in the show notes just to give everyone an idea of how just obscure this is.  "One register located at hex  206040000 stands out" - I know.



LEO:  I think it's more than 999.  I think.  That's a pretty large number, yes.



STEVE:  It is.  It is.  He says:  "It stands out from all the others because it is located in a separate MMIO block from all the other registers.  It is touched only during the initialization and finalization stages of the exploit.  It is the first register to be set during initialization and the last one during finalization.  From my experience, it was clear," he said, "that the register either enabled or disabled the hardware feature used by the exploit or controlled interrupts.  I started to follow the interrupt route, and fairly soon I was able to recognize this unknown register, and also discovered what exactly was mapped to the register range containing it."



Okay.  So through additional reverse-engineering of the exploit and watching it function on their devices, the Kaspersky guys were able to work out exactly what was going on.  The exploit provides the address for a Direct Memory Access, a DMA write, the data to be written, and a custom hash which signs the block of data to be written.  The hash signature serves to prove that whomever is doing this has access to super-secret knowledge that only Apple would possess.  This authenticates the validity of the request for the data to be written.



And speaking of the hash signature, this is what Kaspersky wrote.  They said:  "Now that all the work with all the MMIO registers has been covered, let us take a look at one last thing:  how hashes are calculated."



LEO:  Breaking down - but we should mention, though, Apple has patched all four CVEs.  Right?



STEVE:  Hardware's still there.



LEO:  Oh.  So is some of this not patchable?  Is that what you're saying?



STEVE:  No, no, no.  It has been closed, and we're absolutely going to...



LEO:  We'll get to that, okay.  I just want to say that upfront so people don't go "Oh, no," and put their iPhone in a steel blank box or something.



STEVE:  Yeah, just definitely update it.



LEO:  Yeah.  Keep it updated.



STEVE:  Okay.  So Kaspersky says:  "Now that all the work with all the MMIO registers has been covered, let us take a look at one last thing:  how hashes are calculated.  The algorithm is shown, and as you can see, it is a custom algorithm."  And Leo, it's in the show notes on the next page, on page 11 on my show notes.



LEO:  It's a lookup table; right?



STEVE:  They said, well, okay, I'll get there.  "With the hash is calculated by using a predefined SBox table."  He says:  "I tried to search for it in a large collection of binaries, but found nothing.  You may notice that this hash does not look very secure, as it occupies just 20 bits, but it does its job as long as no one knows how to calculate and use it.  It is best summarized with the term 'security by obscurity.'"



LEO:  You'd have to know the values in this SBox.



STEVE:  Every single value.



LEO:  Because they're random.  They're not consecutive or anything like that, yeah.



STEVE:  Correct.  He said:  "How could attackers discover and exploit this hardware feature if it is never used, and there are no instructions anywhere in the firmware on how to use it?"  Okay.  So to break from the dialogue for a second, in crypto parlance, an "SBox" is simply a lookup table.



LEO:  In this case, what, 256 values, it looks like?  Yeah.



STEVE:  Exactly.  It's typically an 8-bit lookup table of 256 pseudorandom but unchanging values.  The "S" of SBox stands for substitution.  So SBoxes are widely used in crypto and in hashes because they provide a very fast means for mapping one byte into another in a completely arbitrary way.  The show notes shows, as I said, this SBox and the very simple lookup and XOR algorithm below that's used by the exploit.  It doesn't have to be a lot in order to take an input block and scramble it into, basically, it is a 20-bit secret hash.



So the $64,000 question here is how could anyone outside of Apple possibly obtain exactly this 256-entry lookup table, which is required to create the hash that signs the request which is being made to this secret, undocumented, never seen hardware which has the privileged access that allows anything written to bypass Apple's own PPL protection?  Some have suggested deep hardware reverse-engineering, you know, popping the lid off of the chip and tracing its circuitry.



LEO:  Wow.



STEVE:  But because this is a known avenue of general vulnerability, chips are well protected from this form of reverse engineering.  And that sure seems like a stretch in any event.  Every one of the five processor generations had this same backdoor, differing only in the enabling command and address.



LEO:  Do they have the same SBox?



STEVE:  Yes, same SBox.



LEO:  Yeah, they didn't want to change that every time.



STEVE:  Some have suggested that this was implanted into Apple's devices without their knowledge, and that the disclosure of this would have come as a horrible surprise to them.  But that seems far-fetched to me, as well.  Again, largely identical implementations with a single difference across five year separation generations of processor, and then to have the added protection against its use or coincidental discovery of requiring a hash signature for every request.



To me, that's the mark of Apple's hand in this.  If this was implanted without Apple's knowledge, the implant would have been as small and innocuous as possible, and the implanter would have been less concerned about its misuse, satisfied with the protection that it was unknown, that there was this undocumented set of registers floating out in this massive address space that only they would know about.  They would want to minimize it.  Add to that the fact that the address space this uses is wrapped into and around the region used by the GPU coprocessor.  To me this suggests that they had to be - the GPU coprocessor and this had to be designed in tandem.



Okay.  After Apple was informed that knowledge of this little hardware backdoor had somehow escaped into the wild and was being abused as part of the most sophisticated attack on iPhones ever seen, they quickly mitigated this vulnerability with the release of iOS 16.6.  During its boot up, the updated OS unmaps access to the required memory ranges through their processor's memory manager.  This removes the logical to physical address mapping and access that any code subsequently running inside the chip would need.



LEO:  Doesn't that break stuff, though?  I mean...



STEVE:  No.  Just that, I mean, it wasn't supposed to be there anyway.



LEO:  Oh.



STEVE:  So it's, yeah, it's literally just those critical ranges.  The Kaspersky guys conclude by writing:  "This is no ordinary vulnerability, and we have many unanswered questions.  We do not know how the attackers learned to use this unknown hardware feature or what its purpose was.  Neither do we know if it was developed by Apple, or it's a third-party component like ARM CoreSight.



"What we do know, and what this vulnerability demonstrates, is that advanced hardware-based protections are useless in the face of a sophisticated attacker as long as there are hardware features that can bypass those protections.  Hardware security very often relies on 'security through obscurity,' and it is much more difficult to reverse-engineer than software.  But this is a flawed approach because sooner or later all secrets are revealed.  Systems that rely on 'security through obscurity' can never be truly secure."  That's how they end.  And Kaspersky's conclusions here are wrong.



LEO:  Oh.



STEVE:  The Kaspersky guys are clearly reverse engineering geniuses.  And Apple, who certainly never wanted knowledge of this backdoor to fall into the hands of the underworld for use by hostile governments and nation states, owes them big-time for figuring out that this closely held secret had somehow escaped.  But I take issue with Kaspersky's use of the overused phrase "security through obscurity."  That does not apply here.  The term "obscure" suggests discoverability.  That's why something that's obscure and thus discoverable is not truly secure, because it could be discovered.



But what Kaspersky found could not be discovered by anyone, not conceivably, because it was protected by a complex secret hash.  Kaspersky themselves did not discover it.  They watched the malware, which had this knowledge, use this secret feature; and only then did they figure out what must be going on.  And that 256-entry hash table which Kaspersky documented came from inside the malware which had obtained it from somewhere else, from someone who knew the secret.



So let's get very clear because this is an important point.  There is nothing whatsoever obscure about this.  The use of this backdoor required a priori knowledge, explicit knowledge in advance of its use.  And that knowledge had to come from whatever entity implemented this, period.  Also, we also need to note that, as powerful as this backdoor was, access to it first required access to its physical memory-mapped I/O range which is, and was, explicitly unavailable.



Software running on these iDevices had no access to this protected hardware address space.  So it wasn't as if anyone who simply knew this secret could just waltz in and have their way with any Apple device.  There were still several layers of preceding protection which needed to be bypassed.  And that was the job of those first two zero-day exploits which preceded the use of 38606.



So we're left with the question, why is this very powerful and clearly meant to be kept secret faculty present at all in the past five generations of Apple's processors?  I don't buy the argument that this is some sort of debugging facility that was left in by mistake.  For one thing, it was there with subtle changes through five generations of chips.  That doesn't feel like a mistake, and there are far more straightforward means for debugging.  No one hash-signs a packet to be written by DMA into memory while they're debugging.  That makes no sense at all.



Also, the fact that its use is guarded by a secret hash reveals and proves that it was intended to be enabled and present in the wild.  The hash forms a secret key that explicitly allows this to exist safely and without fear of malicious exploitation without the explicit knowledge of the hash function.  This is a crucial point that I want to be certain everyone appreciates.  This was clearly meant to be active in the wild with its abuse being quite well protected by a well-kept secret.  So again, why was it there?  Until Apple fesses up, we'll likely never know.



LEO:  So let me, before you go on, does this functionality have any purpose that we know of?  Is it used in any way by any Apple...



STEVE:  No.



LEO:  No.



STEVE:  And that's the point.



LEO:  It's weird it's in there.



STEVE:  There is no reference to it.  And that's what the Kaspersky guy looked for.  He looked everywhere.  Nothing uses it.  There is no reference to it anywhere.



LEO:  So if that's the case, it implies it might have been put in there for this purpose.  I mean, what purpose...



STEVE:  That's what I believe.



LEO:  What other purpose could - if it's not being used currently...



STEVE:  Exactly.



LEO:  You make some extreme efforts to not only put it in there, but to protect it.



STEVE:  Yes.



LEO:  You have a 256-byte secret that presumably like one Apple engineer knows, you know, it's kept under lock and key.



STEVE:  Right, right.



LEO:  In a basement office somewhere in Cupertino.



STEVE:  Right.



LEO:  Would the chip manufacturers know this hash?



STEVE:  All they see is a mask which they are imbuing in the silicon.



LEO:  They don't, they can't see it.



STEVE:  No.



LEO:  So there is a secret here which Apple holds.



STEVE:  Yes.



LEO:  Presumably holds tightly.



STEVE:  Yes.



LEO:  To a functionality which nobody apparently uses except maliciously.



STEVE:  Yes.



LEO:  Well, that's interesting.



STEVE:  That's exactly it.  Okay.  So could this backdoor system have always been part of some large ARM silicon library that Apple licensed without ever taking a close look at it?  Okay, well, that doesn't really sound like the hands-on Apple we all know.  But it's been suggested that this deliberately engineered backdoor technology might have somehow been present through five generations of Apple's custom silicon without Apple ever being aware of it.



One reason that idea falls flat for me is that Apple's own firmware Device Tree, which provides the mapping of all hardware peripheral into memory, the documentation of the mapping accommodates these undocumented memory ranges without any description.  Unlike any other known component, nowhere are these memory ranges described.  No purpose is given.  And Apple's patch for this, that's what this CVE fixes, Apple's patch for this changes the descriptors for those memory ranges to "DENY" to prevent their future access.



LEO:  Well, that further confirms that there's no reasonable use for this, if you could just turn it off.



STEVE:  Right.



LEO:  It doesn't break anything.



STEVE:  Right.



LEO:  It's not being used by anybody.



STEVE:  That's right.



LEO:  But it's in there, and it's carefully put in there.



STEVE:  Yup.  The other question, since the hash function is not obscure, it is truly secret, is how did this secret come to be in the hands of those who could exploit it for their own ends?  No matter why Apple had this present in their systems, that could never have been their intent.  And here I agree with the Kaspersky guys when they say "This is a flawed approach because sooner or later all secrets are revealed."  Somewhere, Leo, exactly as you said, people within Apple knew of this backdoor.  They knew that this backdoor was present, and they knew how to access it.  And somehow that secret escaped from Apple's control.



LEO:  Do you think it's possible Apple put that in there at the bequest of a government, a China or Russia?



STEVE:  Well...



LEO:  As a backdoor?  And yeah, we have a backdoor you can use?



STEVE:  That's two paragraphs from here.



LEO:  Okay.  Keep going, sorry.



STEVE:  What we do know...



LEO:  You've got my mind going.  This is fascinating.



STEVE:  This is very good, Leo.  Now you know why we're talking about nothing else this week.



LEO:  Yeah.



STEVE:  What we do know is that, for the time being at least, iDevices are more - that is, after this patch - more secure than they have ever been because backdoor access through this means has been removed, after it became public.  But because we don't know why this was present in the first place, we should have no confidence that next-generation Apple silicon won't have another similar feature present.  Its memory-mapped I/O hardware location would be changed, its hash function would be different, and Apple will likely have learned something from this debacle about keeping this closely held secret even more closely held.



Do they have a secret deal with the NSA to allow their devices to be backdoored?  Those prone to secret conspiracies can only speculate.  And even so, it does take access to additional secrets or vulnerabilities to access that locked backdoor.  So using it is not easy.



LEO:  Could Apple, though, have gone to somebody and said here's the step-by-step?  They would know.  I mean, it does involve this font resizing in TrueType to get in in the first place.



STEVE:  Well, yeah, those are particularly - those are two particular zero-days.  As we know, Apple is constantly patching against zero-days.



LEO:  They don't want these, yeah, yeah.



STEVE:  Right?  So...



LEO:  They may have a more direct access to this that doesn't require a zero-day, perhaps.



STEVE:  Well, remember this thing has been in place now since the A12.



LEO:  Five years.



STEVE:  Yes, yes.  So it's been blocking things, yet people have still been getting in.  This may have been used for a long time, and Kaspersky, I mean Kaspersky, finally just found it.



LEO:  And we don't know - the reason they found it was it was embedded on iPhones of many Kaspersky employees.



STEVE:  Yes.



LEO:  But what we don't know is...



STEVE:  The bad guys...



LEO:  ...who did that.  It's almost certainly a nation-state.  Would you agree?



STEVE:  You know...



LEO:  It's got to be [crosstalk] or somebody; right?



STEVE:  It was a mistake to attack Kaspersky.



LEO:  Yeah, maybe that, yeah.



STEVE:  Now the world knows, and Apple turned it off.  Was Apple forced to turn it off?  Are they glad they turned it off?  Were they surprised?  You know, I would prefer to believe that Apple was an unwitting accomplice in this, you know, that this was an ultra-sophisticated supply chain attack, that some agency influenced the upstream design of modules that Apple acquired and simply dropped into place without examination, and that it was this agency who therefore had and held the secret hash function and knew how to use it.



As they say, anything's possible.  But that stretches credulity farther than I think is probably warranted.  Occam's Razor suggests that the simplest explanation is the most likely to be correct.  And the simplest explanation here is that Apple deliberately built a secret backdoor into the past five most recent generations of their silicon.



One thing that should be certain is that Apple has lost a bit of its security shine with this one.  A bit of erosion of trust is clearly in order because five generations of Apple silicon contained a backdoor that was protected by a secret hash whose only reasonable purpose could have been to allow its use to be securely made available, after sale, in the wild.  And that leaves us with the final question:  Why?



LEO:  And we can only speculate.  I mean, we don't know who put it on the Kaspersky iPhones.  Could have been the Russian government.  Much more likely the U.S. government.  Or the NSA.



STEVE:  Somebody had knowledge, yes. 



LEO:  It's speculation, but it's not a long stretch to think that the NSA required a backdoor of Apple.  Apple complied, gave the NSA...



STEVE:  Made it super inaccessible.



LEO:  Made it hard.



STEVE:  It was safe.  It was safe to use.



LEO:  If you were going to design a backdoor, Steve, this is how you would do it.



STEVE:  Yes.  Yes.  It is absolutely safe.  And so yes, you're right, Leo, the NSA could have said, you know, we require a means in.  You know?  Make it secure.  And they made it secure.



LEO:  And we know because of the Patriot Act they could have done it with a National Security Letter that would require Apple not reveal that this had been done.



STEVE:  Yup, ever.



LEO:  Ever.



STEVE:  And certainly allowed Apple to foreclose it the instant it became public.  And they did.  And as I said, there is nothing now that would lead us to believe it will not reappear  under a different crypto hash in the next iteration of silicon.



LEO:  Well, and not to be conspiracy theory minded, but there's nothing to make us think that every other phone manufacturer hasn't done the same thing for the NSA.



STEVE:  Now, one interesting thing, and maybe this will come out after people have had a chance to look at other ARM silicon...



LEO:  That's a good question, is this an ARM problem.



STEVE:  If this was inherited by an upstream supply, we would expect other devices to have the same thing.



LEO:  Yeah.  Yeah.  So that's what's unknown.  Is this Apple only?



STEVE:  Is this exclusive to Apple, or do other devices have this, too?  The problem is...



LEO:  It looks like it is, though.  Yes?



STEVE:  You have to catch it in use.  I mean, Kaspersky only caught it because they had malware that was doing it.



LEO:  It makes for, by the way, an excellent spy novel or movie because the secret is a 256-byte chunk that could be sitting on any variety of things.  It could be put in a JPG with steganography.  It makes for a very interesting spy novel.  Possibly the NSA got to an Apple engineer.  But it'd have to be the Apple engineer.  I can't imagine this secret was held by many people.  Maybe not by any one person at all.  If I were Apple, I'd split it up.  Very interesting.



STEVE:  Yeah.



LEO:  Very interesting.  You know, I mean, the most obvious explanation is, as you said, the NSA and a secret backdoor.



STEVE:  Yup, said you have to put this in because we need access.



LEO:  Yeah.  In which case maybe you can't find fault with Apple because that's the problem with being a business in the U.S.  You have to follow the laws of whatever country you're doing business in, whether it's China or the U.S.  They're not in Russia anymore.



STEVE:  And Apple could argue it is safe.  I mean, what they did, it might as well not exist except for the fact that the secret got loose.  Well, or maybe it didn't.  Maybe the NSA still has it closely held.  On the other hand, reverse engineering the malware, and now we see.



LEO:  That's right, we only know because we've got the malware.



STEVE:  Right.



LEO:  You know, somebody in the Discord is pointing out, Adrod points out that both the Chinese government and the Russian government have within the last year forbidden the use of iPhones in government business.



STEVE:  Yeah, that's true.



LEO:  Wow.  This is quite a story.



STEVE:  This has been present for the last five years.  This went into - this first appeared in the A12, five generations ago.



LEO:  And if they can do it once, they can do it again.  There's no guarantee that...  



STEVE:  That's exactly my point.  If they turn this one off, it'll be in the next generation of silicon.  And again, we'll have no idea.



LEO:  Well, for all we know there's three backdoors in the existing generation.



STEVE:  That's a very good point.  That's a very good point.  There could be two different, two other hashes and the same access so that if one is discovered, the other two remain active.



LEO:  I am going to vote in favor of Apple and say they were compelled to do this by the NSA.  They had no choice.  They couldn't reveal it.  They did it in as secure a way as possible.



STEVE:  All the evidence suggests that so far.



LEO:  And I think in their favor they had no choice.  I mean, if the NSA says it, you get an actual security letter, you don't have a choice.  And there's a FISA court somewhere that ordered this.



STEVE:  Wow.



LEO:  Wow.  The problem with this is you just listened to one of the most technical podcasts for the last two hours.  There are very, very few people who would listen to this, understand it, and understand the implications.  And that's another thing that - that's another form of obscurity because I wouldn't expect even Ron Wyden or his very, well, Chris Soghoian would understand.  Maybe Chris Soghoian is whispering in Ron Wyden's ear right now and saying, hey, you know, this is a problem.



STEVE:  Yeah, we've got to ask Apple what the hell.



LEO:  What is going on here.  What a must-listen-to episode.  As always, Steve, thank you so much for your deep work.  When I said on MacBreak Weekly I'm sure Steve will cover this, I had no idea how deep this rabbit hole was going to go.  Fascinating.



Steve Gibson is at GRC.com.  SpinRite, what's going on?  How soon?  Don't say a date.  That's all right.  You can't.  You can win with it.



STEVE:  I can't.  I was actually a little frantic over the holidays trying to make my own deadline of before the end of the year.



LEO:  No, no, no.  Do it right.  Do it right. 



STEVE:  And I have no choice.  Everyone will get the payback of my getting it done right.



LEO:  This is why we love Steve.  He does it right, first.  He doesn't wait, say I'll fix it in post.  GRC.com.  You can get a 6.0 version of SpinRite, which is the current version.  You'll get a free upgrade to 6.1 as soon as it's out.  You'll be the first to know.  You can even participate - as you can see, many have - in its development.



Copyright (c) 2024 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#956

DATE:		January 9, 2024

TITLE:		The Inside Tracks

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-956.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  I want to start off this week by following up on last week's podcast about the hardware backdoor discovered in Apple's silicon, to support the conclusion I've reached since then, that this was deliberate on Apple's part, that they always knew about this, and why.  Then we're going to wonder whether everyone is as cyber-vulnerable as Ukraine appears to be.  And if so, why and just how serious could cyberattacks become?  What's the latest on the mess over at 23andMe?  How's cryptocurrency been faring; and are things getting better, staying the same, or getting worse?  What Google Mandiant account got hacked?  Just how seriously, and legally, do we take the term "war" in "cyberwar," and what are the implications of that?



LastPass recently announced some policy changes; even if they are about two years late, what lessons should the rest of the 'Net take away?  During 2023, how did Windows 11 fare against Windows 10?  What happens when users discover that Chrome's Incognito mode is still tracking them?  And then, after exploring some questions from our terrific listeners, I want to share the result of some interesting research I conducted last week during the final days of the work on SpinRite 6.1 for this week's podcast, titled "The Inside Tracks."



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  An update on that CVE that provided a backdoor into Apple hardware.  He's got some interesting afterthoughts.  He also talks a lot about cyberwarfare and the vulnerabilities in Ukraine, plus 23andMe's latest disclosure.  Is it really enough?  All that and more coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 956, recorded Tuesday, January 9th, 2024:  The Inside Tracks.



It's time, I know you've been waiting all week for it, Security Now! on the air.  Steve Gibson, our man about town.  He's the expert on privacy, security, how things work, and really I think one of the most trusted people in this business.  He's right here.  Hi, Steve.



STEVE GIBSON:  Hey, Leo.  Great to be with you again, as we're plowing through year 19 on the podcast.	



LEO:  Yikes.



STEVE:  Yeah.  Okay.  So this was a little bit sparse week in terms of security news.  And I have to say it's difficult to follow last week's blockbuster with something similar, although I do think I have something I'm lining up for next week.  So, but there's still lots to talk about.  I want to start off this week by following up on last week's podcast, you know, which was obviously about the hardware backdoor that had been discovered in Apple's silicon, to support the conclusion I've reached since then, as I've continued to think about it, that this was deliberate on Apple's part, that they always knew about this, and why.



Then we're going to wonder whether everyone is as cyber-vulnerable as Ukraine appears to be; and, if so, why, and just how serious could cyberattacks become?  What's the latest on the mess over at 23andMe?  How's cryptocurrency been faring, and are things getting better, staying the same, or getting worse?  What Google Mandiant, you know, their big security firm, account got hacked?  Just how seriously, and legally, do we take the term "war" in "cyberwar," and what are the implications of that?  LastPass recently announced some policy changes.  Even if they're two years late, what lessons should the rest of the 'Net still take away from this?



During 2023, how did Windows 11 fare against Windows 10?  What happens when users discover that Chrome's Incognito mode is still tracking them?  And then, after exploring some questions from our terrific listeners, I want to share the result of some interesting research I just conducted last week during the final days of the work on SpinRite 6.1.  Thus today's podcast is titled "The Inside Tracks."



LEO:  Oh, of the inside tracks of your hard drive.



STEVE:  Uh-huh.



LEO:  Yeah.  Oh, I like that.  Someday hard drives will no longer have inside tracks, but...	



STEVE:  Some interesting news.



LEO:  Yeah.  Oh, good.  Steve, I'm ready for the Picture of the Week.  I've got it all cued up here.  Haven't looked at it yet, though.  I like to be surprised.  Oh, I get it.  That's great.  Here, let me show everybody.



STEVE:  It's perfect for the podcast.



LEO:  Yeah.



STEVE:  Perfect for the podcast.  So this is the, today actually, on January 9th is the 17th anniversary or birthday of the introduction of the Apple iPhone.



LEO:  Yes.



STEVE:  And anyway, so this is just an apropos birthday cake.  And really, Leo, when I thought about it, until I realized, okay, most people wouldn't get it.



LEO:  Wait a minute.  There's only eight candles, Steve.



STEVE:  That's true.



LEO:  And only two of them are it.



STEVE:  That's true.



LEO:  Well, is that binary?



STEVE:  Uh-huh.  Eight candles is a byte; right?  An 8-bit byte.  And, yes, binary.  So the candle representing 16 is lit.



LEO:  That's hysterical.



STEVE:  As is the candle representing one.  Now, what occurred to me though is, first of all, you could probably get away with only seven, right, because that would bring you up to 127 years old, which that's, you know, humans don't live that long.



LEO:  Plus you want a significant bit so you can have negative numbers, too.



STEVE:  Uh, well, that would be interesting, yes.



LEO:  Yes.



STEVE:  Now, the danger is, though, that you can only approach this cake from the front.



LEO:  From the right side, yeah, yeah.



STEVE:  Yes.  Because if it's backwards, then 17 turns into 136.



LEO:  Well, save it because in 129 years, no, 119 years, you'd be able to use this.



STEVE:  Leo, I'm afraid this podcast will not last that long.



LEO:  Okay.



STEVE:  We're going past 999, but not to 9999999.



LEO:  We can flip it.  I love it.  All right.  Very nice.



STEVE:  Okay.  So I want to begin, as I said, with a bit of a follow-up to last week's news.



LEO:  Yeah, we've been talking about it.  We talked about it on MacBreak Weekly today.  We talked about it on TWiT on Sunday.  I think it's really a big deal story, which I'm not seeing anywhere but here.



STEVE:  That's exactly right.  And in fact one of the - I have some, we have some Q&A later.  And one of our listeners said, how is this not getting more attention? 



LEO:  Yeah.



STEVE:  Anyway, we will talk about that when we get there.  But the way we left things last week in the wake of this revelation was with a large array of possibilities.  Since then, I've settled upon exactly one, which I believe is the best fit with every fact we have.  You know, again, no speculation here.  Although, again, we're never going to have a lot of answers to these questions.  Many people sent notes following up on last week's podcast.  Many doubted the NSA conspiracy theory because of those other, not easy to effect steps, involving other clearly inadvertent mistakes in Apple's code, that were needed by this particular malware.



I don't know why it didn't occur to me last week, but it has now.  As we know and have covered here in great detail in the past, Apple has truly locked their iPhone down every way from Sunday.  I believe from all the evidence and focus that Apple has put into it, that Apple's iPhones are truly secure.  But would Apple actually produce a smartphone handset that they, and I mean they, absolutely positively truly could not get into, even if it meant the end of the world?



LEO:  Oh, that's a very good point.



STEVE:  Right.  If Apple believed that they could design and field a truly and totally secure last resort backdoor means of accessing their devices in the event that the world depended upon it, I believe that they would have designed such a backdoor.  And I believe that they did, deliberately and purposefully.



LEO:  For their own use.



STEVE:  Yes.  And I do not think less of them for it. In fact, I think that the case could be made that it would be irresponsible for Apple not to have provided such a backdoor.



LEO:  Yeah.  What if Dr. Evil had an iPhone with the launch codes on it; right?



STEVE:  Well, that's where I'm going here.  We'll likely never know whether any external agency may have made them do it.  And yes, doing so could hardly be more controversial.  But I can imagine a conversation among just a very few at the very top of Apple, Tim Cook and his head of engineering and of security.  They had to have had a conversation about whether it should be possible, under some set of truly dire circumstances, for them to get into somebody else's locked phone.  Obviously, the security of such a system would be more critical than anything.  But their head of engineering security would have explained, as I did last week, that as long as the secret access details never escaped, because it's impossible to probe anything that must be accompanied by a signature hash, there would truly be no way for this backdoor to be discovered.  As I said last week, from everything we've seen, it was designed to be released in the field where it would be active, yet totally safe.



So if Tim Cook were told that Apple could design and build-in an utterly secure, emergency, prevent the end of world escape hatch into their otherwise utterly and brutally secure devices, and this escape hatch could never possibly be opened by anyone else ever, I imagine Tim would have said, under those conditions, yes.  I think that most CEOs who are in the position to understand that with great power comes great responsibility, when assured that it could not possibly be maliciously discovered and abused to damage their users, would say, yes, build it in.



I trust Apple as much as it's possible to trust any commercial entity operating within the United States.  I believe that they absolutely will protect the privacy of their users to the true and absolute limit of their ability.  If the FBI were to obtain a locked iPhone known to contain, exactly as you said, Leo, the location and relevant disarming details of a nuclear weapon set on a timer and hidden somewhere in Manhattan, I would be thanking Apple for having the foresight to create a super-secure means for them, and them alone, to gain entry to their device.



And I'd argue that in doing so they did have the best interests of their customers in mind.  In this scenario, a great many iPhone users' lives would be saved.  There are all manner of conspiracy theories possible here.  Yeah, obviously.  And this one of mine is only one of many.  But of all the possible theories, I believe this one fits the facts best and makes the most sense.



Of course, the first thing everyone will say is, "Yeah, but Gibson, they did lose control of it, and it was being used by malware to hurt some of Apple's users."  And that's true.  In fact, that's the only reason the world learned of it.  If this scenario is correct, Apple never divulged this to any entity, and never would.  This would never have been meant for the FBI, CIA, or NSA to have for their own use.  If an impossibly high bar were reached, Tim Cook would say "Have an agent bring us the phone, and we'll see what we can do."



But somewhere within Apple were people who did know.  Perhaps someone inside was set up and compromised by a foreign group.  Perhaps Apple had a longstanding mole.  Perhaps it was gambling debt, or the threat of some extremely embarrassing personal information being disclosed.  One thing we've learned and seen over and over on this podcast is that when all of the security technology is truly bulletproof, the human factor becomes the lowest hanging fruit.  Just ask LastPass how they feel about the human factor, which bit them badly.



Okay, so where does this leave us today?  We know that all Apple iPhones containing the A12 through A16 chips contain this backdoor and always will.  We don't know that it's the only backdoor those chipsets contain, as we touched on last week.  But Apple doesn't need another backdoor since they still have access to this one.  They locked a door in front of this one, but they can always unlock it again.  After being contacted by Kaspersky, Apple's iOS updates blocked the memory-mapped I/O access that was discovered being taken advantage of by malware.  But Apple is able to run any software they choose on their own phones.  Which means that Apple still has access to this backdoor, should they ever need to use it.



And this means that they've lost plausible deniability.  They have the ability to open any iPhone that they absolutely must.  So this poses a new problem for Apple when law enforcement now comes knocking with a court order, as it's almost certain to, with way-below-that-bar requests for random iPhone unlocking to "assist" in this or that case.  So this is a new mess for Apple.  I'm sure they're facing that.



Apple's most recent silicon is the A17, yet Kaspersky told us that this facility had only been seen in the A12 through A16.  If the malware did not contain that initial unique per-chip-generation unlock code for the A17 silicon, and we know that it didn't, then this backdoor might still be present in today's iPhone 15 and other A17-based devices.  That's the most reasonable assumption, since it was there for the first, you know, for the previous five generations.  Apple obviously likes to have it.  But what about the next iteration of silicon for the A18?



Another thing we don't know is what policy change this disclosure may have for the future.  We don't know how committed Apple was to having this capability, but I think I've made a strong argument for the idea that it has to have it.  Have they been scared off?  Well, maybe.  We'll see what happens now, you know, as I said, with law enforcement asking them to unlock everybody's iPhone.  Will they move it to a different location within the ARM's 64-bit address space, yet keep it around?



As we were after last week, we're left with a handful of unanswered and unanswerable questions.  But my intention here was to hone this down to explore what appears to be the single most likely scenario.  Apple designed, and is quite proud of, the GPU section of those chips which contains the backdoor hardware.  There's no chance they were not aware of the little hash-algorithm-protected DMA engine built into one corner of the chip.  People within Apple knew.



Listeners of this podcast know that I always separate policy decisions from mistakes, which unfortunately happen.  So I sincerely hope that Apple's policy was to guard this as perhaps their most closely held secret, and specifically that it was never their policy to disclose it to any outside agency, of any kind, for any reason.  Somewhere, however, a mistake happened.  And I'd wager that, by now, Apple knows where and how that mistake occurred.



LEO:  What about the assertion that it may be as an ECC, that that SBox is an ECC table?



STEVE:  We've seen the SBox.  That's not error correction.



LEO:  It's not.



STEVE:  In fact, when I first saw that posting, I was excited that there might be somebody who actually had some information.  But they know nothing more than anybody else.  The idea that that hash is ECC is complete nonsense.  And so for me, I just thought, okay, well, these guys don't know anything more than the rest of us do.  



LEO:  It's easy to speculate, yeah.



STEVE:  Yeah.  Well, we know what that is.  That's not ECC.  Anybody who knows about error correction, and I have to because of SpinRite, knows that's got nothing to do with error correction.



LEO:  Aha.



STEVE:  So that was just - that was complete nonsense.



LEO:  It was B.S., okay.



STEVE:  Yes, yes.



LEO:  All right.



STEVE:  Error correction, you feed something in.  It does a lot of work.  You get a syndrome, which is an XOR mask against the data, and it also presides a position of where that mask is slid to provide the XORing which flips the bad bits into good bits.  That's not what that SBox is.  That SBox is a simple hash.



LEO:  Cool.



STEVE:  So that's not ECC.  But I'm glad you connected with this thought of mine, Leo, that, you know, of course they have...



LEO:  There had to be a backdoor.  Yes, of course.



STEVE:  Yes.  Of course they have, they and they alone, to prevent the end of the world, can open up any of their devices.



LEO:  That makes sense.  And then they would super-strong secure it.



STEVE:  Yup.



LEO:  And of course they didn't need to go through the other three exploits to get to that part.



STEVE:  Right.



LEO:  Bad guy would, right.



STEVE:  Right.  And they didn't even know it was being abused until Kaspersky said, uh, look what we found over here.  And they're like, oh, fit, you know, [crosstalk].



LEO:  But this is the problem with a backdoor of any kind is it's impossible to secure.



STEVE:  And that's what everyone has said.  Yes, I mean, that argument that, you know, and this of course has been the argument that everyone has always used against, you know, the EU saying, oh, just give us a way in.  It's like, no.  It will be abused.  This was.  And this is really unfortunate.  But I don't think Apple was wrong to do it.



LEO:  No, when you put it that way it makes sense.  It seems like a good thing to do.  And now I'm hoping that Dr. Evil doesn't put the nuclear codes on his iPhone, unless they have another backdoor, which is possible.



STEVE:  Well, they haven't lost this one, Leo.  This one is still available.



LEO:  Oh.



STEVE:  All they have to do, I mean, they...



LEO:  They can reenable it.



STEVE:  They locked a door in front of this one with an update.  They can un-update any phone that they want to.



LEO:  Oh, of course, duh.



STEVE:  Yeah.  So they still have full access to A12 through A16 get out of jail free card.  And this is the problem, too, because now the FBI, well, if the FBI didn't know, the NSA will now tell them.  Maybe someone is listening to this podcast.  Now Apple can open any, you know, now that cat is out of the bag.



LEO:  Yeah.



STEVE:  They can no longer say what they have been saying to the courts, we have no way in.



LEO:  Oh, interesting.



STEVE:  Yes, you do.



LEO:  Oh, boy.  That's interesting, too.



STEVE:  You always have had a way in.  You just didn't tell anyone.  Well, now we've found out because it leaked.  It leaked from inside Apple, unfortunately.



LEO:  Okay.



STEVE:  So we talked earlier about the interesting abuse of Internet-connected cameras by Russia to obtain attack and improved targeting of their weapons strikes inside Ukraine.  I saw a bit of an update on that last week after Russian hackers successfully hijacked at least two security cameras and used those cameras' live video feeds to adjust their missile strikes targeting the city of Kyiv last week.  Once Ukraine's SBU, which is their Security Service, detected the attacks, they took down the cameras to prevent their further abuse.  In part of the coverage of that was the information that, since the start of Russia's invasion, more than 10,000 security cameras have been taken down across Ukraine.  Ten thousand.



So hearing this, you know, I kind of paused to wonder and also to worry about the situation here in the U.S., in Europe, and with any of our allies.  Back when we began this podcast, Leo, you know, when loincloths were in fashion...



LEO:  We're even older than the iPhone.



STEVE:  That's right, baby.  The idea of "cyberwar" was still squarely the stuff of fiction.  From everything we hear, there are constant low-level "cyber skirmishes" going on all the time.  We know that the usual suspects of China, North Korea, and Russia have talented hackers who are more or less continually poking around inside our computer networks.  And for our own sake, I hope we're giving at least as well as we're getting, since a cyber standoff is in everyone's best long-term interest.



Everyone in the world, however, is pulling from the same common pool of technology.  So we're probably all about equally vulnerable to each other.  There's no reason to believe that the cameras we have everywhere here in the states and that Europe has through their countries are any more secure than those that Ukraine was using.  And the same applies to all of our other interconnected technology.  I've often wondered what I would do if I were starting out in the world today.  I've always had a strong intellectual curiosity about whether I could hack other people's stuff.  But doing so is both unethical and illegal.  So I never have.



But participating in my country's defense and, if necessary, its offensive operations, I have to admit, that has some appeal, while also resolving the ethical and legal roadblocks.  One thing is very clear.  This is no longer the stuff of sci-fi.  It's very real.  And it appears that our countries need us.  So I've said it before, I'll say it again to our younger listeners who maybe haven't chosen a career path.  Get really good at this stuff; and, you know, your country needs you.  Seriously.  We know that our country is hiring, and this is real.  And, boy, it would be a lot of fun.  I don't know about the camo, though, Leo.



LEO:  You don't have to wear camo.  You can wear your BDUs at home and wear civvies to work.  It's okay.  I give you permission.  General Leo says it's okay.



STEVE:  We have heard, however, we've heard from our listeners, oh, no, I've got to put my camo on every morning.  But they did assure me, though, that they're comfortable, that they're not like stiff and starchy.



LEO:  Oh, yeah.  Oh, yeah, yeah.



STEVE:  You know, they've got some sweat rings under their armpits, so that's good.  Okay.  So I think this topic is important enough for me to spend a bit more time on a specific example.  Last Thursday, Reuters news service published an article titled "Russian hackers were inside Ukraine telecoms giant for months."  So here's some new information that was not public before that Reuters just published.  They said:  "January 4th.  Russian hackers were inside Ukrainian telecoms giant Kyivstar's system from at least May last year in a cyberattack that should serve as a 'big warning' to the West, Ukraine's cyber spy chief told Reuters.



"The hack was one of the most dramatic since Russia's full-scale invasion nearly two years ago knocked out services provided by Ukraine's biggest telecoms operator for some 24 million users for days starting December 12th.  In an interview, Illia Vitiuk, head of the Security Service of Ukraine's (SBU) cybersecurity department, disclosed exclusive details about the hack" - which I would call more than a hack - "which he said caused because, he said, caused 'disastrous' destruction and aimed to land a psychological blow while gathering intelligence."  He said:  "This attack is a big message, a big warning, not only to Ukraine, but for the whole Western world to understand that no one is actually untouchable."



He noted that Kyivstar was a wealthy private company with heavy investments in cybersecurity.  The attack wiped "almost +everything," including thousands of virtual servers and PCs, he said, describing it as probably the first example of a destructive cyberattack that "completely destroyed the core of a telecoms operator."  Later, following some investigation, on December 27th he said that they found that the hackers probably attempted to penetrate Kyivstar originally in March or earlier, much earlier last year.  He said:  "For now, we can say securely that they were in the system at least since May of 2023.  I cannot say right now when they had full access, probably at least since November."  The SBU assessed the hackers would have been able to steal personal information, understand the location of phones, intercept SMS messages, and perhaps steal Telegram accounts with the level of access they had gained.



A Kyivstar spokesperson said the company was working closely with the SBU to investigate the attack and would take all necessary steps to eliminate future risks, blah blah blah.  Of course that's, you know, the PR guy at the company that got blasted.  And following the major break there were a number of additional attempts aimed at dealing more damage to the operator.



Kyivstar is the biggest of Ukraine's three main telecoms operators, and there are some 1.1 million Ukrainians who live in small towns and villages where there are no other choices, no other providers.  People rushed to buy other SIM cards after the attack, which created large lines.  ATMs using Kyivstar SIM cards for the Internet all ceased to work; and the air-raid sirens, which are used during missile and drone attacks, also did not function properly in some regions.



Post attack forensics are made more difficult because of the wiping of Kyivstar's entire infrastructure.  But Vitiuk said he was "pretty sure" it was carried out by Sandworm, a Russian military intelligence cyberwarfare unit that has been linked to cyberattacks in Ukraine and elsewhere.  A year ago, Sandworm penetrated a Ukrainian telecoms operator, but was detected by Kyiv because the SBU had itself been inside Russian systems.  Vitiuk said the pattern of behavior suggested telecoms operators could remain a target of Russian hackers, and during 2023 the SBU said it had thwarted over 4,500 major cyberattacks on Ukrainian government bodies and critical infrastructure.



So, okay, again, sadly, there's no actual reason to believe that things are any different anywhere else.  Ukraine is using the same technology as everyone else.  As I've said, all of the evidence we have suggests that our actual security is far more soft than we would like.  What's generally and thankfully missing is the motivation to abuse it.  But the rise of cryptocurrency created the motivation to extort enterprises that smugly believed until then that their IT security budget was sufficient, and that the threats were being overblown.  No one thinks that any longer.  The last thing we need is an escalation.



LEO:  Wow.



STEVE:  Uh-huh.  I know.  It is sobering.  You know, we've got little poking around the edges.  But as I said, I hope that we're able to give as good as we get because, you know, we only hear about attacks against us.  We don't get any information about, you know, what we're doing, how we're in other people's networks.  But as I said, there's a career there.



LEO:  I also think there's some reluctance to go full bore on this because you really, I mean, the obvious end game is attacking infrastructure, which could be horrific to civilians.



STEVE:  Well, and when you do so, you're no longer covert.



LEO:  Right.



STEVE:  So it is a use it and then lose it.  So nobody wants...



LEO:  Right.  And the threat of retaliation is so strong because you don't need - it's not like building a nuclear weapon.  You know, you need a few good hackers.  And probably almost any nation-state could muster up enough hackers to be a threat.



STEVE:  Leo, North Korea.



LEO:  Right.



STEVE:  Like, where did they get their education?  They didn't come over here and get taught at MIT.



LEO:  Actually, apparently some did.  But anyway - covertly.



STEVE:  Whoops.



LEO:  But that's the point is that this information is out there.  And it doesn't cost a lot to create a Fancy Bear.  So it doesn't, you know, it's really an interesting issue.  It's not the end of the line, I just want to say.



STEVE:  You do not need huge rooms of spinning centrifuges...



LEO:  No, exactly.



STEVE:  ...and years of time.



LEO:  Right.



STEVE:  You know, you need literally some guy in his mother's basement.



LEO:  Literally.



STEVE:  And Leo, let's take a break, and we're going to talk about 23andYou.



LEO:  And Me.  And I'm excited, or not excited, but I'm interesting being a longtime 23andMe user.  And I was really kind of mad at the company because they said it was our fault.



STEVE:  You should be.  You should be, Leo.  I know, you should be.



LEO:  Now, tell me how much trouble I'm in with my DNA; okay?



STEVE:  Okay.  So things are still a mess at 23andMe.  They've been hit with 30, three zero, lawsuits.



LEO:  Oh, gosh.



STEVE:  Since last - uh-huh.  Yeah, I mean, and people take their DNA as like a privacy issue.  Who would have thunk?



LEO:  Mm-hmm.



STEVE:  So anyway, this was, you know, last December was the revelation of the breach which disclosed the personal information - and this is the number that stuns me - of 6.9 million of their users.  Okay, now, just to remind everyone, the story is that 14,000-some accounts were first directly compromised using simple credential stuffing, you know, reusing known, previously used passwords of each victim.  I would argue that this had to be detectable right there, but you won't see what you're not looking for, and nothing else these guys did seems particularly impressive on the IT side, so...



LEO:  They weren't looking.



STEVE:  They apparently weren't looking.



LEO:  They weren't looking.  They had their heads turned.



STEVE:  So, okay.  From there, we're told that simply using the API that was available to any logged-on user - since that's all these bad guys apparently were - the attackers were then able to siphon off the personal data of an additional, expanded, 6.9 million unbreached users.



Now, I suppose I'm still skeptical about this explanation because in my gut I find it difficult to believe that the designers of 23andMe's architecture could deliberately have set things up so that anyone logged into their system could have direct access to, on average, the personal data of 493 other members.  6.9 million divided by 14,000 is 492.857, which is the average "disclosure reach" of each of 23andMe's 14,000 logged-on users.  In order to believe that this is what actually happened, 23andMe's system had to be horribly designed from the start, which is quite dispiriting.



And then, in the wake of this catastrophe of their own making, adding insult to injury, 23andMe attempted, and I'm sure you saw this, Leo, to change the Terms of Service for their users retroactively, if you can believe that, to require them to agree to settle any disputes through arbitration in lieu of other legal action.  That didn't pass notice, and you can imagine that it didn't go over very well.



As we know, anyone can make a mistake.  But they're directly responsible for their apparently incredibly crappy system design which, again, if they're to be believed, allowed any legitimate user to log on with their own credentials, and then have access to the personal details of, on average, nearly 500 other users who they don't even know.  So, wow; you know?  It looked like a good deal.  I'm also a member, for what it's worth.  I spit in the tube years ago because it seemed like a curiosity.  Wow.



Okay.  And speaking of the incentives created by cryptocurrency, the Estonian cryptocurrency platform CoinsPaid - don't think they knew who their coins were going to be paid to - was the victim of another cyberattack, losing an estimated $7.5 million worth of crypto assets. I said "another" since this is the second time this company has been hacked.  The first time it lost $37.3 million in July.  CoinsPaid blamed last year's incident on, guess who, North Korea.  I wonder who you call in North Korea to negotiate a settlement?  "Hey, how 'bout we'll give you a 10% bounty and no hard feelings if you'll return the rest?"  Right.  I wouldn't hold my breath on that happening.



Meanwhile, the Gamma cryptocurrency platform says it lost $6.1 million worth of assets after a threat actor abused the infrastructure of one of its providers to manipulate exchange prices, and another threat actor has stolen nearly $4.5 million worth of crypto assets from the Radiant Capital cryptocurrency platform.  The technique used there was a so-called "flash loan attack."  So I don't know where all this money is, you know, where it's coming from and going to.  But I am sure glad that none of it's mine.



LEO:  Somebody's getting a yacht.



STEVE:  My god, Leo.



LEO:  Geez.



STEVE:  Oh, yeah, we lost 37 million.  Well, you know, that's, you know, stuff happens.



LEO:  We can make more.  It's okay.



STEVE:  Call North Korea and ask them if they'll accept a bounty and give us 90% back.  No.  It's the North Koreans who are having a party and got a yacht.



LEO:  Yeah.



STEVE:  Okay.  But, you know, let's take a larger view.  Stepping back, overall, during all of last year, hacking attackers made off with more than $1.8 billion U.S. worth of crypto assets, and that was across 751 individual security incidents.  There is some good news here, though, since that number is way down, as in by half, from the $3.7 billion U.S. that were lost the year before, during 2022.  So that's good.  Either people are like pulling their money out of this crazy business, or security is beginning to get better.  I mean, it certainly was the Wild West there in the beginning, you know, when I don't remember the details, Leo, but Kevin was like buying icons of monkeys or robots or something?



LEO:  Yeah, he was.  He was buying bored apes.



STEVE:  Like what the heck?



LEO:  And then crypto punks.  And then, because he saw the writing on the wall, he offered his own icons.  They were owls.  And I think he and the consortium that did this, including a number of well-known NFT people, like people, made $50 million.



STEVE:  And where were we?



LEO:  It's unbelievable.



STEVE:  Ah, those youngsters.



LEO:  Those kids.



STEVE:  Anyway, according to the blockchain security firm CertiK, whom we've quoted from time to time, last year's top 10 most costly incidents accounted for more than $1.1 of that total $1.8 billion stolen in total.  So it's not like it's, you know, all thefts are equal; right?  The top 10 got 1.1 of the total 1.8.  And speaking of behind the curtain, the most costly incidents were linked to leaks or compromises of private keys.  So that's how these attacks, the biggest ones, happened was that people's private keys got loose.  More than $880 million was stolen just that way last year.



According to TRM Labs, North Korean hackers were linked to $600 million of those total stolen assets.  In other words, one-third of all the cryptocurrency lost last year, the $1.8 billion, went into North Korea.  So, yeah, maybe those are some of the MIT grads that they've got over there at work.  Anyway, they're not slouches.  



LEO:  Nope.



STEVE:  Oh.  And lest we believe that these things only happen to people with low security awareness, an unknown threat actor recently hijacked the Twitter account of Google's Mandiant division.  Right?  Like the high-end cybersecurity gurus.



LEO:  That doesn't bode well.



STEVE:  No, it doesn't.



LEO:  Not the best on your rsum.



STEVE:  No.  The account takeover was used to promote a - guess what? - cryptocurrency scam.  The attack was just one of a number of similar incidents that hit many high-profile Twitter gold badge accounts at the start of the year.



LEO:  I don't know if you saw this morning, the SEC, as in United States Securities and Exchange Commission, was hacked on Twitter to put an announcement they have just approved Bitcoin ATF spot purchases, which they were quick to point out they hadn't, and that was a hack.  So...



STEVE:  Nice.  Nice.



LEO:  I don't, you know, at this point I don't know if I want to blame the accountholders.  I don't know if I blame Mandiant.  Who knows what the Twitter security status is these days.



STEVE:  Good point.  Good point.  It could easily be someone inside.



LEO:  Yeah, yeah.  It could be [crosstalk].



STEVE:  I saw a little blurb...



LEO:  Who knows?



STEVE:  Well, I saw a little blurb that - it just passed by on my phone, saying that Elon had used illegal drugs and that executives at Tesla and SpaceX were concerned.



LEO:  I brought this up, this was a big...



STEVE:  Hey, you know.



LEO:  This was a big story in The Wall Street Journal on Sunday for some reason.  I brought this up on TWiT on Sunday.  And the panel said, "Everybody knows that.  We've known that for years."  Oh, my god, he's using illegal drugs.



STEVE:  Yeah.  And I would just note that there wouldn't be executives at Tesla or SpaceX, neither would exist were it not for Elon.



LEO:  Yeah.



STEVE:  And who knows?  Maybe as a result of some of those illegal drugs.



LEO:  Maybe there's a secret.  Maybe that's his secret, yeah.



STEVE:  That's right.  Okay.  So just how seriously, and legally seriously, do we take the term "war" when it's used in the phrase "cyberwar"?  Remember back nearly seven years ago - and Leo, you're going to get a kick out of this because you said, "Whaaaaaat?" at the time.  In 2017, the monster American pharmaceutical company Merck suffered a serious ransomware cyber breach by the NotPetya group.  What stunned us at the time, and as I said, I remember your, like, "Whaaaaaat?" was Merck, who was carrying significant cyberattack insurance, was claiming that the attack, which they said affected 40,000 of their PCs, would cost them $1.4 billion, with a "B," to clean up.



LEO:  And I still say "Whaaaaaat?"



STEVE:  And it's like...



LEO:  You're kidding.



STEVE:  Oh, wow.  So naturally, their cyber insurance carriers, of which there were three, were none too pleased by the prospect of having to fork over $1.4 billion to, what, finance Merck's physical replacement of their entire PC inventory?  I mean, it's not as if the machines melted.  So, what?  I would love to see the justification for this.  Anyway, we covered this of course at the time.  And recall that the three Merck insurers who were on the hook for this were attempting to get out of their policy obligations by claiming that an exemption applied in the case of "Hostile/Warlike Action."  That's literally in quotes.  That's what it says.  It's a commonly present policy exclusion.



So the question that has ever since then been working its way through our U.S. legal system was whether or not a "cyberattack" could and should be considered to fall under this standard "Hostile/Warlike Action" policy exclusion.  And of course this would be precedent-setting since devastating cyberattacks are no longer theoretical, and insurance to make enterprises whole in the wake of one have become crucial and ever more costly, both in premium and in reimbursement.



LEO:  Yeah, I think that that clause, by the way, is pretty common.  Acts of god and acts of war are often exempt, you know, because...



STEVE:  Yup, yup, exactly.  And so the question is...



LEO:  We can't be expected to insure against that.



STEVE:  ...is this an act of war?  So until last week, when New Jersey's state Supreme Court was set to hear oral arguments from both sides, a lower New Jersey appeals court had ruled that Merck was entitled to half of what they were seeking under their policy coverage.  In other words, $700 million.  The insurers still wanted to pay less, and Merck wanted more.  But just hours before oral arguments were set to begin, the parties announced that they'd reached a settlement, though the terms of that settlement have not yet been disclosed.  Given that Merck is a publicly traded company, owned by its stockholders, I would imagine that the terms of the settlement will eventually become known.



Interestingly, in amicus briefs filed before the scrapped oral argument, national associations for big business, manufacturers, and corporate insurance litigators had all argued the court to uphold the ruling that cybercrime did not fall under an insurer's "Hostile/Warlike Action" policy exclusion and plant a national flag, as they put it, on this issue to benefit insured businesses.  But the decision, it turns out, was not cut and dried.  Dueling briefs from international law scholars debated whether foreign-linked hacking against corporations is warlike action.



The takeaway for insurers should probably be that they are going to need to stand behind their cyberattack policies, and those paying for coverage by those policies should probably demand some explicit clarification from any policy that contains such potential wiggle room language because we haven't seen the last of cyberattacks.  And unfortunately, we didn't really get the hard precedent set that many people were hoping one way or the other that this case would create.  So, you know, last minute settlement, and it's like, okay, fine.  Move along.



Last Tuesday, LastPass posted a blog titled "LastPass Is Making Account Updates.  Here's Why."  So I'm just going to share the opening paragraph because, you know, we're all well able to read between the lines.  But LastPass said:  "You may have noticed that lately we've been asking our customers to make some changes to their LastPass accounts.  These changes include requiring customers to update their master password length and complexity to meet recommended best practices and prompting customers to re-enroll their multifactor authentication, among other changes.



All of these changes are intended to make our customers more secure, and we want to share additional context about the evolving cyberthreat environment that's driving these requests so customers can better understand why these changes are important.  To do this, we'll address some of these recent changes, and explain what threats are driving them, and how these updates are designed to help."



Okay.  So anyway.  And it goes on.  My only complaint, of course, is that it's closing the barn doors after the horses have all run off; you know?  This would have been very nice to see several years ago, and history would have been written differently had that been the case.  This effort is clearly an attempt to respond to the theft of the master data vault and to mitigate future disasters.  Requiring everyone to "reenroll" their multifactor authentication basically means get a new private key at each end so that if that has also somehow been compromised, nobody, you know...



LEO:  Okay, I was wondering about this.  So it's the theory being that the secret, which is key to a time-based one-time password, has also been leaked.



STEVE:  Right.



LEO:  Okay.



STEVE:  Or could be.  So they're just saying, let's, you know...



LEO:  Start over.  A new secret.



STEVE:  Yeah, no reason not to.



LEO:  Yeah.



STEVE:  It's sort of the equivalent of like people saying, oh, change your password just because we think it's time.  It's like, uh, okay. 



LEO:  Yeah, I've always felt like, wait a minute, if my password's good, why am I changing it?  But, yeah.



STEVE:  And as we know, that advice has been reversed now.  It is no longer thought...



LEO:  Yes, NIST took that back, yeah.



STEVE:  Yes.



LEO:  But it isn't a bad idea to occasionally redo your two-factors, it sounds like.



STEVE:  I would agree.  There's, I mean, [crosstalk] been some leakage.



LEO:  That secret is kept in the clear, generally.



STEVE:  Well, that secret is at their end and in your authenticator.



LEO:  Right.



STEVE:  So you don't want, you know, if there's been a breach, I mean, this sort of says that maybe they also lost their two-factor authentication data.  And, you know, instead of just their user vault data.



LEO:  It's hard not to read between the lines and say, what, you had another problem here?



STEVE:  Or this was a little more extensive than you said it was.



LEO:  Maybe.  Wow.  Holy cow.



STEVE:  Yeah.  Now, as for this new 12-character minimum password complexity requirement, that only makes sense.  And I want to talk about that a little bit.  What should really be happening at this point across the Internet, and I mean everywhere, is that users should begin to be forced to increase the security of their logons.  It should not just be happening at scattered sites in the wake of devastating attacks.  Any service that supports logons where a breach could have devastating consequences for its users should start doing the same.  Users really want to reuse "their," and I have that in air quotes, personal password everywhere.



LEO:  Yeah.



STEVE:  You know?  Monkey123 forever.  That's still today, 2024, that's the typical behavior.  Obviously, not among this podcast's listeners, but pretty much everywhere.  Never underestimate the strength of inertia.  Users do not want to change, and they will not change unless and until they are forced to.  We now have the technology to enforce password complexity rules on the user in their browser thanks to client-side JavaScripting.  Users hate password requirements.  Why? Because those requirements prevent them from using their favorite universal pet password everywhere.  And those requirements mean that they may need to deal with unique passwords per site, at least to some degree.  The question is whether the Internet should continue to let them.



If the Internet continues to allow this past behavior, it will never change.  We all know that.  Why would it change?  Users will need to be forced.  But every site is understandably terrified of doing that because they don't want to alienate their users.  The rational solution is for sites not to pretend that their users have security that does not exist.  If a site is not going to enforce a sufficiently high level of password complexity, then it should not assume that its users have any actual logon protection, and it should act accordingly.



Or perhaps the client-side JavaScript, which can see the user's plaintext password for itself before it is locally hashed and then sent to the server, should examine, the JavaScript should examine the password's complexity and send along a complexity ranking of the hashed password's strength.  Then a site that does offer some sensitive services could explain to its logged-on user that the password they are using is fine for logging on; but, for their protection, a better password will be required before they're allowed to do anything sensitive that they would not want hackers to be able to do in their name.



So I suppose I'm saying that the industry has clearly been dragging its heels because it has not been forced to change, and this has allowed users to, in turn, drag their heels and continue with habits that no longer serve their best interests.  Web portal designers would be well served to keep this in mind.  So good thing that LastPass said, okay, we're going to make some changes.  But gee, you know, had they been keeping up with current practice and recommendations, that would have been happening all along.  And talk about a base of users who would understand.  I mean, it's one thing, you know, to ask logons at  Granny's Cookies site to, you know, do complex passwords.  But LastPass?  Obviously people would be willing to do this.



LEO:  But also talk about inertia.  I mean, who's still using LastPass except somebody who's said I'm not going to change?  I'm still - I'm not going to change.



STEVE:  True.



LEO:  I mean, I would think many people would have done what you and I did and switched.



STEVE:  Yup.  Remember the podcast title was "Leaving LastPass."



LEO:  "Leaving LastPass," yeah.  I mean, I guess the theory is, well, now LastPass will be more secure than anyone because they got bit.  And so they're going to do everything they can not to get bit twice.  I guess.



STEVE:  Except that we know that they thought they were secure; right?



LEO:  Right.



STEVE:  I mean, that's always the conundrum.  They thought they had this covered.



LEO:  Right.



STEVE:  And whoops.



LEO:  Yeah, whoops.



STEVE:  So as of - just a little quickie.  As of the beginning of 2024, because this fascinates me, Windows 10 is holding onto two-thirds of the desktop, while Windows 11 has been gradually creeping upward from about 16% to now 26% of desktops across 2023.  People generally like what they have, and again, inertia.



LEO:  Yeah.



STEVE:  We should really rename the podcast.



LEO:  The Inertia Show.



STEVE:  Yeah.  I love what you and Jeff do, This Week in General.  That's a good title.



LEO:  Yeah, yeah.



STEVE:  And I think, you know, This Week in Inertia would be...



LEO:  I'm not changing.  I'm happy.



STEVE:  Yeah, just pry it out of my...



LEO:  This from the guy who's been on Windows 7 since, you know, before the Stone Age.  But okay.



STEVE:  Sitting in front of it right now.



LEO:  Okay.  Okay, Steve.  I notice Windows 7 is holding strong, by the way, has not gone down.



STEVE:  I think that's the yellow line on that chart; right?



LEO:  Yeah, yeah.



STEVE:  Yup.  Yeah, that's me.  I'm there.  In fact, there's a little uptick there, no, I can install another one.  No.  I'll be moving to Windows 10.



LEO:  Yeah, there's Steve, he's installed another one.



STEVE:  I'm Windows 10 in the evening.  I'm Windows 7 during the day because it works great.



LEO:  It's like a mullet, you know, Windows 7 in front, Windows 10 in back.



STEVE:  Is that there then a mulligan?



LEO:  Yeah, no, a mullet, you know, that's the haircut.



STEVE:  Okay.  So again, the following bit of Google tracking news made a lot of headlines recently, so I thought I would just mention it, too.  Remember back in 2020 when Google was found to be tracking users in "incognito mode," and this resulted in a ridiculously large class action lawsuit.  And just for the record, everybody, I know you already know it, but Leo and I are not generally fans of ridiculously large class action lawsuits.



LEO:  No.



STEVE:  Because it's just attorney-enriching.  So the news is that the lawyers on each side of this dispute have reached an agreement, as happens more often than not on the eve of such cases moving forward to trial.  When you're big, you tend to be a target of attack since the presumption, at least among scummy attorneys, is that it's worth some money from the big guy just to make the nuisance lawsuit go away because they're going to spend more money defending this nonsense than they are just saying, fine, here, buzz off.  At the same time, unfortunately, being big also increases the tendency of companies to throw their weight around, bully others, and imagine that they can get away with whatever behavior they want.



Thursday before last, U.S. District Judge Yvonne Gonzalez Rogers put the trial that had been scheduled for this case on hold in California after attorneys said they had reached a preliminary settlement.  Judge Rogers had previously rejected Google's bid to simply have the case dismissed, saying she could not agree that users consented to allowing Google to collect information on their browsing activity when in incognito mode.  The class action, which was filed in 2020 by the law firm Boies Schiller Flexner, and that's "Boies" as in David Boies...



LEO:  Oh, yeah.



STEVE:  Uh-huh.  Don't mess with David.



LEO:  Don't mess with David, nn-nnn.



STEVE:  Unh-unh, claimed that Google had tracked users' activity even when they set the Google Chrome browser in incognito mode.  It said this had turned Google into an "unaccountable trove of information" on the user preferences and "potentially embarrassing things."  It added that Google could not "continue to engage in the covert and unauthorized data collection from virtually every American with a computer or phone."  Oh, and I forgot to mention, the class-action lawsuit, $5 billion with a "B."



LEO:  Oh, boy.



STEVE:  Dollars.



LEO:  That's actually low for class-action.



STEVE:  More than the cryptocurrency, all the cryptocurrency lost in the last two years combined.



LEO:  Oh, my.



STEVE:  I'm sorry, you were saying...



LEO:  I think that's actually low for a class-action lawsuit.  But maybe I'm...



STEVE:  Against Google.



LEO:  Yeah.  Apple just decided to settle its half billion dollar lawsuit.  People are getting $92 each.  But you had to - so I guess half a billion compared to five billion.  Wow.



STEVE:  Yeah, 10 times.  Anyway, so I think my take on this is that it's a case of the fine print coming back to bite you.  Google claims that the users of their incognito mode were duly informed and knew that tracking was still occurring even though the post-incognito mode residues from their browsing such as history and cookies were not retained.  Apparently some of their users disagreed and felt betrayed.  So anyway, just, you know, another lawsuit settled.  The industry moves on.  Maybe this creates some pressure on Google to change this aspect of their behavior.  I don't imagine most people spend much time in incognito mode.  They only jump in to do something that they don't want to have...



LEO:  Well, it's really hide it from your spouse mode.



STEVE:  Yeah.



LEO:  And that's what people probably try to explain to people.  But they didn't do a very good job of it.



STEVE:  So as I said, this was a rather thin news week.  I think we made the best of it, talked about a lot of interesting stuff.  I think I may be onto an interesting independent analysis of the privacy protections created by Google's Topics API and other components of Chrome's privacy sandbox.  If it pans out, I'll have that for next week.



LEO:  Steve, you're up.



STEVE:  So as I mentioned at the top, we had a listener, Carl Smith, who sent a tweet.  He said:  "@SGgrc, how has Operation  Triangulation not received more press coverage?"  He says:  "This is huge," and then four exclamation points.  And of course as I mentioned, I agree with Carl.  I suspect that Apple is benefiting hugely from the fact that while what's really going on here, which obviously everyone who listens to this podcast understands, is truly monumental, it was also "patched" with yet another iOS update, and the public at large has no way of discerning that this one is any different from any of the others that preceded it through the years.



And really, you know, some Russian security analysts found something they presented during the Chaos Communications Conference in Hamburg?  What's that?  You know, that's not going to make the nightly news.  The popular news media cannot begin to explain this to the average consumer.  So I bet the news producer just says "Talk about the weather," while Apple breathes a huge sigh of relief in the knowledge that they didn't take any PR hit from what might have been a disaster for them.



LEO:  And let me put a plug in for you, Steve.  That's why people listen to this show, that's why they listen to TWiT, because we can cover technical stuff in a way that's intelligent, and so that you get that information.  If you're not a member of Club Twit, support what Steve's doing, twit.tv/clubtwit.  That's all I'm going to say.  But this is why you need us, and we need Steve.  Sorry, didn't mean to throw you off there.  I just, I couldn't resist.  It's like, this is why we do what we do, because the mainstream media's not going to cover this stuff.



STEVE:  No, never.  And even the tech press, it waters it down.  You know, I mean, they're in a hurry.  They've got lots of other stuff to do.  You know, this is just one of a gazillion stories that they're trying to cover.  For us it's like, whoa, hold on, stop the presses.  This is a podcast today.



So Vjirasek said:  "Hi, Steve.  Great work on SN-955."  That was last week.  He says:  "I am wondering why Apple has not implemented ROP attack protection similar to what Intel has done.  Would this break the chain of this sophisticated attack?  Also concerning to see that Apple has left the back door in the SoC to get in.  Thank you for your hard work."



So he's referring to the use of ROP, Return Oriented Programming, which we mentioned and talked a bit about last week.  It's a "living off the land" practice of using bits of code that's already present in the target device to obtain the effects that are needed for the attack.  I'm certain Apple has Return Oriented Programming attack prevention in place, as must any highly secure attack-prone operating environment these days.  But while ROP makes attacks far more difficult by scrambling and randomizing the memory locations occupied by code, that code is still present in memory.  It's just been moved at load time into initially unknown locations.



One of the things the Kaspersky guys noted was that a huge amount of the malware bulk was spent examining the system's memory.  Now we know why.  So that would likely have been code designed to locate the bits of executable code that they needed in order to execute their exploit.  So while ROP can make attacks much more difficult, it's also not a perfect solution.  We still don't have one except let's not have any bugs.  And we certainly haven't gotten there yet.



Two notes.  Robin Ramaekers, he said:  "On SN-955 you had Ethan Stone giving you a quick note that he had problems closing the Edge browser.  While it is true that when you close the window, the Edge processes keep running, there is an easy way to close the browser.  If you click the ellipsis in the upper right you find the option" - it's actually way down at the bottom, the very bottom - "to close Microsoft Edge all the way down.  This is in contrast to clicking the X close box at the upper right, which may only close the user interface."



And someone calling himself Warwagon, who posts often, he's a well-known contributor to GRC's newsgroups, he wrote:  "Here's a fix for Edge running in the background.  Open Edge.  Click the three dots in the top right and click Settings in the dropdown menu.  In the top left do a search for 'startup,'" S-T-A-R-T-U-P, one word.  And for me I had to wait a while.  It takes Edge like a surprising long time to produce any results, but it does.  He says:  "On the right you'll see two options, 'Startup Boost' and 'Continue running background extensions and apps when Microsoft Edge is closed.'  Turn both of those off.  That should remove Edge from the Task Manager when it's closed unless you also have some website notifications enabled."



Okay.  So those are some great suggestions.  Here's what I found:  When I closed my instance of Edge, like just using the standard X close box in the upper right, it did not continue, my Edge did not continue running anything in the background.  Following Warwagon's advice, I found that I had the Startup Boost option turned off already.  And that's what made the difference.



LEO:  Ah.



STEVE:  With Startup Boost turned on, Edge does not close unless you open the ellipsis menu and choose "Close" down at the bottom.



LEO:  So that's what Startup Boost means, just stay, just don't go away.  Never stop.



STEVE:  Yes.  Yes.  Just clicking the UI's X box only closes the UI, and it definitely leaves a bunch of processes.  I initially had 16 Edge processes running.  It whittled itself down to 10 eventually.



LEO:  That's crazy.



STEVE:  But still, it's like, it's just sitting there squatting on RAM and obviously taking up some time from your machine.  So anyone who wants to truly close Edge will need to either turn off Startup Boost or use the ellipsis menu and select Close down at the bottom of that menu, if you've got Startup Boost turned on.  If you have ample memory and would rather have Edge pop onto the screen instantly because basically it's always actually running, then you can turn on Startup Boost, and that's what you'll get.  So thank you guys for the feedback.  I'm glad that we got some closure there.



Thomas Tomchak, he tweeted, he said:  "I'm guessing this is out of scope for how SpinRite should be used, but I tried to boot my Windows VM into SpinRite because I wanted to run it on the internal drive of the VM.  I first tried this with 6.0, and it booted up.  So I then downloaded the pre-release Windows EXE and created a new ISO.  I uploaded that to the vSphere host, attached it as a CD, and told the VM to boot using the BIOS.  This time it went right into the attached screen."



And I have to tell you, when this thing came up in Twitter, what is it, my heart went into my throat or something, or my hope sank.  Anyway, I thought, oh, no.  Anyway, he said:  "I'm sending it in case it's of any help to you, but understand I'm using the software in a way it wasn't intended to be used.  Hopefully it helps in some way."



So anyway, yes.  As I said, when I saw Thomas's screen capture showing that SpinRite had intercepted the processor's attempt to execute an illegal instruction, and I saw at the very top of the screen he was running the latest release 5.06 which is believed to have no such remaining loose ends, my first thought was "Oh no, now what?"  But then I was greatly relieved to read that this was the result of him attempting to run SpinRite 6.1 within a virtual machine.



Okay.  I decided to share this question because there has been a great deal of interest in running SpinRite in virtual machines for various reasons.  So I need to discourage and disabuse everyone of that idea.  SpinRite 6.0 was and is a very tame and well-behaved "generic" DOS application.  By comparison, SpinRite 6.1 really is not.  SpinRite now assumes that it has access to true physical hardware, and it does things like briefly switch the processor from real mode into protected mode in DOS - in DOS - then directly alters the processor's memory management segmentation registers to remove real mode's traditional 64Kb segment limitations.



LEO:  So what you're saying is don't run this through VirusTotal.



STEVE:  Well, and it's surprising.  This version is tripping zero...



LEO:  Oh, that's interesting.



STEVE:  ...of Virus Total's - but what I will say is don't try to run this on other than...



LEO:  VM, yeah.



STEVE:  ...real, yes, on other than real hardware.  You know, after it tweaks the hardware segmentation registers, it switches back into real mode using an oversight on Intel's part.  They had an original bug in the 286.  And because of that, bless their heart, they never changed that behavior, which allows hackers like me - and this was also the way very large games like Doom were able to run under DOS...



LEO:  Oh, yeah.  Oh, yeah.



STEVE:  ...is this allows you basically a flat 32-bit, 4GB address space, even though you're in real mode.



LEO:  Right.



STEVE:  So it creates a hacked but quite reliable pseudo mode known as "flat real mode."  And that allows SpinRite 6.1 to talk to the AHCI driver memory-mapped I/O up at the high end of the 32-bit address space which it would otherwise have no access to, and to be able to use 16MB or even larger buffers.  So it's very safe to say that SpinRite 6.1 and any kind of emulated virtual environment are going to not be seeing eye to eye.



Guillermo Garca, he said:  "Hi, Steve.  Just listened to SN-955 and your description of the certificate discovery tool.  As I consider using it, I'm wondering how to reinstall a certificate that I might erase or delete and later realize that I need?"  Okay.  So there's actually a very cool solution to this.  The Windows Certificates Snap-In that you, Leo, demonstrated last week, has a number, a large number of preexisting folders; and it's possible to simply drag and drop certificates between the folders.



LEO:  Oh.



STEVE:  There already is an Untrusted Certificates folder which on my Win10 machine contains a Certificate Trust List folder and I think it had one thing in it.  But if you drag a certificate from the Trusted Root Certificate Authorities folder onto the Untrusted Certificates folder, the system will spontaneously create a nice new Certificates folder underneath the Untrusted Certificates folder which can contain and document any certificates that you have chosen not to trust.



In fact, you can experiment with using this on any of the expired CA certificates that are currently in the Trusted Root Certificate Authorities folder.  It turns out there's a bunch of them, and they would not be trusted anyway because they're expired.  If you sort by expiration date, you'll see that brings all the unexpired ones to the top.  You'll find that there are a bunch in there that would never be valid anyway.  So you could just, if you want, drag them over into the Untrusted Certificates folder, which makes them untrusted and takes them out of circulation.  So anyway, dragging these certificates back and forth is simple, and I would expect that to be error free.  And should you ever discover that you needed one that you had dragged into the Untrusted Certificates folder...



LEO:  You still have it.



STEVE:  It's still there.



LEO:  That's awesome.



STEVE:  Just drag it back.



LEO:  Wow.  Very simple answer.



STEVE:  So Guillermo, thank you for asking the question.  A.J. Druda, he said:  "Steve, do you list on GRC.com how to lock credit at the credit bureaus?  All I keep finding are paid sites that will do it for me."



LEO:  Oh, don't pay anyone to do that.  Oh, boy.



STEVE:  No.  Crazy.  Believe it or not, this is all so messed up that the terms "lock" and "freeze" have important and different meanings.  This listener used the term "lock," how to lock the credit, but a "freeze" is what everyone wants.



LEO:  Right.



STEVE:  And be careful not to go for a "lock," since some of the services actually charge a fee, they're allowed to, for locking.



LEO:  They used to charge for freezing.  The Fed's fixed it.



STEVE:  Right, they are no longer allowed to charge for freezing.



LEO:  They used to make freezes free, but then it was like $35 to unfreeze in some states.  It was crazy.



STEVE:  Oh, I know.



LEO:  So that is a federal law requires them to freeze and unfreeze unlimitedly for free.



STEVE:  Yup.  So I don't have a page at GRC, but Investopedia has a terrifically clear page which explains all the details and provides very good links to each of the credit reporting bureaus.  I have the full, long "How to freeze and unfreeze your credit" Investopedia link in the show notes, but it's also this week's GRC shortcut of the week, so anybody can easily find it.  Just go grc.sc/956.  And that'll bounce you to the Investopedia page where you'll find links to, I mean, I've checked them all, directly into the freeze and unfreeze pages of each of the credit bureaus.



LEO:  Yeah.  And so does the Federal Trade Commission.  They have a very nice page, if you don't trust Investopedia, which you should.  But there's a government page also that describes all this.  And I use this all the time because once you freeze it, you can't apply for credit.  You know, so you've got to unfreeze it.  And when I just recently bought a new car, I unfroze it for three days.  I said, "Which reporting service do you use?"  They said TransUnion.  They make it fairly easy.  They don't want you to unfreeze or freeze because it costs them money.



STEVE:  And did you see an automatic expiration?



LEO:  Yeah.



STEVE:  An automatic - yes.  That is the cool - I think that's the cool thing.  As I mentioned a couple weeks ago, I applied for an Amazon credit card since I purchase so much through Amazon it just seemed like it made sense.  But that's the first time.  I already had all of my bureaus frozen.  So when I went to do it, they automatically have an automatic refreeze after an expiration time that you're able to set.  So I think that's very cool.



LEO:  Remember, these credit reporting agencies make their money by selling your information to credit cards and others so they can make you offers.  So they don't want you to do this.  But the Fed said no, you have to allow this.  And so they do, somewhat grudgingly in some cases.  The only one that didn't have an automatic unfreeze was Experian.  For some reason they said, well, if you turn it off, it's going to be off, man.  So I have to now go back to Experian and turn it back on.  But everybody else had an automatic unfreeze.  Of course they want that; right?  They don't want you to have it frozen.  Oh, no, but that's right.  They don't want you to unfreeze.



STEVE:  They want you to forget and leave it unfrozen.



LEO:  That's right, yeah.



STEVE:  Andre Couture said:  "Hi, Steve.  Regarding the Picture of the Week for Episode 955."  Remember that was the EU to U.S. power converter made out of paper clips and baling wire and, you know, grandma's stockings.  Anyway, he said:  "Well, I remember having to do something very similar many years ago while traveling to Europe for a presentation I had to give.  I had forgotten the European power adapter.  So I used what I had on hand and in my luggage to establish a connection.  You do what you have to do; right?  LOL."  And I, well, I suppose if there's no other choice, then, yeah, one does what one must.  I can imagine that it would be something well remembered.



And Peter G. Chase tweeted:  "Re Ad Hoc Adapter."  He said:  "I can't possibly be the first one to point out that, while different countries may vary, the EU voltage is usually 240, while of course the U.S. and Canada are both 120.  So whatever appliance was on the other end of that cord very likely got fried almost immediately."



LEO:  Actually, not so.  Not so.  Because - and the reason I know this is your laptop, many appliances now in the U.S. are rated for 110 to 240.



STEVE:  Are able to handle either voltage.



LEO:  They can handle it, yeah, yeah.



STEVE:  Right.



LEO:  So that's why in many cases you can just use an adapter.  I would still not recommend that method, but you can use an adapter without a transformer.  You don't need to go through that.



STEVE:  Again, I would say, "Everyone, don't try this at home."



LEO:  No.



STEVE:  Or in this case don't fry this at home.



LEO:  Please.  I beg of you.



STEVE:  Okay.  So finally, "The Inside Tracks."  I'm feeling very good about where SpinRite is today.  No new significant problems have arisen for several weeks despite significant and continual testing.  And those people whose drives SpinRite was previously having trouble with have all reported back in that SpinRite's latest prerelease managed to plow through those drives' known sticky spots while effectively recovering and repairing everything that it encountered.  Several have publicly stated that they've been amazed and impressed.  So it very much feels as though SpinRite is back, and that I will be letting it go shortly.



To share some sense for where my recent focus has been, I spent the past few days exploring whether I could improve SpinRite's remaining time-to-work prediction.  Back when SpinRite was born, in the late '80s, drives were sectored like pie slices with radials stretching out from the center, which described the region of each "sector" around the circumference.  In fact, we've grown so used to using the term "sector" that it has completely lost its original meaning.  The term was born when these were literally angular sectors of a disc.



The problem with this simple sectoring was that the tracks at the outside of the disc were physically longer than the tracks nearer to the center; yet back then, all tracks contained the same amount of data.  If all tracks contain the same amount of data, and the outer tracks have a longer circumference, and the inner tracks have a shorter circumference, that meant that the individual bits were being written with reduced density around the outer tracks and increased density around the inner tracks.



Disk drive read-and-write electronics were originally separated from the drive in an outboard controller, and drives had no intelligence at all.  They were basically just some read/write electronics and a stepping motor.  But IDE drives, where IDE stands for Integrated Drive Electronics, changed that by placing a drive's read-and-write electronics onto each drive.  Once that was done, the drive was able to become something of a black box.  It could simply declare how many sectors-worth of storage it contained, and everything about how it worked in detail could be kept internal.



Drive designers very quickly saw that this meant they could dispense with the whole original notion of sectoring as it once was done.  If the outer tracks had a larger circumference, they could take advantage of that to store more data around those longer tracks.  And this also allowed them to push tracks further inward toward the center of the drive by reducing the storage bit rate so as not to be cramming too many bits into too small a circumference.  This in turn allowed them to squeeze every last bit of storage into each drive and to make more complete use out of each physical disk's surface.



There is, however, one cost to that which is often overlooked, which is that the data transfer rate drops as we move inward toward the inner tracks.  If we think of the beginning of the disk's storage as the outer tracks, then the end of the drive is the inner tracks where things are slower.  The reason this matters to us today is that - or especially to me, but also to SpinRite's users - is that SpinRite's original remaining time estimation system assumed a uniform data rate across the entire drive.  In other words, it performs a linear estimation.  It continually monitors the total elapsed time required to get however far along it has and projects its completion time assuming that the rest of the drive will be the same as the average of everything it has seen so far.



Now, that was accurate, and it worked well for SpinRite versions 1 and 2 and 3, but it has become less and less true as the end of drives have become slower and slower as advancing technology has allowed them to push more data closer in to the disk's center where tracks are the shortest.  The result is that for today's spinning drives, SpinRite's estimation will always underestimate the total time it will require.  So as I said, I've spent the last few days looking closely into this to see what I might be able to improve.  I've learned some interesting things that I thought I'd share while they're still fresh in my mind.  What I found, after examining a handful of different multi-terabyte spinning drives, is that the ends of those drives have half the performance of their beginning tracks.



LEO:  Wow.  That is a big drop-off.



STEVE:  Half, right.



LEO:  We always, I mean, this goes back to the old days when I'd always put my swap drive at the beginning of the hard drive, right, because it was faster.



STEVE:  Exactly.



LEO:  Than the internal drives.



STEVE:  We did know that that was the case.  Now, okay, at first blush that sounds awful; right?  But the decrease in performance is not linear.  What we're really looking at is area rather than circumference.  And as we know, area changes with the square of a circle's radius.  What this means is that while a drive's data transfer performance does steadily decrease as we move inward toward the drive's end, the decrease is very gradual until we get much closer to the end of the drive, where it finally begins to drop significantly.



Okay.  So let's put some numbers to this.  In general, SpinRite's current linear estimator takes about a minute to stabilize, which is to say it needs 60 seconds of operation to have established a sufficient baseline of work in time and distance to settle into a prediction that no longer varies.  And what I found through lots of experimentation on many different contemporary spinning drives is that it's necessary to add an additional 30% to SpinRite's initial front-of-drive-only linear estimation.  So, for example, to make the math easy, say that SpinRite predicts a 10-hour run for a drive.  The actual running time, due to the very end of the drive being much slower, will be 13 hours, so 10 plus 30% of 10.  Did I say 30?  Thirteen, 13 hours.  So you're adding three hours to SpinRite's initial prediction of 10.



Okay, now, here's another interesting factoid that falls out from the math and which I've verified multiple times experimentally.  The first 60% of the drive requires exactly half of the total running time.  So the last 40% of the drive requires the second half of the total running time.  Or expressed another way, whatever length of time is required for SpinRite to get 60% of the way through the entire drive is the amount of time that will be required for it to finish.



Now, I'm not certain yet exactly what I'm going to do with this information, but I needed to gather it to know what I was dealing with.  This obviously does not apply to solid state storage since it's not spinning, or even to shingled magnetic storage, you know, SMR format drives, since both of those technologies track when memory has been written to, I should say when and if memory has been written to them, and so they don't read anything from their media when SpinRite checks to see what's there if nothing has ever been written, as is often the case at the end of those drives.  So, and we've seen this.  We've seen that later portions of those drives, both SSDs and SMR spinners, appear to be performing much faster at the ends than at the front where they have stored data.  They don't slow down as we go along; they speed up.



And since drives are supposed to be "black boxes" which we just trust with our data now, there's no requirement for any drive to declare what technology it's using.  And many, if not most, do not give SpinRite any indication of what lies beneath their interface.  The 30% rule could just be a common rule of thumb for SpinRite's users.  At least initially, they know better than SpinRite knows whether they're testing a spinner or a solid state drive. So the rule of thumb for spinning media would be to start SpinRite, give its predictor a minute to settle down, see how long it expects to be running, then add 30% to that.  And that's a pretty good indication of where you will be with a spinning drive.



The other thing I'm considering, and I think I'm probably going to do it, is changing SpinRite's label on the screen.  It's currently right-justified, and it's got a bunch of spaces in front of the word "time."  I'm thinking I'm going to change it to est. time, as in estimated time.  Then, as soon as SpinRite gets to the 60% point, it will have acquired sufficient awareness of the drive, maybe having seen a gradual decrease in performance over that span of time, to get to 60%, to be able to reliably determine that the drive is spinning, and that as much time remains as has been spent so far.  So at that point, it would adjust its timer and change the "est." to, I don't know, "real time" or "true time" or "good time," you know?



LEO:  Good time's good, but confusing.



STEVE:  So that at that, yeah, at that point, as soon as SpinRite said "true time," then you would know that it was at the halfway point, and SpinRite would be able to project what its probable completion time would be much more accurately.



LEO:  Yeah, makes sense.



STEVE:  So in any event, after I'm finished with this podcast ere today, which I will be in about one sentence, I plan to make those final changes, after which I believe I'll finally be content to declare SpinRite 6.1 finished and ready for the world.



LEO:  Now, will you wait for another episode of Security Now! to announce that?  Or will you just do it?



STEVE:  The nature of this is sort of a soft event.  For example, I'll take - right now there's all this prerelease jargon all over the UI.  So I'll take that out so it no longer says "prerelease."  I'll declare an actual release candidate.  I need to just let a day or two go by, or five, to like make sure, like see if anything happens.  You know, who knows maybe SpinRite...



LEO:  No showstoppers.



STEVE:  Yeah.  Like SpinRite works better if the word "pre" is in front of release.



LEO:  You never know.  That would be a weird regression, but you never know.



STEVE:  It's computers, yeah.



LEO:  I know.  Turns out that was an important part of the lookup table.  Oh, no.  Well...



STEVE:  And we actually have an example from our own experience.  Nobody knows to this day, I don't know, why SpinRite 5.0 is better at recovering data from diskettes than SpinRite 6.  It is, like, spooky.  I've stared at the 6.0 code.  I didn't change anything.  Actually, I kind of remember that I did, but I don't remember what it was.  And but then I've gone back and looked, and I've got this 5.0 source, and I've got the 6.0 source.  They look the same.  But spooky, you know...



LEO:  Something happened.



STEVE:  There's like for some reason - now, the good news is floppies are gone.  But we actually do routinely recommend to 6.0 users who are having problems, like really need to recover data from a diskette, use 5.0.  And all 6.0 and 6.1 owners have access to 5.0 if they really do need to recover something from a diskette.



LEO:  Isn't that funny.



STEVE:  5.0 is better, and no one knows why.



LEO:  Computers.  I tell you.



STEVE:  There's a little spookiness going on.



LEO:  It's a great mystery.  Steve Gibson, all the spookiness happens at GRC.com.  While you're there pick up your copy of SpinRite 6.  You'll get 6.1 pretty soon, I would guess.  Free, free upgrade for all 6.0 buyers today at GRC.com, world's best hard drive maintenance and recovery utility, floppy disks and SSDs.  So the works.



Copyright (c) 2024 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.






GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#957

DATE:		January 16, 2024

TITLE:		The Protected Audience API

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-957.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  What would an IoT device that had been taken over do?  And what would happen to the target of attacks it might participate in?  What serious problem was recently discovered in a new post-quantum algorithm, and what does this mean?  What does a global map of web browser usage reveal?  And after entertaining some thoughts and feedback from our listeners and describing the final touch I'm putting on SpinRite, we're going to rock everyone's world (and I'm not kidding) by explaining what Google has been up to for the past three years, why it is going to truly change everything we know about the way advertisements are served to web browser users, and what it all means for the future.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  My goodness.  What would you do if you found out your washing machine was uploading 3.6GB of data every single day?  Why would that be?  Well, Steve's got a good solution.  We'll find out which browser is now totally dominant in the world.  And then we'll find out what Google's doing to protect your privacy and still give advertisers the information they need to target you.  Is that possible?  Stay tuned.  Security Now! is next.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 957, recorded Tuesday, January 16th, 2024:  The Protected Audience API.



It's time for Security Now!.  Normally Steve would be over my left shoulder here, but he's actually over to my right because - or you meant left.  He's over there.  He's over there.  I'm here in Rhode Island in my mom's house, and visiting Mom, and Steve's at his house.  And we are going to do the show from here.  But the good news is the quality continues on.  Steve Gibson, hello.



STEVE GIBSON:  Yo, Leo, great to be with you, wherever you are.



LEO:  Yeah.  It snowed here.



STEVE:  In the snow, where it's already getting dark because you're in the northern latitudes.



LEO:  Yeah.  You don't want to - yeah.  This is - it snowed, and then it rained, and it turned into slush, and now it's going to freeze.  And it's just, oh, boy, yeah, arctic chill.  You're in beautiful Southern California where it's always perfect.



STEVE:  I am.  And there's no indication that I'll be leaving anytime soon.  So I won't have to change any of my cabling.  So, okay.  Today's topic, today's title, is one of the driest-sounding titles in a while.  This is Security Now! podcast 957 for January 16th, 2024, titled "The Protected Audience API." 



LEO:  Well, that sounds fascinating.



STEVE:  Begs many questions.  What is the audience being protected from?



LEO:  Right.



STEVE:  And what do they need an API for?



LEO:  Right.



STEVE:  Okay.  So we're going to explain all that.  But first we're going to examine what an IoT device that had been taken over would look like and do.  What would happen to the target of the attacks that it might participate in?  What serious problem was recently discovered in a new post-quantum algorithm - oops - and what does this mean?  What does a global map of web browser usage reveal?  And after some entertaining thoughts and feedback from our listeners, and describing the final touch, I think it's going to be final, that I'm putting on SpinRite, we're actually going to rock everyone's world.



LEO:  Oh, boy.



STEVE:  And I'm not kidding, by examining and mostly understanding what Google has been up to for the past three years, why it is going to truly change everything we know about the way advertisements are served to web browser users, and what it all means for the future.  And the way we kind of got to this podcast today is odd because I thought I had an idea for what I was going to talk about this week, and I mentioned that I'd had an idea last week.  Then when I got into it yesterday I thought, oh, no, this doesn't really - this is not going to work.  So the guy was into law.  And so but that dragged me into what he was looking at, which was completely, like, what is Google talking about?  So then I thought, okay, I can't even talk about that.  After I'd invested rather significantly in getting ready to talk about that, I thought, okay, no.



So then I was upset, and I moved it from being what we would talk about into just an item.  But then when I tried to sort of massage it away from being our main topic into just a news item, then I thought, oh, I think I kind of understand this.  So then I moved it back into our main topic and expanded it further.  And it took up pretty much all the air of the podcast.  So I'm already tired.



LEO:  Just the explanation is exhausting.



STEVE:  But believe me, this one, as I was writing this, I was thinking, okay.  As soon as this thing gets produced and is posted, I need to point Jeff Jarvis at it because this is going to wind him up.  I mean, in a good way.  He's going to - because, you know, because Jeff likes to understand things, and he keeps telling us how nontechnical he is.  Well, everybody's going to understand this, and this is really important.



LEO:  Is this the sequel to what Google's doing with killing third-party cookies and, what was it, FLoC and Topics and all the different things they were trying to do to make ads viable without invading privacy?



STEVE:  Yes.  Yes.  Yes.



LEO:  Okay.  So it sounds like you think pretty highly of it.



STEVE:  It's going to happen.  And what is I think the most surprising thing is that the good news is, you know, Tim Berners-Lee is not in a grave from which he could roll over or turn over.



LEO:  No.  In fact, he's very actively running the World Wide Web Consortium, yeah.



STEVE:  He's looking great.  But what Google has done to their browser - and they did this last summer, this has been active since July of last year - what they have done is astonishingly huge.  And by the end of this podcast our listeners are going to understand how the world has changed.



LEO:  Interesting.



STEVE:  And we just haven't woken up to it yet.



LEO:  Yeah.  Well, this obviously sounds like something everybody should listen to.  This is the argument, by the way, and we have this argument a lot on Windows Weekly, about why there shouldn't be just a monoculture with browsing because it gives Google outweighed importance in all of this.



STEVE:  Well, the good news is that - and of course everything they're doing is open source.  So Firefox will end up incorporating this into it.  So what Google has essentially - our browsers used to be HTML renderers.



LEO:  Right.



STEVE:  What this turns our browser into is an ad-auctioning server.



LEO:  Oh, no.



STEVE:  It is - I know, Leo.  It is huge.  It is...



LEO:  Oh, boy.



STEVE:  But it's the only way for Google to deliver what they want and what we demand.



LEO:  Okay, good.  I mean, this is interesting.



STEVE:  Yeah.  But this is a seminal podcast.



LEO:  All right.  We're going to have to listen and stay tuned.



STEVE:  And I'm a little worried about that word "seminal," but, you know, you know how I mean it.



LEO:  Just keep the camel's nose out of your kimono, and you'll be good.  All right, Steve.  I'm ready for the Picture of the Week.



STEVE:  So, okay.  So I've had this one in my bag of tricks for a month or two, just sort of waiting for the right time.  And I just love this.  So for those who can't, who aren't seeing the picture in live feed, I don't know - we can only see, like, where this object is that is the focus of the picture, not the setting, the larger setting which it's meant to be describing.  But we have this large square, it's probably metal, embossed sign where in big huge all-caps, in relief, it says "Please Do Not Touch."  So it's referring to something in its environment that we are being told, whoa, do not touch.  The punchline here, however, is that that admonition is repeated in Braille below the sign.



LEO:  That's something you don't expect to see in Braille is please do not touch, because you're touching it.



STEVE:  Yeah.  So, and I'm wondering what happens if a non-sighted person reaches out and scans this with their fingers.  Do they then jump back?



LEO:  They leap back.  Oh, I'm touching it.



STEVE:  Because oh my god, you know, I'm not supposed to touch this.  Anyway, I gave this picture the caption "Please provide a clear visual example of 'Irony.'" 



LEO:  Love it.  Very nice.



STEVE:  And Please Do Not Touch in Braille, yeah.  Okay.  So What would an IoT device look like that had been taken over?  That's something we've never talked about.  You know, we talked a lot about the threat that's posed by the remote takeover of IoT devices.  We know without any question that there are a great many very large bot fleets, and that they are composed of individual unattended Internet-connected devices.



Well, one of our listeners, Joe Lyon, sent me an image of a Twitter posting where the poster is rhetorically asking why his LG washing machine is using 3.6GB of data per day?



LEO:  Wow.



STEVE:  Yeah, 3.6GB.  And he attached an image to his Twitter posting that was produced by some network monitoring tool, showing that something on his network whose interface is labeled "LG Smart Laundry Open" is, indeed, quite busy on the network.



LEO:  A little too smart.



STEVE:  Yeah, exactly, a little too smart for its own good.



LEO:  Just wash the damn clothes.  You don't need to surf on the 'Net while you're doing it.



STEVE:  And, you know, whatever is going on is happening very uniformly for a full 24 hours, because this chart that we've got on the show notes shows 24 hours of use with only one hour of the 24 showing a reduced total bandwidth during that hour.  So, yeah, there's certainly something sufficient there to raise suspicion.  Now, what also caught my eye was that the labels on the traffic flow show a download of 95.75MB like for the day, and a whopping upload of 3.57GB.



LEO:  That's not good.



STEVE:  Now, anyone who's worked with networking gear knows that it's very important to know which directions "up" and "down" are referring to.  Cisco has always used - I was very pleased with them about this - the unambiguous terms "in" and "out," as in, traffic flowing into or out of a network interface.  So if the interface is facing toward the Internet, then traffic flowing out of it would be up toward the Internet, and traffic flowing into it would be down from the Internet.  But if the interface is facing inward toward, for example, connected to a local area network, then the meaning of "in" and "out" would be reversed.



Okay.  So without a bit more information about the network's configuration shown in this picture, we can't be 100% certain.  But either the washing machine's networking system is badly broken, causing it to continuously download at a rate of 3.5GB of something per day or, as does seem more likely given the evidence, the label "Upload," even though we cannot be certain what that means, suggests that this washing machine has probably become a bot in someone's army.  So it's busy doing its part uploading 3.6GB of junk on a continuous basis, presumably nonsense traffic, just causing some remote person grief.



LEO:  That makes - now, see, I saw this story, and I thought, well, what could it - is it keeping track of what clothes you're washing?  No, it's been compromised.



STEVE:  Yes, yes.



LEO:  Yeah, that makes a lot more sense.



STEVE:  That would be the conclusion.  This is what an IoT device looks like when it's been compromised.  So this brings me to two final observations.  First, since the typical consumer is not monitoring their local network's traffic in any way, they would have no way of knowing that this was going on at all, ever.  And given the closed turnkey nature of an LG washing machine, it's unclear how one would go about reflashing its firmware to remove the bot, even if you knew that one was in there.  You know, it might be just living in RAM, in which case pulling the plug, counting to 10, and powering it back up might be all that's needed to flush it out of the system.  But then the device might become reinhabited again before long, as we know happens.  So the only real solution would be to take the washing machine off the 'Net.  Which brings me to my second point.  What the heck is a washing machine...



LEO:  Doing on the Internet?



STEVE:  ...doing being connected to the Internet in the first place?



LEO:  Exactly.



STEVE:  You know, is this another of those "just because we can doesn't mean we should" situations?  You know, I've owned washing machines my entire adult life.  After Mom stopped washing my underwear for me, I took that over.  The only thing any of them have ever been connected to is AC power.  So is it really necessary for us to initiate a rinse cycle while we're out roaming around somewhere, or to be notified with a message delivered through an app when our clothes are dry?



LEO:  But that is the purpose of that.  I know I've seen them sell it that way, like you can control your washing machine from anywhere.



STEVE:  Oh, that's great.  So, you know, I get it that if all of that amazing functionality is free and included, and you know these days nothing costs anything anymore, then why not set it up and get it on the Internet?  But we're talking about this because of maybe why not to do that.  Maybe something has crawled into that machine, and not just because you needed to wash your clothes more often, and set up housekeeping there.  Maybe the only thing it's currently doing is flooding hapless remote victims with unwanted Internet traffic.  And maybe also, if it wanted to, it could pivot and start poking around inside your residential network.



LEO:  Oh, yeah.



STEVE:  And just maybe that could end up being a high price to pay for the luxury of being notified by an app when the lint filter needs to be changed.  So if these sorts of things are going on, like if these sorts of things, these appliances are going to be connected to your network, again, give some thought to sequestering them on a separate guest LAN which has no access to your high-value LAN.  Most of today's consumer routers now offer this feature.  That makes it easier to implement than it was back when we first started talking about the idea of LAN separation many years back.  You know, remember my "three dumb routers" concept for how to create isolated LANs when that feature was not already built into our routers.  Well, the good news is...



LEO:  Or better yet, just don't connect it at all; right?



STEVE:  Exactly.  Ask yourself, do I really need - and here's the problem.  This was thrown into a washing machine by people who are more concerned about whether it actually gets your clothes clean than it being on the Internet.  So the Internet is a throwaway for them.  They're not going to be that concerned about the security of their own washing machine that they're shipping.  This is not Cisco who is selling you a washing machine; you know?  This is LG.  



LEO:  And probably they have a module they put in all their appliances; right?  This is just, you know, the LG Internet of Things module, and we'll figure out how to sell it.



STEVE:  Right.  And it's using code from the dawn of the Internet.



LEO:  Yeah.



STEVE:  Because, you know, it worked, and they don't care.  So, you know...



LEO:  Unbelievable.



STEVE:  The end-user needs to care.



LEO:  Wow.



STEVE:  Okay.  And speaking of DDoS attacks, this related bit of news was also pointed to by a listener, Sukima, who's at twit.social.  He wrote:  "I use this service for all of my personal projects, and liked it so much I was motivated to support them financially.  And yet they are having a massive DDoS attack and thought it worth talking about publicly, especially as examples of tech doing everything right while still being vulnerable."  And in his tweet to me he sent the URL outage.sr.ht.  So I went over and took a look.  And I wanted to share what I found because it's just such a perfect example.  And then we'll talk a little bit more about mitigation strategies.



One of the three guys who runs the service, actually its founder, over at SourceHut, which is the name of the service, he wrote:  "My name is Drew.  I'm the founder of SourceHut and one of three SourceHut staff members working on the outage, alongside my colleagues Simon and Conrad.  As you've noticed, SourceHut is down.  I offer my deepest apologies for this situation.  We've made a name for ourselves for reliability, and this is the most severe and prolonged outage we've ever faced.  We spend a lot of time planning to make sure this does not happen, and we failed.  We have all hands on deck working the problem to restore service as soon as possible.



"In our emergency planning models, we have procedures in place for many kinds of eventualities.  What has happened this week is essentially our worst-case scenario:  What if the primary datacenter just disappeared tomorrow?  We ask this question of ourselves seriously, and make serious plans for what we'd do if this were to pass.  And we are executing those plans now, though we had hoped that we would never need to.  I humbly ask for your patience and support as we deal with a very difficult situation.  And again, I offer my deepest apologies that this situation has come to pass.



"So what happened?  At 06:30 UTC on January 10th, two days prior to the time of writing, a distributed denial of service attack (DDoS) began targeting SourceHut.  We still do not know many details.  We don't know who they are or why they're targeting us.  But we do know that they are targeting SourceHut specifically.  We deal with ordinary DDoS attacks" - okay.  So just that.  His phrase "We deal with ordinary DDoS attacks in the normal course of operations."  It's like, okay, it's a sad state of affairs that you refer to "ordinary DDoS attacks."



And he says:  "And we are generally able to mitigate them on our end.  However, this is not an ordinary DDoS attack.  The attacker possesses considerable resources and is operating at a scale beyond which we have the means to mitigate ourselves.  In response, before we could do much ourselves to understand or mitigate the problem, our upstream network provider null routed SourceHut entirely, rendering both the Internet at large and SourceHut staff unable to reach our own servers.



"The primary datacenter, PHL, was affected by this problem.  We rent colocation space from our PHL supplier, where we have our own servers installed.  We purchase networking through our provider, who allocates us a block out of their AS" - you know, we've talked about AS numbers, right, autonomous system numbers - "and who upstreams with Cogent, which is the upstream that ultimately blackholed us.  Unfortunately, our colocation provider went through two acquisitions in the past year, and we failed to notice that our account had been forgotten as they migrated between ticketing systems through one of these acquisitions.  Thus we were unable to page them.  We were initially forced to wait until their normal office hours began to contact them, seven hours after the start of the incident.



"When we did finally get them on the phone, our access to support ticketing was restored, they apologized profusely for the mistake, and we were able to work with them on restoring service and addressing the problems we were facing.  This led to SourceHut's availability being partially restored on the evening of January 10th, until the DDoS escalated in the early hours of January 11th, after which point our provider was forced to null route us again.



"We have seen some collateral damage, as well.  You may have noticed that Hacker News was down on January 10th.  We believe that was ultimately due to Cogent's heavy-handed approach to mitigating the DDoS targeting SourceHut."  He said "Sorry," and then he said, "Hacker News, glad you got it sorted."  Then he said:  "Last night a nonprofit free software forge known as Codeberg also became subject to a DDoS, which is still ongoing and may have been caused by the same actors.  This caused our status page to go offline.  Codeberg has been kind enough to host it for us so that it's reachable during the outage.  We're not sure if Codeberg was targeted because they hosted our status page, or if this is part of a broader attack on free software forge platforms."



Okay.  So we were just talking about, of course, the LG smart washing machine and the idea that it was apparently sending a continuous stream of traffic, totaling about 3.5GB per day, out onto the Internet for some purpose.  So I wanted to put a face on this to make it a bit more real for everyone.  What I've just shared is a perfect example of where such traffic goes, like that this washing machine was apparently emitting onto the Internet, and its very real consequences for people.  You know?  People are having their lives seriously affected by these sorts of attacks.



Now, Drew used the term "null routing," which is the action taken by major carriers, such as Cogent in this case, when some client, or client's client, or client's client's client, because Cogent is a Tier 1 Provider, is undergoing a sustained attack.  They essentially pull the plug.  You know, they have no interest in carrying traffic that is indirectly and inadvertently attacking their network.  When an attack originates, as most do now, from a globally dispersed and distributed collection of anonymous and autonomous bots, that traffic, which is all aimed at a single common IP address somewhere, will enter the network of a major carrier like Cogent all across the globe, as well.  So that means that the attack is crossing into Cogent's routers from all of its many various peering partners who are the ones whose networks have been infected with some bots, or perhaps the traffic is just transiting across their network and originates from some other major carrier's network.



Whatever the case, the real danger of these attacks is its concentration.  As the traffic hops from one router to the next, with each hop bringing it closer to its destination, that traffic is being aggregated.  It is growing in strength, and it can get to the point of debilitating the routers it's attempting to pass through.  This means that the optimal action for any major carrier like Cogent to take is to prevent this traffic aggregation by blocking the attacking traffic immediately at all and each of the many points of ingress and entry into their network from their peering partners.



So Cogent sends routing table updates out to every one of the peering routers on their border, instructing that router to "null route" - meaning immediately discard - any packets attempting to enter their network which are bound for the IP that's under attack.  This neuters the attack without causing any harm to their network because it's unable to concentrate.  And since there will almost certainly be malicious bots running inside the networks of some of Cogent's client ISPs, this null routing must also be applied internally, as well as on their border.



Okay.  But notice that now, with the targeted IP null routed, it's also impossible for any benign traffic to reach its destination service.  As Drew wrote, they were unable to even reach their own servers, you know, even if they had some back way into them, because of this null route.  No traffic was getting to their servers, good or bad.  A major carrier's null routing inherently not only blocks the attacking traffic, but any and all traffic to that service, no matter what.  In fact, once the attack has subsided, and full service could be restored, that site will remain dark until someone at the service provider notices that the attack has mitigated and then lifts the network-wide block to allow regular services to resume.



DDoS attacks like this one have become a fact of life on the Internet.  Anyone who's working for any major service provider sees them and deals with them now as part of their daily routine.  But as we've just seen, this doesn't make such attacks any less significant and potentially devastating to anyone who is committed to keeping whatever services they offer available.  And we also know where these attacks originate.  They originate from devices exactly like that LG smart washing machine, a gadget that largely operates autonomously where networking is not its primary focus.  It was tacked on, as we said earlier, as a feature; so it never got the attention that it needed to be a truly secure networking device.



And we also know that the phrase, unfortunately, "truly secure networking device" almost needs to be said with tongue in cheek because, sadly, it's become an oxymoron.  You know, truly secure networking device.  Well, it's almost become the holy grail.  Everything we've learned is that it is truly difficult to create and maintain a truly secure networking device.  And the more features are added, the more quickly the challenge grows.



LEO:  All right.  This is really a good episode.  You get so much juice in this.  But wait a minute.  Quantum crypto problems?  That's - wait a minute, now.



STEVE:  Yeah.  Okay.  So BleepingComputer recently reported the news that many implementations of the already-in-widespread-use post-quantum key encapsulation mechanism known as "Kyber" - which as I said is in use, for example, by the Mullvad VPN and to provide Signal's post-quantum redundancy, we talked about that before - they jumped right on it, and we said, yay, great.  But whoops.  So it's been found to be subject to timing attacks.  The set of attacks have been dubbed "KyberSlash."



Okay.  Now, the first thing to understand here is that nothing has been found wanting from the post-quantum Kyber algorithm itself.  As far as anyone knows, Kyber still provides very good quantum resistance.  The problem and vulnerability is limited to some of the actual code that was used to implement the Kyber algorithm.  And this is part of the typical shaking out process that new algorithms undergo.  First we need to get the theory right.  Then it's tested to prove that it does what we thought.  Next, the code is implemented into libraries where it can actually be used and tested in the real world.  And it was at this point in the process that these troubles arose.



The problem is that the vulnerable algorithms perform an integer division instruction where the numbers being divided are dependent upon the algorithm's secret key.  Whoops.  Since division is an instruction whose timing can vary widely based on the binary bit pattern of the specific numbers being divided, this naturally results in a situation where the secret that needs to be kept has the potential to leak out through a timing side-channel.  And that's exactly what happened here.  



Now, it's such an obvious and well understood problem that it's kind of shocking that this could happen.  And really, whoever wrote that code should be scolded a bit.  You know, perhaps they were just having a bad day.  Or perhaps they're solely focused on theory and not enough on practice.  Who knows?  But in any event, the problem was found, and it's well understood.  And many of the libraries that implemented the vulnerable reference code have now been updated, and more are being updated.  So it's a good thing that we're doing this now rather than two years from now, or 10 years from now, or whenever it might be that we actually become dependent upon the strength of these post-quantum algorithms to protect against quantum-based attacks.



We're okay, you know - and to their credit, Signal, remember, added this to their existing crypto rather than switching over to this, recognizing that its unproven nature meant that it really couldn't be trusted fully yet.  So Signal was never in danger, and now they're less so.



Okay.  And Leo, we've got a cool picture here.  And this is apropos of the podcast's main topic.  StatCounter produced this somewhat bracing screenshot of global web browser use 12 years ago, back in 2012, and two years ago in 2022.  So since today's podcast is all about Google and their Chrome browser, you know, yikes.



LEO:  Oh, this is not good.



STEVE:  So what I want to know is what's going on in Iceland.  I think that's Iceland, you know.



LEO:  It's gray.  What is gray?



STEVE:  Yeah, well, that's Safari.



LEO:  Oh.  Wow.



STEVE:  But, so okay.  So for our listeners who can't see because they're listening, thus listener, anyway, the first picture from 2012 shows, you know, what the world looked like, what, 12 years ago.  All of the U.S. and Canada and Alaska and Iceland and sort of the northwest of Africa, looks like all of Australia, anyway, all of that is like a blue, and that was IE.



LEO:  Yes.



STEVE:  Everybody 14 years ago was using Internet Explorer.  Interestingly, Mozilla's Firefox was scattered around.



LEO:  Yeah, look at France.



STEVE:  In Europe, exactly, Europe and Italy and...



LEO:  Asia.



STEVE:  Yeah.



LEO:  Yeah.



STEVE:  So there was a lot of Firefox use.  And of course Chrome was there.  It looks like Africa, the whole continent, most of it, except for a little bit of IE, was Chrome.  Russia was all Chrome.  And there was also some scattered bits of Opera.  Anyway, so that was then.  Whoa.  Take a look...



LEO:  That should be the title of the show, Scattered Bits of Opera.



STEVE:  Little Bits of Opera, yeah.  Okay.  Now...



LEO:  Holy cow.  This is depressing.



STEVE:  On the key of this second updated chart from two years ago, there is blue for Microsoft Edge.  I don't know where it is on the map.  Literally on the map.



LEO:  I don't see any, yeah.



STEVE:  I don't see any blue.  All I see is Chrome.



LEO:  Christ.



STEVE:  Chrome has taken over.



LEO:  Is that Iceland or Greenland?  What is...



STEVE:  Oh, good question.  Anyway...



LEO:  Oh, there's the blue.  We found the blue.  It's Chad.  I don't know where that is.



STEVE:  This could also be a map of COVID, unfortunately.



LEO:  Unfortunately.  It's all green.



STEVE:  You know?  So Chrome is the COVID of browsers.  It's just - it's everywhere.  Okay.  So with that in mind, we will be talking about what Google has done to browsers, which is to say all browsers because they're pretty much one and the same.



I'm going to - I have a couple little bits of feedback from our listeners I want to talk about, and then we're going to plunge in.  So of course there were predictably many replies from our listeners about my follow-up discussion last week of the Apple backdoor.  I just grabbed one to finish out this subject.



David Wise wrote:  "Hey, Steve.  Listening to your podcast about the Apple vulnerability.  Could this be a supply chain hack with the phones being built in China?"  My answer?  Sure.  Absolutely.  Unfortunately, literally anything is possible.  I think it's safe to say that by the nature of this we'll never have a satisfactory answer to the many questions surrounding exactly how or why this all happened.  All we know for sure is that backdoor hardware exists in the hardware that Apple sold and shipped.



And notice by David's question that plausible deniability exists here.  All of the several possible sources of this can claim absolute surprise and total ignorance of how this came to pass.  Is it significant that it first appeared in 2018, three years after the famous high-visibility 2015 San Bernardino terrorist attack, several years being the approximate lead time for a new silicon design to move all the way through verification, fabrication, and into physical devices?  Again, we'll never know.  And, yeah, that's annoying.



Osyncsort tweeted:  "Hi, Steve.  I'm a fan of the podcast and all the great work you've done in your career.  Especially a fan of SQRL.  Thank you for this great work, and I wish it to be mainstream in the near future."  Well, so do I, but don't hold your breath.  "I recently listened to Episode 885."  885, okay, a while ago.  "And you briefly touch on a subject that I've been contemplating, getting into infosec.  I'm currently thinking about getting into infosec as a career.  I'm in my 40s and wanted to know, from your perspective, if 40s is too old to get into the field.



"My career is online marketing, and I've been fortunate to have been doing it from the early days of Web 1.0 to what is now referred to as Web 3.0.  However, after COVID, I have not had the same opportunities in the marketing world.  So I find myself looking for a new career and thinking infosec may be the solution.  Any advice/opinion is welcomed.  Thanks in advance."



Okay.  So the good news is that there is a huge and unmet and even growing demand for information security professionals today.  I think that the trouble with any sort of "am I too old" question is that so much of the answer to that depends upon the individual.  A particular person in their early 30s might already be too old, whereas someone else who's 55 might not be.  But speaking only very generally, since that's all I can do here, I think I'd say that someone in their 40s probably spans the range where the question of "too old" might start to be an issue if it's a concern for them.  Early 40s, not so much.  Late 40s, well, maybe a bit more so.



But regardless, there's definitely something - a lot, actually - to be said for having some gravitas and life experience that only comes with age.  An IT guy who's more world-wise will generally be much more useful than a fresh newbie who is still addressing the world with impractical expectations.  And especially for an IT guy, knowing how to talk to others is a skill that should not be undervalued.  So I think that on balance I'd say go for it, and know that the demand for that skill set will only be increasing over time.



LEO:  This is where I would put in the plug for ACI Learning, our sponsor, or ITProTV.  And, you know, you could certainly learn, I don't think it's ever too late to learn the skills.



STEVE:  Right.



LEO:  So really the only question you're asking is can I get hired.  And, you know, that's in the pan if you've got the skill set, if you've got the certs.  I think it's...



STEVE:  And Leo, if this guy has been listening to this podcast for, like...



LEO:  Yeah, that's a good way to start.



STEVE:  From the beginning.



LEO:  Yeah.



STEVE:  We keep hearing from people, it's like, yeah, I just went in, I didn't even study, and I passed the test.



LEO:  Good training.  Yeah, yeah.



STEVE:  It's like, okay, that's great.



LEO:  Yeah, that's a good point.



STEVE:  So someone whose handle is 3n0m41y, which gave me some pause until I realized it was supposed to be anomaly, he said:  "Hi, Steve.  I would like to get your opinion on Proton Drive versus Sync as a secure cloud storage network.  Recently the iOS Sync app has broken the ability to natively use the built-in iOS Files app to navigate Sync's folder structure properly.  What happens is that after drilling down one to two directories, the Sync app pushes the structure back to the root folder.  While this is not a show stopper, it does break the use of other third-party apps on iOS.  I've reached out to the Sync dev team, but they've responded that it will take 'quite a while' to fix.  This functionality broke about two months ago.  So I just want to get your take if Proton has matured enough to be a replacement for Sync.  Cheers, and Happy New Year."



Okay.  So, first of all, let me just note that I'm very disappointed when something I've deeply researched and then strongly endorse - anyone remember LastPass? - later evolves, or devolves, in such a way that I come to regret my previous full-throated endorsement.  So I'm disappointed to learn that Sync is not standing behind and repairing their various client apps in a timely way.



As for Proton Drive, I have not looked deeply enough to form any opinion one way or the other.  However, its "Proton" name should be familiar, since these are the same Swiss people who offer the very popular Proton Mail service.  So my strong inclination would be to trust them.  What I have no idea about is how their feature-rich offering might match up against Sync.  But my shooting-from-the-hip thought would be that, if it does what you want, for a price that makes sense, I'd say that based upon their past performance on everything else we know they've done, I'd be inclined to give them the benefit of any doubt.  That's obviously not definitive, but at least it's something.



Okay.  So, and last a note about SpinRite.  It has been so extremely useful these past final months of work on SpinRite to have this podcast's listeners taking the current SpinRite release candidate out for a spin and providing feedback about their experiences.  That feedback has been the primary driving force behind the last few improvements to SpinRite 6.1, which turned out to be quite significant.  So I'm glad that I did not declare it finished before that.  And it's been a slowly growing chorus of feedback about something else that caused me to decide that I needed to change one last thing.  Sort of echoing Steve Jobs.



If you've been following along, you'll recall that one of the astonishing things we discovered during SpinRite's development was that all of the original and past versions of the PC industry's most popular BIOS, produced by American Megatrends and commonly known as the AMI BIOS, contained a very serious bug in its USB handling code.  Any access to any USB-connected drive past the drive's 137GB point - which is where 28 bits of binary sector addressing overflow into the 29th bit - causes these AMI BIOSes, which are in the majority, to overwrite the system's main memory right where applications typically load.



When this came to light, I was so appalled by that discovery, and by the idea that this could be very damaging, not only to SpinRite's code in RAM but potentially to its users data, that I decided to flatly prohibit SpinRite 6.1 from accessing any USB-connected drive past its 137GB point.  The next SpinRite won't suffer from any of this trouble since it will have its own direct high-performance native USB drivers.  So my plan was just to get going on SpinRite 7 as quickly as possible.  But SpinRite's early users who have attached larger-than-137GB drives via USB, and then had 6.1 tell them that they could only safely test the first 137GB of their drive, have not been happy.



And also since then, one of the guys who hangs out in GRC's newsgroups, Paul Farrer, whom I've referred to before when this bug was happening, he was curious to learn more about this.  So he looked into the problem while I continued to work on other aspects of SpinRite, working towards just getting it done.  Paul wrote a set of exploratory DOS utilities and tested them with a bunch of old motherboards owned by some of our SpinRite testers in the newsgroups.



What he discovered suggested that more could be done than just turning my back on all USB BIOS access in disgust.  And the disappointment I was seeing from new people being exposed to SpinRite's refusal to work with any USB BIOS convinced me that I needed to fix this.  So I started on that last week, and I expect to have it finished this week because it's not a big deal.



Since only the AMI BIOS is known to have this problem, SpinRite will start by lifting this blanket ban from all non-AMI BIOSes.  Then for AMI BIOSes, since they don't all have this trouble, I've reverse-engineered the code surrounding the bug, and I now fully understand what's going on.  So SpinRite can now detect when the bug is actually present and can patch the buggy BIOS code in place, to raise SpinRite's access limit from 130GB to 2.2TB.  The buggy AMI USB BIOS code will still have a bug that prevents SpinRite from working past 2.2TB, but that's way better in today's world than clamping all USB BIOS access for everyone at 137GB.  So that's the SpinRite that everyone will get, and maybe next week.  So again, a nice feature benefit.  And again, I'm glad that I waited and am putting this in. 



LEO:  Next week, really, next week?



STEVE:  Like, yeah.



LEO:  Maybe.



STEVE:  Close.  I mean, like...



LEO:  No promises, please.



STEVE:  There's really nothing left.  I'm really happy with it.  Nobody has found any problems at all for the last couple months now and, you know, while I've been like letting it sit and stew and cook.  So, yeah.



LEO:  That's awesome.



STEVE:  We're right there.



LEO:  Well, we'll have a cake ready for you.  We'll have a party.  Fireworks.



STEVE:  Just don't throw it in my face.  



LEO:  Yeah, well, no, I won't - we might have some confetti that we can throw in the air instead.  How about that?  A little party for you, Steve.



STEVE:  That would be very nice.



LEO:  All right.  We're going to get to the heart of the subject.  I can't wait.



STEVE:  Oh, boy.  Oh, boy.



LEO:  What is Google doing to protect your privacy and to help advertisers?  We'll find out.  This is kind of breaking news, I think.  So I'm looking forward to hearing this.  All right, Steve.



STEVE:  Okay.  



LEO:  I'm ready.  Let's find out what's going on.  What's Google up to now?



STEVE:  This is big.  I mentioned last week that I thought I might be onto an interesting topic to explore this week.  It turned out that while the guy I stumbled upon was the real deal, his several blog postings were sharing pieces from his Master of Law dissertation for the University of Edinburgh.  After I looked into it more deeply, it didn't really make for the sort of content I know our listeners are looking for.  This scholar was carefully examining the legal and policy implications of Google's recent work on the web, the set of new technologies collectively known as "The Privacy Sandbox."  And he was looking at it against EU and UK laws like the GDPR, what would it mean in that context.



And this guy was not some lawyer.  He is a deep technology guy who has been actively involved with the W3C, serving on many committees and having co-authored a number of web specs.  His focus has always been on privacy; and he's the guy who, years ago, realized that the addition of a high-resolution battery level meter into the HTML5 specifications would provide another signal that could be used for fingerprinting and tracking people across the web.



But as I said, his focus was on what Google's recent work would mean legally.  And for what it's worth, this very well-informed legal and technical academic, this guy who is also a privacy nut, is quite bullish on the future Google has been paving for us.  So that just means that what we are going to talk about this week is all the more relevant and significant.



And what we are going to talk about this week is something known as the Protected Audience API.  It's another of the several components which make up what Google collectively refers to as their Privacy Sandbox.  Now, the name Protected Audience API is every bit as awkward as many of Google's other names.  You know, they're a big company.  They could afford to employ someone with the title of Director of Naming Things, and give this person a big office and a staff, because it's clear, and it will soon become much clearer, that the nerds who invent this technology should not be the ones to name it.  In this instance, what's "protected" is user privacy, and "audience" refers to the audience for web-based display advertising.  But as it is, calling this the Protected Audience API only tells you what it is after you already know, which is not the definition of a great name.



In any event, this collection of work that Google has called their Privacy Sandbox currently contains a handful, dare I say a plethora, of different APIs.  There's the new Topics API which we've previously covered at length.  And there's the Protected Audience API which is what we'll be looking at today.  But then there's also something known as the Private State Tokens API, the Attribution Reporting API, the Related Website Sets API, the Shared Storage API, the CHIPS API, the Fenced Frames API, and the Federated Credential Management API.  And if you didn't already know what those things are, knowing their names only helps with, you know, very broad strokes.



But here is what everyone listening really does need to know:  All of this brand new, serious, deliberately user privacy-focused technology which Google's engineers have recently created and somewhat unfortunately named is real.  It collectively represents a truly major step forward in web technology.  We all grew up in, and cut our teeth on, extremely simple web technology that its founders would still clearly recognize today.  You know, even after many years, this baby hadn't grown much, and it was still far from mature.  We had cookies and JavaScript and ambition, and a lot of ideas about what we wanted to do with the web.  But everyone was heading in their own direction, doing whatever they needed for themselves just to get the job done, and no one was thinking or worrying about longer term consequences.



The web lacked the architectural and technological depth to get us where we wanted to go in the way we needed to get there.  So we wound up with the absolute chaos of tracking and identity brokering and personal data warehousing, deanonymizing, and all the rest of the mess that defines today's world wide web.  And an example of the mess we're in today is the utter pointless bureaucracy, you know, the bureaucratic insanity of the GDPR forcing all websites to get cookie usage permission from each of their visitors.



We know that Google is fueled by the revenue generated from advertising.  Advertisers want to know everything they possibly can about their audience.  They want to optimize their ad buys.  And users are creeped-out by the knowledge that they're being tracked around the Internet and profiled.  And being the super heavyweight that it is, Google is increasingly coming under scrutiny, you know, under the microscope.  But they also have the technological savvy, probably unlike most other players on Earth at this time in our history, to actually solve this very thorny problem which arises from the collision of apparently diametrically opposed interests on today's web.  One thing is clear:  We're in desperate need of more technology than cookies.



Google began the work to truly solve these problems in earnest three years ago, at the start of 2021.  And this wasn't some half-baked attempt to gloss over the true problems that are inherent in the use of a system that was never designed or intended to be used as it is being used today.  Google's Privacy Sandbox initiative was, and today is, a significant step forward in web browser technology and standards which is designed to allow the web to finance its own ongoing existence and services through advertising, without in any significant way compromising the privacy of its users.



Okay, now, I get it.  We've all been so badly abused by the way things have been that it may be difficult to accept that there truly is a way for this to be accomplished.  But there is, and Google has done it.  In the future, the use of the web will be much more private than it ever has been since it was first conceived.  What's required to make this possible is way more technology than has ever been deployed before.  What's been done before now couldn't even be called a half measure.



All of the various APIs I mentioned above, you know, whatever it is they each do, became available in the middle of last year at the start of the third quarter of 2023.  They are all operable today, right now, have been for the last six months, and they are in the world's dominant web browser and other browsers that share its Chromium engine.  And it's not as if there wasn't something, well, some wrong turns that were made along the way; right?  But that's also the nature of pioneering where the path hasn't already been mapped out.  FLoC, remember, Google's Federated Learning of Cohorts, was an attempt at generating an opaque token that revealed nothing about the user's browser other than a collection of their interests.  But FLoC didn't get off the ground.



LEO:  It failed.  It fell on the ground.



STEVE:  It was later replaced - yes.  It was later replaced by Topics, which is a complex, but extremely clever system for doing essentially the same thing, but in a far less opaque and thus far more understandable fashion.  Topics allows the user's browser to learn about the user by observing where they go on the web, all of which information is retained by and never leaves the browser.  Then, through the use of the Protected Audience API, which I'll get to, the user's browser is able to later intelligently select the ads that its own user will see.  I know.  If that comes as something of a surprise, it should, since it's certainly not the way any of this has ever worked before.



Okay.  We've got a lot to cover.  It's good stuff.  One of the key features to note and to keep in mind is that this expands the role of the web browser significantly.  There is now far more going on under the covers than ever before.  It was once fun and easy to explain how a web browser cookie worked.  It wasn't difficult to explain because there wasn't much to it.  But there is very little that's easy to explain about how these various next-generation Privacy Sandbox browser APIs function.  And this is made even more difficult by the fact that they're all so deeply interconnected.



When we originally discussed Topics, we had no sense that its purpose was to allow the user's browser to autonomously perform ad selection.  But that was always Google's intention.  We just needed to see more of the whole picture.  And even when we were only seeing a small portion of the whole, explaining the operation of Topics required a very careful description because it is laced with important subtleties.  And I suppose that's the main point I want to convey here because we're now asking so much from the operation of the web, even wanting things that appear to be in direct opposition, the simple solutions of yesterday will not get us there.



So what is this Protected Audience API?  Believe it or not, opaque as even that name is, the good news is they renamed it to Protected Audience API from what it was before.  Which of course begs the question, "Renamed it from what?"  Okay.  Recall that earlier that they abandoned FLoC (F-L-O-C), which stood for Federated Learning of Cohorts.  In a similar vein, the Protected Audience API was originally named FLEDGE, and that was a painful, you know, they won't give up on these birds.  It was a painful reverse-engineered acronym which stood for First Locally-Executed Decision over Groups Experiment.



LEO:  Oh, that's awful.



STEVE:  Oh, my god, yeah.



LEO:  That's really bad.



STEVE:  Okay, now, not an exactly catchy name.  You know, where is the Director of Naming Things when you need them?  Because nerds should not name things, clearly.  Okay.  And what you're really not going to believe is that FLEDGE grew out of a project named "TURTLEDOVE."  I kid you not.  And yes, TURTLEDOVE was also an acronym, short for Two Uncorrelated Requests, Then Locally-Executed Decision On Victory.



LEO:  God.  That's terrible.



STEVE:  It's really bad.



LEO:  It's worse.  They're getting worse.



STEVE:  They're only missing a word to provide the "E" at the end of DOVE.  So, Excellent?  Everlasting?  Or maybe Excruciating?



LEO:  Yeah.



STEVE:  Yeah.  Anyway, I was able to explain how Topics worked since, while it was a bit tricky and subtle, it was a relatively self-contained problem and solution.  I don't have that feeling about this Protected Audience API because, as I noted earlier, they each only really make coherent sense when they're taken as a whole.  So I'm not going to explain it at the same level of transactional detail.  Okay.  But I want to at least share some sound bites so that you can come away with some sense for what's going on here.  And believe me, that will be enough.



So at the start of Google's Protected Audience API explainer page, it opens with one sentence that needs to be taken absolutely literally.  Okay.  They start with:  "On-device ad auctions to serve remarketing and custom audiences, without cross-site third-party tracking."  Okay.  "On-device ad auctions."  Wow.  Okay, now, I don't expect anyone to understand in any detail what follows.  I don't.  So just let it wash over you, and you'll get some very useful feeling for what's going on.



Google "explains," and I have "explains" in air quotes:  "The Protected Audience API uses interest groups to enable sites to display ads that are relevant to their users.  For example, when a user visits a site that wants to advertise its products, an interest group owner can ask the user's browser to add membership for the interest group.  If the request is successful, the browser records the name of the interest group, for example, 'custom bikes'; the owner of the interest group, which is a URL, like 'bikes-r-us.example'; and interest group configuration information to allow the browser to access bidding code, and ad code, and real-time data, if the group's owner is invited to bid in an ad auction."



Okay.  I know.  Now, just let your head spin.  It'll be okay.  So there is a feeling of the way Topics works here.  The key is that the user's browser visits a site like "custom bikes."  And because their browser is at that site, thus the user is implicitly expressing their interest in custom bikes, an advertiser on that site can ask the user's browser to collect and retain some information that might be used in the future if an ad from that advertiser will be displayed.  Okay, now, note, importantly, that the advertiser learns exactly nothing about the visitor to the site.  All of the information flow is into the user's browser, and only because of the website they're visiting.



Okay.  Now, Google and I continue, I because I had to fix this language to even give us a hope of understanding it.  So I clarified this.  So they said:  "Later, when the user visits a site with available ad space, the ad space seller, either a seller-side provider or the site itself, can use the Protected Audience API to run a browser-side ad auction which will select the most appropriate ads to display to the user.  The ad space seller calls the browser's new - there's a function, navigator.runAdAuction() function, to provide the browser with a list of interest group owners who are invited to bid.



"Bids can only be provided by interest groups that the browser already became a member of when it had previously visited a website where it was able to collect that group, and when the owners of those interest groups had been invited to bid.  Bidding code is retrieved from a URL provided in the interest group's configuration that was received earlier.  This code, which is JavaScript, provides data about the interest group and information from the ad seller, along with contextual data about the page and from the browser.



"Each interest group providing a bid is known as a buyer.  When the visited site's JavaScript calls the new browser function to run the ad auction, each buyer's bidding code generates a bid with the help of real-time data provided by their Protected Audience Key/Value service," whatever that is.  "Then the advertising space seller receives these bids, as well as seller-owned real-time data, and scores each bid.  The bid with the highest score wins the auction.  The winning ad is displayed in a fenced frame" - which is one of those new APIs - "which absolutely prevents it from having any interaction with anything else anywhere.



"The ad creative's URL is specified in the bid, and the origin must match one in the list provided by the interest group's configuration, that same information that was received earlier.  Finally, the advertising space seller can report the auction outcome, with a function known as reportResult(), and buyers can report their auction wins with a new function, reportWin()."



Okay.  And finally, a bit later, Google offers a bit more detail, writing:  "In the Protected Audience API, an ad auction is a collection of small JavaScript programs the browser runs on the user's device to choose an ad.  To preserve privacy, all ad auction code from the seller and buyers is run in isolated JavaScript worklets that cannot talk to the outside world.  A seller, a publisher or a supply-side platform, initiates a Protected Audience ad auction on a site that sells ad space, such as a news site.  The seller chooses buyers to participate in the auction, indicates what space is for sale, and provides additional criteria for the ad.  Each buyer is the owner of an interest group.



"The seller provides the browser with code to score bids, which includes each bid's value, the ad creative URL, and other data returned from each buyer.  During the auction, bidding code from buyers and bid-scoring code from the seller can receive data from their Key/Value services.  Once an ad is chosen and displayed, in a fenced frame to preserve privacy, the seller and the winning buyer can report the auction result."



Okay.  Now, if all of this sounds insanely complex, you're right.  This is not your grandpa's third-party cookies anymore. Nor are our web browsers simple apps running on our chosen OS to display HTML code.  Those are the days that are long gone, and they're not coming back.  It should now be abundantly clear to everyone that what Google has done with this Privacy Sandbox is to radically transform our web browsers from passive displays of whatever page is sent to them, into proactive advertising management engines.  All of this new technology is already built into Chrome and has been there for the past six months.



Does all this probably give Sir Timothy John Berners-Lee, the web's original inventor, a huge headache?  I would not be at all surprised if it did.  Nothing less than an incredible mess is required to deliver interest-driven advertising to users without revealing anything about those users to their advertisers.  And by the way, "An Incredible Mess," as I said earlier, was the runner-up title for today's podcast.  A large part of what I want to convey here is that nothing short of this level of complexity is required to protect our privacy while providing what the websites we depend upon, and want unpaid access to, say they need.



Now, the nature of inertia means that we would never, and I really mean never, move from the absolute mess we're in today to this new promised land were it not for a behemoth like Google to, first, carefully design and craft this solution, doing so openly and in full public view, inviting collaboration and industry participation at every step of the way, as they have; and, secondly, to then literally force it down the closed, choking throats of the rest of the existing advertising technology industry by taking Chrome, their world domineering browser, and gradually deprecating and foreclosing upon the operation of all of the previous tricks and techniques that have historically been used for user tracking and compromising users' privacy in the service of advertising tech.



No one else could do this but Google.  This is not something where consensus could ever have been reached.  It would never happen.  It would be "committee deadlock."  I've looked at the various ad tech blogs, and they're all screaming and pulling their hair out over this.  But they're all also busily conducting experiments and getting ready for what they, too, understand is already inevitable.



Notice that one of the things Google has done with this reconceptualization of web advertising is to move the advertising auctioning process away from the advertiser and into the browser.  Traditionally, an advertiser would purchase real estate on the web on website pages.  Then they would run their own real-time auctions to determine which of their many advertising clients' ads should be inserted into that space for any given visitor, given everything that the advertising seller knows about the visitor from tracking them across the Internet.  This changes all of that.



Now, all of the work is being done on the client side rather than on the server end, and doing this starves advertisers of all the data they were previously collecting while convincingly arguing against their having any further need to ever collect anything.  In this new world, advertisers place static purchase offers to display content on website real estate with whatever ads they have to display, organized by interest group.



Using Google's new APIs, browsers that had previously visited websites representing various interest groups are now able to collect the advertiser's material that will later be needed to display ads for those interested.  Then later, when browsers visit other websites with sell offers behind available advertising real estate, all of the information about the offers flows into the browser, which then itself conducts the auction and selects the ad that is most relevant to its user, based upon the places the browser has visited during the past few weeks.



The results of the auction are returned to all interested parties, and the ad tech company pays a piece of the action, or of the auction, to the site that offered up the real estate.  In something of a follow-up, Google explains:  "Understanding user interests can enable more relevant ads than just choosing ads based on site content (contextual targeting) or by using information provided by a user to the site on which the ad appears (first-party ad targeting).  Traditionally, ad platforms have learned about user interests by tracking their behavior across sites.  Browsers need a way to enable ad platforms to select relevant ads, so content publishers can get ad revenue without cross-site tracking.  The Protected Audience API aims to move the web platform closer to a state where the user's browser on their device, not the advertiser or ad tech platforms, holds the information about what that person is interested in."



And that states it perfectly, I think.  The way the entire web advertising world has worked until now is that every advertiser had to collect all of the information they possibly could about every individual who was surfing the Internet for the sole purpose of selecting the best advertisement to show them.  The result was massively intrusive, massively redundant, and an ultimately ineffective utilization of resources.



But in the new world of Google's Privacy Sandbox, it's the user's browser that collects the information about its own user's interests by watching them navigate the web. As the browser moves around the web, future advertising opportunities are collected by the browser.  And later, when visiting a site that is offering some available advertising space, the browser itself runs an auction on the fly to decide which of the opportunities it previously collected should be presented to its user based upon the criteria that it solely maintains.



This is obviously a big deal.  But what seems just as obvious is that no lesser of a deal would get this important job done right.  We can argue, and we'll always be able to argue - we certainly know that the EFF will always argue - that all website user-driven advertising customization should simply be ended, and that advertisers should settle for contextual advertising - placing their ads on sites which are offering content that's relevant and related to their ads - just like in the pre-tracking days.  Unfortunately, multiple studies have shown that this would reduce website advertising revenue by about half, and many websites are barely making ends meet as it is.  So the EFF's ivory tower stance is simply not practical, and it's never going to happen.



The only way to permanently end tracking is for it to be flatly outlawed.  But tracking will never be outlawed while the case can be made that advertising customization is the only thing that's keeping today's web alive and financed, and that there's no alternative to tracking and compiling interest-profiling dossiers on everyone using the Internet.  So what Google has done is to create a practical and functioning alternative.  Tracking is no longer necessary.  User privacy is preserved.  And once this new system has been established, we can anticipate that we will finally see legislation from major governments - probably with Europe taking the lead - which will flatly and without exception outlaw any and all Internet user profiling and history aggregation because it will no longer be required.



Google's Privacy Sandbox masterpiece has been in place, as I've said several times, for the past six months.  And although they've already been kicking and screaming, all other serious advertisers have been exploring it in anticipation of the future, which appears to be all but certain.  As we move into 2024, fingerprinting will become increasingly fuzzy, and Chrome's third-party cookie support will be gradually withdrawn from its ubiquitous web browser.  And finally, once the dust settles on all this, we can anticipate the end of the annoying cookie permission request pop ups.



LEO:  I hope you're right.



STEVE:  We are heading toward a brand new web.



LEO:  Do you think that, like Manifest V3, this will be adopted by other browsers at some point?  Although as you pointed out earlier, Google has complete dominance in the browser usage.



STEVE:  They have complete dominance.  Not only them, but all Chromium.  So really it's Safari and Firefox that are the remaining wildcards.  And this, I mean, this is what Google is going to do.  I think they've nailed it.  You know, they have a solution.  I mean, and the way they've nailed it is by massively burdening the browser with, like...



LEO:  Well, I'm going to say that, is that my system is now working really hard to deliver ads.  It just makes - by the way, the good news is this will be very easy to block.



STEVE:  Yes.  And in fact, you can opt out of this.



LEO:  Oh, can you?  Oh, interesting.



STEVE:  Absolutely.  There is a user-facing API that lets you just say no.



LEO:  Oh.  Okay.  That's smart.



STEVE:  Google knows most people will not say no.



LEO:  Right.



STEVE:  And I will not say no.  If my use of the web is now private, and my browser is selecting the best ads for me to see, which is returning the highest amount of revenue to the websites I'm visiting, it is a win-win-win.



LEO:  It's really an interesting idea.  It's a great solution in terms of, you know, protecting your privacy, for sure.



STEVE:  Yes.  It turned the entire model on its head.  And the fact is today's, I mean, you know, once upon a time a browser was a little HTML rendering engine.



LEO:  Yeah.



STEVE:  You know?  Now it is literally a behemoth.  I mean...



LEO:  Well, that's one of the things that bothers me is now, I mean, the browser's going to be 90% of your CPU pretty soon.



STEVE:  It will.  I mean, although it is little lightweight scripts.  And we know that Google has a frenzy about performance.



LEO:  Right, right.



STEVE:  You know.  And how quickly this all displays.



LEO:  Here's where Tim Berners-Lee might actually like this.  He's been working toward a solution where you control your own data.  You know, that your data is yours, and you lease it out, in effect, to people, which this is basically an implementation of.  So it fits right into what Tim Berners-Lee has been doing of late. 



STEVE:  Yeah.



LEO:  So I think that it's possible Google may have found a way to give what we would like.  Our holy grail would be for us to control our own information about ourselves, and then have the opportunity, if we wished, to share it, but at a price, you know, that we get something out of it.  And that this is a step toward that.



STEVE:  As far as I know, there is no sharing opportunity.  What there is in the UI is you can even browse the interest groups if your browser has said yes.



LEO:  Yeah, yeah, see what you're saying, yeah, what it's saying.



STEVE:  And if you object to any, you're able to delete them, and you're able to mark them as never come back if you really don't want it.



LEO:  Yeah.  It's interesting.



STEVE:  No, they've really - they've nailed this.



LEO:  Yeah.



STEVE:  I mean, and this is where they're going, and we know who "they" are.  So, and their browser, it's funny, too, because I, you know, we've given a lot of space to the notion of fingerprinting, I think because it's kind of a cool technology.  Everybody is still using cookies.  Cookies is - and so when Google talks about right now, as of the beginning of the year, 1% of their users have third-party cookies turned off.  And they're going to be, you know, they're doing that as an initial experiment.  And then they're going to be deprecating the rest of third-party cookies.  There will be no more third-party cookies by the middle of this year.



LEO:  That's huge.  That's so good, yeah.



STEVE:  And it is - and now so that's what's got the advertisers screaming and thinking, well, you know, we liked knowing all this about people.  But we're going to have to fall in line.



LEO:  This is the future, yeah.



STEVE:  It is the future.



LEO:  And it really is a response to widespread ad blocking, the cookies, and other GDPR requirements.  Yeah, I think it's interesting.  Let's see what happens.  They've thrown so many ideas up against the wall.  None of them have stuck.  This might be the one.



STEVE:  It does solve the problem.  I see nothing wrong with it.



LEO:  Yeah.  Good.  Thank you for filling us in.  The Protected Audience API.  Terrible name, but a very interesting concept, yeah.  Your browser is the one that determines what you see.



STEVE:  Yeah.  And even Privacy Sandbox.  I mean, that doesn't tell you anything.



LEO:  No.



STEVE:  Like, you know, don't kick sand in my eyes.



LEO:  Good.  We'll talk about it tomorrow with Jeff.



Copyright (c) 2024 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.






GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#958

DATE:		January 23, 2024

TITLE:		A Week of News and Listener Views

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-958.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  What mistake did Microsoft make that allowed Russians to access their top executives' email?  What does the breach of U.S. Health and Human Services teach us?  What does Firefox's complaint about Apple, Google, and Microsoft mean?  Why has the Brave browser just reduced the strength of its anti-fingerprinting measures?  Last year CISA started proactively scanning.  How'd that go?  What new feature of smartphones has become a competitive advantage?  And just how incognito is that mode?



Then we'll wrap up the week by looking at some of the best feedback from our listeners, including what's the future of fraudulent media creation?  How should a high school listener of ours get started with computing?  Why did a popular Android app suddenly become sketchy?  Does Google's Privacy Sandbox allow websites to customize their presentations to their visitors?  How might last week's LG smart washing machine have become infected?  Does the Protected Audience API also protect its audience from malvertising?  And why do big ISPs just pull the plug on DDoSed sites rather than attempt to protect them?



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  And, man, do we have a jam-packed show.  It's my favorite kind of show, lots of listener questions and lots of Steve's answers.  We'll also learn what farbling is, and why it turns out too much farbling is too much of a good thing.  Or something like that.  Why Mozilla is unhappy with the heavy-handed tactics of Big Tech, and I kind of don't blame them.  And of course a Picture of the Week that makes absolutely no sense.  It's just part of the fun every week, right here on Security Now!, next.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 958, recorded Tuesday, January 23rd, 2024:  A Week of News and Listener Views.



It's time for Security Now!.  I know, I know, you've been waiting all week, and here it is, Tuesday.  It just never comes around fast enough.  But Steve Gibson is here with another thrilling, gripping, how many pages did you say, 19 pages?



STEVE GIBSON:  Nineteen pages.  And Leo, this week people on Apple platforms will be able to see the pictures.



LEO:  All around you.  You didn't buy a Vision Pro, huh?



STEVE:  No, no.



LEO:  No, me neither.



STEVE:  I'm astonished by the technology, and I don't think I know anybody except through TWiT who will have any.



LEO:  Well, I'm coasting on those guys.



STEVE:  I'm not a big - long term, I think VR can be astonishing because, you know, our animal natures get hooked by what our senses perceive, I mean, powerfully.  And in fact I actually have some comments about that later in today's show.  But so I think VR can be very powerful.  I just think, you know, I remember when we tried to do the first generation of laptops, and they had to have wheels because, okay, well, you can move it around, but you need to get Bruno to put it up on the desk for you.  Finally we got actual laptops.  But it took a decade from when we first began trying.  I think VR is like that.  We're going to get there.  And until we do, everyone's going to be like, oh, VR doesn't work, it's nonsense, blah blah blah.  It's like, no, it's just too soon, you know.  Our ambition is exceeding our technology at the moment.



LEO:  Yeah, you heard and agreed with, I think, Jason Snell's take.



STEVE:  Yes.  



LEO:  On MacBreak Weekly earlier.



STEVE:  Yes.  I think it absolutely makes sense for Apple to be launching this now to get a whole bunch of developers going.  They're going to learn so much, Apple is, from all the feedback and experiences.  And this is also first-generation technology.  It's incredibly impressive.  It's not clear at the moment how it can be like super reduced.  But, you know, there was a great movie, remember "Brainstorm"?



LEO:  Oh, dimly.



STEVE:  With Christopher Walken.



LEO:  Yeah.



STEVE:  Christopher Walken.  And the very first...



LEO:  It was a Michael Crichton book; wasn't it?



STEVE:  I think it was, yes.  The very first cap was like, you know, all this crap and a huge umbilical going down to a cart that they had to push around next to the guy.  And through a succession of innovations, they reduced it to just a small little clip that you stuck on your head that still had the same effect.  And, yeah, we're going to get there.



LEO:  It will get there.



STEVE:  We're just not there yet.  I look at that the most modern, the underside of today's hard drives, and there's a 16TB hard drive.  It's got a little itty-bitty circuit board like that runs along the connector.  And it's like, what we used to have was crazy compared to the degree of integration that we have today. 



LEO:  We used to have a card in a slot to run a hard drive; you know?



STEVE:  Yeah and sometimes a daughter board and lots of cables running around.  



LEO:  And you didn't have - and now they've got 30TB hard drives which we never thought would happen.  And I certainly bought those little ones.



STEVE:  I don't think they actually exist, Leo, because no one's ever managed to actually fill one.  So how would we know?



LEO:  We wouldn't know.  You've got to run your little program on it and see.



STEVE:  That's right.



LEO:  On 30TB it might take a little while.  What do we have?  What's on the menu for today?



STEVE:  We have Episode 958 for this second to the last, that would make it the penultimate episode of January's Security Now!.  No single topic jumped out and grabbed me.  So I titled this "A Week of News and Listener Views."  But that is not to say there's not a lot going on.  We're going to find out what mistake Microsoft made that allowed Russians to access their top executives' email.  What does the breach of the U.S. Health and Human Services Department teach us?  What does Firefox's complaint about Apple, Google, and Microsoft mean?  Why has the Brave browser just reduced the strength of its anti-fingerprinting measures?  Last year CISA started proactively scanning.  How'd that go?  What new feature of smartphones has become a competitive advantage, thankfully?  And just how incognito is that mode?



Then we'll wrap up the week by looking at some of the best feedback from our listeners, including what's the future of fraudulent media creation?  How should a high school listener of ours get started with computing?  Why did a popular Android app suddenly become sketchy?  Does Google's Privacy Sandbox allow websites to customize their presentations to their visitors?  How might last week's LG smart washing machine have become infected?  Does the Protected Audience API also protect its audience from malvertising?  And why do big ISPs just pull the plug on DDoSes rather than attempt to protect them?  Of course we have a great Picture of the Week for our listeners to view, and I think another great podcast this week.



LEO:  So somebody's saying that "Brainstorm" was not a Crichton book.  It sounds like one, though.



STEVE:  Yeah.



LEO:  It seems like it would be.  I'm going to have to watch that tonight.  Maybe I'll watch it today.  I love Christopher Walken.



STEVE:  It's a great - Christopher Walken.  Also...



LEO:  Natalie Wood, Cliff Robertson, Louise Fletcher?  What a cast.



STEVE:  Fletcher is the one, yes, it's got a great cast.



LEO:  Nurse Ratched's in it, yeah.



STEVE:  And it's also - it's a very cautionary tale about VR.  I mean, it is the early VR movie, early VR sci-fi.  And I'm tempted to say more.  But I don't want to spoil it for any of our listeners because he does something, well, there are two very controversial aspects of it.  So anyway, said enough.  Great movie, definitely.



LEO:  And I will watch it.



STEVE:  It is a great, it is a good sci-fi movie.



LEO:  From 1980, what, '83?  Wow.



STEVE:  And it was Natalie Wood's last movie.



LEO:  Oh, that's sad.  I will watch it.  Loved her.  All right.  Let's take a little break; shall we?  And then we will get into the show, in the depth.  You know, as you read your list of topics for today, I realized your show really is more and more kind of about everything going on in computing.  I mean, it is as much a, you know, here's what's happening in computing show as TWiT is, frankly, since so much of it revolves around security these days.



STEVE:  And privacy.



LEO:  And privacy technology.  So I can't wait.  I have some thoughts about some of these topics, too.



STEVE:  Cool.  I just got my renewal notice from [Bitwarden] since it was a year ago.



LEO:  What do you pay, 10 bucks a year.



STEVE:  Ten bucks.  I don't need to, but I want to keep them going.



LEO:  Right, exactly.  I pay the 10 bucks too for the premium account.  I don't need to.  But I want to support them.  I just think they're the greatest, yeah.



STEVE:  Okay.  So I assume you've not yet seen the Picture of the Week.



LEO:  No, I always try to keep my eyes clean.



STEVE:  When you can, that's good.  And this will be one of those because this one is - oh.



LEO:  All right.  I'm scrolling up now.  I'm looking at the show notes.  You can do it with me, folks.  Let's scroll up.  "You have to wonder," Steve writes, "how much use that peephole gets."  Okay, Steve.  You got me.  That's hysterical.  Do you want to explain?



STEVE:  Okay.  So we're all familiar with the little peephole that people have on like the doors of their residences, where there's some lensing so that, you know, before you open the door to admit someone, you're able to stick your eye up to it and see who's standing on the other side.



LEO:  But that's usually on solid doors.



STEVE:  Yes.  Not on a door that is basically four large glass panes.  That's like, what?  What?



LEO:  That's pretty funny.



STEVE:  And there's also a glass side panel.  So even if the door was solid, you'd still have to be careful about how you approached the door.  You couldn't approach from the left on the inside or they'd see you walking over to the door.



LEO:  That's very funny.



STEVE:  So anyway, so the point - now, the only thing I can think, Leo, is that the door comes in glass or non-glass panel options.



LEO:  Oh, that's what it is, sure.  Yeah, yeah.



STEVE:  So you could have had it where the whole thing was white, with opaque white panels.  Then the peephole would have mattered.  So the peephole is probably there regardless of whether you get glass or not.  But it does make for a funny picture because, you know, who's going to - and actually it would be fun if the people who lived there actually had some fun with it, like if someone's knocking at their door, so they go over to the door and look through the peephole to see who it is.



LEO:  Oh, how funny.



STEVE:  Anyway, thank you, our listeners.  Our listeners are providing a constant stream of great photos that they think, okay, saw this and thought of you, Gibson.



Okay.  Last Friday the 19th, the rest of the world learned that Microsoft's top executives had fallen victim to a Russian state-sponsored password attack - like, what, password? - which breached their email accounts.  Here's what Microsoft shared in their Friday blog posting, which was titled "Microsoft Actions Following Attack by Nation State Actor Midnight Blizzard."



They wrote:  "The Microsoft security team detected a nation-state attack on our corporate systems on" - now this, you know, a nation-state attack is how you dramatize the fact that somebody guessed your password, it's like, whoops - "on January 12th, 2024, and [the Microsoft security team] immediately activated our response process..."



LEO:  We pulled the chain that said "Emergency Only."



STEVE:  Right.  Break glass and pull all the wires, yeah, "...activated our response to investigate process to investigate, disrupt malicious activity, mitigate the attack, and deny the threat actor further access."  Yes, we changed our password.  "Microsoft has identified the threat actor as Midnight Blizzard..."



LEO:  How did they identify it, since any moron could have done this?  It wasn't like a hefty, you know, eight-stage attack involving a secret hash table or anything like that.



STEVE:  Let's try Monkey123.  Oh, what do you know, we're in.



LEO:  Read the sentence about what they did.  I love this.



STEVE:  Okay, well, we'll get there, "...the Russian state-sponsored actor also known as Nobelium."



LEO:  Right.



STEVE:  Now renamed Midnight Blizzard.



LEO:  They must have [crosstalk].



STEVE:  Because it sounds much more dramatic and, like, how did you manage to survive the Midnight Blizzard?  "As part of our ongoing commitment to responsible transparency" - in other words, we're a publicly traded company, and we have to tell you, "as recently affirmed in our Secure Future Initiative (SFI), we're sharing this update.



"Beginning in late November 2023, the threat actor" - okay, now again, late November, right, so now we have December and up to January 12th.  So they've been roaming around, apparently for a while.  "Beginning in late November 2023, the threat actor used a password spray attack..."



LEO:  Yeah, what's that, Steve?  What's a password - oh, and worse, on what, Steve?  What did they use that password spray attack on?



STEVE:  Yeah, "...to compromise a legacy non-production test tenant account and gain a foothold" - and in other words, you know, they guessed the password.



LEO:  Guessed the password of some old machine that's probably in a corner somewhere.



STEVE:  Yeah.  They sprayed it, Leo.  And, you know, it should have been sprayed with cleanser.



LEO:  Password spray account attack.  Well, all right.  That's why we know it's Nobelium.



STEVE:  They gained a foothold, "...and then used the account's permissions" - which apparently were too permissive...



LEO:  Well, that's the real question; right?  Okay.  So you got in this old server in a closet somewhere, fine, with a bad password and no two-factor.  Fine.  But what then happened?



STEVE:  And then in their attempt to minimize this, they said:  "to access a very small percentage" - but apparently it was sufficient.



LEO:  It was the right percentage.  They weren't interested in Joe in accounting.  For some reason they only wanted to see Satya Nadella's email.



STEVE:  But get this.  They have to enumerate, right, "a very small percentage" - this is because Microsoft has so many corporate accounts - "a very small percentage of Microsoft corporate email accounts, including..." that small percentage included...



LEO:  Tiny percentage.  Yeah, just tiny.



STEVE:  "...members of our senior leadership team and employees in our cybersecurity, legal, and other functions..."



LEO:  Maybe they did get Joe from accounting.



STEVE:  Yeah, it's not good, "...and exfiltrated some emails..."



LEO:  Oh boy.



STEVE:  Some, right, we don't know how many.  They know, they're not telling us, "...and attached documents."



LEO:  Oh, gobs.



STEVE:  "The investigation indicates they were initially targeting email accounts for information related to Midnight Blizzard itself."  Like what do they know about us?  Why did they change our name?  We liked Nobelium.  Now we're Midnight Blizzard?  What?  "We are in the process of notifying employees whose email was accessed."  Because apparently that's a big process.  So, okay.



LEO:  It was only a small fraction, Steve.  Let's not blow it out of proportion.



STEVE:  It's a small fraction of a big number, though, Leo.  So it's going to take a while for us to notify all those employees.



LEO:  Of course the guys who got in weren't interested in 99% of the email accounts.



STEVE:  Only the good stuff.



LEO:  Just the good ones.



STEVE:  Right.  Yeah, we've got cybersecurity, legal, and other not specified functions.



LEO:  And the people who run the freaking company.



STEVE:  Yeah.  The leadership team.  You know, those guys.



LEO:  The fact that they say...



STEVE:  But apparently we can't get a hold of them right now.  So we're in the process...



LEO:  We're going to notify him.  We don't know where he is.  Oh, lord.



STEVE:  Now, here we come.  "The attack was not the result of a vulnerability" - what a nice, fresh change - "in Microsoft products or services.  To date, there is no evidence that the threat actor had any access to customer environments, production systems, source code, or AI systems."  I thought it's interesting now that we're being specific about whether they got into our AI or not.  "We will notify customers if any action is required."  Not only that, but they sprayed everything, apparently.



"This attack does highlight the continued risk posed to all organizations" - right, not just Microsoft, you know - "from well-resourced nation-state threat actors like [the newly renamed] Midnight Blizzard.  As we said," they wrote, "late last year when we announced Secure Future Initiative, given the reality of threat actors that are resourced and funded by nation states, we are shifting the balance we need to strike between security and business risk."  In other words, they've decided they're going to get more security focused.  "The traditional sort of calculus is simply no longer sufficient," they say.  "For Microsoft, this incident has highlighted the urgent need to move even faster" than they apparently were.



LEO:  This is the worst kind of corporate doublespeak.



STEVE:  It really is.



LEO:  They're implying - someone guessed the password of a little used machine that happened to have permissions to access their servers.



STEVE:  Yup.



LEO:  That is the worst possible corporate governance.



STEVE:  Yup.



LEO:  This is embarrassing to them.  So they've waved - there's a lot of hand-waving about, oh, no, it was a big nation-state.  They guessed their password.



STEVE:  And everybody else is at risk.  Oh, my god, yeah.  So they explained...



LEO:  So infuriating.



STEVE:  Yeah.  So, "We will act immediately" - because we didn't before, so we're going to do it now - "to apply our current security standards to Microsoft-owned legacy systems and internal business processes, even when these changes might cause disruption to existing business processes."  Oh, this causes disruption.  "This will likely cause some level of disruption while we adapt to this new reality."  Whoa, wakeup call.



LEO:  We've got to lock these things down.  Holy cow.



STEVE:  "But this is a necessary step, and only the first of several we will be taking to embrace this philosophy."



LEO:  Oh.  You know what I think?  It was a PlayStation 2 in Satya Nadella's office.  That's what I think.



STEVE:  They're not telling us.



LEO:  No, they won't tell us.



STEVE:  "We are continuing our investigation and will take additional actions based on the outcomes of this investigation.  We'll continue working with law enforcement and appropriate regulators.  We are deeply committed to sharing more information" - right, oh, and here we come - "and our learnings."  We're going to get some more learnings sharing, Leo.



LEO:  Oh.



STEVE:  "So that the community can benefit from both our experience and observations about the threat actor."  We can't see them, of course, because it's midnight, and there's a blizzard.



LEO:  We're going to sketch them.



STEVE:  "We will provide additional details as appropriate.  You will never hear from us again."  Oh, no, I added that part.



LEO:  This is clearly written by, I mean, the fact they used the word "learnings" is the tell off, by corporate PR, not by a technical person.  But it's embarrassing.



STEVE:  Leo, the technical people would spill the beans.



LEO:  Yeah.



STEVE:  We can't have that.  We have to have someone who knows nothing write our announcement of what happened so that nothing will be known afterwards.



LEO:  Yeah.  Wow.



STEVE:  So it does read as a bit of a wake-up call for Microsoft.  And it's interesting because arguably they were telling us, and they're right, there's a lesson here for every large enterprise.  You know?  As you said, Leo, read between the lines, and actually reading the lines, it sounds as though some older systems still have older levels of security, and they've been allowed to continue purring along undisturbed since, you know, they aren't bothering anyone.  But they were still online and obviously accepting incoming logons and so getting sprayed with passwords.  And, you know, presumably newer systems are being deployed with stronger password quality minimums, multifactor authentication, brute force detection, you know, spray prevention, and all of the additional layers of security that have become modern standard practice.



The lesson here, which Microsoft has just learned the hard way, and which I wanted to bring up, you know, I wanted to bring to the attention of our IT-managing listeners, is that the law of the lowest hanging fruit applies to legacy machines that are not bothering anyone.  You know, they aren't bothering anyone until they become the source of ingress into an enterprise's interior.  So, just a note of caution to remember that bad guys won't attack the most secure entry points to an organization, or if they do they won't get in.  They will attack successfully the weakest.  And that might be some machine that still has a password and policies that have not been considered safe since the turn of the century.



And this is really a thing.  We all know the lesson LastPass learned by failing to proactively enforce PBKDF iteration counts which were current at the time that they were introduced, and they never changed them again.  Security really is a moving target, and older systems won't improve their older security on their own without it being revisited.  You know, we would not expect Microsoft, to your point, Leo, to not to put the best face on this possible.  So we can assume that they did.  But the news of this breach received some harsh criticism from other quarters.



Here's what one respected security reporting group had to say.  They started by speaking about some email content, writing:  "Microsoft's disclosure language does not specifically state that this was the only stolen material, but it is worth pointing out that Microsoft is currently hosting the Ukrainian government's entire network on its Azure cloud infrastructure."  That's interesting. I hadn't encountered that little tidbit before.



Anyway, they continue, writing:  "The breach has drawn quite an avalanche of criticism and ridicule for Microsoft for various and well-deserved reasons.  First, Microsoft disclosed the breach late on a Friday night, a well-known scummy tactic," these people write, "to hide the incident from extended media coverage.



"Second, the breach took place weeks after Microsoft announced, with bells and whistles, its new Secure Future Initiative, a new plan to re-focus the company's engineering efforts to improve the security of its own products.  The new initiative was meant to mimic a similar pledge made by Bill Gates in 2002, named Trustworthy Computing" - something we all know well, those of us who've been in the industry - "that led to significant changes to Microsoft's security posture and the creation of what we now know as Patch Tuesday.



"Third, the new breach took place four months after Microsoft disclosed another state-sponsored hack, this one by China's Storm-0558, which also had access to its internal network."  And that's the one we talked out extensively previously.  "And fourth," they wrote, "after promoting its multifactor authentication as the next evolution of online account security, the fact that one of its test accounts got popped via a password spray suggests Microsoft was not high on its own supply.



"The hack is quite bad, but not for most of you reading this.  It may not have a material impact on day-to-day Microsoft users, but it has quite a reputational damage on Microsoft's position in the cybersecurity market.  Having Russian intelligence services breach your cybersecurity team's email accounts to steal data about themselves four months after the Chinese breached your production systems to steal U.S. government emails is not what this industry calls trustworthy."  So indeed.  Most of that criticism is covered by the observation that they didn't update their older systems.  But I just want to say it is certainly the case that old systems need attention, and they should get it.  So a useful reminder about that.



A still unknown threat actor stole $7.5 million from the U.S. Department of Health and Human Services in a security breach that took place between March and mid-November of last year.



LEO:  Why, that's almost six months, Steven.  It's almost nine months.



STEVE:  I know.  It struck me as interesting that the range is that broad.



LEO:  It's kind of large, yes.



STEVE:  There's quite a lot of time between March and November.



LEO:  They need a Thinkst Canary.



STEVE:  Uh-huh.  The unknown attackers are believed to have gained access to an HHS system that processes civilian grant payments using spear-phishing.  They then proceeded to hijack payments for five grant recipients before being detected.  The investigation to identify the perpetrators is still underway.  So our takeaway here is that, once again, the human factor remains Security's number one Achilles heel.  Having strong outbound security - such as I was reminded by that provided by the ADAMnetworks guys who so impressed me when I talked about them last year - and also training, training, training, including reinforcing that training on a continuing basis.  You've got to teach your people not to click on links.



LEO:  Yeah, yeah.



STEVE:  Like, you know, you just have to.  And then what the ADAMnetworks guys do is a great job on neutering the clicking of the link if someone still does.



LEO:  Right, right.



STEVE:  So, you know, it's so easy for a harried worker who has too much going on to click a link that they shouldn't.  And, you know, that's what happened here.



LEO:  Yeah, but Steve, when they use the devastating password spray attack, all bets are off.



STEVE:  You met my good friend Bob Basaraba.



LEO:  Oh, yeah.



STEVE:  A Canadian.  His brother is actually a Hollywood actor, Gary Basaraba, who I see on, like, he makes little appearances here and there.  Gary is nontechnical, but just larger than life, just a big guy.  Bob was telling him once about a ping flood and how like a ping flood was like pushing somebody off the 'Net.  And Gary, who knows absolutely nothing about computers, said, well, why didn't they just use a reverse ping attack?



LEO:  Oh.  There you go.  Another one to add to my quiver of tools.



STEVE:  Reverse ping attack.  That'll teach them to ping you.



LEO:  Ping 'em back.



STEVE:  Also last Friday, Mozilla posted a complaint to the industry under the heading "Competition."  The title of this posting was "Platform Tilt:  Documenting the Uneven Playing Field for an Independent Browser Like Firefox."  Okay.  So here's what Mozilla wrote.  They said, correctly:  "Browsers are the principal gateway connecting people to the open Internet, acting as their agent and shaping their experience.  The central role of browsers has long motivated us to build and improve Firefox in order to offer people an independent choice.  However, the centrality of the browser creates a strong incentive for dominant players to control the browser that people use.  The right way to win users is to build a better product, but shortcuts can be irresistible.  And there's a long history of companies leveraging their control of devices and operating systems to tilt the playing field in favor of their own browser.



"This tilt manifests in a variety of ways, for example, making it harder for a user to download and use a different browser, ignoring or resetting a user's default browser preference, restricting capabilities to the first-party browser, or requiring the use of the first-party browser engine for third-party browsers.  For years, Mozilla has engaged in dialogue with platform vendors in an effort to address these issues.  With renewed public attention and an evolving regulatory environment, we think it's time to publish these concerns using the same transparent process and tools we use to develop positions on emerging technical standards.  So today we're publishing a new issue tracker where we intend to document the ways in which platforms put Firefox at a disadvantage.  We wish to engage with the vendors of those platforms to resolve these issues.



"This tracker captures the issues we experience developing Firefox, but we believe in an even playing field for everyone, not just us.  We encourage other browser vendors to publish their concerns in a similar fashion, and welcome the engagement and contributions of other non-browser groups interested in these issues.  We're particularly appreciative of the efforts of Open Web Advocacy in articulating the case for a level playing field and for documenting self-preferencing.  People deserve choice, and choice requires the existence of viable alternatives.  Alternatives and competition are good for everyone, but they can only flourish if the playing field is fair.  It's not today, but it's also not hard to fix if the platform vendors wish to do so.  We call on Apple, Google, and Microsoft to engage with us in this new forum to speedily resolve these issues."



Okay, now, of course, many of us prefer to use Firefox as our browser of choice.  I have Chrome and Edge, but URL clicks are always sent to Firefox.



LEO:  Me, too.  On all our machines, yeah.



STEVE:  It's my default browser.  And I have Firefox installed on all my various Apple iOS devices.



LEO:  Oh, see, I haven't gone that far.  They really don't make that easy.



STEVE:  Unh-unh.



LEO:  And you're not really using Firefox.



STEVE:  Exactly.  So I dug a bit deeper into this new issue tracking system, and it was quickly apparent that Apple had the most strikes against it.  At this moment, Mozilla is complaining about Apple Store forbids third-party browser engines; support for third-party multi-process applications on iOS; JIT [Just in Time] compilation support on iOS; accessibility APIs on iOS; messages integration; importing browser data; setting and checking default browser; origin-based Associated Domains dependent features for third-party browser engines; browser extension support; and beta testing support on iOS.



Now, we know how heavy-handed Apple is.  I'm an avid user of Amazon's Kindle readers and also of Amazon's Kindle app on iOS, where I use it on iPads and my iPhone.  And it is a constant and ridiculous annoyance that Apple refuses to allow Amazon users to purchase books through the Amazon app.



LEO:  Absolutely.



STEVE:  It is so dumb.  It's necessary to use a web browser.  Why?  Because Apple has iBooks and cannot stand the competition.



LEO:  Well, they would let Amazon do it, but they would get 30% for it.  And Cory Doctorow was talking about this on Sunday, that margins on the books, the eBooks, is lower than 30%.  So Amazon would have to give them all the profit and then some.



STEVE:  Right.



LEO:  So it's just not going to happen.



STEVE:  It's just dumb.  So I'm sure that Apple's reticence to allow Chrome and Firefox and any and all other non-Safari browsers to enjoy the same privileges they have on other platforms is largely about security.  I mean, I get it; you know?  As we know, browsers have become the number one way for evildoers to crawl inside our computers.  So I don't blame Apple for that.  But given my experience, you know, elsewhere and with Apple, I also have no doubt that some of this is just pettiness which, as I said, should be beneath Apple.  For what it's worth, though, I'm sure Apple is not singling out Firefox for prejudicial treatment.  They treat anything that's not Safari as suspect.



LEO:  Yeah, exactly.



STEVE:  Mozilla is also unhappy with their experience over on Google's Android platform.  There, they voice three complaints:  importing browser data on Android; some Android features launch Chrome instead of the user's default browser; and lower quality search results in third-party browser engines on Android.  I was curious to look into these three a bit further, especially the last one, which we'll get to in a second.  What I found was interesting.



In detailing their complaint about importing browser data on Android, Mozilla explained:  "Browsing information like history, bookmarked sites, and cookies is not accessible to third-party browsers on Android.  This data is kept within a web browser application's data directory, which is not directly accessible to third-party browsers, and there's no API or ContentProvider to enable it to be imported.  While this is sensitive data," Mozilla agrees, "similar import functionality is possible on all major desktop platforms, and Android is able to mediate access to other sensitive data with user consent.  Not being able to import data creates significant friction to change from Chrome.  A user should be allowed to bring their data with them to another browser."



And I think that seems like a legitimate complaint and a slippery way for Google to give Chrome an anti-competitive edge over any other browser its user might wish to switch to.  And the second issue raised, "some Android features launch Chrome instead of the user's default browser," that seems even more insidious.  Mozilla explains:  "Features like Google Search or Discover, in the pre-installed Google application, ignore the user's default browser choice.  Links to websites outside of the application are always opened in Chrome, regardless of the default browser.  This is a widely used application," they say, Mozilla says, "with additional entry points from built-in features such as the search bar on the home screen and app launcher.  Each time it opens a link in Chrome, a user is driven away from their default browser.  All built-in applications and affordances that open external links should open them in the user's default browser."



Right.  That would really annoy me, and this issue will be quite familiar to anyone who has heard Paul Thurrott ranting about Microsoft and Edge doing the same thing.



LEO:  Oh, yeah.  Edge launches, you breathe on it, it launches, yeah.



STEVE:  Yes.



LEO:  What?  Me?  You want me?  You want me?  Yeah, I'm here, yeah.



STEVE:  As Mozilla says, every time Chrome is launched when the user has installed Firefox and asked Android to use it, drives the user toward Chrome despite their clearly expressed browser preference.  And it was the third item in Mozilla's "Platform Tilt" list of grievances that most caught my eye.  They wrote:  "Lower quality search result pages in third-party browser engines on Android."  That seemed like a real antitrust showstopper.



Here's what Mozilla explained.  They said:  "The web search experience is tightly integrated with a number of built-in features in Android, and the experience provided to Firefox is inferior compared to the version provided for Chrome.  As seen in the screenshots, identical search terms show less information and receive a lower quality design in Firefox on Android."  And for anyone who's interested, I have the side-by-side screenshot in the show notes, where is it, on page six of the show notes, where indeed you can see the Chrome as a fancy-looking display, and Firefox not so much.  It's kind of got a little textual summary instead of some nicer graphics.



LEO:  You know you can't do graphics in Firefox.  It doesn't - you can only do it in a proper browser, you know.



STEVE:  Mozilla said:  "While strictly speaking this is an issue with the Google Search website, given the prominence and integration of search on Android this is a meaningful user experience gap that creates an incentive for users to not choose a third-party browser."



LEO:  They've got a really good point.  I mean, this is the same data coming from the same site, a Google site.  But they intentionally poorly render it on anything but Chrome.



STEVE:  Yes.  And you know why?  It's the user-agent header.  It turns out that Google search results are biased against non-Chrome browsers.



LEO:  That's terrible.



STEVE:  I know.  It is so wrong.  If the user-agent string is changed, then Google will provide the same improved experience to Firefox users as Chrome users.  Now, user-agent dependency is nothing new.  And once upon a time Chrome's page results rendering may have necessitated producing different results to differing browsers.  But those days, you know, those are long gone.  This sort of deliberate bias is showing Google's own extreme pettiness.  And speaking of Microsoft and Windows, well, Microsoft's own incestuous ties to its own web browser actually, as we know, have been the subject of antitrust lawsuits in the past, and big ones.



Mozilla lists three complaints about Microsoft's and Windows's treatment of Firefox, and you can imagine where this is going.  The three complaints are setting default browser on Windows; default browser is set to Edge by several Windows flows; and some Windows features launch Edge instead of the user's default browser.  It is starting to sound like a refrain.  Under "Setting default browser on Windows," Mozilla writes:  "Allowing a third-party browser to programmatically set itself as the default is an important platform feature.  Without this, even after the user has installed the browser of their choice, they must navigate operating system settings and make the choice there, as well.  This adds friction and creates inertia to continue using Edge, despite the user's obvious preference.



"A well-established design pattern is to allow the third-party browser to invoke a system prompt which permits the user to easily confirm or reject the request to set the current browser as the default.  This is an intuitive user experience that mirrors similar permissions models used in operating systems, browsers, and web applications.  Android and macOS offer such a capability.



"Unfortunately, Windows does not support anything like this for third-party browsers.  Browsers are forced to 'deep link' into the Windows settings UI.  On Windows 10, this requires several clicks and a double confirmation in the settings UI.  On Windows 11 there is a 'Set default' button.  Neither is sufficient.  Windows should instead provide a method for third-party browsers to programmatically request they be set as the default."



And to that I'll just say, yup.  This is the traditional way that we've all historically experienced the addition of a third-party browser being installed.  The browser notices that it's not currently the system's default URL handler and asks its user whether they would like it to switch them over to using this browser instead.  The user says "Yes, please" or "No, thanks," and it's done.  But no longer. Microsoft,  exhibiting the same pettiness we see from Apple and Google, clearly wishes to hold onto the use of Edge every way possible.  As I sit in front of Windows 10, I'm periodically reminded of just how much my life could be improved if only I would allow their Edge browser to service my needs.  No, thank you.



And speak of the devil, here's Mozilla's second complaint:  "In general, the Windows 10 and 11 operating systems have persistent messaging that Microsoft Edge is the 'recommended' browser for Windows, and offer affordances to change the default browser to Edge.  In some cases the wording is misleading, asking a user to adopt 'recommended browser settings,' which does not obviously suggest a default browser change.  This messaging is a moving target, with examples added and removed from Windows over time, often on UI surfaces that appear automatically on update or otherwise, making it difficult to enumerate specific examples." Right, so they've become quite slippery.



"In all cases," Mozilla says, "these Windows components are able to change the user's default browser directly, and are not forced to use ms-settings: protocol deep linking that browsers are required to use.  Windows should consume the same affordances and APIs that are available to third-party browsers for setting-to-default."  Yep.  Just another of the many reasons I am perched in front right now of my trusty, and crusty, old Windows 7 system.  You know?  I'm subjected to none of that extraneous crap.



Okay.  And lastly, Mozilla says:  "There are at least three prominent Windows features that open URLs in Microsoft Edge and not in the current default browser.  The user's default browser choice should be respected when web pages are opened by built-in operating system features.  The first is Windows Search, also known as Start Menu Search, and formerly known as Cortana.  The UI for this feature is represented by a taskbar search box or search button, depending on the user settings; and a search suggestions/results UI that appears when activated and updates as the user types.  The suggestions and results UI also appears if the user starts typing when the start menu is open, and by the WIN+S hotkey.  All links from this UI, whether they initiate web searches or link directly to articles or results, open in Microsoft Edge, regardless of the user's default browser.



"The second is the new Windows Copilot, currently only available on Windows 11, which appears as a docked window on the right side of the screen.  If Copilot produces links in its responses, or offers other links within its rendering area, these links open in Microsoft Edge, regardless of the user's default browser.



"Third, there are Windows 'widgets' which are called 'news and interests' on Windows 10, and a UI surface which can be activated by a taskbar button.  These show information like news, weather, stocks, and sports scores.  On Windows 11, new widgets can be added from third parties.  Regardless, all links to a web page from widgets will open in Microsoft Edge, regardless of the user's default browser."



So, okay.  In summary, what we have from Mozilla is highlighting and detailing pervasive pettiness.  And, you know, not playing fair on the parts of Apple, Google, and Microsoft.  It's not like they absolutely refuse to accept a default, you know, to accept a browser change.  That would probably get them in some serious hot water; right?  Instead, it's like, oh, yeah, you can install a browser.  And then they do everything they can not to really let the user use the browser, not the way they obviously could if they wanted to.  Clearly not playing fair.  So Leo, what do you think is behind this?



LEO:  It does seem...



STEVE:  Is this like gearing up for a little antitrust activity here?



LEO:  Yeah.



STEVE:  Microsoft seeing their market share dwindle?



LEO:  It's hard to explain because with Edge they had an opportunity in the early stages, and Paul Thurrott's always saying this, to say, hey, we're a better Chrome than Chrome.  We're Chrome without the privacy problems, you know, they've always knocked Google for its privacy plan.  I mean, this bugs me about Microsoft all around is that they could actually do very well by saying we're a private platform.  We respect our users.  We keep it private.  We have a better browser that's got less stuff.  Instead, they put coupon codes in there and all sorts of crazy...



STEVE:  Well, and then it's got - I've got shopping fact on - it's like, what the heck?



LEO:  Yeah.  And so is it, I mean, they don't need the money.  Except Apple doesn't need the money by charging 27% instead of 30% if Kindle uses its own Amazon link.  These companies are greedy.  I think really what's happening, and we're seeing it again and again, we're seeing it in Big Tech, but we're also seeing it in our politics, where people go, I don't care if you don't like it.  It doesn't matter.



STEVE:  Right.



LEO:  I'm going to do what I want to do.



STEVE:  You're right.



LEO:  Screw it, you know.  And the heck with the FDC and...



STEVE:  Basically abusing the power that they have.



LEO:  We're going to abuse the power.



STEVE:  They have the power.  They're going to abuse the power.



LEO:  And they no longer care about governments.  They no longer care about users.  They care about profit.  It's very disappointing.



STEVE:  Microsoft knows nobody has a choice.  They're, like, forcing people up to newer versions of Windows, and those newer versions of Windows are increasingly creating lock-in.



LEO:  The thing that's frustrating is they don't need to do this, and they could do so well for themselves if they said, no, no, it's a better Chrome than Chrome.  We're privacy focused.  We don't need to do all of that.  You own the computer.  You own the platform.  They would, I think, do better by doing that.  And so it's baffling.



STEVE:  And look at Chrome's market share.  I mean...



LEO:  Oh, they're gone, completely gone.



STEVE:  My wife has Chrome on Windows 10.



LEO:  Of course she does.



STEVE:  Because she doesn't - and she keeps complaining about this Bing, and like it keeps Binging her.  She says, "How do I stop this?"



LEO:  It's unfathomable.



STEVE:  And it's obviously not working.



LEO:  But I honestly think that these companies have gotten to the stage where they don't care, and they just don't need to.  And this is what Cory Doctorow's always talking about with [crosstalk].



STEVE:  They're too big to care.



LEO:  They're too big to care.  It's time for them to squeeze us.  Steve, it is your turn once again.



STEVE:  So, meanwhile, last Thursday, the Brave browser, which is super popular among those who are truly privacy and anti-tracking concerned...



LEO:  That's the one Paul Thurrott likes.



STEVE:  Yup, notified its users that Brave would be reducing the strength of its anti-fingerprinting protections.



LEO:  That's weird.



STEVE:  Under the heading "Brave browser simplifies its fingerprinting protections," the Brave team wrote:  "With desktop and Android version 1.64 in a couple of months, and in today's nightly release for testing, Brave will sunset Strict fingerprinting protection mode.  This does not affect Brave's industry-leading fingerprinting protection capabilities for users."  What?  "Instead, it will allow us to focus on improving privacy protections in Standard mode and avoid web compatibility issues."



Okay.  Now, they say:  "Brave will sunset Strict fingerprinting protection mode," and then immediately follow that with "This does not affect Brave's industry-leading fingerprinting protection capabilities for users."



LEO:  What?



STEVE:  I know; right.



LEO:  How does that not - what?



STEVE:  Yeah.



LEO:  You're reducing protection but it doesn't affect your protection?



STEVE:  That's right.  So...



LEO:  Huh?



STEVE:  They said:  "Brave currently offers two levels of fingerprinting protections which make it harder for tracking companies to identify you as you browse the web, Standard and Strict mode.  Over time, however, we've observed significant disadvantages of Strict mode."  Okay.  Here they are.  "First, in order to block fingerprintable APIs, Strict mode frequently causes certain websites to function incorrectly or not at all."



LEO:  Oh, okay.



STEVE:  Whoops.



LEO:  So it breaks functionality.  I can understand that, yeah.



STEVE:  "This website breakage" - yes.  "This website breakage means that Strict mode has limited utility for most web users.  Next, fewer than half a percent of Brave users are using Strict fingerprinting protection mode, based on our privacy-preserving telemetry data."  So, you know, we know that, but don't worry, we're not spying on you.  "Third, this tiny cohort of users could be more vulnerable to being fingerprinted because they stand out as a result of using Strict mode."  Which, you know, that's an unintended consequence.  "Although we've not seen issues around this, it is a valid concern given that users who use Strict fingerprinting protection might have done so because of an elevated concern about tracking."  Right.  Why else would you do it?  And "Fourth, maintaining Strict mode and debugging why some websites are broken on Brave takes our engineers' time away from focusing on default privacy protections that can benefit all of our users."



They said:  "These observations have led us to the conclusion that sunsetting Strict mode in Brave will actually be beneficial to our users' privacy."  And they explain:  "Brave's Standard fingerprinting protection is already very extensive and the strongest of any major browser.  Brave's innovative farbling of a number of major fingerprintable Web APIs makes it difficult for fingerprinters to get a reliable unique ID on your browser.  Going forward, we will continue to strengthen and expand Brave's Standard fingerprinting protections so that all our users have ever-improving protection against fingerprinters, while maintaining the highest possible level of compatibility with websites."



Okay, first of all, you did hear me use the term "farbling."  I have no idea where they came up with that, but okay.  I tracked it down, and it's Brave's term for introducing some random jitter noise into the values being returned by the Web APIs that are commonly used for fingerprinting.  Those APIs are the Canvas API, WebGL, WebGL v2, the WebGL Extensions, the contents of the browser's user-agent header, web audio, the browser's plugins, hardware concurrency, the enumeration of system devices - both their ordering in the enumeration and their labels and IDs - and the user's dark mode setting.



Since I was curious and knew our listeners would be, too, I tracked down the difference between Brave's soon-to-be-discontinued "Strict" anti-fingerprinting mode and the mode that all Brave browser users will be left with.  So here's how Brave describes the two modes.  They said:  "Brave has two levels of fingerprinting protections.  In the default 'standard' configuration, Brave adds subtle noise to APIs commonly used to fingerprinting scripts."



LEO:  Ah, fuzzing.



STEVE:  Without, yes, without breaking websites.



LEO:  Interesting.



STEVE:  "And will provide good protections against web-scale online trackers.  Brave also includes a 'strict' option.  When set to 'strict' mode, Brave only returns random values from APIs commonly used by fingerprinters.  This provides a higher level of protection against highly determined attackers, who may attempt statistical and/or targeted attacks to identify users.  This mode will also break websites who depend upon these features to work correctly."



LEO:  So it's fuzzing versus farbling, I guess you could say.



STEVE:  Well, actually, I would say that the milder "standard" mode uses, dare I say, only moderate farbling of API values which do not cause website issues because only some of the least significant bits are being farbled.



LEO:  Well, then, there you go.



STEVE:  Well, yeah.  Because what you want in your least significant bits is a little bit of farbling.



LEO:  Farbling.



STEVE:  But what strict mode does is to entirely discard the true API values and replace them with fully random values for these API calls that bear no resemblance to reality.



LEO:  Of course that's going to break stuff.



STEVE:  Yes.  My reading of this is that the original designers of Brave's anti-fingerprinting technology probably got their farble turned up too high.



LEO:  Oh, I hate it when that happens.



STEVE:  I do not want this.  You know, they probably thought, you know...



LEO:  Set farble to stun.



STEVE:  If a little farbling is good, just how great it would be...



LEO:  More is better.



STEVE:  ...if we just farbled the crap out of this.  But apparently, after gaining more experience with this, they learned that some websites became quite upset when they were over-farbled.  You never want to over-farble, but especially not on a school night.



LEO:  Oh, yes.



STEVE:  Anyway.  And actually I can see how the statistical analysis they refer to could theoretically be a problem, since over time the results from a low and safe level of farbling could be averaged out to obtain the true value around which the farbled values are clustered.  But on balance I wouldn't worry about that too much.  I think Brave is doing the best they can while not causing more trouble than the farbling is worth.  So elimination of Brave's "strict mode" sounds like a good thing.



LEO:  Actually, they made a really interesting point, which is that so few people use the strict mode, it itself could be a form of fingerprinting.



STEVE:  Yes.



LEO:  I thought that's fascinating.  You want to be in the herd.  You don't want to be the one on the outlying edges of the herd.



STEVE:  You don't want to be singled out.



LEO:  Yeah, yeah.



STEVE:  And so if they saw someone's browser that's like producing wildly bizarre values, they'll go, aha.



LEO:  We have an over-farbler.



STEVE:  We weren't sure, but this guy's over-farbling, and there aren't that many of them out there.



LEO:  Yeah.  That's really - that's actually a fascinating insight into how this stuff has to work.  You cannot de-fingerprint people by doing things that only a handful of people do because then they get identified.  It's really interesting.  Good on Brave.  You know, it sounds like they did the right thing.



STEVE:  Yep, I think so, too.  And we have added a word to our lexicon, Leo.



LEO:  They mean fuzzing; right?  I mean, that's fuzzing.



STEVE:  I could have named the episode "Never Over-Farble."



LEO:  On a school night.  You should have.



STEVE:  That's right, on a school night.



LEO:  You should have named it that.  It's not too late to change, Steve.  We can work it.



STEVE:  Okay.  So coming up on a year ago, in the middle of March 2023, I noted and was quite glad to share that CISA, our already very proactive U.S. Cybersecurity and Infrastructure Security Agency - I never thought I was going to be able to just have that roll off the tongue.  But yeah, Cybersecurity and Infrastructure Security Agency...



LEO:  Next is Roskomnadzor, and you'll be the king.



STEVE:  ...was launching an even more proactive initiative.  They called it the Ransomware Vulnerability Warning Pilot, and thank god they didn't try to make the abbreviation pronounceable, it's RVWP.  So, you know, they're not Google.



LEO:  Yeah.



STEVE:  And they described it this way.  They said, this is back in March:  "Organizations across all sectors and of all sizes are too frequently impacted by damaging ransomware incidents.  Many of these incidents are perpetrated by ransomware threat actors using known vulnerabilities.  By urgently fixing these vulnerabilities, organizations can significantly reduce their likelihood of experiencing a ransomware event.  In addition, organizations should implement other security controls as described on stopransomware.gov.



"However, most organizations may be unaware that a vulnerability used by ransomware threat actors is present on their network.  Through the Ransomware Vulnerability Warning Pilot, which started on January 30th, 2023, so coming up on a year ago, CISA is undertaking a new effort to warn critical infrastructure entities that their systems have exposed vulnerabilities that may be exploited by ransomware threat actors.



"As part of RVWP, CISA leverages existing authorities and technology to proactively identify information systems that contain security vulnerabilities commonly associated with ransomware attacks.  Once CISA identifies these affected systems, our regional cybersecurity personnel notify system owners of their security vulnerabilities, thus enabling timely mitigation before damaging intrusions can occur.



"CISA accomplishes this work by leveraging its existing services, data sources, technologies, and authorities, including CISA's Cyber Hygiene Vulnerability Scanning service and the Administrative Subpoena Authority granted to CISA under Section 2209 of the Homeland Security Act of 2002."



As our listeners know, I'm 100% behind the idea of having the good guys proactively scanning for vulnerabilities.  We know that the bad guys are.  So to the good guys, my only question would be what took you so long?  Anyway, CISA just published their 2023 Year In Review, and it contained some gratifying news of the results from the first year of this pilot program, which I hope they can remove the "pilot" from it.



During this first year, CISA sent out more than 1,200 notifications to U.S. and international organizations, notifying them of early-stage ransomware activity on their networks.  CISA also sent 1,700 notifications to organizations that had systems vulnerable to common ransomware entry vectors.  In other words, the U.S. is finally proactively scanning the public Internet for vulnerabilities.  Together, this totals an average of eight such notifications sent every day of the year last year.  And it's difficult to imagine that anyone would blow off a notification from this U.S. agency saying that they've already found evidence of an existing network intrusion or of an existing public-facing vulnerability.  So bravo, CISA.  Yay.



LEO:  I'm looking for farble.com or perhaps farbled.



STEVE:  Well, Leo, you do not want to look up farble in the Urban Dictionary.



LEO:  Oh.  Is there actually a word?  Oh.  No.



STEVE:  I made that mistake.  You will not want to know what it is to farble.



LEO:  Okay.  I will...



STEVE:  So says the Urban Dictionary, and I now know that all of our listeners are doing so.



LEO:  Oh, boy.



STEVE:  Sorry for bringing it up.



LEO:  Yeah.  Oh.  You managed to withhold that information until later in the show.  Okay.



STEVE:  So in news of a growing and necessary trend which we've been seeing recently, Samsung's just launched S24 series of smartphones will be receiving seven years of software and security updates.



LEO:  Yes.  That was good news.



STEVE:  Yes.  That's an increase from the company's previous smartphone coverage, which were five years.  So Samsung joins Google to be the only vendors to offer seven years of security updates for their Android devices.  And the best news of all is that this suggests that the longevity of security support has finally become a recognized competitive advantage.  That's nothing less than a big win for consumers and 100% great news.



Remember that we were recently talking about the lawsuit against Google over the consumers' "misunderstanding" - I'll put that in air quotes - "of the protections provided by Chrome's Incognito Mode.  Now, it turned out it wasn't as incognito as we thought.  In response to that, Google is changing the text that appears in Chrome's Incognito Mode.  The new text much more clearly informs its users that their activity will continue to be tracked, even while they're in the somewhat less than entirely incognito mode.  It now reads:  "Others who use this device won't see your activity, so you can browse more privately.  This won't change how data is collected by websites you visit and the services they use, including Google.  Downloads, bookmarks, and reading list items will be saved.  Learn more."  And then you can click if you want more information.  So anyway, they decided, uh, maybe we need to be a little more clear about that.



Oh.  I wanted to acknowledge that I received our listeners' many notes that for reasons I think I understand now the images contained within the previous two weeks of show notes were not visible to users of Apple's iOS and macOS devices.



LEO:  Oh, that's interesting.  I didn't know that.



STEVE:  Uh-huh.  It was also a coincidence that I had captioned last week's Picture of the Week "Please provide an example of irony," and that that picture was missing.



LEO:  Ironically.



STEVE:  Ironically, yes.  In any event, it appears that the trouble was that I always run the final show notes PDF, which I download from Google Docs, you know, where each document is authored, through Acrobat's PDF optimizer, from which it very nicely reduces the document size, typically from a couple megabytes down to a few hundred kilobytes.  And I don't know what may have changed since it was nothing at this end.  On the other hand, since my Acrobat...



LEO:  I can see them, though.  This is my Macintosh.  I can see them fine.



STEVE:  Last week?



LEO:  Yeah.



STEVE:  That's interesting because I got all - I verified on my - oh, wait, no.  I know why.  Because I fixed them.



LEO:  Oh.



STEVE:  Yes.



LEO:  But this is the attachment in your email.  So this would have been the thing that you sent to me.



STEVE:  That's, you're right, that would have, and that should have been broken.  I verified...



LEO:  Now, I'm viewing it in the browser, and let me see if I can view it in Apple's Preview.  Because as you know I save all of your show notes because god knows I wouldn't want to lose any of them.



STEVE:  Well, you want to feed them into some AI and have them, you know...



LEO:  Yeah, I did do that, by the way, and it was not satisfactory.



STEVE:  Oh.



LEO:  So you're safe.  No, yeah, it works.  I'm looking on my - this is on my Mac, both in Preview and in - and this is the one you attached to your email, so it's the original.



STEVE:  Well, my iOS - I have an iOS device, and indeed it did not show the pictures originally.



LEO:  Oh, iOS, not Mac.



STEVE:  Well, I was told Mac, but apparently...



LEO:  That's a Mac.  Let me look on my iOS.  Huh.  That's interesting.



STEVE:  Anyway, my Acrobat, of course, is v9, with a copyright of 2008.



LEO:  Which, by the way, should make it more compatible, not less.



STEVE:  I know, except that - so what I believe, because nothing else happened, nobody else complained, obviously you're able to see it elsewhere, I think I had it set back to version 5 PDF compatibility.  I think Apple decided to stop supporting some feature of older PDFs.



LEO:  Oh, yeah, here it is on my iPhone.  There's a blank where it should be a clear visual example of irony.  Okay.



STEVE:  Yup.



LEO:  Okay.  All right.  Yeah.  So on the Mac it's okay, but apparently on iOS they don't.



STEVE:  And I did find and fix the problem.  So all of our listeners, both retroactively for those previous two weeks and also going today and in the future, well past 999, we will have pictures.



LEO:  I'm going to open it in some other app on iOS.



STEVE:  You're really curious about this; aren't you.



LEO:  Yeah, because, well, because this is the kind of thing I'm going to get calls about down the road.



STEVE:  Oh.



LEO:  So I just want to - I want to make sure that, like if I have a workaround, that oh, well, if you open it in Google Drive, you'll be able to see it.  Then that would be, you know.  Let me see.  I'm going to upload it to Google Drive and then open it.  And no, still it's a blank.



STEVE:  Yeah.  I think it - oh, really.



LEO:  Well, wait a minute.  Let me...



STEVE:  Oh, so when displayed on an iOS device.



LEO:  Yeah.  Maybe that's it.  Let's see.  Here's the Google Drive, and I just uploaded it.  All right.  Let's see if I can read this.  Oh.  Oop.  Did you see that?



STEVE:  I did.



LEO:  That was interesting.  The thumbnail was there.  Hmm.  So even in Google Drive I'm not seeing it.  Let's do this again.  Watch this.  Yeah.  Did you see it?  Briefly it showed up.



STEVE:  Yeah.  Yep.



LEO:  Yeah.  I don't know what that means.  Oh, well.  I'll leave the detective work to your fine forum members at GRC.com.



STEVE:  It's fixed.



LEO:  All right.  Q&A time.  This is actually - we used to do this every other show; right, Steve?



STEVE:  Yeah.



LEO:  I love this.  I used to love doing this.



STEVE:  Well, listener feedback we...



LEO:  We still do some.  We still do some.  Close the loop.



STEVE:  Yeah, Closing the Loop.  So our listener said:  "Hello.  I started my journey in cybersecurity two years ago.  I've learned a lot and still have a lot to learn.  Recent news of AI-generated videos got me scared because of the future world my two-year-old son might be growing up in."



LEO:  Too late.



STEVE:  "In your opinion" - sorry?



LEO:  Too late.



STEVE:  Yeah.  "In your opinion, what would be the best solution for automatic verification of a video, image, or audio?  I'm thinking of some kind of encryption from the camera like a few episodes ago where photos are signed.  But if I remember correctly, this is flawed because anyone can buy such a camera, dig the key from the hardware, and sign fake images.  I believe this needs to be addressed as soon as possible and not like in 10 years.  AI really took off.  Who knows what might be next?"



Okay.  So through the years we've observed everything that's happening around us.  This podcast as a consequence has arrived at a number of "rules of the road," guiding principles which seem to apply.  One of those that I've occasionally marched out is quite unsatisfying, though it doesn't render it any less true.  And that is "Not all problems have good solutions," an example we were talking about last week being Internet DDoS attacks.  Like it or not, the fundamental design of the Internet has made it inherently vulnerable to spoofed bandwidth flooding attacks and other sorts of attacks.



And I believe that we have the same problem here.  When I was growing up, I was fascinated by optical illusions.  One that pops to mind is that two parallel lines can be drawn on paper.  And yep, they look perfectly parallel.  But place a series of radial lines exploding outward from a central point, and those still perfectly straight parallel lines look curved.  No matter how you try to tell yourself they are not curved, curved is what you see.  The trouble is, we're built to believe our senses, and our senses can be fooled.



The corollary rule to "Not all problems have good solutions" is that technology cannot solve all of our problems for us.  In fact, it's probably a zero-sum, with technology inadvertently creating just as many problems as it solves.  Unfortunately, I am virtually certain that this listener's two-year-old son is simply going to grow up in a very different world than we have.  It's going to be a world where the many things we were able to take for granted as being real depictions of events will simply never be the case in a world for someone born recently.



And of course there's been some of that for us.  The phrase "Oh, that image was Photoshopped," you know, that's long been a common meme.  Until now, it's been the exception.  The fact that everyone perceives that what we're about to witness is a wholesale explosion in the volume of fictitious content masquerading as authentic suggests that, if nothing else, it's going to be a self-fulfilling prophecy.



And I don't see this view as pessimistic.  I think it's realistic.  As they say, being forewarned is to be forearmed.  For those of us who have been around for a while, this seems like a big change for the worse.  We're accustomed to trusting our senses and believing what we see.  But it seems all but certain that this is a comfort future generations will simply not have.  But then neither will they miss it, since things will have always been that way for them.  We old codgers will eventually die off, grumbling, "When I was a boy..." and so forth.



LEO:  I wonder what you think.  So Canon announced this a couple of months ago, and they're doing it with a Stanford lab and a USC lab and Thomson Reuters.  It's a proof-of-concept, but the idea is using cryptographic methods they're going to embed in the image's metadata information about the image, so Canon will basically verify in the metadata this image was created with a camera.  Nikon and Leica have approached the same idea.  There is something called the Content Authenticity Initiative.  But I'm kind of with you.  Anything that these companies can come up with, bad guys will just do an end-around.  And in fact it's a false sense of security because you say, well, this has a Canon stamp.  This is Canon, so it must be real.



STEVE:  Yes, that's exactly right.  Exactly.  I mean, it is exactly analogous to the example we've always drawn of the DVD decryption keys in the DVD player.



LEO:  Yeah.



STEVE:  You know, the DVD player sitting on the consumer's shelf had to have the keys to decrypt the DVD.  It didn't take long until everybody had the keys because they were sitting in the DVD player, and they got extracted.  You know, it was going to happen.  And so Canon's camera digitally signs the photo.  And actually it's, I mean, they've done so much technology.  There's like this audit trail.  You can only use Adobe's tools to make changes, and the Adobe tool creates like a pen's metadata creating an audit trail of all the modifications that were made to the photo.



LEO:  It's got a chain of custody for the photo so you know exactly who's had it and who's touched it.



STEVE:  Yeah, I mean, you know, it's just going to make - it's trying to make money for people, and it's, again, won't be long before it's hacked and cracked, and there'll be photos that are like proven to be authentic of like the sun going supernova.



LEO:  Right.



STEVE:  It's like, wait a minute, I don't think that happened.



LEO:  Yeah.  You know, this is why I'm glad you're going past 999 because this will, of course, be announced with fanfare, and then it will be hacked, and this is how we're going to learn if it's been hacked or if it's reliable because Steve will let us know.  So I'm very grateful because this isn't going to happen in the next year and a half.  But, boy, before the next election we're going to see a lot of fake stuff floating around.



STEVE:  And I do think that to this listener's question the world has changed.



LEO:  It has.



STEVE:  The fact, I mean, the fact that you guys on MacBreak and Andy can create these astonishing images just by asking for them, that is earth-shattering.



LEO:  Yeah.



STEVE:  And it just means that people growing up now will not - they will just always have been in a world that had the Internet, which we didn't have in the beginning.



LEO:  Right.



STEVE:  That was a big change.  And they will have always had the Internet, and they will have always had a world where you can't believe the things you see.



LEO:  Yeah, yeah.



STEVE:  They'll just know that.



LEO:  And they will just not trust anything they see; right?  That's not...



STEVE:  Right.  I was reminded of that wonderful sci-fi movie, which was it, where the aliens believed - they were receiving our TV transmissions from Earth, and they had no concept that they had...



LEO:  Didn't understand fiction.



STEVE:  They had no concept of fiction.  And so they thought that they were documentaries of...



LEO:  To the moon, Alice.  Ricky.  Yeah, they thought that was real.  They didn't - they thought they were watching documentaries.  I don't remember which book that was, either, but that was a great idea for a story.



STEVE:  It was a movie.  Tim...



LEO:  Burton?



STEVE:  No, the comedian.  Boy, I'm blanking on everything.



LEO:  Oh, oh, I know what movie that is.  That was a great movie.



STEVE:  It was a fun movie.  



LEO:  Tim Allen.  It was a Star Trek movie except it wasn't.



STEVE:  Yes.



LEO:  That was actually a great movie.  What was the name of that?



STEVE:  Sigourney Weaver was there, and a bunch of others.



LEO:  Wonderful movie.



STEVE:  It was fun.



LEO:  Yeah, I forgot about that, yeah.  Well, we'll keep an eye on this.  You know, there's also some real benefits, let's point out.  We showed a video yesterday, or Sunday on TWiT, of the President of Argentina speaking at Davos.  And of course what Davos saw was simultaneous translation, which kind of took all the life out of what he was saying.  And then Alex Lindsay came up with this, a deep fake of it with true translation and his mouth moving in sync with the English language.



STEVE:  Wow.



LEO:  And spoken as if he, you know, in his own way, matching his own intonation and prosody, they call it, and it was really  much better.  So we're going to see some amazing things, some very valuable things.



STEVE:  Well, and so much of the content that Netflix has is in multiple languages.



LEO:  That's right.



STEVE:  And if you're not watching the native language of the movie, you know, your subtitles...



LEO:  And the dubbing is terrible, yeah.



STEVE:  Yeah.  And so imagine if they could run their content through that and, like, fix it.



LEO:  That's just around the corner.  I mean, literally this year kind of thing.



STEVE:  Yeah.



LEO:  Yeah.  "Galaxy Quest."  That was the name of the movie.



STEVE:  Yeah, "Galaxy Quest."  Yes, yes, yes, yes.



LEO:  Thank you, chatroom.  Thank you, Discord.



STEVE:  So Matthew Burrell said:  "Quick guidance question, please.  What three to four computer languages should I learn?  That as a kid graduates high school and looks to start a business.  That's mostly ground-up, open source, secure, from server side, that can do almost it all, et cetera, backup, database, et cetera..."



LEO:  I can't wait to hear your answer.



STEVE:  "...to web interface, basically website, login, manage clients, database, other, et cetera.  Is there a good platform to start from like Synology or something that those languages could be built on top of?  Thank you for all your knowledge, guidance, and all that you do."



Well, Matthew, I presume that you're describing yourself here.  So you're a young person who is interested in computing technology and want to create intellectual property with computers and eventually support yourself.  What you need more than anything is knowledge and experience.  I told a story many years ago that had a somewhat surprising moral.  The story was about my misadventures surrounding my construction of a sonic beam weapon which, being a high schooler at the time myself, I had named "The Portable Dog Killer."  No dogs were killed.



LEO:  It did not kill dogs.  It just chases them away.



STEVE:  That was just - that was my name.  The moral of the story was that all sorts of interesting and unexpected things transpired, but only because I was actively doing things.  I was not sitting on my butt playing videogames.  Okay, so we didn't have video games back then.  But there were still plenty of similar ways that my peers managed to burn away the seemingly endless hours of their day not learning anything, not pushing themselves, and rarely experiencing anything new.  I really didn't have any choice since I loved electronics back then as I love computers and computing today.



So if you truly love computers, being active, not passive, is the key.  Turn off the videogame that someone else created and start figuring out how to create your own stuff.  And more than anything, don't let having no idea what you're doing in the beginning stop you.  That's not where you stop.  That's where everyone starts.  So pick a language, any language.  It really doesn't matter which one.  Python is nice, general purpose, easy to get going with, lots of help available online, and it can probably take you anywhere you want to go.  Figure out how to get it to print "Hello, world!" and you'll be off and going.  Then choose another problem that's not much harder than that, and solve that one.  And so on. And before you know it, you'll be programming.  The key is start.



LEO:  Do it.  Get out and do it.



STEVE:  Do it, exactly.



LEO:  Yeah.  I'll give you a couple of specific suggestions.  I agree with you on Python.  Harvard University offers a free online version of its Introduction to Computer Science course, which is excellent, CS-50, five zero.  They update it every year.  It does, in fact, use Python as its core language.  Python's nice because it's kind of like BASIC for us.  It's not a language maybe that you would use to write production code in.  But because it's interactive and it has a REPL and you can kind of try and see what happens and stuff, it's a great language to start learning with.  There is a famous and I think very good book that you can use that's free, it's available, it's out there everywhere, called "How to Think Like a Computer Scientist."  And it is more than just Python.  It is a Python book, but it kind of teaches you about...



STEVE:  That looks great, Leo.



LEO:  ...kind of the concept.  It's really kind of a classic now.  And I think that's a really good way to start.



STEVE:  And Python is on all platforms.



LEO:  It's everywhere.



STEVE:  I mean, yeah.



LEO:  It's perfect for you, just as said, because you can start right now.  In fact, you can get a Raspberry Pi and run Python on it, and you can be doing stuff instantly, almost instantly.  So it's a very good choice.  I often tell people not to - just like you.  Don't focus on the career or the business you want to start.  Just start playing with it because ideas will come to you.  This is how you kind of get into it.  That's how we got into it is we start playing with it.  And if you really love this stuff, as soon as you start doing it, you'll go crazy.  You'll go, this is amazing.  Look what I can do.  I can make it say "Hello" a thousand times or whatever.  And you will get excited about it.



If you really get serious about learning how to - it's like, if coding is what you want to do, there's another free book that I recommend called "How to Design Programs," HTDP.  It was used for years as an introductory course at Rice and at MIT.  It uses a student version of a language called Scheme, but the language isn't important.  But they strip out all the complicated stuff so that you can just focus on concepts.  And by the time you get through that, you will really be a proficient programmer.  And then the sky's the limit.  So HTDP.  There are courses in fact at EDX.org.  EDX has CS50 for free.  And it has an excellent two-part course on how to design programs by a legendary programmer, Gregor Kiczales.  I took that, and it was a really wonderful course.  But, you know, make it - you can start simple.  Just go online, get Python, read this book.  It even works you through how to install Python and everything.



STEVE:  That's a perfect book to start with.



LEO:  Yeah.  Yeah.  It's really good.  I have a lot of opinions on this, as you might imagine.



STEVE:  Well, yeah.  And I know you've been asked through the years on your radio station.



LEO:  And I love coding.  It's so much fun.  Even if you don't do it for a living, it helps you understand how computers work.  You'll be much better at troubleshooting and using computers.  And it's a wonderful hobby.  I don't do anything serious with it anymore.  But I love doing those coding challenges and stuff.  It's just - it's like doing crossword puzzles.  It's fun.



STEVE:  Yeah.  A listener, DBloor, he said:  "Hi, Steve.  I'm a computer forensics instructor up in Canada and have been listening since Episode 20-ish and love the podcast.  Thanks for agreeing to push past 999."



LEO:  Yay.



STEVE:  "I just wanted to see if you think my hypothesis holds up for a $720-plus flashlight app.  Did this app potentially get hijacked?  I've had this free flashlight app on my Pixel phone for over a year, as it allows me to control the brightness since the stock flashlight doesn't have this option.  It's been great and simple.  I think it used to show a small banner ad occasionally, but nothing intrusive.  Today I tried to use it, and I got a pop-up video ad play for about 20 seconds before I could use the light.  I thought, okay, maybe they need some money to keep development going.  How much can the paid version be?  I really like this app, so why not chip in a bit?



"$15 U.S. per week!  You read that right, a subscription for a flashlight like 4x the price of Netflix!  $60/month or $720/year for a flashlight.  My hypotheses are, one, the app developer got compromised or hijacked, and someone is trying to scam its users of hundreds or thousands of dollars; or the developer had this in mind all along, hoping to get a handful of users subscribe, thinking a flashlight app couldn't possibly be worth more than $15, not realizing it's a subscription.  Either way, wow.  Is this what our world has come to?  Subscription-based flashlights?  Anyway, keep up the amazing work.  I recommend this podcast on all my courses.  The app is called Simple Flashlight, produced by Simple Mobile Tool, and has one  million-plus downloads."



LEO:  Wow.  Who needs a flashlight app?  Every phone does flashlights, built-in.



STEVE:  Yeah.  Except that this app allows variable brightness, which he really likes.  He didn't want it to be just...



LEO:  And it does Apples and, yeah, everything.



STEVE:  ...set up to blinding.



LEO:  You can do that with Apple, too.



STEVE:  Okay.  The fact that this app has over one million downloads and that it played a 20-second ad video, and that the app is just a once-free flashlight app, strongly suggests that its original developer, who had acquired a large user base, accepted an offer to sell the app to another party.  We covered this happening many years ago, also in the Android app store.  The developer would be conscientious and well meaning, perhaps tired of keeping an app updated and current for little to no return.  Then someone would come along and offer to buy the app from them outright.  The developer of the free app, seeing one last chance to cash in and make some money, would take the deal and turn over his developer keys to its new owner.



The new owner, a scam artist, would quickly burden the app with crap designed to make more money than the purchase price they paid to acquire the app, which was likely not very much.  The scammer would figure that all of those million-plus users would run it and generate revenue from the ad.  And just as our listener suggested, there might be a few, if only a tiny fraction, who might not be paying attention and who would inadvertently subscribe at this inflated rate.  Technically, the app's new owner had done nothing wrong, but neither is this a particularly upstanding way to generate income.



And anyway, that would be my guess is that the app changed hands, and its new owner decided, I mean, it changed hands specifically so that the new owner could squeeze its install base and just, you know, basically kill it.  And, you know, with a bazillion apps on the Play Store, who cares if there's one, as you said, Leo, one fewer flashlight app because eventually no one uses it anymore because it wants a ridiculous payment and makes you watch an ad.



LEO:  Yeah.  I mean, the fact that a guy made a flashlight app tells you that he already kind of a scammer to begin with.



STEVE:  Yeah.  Michael Garrison said:  "Hey, Steve.  I'm listening to Episode 957 about the Protected Audience API, and I have a question I'm hoping you can help me out with.  I work with small businesses who have no interest in putting ads on their site, but I'm wondering whether they can still make use of the new ad functionality.  Say a company like TWIT wants to be able to customize what shows they feature on their homepage for new visitors.  With the proposed (and abandoned) FLoC proposal, the bitmap of interests stored by the browser were available to the site itself.  So if they knew which bits represented interest in space, they could alternatively move the This Week in Space banner higher on the page, or move it below other banners.



"With the Topics API, my understanding is that wouldn't have been possible because the same requester, the ad company, would have to have seen the same browser on multiple other sites, obviously leaving first-party site owners in the dark, probably by design.  Now, with the new API, I can't find a solid answer yet on whether the site itself would be able to see what categories of interests visitors might have that are visiting it.  I assume it won't be available to the site owners; but if you know one way or the other, I'd love confirmation.  Thank you for your great work on the show.  Can't wait to see what email system you come up with for communicating in the future."



Okay.  My reading and understanding of Google's Privacy Sandbox system, which I should say subsumes both the Topics API, which is part of it, and the Protected Audience API, altogether, my reading agrees with Michael's.  One of the significant objections - I remember, Leo, you talking about this on some of the podcasts after FLoC was introduced.  One of the significant objections raised against the earlier FLoC - that was the Federated Learning of Cohorts system - was that a first time visitor to a site would be disclosing information about themselves to that site without any previous interaction.  And many privacy advocates found that to be a big step in the wrong direction.



The Privacy Sandbox is vastly more complex than FLoC, and it employs that complexity to effectively blind all of the parties so that all information flows into the user's browser, and none flows outward.  And when ads are shown, their fetch and display frames enforce a new level of inter-frame and page isolation.  The objections to FLoC taught us that websites are specifically unable to learn anything about their visitors.  That was a big privacy no-no that FLoC had, and the Privacy Sandbox enforces that privacy, even for the sites users are visiting, because that's what they want.  They want that privacy.



Guillermo Garca said:  "I have a question about the washing machine bot.  Is it safe to assume that this malware is configured to infect this specific washing machine?  In other words, is someone writing code for this model?  How can malware infect various IoT devices?  Or is there a unique one for each make and model?  Many thanks, and I'm looking forward to being part of 999 and beyond."



Okay.  So I'd suggest that the best answer to that question is kind of an all of the above.  In the generic case of scanning the Internet for potential victims, we know that there have been turnkey IP stacks sold into the embedded device market which were later found to contain critical remote code execution vulnerabilities, often in their fragmented-packet reassembly implementation, since that's an example of something that's been very easy to specify and turned out to be surprisingly difficult to implement securely.  So it would be possible to scan the Internet for any IP presence that could be compromised by such an attack that might be common to a wide variety of different IoT devices, different makes and models of vendors who had all purchased this same embedded IP stack to start with.



And then we definitely have the frequent and common case - and the washing machine might be part of that, too - of a known vulnerability having been discovered in some specific Internet-connected appliance, after which those devices are directly targeted.  And then we have the other frequent case of a patch being made to a widely popular device, and that patch being quickly reverse engineered to start an arms race to see how many devices can be compromised before each individual device's administrator has applied the patch to prevent just such remote takeover.



In the case of the LG Smart Washer, I was wondering myself last week how a remote attacker might have gotten into such a machine in the first place, if that's what indeed happened to cause that 3.5GB of upload bandwidth per day.  Any such device would be behind a NAT router that would not be admitting unknown traffic.  Now, the washing machine might support UPnP, which would allow it to open a port for incoming traffic.  But why would a washing machine need to be publicly visible?  Another means of compromise might have been entry through some other means, such as a border router vulnerability, for example, if its owner had enabled remote web administration and a problem had been found there.  Which, as we know, are not uncommon.



Then, once inside the network, a scan would likely have found everything on the LAN, and at that point a known vulnerability in that specific washing machine might have been exploited to install a bot.  Or the machine might be running a small Linux.  So a generic exploit against Linux could install a generic Linux bot, and then it would have joined a botnet.  So, you know, any or all of the above could have happened.



Roger Stenerson asks:  "Hi, Steve.  The Protected Audience API sounds interesting and promising.  However, the number one reason I use uBlock Origin and ScriptSafe is to block malvertising.  Will the Protected Audience API help in that area?  To 999-plus and beyond.  Thanks for all you do.  Best regards."



Thank you, Roger.  Okay.  So I'm pretty certain that individual ads, once selected by the web browser, will still be able to run their own scripts within their "fenced frame," Fenced Frame being the name of one of the APIs, the Fenced Frames API.  So we should not expect any added protection from malvertising.  Roger's question caused me to do a bit of digging into the related Fenced Frames API.  The fencing that's created applied the same sort of cookie and other asset stove piping that Firefox implemented quite a while ago, the idea being that, rather than all cookies and other asset storage sharing a single large database which is indexed by the domain performing the access, thus allowing, for example, an advertiser to access information stored under their domain from any website hosting one of their ads because now each first-party website - I got myself confused.  Under their domain from any website hosting one of their ads.  That's the way things have been.



What's happening now moving forward, and Google is introducing this with this Fenced Frames API, is that, as with Firefox, each first-party website has its own private database containing anything that any third party might set while at that site.  But if you go to a different site, that same third party is now setting its data in a completely separate site for that website.  So there's no longer any chance for advertisement scripts to share data across sites.



So that's what we're getting with Protected Audience API.  But I'm virtually 100% sure no explicit malware protection.  That still isn't something, I mean, Google is protecting their users against any malicious scripting, whether from ads or not.  So we have that.  But, you know, malware that presents a link that says "click here" for a special discount on your next, you know, on your car insurance renewal, well, you take a risk when you click the ad.



Defensive Computing's Michael Horowitz, who runs a number of sites, one is Defensive Computing, he also has one that's really great about router security, he says:  "Steve, regarding the hacked washing machine, if the router supports outbound firewall rules, the hacked device can be blocked from making any outbound  network connections; or, depending on the router, perhaps blocked from contacting certain IP addresses or certain domains.  Surely, he says, pfSense can do this."



Okay, now, of course pfSense will do this.  And since firewall rules can be tied to the machine's fixed Ethernet MAC address in case its IP should ever change, you could even make a firewall rule that would track the machine's IP changing to block it.  But I'm not the one with the LG Smart Washer on the 'Net, so I'm not the one with the problem.  I was more referring to the typical LG Smart Washer owner who would typically have no idea what was going on with their own network.  I'm certain that our Security Now! podcast listeners could readily block this activity.  So really that's not an issue.



But Michael added something else that I thought was interesting.  He said:  "I live in an apartment building with a laundry room in the basement.  Both the washing machines and the dryers report their status to the Internet - running, not running, or X minutes until finished."  He says:  "This was helpful in the pandemic to avoid personal contact."  And I thought that was interesting.



LEO:  With your washing machine?



STEVE:  Yeah.  Well, because it's an apartment building.  We don't know how large.



LEO:  Oh, I see, yeah, yeah, yeah, okay.



STEVE:  But it's a common tool of shared...



LEO:  So if you're washing, you go upstairs, and then it lets you know when it's done.  Yeah, yeah, that makes sense.



STEVE:  Exactly.  So anyway, I had said last week that I could not imagine why anyone would have their machines online.  This is a pretty good example.



LEO:  Yeah.



STEVE:  So if you're on like the ninth floor, and you want to know if the machines are in use before you take the elevator down to the basement and find out that, whoops, they're all busy.  So that's kind of cool.  I can certainly see a use case for that.



And finally, SKYNET tweeted:  "Hi, Steve.  Is there really no way for ISPs like Cogent to differentiate between good and bad traffic so that when a DDoS occurs they can null route only the bad traffic?  Can you explain why an ISP is not able to do this?"  Okay.  Could they?  Perhaps.  The best way to describe it is that doing so, and I'm not kidding you, is beneath them.



LEO:  It's not the kind of thing we as an ISP would do.



STEVE:  That's exactly right.  They simply cannot be bothered.



LEO:  It's just not worth it.



STEVE:  I've had about 30 years of experience with ISPs of all sizes, and I've seen that the smaller the ISP, and the closer they are to their subscriber, the more individualized service it's possible to have.



LEO:  Right.  That's why we love SonicNet.  That's exactly right.



STEVE:  Yep.  But the top-tier Internet backbone carriers like Cogent don't need to be bothered with those details, so they aren't.



LEO:  In the words of Lily Tomlin, "We don't care.  We don't have to."



STEVE:  Exactly.  If some traffic is causing their downstream equipment any trouble whatsoever, they'll simply drop that traffic as far upstream as it's possible to do so.



Now, the other change we've seen since the first early attacks is in their blockability or lack thereof.  The days of the simple ICMP, the TCP/SYN packet, or UDP reflection floods have waned a bit.  Back when spoofing source IP addresses was important to hide the traffic source, they were used.  They still exist, but they've largely been replaced by non-spoofable HTTPS query floods.  And those require highly specialized services such as those we often talk about offered by Cloudflare and others, to block.  So these attacks can no longer be selectively blocked by simple firewall rules.  So, yeah.  Some ISPs like Cloudflare can.  Lots behind, you know, who are just, you know, if you're just getting generic traffic from the Internet, you're going to get flooded, and your ISP's going to say, oh, sorry, and pull the plug on you until the flood stops.  That's the way of the world these days.



LEO:  But you could get something like Cloudflare or Amazon CloudFront in front of your IP address and protect yourself from that.



STEVE:  Yes.  Yeah.  Now, you're paying a price for the protection.



LEO:  Right.



STEVE:  So you're getting connectivity, and you're getting protection both.



LEO:  Yeah.  I think Cloudflare has a free tier.  I don't know if it would work for what you want it to.



STEVE:  They actually do have a free tier.  You're right.  And I'm not sure...



LEO:  It's very good, I think, yeah.



STEVE:  ...how much - yeah, yeah.  Okay.  And lastly, a note about SpinRite since we appear to be for the moment on the cusp of SpinRite's imminent release.  So I thought I'd update everyone very briefly.  As I planned, after last week's podcast, as I said I would, I finished the work on identifying and patching the known-buggy AMI BIOS which handled USB-connected drives.  And after some verification and testing, later in the week I posted the next incremental release of SpinRite.



LEO:  Oh, boy.



STEVE:  The overall reaction within SpinRite's testing community was jubilation since, I mean, it was - I got so much, you know, yay, since SpinRite had now lifted what I had been feeling, and I expressed last week, as my previously heavy-handed 137GB clamp on any and all USB access.  But it wasn't long before reports of new SpinRite crashing began to be seen.  People were running SpinRite on their larger drives plugged into a USB port, and SpinRite's own attempt to execute an illegal opcode capture screen began popping up.  In other words, something somewhere, a bug in the BIOS was causing the BIOS to execute an illegal opcode; and SpinRite, which traps those things, popped up a notification saying, whoops.  Something is not right here.  And it brought everything to a halt.



Well, it turned out that HP and Lenovo and other BIOSes on older machines were also being found, unfortunately by SpinRite's testers at this point, to be buggy; and they were altering main memory and causing application crashes.  SpinRite's ability to patch the flaw that we found in those AMI BIOSes to allow them to then safely work past 137GB was a fluke which just happened to work.  I couldn't believe it when Paul Farrer put some NoOps in the BIOS and suddenly it worked past 137GB.  Again, now I know why because I completely reverse-engineered that aspect of the AMI BIOS.  I see what it was doing.  I understood why it was a solid fix, as it turned out.  And I added that code to SpinRite.  SpinRite now patches the AMI BIOS, and then it works.



But in general, altering something as significant as an access size limitation would not be expected to be simple.  And it turns out that HP BIOSes are in ROM, so they're not even patchable.  Anyway, I have a new plan which I will be starting on tomorrow.  I think it's going to work.  So I'll have news of that next week.  I think I know how to solve this problem, even though it has really turned out to be a sticky wicket.  But so many people are so excited to have this ban lifted, I mean, I could just simply put the clamp back on as it has been up until last week, and we'd be safe.  But some of our listeners would be unhappy.  So I think I've figured out how to slice this thing just right, and everybody will have a win.



LEO:  And this is why, when I code, I don't write it for any general purpose computing of any kind.  There's too many things out there that can go wrong.  I just don't want to deal with that.



STEVE:  When Peter Norton had me up for lunch to Santa Monica and told me he wanted to buy SpinRite, he said:  "You know, Steve, it's the most requested feature for the users of the Norton Utilities.  They all want it.  So now I want to buy it from you."  Obviously I told him no, which was the best business decision I ever made.  He said, and we were very high in a tower in Santa Monica, and so he looked to the south, because that's where I was located, and he said:  "When I first heard about SpinRite," he says, "I thought I was going to look out there and see a big mushroom cloud because you can't do safely what you were doing."  He said:  "It can't work."  But he said:  "Somehow you did it.  You pulled it off."  So, yeah, that was 35 years ago, and I'm still pulling that off.



LEO:  Still pulling it off.  And it ain't easy, let me tell you.  But, you know, I think you probably appreciate being able to do it all by yourself, rather than have a team of people and having to kind of get that code to work with this code and all of that.  There's just too many problems.



STEVE:  Actually, I have the best of all worlds.  It's me in my little hovel, my little cave, and hundreds of testers.  We have 800 people, I think it was 487 people downloaded the most recent release.



LEO:  Nice.



STEVE:  And then ran it.  And oh, my god, is that important.  So it's just perfect.  And we've got great communication.  It's just, it's ideal.  And, boy, I can't wait to get this done and started on SpinRite 7.



LEO:  Nice.



STEVE:  Because it won't have any of these problems because it won't have any BIOS.  Thank god we're getting rid of the BIOS.



LEO:  Although that was the thing; right?  You were able to do Interrupt 18 and let the BIOS do all the hard work; right?



STEVE:  It's why SpinRite was compatible for so long.



LEO:  With everything; right.



STEVE:  Was that it was hiding behind the BIOS, and the BIOS would deal with all of these problems.



LEO:  Right, right, right.  Now you've got to do it.  Now you've got to do it.  Which is better in the long run, of course.



STEVE:  Oh, Leo, it's so fast.  It turns out that my half a terabyte per hour estimate was low.  SpinRite is doing better than that.



LEO:  Nice.  That's fantastic.



STEVE:  It's suddenly really practical.



LEO:  Can't wait.  So here's how you get it, folks.  Go to GRC.com.  Now, admittedly, you're getting 6, not 6.1.  But this way you're in, you're part of that team of people who are helping Steve make this the best product ever.  And you will get a free copy of 6.1 when it comes out.



STEVE:  And if you get it right now, you can use it to crash your machine, if you have a buggy BIOS and plug in a big drive.



LEO:  We call it the Buggy BIOS Tester.  See, I'm thinking.  You have ValiDrive and you have Buggy BIOS Tester.  It's great.  It's perfect.  Actually, ValiDrive's another reason to go there and make sure the thumb drive you're buying actually has what it says it has for storage.  And so many other things.  And those are all free.  ShieldsUP!, I mean, he does so much great work at GRC.com.



Copyright (c) 2024 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.






GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#959

DATE:		January 30, 2024

TITLE:		Stamos on "Microsoft Security"

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-959.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  What changes will the EU's soon-to-be-in-force Digital Markets Act be bringing to Apple's traditional iOS policies?  What OS is ransomware unable to infect?  What has HP done now with their printer ink policy?  How many stolen user database records will fit in 12TB?  Can't you just delete that incriminating chat stream?  Did Mercedes-Benz leave their doors unlocked?  What's the latest on ransom payments rates?  And after entertaining some questions from our terrific listeners and a long-awaited announcement from me, we're going to take a look at Alex Stamos's reaction to Microsoft's most recent security incident response.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here with lots to talk about.  Apple's response to the EU Digital Markets Act.  Yes, the browser ballot is back.  How many stolen user database records will fit in 12TB?  I'll give you a hint, it's more than the total number of people in the entire world.  And finally, Alex Stamos explains what we knew, what we suspected all along.  Microsoft has not been fully forthright about this most recent data breach.  Learn how it may affect you.  All coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 959, recorded Tuesday, January 30th, 2024:  Stamos on "Microsoft Security."



It's time for Security Now!.  Hello.  I know, you waited all week for this guy to show up.  Here he is.  All you have to do is say his name three times - Steve Gibson, Steve Gibson, Steve Gibson - and he appears.  Mr. G.



STEVE GIBSON:  I actually do have clicking one's heels together three times later in the show.



LEO:  Oh, my god.  Great minds.



STEVE:  So what do you know.  That's right.  I'm sure that explains it, Leo.  That's definitely how it happened.  Okay.  So we're here, last show of, what month is this, still January, with a bunch of stuff to talk about.  We're going to answer some questions, as we always do.  What changes will the EU's soon-to-be-in-force Digital Markets Act be bringing to Apple's traditional iOS policies?  I know that's been a big topic for many of the podcasts on TWiT recently.  There are a couple specific aspects that affect us that we'll be talking about.  What OS is ransomware unable to infect?  I could give you, not 10, not 20, not even 50 questions, or attempts to answer that.  And I think you wouldn't pick up on this interesting result.



LEO:  Except that I want to use that OS.



STEVE:  Uh, well, you will until you hear what it is.



LEO:  Oh.



STEVE:  What has HP done now with their printer ink policy?  More nonsense.  How many stolen user database records will fit into 12TB?  We have the answer.  Can't you just delete that incriminating chat stream?  Maybe not.  Did Mercedes-Benz leave their doors unlocked?  What's the latest on ransom payments rates?  And after entertaining some questions from our terrific listeners and, yes, a long-awaited announcement from me, we're going to take a look...



LEO:  What?  That was a little subtle tease.



STEVE:  I bet you can guess what it is.  That one you only need one answer for the question.



LEO:  I won't say anything.  I won't say anything.



STEVE:  We're going to take a look - all of our listeners know, too.  We're going to take a look at Alex Stamos's reaction to Microsoft's most recent security incident response.



LEO:  Oh, I can't wait.



STEVE:  Update, oh, it's fun.



LEO:  I can't wait.



STEVE:  So, yeah, it's definitely earned itself our title topic for the week.  And of course we've got a fun Picture of the Week.  So I think another great podcast for our listeners.



LEO:  I'm a big fan of Alex's.  He's been on TWiG many times.



STEVE:  Yeah.



LEO:  And TWiT.  And he's just so great.  He has a real job now.  They sold the Krebs Stamos security firm.



STEVE:  Yup.



LEO:  So he's got to work for a living.



STEVE:  Got absorbed into SentinelOne.



LEO:  Yeah.



STEVE:  Yeah, he's now Chief Trust Officer at SentinelOne.



LEO:  Great, that's great.  Oh, I can't wait to hear what he has to say.  Steve, you got a picture for me?  I haven't looked.



STEVE:  I do.



LEO:  I haven't looked.



STEVE:  This week we answer the question, how do you get hipsters to obey those "Keep Out" warnings?



LEO:  Well, now, wait a minute, let me see?  Danger.  Danger, danger.



STEVE:  And you can see we have a hipster approaching in the distance, and there's no way she's going to cross that yellow tape line because the sign makes it clear:  "Danger.  No Wi-Fi beyond this point."



LEO:  And there's also, it looks like, another one.  I wish we could read this other one.  It starts "Danger, Lost Art," but I don't know...



STEVE:  Yeah, I wondered what that other one down in the corner said.



LEO:  Something, Lost Art something In Progress is what I think it says.  That's hysterical.  So clearly this is a coffee shop with a sense of humor, I think.



STEVE:  Somebody's got a sense of humor.  It's got like sort of an SR license number at something or other.



LEO:  Yeah, that's all made up, yeah.



STEVE:  So, yeah.  And, yeah, I don't know what that is.  But anyway, I thought that was great.  And once again, thanks to our listeners for providing me with a constant flow.  I've got some in the queue that are really wonderful, too.



LEO:  This is really great.  I love it.



STEVE:  Okay.  So in perfect timing, following on from last week's discussion of Mozilla's complaints against Apple, Google, and Microsoft, we have the news, initially reported by The Verge, that Apple will be changing their browser engine policy for the EU, the European Union.  Under their headline, Apple is finally allowing full versions of Chrome and Firefox to run on the iPhone.  And I'll have a lot more to say about this after I share what The Verge said.



What they said was:  "With iOS 17.4" - and what are we on, .3 now I think, right, or .3.something.  Anyway, we all updated last week because there was a zero-day, the first one of the year that Apple had to remediate.



LEO:  It's hard to keep track.  We're on, yeah, 17.3, yeah.



STEVE:  Yeah.  We had 20 last year, and so we're at the first one here at the end of January.



LEO:  Amazing, yeah, yeah.



STEVE:  Anyway, "Apple is making a number," they wrote, "of huge changes to the way its mobile operating system works in order to comply with new regulations in the EU.  One of them is an important product shift.  For the first time, Apple is going to allow alternative browser engines to run on iOS  but only for users in the EU."



Verge says:  "Since the beginning of the App Store, Apple has allowed lots of browsers, but only one browser engine:  WebKit.  WebKit's the technology that underpins Safari, but it's far from the only engine on the market.  Google's Chrome is based on an engine called Blink, which is also part of the overall Chromium project that's used by most other browsers on the market.  Edge, Brave, Arc, Opera, and many others all use Chromium and Blink.  Mozilla's Firefox runs on its own engine, Gecko.  On iOS, though, all those browsers have been forced to run on WebKit instead."  And of course this is exactly what we were talking about last week with Mozilla's complaint, that's like all we can do is put a skin on this thing.



Anyway, "Which means," writes The Verge, "many features and extensions simply won't work anymore."  Actually, never would work.  "That changes with iOS 17.4."  But does it?  We'll see.  "Anyone building a browser, or building an in-app browser for their app, can use a non-WebKit engine if they wish."  And those are the important three words.  "Each developer will have to be authorized by Apple to switch engines 'after meeting specific criteria and committing to a number of ongoing privacy and security mitigations,' Apple said in a release announcing the change, at which point they'll get access to features like Passkeys and multiprocessing.  Apple's also adding a new choice screen to Safari so that, when you first open the browser, you'll be able to choose a different default if you wish.



"Apple is only doing this," they write, "because it is required to by the EU's new Digital Markets Act (DMA), which stipulates, among other things, that users should be allowed to uninstall preinstalled apps  including web browsers  that 'steer them to the products and services of the gatekeeper.'"  You know, meaning the platform provider.  "In this case, iOS is the gatekeeper, and WebKit and Safari are Apple's products and services.  The same section of the DMA also means Microsoft has to let people disable Bing web search and uninstall Edge, and it will cause other changes, too."  Yeah, we'll see how that goes.



"Even in its release announcing the new features, Apple makes it clear that it is not pleased:  'This change is a result of the DMA's requirements, and means that EU users will be confronted'" - oh no - "'with a list of default browsers before they have the opportunity to understand the options available to them.'"  Oh, boy.  "'The screen,' continues Apple, 'also interrupts EU users' experience the first time they open Safari intending to navigate to a webpage.'"  Oh, the horrors.  "Apple's argument for the App Store has always amounted to only Apple can provide a good, safe, happy user experience on the iPhone.  Regulators don't see it that way, however, and Apple's furious about it."



The Verge says:  "Again, these changes only apply for iPhone users in the EU.  Apple says it allows European users to venture forth, to travel without breaking their browser engine.  But it will make sure only accounts belonging to people who actually live in the EU will get these new engines."  Or at least that obstructive experience of having to choose one.  "Elsewhere in the world, you'll still be getting WebKit Chrome" - oh, yeah, right, WebKit's look - "and WebKit itself.  Apple argues, without merit or evidence," says The Verge, "that these other engines pose a security and performance risk, and that only WebKit is truly optimized and safe for iPhone users.



"In the EU, we're likely to see these revamped browsers in the App Store as soon as iOS 17.4 drops" - this is still The Verge talking because I don't think so - "as soon as iOS 17.4 drops  in March."  As in, like, you know, five weeks from now.  "Google," they write, "for one, has been working on a non-WebKit version of Chrome for at least a year."  Which is really interesting.  Makes you wonder what they knew that we didn't.  "European users are about to get a serious browser war on their iPhones."  Or maybe not.



Okay.  So before The Verge's piece ended when they suggested that users would see new browsers in March, I was already, as I was reading this the first time, shaking my head, since porting an entire browser engine to a new OS is no small task.  Putting a browser skin over the WebKit engine, which is what everyone else has done until now, is entirely different from running Chromium's Blink or Firefox's Gecko engines under iOS.



And if this will only be allowed for users having accounts based in the EU, I'm wondering why anyone really cares.  The branding skin is all anyone sees.  And I was watching some of MacBreak Weekly, your previous podcast today, Leo, and I saw your very Apple-knowledgeable co-hosts, you know, nodding their heads, like Andy was like - or and Alex, yeah.  You know I don't care what's under there.



LEO:  Right.



STEVE:  So, and again, the branding skin is all anyone sees.  And I mentioned last week I use Firefox on my iPhone and multiple iPads.  To me, it looks and acts like Firefox, and I appreciate its various features.  For example, it's possible for my login to my Mozilla account to function and then sync tabs across my various devices, including my iOS devices.  Just now, when I picked it up to double check on that, it asked me whether I wanted to pick up editing this podcast's Google document on my iPad.  So again, you know, and we know that, for example, add-ons that are Firefox add-ons have no prayer of operating there.  But it's still, you know, it's got the Firefox little logo, and it looks like Firefox.



So I presume that compatibility with the EU's DMA, their Digital Markets Act, will mean that the sort of link-stealing behavior Mozilla was complaining about, which we talked about last week, you know, which continually pulls users back to Safari, that'll probably actually have to disappear.  If Mozilla is resource constrained, as they presumably are, I would much prefer to have them keep their focus on the desktop, where it matters, and ignore this as a distraction which, after all, only applies to EU territory.  So, you know, like why would they bother to go to all the trouble of porting the true Firefox code base over to iOS for a piece of the world when it already looks like everybody has Firefox who wants to.



Okay, now, before I share the big worry that this story prompted in me, I want to share a bit more about this.  The day after The Verge's coverage, MacRumors followed up with additional coverage under their headline "Apple Further Explains iOS 17.4's New Default Browser Prompt in the EU."  So MacRumors said:  "After updating to iOS 17.4, which is currently in beta, iPhone users in the EU will be prompted to choose a default web browser when they first open Safari.  In an email today, Apple shared additional details about how this process will work.  Apple said iPhone users in the EU will be presented with a list of the 12 most popular web browsers from their country's local App Store at the time, and noted that the options will be shown in random order for every user."  You know, that's much as like ballots in the U.S. have their orders randomized in order to remove the bias of just picking the first one if you're in a hurry.



"Apple shared," as an example, "an alphabetical list of the browsers that will currently be shown in every EU country."  Okay.  So that's a long list, 12 of the most popular browsers.  So MacRumors said:  "We've elected to highlight browsers that will be shown in France, Germany, Italy, and Spain as examples."



So, okay.  You're in France.  You open Safari for the first time.  And so these are not randomized.  These are alphabetical; right?  We have the Aloha browser.  What?  I guess it's better than Sayonara.  We have Brave.



LEO:  It's hello and goodbye.  That's pretty good.



STEVE:  We've got Brave, Chrome, DuckDuckGo, Ecosia?  Okay.



LEO:  Oh, yeah, yeah, yeah.  That's a browser that is a NetZero browser, so they have a carbon neutral browser.  Ecosia, yeah.  They also have a search that's carbon neutral, yeah.



STEVE:  Meaning what?  You have to only inhale when you're using it?



LEO:  Yeah, something like that.  They probably, I mean, you know, maybe they have green network centers, or more likely they buy carbon credits.  But anyway, that's their pitch.



STEVE:  Well, okay.  Then we have Edge, we've got Firefox, we  have the Onion Browser.  I thought that was interesting.  I didn't know there was a - yeah.



LEO:  Yeah.



STEVE:  Then Opera, Private Browser Deluxe - ooh.



LEO:  Ooh.



STEVE:  Yeah.  Qwant and Safari.



LEO:  Yeah.



STEVE:  Now, okay, that's in France.  Germans, oh, they get their own list.  Aloha, some are very familiar by now, Aloha, Brave, Chrome, DuckDuckGo, Ecosia - keep inhaling - Edge, Firefox, Ivanti Web@Work, then the Onion Browser, Opera, Safari, and You.com AI Search Assistant made the top 12 over in Germany.



LEO:  Yeah.



STEVE:  Italy looks pretty much the same as Germany.  Let's see, Spain, yeah, pretty much the same as the other guys.  So anyway, sort of an overwhelming, like, what browser would you like to pretend to be using? 



LEO:  They've done this before.  This is a so-called "browser ballot."  And they made Microsoft do this for a while.



STEVE:  Wow.



LEO:  Yeah.



STEVE:  And were there many when Microsoft did this?



LEO:  Yeah, yeah.



STEVE:  Did you have to, like, you had to scroll the page in order to...



LEO:  Yeah.  Yeah.



STEVE:  Wow.



LEO:  Is Sleipnir on any of these?  Because that was the one, the Scandinavian browser that really got a lot of benefit from...



STEVE:  Well, there are 23 other countries in the EU that this change applies to.  So each country will get its own list.



LEO:  Each one will get its own.  Wow.



STEVE:  And, yeah, you may be able to find your favorite, depending on where you are.  So MacRumors says:  "It's been possible to change an iPhone's default web browser through the Settings app since iOS 14.  Apple has now gone a step further and added the default browser prompt in Safari to comply with new regulations under the EU's DMA.  In the EU, iOS 17.4 also allows web browsers to use web engines other than Apple's WebKit," which of course is of most interest to us.  And again, in March this is going to happen.



Okay.  So, okay, this clarifies a few things.  If all of these browsers are currently the top 12 most popular in each EU country's regional Apple App Store, then they're all, as we know, currently using simple skins over WebKit since that's all that's been possible until now.  That means that users will likely not initially be changing the underlying engine.  Just the skin.  They'll be prompted to proactively pick a skin.  And I imagine that a great many of them, I don't know, given a choice, will opt for Chrome, you know, since that's the browser that dominates the desktop.  Then after that, if specific browser vendors see some reason to invest in porting the Chromium or Mozilla engines over to iOS, then that might happen.



Now, the one reason I can see Google investing in a full Chrome port is that they badly need their Privacy Sandbox API to be running everywhere.  If tracking - now, that's assuming that Apple doesn't port the Privacy Sandbox API into WebKit, which I hope they do, much in the same way that I hope that Mozilla adds it to Firefox.  It's open source.  And, you know, Google would like to see this thing become a standard.  But if Apple doesn't do that, Google needs their Privacy Sandbox API to be running on iOS devices so that when users choose the Chrome skin, it's more than just skin.  And of course that can happen a ways down the road; right?



So in March people in the EU get this choice, and they say, well, what do you know, now I want to go with Chrome.  And initially - now, I should also mention that Google's been working on this port for a year, as I mentioned before.  Maybe they knew something.  So before long it looks like it will be possible to actually have real Chrome and the extended advertising user interest API over on iOS platforms.  But again, it's really difficult to imagine that Mozilla would either have the resources or would choose to spend them because this is only happening in the EU.  We don't know what the future holds for this.



I mentioned a big worry that this announcement triggered in me.  What we see here is Apple capitulating to the demands imposed by a regional legal framework.  I suppose they have no choice if they wish to continue operating in the EU.  The Verge made it clear that Apple is furious about this, but capitulating they are.  And this reminded me of the pending European eIDAS 2.0 legislation, remember, the one which intends to compel the world's web browsers and operating systems to accept, without recourse, any and all root certificates that the EU may choose to require browsers to honor.  The EU's DMA is about competition and antitrust.  Its aim is to water down Apple's vise grip on its traditional heavy-handed business practices there.



So it's not directly comparable to the eIDAS 2.0 legislation, but I do get a sinking feeling about this.  We may be - it may be a good thing that I'm no longer committed as my first and only priority to getting SpinRite finished because there may be a need to keep our browsers clean of EU-enforced certificates if our browsers when we're not in the EU try to impose those on us.  We'll see.



Okay, now, Leo.  We're going to answer the question, what operating system cannot be infected by ransomware?  Under the heading "Dodged a Bullet," we have the news...



LEO:  Okay.



STEVE:  We have the news that the third largest bank in the world, which is China's ICBC, was hit with a ransomware attack which got into and would have compromised their entire network,  except for one tiny detail.



LEO:  Running Windows 4.  I don't know.



STEVE:  Believe it or not, in this Year of Our Lord 2024...



LEO:  3.1?



STEVE:  ...the critical currency trading network used by China's ICBC bank was being run by a Novell Netware server.



LEO:  There you go.  Nice choice.  Excellent job.



STEVE:  As they say, if it's not broken.  Anyway, a Novell Netware server was so entirely alien to the ransomware, which had no idea how to infect the server or get up to any other mischief, that nothing happened.  Consequently, the bank just shrugged off the attack, cleaned some modern workstations that had succumbed, and got on with their day in this Year of the Dragon.



LEO:  Wow, Netware, wow.



STEVE:  Yeah.  Yeah.  You know, they probably have text, you know, ASCII text terminals, and they're typing, you know, wow.  I guess it wouldn't be ASCII, though, in China, would it.  It would be...



LEO:  Oh, yeah, that's right.  I don't know.  That's a good question.



STEVE:  Okay.  So the news that many of our listeners forwarded to me recently was that HP has once again been bricking their printers when those printers are found to contain third-party ink cartridges.  Here's what 9to5Mac wrote under the headline "Third-party ink cartridges brick HP printers after 'anti-virus' update."  They said:  "HP is pushing over-the-air firmware updates to its printers, bricking them if they are using third-party ink cartridges.  But don't worry, it's not a money-grab, says the company.  It's just trying to protect you from the well-known risk of viruses embedded in ink cartridges."  What?



LEO:  What?  Well-known.



STEVE:  What?  What?  "HP has long been known," they write, "for sketchy practices in its attempt to turn ink purchases into a subscription service.  If you cancel a subscription, for example, the company will immediately stop the printer using the ink you've already paid for."  In other words, disconnect from the network before you cancel your subscription.  Wow.  This is just so wrong.  



"HP's CEO Enrique Lores somehow managed to keep a straight face," they said, "while explaining to CNBC that the company was only trying to protect users from viruses which might be embedded into aftermarket ink cartridges.  He said:  'It can create issues where the printers stop working because the inks have not been designed to be used in our printers, to then create security issues.  We have seen that you can embed viruses in the cartridges, and through the cartridge, go to the printer, from the printer to the network.'"  And then it takes over the world.



Ars Technica asked several security experts, actual experts, whether this could happen.  And they said this is so far out there, it would have to be a nation-state attack on a specific individual.  You know, like who somehow got a cartridge sent to them from Russia.



LEO:  Your free cartridges are here.  Congratulations.



STEVE:  Why is the label printed in Russian?  Well, we don't know.  But trust us.  Put it in.



LEO:  It's going to be good, yeah.



STEVE:  Perfectly good ink.  So one expert said:  "Purely from a threat-modeling perspective, I'm skeptical, unless it's a nation-state doing a tailored attack."



Another expert said:  "As someone who works for a different inkjet print company, I'd say it's pretty terrible engine design if you could maliciously craft a cartridge to contain a virus."  And we're not talking about a liquid virus; right?  It's not something that you don't want to inhale.  This is like, you know, a computer virus on, like, what?  The chip that monitors the ink level?



Anyway, he said:  "The amount of information which needs to be stored on the cartridge is fairly small," like a serial number; right?  "If the data is not in the format you expect, reject it as invalid."  And as a matter of fact, HP's known to be quite good at rejecting such things.  



The last expert asked said:  "I've seen and done some truly wacky hardware stuff in my life, including hiding data in SPD EEPROMs on memory DIMMs and replacing them with microcontrollers for similar shenanigans.  So believe me when I say that this claim is wildly implausible even in a lab setting, let alone in the wild, and let alone at any scale that impacts businesses or individuals rather than selected political actors."  So these experts are recognizing that, well, you know, given enough motivation in designing a custom attack, in a lab, somehow getting a specific cartridge into someone's printer, eh, maybe.  But not like HP is protecting their customer base and the world because, you know, those cartridges.



Anyway, HP is facing a class action lawsuit - no surprise there - for deploying the bricking code without informing printer buyers of its intention to do so.  The lawsuit explains:  "This is a class action brought against HP, Inc., for requiring customers who had purchased certain brands of printers to use only HP-branded replacement ink cartridges, rather than purchasing ink replacements from its competitors.  HP accomplished this through firmware updates it distributed electronically to all registered owners of the printers, which effectively disabled the printer if the user installed a replacement ink cartridge that was not HP-branded.  In the same time period, HP raised prices on the HP-branded replacement ink cartridges.  In effect, HP used the software update to create a monopoly in the aftermarket for replacement cartridges, permitting it to raise prices without fear of being undercut by competitors."  So sometimes you get what you deserve.



LEO:  That's pretty appalling.  It's just appalling.



STEVE:  In this case, it is just over-to-top bad.



LEO:  It's greed.  



STEVE:  Yeah.  And actually we will be discussing greed a little bit later today because it seems to be a...



LEO:  Comes up a lot, yeah.



STEVE:  ...recurring topic, unfortunately.



LEO:  Yes.



STEVE:  So we have a leak that's being called MOAB because it's the Mother Of All Breaches, M-O-A-B, totaling an astounding 12TB of data contained within 26 billion (with a B) database records.  So if you were wondering whether you would ever have an actual need for that 15TB hard drive you purchased recently, well, yes.  There would be three terabytes left over after you transferred the MOAB breach onto that drive.



The super-massive leak, as it's being called, contains data from numerous previous breaches, including data from LinkedIn, Twitter, Weibo, Tencent, and other platforms' user data, making it the largest collection of stolen user data ever discovered. The data includes records from thousands of meticulously compiled and reindexed leaks, breaches, and privately sold databases.



Bob Diachenko, we've mentioned him before.  He's the guy who appears to specialize in discovering open and exposed databases online.  He was behind this discovery.  Although the owner of the database was initially unknown, an outfit known as Leak-Lookup, which is a data breach search engine, said it was the holder of the leaked dataset.  The platform posted a message on X, saying the problem behind the leak was a "firewall misconfiguration" - and how - which was fixed.



While the leaked dataset contains mostly information from past data breaches, it almost certainly holds some new data that has never been published before.  For example, the Cybernews data leak checker, which relies on data from all major data leaks, contains information from over 2,500 data breaches with 15 billion records.  But MOAB contains 26 billion records, that's an additional 11 billion records...



LEO:  That's more people than there are in the world.



STEVE:  Yeah.  There's probably some duplicates.



LEO:  Duplicates, yeah.



STEVE:  Yes.  That is organized in 3,800 folders, presumably 3,800 individual data breaches occurring through time, with each folder corresponding to a separate data breach.  While this doesn't mean that the difference between the two automatically translates to previously unpublished data, billions of new records point to a very high probability of that being the case, that there is some never-before-seen information.  Researchers believe that the owner of the MOAB has a vested interest in storing large amounts of data and, therefore, could be a malicious actor, data broker, or some service that works with large amounts of data.



The researchers said:  "The dataset is extremely dangerous as threat actors could leverage the aggregated data for a wide range of attacks, including identity theft, sophisticated phishing schemes, targeted cyberattacks, and unauthorized access to personal and sensitive accounts."  While the team identified over 26 billion records, duplicates are also likely.  However, the leaked data contains far more information than just credentials.  Most of the exposed data is sensitive and, therefore, valuable for malicious actors.



A quick run through the data tree reveals an astoundingly large number of records compiled from previous breaches.  The largest number of records, 1.4 billion, just one source, 1.4 billion, comes from Tencent QQ, a Chinese instant messaging app.  However, there are 504 million from Weibo, 360 million from MySpace, 281 million from Twitter, 258 million from Deezer, 251 million from LinkedIn, 220 million from AdultFriendFinder, 153 million from Adobe, 143 million from Canva, 101 million from VK, 86 million from Daily Motion, 69 million from Dropbox, 41 million from Telegram, and on and on down the list.



In addition to data on individuals, the leak also includes records of various government organizations in the U.S., Brazil, Germany, Philippines, Turkey, and others.  If anyone wonders where and how targeted credential stuffing attacks originate, one would need to look no further.  The database contains names and addresses, very personal information, password hashes, and in-the-clear email addresses.



So, you know, the people who discovered this are to some degree, I think, understandably hyping it up a bit.  But this is not to say that it's not a seriously worrisome collection of potentially potent data.  We should also keep in mind, however, that it is a collection of data gathered from all previous data breaches.  That means that it is aging and is no longer current, at least much of it is no longer current.  You know, no one should ever have their security breached.  But anyone who is still using "123456" as their single global password - which, fortunately, is becoming quite difficult to do any longer, thanks to password policies.  If you're still using 123456, you should not be surprised if your account were breached.  And really, no big database is required, you know, to even use - to even try using 123456 in order to get in.



So there is - we know that there's something to be said for pulling all this together and indexing it and making it rapidly searchable.  That's I think the threat, where for example who knows whether the people behind the 23andMe hack took the account data that they could see, you know, pulled from a massively large and indexed database like this to obtain a bunch of hashes which had been reversed and then used those for password spraying.  Or, speaking of spraying, the same thing could go for Microsoft.  So anyway, just interesting that we're talking about a single one-stop shop for 12TB of data.  You know, you ask what it contains, the answer is, well, what doesn't it contain?  Because apparently everything that has ever been breached, that's ever been put out on the dark web or elsewhere has been pulled together.



Federal investigators are warning companies which are under investigation that they may not and must not delete chats, and that they must arrange to preserve conversations that have taken place via business collaboration and ephemeral messaging platforms.  In dual coordinated press releases last Friday, the U.S. Department of Justice and the U.S. Federal Trade Commission announced updated language in their preservation letters and specifications, documents they send to companies which fall under federal investigation.  The new language updates evidence preservation procedures to cover modern tech stacks such as Slack, Microsoft Teams, and Signal.  I guess without that they figured that, you know, unless they were very clear what they meant by "thou shall not delete," companies could say, oh, well, we didn't know you meant that.



Companies that receive subpoenas or other legal notifications must take steps to preserve chat logs and disappearing IM messages, and any who do not will be subject to obstruction of justice charges.  The problem, of course, is that being charged with obstruction of justice might be better than revealing what they were deliberately, you know, talking about, and then chose to delete.  The Deputy Assistant Attorney General of the Justice Department's Antitrust Division said:  "These updates to our legal process will ensure that neither opposing counsel nor their clients can feign ignorance when their clients or companies choose to conduct business through ephemeral messaging."



And this updated guidance comes as the DOJ face difficulties pursuing its antitrust lawsuits against Google and Amazon.  So, you know, there were some targets they had in mind.  February last year, the DOJ accused Google of lying when it claimed it auto-suspended its chat auto-deletion feature.  In addition, the DOJ claimed that for a period of four years, Google trained employees to delete internal chats and move conversations to off-the-record platforms because it anticipated facing antitrust litigation in the near future.  Later, in November last year, the FTC accused Amazon of deleting more than two years' worth of internal Signal employee chats after the agency started a multi-state antitrust lawsuit.



I have a representative snippet of the DOJ's evidence-hiding complaint in their antitrust case against Google.  It's just a few lines.  It reads, and so this is from the federal complaint, says:  "The newly produced chats reveal a company-wide culture" - speaking of Google - "of concealment coming from the very top, including CEO Sundar Pichai, who is a custodian in this case.  In one chat, Mr. Pichai began discussing a substantive topic, and then immediately wrote 'also can we change the setting of this group to history off.'  Then nine seconds later, Mr. Pichai apparently attempted unsuccessfully to delete this incriminating message."



And there's a reference to a piece of evidence where there's like a serial number, but it does have the words GOOG-PLAY.  So one wonders what they're talking about there.  The complaint continues:  "When asked under oath about the attempted deletion of the message, Mr. Pichai had no explanation, testifying 'I definitely don't know' and 'I don't recall.'



"Like Mr. Pichai, other key Google employees, including those in leadership roles, routinely opted to move from history-on rooms to history-off chats to hold sensitive conversations, even though they knew they were subject to legal holds."  Meaning after they had been told they need to retain all records.  This thing says:  "Indeed, they did so even when discussing topics they knew were covered by the litigation holds in order to avoid leaving a record that could be produced in litigation.  As the examples below make clear, Google destroyed innumerable chats with the intent to deprive Plaintiffs" - meaning the federal government - "and other litigants of the use of these documents in litigation."



Okay.  So the federal government is making it very clear that digital recordings of private conversations may not be deleted from the moment of notification of pending litigation.  If executives wish to hold private off-the-record conversations, they're going to need to do it the old-fashioned way, face to face in a private setting with no one recording.  And, you know, Leo, it sort of begs the question, too, what if you use a system where one of its features is not to record long-term history?  



LEO:  I guess you're not allowed to, is the point.



STEVE:  Right.



LEO:  But are you allowed to then have a private - you always said take your clothes off, go in the middle of a field, and have a [crosstalk]...



STEVE:  And throw a big thick comforter, a blanket over yourself, yeah.



LEO:  I've been watching an old movie called "The Yards," with Joaquin Phoenix.  And he was meeting with - he's kind of a mobster meeting with a city councilman, or no, I guess it was a borough president.  And the borough president made him take off all his clothes, and then he took off all his clothes, before they had a conversation, to make sure that they weren't a wire.  So there's a longstanding precedent for this, I guess.  I don't - it's pretty funny.



STEVE:  It's the only way to be sure.



LEO:  But I'm guessing that they can't say you may not have any conversations in person.  The court can't say that.



STEVE:  Right, right.



LEO:  But they can say you may not use any technological means that don't leave a paper trail.  Right?  I guess they can.



STEVE:  I think that must be what they can say, yeah.



LEO:  Yeah, yeah.  Isn't that interesting, yeah.



STEVE:  Wow.  Yeah.  Okay.  So here's one that'll really, well, really ruined Mercedes's day.  Mercedes-Benz accidentally exposed, putting it mildly would be a trove of internal data.



LEO:  Not a MOAB, but a trove.



STEVE:  It's not a MOAB, no.  It's a trove.  By leaving a private key online that gave unrestricted access to the company's source code.



LEO:  Yikes.



STEVE:  And that key was there, exposed, for more than 90 days, almost 120 days, before it was discovered and responsibly reported by a cofounder and the Chief Technology Officer for the London-based group RedHunt, as in "Hunt for Red October," RedHunt Labs.



LEO:  Let me guess.  They pushed it into a git and published it.



STEVE:  Uh-huh.



LEO:  Did they really?  Oh, that's hysterical.



STEVE:  Yep.  What RedHunt discovered during a...



LEO:  [Crosstalk] by accident, I've got to say.



STEVE:  Yes, yes.  What they discovered during a routine Internet data scan earlier this month, earlier January, was a Mercedes employee's authentication token sitting in a public GitHub repository.  This token served as an alternative to using a password for authenticating to GitHub.  As such, it would and did grant anyone full access to Mercedes's GitHub Enterprise Server, which would in turn allow the download of the company's entire collection of private source code repositories.



RedHunt said that the GitHub token gave "unrestricted" and "unmonitored" access to the entire source code hosted at the internal GitHub Enterprise Server.  The repositories - now, here's where it goes from bad to worse.  The repositories include a large amount, shy of a MOAB, I agree, but still, of intellectual property.  Get this:  connection strings, cloud access keys, blueprints, design documents, single sign-on passwords, API keys, and other critical internal information.  RedHunt provided evidence that the exposed repositories contained Microsoft Azure and Amazon Web Services keys, a SQL database, and Mercedes source code.  It's not known if any customer data was contained within the repositories.



LEO:  This is why we're glad we drive BMWs.



STEVE:  Oh.



LEO:  Wow.



STEVE:  Wow.



LEO:  But it's so easy to do.  You go git add, and then you push it, and it's all there.



STEVE:  Yes.  Last Monday TechCrunch, serving as a middleman for RedHunt, disclosed the security issue to Mercedes.  On Wednesday, a Mercedes spokesperson confirmed that the company "revoked the respective API token and removed the public repository immediately."  They said also:  "We can confirm that internal source code was published on a public GitHub repository by human error.  The security of our organization, products, and services is one of our top priorities.  We will continue to analyze this case according to our normal processes.  Depending on this, we'll implement remedial measures."



Now, since the exposed key was first published last September, it sat there through the balance of September, all of October, all of November, all of December, and most of January of this year.  What's not known is whether anyone besides RedHunt may have discovered and taken advantage of the exposed key.  And, you know, weren't forthcoming as RedHunt immediately was. Mercedes declined to say whether it is aware of any third-party access to the exposed data or whether the company has the technical ability, such as through access logs, to determine if there was any improper access to its data repositories.  The spokesperson cited unspecified security reasons.  Uh-huh.



We've previously, of course, and to your point, Leo, covered that GitHub has begun proactively scanning repositories for these sorts of inadvertent disclosures when they recognize them.  But, you know, they don't know everybody's format of anything that they would consider sensitive.  So they're doing the best job they can.



LEO:  It's so easy to do.



STEVE:  Yes, it is.



LEO:  I guess the question is why are companies using GitHub?



STEVE:  Yes.



LEO:  Instead of their own Git servers.



STEVE:  Yes.  You know, our software and intellectual property management systems have become so complex and interdependent that they have also become brittle to these sorts of human errors.  And I don't see that changing; you know?  We're moving in this direction.  And they provide a great deal of power and flexibility and leverage.  But as you said, boy, if you make a mistake, it also amplifies the mistake just as much as it amplifies the power that it provides when it's all working correctly.  Wow.



The good news is fewer ransoms are being paid.  The number of ransomware victims who opted to pay ransoms fell to an all-time low by the end of last year.  The cybersecurity firm Coveware -  we've talked about them before, they track these things -  estimates that only 29% of victims paid ransoms during the fourth quarter of 2023.  That's down from 85% who were choosing to pay back when we began talking about this and when they started tracking it, which was the first quarter of 2019, you know, so four years ago when this all began to really ramp up.  So 85% initially.  Now we're down to 29%.  So that's great.



Coveware attributes the fall to improved data backup and recovery strategies in corporate environments, and companies getting smarter about not trusting empty promises made by ransomware groups.  So it's like, yeah, I mean, here we are years downstream.  Not only does the CIO absolutely know about this, but there's no way that the CEOs and COOs in these organizations are not all aware of the threat posed and somewhere along the way said, you know, to the CIO, you know, what resources do you need that you don't have?  If we get hit by ransomware, we don't want to be taken down.  So the world has changed.  That's good.



Okay.  I have some feedback from our listeners.  Conradical, he tweeted:  "Steve, please take a deeper dive into the technology behind verified camera images.  My gut reaction," he writes, "is you've overlooked something because public key cryptography should allow the images to be verifiable and unmodifiable."



Okay.  There are without a doubt many amazing things that public key crypto can do.  And I am deeply enamored of them.  That's what I built the whole SQRL system around.  It was all public key crypto based.  But in the case of the verified camera images, you must ask yourself what could a camera contain that cannot be copied by someone who gets their clutches on such a camera?  I contend that anything a camera can know, someone can find a way to pry out of that camera to duplicate whatever it knows; and, in doing so, duplicate its ability to make a strong assertion of an image's origin.  In other words, this entire system depends upon the camera, which is out in public, being able to keep a secret.  And everything we know tells us that's almost certainly not possible if someone is sufficiently motivated.



The most common application we have today of public key crypto is the dynamic creation of secure connections to remote web servers, where those servers are asserting their identity.  Only one thing allows that system to work, which is that those servers are not accessible to others.  If they were, the secrets they're protecting could be stolen, and others could impersonate them.  That's the difference in the security model of the camera versus a remote server.  It's the remoteness of the server that allows it to protect its secrets, the fact that it can only be accessed through a carefully managed TCP connection.  The infamous Heartbleed vulnerability demonstrated what would happen if server secrets could be accessed through a side channel.  The server's secrets would be compromised.



So it's not that the public key crypto doesn't still require secrets.  It does.  It's just that only one side of the transaction needs to be able to keep something secret.  Unfortunately, when a camera is signing the pictures it takes, it's the private key that the camera is using to perform the signing that needs to be kept secret.  Building a state-of-the-art hardware security module into the camera, which is I'm sure what they've done, will likely make it as difficult as possible to extract the HSM's key.  The unanswered question is, will it be difficult enough?  And only time will tell.



LEO:  So this is one of the first cameras to do this.  This is the Leica M11-P, which is supporting the content authenticity initiative.  



STEVE:  Yup.



LEO:  Launched by Adobe, Twitter, and The New York Times.  And it's exactly as you say.  And by the way, it may be that it is difficult to do that, and that's why it's such - this is a $10,000 camera - why it's, you know, starting here is that they probably did build in a Secure Enclave.  I mean, they must have; right?



STEVE:  Oh, yeah.  It's got, I mean, I'm sure they went to every length they could to keep anybody from ever extracting its private key.  



LEO:  And the thing of course that happens is you can strip it out.  You can get a JPG of the image that doesn't contain it.  What's interesting about this, though, I think it's kind of cool, is it's signed when you take the picture.  And it's a form of metadata, but it's not in the Exif.  It's signed.  And then when you modify anything, that's also recorded when you modify it in Lightroom.



STEVE:  Yep.  Yep.



LEO:  So there's a chain of custody, which I think is very interesting.



STEVE:  Yeah.  The Adobe software maintains a complete audit trail of any changes that are made to the image.  So you always have those, and you're able to rewind it all the way back.



LEO:  So I think the main point I guess is that you'll certainly see images that don't have these credentials.  But if you see an image that has the credentials, you're supposedly going to be able to say, oh, I see who took this, and I see how it was modified.  Is this an actual recording of something that really happened?



STEVE:  Right.



LEO:  And, yeah, I guess the only way that could be forged is they'd have to get the camera and somehow get the credentials out of the camera.



STEVE:  Well, yeah.  And that's just it is that everybody, you know, these cameras will be floating around.  Now, maybe, and it's probably the case, that every camera has a unique private key.



LEO:  I think that's the case, yes.  



STEVE:  So it would be, you know, this camera signed this, you know, alleging that it signed this image.  So what they could do is make any extraction of the key a destructive process.  You know, so the point being that the only way to really pull off a spoof would be to arrange to extract the key and leave the camera still intact, and its owner not knowing that anything had been done.  But in general the problem is we're being, you know, the idea is that it's being put forth as a means of detecting any spoofing of the image.  And like so the images signed with the CAI system are trusted at a higher level.  



LEO:  Yeah.



STEVE:  And, you know, that will be true until I'm sharing a piece of research a couple years from now from the guys from the University...



LEO:  DVD John.  Yeah, yeah.



STEVE:  The University of the Negev, you know, who were able to hear a conversation across the quad by bouncing a laser off of a plant leaf.  Those guys are going to say, oh, yeah, you know, unfortunately there's a side channel attack that's available.  We were able to aim our sniffer at it while we took a picture, and now we know what the key is.



LEO:  Steve, you do, I mean, you trust the - like a credential from an iPhone; right?  I mean, that's a similar thing with a Secure Enclave and so forth.  That hasn't been cracked; right?



STEVE:  Yeah.



LEO:  So I think it's similar to this.  Anyway, you've given a good excuse to buy this $10,000 camera and report back to you.  So I will.  I'll just pick one up, and...



STEVE:  Leo, you need one.  It is crucial...



LEO:  It's now tax deductible, thanks to you.



STEVE:  It's crucial that the pictures you take of Lisa's birthday party be authenticated.



LEO:  Authentic.  Only...



STEVE:  Absolutely.



LEO:  Yes.  And unmodified.  Okay.  So, yeah.



STEVE:  Okay.  Anyway...



LEO:  Now I have to get - I've got your approval.  I just have to get Lisa's now, and we're set.



STEVE:  That's right.  Good luck with that.  So...



LEO:  It's for work, honey.



STEVE:  Jg1212G, he tweeted:  "Hi, Steve.  I was just listening to Security Now! and got hooked into the $15 per week flashlight story.  I had to look into it.  I found it on the Play Store and followed the link to their website, simplemobiletools.com."  He said:  "I thought, very strange, the site says open source and free.  So I clicked on the GitHub link at the bottom.  Sure enough, it's open source.  So I looked at the developer's page on GitHub."  And that's github.com/tibbi.  He says:  "Wow.  His graph shows he was extremely active up until the end of October 2023, then completely stopped."  He says:  "That's so strange.  I would really like to know what happened to him.  If you hear any news, please let us know.  I love a good mystery.  Thanks, Jason."



Jason, ask and you shall receive.  Our listener megascrapper brings an end to the mystery.  Megascrapper, tweeting from @megascrapper, says:  "Hi, Steve.  I'd like to follow up on last week's listener feedback about the absurd subscription prices for a flashlight app.  I was made aware of the entire Simple Mobile Tools suite, which includes Simple Flashlight, after watching a video by Brodie Robertson."  And we have a link in the show notes for anyone who's curious.



"Unfortunately," he writes, "what happened with Simple Flashlight was exactly what you presumed in your reply to that listener last week.  With very little notice, the owner/primary maintainer of the app sold the entire suite to an Israeli publisher, ZipoApps" - good old Zipo - "which is notorious for the practice of acquiring existing apps and slapping on an outrageously expensive subscription plan.



"But not all hope is lost," he says.  "The entire suite is open source, GPLv3 licensed.  And one of the maintainers already forked it under a project called Fossify, including Simple Flashlight.  It seems to be still in early development, and I can't find the app on Google Play Store, but keep an eye out when it gets released.  Thank you very much for your work.  Look forward to 999 and beyond."



So, thank you, megascrapper, for your follow-up on this and for the confirmation that this is the sort of thing that happens with highly popular apps in the Google Play Store.  The description for the video that he linked says:  "I was a fan of the Simple Mobile Tools suite for a really long time, and then out of nowhere the developer Tibor Kaputa just sold the entire project and ran away with the bag."



LEO:  Would you say it's "kaputa"?



STEVE:  Kaputa, that's very good, Leo.  Yes.  Wow.  So anyway, we would agree it's certainly Tibor's right to do whatever he wanted to with his own intellectual property.  It's clear that since the entire project is open source, it was his project's developer keys that was of actual value because they allowed its purchaser to take over the official popular app and then upgrade it into the existing channel of owners.



And, boy, this caught everybody's attention.  Jon Dagle, he tweeted:  "Hey, Steve.  In response to the Flashlight app story.  First, to access the flashlight brightness, swipe down from the top right to get Control Center."  Okay, so now he's talking about iOS rather than Android.  He says:  "Long-press the flashlight icon.  Solved."  Okay.  I tried it on my iPhone, and I was amazed.



LEO:  I tried to tell you this last week.



STEVE:  You did tell me.  But I didn't know you meant iPhone.  It has, Leo, it has four levels of brightness.  I never knew.



LEO:  Yeah.



STEVE:  Now, this is super useful to me since the flashlight defaults...



LEO:  Really bright, yes.



STEVE:  ...to a setting that should be labeled "Visible from Orbit."  All I want to do is I want to read the menu in a darkened restaurant.  I'm not trying to signal aliens for pickup.  So I immediately set it to its lowest level, which will be much more appropriate in the future, and it won't blind my fellow diners if I inadvertently pass its laser beacon across their vision.



Okay.  Before I get to Jon's second point, I just want to mention something that's quite annoying.



LEO:  Yes.



STEVE:  I have this dull sense that there is vastly more available from today's iPhone than I'm aware of.  You know?  But how would I ever discover this on my own?  I guess I just have to sit around and press on everything to see if anything happens, which is annoying.  The original concept of the graphical user interface was that it was discoverable.  You know, that's what, you know, what was so cool about it was that you had nested drop-down menus running along the top of the screen.  Unlike the text command interface that preceded them, you could sit down and run the mouse around the screen and find everything that you might need.  Today, it's easy to do the basic things with a phone, but it's annoying to imagine just how much more remains hidden behind the need, you know, and here's where I said at the top of the show, you need to click your heels together three times in order to discover something.  How would you know?  Just as Dorothy had no idea how to get home to Kansas.



Jon's second comment:  "I recently ran across another long-time trusted app that was sold.  It's the super excellent Network Toolbox on iOS.  I think it's been mentioned on Security Now! in the past.  It has a host of powerful networking tools.  But the longtime developer sold the app sometime late in 2023.  When I first opened the app after recently resetting all settings, I got the 'Network Toolbox wants to track you across websites' alert."  He said:  "The sale/transfer was all silent as far as I'm aware.  Considering the app has a lot of sensitive functions, the trustworthiness of the developer is rather important.  So beware."



And all of these stories got me to thinking that perhaps Google and Apple really ought to consider adding proactive notification to apps when their ownership changes hands.



LEO:  Oh, I like that, yes.



STEVE:  Yeah.  I've never participated in such a transfer, so I don't have any clear sense for whether a developer might simply turn over their entire online identity to a third-party purchaser, or whether there's some more formal and controlled process for doing so.  But if it's knowable to Google or Apple, it would seem useful to add a bit of friction and visibility to this otherwise very slippery and transparent process.  This is,  you know, there's a lot of trust that's built up over time.  So if that publisher changes, it seems to me that those being asked to trust someone new should know.



LEO:  Yeah.  And this is a big issue, as you mentioned, in Google Chrome extensions, where this seems to happen a lot.



STEVE:  Right.



LEO:  So, yeah.  I think, I mean, I think with the Apple thing you kind of have - you have to have a developer count, so they should know.



STEVE:  Right.



LEO:  That should be clear, I think.



STEVE:  If it moves to a different developer.



LEO:  Yeah, yeah, yeah.



STEVE:  Right.  Brian Doyle tweeted, well, he brings us another example of Mozilla which they can add to their growing list of "good luck with that" grievances against the tactics being employed by those who wish to use their own platforms to their competitive advantage.  He tweeted:  "Hi, Steve.  I came across this message while looking for a way to save full web pages into OneNote, and had to laugh at Microsoft implying that Firefox is not a 'modern' browser.  Thought you might enjoy.  Here is the original site."  So, and I went there, too, and grabbed a screenshot.  It's onenote.com/clipper, C-L-I-P-P-E-R.  And if you go there in Firefox, up comes a state-of-the-art-looking website, and it says right up at the top of the screen:  "OneNote Web Clipper is no longer supported on Firefox browser and works best using a modern browser like Microsoft Edge."



LEO:  Oh, boy.  Oh, boy.



STEVE:  Yeah, not that stinky old Firefox.



LEO:  Oh, yes, so out of date, Firefox.  Oh, boy.



STEVE:  Yeah, uh-huh.  That's right.



LEO:  Come on.  Geez.



STEVE:  So, wow.



LEO:  Sad.



STEVE:  Just, you know, cheap shot.  It really is. 



LEO:  It is a cheap shot.



STEVE:  Yeah.  Someone whose Twitter handle I didn't really get until I said it phonetically is ShipRkt, clearly Shipwrecked.  He said:  "Hello, Steve.  I hope you don't mind me sending you a message."  No, that's Twitter.  "Could you discuss on a future Security Now! episode why Credit Karma is storing over 1GB of data on my iPhone?"



LEO:  Yikes.



STEVE:  "What on earth" - yeah.  "What on earth uses that much data for a credit app?  Thanks for your time."  Okay.  So as I said, I don't mind receiving messages, which is why I go in search of them every week for the podcast.  But neither do I have any idea why the Credit Karma app might be storing over 1GB of data on anyone's iPhone.  One thought I had was to wonder whether this 1GB might include the app itself in that total.  One of the sad trends we see is applications becoming increasingly and, in fact, obscenely bloated.  They evidence no respect whatsoever for the user of their apps.  I'm sure that very few consumers are even aware of this, which is why there's little cost associated, you know, reputational cost associated with being so careless with the consumption of other people's storage.



Anyway.  Curmudgeon rant off.  I would love to put this question to our listeners, who we know are quite resourceful.  I poked around a bit, but I didn't find anything obvious about Credit Karma's iOS app resource consumption.  So if anything finds anything, I'd be happy to share it.



LEO:  I would just, I mean, you can just delete it.  When you delete something from the iPhone it says "Do you want to delete the associated data?"  You say yes.  Install it again, login again, and I don't think you've lost anything from Credit Karma.  It may be that it's recording every transaction you make, and that's added up over time.  But I think starting over probably wouldn't hurt.  They should be storing that on their servers, not yours, personally.



STEVE:  That's a very good point.  Mark Guy, and it looks like from his Twitter handle, it's @SDTwitGuy, he appears to be a fan of the network.



LEO:  Or Twitter.  A lot of people call Twitter "Twit," which bugs the hell out of me.



STEVE:  Oh, that's a very good point.  But he is also listening to Security Now!.  So he said:  "I heard your comment about staying on Windows 7 on the 1/23/24 podcast last week."  He said:  "My main system is Windows 7 Ultimate.  They'll have to pry it out of my cold, dead hands."  He said:  "It's stable.  It runs perfectly.  I subscribe to 0patch and still get updates for MS Security Essentials, plus I use Malwarebytes Premium.  Never had any problems.  Plus I know where everything is.  I bought a used Windows 10 laptop, and I can barely find anything.  I also am an avid fan of Windows Media Center.  Nothing else comes close to its functionality.  It's how I watch and record TV, so I will never update my system."



LEO:  Yeah, because it will remove it if you do.  So he's probably right to stick with 7, yeah.



STEVE:  Yeah.  He said:  "I'm also a huge sci-fi fan, and I LOVE [in all caps] that you and Leo talk about your fave sci-fi authors, books, and series.  Thank you."



Okay.  So I wanted to mention two things from Mark.  I know that Mark and I are far from alone among the listeners of this podcast.  Just like with that Novell Netware server, it's working, so don't mess with it.  And yes, at some point I'll rebuild my machine around Windows 10.  Since I'm an MSDN developer I could still actually register a new machine as Windows 7.  But I'm not totally insane.  I'm typing this into a Windows 7 workstation mostly because moving to Windows 10 would take a non-zero amount of time.  And like Mark and many of our listeners, why bother when this 64-bit edition of Windows 7 is working fine for me.



The second thing I wanted to mention follows from Mark's comment where he said "I'm also a huge sci-fi fan" and love it that we talk about this stuff.  I've been intending to mention that after investing in about six of those Aeon 14 novels, you know, the ones invariably featuring voluptuous, heavily armed female commandos on their covers, despite the fact that another hundred or so of those novels remain, I had finally reached my limit.



Following a number of recommendations, I gave the "Expeditionary Force" novels a try, but they just didn't grab me.  They're written in a first-person narrative style that just didn't work for me, and I kept waiting for something to happen.  Now, my trouble might be that they're a bit too realistic, and not that much actually happens in life.  Once you've read much of Peter Hamilton's work, you're somewhat cut loose from the need for an excess of reality.  Which, you know, never kept Peter from telling a story.



But in the meantime, Ryk Brown, spelled R-Y-K Brown, the prodigious author of the Frontiers Saga series, had dropped a few more books in his third of five planned 15-book story arcs.  Since we're up to book 10 in arc #3, we've passed the halfway point.  You know, 75 books is what he's got planned.  I've turned a number of my very close friends and family members onto this series, and I have been unable to shake them loose.  They want nothing to do with anything else.  They just want more Ryk Brown.  As we know, I've wandered around while I've been waiting for more.  I happily consumed the entire Silver Ships series following another recommendation from a listener, and of course some of those Aeon 14 series.



Anyway, I'm bringing all this up because Ryk Brown's writing style, his deep characterization, his perfect management of a large and growing number of very different and distinct characters, and the fact that you never need to wait long for some action, continues after 40 books, which is how many I've consumed, to be absolutely enjoyable and gratifying.  All the books are available under Amazon's Kindle Unlimited plan and as audio books from Amazon.  Through the 19 years of this podcast we've shared our discoveries of many terrific books.  For sheer solid entertainment value, I think this series deserves everyone's attention.  So I just wanted to be sure that, again, it was on everyone's radar.



A couple last things.  Dizzle Von Dazzle tweeted:  "Quick question."  Yes, Mr. Dazzle.  Dizzle von Dazzle.  He said:  "As you are an avid user of Windows 7, how do I continue to use websites that use HSTS?"  You know, that's HTTP Secure Transfer Security.  STS.  I've forgotten what it's - I know what it is.  I forgot the abbreviation.



LEO:  Yeah, something like that, yeah.



STEVE:  HSTS.



LEO:  Yeah.



STEVE:  Secure, uh, anyway, someone will tell us.  Anyway, he said:  "It's a new install on an old-ish Lenovo IdeaPad All in One."  So he recently installed Windows 7.  He says:  "Is there a way to update the SSL libraries, as none of the update managers for different music production applications I own seem to work either."  He finishes:  "Keep up the amazing work on SpinRite, and here's to episode 999."



Okay.  I'm having no trouble with Windows 7 and HSTS sites, such as GRC, which was one of the earliest to adopt HSTS, even though I've forgotten what it stands for, and its permanent registration in Chrome, which GRC also was an early adopter of.  Under my Win7 setup notes, I have a subdirectory named "Before registering or installing Win7 updates," and that subdirectory contains three specific Microsoft updates.  There's an SHA-256 update, a Servicing Stack update, and an update which is KB3102810.



From my notes, it appears that you should find those three individual standalone updates and install them in that order.  Then you can successfully bring Windows 7 current, and all should be well.  So the fact is Windows 7, which was first published in 2008, yes, it's showing its age.  It didn't support signing things, things being signed with SHA-256.  It only knew SHA-1.  And so that would explain why he's unable to update his other, what was it, different music production applications.  Their updating systems are probably using SHA-256, which Windows 7 does not support out of the box.  You need to install the SHA-256 update for Windows 7.  Then it will probably work.  And are we finally - almost.



LEO:  A couple more.



STEVE:  Two left.  A listener who asked to remain anonymous said:  "Steve, my company is switching to Bitwarden from LastPass as a result of me raising the issue a year ago, which is a result of your discussion on the podcast.  Please keep this anonymous if you mention it on the SN podcast.  My question is, can I get a readout from you on the advisability of adding TOTP" - you know, Time-based One-Time Password - "codes and secrets into Bitwarden so that it can fill in the field on sites you're logging into?"  He says:  "Personally, it gives me a 'Gibsonian response' and feels like all your eggs are in one basket if you do that.  What do you think?  Regards, long-time listener, et cetera."



Okay.  So we've mentioned before about this, but it's worth just covering again.  And I know that, Leo, you concur since I've heard you say the same thing on other podcasts.  But I've also had some time to think about this and perhaps to mellow about it a bit.  I understand the convenience.  But it's also true that it represents a classic tradeoff between convenience and maximum security.  My honest feeling is that the actual risk of having all the eggs in one basket is likely less significant than the benefit that comes from ease of use.  That is, the risk is less of a concern than the benefit it provides in terms of ease of use.  So if, for example, it was ever a matter of not registering and using a Time-based One-Time Password due to the inconvenience of needing to use a second authentication device, which would be more secure - and, for example, last week Paul Thurrott was explaining that his wife has absolutely zero interest in anything that gets in her way  then yes, it would be better to have Bitwarden able to automatically fill-in the one-time password field than not to use time-based multi-factor authentication at all.



I have no problem keeping my one-time password tokens in my iPhone and in manually transcribing them.  I don't have to do it that often, and I really appreciate the real, the true sense of security I get from that.  But that's just me.  So better to use any one-time password than none, even if it's being automatically filled in by the browser.  And if given a choice, it's better not to have the browser filling it in, even though the actual danger, I think, is realistically small.  You know, Bitwarden wouldn't have done it if it was really a big problem.  They did it because it's like, okay, you know, you still get all these benefits from a one-time password.  Why burden the user if we can do it for them?



LEO:  If there were some way they could like separate, like keep the TOTP secrets in a vault somewhere on a different country from where the, you know, in case there's a breach, that's the fear; right?  In the LastPass breach, if somebody had gotten both LastPass password vault and the TOTP secrets, which many people did use in LastPass, then you would not be quite as secure.  So if there could be, you know, you can with Bitwarden, in a personal account, anyway, store your vault yourself.  Maybe if you just stored - I wonder if you could separate the TOTP database out.  I'll have to look into that.  But you're right.  You know what?



STEVE:  Exactly.



LEO:  I'm sure it's fine.  Use it, you know what, use Scrypt, or I guess you have to use Argon2 as your PBKDF2.  Use a really long, you know, miserable master password, or better yet, passkeys.  I've been using passkeys now with Bitwarden for passwordless login.  And while it doesn't work everywhere, it is very convenient where I can use it.  You know, we're putting passkeys in Bitwarden, might as well put your TOTP secrets in there, too; right?



STEVE:  Yup.



LEO:  Yeah.  I agree with you.  Convenience.



STEVE:  Okay.  Yeah, again, it's better than not using them at all.  Way stronger than just a username and password.  So, you know, if that's what it takes, do it.



LEO:  Depends on your threat model, you know.  If he's working for the NSA, well, then, you know, you should do something else.



STEVE:  Yeah, yeah.  George Palfi, that's @PalfiGeorge, he said:  "Steve.  I'm a devoted listener and longtime SpinRite owner, though I wish it worked on Macs.  I gave up Windows completely years ago."



Okay, George.  The good news is I made some changes a few months ago to allow SpinRite 6.1 to run on Macs where it can.  "Where it can" means Intel Macs, where it's possible to boot from a USB or a CD, typically through Boot Camp.  The previous trouble with SpinRite on Macs had been with the keyboard, since SpinRite was accessing the keyboard hardware rather than using the BIOS.  I changed that so that SpinRite could work with some less PC-compatible Dell machines, and we got Mac compatibility in the bargain.



LEO:  Nice.



STEVE:  A number of testers have confirmed their ability to now run SpinRite on Macs.



LEO:  You were using the A line for the BIOS on the keyboard?



STEVE:  It's, yeah, they are, well, no.  Even PCs that use USB keyboards do take the time to stuff the keyboard data into low RAM.



LEO:  Wow.  Wow.  Wow.  



STEVE:  Which is what the hardware does.  Mac doesn't do it.  So the Mac's PC emulation is slightly less compatible than all the others.  And I am finally able to announce...



LEO:  I need drum rolls, please.



STEVE:  Yes, that after more than three years of work, I am completely satisfied that SpinRite 6.1 is as good as it can be, and that it is finally ready for release.  There is nothing left I'm aware of that could be done to further improve SpinRite's functions.  I could keep fussing with it forever, adding this or that convenience feature around the edges, but it's already received a large collection of new convenience features, and it is by far the best SpinRite that's ever been created.



LEO:  Woohoo!



STEVE:  It's been proven to work in every environment that it's been placed in by more than 818 testers who've registered with our GitLab instance and who have obtained it through my release announcements in GRC's web forums.  It's finally done.



LEO:  Hallelujah.  Wow.



STEVE:  Officially, its code still calls itself Release Candidate 6.  And it makes sense to let it rest for a bit before it's moved to Final Release 1 since I would prefer not to have to be tweaking the code after it's been released.  And there's really no hurry.  While the paint is still wet and drying, I'll be working on SpinRite's documentation, which will all be browsable and explorable online. Since many people prefer to click on a video than to read text, I'll also create some video walkthroughs as I did for ReadSpeed, so that someone can get a feel for what SpinRite looks like while it's running.



LEO:  Nice.



STEVE:  Next, once the documentation is finished, I'll be bringing GRC's long-awaited email facility online to get our promised incoming email bag setup to receive incoming mail from this podcast's listeners.  So many people have written that they had to painfully login to Twitter just to get a note to me.  Anyway, I get it.  That'll finally be changing.  And I'll create a weekly mailing list for this podcast so that those who would like to receive a weekly summary and link to the show notes will be able to get that, as well.  I'm sure I will continue posting on Twitter.  I'm not yet sure whether I'll continue monitoring incoming tweets and DMs there.  I'll just play that by ear.  I'd very much like to consolidate the channels that I need to follow, and email is the most universal medium which we all share.  I've had so much positive feedback from people saying,  yes, yes, yes, please, just let me use email.



And once all of that is in place, I'll finally begin the process of notifying all 20 years' worth of SpinRite's past purchasers.  Since I imagine many of those 20-year-old email addresses are no longer valid, my plan is to send out the announcements off the free availability of 6.1, starting from the most recent and gradually heading toward the least recent so that, you know, so that I'm not seeing...



LEO:  What's the oldest?  What's the oldest account?



STEVE:  ...100% bounces of everything, yeah.



LEO:  Who's the longest owner of SpinRite, after you?



STEVE:  Well, I mean, this is only - my online database only goes back to 2003.  And it's funny, too, because Lorrie and I had Sue, my employee for the past 40 years, over to dinner.  And we were talking about this because once 6.1 is available, we're not going to continue allowing people to upgrade from earlier versions of SpinRite because, you know, it's been 21 years.



LEO:  Yeah, I think that's fair.



STEVE:  Since, you know, I mean, it's like it's just ridiculous.  It's like, come on, guys.  So she was excited because that meant she no longer needed to run FoxPro in a DOS box.  Our original database, where from every single person who ever bought any copy of SpinRite is in a FoxPro database which we call Dino because, yes, it is a dinosaur.



LEO:  It's a dinosaur.  Well, you know, dinner parties at the Gibsons', they're really - conversation is fascinating.  Actually there's quite a few people listening right now who would love to have been at that party.  Well, that's great.  Congratulations, Steve.  That's really good news.  That's great.  So look for your email, if you still have that account from all those years ago.



STEVE:  Well, and the reason I'm bringing it up this way, I did some research in the last week about emailing.  And the world has become such a sewer with spam that GRC cannot just suddenly start sending out email in great volume because we don't have a reputation.  So the beauty is, by using the Security Now! email and having people sign up, and I'll send them a confirmation, and then that way the world will start seeing GRC doing mail which is valid and it's being accepted by people, instead of like, what the hell is this?  And that'll allow us to establish a reputation.  And it turns out reputation matters in the same way that it does with the use of a digital certificate to sign software.  So, and I'm not in a big hurry.  I don't have to notify everybody in one day.  I'll just let it kind of dribble out slowly over time so that, again, we're not setting off any alarms in the anti-spam centers of all ISPs in the world.



LEO:  Actually, I should have asked you about this because starting February 1st, Gmail is going to require that all messages are authenticated with DMARC and SPF and DKIM.



STEVE:  We already have been for quite a while.  We're SPF, DKIM, and DMARC.  



LEO:  Yeah.



STEVE:  In fact, there is an amazing site, you have to go look at it, it's - I think it's called LearnDMARC.  Let me see if I can bring it up.



LEO:  Not the easiest thing in the world, I can tell you right now.



STEVE:  It is so overdesigned, I just - oh, there it is.  Yeah, LearnDMARC.com.  This guy, whoever he is, is a man after my own heart.  He so overdesigned this thing.  It is just - it is a gorgeous site.  So he gives you a one-time use email address to which you send any piece of email.  That allows him to pick it up and then check all of your security settings - SPF, DKIM and DMARC - which basically uses SPF and DKIM, and verifies all the proper security settings of whoever it is who's sending out email on your behalf.  And I passed all of the tests with flying colors.  But it was just a beautiful experience.  He's got stuff floating around the screen to fill in the fields.  It was just - it was delightful.  So Learn...



LEO:  [Crosstalk] send them some email.



STEVE:  Yeah.



LEO:  I'm pretty sure our - I use of course Fastmail, our sponsor.  And I'm pretty sure that Fastmail is doing that all from [crosstalk].



STEVE:  I didn't fill out anything.  I didn't fill out anything.  And he said what you just got, he says, "Waiting for incoming mail."  And after waiting a while, you don't have to write us an entire love letter.  



LEO:  Wait a minute.  Did I not send it?



STEVE:  This guy is great.



LEO:  You call it a beautiful site.  It is, let's say, text heavy.



STEVE:  Just wait.  Just wait.  



LEO:  Oh, yeah, look.  It's getting pretty quiet here.  Waiting.  Oh, so after you send it, it gets pretty; huh?



STEVE:  There it is.



LEO:  Oh, it came, yeah.  Wake up, Neo.  Hi, there.  LearnDMARC.  My name is Deo.  Oh, this is so cute.



STEVE:  Oh, just wait.  It gets better.  Look at that.



LEO:  Oh, you're right.  This is nice.



STEVE:  It's so well done.



LEO:  Oh, this is nice.  Oh, my gosh.  You're right.  I take it back, Steve.  This is a beautiful site.  Holy cow.  Let me shrink it down so you can see the whole thing.  Press any key.  Okay.  Is used to look up the domain's SPF policy, running SPF, yes, Laporte.email has an SPF policy.  It should have all of these - DKIM, DMARC, and SPF - because I use Fastmail.  It's one of the reasons I have my domains hosted by Fastmail.  Oh, this is really nice.  He did a great job.



STEVE:  Yeah.



LEO:  And this is nontrivial.  I've got to say the stuff behind the scenes that he's doing to do the validation, that's great.  Very nice.



STEVE:  Anyway, it keeps going as you press keys, and runs through, performs all the tests to demonstrate that the person doing your mailing has got their act together.



LEO:  Somebody says, outofsync in our Discord says, that looks like a web dev's rsum.  You're probably right, yeah.



STEVE:  That's good, yeah.



LEO:  Yeah, you're probably right.  A lot of CSS.  Oh, look at that sliding on over.  Look at that.



STEVE:  Ah.  It's just beautiful.



LEO:  Yes, my DKIM I think is okay.  I see you've included a DKIM signature.  I've retained - the signature passed validation.  All right.  There's a pass.  Yeah, this is almost like a videogame.  This is hysterical.



STEVE:  It's just gorgeous.



LEO:  So I know I'm going to pass all three because I'm using Fastmail.  But still, that's great.



STEVE:  Yup.



LEO:  This is sponsored by URIports.  Uh-oh.



STEVE:  Because you scrolled...



LEO:  Did I scroll?



STEVE:  Yeah, you scrolled.



LEO:  Don't scroll, kids.  Do not scroll.  Do not do what Leo did.  Alignment is - yeah, looks like I'm okay; right?  Yeah, there we go.  Now I'm getting passes.  It's because I scrolled up and couldn't find the pass.  Okay, you need to work on that CSS a little bit.  Yeah, we got passes everywhere.  Very nice.



STEVE:  Yeah.



LEO:  Very nice.  So the bottom line is, because of spam, you're going to have to really start making sure your email's provider is doing all this.  Of course if you're going Gmail to Gmail, you don't have to worry about that.  But if you're going any other email provider including especially your ISP, you might want to check and make sure it's okay.  I like this.  What is it again, LearnDMARC.com.



STEVE:  DMARC, for those who are listening, yeah.  Really, really great.  I just stumbled on it because I was doing a little research into what am I going to have to do in order to not get, you know, not get blocked on DNS blocklists and all that crap.



LEO:  This is actually, I guess, URIports is the company that does this DMARC monitoring.  So that's why they're so good at it.



STEVE:  Okay.  So recall that "Midnight Blizzard" is the dramatic renaming Microsoft gave to the Russian state-sponsored group, originally known as Nobelium, which most recently managed to crawl inside Microsoft's network to obtain access to data belonging to their uppermost top-level executives.  As we covered last week, late Friday night before last, Microsoft slipped out the news that a lesser-protected system had succumbed to the Russians after being sufficiently sprayed with passwords.  What Microsoft shared at the time left no one feeling satisfied. 



So last Thursday on the 25th, Microsoft attempted to offer additional useful information.  Most observers, however, have still been left wanting.  The reading between the lines that we did last week appears to have been correct.



At the top of last Thursday's lengthy update, Microsoft wrote:  "As stated in the MSRC blog, given the reality of threat actors that are well resourced and funded by nation states, we are shifting the balance we need to strike between security and business risk.  The traditional sort of calculus is simply no longer sufficient.  For Microsoft, this incident has highlighted the urgent need to move even faster.  If the same team were to deploy the legacy tenant today, mandatory Microsoft policy and workflows would ensure MFA and our active protections are enabled to comply with current policies and guidance, resulting in better protection against these sorts of attacks.



"Microsoft was able to identify these attacks in log data by reviewing Exchange Web Services (EWS) activity and using our audit logging features, combined with our extensive knowledge of Midnight Blizzard.  In this blog, we provide more details on Midnight Blizzard, our preliminary and ongoing analysis of the techniques they used, and how you may use this information pragmatically to protect, detect, and respond to similar threats in your own environment.



"Using the information gained from Microsoft's investigation into Midnight Blizzard, Microsoft Threat Intelligence has identified the same actor has been targeting other organizations and, as part of our usual notification process, we have begun notifying these targeted organizations."



Okay, now, that's enough of that.  As I noted, many if not most observers of Microsoft's handling of this incident have come away less than impressed.  So I wanted to share the highlights of an important industry-shaping interview Alex Stamos conducted with CNBC last Friday.  To remind everyone, Alex is a computer scientist.  He obtained his EECS degree from Berkeley.  Today he's an adjunct professor and lecturer at Stanford University's Center for International Security and Cooperation.



He first popped onto our map when he left Facebook after serving as their Chief Security Officer and then, in 2021, teamed up with ex-CISA director Chris Krebs.  Recall that Chris was fired from his position as director of CISA by President Trump after CISA put out a statement declaring that the 2020 U.S. Presidential election had been the most secure election in American history.  So Chris and Alex were both free, and they formed the Krebs Stamos Group.  That group later became part of SentinelOne where Alex now has the title of Chief Trust Officer.  He often serves as an expert witness in court and provides expert testimony to Congress.



Okay.  So Alex's credentials are well established within the industry and government.  The following, which I wanted to share, is what he posted last Friday following his interview on CNBC and following Microsoft's updated breach disclosure.  The title Alex gave his LinkedIn posting was "Microsoft's Dangerous Addiction to Security Revenue."  Under that headline, he wrote this.



He said:  "On Monday, CNBC gave me a chance to discuss Microsoft's Friday night news dump of a new breach by Russian intelligence services, in which I called for more details from Microsoft so that other organizations could defend themselves.  Yesterday, we gained a bit more transparency in the form of a blog post from 'Microsoft Security'" - again in air quotes - "the commercial security division of Microsoft."



"Some reactions," he wrote.  "First, Microsoft buries the lead with this paragraph."  Then he quotes them.  "Using the information gained from Microsoft's investigation into Midnight Blizzard, Microsoft Threat Intelligence has identified that the same actor has been targeting other organizations and, as part of our usual notification processes, we've begun notifying these targeted organizations."



Alex says:  "Translation:  Since the techniques outlined in the blog only work on Microsoft-hosted cloud identity and email services" - whoops - "this means that other companies were compromised using the same flaws in Entra, better known as Azure Active Directory, and Microsoft 365.  Microsoft's language here plays this up as a big favor they are doing the ecosystem by sharing their 'extensive knowledge of Midnight Blizzard' when in fact what they are announcing is that this breach has affected multiple tenants in their cloud products."



LEO:  Oh, my god.



STEVE:  Uh-huh.  And, he says, in a subsequent update to his original posting, Alex notes that Joseph Menn of the Washington Post has several sources indicating that at least 10 other companies were breached and will be disclosing those breaches soon.



LEO:  This isn't the same as the Exchange Server vulnerabilities we've been talking about.



STEVE:  Not the one that China used...



LEO:  This is Entra.  This is different.



STEVE:  Yeah.  Yeah.



LEO:  Oh, boy.



STEVE:  Ten more breaches, at least 10 that were part of this.



LEO:  They sure downplayed this.  I knew they were, too.  You could tell they were burying it, yup.



STEVE:  Yup.  Yup.  Second point he makes:  "Microsoft continues to downplay the attack by abusing the term 'legacy.'"  He says:  "One of the big open questions from last week was how an attack against a 'legacy non-production test tenant' could lead to access to the emails of key Microsoft executives."



LEO:  Yeah.  Yeah.  How did that happen?



STEVE:  He says:  "We get a bit more detail in this paragraph."  And we quote them now:  "Midnight Blizzard leveraged their initial access to identify and compromise a legacy test OAuth application that had elevated access to the Microsoft corporate environment.  The actor created additional malicious OAuth applications.  They created a new user account to grant consent in the Microsoft corporate environment to the actor-controlled malicious OAuth applications.  The threat actor then used the legacy test OAuth application to grant them Office 365 Exchange Online full_access_as_app role, which allows access to mailboxes."



Alex says:  "I've seen this fundamental problem in multiple investigations, including the one that Microsoft worked so hard to label as 'SolarWinds Incident.'"  He says:  "Azure AD is overly complex, and lacks a user experience that allows for administrators to easily understand the web of security relationships and dependencies that attackers are becoming accustomed to exploiting.  In many organizations, Azure AD is deployed in hybrid mode, which combines the vulnerability of cloud," he says, "(external password sprays) and on-premise (NTLM and Mimikatz)" - meaning combining vulnerabilities both outside and in.  He says:  "Identity technologies in a combination that smart attackers utilize to bounce between domains, escalate privilege, and establish persistence."



He says:  "Calling this a 'legacy tenant' is a dodge.  This system was clearly configured to allow for production access as of a couple of weeks ago, and Microsoft has an obligation to secure their legacy products and tenants just as well as ones provisioned today.  It's not clear what they mean by 'legacy'; but whatever Microsoft's definition, it is likely to be representative of how thousands of their customers are utilizing their" - meaning Microsoft's - "products today.  Microsoft does," he says, "however, offer all of us some solutions."



Which brings us to point number three, which he labels:  "Microsoft is using their own security flaws as an opportunity to upsell."  He writes:  "These sentences in the blog post deserve a nomination to the Cybersecurity Chutzpah Hall of Fame."



LEO:  Oh, I love you, Alex.



STEVE:  "As Microsoft recommends that potential victims of this attack against their cloud-hosted infrastructure first detect, investigate, and remediate identity-based attacks using solutions like Microsoft Entra ID Protection."



LEO:  Oh, yeah, you need that, yeah.



STEVE:  That's right.  "Number two, investigate compromised accounts using Microsoft Purview Audit Premium."



LEO:  Oh, yeah.  You don't have that yet?  Oh, we should get that, too.



STEVE:  Got to get that.  And three, "Enforce on-premises Microsoft Entra Password Protection" - don't want to get sprayed - "for Microsoft Active Directory Domain Services."  He says:  "In other words, Microsoft is using this announcement as an opportunity to upsell customers on their" - meaning Microsoft's - "security products, which are apparently necessary to run their identity and collaboration products safely."  He says:  "This is morally indefensible, just as it would be for car companies to charge for seat belts or airplane manufacturers" - you know where he's going - "to charge for properly tightened door bolts."



He says:  "It has become clear over the past few years that Microsoft's addiction to security product revenue has seriously warped their product design decisions, where they hold back completely necessary functionality for the most expensive license packs or as add-on purchases."



Okay, and I'm going to interrupt Alex for a moment to note that while all of this is highfalutin' enterprise stuff, I've long made the same point about Microsoft leveraging the insecurity of their "out of support" operating systems.  They blithely offer additional years of extended security support for their otherwise "out of support" operating systems to their enterprise customers, while at the same time starving the end users of those same operating systems of that vital security in a bald effort to force users to move to newer operating systems which they neither need nor want.  If the security updates are available anyway, deliberately withholding as ransom the patches to your defective operating system because you can is morally indefensible and reprehensible.  Anyway, my two cents.



Referring to Microsoft's two recent posts, Alex says:  "While these two arrogant and circumspect posts do, at least, admit 'the urgent need to move even faster' in securing their products," he says, "I would argue that Microsoft has a much deeper cultural problem to solve as the world's most important IT company.  They need to discard this poisonous idea of security as a separate profit center and rededicate themselves to shipping products that are secure by default, while providing all security features to all customers."  He says:  "I understand the need to charge for log storage or human services, but we should no longer accept the idea that Microsoft's basic enterprise offerings, including those paid for by the U.S. taxpayer, should lack the basic features necessary to protect against likely attacks."



He says:  "My current employer competes against some of these products from Microsoft.  But if Microsoft did a better job by default, that would reduce the need for SentinelOne and other security vendors to provide basic safety protections.  For all the language about the sophistication of the hackers behind this attack, there's nothing here that is outside the norm for ransomware groups attacking Microsoft's technologies, and Microsoft customers of all sizes should be concerned that these techniques will be deployed against them if they do not pay extra for the secure version of Microsoft's cloud products."



LEO:  Wow.



STEVE:  "Twenty-one years after the Trustworthy Computing memo, it's once again time for some soul searching in Redmond."



LEO:  You go, Alex.



STEVE:  So bravo, Alex.



LEO:  Yeah, yeah.



STEVE:  You know, I love the system of free enterprise we enjoy in these United States.  The profit motive provides strong impetus to innovate and provide value.  But the lure of increased profit carries a danger when an executive faces a decision about whether to include a desirable and important feature in the base product, or to charge extra for it.



A crucial feature that's necessary for this system of free enterprise to deliver its maximum value to the public at large, rather than to simply further line the pockets of those executives and their shareholders, is competition.  While it's an enviable position to be in, Microsoft is only able to get away with these usury practices because they have no real competition in the markets they dominate.  This has been a problem for them in the past, and it may be again in the future.



LEO:  Yikes.



STEVE:  I'm glad that people like Alex are saying this on CNBC and posting this in his LinkedIn feed because, you know, we need to shine a light on this.



LEO:  We'll have to talk about it tomorrow on Windows Weekly, too, because this is really a much worse attack than we had hypothesized, at least the...



STEVE:  It is much worse than we thought.



LEO:  Yeah, yeah.



STEVE:  It is broader, and apparently a huge number of Microsoft's customers...



LEO:  Well, 10 anyway, yeah.



STEVE:  Yeah.



LEO:  That's just the tip of the iceberg, I'm sure.



STEVE:  Bye.



Copyright (c) 2024 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION



SERIES:		SECURITY NOW!

EPISODE:	#960	

DATE:		FEBRUARY 6, 2024	

TITLE:		UNFORESEEN CONSEQUENCES

HOSTS:	STEVE GIBSON & LEO LAPORTE

SOURCE:	SN-960.MP3

LENGTH:	108 MINUTES



DESCRIPTION:  What move has CISA just made that affects our home routers?  What serious flaw was discovered in a core C library used everywhere by Linux?  Does OpenSSL still have a future?  What's Roskomnadzor done now?  How can a password manager become proactive with Passkey adoption?  Which favorite browser just added post-quantum crypto?  What prevents spoofing the images taken by digital signing cameras?  Why are insecure PLC devices ever attached to the Internet?  And what may be an undesirable and unforeseen consequence of Google's anti-tracking changes?



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is ready.  He's got some great stuff to talk about, including the new CISA recommendations for home routers.  I hope they're adopted.  A massive flaw that really affects every version of Linux.  It's being patched or has been patched, but you should know about it.  Post-quantum crypto added to our favorite browser.  And then an unforeseen consequence of Google's new anti-tracking changes.  That's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 960, recorded Tuesday, February 6th, 2024:  Unforeseen Consequences.



It's time for Security Now!.  Steve Gibson, here he is.  Or was that one of the sounds that goes off when something happens, one of the alerts?



STEVE GIBSON:  I can't wait.  And speaking of sounds, all of our listeners will be glad to know that little annoying beep in the background finally died.



LEO:  You couldn't find - it was a smoke detector; right?  But you couldn't figure it out.



STEVE:  No, actually it was a water alarm which I had installed because my air conditioning condenser was backing up and overflowing, so I needed to be alerted if that was happening.  But I replaced the whole AC system a couple years ago with a brand new one that has all that built in.  So I removed the water sensor and just stuck it aside.  And as happens here where we need to have archeological digs to find things...



LEO:  Oh, it was in a pile somewhere.



STEVE:  Yes.  It was just buried.  Literally buried.



LEO:  It was in the midden heap somewhere south of your living room.  Okay.



STEVE:  And then at some point it began going "beep," like very briefly, very high-pitch, and not often.  And some of our listeners began saying, Steve, you've got to check the batteries in your smoke detector.



LEO:  Yeah, yeah.



STEVE:  Because, you know, apparently there's a problem there. Well, no.  And I could not find it.  And it had been going on, I don't know, a couple years maybe?  And I just...



LEO:  But I stopped hearing it.  I haven't heard it recently.  Has it still been going?



STEVE:  Oh, it was going last week during last week's podcast.  If you play the podcast, every so often, "beep."  Anyway, so...



LEO:  Oh, and that's got to drive - because we have a lot of OCD listeners, I mean, people who really can't handle that kind of thing.



STEVE:  Yes.  



LEO:  That must, I mean, I'm sorry.



STEVE:  I stopped hearing it.  I had adapted to my environment.



LEO:  Right.



STEVE:  And so it's like, you know, I step over things that are in the way rather than...



LEO:  I thought you'd fixed it.



STEVE:  No.



LEO:  Oh, my god.



STEVE:  So when I came in yesterday morning I heard [screeching sound].  So I thought, oh, thank god.  I knew at some point the battery would actually finally die so that I couldn't even make these beeps.  It was like [screeching sound].



LEO:  Did you find it?



STEVE:  I went right to it.  Just directly, I just, like, pulled some things out of the way.  There it was.  And that's indeed what it was.



LEO:  Did you stamp on it?



STEVE:  I mean, even moving it around it went [screeching sound].  It was just on its last volt.



LEO:  Oh, my god.  Oh.  Well, anyway...



STEVE:  So silence.



LEO:  Somebody, Chickenhead21 in our Discord wants to know, was Elaine actually typing "beep" when it went off in the transcripts?



STEVE:  Bless her heart, I wouldn't be surprised.  She just had a little parenthetical "beep," you know.



LEO:  I haven't heard it in months.  I knew about it.  People had written in about it.  And I thought you'd fixed it last year.



STEVE:  Oh.  No.  



LEO:  I just wasn't hearing it.  Maybe like you I'd either grown attuned to it or I'm so "deef" now that I can't hear that frequency.  Wow.  Well, thank you for fixing that.  That's...



STEVE:  Well, thank you, thank me for my patience.  It is now finally gone.



LEO:  I told you about that Avenue - I remember when we talked about this last, I told you about that "Avenue 5" episode.  I don't know if you ever watched "Avenue 5" after we talked about it.  But they're on the spaceship; right?  And there was a beep, and nobody could figure out where - it was keeping people up.  It was the whole thing.  So this is not an unusual phenomenon.  You maybe should make a variation of the Portable Dog Killer that is the Portable Beep Locator.



STEVE:  Believe me, when this began, I gave it some serious thought.  It was impossible for me to find it.  So I considered putting two microphones some distance apart.



LEO:  Triangulating it.



STEVE:  Exactly, and locking onto that sucker.  But then I thought, well, we really do want SpinRite 6.1 eventually.



LEO:  Won't you be glad when you retire that you can devote your time to things like that?



STEVE:  Retire?  What?



LEO:  Never.  Never.



STEVE:  Oh, no.  I've got to move SpinRite 7 onto the Vision Pro.



LEO:  Yes.  We announced that during MacBreak Weekly that you would make a version for the Vision Pro.  



STEVE:  Not a problem, yes.  Imagine walking through the bits of your mass storage, looking around and saying, ooh, look at that bad spot there.  Let's pluck that out.



LEO:  Oh, gosh.



STEVE:  Yes.



LEO:  So what is coming today on Security Now!?



STEVE:  Oh, boy.  This is Security Now! 960 as we begin February.  This podcast is titled "Unforeseen Consequences," which sort of crept up on me when I stumbled upon an odd reference to a piece in the Financial Times.  Now, the Financial Times has one of the strongest paywalls you can find.  I mean, they're not screwing around.  They're like, hey, you know, we're just going to tease you with a headline.  You're not going any further.  Except they also allow themselves, like I just googled the headline, and there it was.



So it's like, okay, well, you're not that worried.  I mean, you know, they want to bring people to their paywall so you can decide if you want it.  Anyway, they had a really interesting piece that talks about some consequences we've never considered that are, like, the dark side of Google's killing third-party cookies.  So it's going to be really interesting.  This is going to be a riveting episode.



But first we're going to talk about what move CISA has just made that affects our home routers.  What serious flaw was discovered in a core C library used everywhere by Linux?  Does OpenSSL still have a future?  And what's Roskomnadzor done now?  How can a password manager become proactive with Passkey adoption?  Which favorite browser has just added post-quantum crypto?  What prevents spoofing of the images taken by digital signing cameras, if anything?  And why are those insecure PLC devices, you know, the programmable logic controllers which run process automation everywhere, ever being attached to the Internet?  And what may be an undesirable and unforeseen consequence of Google's anti-tracking changes?



LEO:  Uh-oh.



STEVE:  Yeah.  It's going to be a great episode.  And oh, Leo, we do have a Picture of the Week.



LEO:  I only see the caption.  I haven't scrolled up yet.  But I can tell from the caption it's going to be a good one.



STEVE:  Yes, it is.  It may explain the power outages you've been having at TWiT Studios.



LEO:  Holy cow, yeah, we were in the middle of TWiT on Sunday.  Fortunately, we were not in the middle.  We were actually within minutes of ending it, and everything just went dark.  And I had to go home and finish the show at home because there was no power here.  And then of course as you noticed I come in the studio, and everything is all messed up because they don't survive power outages very well.  I had to play with a bunch of things.  Anyway, we got it all working.



STEVE:  Now, Leo, you may scroll up.  



LEO:  And reveal, huh?



STEVE:  And reveal the cause of the power outages at TWiT Studios.



LEO:  But this, the caption, "But this is where you said you wanted the dangerous high-voltage terminal box."  Oh, just sitting right out there.  Right out there in the public.  I bet you there's a playground right next to it.



STEVE:  Well, and look what's on it, or aimed at it.  Scroll down a little further.



LEO:  Oh, I missed that part.  There's a sprinkler, sprinkling it.



STEVE:  So for those who are listening...



LEO:  Oh, I hope it's weather sealed.  Holy cow.



STEVE:  Out in the middle of something is this scary-looking high-voltage box that says "Attention, Attention" with the lightning bolt saying, you know, high voltage.  And there's a sprinkler, you know, one of those, like those things that shoots out a beam of water that's supposed to go about a thousand yards, which slowly rotates to water the entire park.  Well, this box is about three feet away from it.



LEO:  Right on it.



STEVE:  Receiving the full force of this water blast.



LEO:  Oh, my god.



STEVE:  Right in its face.  You know, it's surprising there aren't sparks flying out of this thing.



LEO:  Oh, my god.



STEVE:  Anyway, yeah, you want to step cautiously on the wet lawn that surrounds this electrical box.



LEO:  That's a great picture.



STEVE:  You could probably charge your Tesla just by parking on the lawn next to it.



LEO:  That's hysterical.  That's just great.



STEVE:  Yeah.  Wow.



LEO:  It's liquid cooled, MashedPotato says in our Discord.



STEVE:  He said what?



LEO:  It's liquid cooled.



STEVE:  Liquid covered, right.



LEO:  Yes, never gets hot.



STEVE:  Okay.  So under the headline "CISA and FBI Release Secure by Design Alert Urging Manufacturers to Eliminate Defects in SOHO Routers" - and I think everyone knows SOHO, S-O-H-O, Small Office Home Office is what that abbreviation is.  So last Wednesday, CISA and the FBI published guidance, this is the third such release of theirs, they've kind of - and this is the first aimed down at the consumer.  Previously they were talking at the enterprise level.  So they published guidance on Security Design Improvements for SOHO Device Manufacturers, which is part of their new Secure by Design Alert series, which focuses on how manufacturers should shift the burden of security, thank god, away from the customers who, you know, they just want this stuff to work - plug it in, set it and forget it - by integrating security into the product design and its development.



So this third publication in CISA's series examines how manufacturers can eliminate what they call "the path threat which actors are taking" to compromise small office and home office routers.  Now, they were specifically referring to a recent initiative.  There is a group out of China known as the Volt Typhoon group, which the FBI just somewhat controversially took down by patching these routers.  And it was my intention initially to talk about that as our main topic this week.  But I ran out of space, actually, on the podcast, and time, and I really needed to talk about the consequences of what I realized was going to be happening as a consequence of stumbling upon this Financial Times piece.  So I have that queued up for next week.



But there was something that caught my attention in this which was unsuspected, or unanticipated.  They said, CISA did in this joint FBI release, that they wanted manufacturers to do three things:  automate update capabilities, remove web management from the WAN interface, and require a manual override to remove security settings.



Okay.  So all of this podcast's listeners have probably grown tired of hearing me talk about those first two points: automate updates and remove all device management from the public-facing interface, the WAN interface; right?  You just don't need to use a web interface aimed at the Internet so that you can access your device across the Internet.  What we keep learning is that we don't know how to do that safely because everyone keeps making mistakes.  So, and you don't have to expose it to the public because there are plenty of ways to get over onto the private LAN from the public Internet and then access the device from the LAN side.  That's the way we should do it.



Anyway, the third one was really interesting.  I think it's brilliant.  They say:  "Require a manual override to remove security settings."  In other words, routers should not accept remote or any even local over-the-wire instructions which reduce their security in the absence of a manual physical local confirmation of some kind.  There is no substitute for the affirmation of one's physical presence at a router's location.  Pressing a "I want to change my router's configuration" button is the one thing no remote attacker in Beijing is able to do from the comfort of their cyberwarfare bunker.



I think that the best way to do this would be to require a button to be pressed in order to place the router into configuration change mode.  So if a user logs into their router, they're welcome to do that.  They're welcome to poke around and look at the router's various settings.  But the moment the user attempts to change something which is important to the security of the system, the router's UI will pop up a little box and say: "Please press the Enable Configuration Changes button on your router to proceed."  And it'll just wait.  Once the button is pressed, the router will take down that little message and will allow the user to change its configuration until the user either logs out of the interface, or after some period of inactivity because most people just leave their login cookie present and logged in so they can get back to it easily if they need to. 



So would this be potentially a pain in the butt?  Yeah, especially if the router is in the attic.  But, you know, it's a classic trade-off between security and convenience.  Requiring a one-time password is certainly not as convenient as not using one, but that requirement is clearly much more secure.  So the problem being addressed in this case is very real; you know?  We are populating the world with insecure yet increasingly powerful consumer routers which are actually being taken over by malign remote forces that wish to exploit our traditional lack of focus on security.



So once again I give big props to CISA for leading this truly necessary change.  I think this makes so much sense.  Yes, again, it will be a bit of an annoyance to have to physically go to the router and press the button saying "I want to enable configuration changes."  But it's a brilliant requirement, and I do hope that we see this.  And really we're not doing this all the time.  And if you are, don't put your router in the attic.  Put it somewhere a little more accessible, and that'll just become, you know, the way we do things in the future.  I think this makes so much sense.



While we were recording last week's podcast, the Qualys Threat Research Unit, they call it the TRU, which is kind of a cool abbreviation, was informing the world that they had recently unearthed four significant vulnerabilities in the GNU C Library, which forms a cornerstone for countless applications in the Unix, I'm sorry, in the Linux, well, probably Unix, too, well, not GNU, but in the Linux environment.  One of these four which they found is a severe vulnerability tracked as CVE-2023 - notice it's late last year - 6246.  This vulnerability affects major distros like, well, every version of Linux, I think it's safe to say, but of course including Debian, Fedora, Red Hat, and Ubuntu.



LEO:  Yeah, everything.  It's C Lib; right?



STEVE:  Yes.  It's glibc. 



LEO:  Yeah.  Yeah, glib, yeah, yeah, yeah, yeah, yeah, yeah, yeah.  That's a big one.  That's everywhere.



STEVE:  You know, and that's the core C library that C depends upon.



LEO:  It's basic standard functions, yeah.



STEVE:  Yes, it's linked into everything.  So the bug impacts versions going back to August of 2022, which is when the bug was introduced.  It is an elevation-of-privilege flaw that can allow local attackers with access to a system to obtain root privilege access.  So we dodged a big bullet here, folks, because if this had allowed remote attackers to get root...



LEO:  Oh, then we'd have trouble, yeah.  Yeah.



STEVE:  Oh, baby.  So here's what Qualys explained about their discoveries.  They started by saying:  "Before diving into the specific details of the vulnerabilities, it's crucial to understand these findings' broader impact and importance.  The GNU C Library, or glibc, is an essential component of virtually every Linux-based system, serving as the core interface between applications and the Linux kernel.  The recent discovery of these vulnerabilities is not just a technical concern, but a matter of widespread security implications."  And actually more about the bullet that flew by and we dodged.  We'll get to more of that in a second.



In other words, so it was more than a little bit shocking to Qualys to discover serious exploitable vulnerabilities in a core component of a system that is this widespread.  Needless to say, Linux is everywhere, including in every one of those SOHO routers we were just talking about.  And we all need to keep in mind that fixing it today doesn't automatically fix it yesterday.  Which is another strong argument for allowing autonomous updating of unattended and unmanaged IoT devices.



Anyway, Qualys continues, writing:  "The vulnerabilities identified in glibc's syslog and qsort functions highlight a critical aspect of software security.  Even the most foundational and trusted components are not immune to flaws.  The ramifications of these vulnerabilities extend far beyond individual systems," they write, "affecting many applications and potentially millions of users worldwide.  This article aims to shed light on the specific nature of these vulnerabilities, their potential impacts, and the steps taken to mitigate them.



"The first vulnerability (CVE-2023-6246), a significant security flaw has been identified in the GNU C Library's _vsyslog_internal() function, affecting syslog() and vsyslog().  This heap-based buffer overflow vulnerability was inadvertently introduced in glibc v2.37 in August of 2022 and subsequently backported to glibc v2.36" - an earlier one - "while addressing a different, less severe vulnerability."  So, oops.  It actually, you know, the flaw was introduced in 2.37, and then they thought they were fixing an earlier vulnerability in 2.36 and broke it, as well.



They write:  "Major Linux distributions like Debian" - that would be 12 and 13 - "Ubuntu 23.04 and 23.10, and Fedora 37, 38, and 39 are confirmed to all be vulnerable.  This flaw allows local privilege escalation, enabling an unprivileged user to gain full root access, as demonstrated in Fedora 38."  So again, somebody standing in front of a machine where you are relying on them not having root and only being able to log in and do things as a non-root user, that reliance broke completely.



They said:  "In our analysis of the same function affected by CVE-2023-6246" - this one - they said:  "We identified two additional, albeit less severe, vulnerabilities.  One is an off-by-one heap-based buffer overflow also in the _vsyslog_internal() function, and an integer overflow issue, also in the same function."  But, you know, not nearly as worrisome as this main one.  They said:  "Based on our assessment, triggering these vulnerabilities appears more challenging than 6246, the primary problem.  Additionally," they said, "exploiting them effectively is likely to be more complex.



"As for the last of the four vulnerabilities, a memory corruption issue was found in the GNU C Library's qsort() function, caused by a missing bounds check.  This vulnerability could be triggered when qsort() is used with a nontransitive comparison function, just such as a simple comparison of a and b, which returns a minus b; and using a large number of elements controlled by an attacker, potentially leading to a memory allocation failure."



Okay.  So what are the implications?  Qualys writes:  "The discovery of vulnerabilities in the GNU C Library's syslog and qsort functions raises major security concerns."  And these are sort of hypothetical concerns, but still worth noting.  They said:  "The syslog vulnerability, a heap-based buffer overflow, can allow local users to gain full root access, impacting major Linux distributions.  Similarly, the qsort vulnerability, stemming from a missing bounds check, can lead to memory corruption and" - get this - "has affected all glibc versions since 1992."



LEO:  Yikes.



STEVE:  Yeah.  In other words, all glibc versions, effectively.



LEO:  Yeah.  Linux is only - yeah, definitely, that's all of them.  Yeah.



STEVE:  Right.  They said:  "These flaws highlight the critical need for strict security measures in software development, especially for core libraries widely used across many systems and applications."  So, yeah.  No kidding.  Now, what happens, or the way this is managed behind the scenes, is always interesting.  So here's a quick blow-by-blow timeline from the discovery through the coordinated release one week ago today.



So this began in early November, November 7th of last year, 2023.  So the end of last year, November 7th, they said:  "We sent a preliminary draft of our advisory" - that is, you know, a disclosure of their discovery - "to Red Hat Product Security."  Eight days later, on the 15th, Red Hat Product Security acknowledged receipt of their email.  The following day, on the 16th of November, "Red Hat Product Security asked us if we could share our exploit with them."  The following day, on the 17th, they sent the exploit to Red Hat Product Security.  Four days later, on the 21st, Red Hat Product Security, they said, "confirmed that our exploit worked, and assigned CVE-2023-6246 to this heap-based buffer overflow in vsyslog_internal."  



Okay.  So that is November 21st.  Now we go to December.  We're on December, the next month, on the 5th:  "Red Hat Product Security sent us a patch for this vulnerability 6246, written by the glibc developers, and asked us for our feedback."  Two days later, December 7th, they said:  "While reviewing this patch, we discovered two more minor vulnerabilities in the same function."  That's where that off-by-one buffer overflow and the other integer overflow surfaced.  They said:  "We immediately sent an analysis, proof of concept, and patch proposal back to Red Hat Product Security and suggested that we directly involve the glibc security team."  That was on December 7th.



"The next day, on the 8th, Red Hat Product Security acknowledged receipt of our email and agreed that we should directly involve the glibc security team.  We contacted them on the same day, and they immediately replied with very constructive comments."  And of course they were already looped into this because Red Hat had previously forwarded this to them and then received the patch back from them which then they sent back to Qualys.



"Three days later, December 11th, the glibc security team suggested that we postpone the coordinated disclosure of all three vulnerabilities until January 2024."  Okay.  So we were at December 11th at this point.  They said:  "Because of the upcoming holiday season," meaning people on vacation, people not around, people less available to respond immediately, as this would require, to the public coordinated disclosure of this.  So they said, yeah, good, let's let the holidays pass, and we'll deal with this immediately afterwards.



So December 13th, still last year before Christmas, Red Hat product security assigned the two additional CVEs to the other two things that had been found.  On January 4th this year, they said:  "We suggested either January 23rd or January 30th for the Coordinated Release.  Glibc developers agreed on January 30th."  That was last Tuesday.  So now we're at January 12th.  "The glibc developers sent us an updated version of the patches for these vulnerabilities.  The next day we reviewed these patches and sent our feedback to the glibc developers."  Two days later, on the 15th, the glibc developers sent us the final version of the patches for these vulnerabilities.



The following day, Qualys says:  "We sent these patches and a draft of our advisory to the linux-distros@openwall list.  They immediately acknowledged receipt of our email and, on the 30th, last Tuesday, coordinated release of this occurred."



So, you know, that's how this actually, you know, like there's an example of everybody being responsible, everybody responding to email, no one sitting on this for months the way we've seen Microsoft do so often.  You know, this is the way it's supposed to happen.  Problem is found, the right people are looped in, it's reviewed, it's verified, patches are created, patches are verified, some more tweaks are made, everybody agrees about, like, looks at the calendar, when would be a good time to let everybody know.  And that's the way it happens.  So a great look at how this happened.  And all the distros have been updated now.



LEO:  Nice.



STEVE:  Everybody who's in a situation where it might be a problem if a Linux system from the last two years is relying upon its protected root privilege, well, it's not as protected as we were hoping.  So you'll want to...



LEO:  But at least somebody - an attacker needs to be physically on your system.  So that's a relief.



STEVE:  Yes, thank goodness.



LEO:  Yeah.  By the way, I bet you you could look at a quick, any quick sort and immediately know if there's a buffer overflow.  This is not a hard thing to write.  Everybody wrote it in Comp Sci 101.  I can see how you'd get buffer overflow, but that seems, like, pretty boneheaded.



STEVE:  Well, so you're able to pass a function to qsort to use.



LEO:  Oh.  Oh, yeah, yeah, because that's the function that determines what's less or greater; right.



STEVE:  Yes, exactly.  So it's the sorting determiner function which is where the problem actually is.



LEO:  That might be a little bit harder to trace, I guess, yeah.  I mean, usually you just pass it less than or greater than.  But okay.  If you did something really elaborate, maybe you'd get something weird, yeah.  Interesting.



STEVE:  Okay.  So speaking of libraries, OpenSSL has lost another big user.  The CDN Fastly, you know, one of the biggies,  announced that they've decided to switch from OpenSSL, which they've been using to date, to the name you've just got to love because this is what you want from your SSL, BoringSSL.  You know, you want a BoringSSL library.  In their announcement they explained.  They said:  "OpenSSL has a long history of high-severity vulnerabilities, including the notorious Heartbleed bug.  In addition to the risk of exploitation, there is a significant operational cost incurred to rapidly test and deploy patches."



And, you know, we're talking about - so I don't think they say this anywhere, but this is on all of their edge system instances.  So all of their edge routing, edge proxies where the CDN's network is interacting with the Internet, this is where this goes.  So, yeah, if some high-severity vulnerability is found in OpenSSL, like every one of those instances needs to be fixed immediately.  And that's a big pain in the butt.



So they said:  "There's a significant operational cost incurred to rapidly test and deploy patches whenever a new vulnerability is announced.  Our primary goal in replacing OpenSSL with BoringSSL was to reduce the frequency and impact of CVEs and improve the security of our TLS termination system for our customers.  BoringSSL is a fork of OpenSSL that was created and maintained by Google.  It is widely considered to be fundamentally more secure than OpenSSL because it is less complex.  OpenSSL remains the Swiss Army Knife of SSL libraries, and a bunch of great work has been done over the years to improve it.  But we are convinced that BoringSSL provides better protection for our customers."



They added:  "Our work began about a year ago with the ambitious idea of replacing OpenSSL on our edge for all incoming connections.  We considered a few alternatives, but stuck with our original vision of migrating to BoringSSL to gain the following benefits:  smaller, more modern code base; a safer API - BoringSSL is an OpenSSL derivative and is mostly source-compatible, making our migration less challenging; extensive fuzzing; used by big players and maintained by Google; and similar performance to OpenSSL."



They said:  "In summary, the consensus was that BoringSSL offers a more focused code base, one without OpenSSL's myriad of legacy code, which makes it intrinsically more secure."  And I didn't have it here just because it would take up a lot of space, but they showed the breakdown of code between OpenSSL and BoringSSL.  The BoringSSL source code base is less than half the size of OpenSSL.  So it just makes sense as a technology is maturing that it's also going to be getting a bit old and creaky along the way.  In the case of OpenSSL, it spans decades, having started in 1998.  So that makes it 26 years old.  And as we know, SSL has evolved itself as a protocol dramatically during those 26 years.  So Google created BoringSSL.  And we know, for example, that Amazon's AWS service is running on their own very small homegrown TLS stack.



I'm sure that OpenSSL will remain the bedrock that it always has been for experimentation and testing.  That's always where new protocol stuff is worked out.  And for being, as Fastly said, the Swiss Army Knife of SSL libraries.  But its deployment in critical new applications has probably seen its day.  And as I was reading this and thinking about it, we've been using GitLab to, like, manage all of the issues during the ending phase of SpinRite's development.  We were just using, you know, newsgroup threads initially.  But one of our participants, well known to all of the people in our newsgroups, Colby, he was suggesting GitLab.  And I looked at it, and I thought, okay, let's, you know, I'll give it a try.



So I brought it up on its own server.  And it's very nice.  The problem is, it has way more features than we are using, just as OpenSSL has way more features than Fastly is using; and they won't leave it alone; and it's so big and complex it's constantly having bugs and problems that are critical.  So the analogy is perfect.



And as a consequence, I am seriously considering moving to a much more modest, better fit for us, like issue tracking system.  There's something called Redmine which looks like it is exactly what I want, mostly because they haven't touched it in a long time.  And I don't want to spend all my time maintaining a tool which is supposed to be helping us to manage a project.  I just want it to manage the project and not require its own maintenance staff.  So I can fully understand the tradeoff that Fastly is looking to make and has made.



LEO:  Back to Steve Gibson, who is going to show us how to write a proper quick sort.  No, he's not.  That's not what he's going to do.



STEVE:  Well, no.



LEO:  Although I would take that class, Steve.  I would.  Sanitize your inputs.



STEVE:  So recall that last December 1st Russia put a new communications law into effect which required all hosting providers of Russian websites to register with none other than Roskomnadzor.  This law requires all cloud and web hosting providers to register with the Roskomnadzor agency, which is of course Russia's telecommunications watchdog.  So far, 266 web hosting providers have registered with Roskomnadzor, and all are local companies.  Not a single external provider has registered.  And those providers are responsible - those providers, the external providers, I'm sorry - the external providers are responsible for about one third of all Russian websites.



Now, I don't know what's up, but this does seem a little suspicious that not a single external provider has registered.  So it makes me wonder whether this is actually, like, you know, a backhanded Russian way of forcing the remaining one third of Russian sites which are currently being hosted by external providers, none of which suspiciously have registered, and all of which - and here's the point - are subject to being cut off at some point in the future, if this isn't some way of forcing all the Russian sites into Mother Russia's hosted services rather than continuing to use, you know, those non-Russian territorial providers.  We'll see how this goes.  But Roskomnadzor has made it clear that at some point non-registered providers will be cut off from access to Russian territory.  So again, don't know what that means, but we'll see.



Also last Tuesday, Google's Security Blog announced a very nice-sounding new feature for Android's Password Manager.  The blog's title is "Effortlessly upgrade to Passkeys on Pixel phones with Google's Password Manager."  Okay.  So it turns out this is less Google-specific than they're making it sound.  I'll explain that in a second.



Here's what Google said.  They said:  "Google is working to accelerate Passkey adoption."  That's good for everybody.  They said:  "We've launched support for Passkeys on Google platforms such as Android and Chrome, and recently we announced that we're making Passkeys a default option across personal Google Accounts.  We're also working with our partners across the industry to make Passkeys available on more websites and apps."  Which as we know is what's required for this to make any sense at all.



"Recently," they said, "we took things a step further.  As part of last December's Pixel Feature Drop, we introduced a new feature to Google Password Manager:  Passkey upgrades.  With this new feature, Google Password Manager will let you discover which of your accounts support Passkeys, and help you upgrade with just a few taps.  This new Passkey upgrade experience is now available on Pixel phones starting with the Pixel 5a, as well as Pixel Tablet.  Google Password Manager will incorporate these updates for other platforms in the future.



"Best of all," they wrote, "today we're happy to announce that we've teamed up with Adobe, Best Buy, DocuSign, eBay, Kayak, Money Forward, Nintendo, PayPal, Uber, Yahoo! Japan, and soon TikTok, to help bring you this easy Passkey upgrade experience and usher you into the passwordless future."  They said:  "If you have an account with one of these early launch partners, Google Password Manager on Pixel will helpfully guide you to the exact location on the partner's website or app where you can upgrade to a Passkey.  There's no need to manually hunt for the option in account settings.



"And because the technology that makes this possible is open" - in other words, yes, it's actually not Google's - "any website or app, as well as any other password manager, can leverage it to help their users upgrade to Passkeys for supported accounts.  It's all part of Google's commitment," they said, "to help make signing in easier and safer."



Okay.  So they're saying that at launch this initially works with Adobe, Best Buy, and so forth.  But why them and not everyone?  It's just that this group is first to adopt a new standard.  We've all seen how our password managers are able to perform a security checkup; right?  Like to notify us when we may have reused a password somewhere, where we're using the same password for two different accounts.  So this is our password managers being proactive about our security.



Well, it turns out that there's an open standard means by which any website that supports Passkeys is able to advertise the fact that it supports Passkeys in a way that any password manager is able to check for and similarly advise.  I did a bit of digging, and I found the page where Google describes this.  It's titled "Promote Passkey upgrades in Google Password Manager."  Of course, this actually applies to any password manager that does this.  There's nothing Google Password Manager-specific about this.



Anyway, they wrote there - now, this is aimed at web app and website developers.  So that's the portion of the site where this was found.  So talking to website developers, they said:  "Integrating Passkeys into your app or website is just the beginning of your Passkey journey.  After your initial deployment, one of the challenges you will likely encounter is making sure your users understand what Passkeys are and how to create them.



"You should suggest creating a Passkey immediately after the user signs in using their password and verifying with a second factor.  Remembering passwords and entering one-time passwords while switching between different apps and tools can be frustrating for users.  Recommending the creation of a Passkey at this moment is an opportune time, as users are likely feeling this frustration.  In addition to the self-managed promotions, Google Password Manager can now suggest creating a new Passkey on behalf of your website or app."



Okay.  So under the user's experience they say:  "On Pixel devices, Google Password Manager discovers that your website or app supports Passkeys, suggests users to create a new Passkey, and directs them to your Passkey creation page."  Okay.  So leaving Google out of this, what this is about is a very welcome standardized and uniform way for any Passkey-supporting site to declare its support in a machine-readable way.  So this is, as I said, more broadly than just Google, this means that any password manager on any platform - are you listening, Bitwarden? - could examine the entire inventory of its user's saved passwords and use this standardized protocol to proactively check the web domain of each password for its support of Passkeys.  And if an available Passkey had not yet been configured on that account, the password manager could take the user directly to that site's Passkey setup page.



The standard used we've talked about before.  It's the /.well-known/ web directory which is located at the root of a domain.  And there's a "Passkey-endpoints" JSON-formatted file there under that /.well-known/ directory that contains two URLs, one to enroll a new Passkey and another to manage existing Passkeys.  So again, any Passkey-supporting site should take every opportunity to enroll its users the next time they're logging into the site, and that the site sees that they're using a Passkey-supporting client.  That's the primary way we can expect Passkeys to become adopted.  But it will also be cool for them to be able to come at this from the direction of the Passkey-enabled password manager to have them reveal the sites to which we could enroll and switch over to Passkey logon and authentication.



LEO:  I agree.  I agree.



STEVE:  So very cool.



LEO:  Now that Bitwarden supports Passkeys, I find myself much more likely to use it because it's cross-platform.  Because I work on all platforms.  So, yeah, Apple, I have my Passkeys for some things in my iPhone.  But if it's not everywhere, it's not useful.  So I really like it that Bitwarden supports it.  And I've used it a number of times now to log into Google and stuff.  And it's like, wow, that was easy.  Really it's good.



STEVE:  Yeah.



LEO:  I wish we'd done SQRL; but, hey, next best thing.



STEVE:  We got, well, if wishes were fishes or something.



LEO:  Right.



STEVE:  So, okay.  And just a quick note that Mozilla has added support - Mozilla - for post-quantum cryptography to its developer Firefox Nightly builds.  So we'll all be seeing it once the release build is published on the main channel.  It can be enabled, as soon as it's available, by going to about:config and then looking for security.tls.enable_kyber, K-Y-B-E-R.  And the good news is that Firefox's search in that about:config, I mean, remember how long that about:config is.  I mean, it's ridiculous.  The scroll bar just disappears on the screen.  There are so many things that you can tune and tweak.  So you're able to do a substring search.  So you can just put in "kyber," and it would immediately bring you to that entry.



LEO:  Nice, nice.



STEVE:  So anyway, just, you know, a nice forward move for Firefox.  And I've got some feedback to share before we get to the main goodie here.



Jeff Zellen, he said:  "Steve, I've been a listener to Security Now! for quite some time and have really enjoyed and gotten a lot out of your [what he calls the] 'correspondence school'" that we conduct here every week.  He said:  "I wanted to let you know there is a way to get your TOTP tokens out of LastPass.  It's a little Python script that rebuilds the QR codes for you.  It also allows you to print them off, in case you didn't know about the 'Steve Gibson offline backup and storage technique.'"  Which of course is printing all - I have printed out every QR code for every one of my one-time passwords and stapled them together in a sheaf, and they're in a drawer.  And it's come in handy a couple times.



LEO:  Sure, no, it's good to have that, yeah.



STEVE:  When I've needed to bring up a new device.



LEO:  Right.



STEVE:  So anyway, Jeff wrote later to say "I didn't write this.  I didn't mean to say that I wrote this."  Anyway, I've got a link to it.  It's on GitHub.  If you search for lastpass-authenticator-export, you'll find it.  I checked it out, and it looks nifty.  It allows you to regenerate your original QR codes, which you may have fed to LastPass.  And if so, display them, capture them, buy a device that may be starved for them, or print them out.  So anyway, just a cool note.  I wanted to make sure that our listeners knew that was available.  Thank you, Jeff.



Brenty said:  "Re oddly inflated app data," he said, "if you look in iPad or iOS Settings > General > iPhone/iPad Storage, wait for the list to load, and then select an app, you'll see that the size of the app itself is listed separately from its 'documents and data.'"  He said - and this is referring to a question that came up last week.  He said:  "When trying to free up some storage space previously, I found a few apps whose documents and data appeared to be way more than seemed reasonable."



Remember it was that some credit, oh, Credit Karma was occupying a gig of space in some guy's phone.  And he's like, uh, what?  So anyway, Brenty says that he deleted the app, reinstalled it, and it was now at one tenth of the size it had been previously.  And, you know, he said:  "So my theory is that some, maybe many, maybe most, have logging, caching, and likely other unnecessary, stale data that builds up over time, which they simply don't bother to clean up on their own."  So yes.  Deleting and reinstalling would likely save you a lot of space.  And of course I have always found the same is true with setting up a new version of Windows.  He's like, oh, let's just start over again.



Someone whose handle is Mental Calm Today, he said:  "Greetings, Steve.  Long-time SN 'student,' TWiT Club member, SpinRite user."



LEO:  Yay.



STEVE:  He said:  "So excited that you have 6.1 ready for prime time.  I'm reaching out to say thanks for your mention of LearnDMARC yesterday."  So he was tweeting on Wednesday.  He said:  "It's really helpful re a confusing protocol."



LEO:  Yeah.



STEVE:  So this serves as a reminder to me to mention that LearnDMARC website that we mentioned, and that's L-E-A-R-N-D-M-A-R-C dot com, we mentioned and took a look at last week.  It was a huge hit among our listeners, from all the feedback that I've seen.  One person said that the site was offline and suggested maybe that it was because we mentioned it.  Well, that would be flattering except that the nature of a podcast is that the listening is well distributed in time.  So it's not like a purely live event, where we bring websites down by talking about it.  And I guess, what, we used to do that back in the TechTV days, didn't we, Leo.



LEO:  Yeah, oh, yeah.  They called it "slashdotting" a site because Slashdot used to do it.



STEVE:  Right, right, right, right.



LEO:  Yeah, yeah.  It's been a while since we've done that.  Internet's gotten more robust, I think.



STEVE:  Well, and frankly having downloads distributed is a good thing because it's better for everybody.  Ron tweeted:  "Hi, Steve.  This is in regards to Sync.  I messaged them after your item on Security Now!, and this is what I received," he said.  And then he quoted me what Sync responded, saying:  "Hi there, Ronald.  Bailey from Sync here.  Thanks for reaching out.  There was a bug identified within the Sync Mobile App, regarding the iOS Files app integration, which prevented folks from navigating within the Sync folders (Files and Vault) via the Files app.  Users were still able to navigate within the Sync Mobile App.  This Files app integration bug has now been resolved."  There's a link to it.  "Let us know if you have any further suggestions.  Thanks again," writes Bailey from Sync.



So anyway, just a follow-up to that previous listener who was feeling a little despondent because the reply he got from Sync suggested that, well, yeah, so don't do that.  We'll get around to it someday.  You know, that put us all off of Sync a little bit.  It's like, what?  But apparently that was a red herring.  Sync did get on it quickly, and fixed it, and it's back up and running.  So thank you, everybody.



Johnathan Rouse said:  "Hello, Mr. Gibson!  Firstly, you have been a role model for me all throughout high school, college, and now as I redirect my career into education."



LEO:  Nice.



STEVE:  "Thank you for the hours of laughs and education, as well as Leo and the rest of the TWIT Team.  I figured you might want to see the response Windows Defender gave" - and then he cites the version of Windows Defender - "when downloading the 6.1 Pre-Release.  After manually allowing the program, it went along perfectly in creating a USB Boot Drive, but regardless I wanted to show you what I encountered.  I'm hoping the new and improved ISO created will work with Ventoy Bootable Drives as well, and I can't wait to try it out.  Thanks again for all the years of dedication, and I hope to be half the teacher you seem to be in your sleep."



LEO:  You're not sleeping, I want to point out,



STEVE:  So first of all, Johnathan, I can only say, and I know that you, Leo, feel similarly, that I am so pleased that this podcast and TWiT have been so useful to you.



LEO:  You bet, you bet, yeah.



STEVE:  The good news is that since you're just starting out, you have a lifetime of teaching ahead of you.  So I do wish you all the best as you launch into your career.  As for Windows Defender's reactions to SpinRite, yes, it continues to be an annoyance.  But I noted that he sent his Tweet last Tuesday, and things may have become better since then.  Most recent experimentation suggests that Windows Defender is happier.  And as for Ventoy, you will likely have discovered that SpinRite 6.1 and Ventoy are not getting along currently.  But that will be resolved shortly.  I'll have more to say about Ventoy in a minute when I update everybody about SpinRite.



LEO:  Yeah.  Huge fan of Ventoy.  I really like that.  I use it all the time.



STEVE:  So thank you.



LEO:  Good, good, good.  Very nice.



STEVE:  Yeah.  Anotherthomas is his handle.  He said:  "@SGgrc:  About crypto signing camera."  He said:  "It can work if the private key is in a removable HSM assigned to the photographer.  She/he will then able to prove that she/he is the author."



Now, okay, that is some nice thinking outside the box, or in this case outside the camera.  If this were done, it would make the private key about the owner of the key, not about the camera.



LEO:  Right.



STEVE:  And the key is presumably more easily protected by them than having the key locked inside the camera.  You know, you still have to protect the key, but owners would have the incentive to do that since their photographic reputation is on the line.  So anyway, I haven't heard anyone talk about that.  I think that's a very neat idea.



LEO:  It's not the problem that they're trying to solve, though.  They're trying to solve the problem of authenticity of the photo.



STEVE:  Correct.



LEO:  And I just - I have been playing with this content, I think you call it, what is it?  There's a name for it.



STEVE:  Right.  The content protection stuff.



LEO:  Yeah.  And I have it turned on on my camera right now.  And it associates the serial number, I guess, with the name.  I don't, you know - now, you can remove it.  You absolutely can remove it because you can remove any Exif information in a photo by just JPEGing it and, you know, saying don't save the - there's lots of ways to strip off Exif.  But I guess the point is that this is going to be used by news organizations where they aren't going to remove it, and they can provably say this is created by this camera at this time.  And that can't be modified.



STEVE:  Right.



LEO:  You know, I think that's the idea, is that this photo is not a fake, you know, and here's the chain of custody.  It even shows in this information, you know, how I edited it and so forth, you know, what program was used to edit.  I think it shows that somewhere.  Maybe not on this one.  But it does have that.



STEVE:  I know that, if you're using Adobe's tools, which are the only ones that are authorized to do this, then it does absolutely create basically a chain of custody through the editing.



LEO:  Exactly, yeah.



STEVE:  Yes.  And you made a really good point because it's not trying to authenticate the reputation of the - it's not trying to authenticate the reputation of the person who took the picture.  The reputation is assumed, like an accredited, well-known news agency.  Which brings us to the next question that DellAnderson asked.  He said:  "Grateful you're going past 999.  Can't help but ask a basic question about digital camera authentication.  What would prevent a very low-tech workaround where the digital camera - Nikon, Leica, et cetera - takes a perfectly authenticated photograph of a digitally manipulated image?"



LEO:  Ah.  Excellent point.  An analog loophole, yup.



STEVE:  Yup.  "How would this fancy Nikon camera know it was photographing a high-resolution 2D image rather than reality?"



LEO:  It wouldn't.  Very good point.  Yup.



STEVE:  And so I replied to Dell that I had the same thought, as I imagine many of us have.  The problem is that the "authentication," and I have that in quotes, does not and cannot extend out to the actual landscape or subject that's being photographed.  This signing technology is intended to prevent the manipulation of an image's digital recording after it's been captured optically.  But doesn't this beg the question, what's to prevent someone from presenting a fake scene to the camera to capture and then sign.  Okay, now, I understand that this is a different problem.  This is not the problem this camera was designed to prevent.  This camera was designed to prevent undetected post-image-capture manipulation.  And what it was designed to prevent is a significant problem.  You know?



So anyway, I think that what we have to keep in mind is the threat model and what it is we're trying to say.  We are unable to say, Leo, as you instantly got, we're unable to say that the scene that the camera took a picture of was authentic.  What we are able to say is to the best of our ability after the camera took the picture we know exactly what was done to it in a verifiable fashion.  So again, you know, and what's cool about this is we talk about threat models and what we can and cannot assert in the realm of security.  So here's a perfect example of what we can and cannot assert and what we can and cannot protect.



LEO:  Which, by the way, I want to thank you, gave me an excuse to buy a new camera.  So I appreciate that, Steve.



STEVE:  Well, Leo, for that research you had to have that.



LEO:  I had to.  I had to do it.



STEVE:  Absolutely.



LEO:  Exactly.



STEVE:  Yeah.  And if the IRS, you know, ever audits you and says...



LEO:  I'll give them this.



STEVE:  Exactly.  You know, absolutely important that you were able to demonstrate that.  Slartibartphast...



LEO:  I love the name.  You know where that's from.



STEVE:  We know where it came from; right.



LEO:  Yeah, yeah, yeah.



STEVE:  "I wonder if Google needs native iOS engine to make the new ad auction stuff work."  And the answer is, absolutely and without question.  The entire Privacy Sandbox API is a collection of new web browser features, intrinsic to the web browser, that requires a bunch of data storage locally.  I'm sure this is why they have been working on a native implementation for iOS, even though it isn't clear to the outside world how they might get it into iOS.  There is, you know, there's so much that we don't know yet about how we're going to get to where we are today.



Google wants to move the entire world.  And "moving the world" is no exaggeration.  Given that advertising supports the Internet, the required size of this change would be difficult to understate.  Like everything needs to change.  Google already has control of nearly all desktops and Android, which are the majority of smartphones.  So I guess my questions are, what are Mozilla and Apple thinking about this?  What conversations may be going on among them?  Because this is big stuff, and actually this is what we're going to be talking about here as we end today's podcast.



Aeon tweeted - and I know what his first name is.  It's not actually Aeon.  He said:  "Steven, I'm personally inviting you to the Gathering of the Stephvens."



LEO:  Note how it's written.  I love it.



STEVE:  Yes.



LEO:  It's "phv."



STEVE:  Yes.  He said:  "Next year, in 2025, we're going to set a Guinness World Record for the most people named Steven in one area.  First goal, gather the Stephvens in this Discord."  And he provided a link.  "Next goal, conquer the world."  And he said, "You down?"



LEO:  You down.



STEVE:  So I thanked Aeon, whose first name is presumably Stephen, for thinking of me.  But I explained that I was pretty sure that traveling to a massive meeting of people with whom I phonetically share a first name, for the sake of contributing with my presence to the setting of a Guinness Book record is not something that, when the time was approaching, I would be glad I was taking the time to do.  But I told him that I looked forward to hearing more about how it goes, even in absentia.  So, you know, thank you, Stephen.



LEO:  We're having fun creating the regular expression for Stephen with a PH or a V in the Discord.  I think we've got it, actually.



STEVE:  Yeah, curly braces and then...



LEO:  Yes, exactly.



STEVE:  ...a couple brackets and...



LEO:  And an OR.



STEVE:  Yup.  Yup.  Okay.  So we've all seen video segments of complex manufacturing facilities where thousands, if not hundreds of thousands, of cans or something, bottles or boxes or whatever, are moving through a complex system that's sorting and  spinning and stamping and printing or counting or whatever it's doing.  You know, like these crazy-looking manufacturing facilities.  Treadmills and gates opening and closing, routing stuff.



LEO:  I love that stuff.  It's one of the things I love on TikTok is there are a bunch of TikTok videos of how stuff's made.  And it's always fascinating.



STEVE:  Very cool.



LEO:  Always fascinating, yeah.



STEVE:  So just as some of those pre-electronics early computers used banks of mechanical relays, back before the advent of computers, process control engineers, as they're called, would design insanely complex control systems built up from individual mechanical relays.  We would call such a system "discrete" as opposed to "integrated."  Then, blessedly, integrated electronic solutions became cost effective, and these large process control solutions were replaced by PLC systems, Programmable Logic Controllers.



These PLCs were not very smart because they didn't need to be.  Basically they were replacing a bunch of relays.  They were essentially, "If A, then B.  Wait until C, then do D.  And once E, go back to the start."  But being solid-state they were at least more reliable.  Now, remember that we have the term of a hardware or software "bug" because back in 1947 a dead moth, you know, a bug, was found to be the underlying cause of Harvard's Mark II relay computer not working correctly.  Anyway, you know, relays are not as reliable as solid-state because they can actually have bugs.



Anyway, we've talked about these PLCs on this podcast multiple times because attaching them to the Internet has turned out to be a generally really bad idea.  They were never designed for that, and it hasn't been turning out well.  I'm bringing all this up today because I received a long, insightful, and interesting Direct Message from a listener whose thoughts about the problems with PLCs are worth sharing.



Here's what Dylan wrote.  He said:  "Good day.  I'm an engineer and occasionally work with Programmable Logic Controllers.  And I have some thoughts on why these sadly make the news in a bad way sometimes.  I believe most of the problems boil down to two root causes.  Number one, increased demand for 'real-time' data.  Just like the CANbus protocol in the automotive industry, PLCs were invented and took hold in manufacturing when security was not a concern.  As time went on, protocols were developed to have PLCs talk to each other and to advanced peripherals like motor controllers, touchscreens, printers, or even SCADA, Supervisory Control and Data Acquisition computers.



"I believe the demand for telemetry and data aggregation is the real reason most PLCs get exposed, not because remote WAN-side control is needed or used.  I have experienced this.  Management wants to know how many widgets were produced, how fast they were produced, how many passed QC, was there downtime, was it planned, are there idle shift hours, is one shift of operators more efficient than another, and on and on and on."



He says:  "I don't need or want to remotely access a PLC in a machine to change anything about it.  It has done the same job, over and over and over, correctly, for a decade.  But the data the PLC can store and transmit is the reason it's connected to a network and polled every 15 minutes for new numbers.  To satisfy this need, PLC manufacturers are building in web servers, SQL Lite databases, TCP/IP stacks, and a lot of things that have no business being attached to a device based on 1960s technology that has no provision for security.  Again, going back to the automotive comparison, the inventors of CANbus at Robert Bosch company could not have imagined cars would be driving down the road with IP addresses connected to a global network all the time, and would have security flaws that let anyone observe and change CANbus communications inside the vehicle."



And then he says:  "Number two, security-conscious staff are not involved with PLCs.  Even though many consider PLCs to be outdated, at the end of the day they are exactly like an Arduino or similar microcontroller.  They store a program that is executed in a loop at high speed, and the code is evaluated every scan through the ladder logic.  And just a quick plug:  They do this for decades, in terrible environments, with noisy electrical signals, and with fantastic circuit protections.  Reverse the polarity on your Arduino, and you're going to Amazon to shop for another one.  Reverse the polarity on a PLC, not a darn thing happens.  You'll realize you made a stupid mistake, flip the polarity back, and everything works.



"Anyway," he says, "the people who program these are aging out, and I suspect globally fewer people know how to program ladder logic than did 20 years ago.  I'm 36, and I learned to program them 15 years ago; but it seems I'm in the minority in my age group amongst peers in my industry.  My observation is this:  IT people don't understand or want to understand PLCs, and PLC programmers have no incentive or instruction to make the devices secure.  IT staff doesn't consult with the programmers to tell them what security practices they should follow, or review the final configuration of the PLC.  Conversely, the programmer just needs the machine to work; and they are probably fighting numerous mechanical, electrical, and pneumatic problems while completing the programming..."



LEO:  Those pneumatic problems, you know...



STEVE:  Yeah, we had a pneumatic problem.  That's why I didn't get the code working.



LEO:  Do not underestimate those.  They can be a nightmare.



STEVE:  You do not want a problem with your air pressure, no.  "Any extra changes could break the house of cards they've been building.  Imagine everything seems to be working, but all that remains is a communication problem.  Some PLCs have manuals 700 to 1,000 pages long, and various communication features are scattered throughout the PDF.  No organization there.  An inexperienced programmer/engineer who's under pressure to complete the already-late project might just start turning everything on, even if they don't know what it is or what the risks are.



"Require authentication?  Nah, uncheck that box, that could be the problem.  Max number of connections equal one?  Well, I don't know what counts and what doesn't, so let's just set it to 10.  Set admin password?  Better make sure that's blank or default.  Don't want to keep something from connecting.  Oh, and don't change the port number.  That other device over there might be assuming the default port is used, and we don't want to break something that works now and lose ground."



He says:  "Honestly, I don't even think we ever are going to fix this.  Either industries will eventually move to more advanced systems, which is already happening in some cases, like PC-based control with National Instruments LabVIEW or their competitors; or existing older PLCs just need to be kept in a DMZ or well-guarded network segment.  The trouble is, when things aren't broke, they don't get fixed.  So already exposed or at-risk PLCs are just going to be sitting there, connected to networks to harvest data, waiting to be leveraged for attacks.  And these are the things that keep massive swaths of our public utilities functioning."



So Dylan, I think you got all of that exactly right.  And I've said it before, I'm sure this won't be the last time I say it, this podcast has amazing listeners.



LEO:  No kidding.



STEVE:  So thank you, Dylan.



LEO:  There's something cool about PLCs.  Is it kind of writing in assembly language to write to one?



STEVE:  Yeah, it's a very low-level tree logic.  So it's literally if a, then b.  If not, or wait this long, then trigger this.  I mean, it is the thing that moves the arms back and forth in those assembly lines.



LEO:  I'm sure there are high-level interfaces, though, to see or, you know, Forth was originally designed to do that, to program those things.



STEVE:  Well, Forth was designed to aim a radio telescope, yes.



LEO:  That's right, yeah.  And I imagine the aiming mechanism was something like a PLC.



STEVE:  It was definitely, you know, turn motor on, wait till star moves to center, turn motor off.



LEO:  Exactly, yeah, Charles Moore, yeah.



STEVE:  Yup.



LEO:  I love this stuff.  There's something cool about putting your code in a hardware device.



STEVE:  Well, Leo, it's a robot.  Robots are cool.



LEO:  Yeah, very cool. 



STEVE:  So it is cool.  It's cool, I mean, like, the way to motivate grade-schoolers is, remember Logo was the original, you know...



LEO:  Yup, a little turtle logic.



STEVE:  Exactly.



LEO:  Yeah, yeah.  And of course Start is a great way for high school students to get into robotics, the Start competition.  That's, yeah, you're right, that's cool.



STEVE:  Yeah.  I think the idea - and I think also that's where, what is that world that you create, oh, Lego blocks thing?



LEO:  Yeah, yeah, Roblox, yeah, Roblox.  They're absolutely learning that kind of logic in Roblox.  Exactly what they're learning, yeah.  Man, I wish I, you know, I wish I had another 50 or 60 years.  I'd like to really get into some of this stuff.  Very cool.  Very cool.



STEVE:  Okay.  So lastly, just quickly on the SpinRite front, last week I rewrote GRC's code signing system.



LEO:  Oh.  You just rewrote it in a week.  No bigs.



STEVE:  Well, I knew how it worked by then.  It took me a month to get it working the first time.  But yeah, I did rewrite it because the way I had done it, which was to build the code signing into GRC's server code, had not proven to be 100% reliable, and it needs to be.  It turned out that when I was restarting the server, the code signing system did not like that restart.  So that was a problem.



Anyway, so I redesigned the system under a client/server model, where we now have code signing as a service.  The code signing service runs in the background, with the web server being the service's client, sending it files to be signed.  And so far I'm feeling really good about it.  It came up.  It worked the first time.  And it has been flawless ever since.  It has never stumbled or had a problem.  So this feels like exactly the right solution.  Oh, and in the process I was able to switch the signing from using an SHA-1 over to SHA-256.  So that feels better, too.



Now, SpinRite's paint continues to dry nicely.  One popular tool - which I think is the right way to put it.



LEO:  I like it, yeah.



STEVE:  One popular tool for carrying around and booting ISO image files is something called Ventoy; which, Leo, you obviously are a fan of.



LEO:  Yes.



STEVE:  When I initially heard someone report that SpinRite 6.0's ISO files worked fine with Ventoy, but the various pre-releases of SpinRite 6.1 did not, I planned to eventually get around to looking into what was going on with that.  That's the sort of thing one does while the paint is drying.  So once I got the signing system redesigned and apparently finally working perfectly, I took a look at Ventoy, which I've never used since I don't do a lot of portable ISO image booting.



LEO:  Yeah, it's widely used for things like having 20 Linux distros on a single USB key, that kind of thing.



STEVE:  Which you are welcome to, yes.



LEO:  Well, here's a good example.  I would love to have SpinRite plus the Windows installer on a single USB key and be able to switch between the two; right?



STEVE:  Right.  So I brought myself up to speed on Sunday.  It is a very slick open source project and tool.  It's installed onto a USB thumb drive.  Then you simply drop ISO files into its directory.  When that drive is then booted, it presents a list of the ISO files it found and allows its user to select any of them to be booted.  So I certainly understand its appeal for anyone who wants to carry a toolkit around on a thumb drive.



LEO:  Right.



STEVE:  Okay.  Anyway, it turns out that the DOS environment Ventoy creates does not have - or the PC machine environment that DOS boots into doesn't have the HMA.  That's the High Memory Area.  Now, okay.  The High Memory Area is one of the cleverest hacks ever invented.



LEO:  Underscore "hack," however.



STEVE:  It is a hack.  It is a 64K memory segment that starts at FFFF, the last 16-byte paragraph of the machine's first 1MB of RAM.  Since memory in a segmented memory model is referenced by a positive offset from the start of a segment, starting a segment at FFFF allows for accessing 64K, minus 16 bytes, past the 1MB megabyte point.  In other words, this allowed PCs still running in Real Mode to access an additional 64K of RAM, when they were only supposed to be able to access a megabyte.  It's actually a megabyte plus 64K minus 16 bytes.  Anyway, it is a neat hack that the PC industry came up with and adopted in the later years of DOS, and all recent DOSes have been able to load themselves and their buffers into that region in order to leave more conventional memory available for their programs to run.



Since the DOS execution environment created by Ventoy does not provide that, it forces DOS to load low, and it turns out that there is just barely insufficient RAM left over for SpinRite 6.1 to run.  And I mean just barely.  It turns out that the slightly smaller size of an unsigned version of SpinRite, which is a few K smaller, does run, as easily does the much smaller DOS-only SpinRite executable.



So after today's podcast, I'm going to tweak the Windows component of SpinRite, which is why we let paint dry, just a bit so that the bootable ISO image it builds will contain SpinRite's 81K DOS executable, rather than the full 250K hybrid DOS and Windows executable.  That smaller SpinRite for DOS should then run without any trouble under Ventoy.  And a bootable ISO has no need for the full larger Windows version anyway.



In the meantime, nothing new, not one new bug has appeared in the last several weeks, despite the fact that more than 1,000 people have downloaded and have been using the pre-release, this release candidate 6 of 6.1.  So I'm going to continue to let its paint dry while I work to get this new SpinRite documented online, then on bringing up GRC's email system.  And at that point we'll start letting everyone know that it is ready for primetime.



LEO:  Very good.  How exciting.



STEVE:  That is very exciting.  And Leo, let's tell our listeners about the advertiser they're excited to hear about.



LEO:  I will.



STEVE:  Then we're going to...



LEO:  Do something exciting and fun.



STEVE:  Or something, look at something very disturbing.



LEO:  Uh-oh.  Well, you want something disturbing?  I got a real story just came in.  Three million malware-infected smart toothbrushes have been used in Swiss DDoS attacks.  These toothbrushes, I have one at home, have RAM.  They have a processor.  And apparently they're hackable and have been enslaved, that's a little bit of an inappropriate word, into botnets.  Conscripted, how about that, into botnets and used in DDoS attacks.  Can you believe that?  This is from Tom's Hardware.  Thank you, Tom's Hardware, for that...



STEVE:  I do.



LEO:  ...dystopian vision.  You might want to secure your toothbrush.  I don't know how you would do that.  You can't take - I guess they're online.



STEVE:  They're online, Leo.



LEO:  I don't know what you can do to - you know?  On we go with the show.  And the scary part is now...



STEVE:  Okay.



LEO:  This is for grownups, this part.  Yes?



STEVE:  So, yeah.  Everybody knows how bullish and excited I am about Google's Privacy Sandbox.



LEO:  Yes.



STEVE:  We all know I'm a bit of a fanboy for technology.  And this is a bunch of very interesting new technology that solves some very old problems.  Google clearly understands that their economic model is endangered due to the fundamental tension that exists between advertisers, primarily themselves, who demand to know everything possible about the viewers of their ads; and those viewers, along with their governments, who are becoming increasingly concerned about privacy and anonymity.  The emergence of Global Privacy Control and the return of DNT, Do Not Track, has not gone unnoticed by anyone whose cash flow depends upon knowing something about the visitors to their websites.



As we've been covering this through the years, we've watched Google iterate on a solution to this very thorny problem.  And I believe, though the final solution was to transfer the entire problem into the user's browser, that they've found a solution that really can work.  But, and this is a huge "but" that informs today's title topic, it appears that the rest of the world does not plan to go down without a fight.  Not everyone is convinced.  Apparently not everyone believes that they're going to need to follow Google.  And it turns out that there is a workaround that is not good.



So a recent Financial Times headline read "Amazon strikes ad data deal with Reach as Google kills off cookies," which was followed by the subhead "Media sector scrambles to deal with fallout from phaseout of cross-website trackers."  So with a little bit of editing for the content for our listeners, the Financial Times writes:  "Tech giant Amazon has struck a deal with the UK's largest publisher, Reach, to obtain customer data to target online advertising, as the media industry scrambles to respond to Google's move to axe 'cookies.'  In one of the first such agreements in Europe, Amazon and Reach unveiled a partnership on Monday designed to compensate for the loss of 'third-party' cookies that help gather information about users by tracking their activity across websites to help target advertising.



"Google said this month that it had started to remove cookies on its Chrome browser, following a similar move by Apple to block them over Safari, aiming to switch off all third-party cookies by the end of the year.  Reach said it will partner with Amazon on sharing 'contextual' first-party data, for example, allowing advertisers to know what articles people are looking at, with the U.S. tech group using the information to sell more targeted advertising on the UK publisher's sites.  The companies said the deal comes 'as the advertising world tackles deprecation of third-party cookies, a long-anticipated industry milestone that Google kick-started in early January.'  Financial details for the arrangement were not revealed.



"The partnership involves the contextual advertising of Mantis, originally a brand safety tool that could ensure that brands were not being presented next to potentially harmful or inappropriate content.  The tool is also now used to place ads next to content users may want to see, helping to better target specific audiences with relevant advertising.  Other publishers also use Mantis.



"Amazon Ads director of EU adtech sales Frazer Locke said that:  'As the industry shifts towards an environment where cookies are not available, first-party contextual signals are critical in helping us develop actionable insights that enable our advertisers to reach relevant audiences without sacrificing reach, relevancy, or ad performance.'



"The loss of cookies means that almost all Internet users will become close to unidentifiable for advertisers.  The risk for advertisers is that their advertising offer becomes much less valuable at a time when they're already losing ad revenues, which has led to thousands of job cuts in the past year.  Reach last year announced 450 roles would be axed.



"Other media groups are also looking at deals involving their customer data, according to industry executives.  Some publishers are experimenting more with registration pages or paywalls that mean people first give first-party information that they can use, such as email addresses and logins.  Reach is already seeking to harvest more such data from readers.



"Jon Steinberg, chief executive of Future, said that the 'elimination of third-party cookies is one of the biggest changes to the advertising market in the digital age.'  He added that 'advertisers and agencies will be looking to publishers that have high-quality editorial, scale, and rich first-party data,' and predicted that 'advertisers, agencies, and quality publishers will work even more closely together to reach audiences that drive outcomes for brands.'



"Sir Martin Sorrell, chief executive of advertising firm S4 Capital, said that some clients that did not have access to first-party data on their customers were panicking.  He said that there would be more focus on getting customers to sign up to websites with their information as companies attempted to boost their stores of 'consented data.'"



Okay.  So let's think about this for a minute.  This notion of requiring more user sign-ups is interesting, and it's not something that had occurred to me before.  This article makes it clear that the advertising industry is not going to let go, and go down without a fight.  They don't want to change.  They don't want to adopt Google's strongly anonymous interest-based solution.  No.  They want to continue to know everything they possibly can about everyone, which is something Google's dominant Chrome browser will begin actively working to prevent, at least using the traditional tracking methodology.  So what are they going to do?  And what's up with this signing into sites business?



It occurred to me that one way of thinking about the traditional presence of third-party tracking cookies is that because they effectively identify who is going from site to site on the Internet, there's no need for us to explicitly sign up when we arrive somewhere for the purpose of identifying ourselves to the site and its advertisers.  Cookies do that for us, silently and unseen, on our behalf.



Who we are when we visit a website is already known from all of the cookies our browsers transmit in response to all of the transparent pixels and beacons and scripts and ads that laden today's typical website.  But soon, all of that traditional, silent, continuous, background identification tracking is going to be prevented, and the advertising industry is finally waking up to that reality.



What this means for a website itself is significant, perhaps even drastic, a reduction in advertising revenue, since as we know advertisers will pay much more for an advertisement that's shown to someone whose interests and history they know.  That allows them to choose the most relevant ads from their inventory, which makes the presentation of the ad that the viewer sees more valuable, and thus generates more revenue for the website that's hosting the ad.  And that's, of course, been the whole point of all this tracking.  That's why websites themselves have never been anti-tracking, and it's the reason so many websites cause their visitors' browsers to contact so many third-party domains.  It's good for business, from the website's perspective, and it increases the site's revenue.  And besides, visitors don't see any of that happening.



So tomorrow, when visitors swing by a website with Chrome, which no longer allows tracking, and those visitors are therefore anonymous and far less valuable to that site's advertisers, how does a website itself deanonymize its visitors to know who they are for the purpose of identifying them to its advertisers so that those advertisers will pay that site as much money as possible?



The answer is horrible and is apparently on the horizon.  The website will require its visitors to register and sign up before its content - and its ads - can be viewed.  At the end of that Financial Times piece, they quoted Sir Martin Sorrell, the chief executive of advertising at S4 Capital, saying "some clients that did not have access to first-party data on their customers were panicking, and that there would be more focus on getting customers to sign up to websites with their information as companies attempt to boost their stores of consented data."



Now, these websites won't be charging any money for this signup.  It's not money from their visitors they want.  It's the identities of those visitors that, for the first time, they need to obtain from that first-party relationship in order to share that information with their advertisers so that they can be paid top dollar for the ads displayed on their websites.  And you can be 100% certain that the fine print of every such site's publicly posted privacy policy will state that any information they obtain may be shared with their business partners and affiliates, meaning the advertisers on their sites.



We thought those cookie permission pop-ups were bad, but things might soon be getting much worse.  And those "signup to create an account" forms may also attempt to obtain as much demographic information as possible about their visitors.  You know, "Oh, while you're here creating an account, please tell us a bit more about yourself by filling out the form below so that we can better tailor our content to your needs and interests."



Uh-huh.  Right.  Such form-fill will likely be a one-time event per browser, since a persistent first-party logon cookie will then be given to our browser to hold and return to the site.  So it will only be a brief hassle once.  But the result of filling out a form to create an account at every site which might begin to require one will be that our visits to that site will no longer even have the pretense of anonymity.  We will be known to that site, and thus we will in turn be known to every one of that site's advertisers.



We may forget that we have an account there, or we may find our name shown in the upper right-hand corner of the screen with a menu allowing us to logout, change our email address, our password, et cetera.  And password managers are likely going to become even more important because typical Internet users will be juggling many more Internet login accounts than they've ever needed before.  Historically, we only ever needed to logon to a site when we had some need to create an enduring relationship with that site.  That is what promises to change.  Sites with which we have no interest or need to be known will begin insisting that we tell them who we are in exchange for access to their content, even though it'll be free.  And the reason for their insistence will be that we become a much more valuable visitor once they're able, in turn, to tell their advertisers exactly who we are.



And it's all perfectly legal because no tracking is happening.  We sign up and implicitly grant our permission for our real-world identities to be shared with any and all of that site's business associates.  Most people will have no idea what's going on.  Maybe it won't actually be that big a deal.  It won't be obvious why sites they've been visiting for years are suddenly asking them to create an account.  They already have lots of other accounts everywhere else, and the site won't be asking for money, just for their identities, which most people are not concerned about divulging.



One thing we can be certain of is that a trend of forced identification before the content of an advertising-supported website can be viewed will cause the EFF to have a conniption.  Nothing could ever be more antithetical to their principles.  The EFF wants nothing short of absolute and complete anonymity for all users of the Internet.  So this represents a massive step directly away from that goal.  The EFF would be well served, in fact, to get behind Google's initiative, which is far more privacy-preserving than this end-around that appears to be looming.



It almost makes third-party cookie tracking look attractive by comparison.  I don't want to be forced to create accounts for every low-value website I might visit briefly.  If this happens, it's going to change the way the Internet feels.  It's going to be interesting to see how all this shakes out.  And yes, I am more glad than ever to be going past Episode 999 since it's going to be very interesting to be observing and sharing what comes next.



LEO:  Completely agree.  Our mission has really just begun.  For a long time, the last five years I thought, well, we've kind of done it all, you know.  How much fun is there in the newest iPhone or whatever.  But no.  I think times are getting very interesting, actually.



STEVE:  Yeah.



Copyright (c) 2024 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#961

DATE:		February 13, 2024

TITLE:		BitLocker: Chipped or Cracked?

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-961.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  What's the story behind the massive incredible three million toothbrush takeover attack?  How many honeypots are out there on the Internet?  What's the best technology to use to access your home network while traveling?  Exactly why is password security all just an illusion?  Does detecting and reporting previously used passwords create a security weakness?  Will Apple's opening of iOS in the EU drive a browser monoculture?  Can anything be done to secure our router's UPnP?  Has anyone encountered the "Unintended Consequences" we theorized last week?  Are running personal email servers no longer practical?  And what's up with the recently reported vulnerability in many TPM-protected BitLocker systems?



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  Lots to talk about.  Yeah, that three million toothbrush DDoS attack thing, maybe that wasn't exactly how it happened.  Steve has the details on that.  Why is password security really just security theater?  Yikes.  And then we're going to talk about what probably many of you heard about, the BitLocker hack.  Is it something you should worry about, and what can you do about it?  Steve's got a very simple fix.  You'll want to listen to this episode for sure.  Security Now! is next.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 961, recorded Tuesday, February 13th, 2024:  "BitLocker:  Chipped or Cracked?"



It's time for Security Now!.  Yes, you wait all week long for this moment.  I know you do.  Steve Gibson is here, the man in charge, with the latest security news.  Hello, Steve.



STEVE GIBSON:  Hello, Leo.  Great to be with you.  This is the 13th, which is regarded as a sort of an unlucky number, at least in the West.  I know that China's got a whole bunch of numbers.



LEO:  Oh, yeah.  Eight is lucky.  I can't remember what the unlucky ones were.  But yeah, yeah.



STEVE:  Yeah.



LEO:  Thirteen, though, this is the day before Valentine's Day, so it's only unlucky if you haven't bought Lorrie a gift yet.



STEVE:  Oh, the best thing about my having - actually, she chose me more than I chose her - is that she could care less.



LEO:  Oh, good.



STEVE:  Absolutely.  I have to, like, you know, by the way, honey, it's your birthday.  What?  Oh.



LEO:  Love that.



STEVE:  So it's wonderful.



LEO:  Love that.



STEVE:  Yes.  I've been - in earlier years of my life this has been that Valentine's Day was my most hated day.



LEO:  It's terrifying.



STEVE:  Oh, and because, you know, girlfriends were comparing what their friends' boyfriends or husbands did.  And it's like, well, he did more than you did.  It's like, oh, god, just shoot me now.



LEO:  You can't win.  You can't win.  You can't win.  So what's in the docket for today's show?



STEVE:  We have a mostly listener-driven show because, as I was going through the incoming from our listeners, they expanded into some really interesting discussions.  So we do of course have what's the story behind the massive incredible three million toothbrush takeover attack.



LEO:  Oh, I'm so sorry I brought that up last week.



STEVE:  No, no, you were right where the rest of the Internet was.  



LEO:  I got suckered.  I got suckered with everyone else.



STEVE:  We were on the leading edge of a fiasco.



LEO:  Although if I had just used my mind, my noggin, I would have realized how hard to believe it was.  But anyway, you'll get to that.



STEVE:  Well, okay.  So there's some interesting stuff that went on with that.



LEO:  Oh, good.  Okay.



STEVE:  Also we're going to look at how many honeypots are out there on the Internet?  It's more than you might think.  Also, what's the best technology to use to access our home networks while we're traveling?  Exactly why - get this - is password security all just an illusion?



LEO:  Oh, no.



STEVE:  Oh, yes.  Does detecting and reporting previously used passwords create a security weakness?  Will Apple's opening of iOS in the EU drive a browser monoculture?  Can anything be done to secure our routers' really problematic UPnP, you know, the Universal Plug and Play?



LEO:  Oh, yeah, I just turned that off.  You told me to turn it off, so I turned it off.



STEVE:  Yay.  Yes.  Has anyone encountered the "Unintended Consequences" we theorized last week?  The answer, uh-huh.  And even I have.  Are running personal email servers no longer practical?  And finally, what's up with the recently reported vulnerability that afflicts, affects many TPM, you know, Trusted Platform Module-protected BitLocker systems?



LEO:  Oh, boy.  Yeah, that's a big one, yeah.



STEVE:  So today's topic or today's podcast titled "BitLocker:  Chipped or Cracked?"  So I think we have another great podcast for our listeners.



LEO:  Well, this would be a good time maybe to mention that you should use a password manager.  I don't know what Steve's going to say, but passwords aren't completely an illusion.  The Picture of the Week.  I am ready to scroll it up now, though, if you're ready.



STEVE:  Yes.  This one, I gave this one the title "Your municipal tax dollars, hard at work."  



LEO:  Uh-oh.  Now, John, I can't show this at this point because you haven't given me a switch for the computer screen.  So I guess, people, you're just going to have to look at your show notes.  Here it is, "Your municipal tax dollars, hard at work."  Oh, I wish you could see this.  Maybe, Steve, you're just going to have to describe it for us.



STEVE:  Okay.  So, well, I always do for our listeners who are driving and commuting and jogging or whatever it is they're doing.  They're listening rather than viewing.  This is just another one of those insane, like, what are they thinking?  So we have a street corner.  We're zoomed in on just one corner, like where you would have sidewalks.  And it looks like a rural community.  We see something in the background with a couple trailer homes and some parked cars and some, you know, screens and things.  It looks like rural U.S.



And, okay, now we have it on the screen, for those who are watching the video.  And there's like a patch of sidewalk concrete, and the curb is dropped down to street level so that if you're rolling up with a wheelchair, you'll not have to go over the bump, or you're using roller skates or whatever.  There's even that special textured, in this case it's bright pink and kind of nubbly rubber on the leading edge so that I guess if you're not sighted, or maybe in a wheelchair, you're able to sense that you're on the edge of the sidewalk.  The problem is, this sidewalk extends maybe a yard?  Maybe three feet?  And then there's this big sign sticking up that says "End of Sidewalk."  Because, well, I mean, it's correct.



LEO:  At least you got up the curb okay.



STEVE:  But I just, you know, Leo, you look at this.  And, I mean, the pictures we've been showing recently, there just has to be a story behind them.



LEO:  I'll tell you the story.  This is malicious compliance.



STEVE:  Yeah.



LEO:  This is complying with ADA regulations.  But the problem is, and this is the same way in Petaluma, I don't know what it's like where you are, but developers don't want to put in sidewalks.  So if they're not absolutely required by city regulations to put in sidewalks, they won't.  And this is an example of they had an ADA compliance feature, which is a curb cut.  But they didn't have to put in a sidewalk, so they didn't.



STEVE:  So it's not a sidewalk, it's a sidestep.



LEO:  Yeah, there's no point.



STEVE:  Because basically you take one step, and then you're done.



LEO:  It's ridiculous.



STEVE:  And I was thinking, maybe the corner is there to join another sidewalk running in the other direction, but it looks like - but there's grass along there, and it looks like this is aimed in only one direction of entry from the street.  Anyway...



LEO:  It's the least they can do, literally.



STEVE:  And in case you weren't sure, Leo.



LEO:  Yes?



STEVE:  The sidewalk has ended because there's grass.  So there's a "End of Sidewalk" sign posted.



LEO:  I like that.  That's a really useful piece of information, yes.



STEVE:  Yeah, because you wouldn't know otherwise.



LEO:  You would if you were in a chair.  You would immediately sense the change in terrain.  Good lord.  Oy.



STEVE:  Wow.  Wow.  Okay.  So just as we were recording, and I did give this one the title "Brushing Up on the Facts," just as we were recording last Tuesday's podcast, news was breaking across the Internet that somewhere around three million electric toothbrushes had all been compromised and had been enslaved into a massive global botnet.  And more, that actually it had been used to attack a Swiss firm, blasting them off the Internet completely.



LEO:  And I owe you such an apology for breaking into the show, breathlessly relaying this story.  I should have known better.



STEVE:  Why isn't that in your notes, Steve?  Well, gee.



LEO:  I should have known better.



STEVE:  So The Independent's - but Leo, really, you were in good company.  The Independent's headline was "Millions of hacked toothbrushes used in Swiss cyberattack, report says."  Fudzilla, well, that's well named, their headline was "Hackers turn toothbrushes into cyber weapons."



LEO:  Oh, boy.



STEVE:  Boing Boing headlined:  "Millions of smart toothbrushes used in botnet attack on company."  Even ZDNet, and they actually made it even worse with the way they ended their headline, ZDNet's headline:  "Three million smart toothbrushes were just used in a DDoS attack.  Really."  Well, not really.  Even Tom's Hardware:  "Three million malware-infected smart toothbrushes used in Swiss DDoS attacks.  Botnet causes millions of euros in damages."  We even know, Leo, what it cost...



LEO:  We don't.  It was made up.



STEVE:  ...the company that was attacked.



LEO:  Oh, god.



STEVE:  And finally The Sun, The Sun reports:  "Over three million toothbrushes 'hacked' and 'turned into secret army for criminals,' experts claim."



LEO:  Now, in my defense, this news headline came over the wire as you were doing the show.



STEVE:  Yes.



LEO:  But I should have used some critical thinking because I have one of those toothbrushes.  They're not connected to the Internet.



STEVE:  Right.  They have Bluetooth.  They do not have WiFi.



LEO:  They do connect, I mean, I guess you could in theory hack them because they do connect to your phone.  So if you had a malicious app...



STEVE:  So it could jump from the phone into the toothbrush.



LEO:  Yeah, and then you had a malicious app that then...



STEVE:  And this really brings a whole new meaning to the notion of disinfecting your toothbrush.



LEO:  Yes, yes.  But there's not enough power in there.  There's not enough memory.  And most importantly, there's no WiFi.



STEVE:  Well, there were many, many similar reports, hundreds, yet none of them of course were true.  Highly respected news outlets repeated the story because, well, talk about clickbait.  Oh, goodness.  So how exactly this massive reporting screw-up came to pass even today remains a little unclear.  I should note that all of the responsible reporting, for example Tom's Hardware I think has had three updates since then and like really been diligent in rolling this thing back and correcting their own record.  So everybody who did this, you know, said whoops, and fixed it.



But I think that, well, part of the problem is, in following up and following back this trail, the parties who were directly involved still to this day disagree about who said what to whom.  This occurred during an interview with, well, I've got the details here, so I'll explain it without quoting myself, or misquoting myself.  But what was originally published was certainly hair curling, if not teeth straightening.



So here's what the world read.  This was in the original article:  "She's at home in the bathroom.  She's part of a large-scale cyberattack.  The electric toothbrush is programmed with Java, and unnoticed criminals have installed malware on it and approximately three million similar toothbrushes.  One command is enough, and the remote-controlled toothbrushes simultaneously access the website of a Swiss company.  The site collapses and is paralyzed for hours, resulting in millions of dollars in damage."



LEO:  I'm so embarrassed.



STEVE:  "This example, which seems like a Hollywood scenario, actually happened."



LEO:  No, it didn't.



STEVE:  "It shows how versatile digital attacks have become."  Yes, even your toothbrush is not safe.  Stefan Zger, head of the Switzerland offshoot of the cybersecurity specialist firm Fortinet, said:  "Each device connected to the Internet is a potential goal, or can be misused for an attack.  Whether baby monitor, web camera, or the electric toothbrush, the attackers do not care."



So the day after hundreds of media outlets worldwide repeated the false claim that a botnet of three million toothbrushes had attacked a Swiss company, Fortinet, the now quite embarrassed cybersecurity firm which was at the center of the story issued a statement.  They said:  "To clarify" - yeah, let's get a little clarification.  "To clarify, the topic of toothbrushes being used for DDoS attacks was presented during an interview as an illustration of a given type of attack, and it is not based on research from Fortinet or FortiGuard Labs.  It appears that due to translations, the narrative on this topic has been stretched to the point where hypothetical and actual scenarios are blurred."  Wow.  Give that PR person a raise.  That's just, well, the hypothetical and the actual met in the middle, and we're not sure where one ended and the other one started.  And after all, it was lost in translation.  Right.



So Fortinet went on to say that its experts have "not observed Mirai or other IoT botnets targeting toothbrushes or similar embedded devices."  Now, Graham Cluley, who's been following this whole mess, the day after that day after, on Thursday the 8th, Graham wrote:  "I can imagine how a Fortinet researcher might have regaled a journalist with tales of how IoT devices like webcams could be hijacked into botnets for DDoS attacks.  After all, this has happened.  However, giving the journalist a juicy hypothetical example of millions of smart toothbrushes taking down a Swiss company is playing a dangerous game."



He says:  "I'm not surprised that journalists seized the story; and as we've seen, then other news outlets repeated it without double-checking its truth.  A more experienced spokesperson would have gone to pains to make it clear that the toothbrush DDoS attack example was hypothetical and had not actually happened.  Failing that, since the original article was published" - get this - "on January 30th, Fortinet had plenty of time to contact the Swiss newspaper and correct the report, or post a clarification on social media debunking the story as the hysteria spread in the press.



"But Fortinet did not do that until skeptical voices in the cybersecurity community questioned the story.  Ironically, Fortinet's researchers have published some genuinely interesting proof-of-concept research in the past on the toothbrush topic, albeit hacking Bluetooth-enabled toothbrushes to mess with brushing time rather than knock a company's website offline."  So anyway, many of the various publications that were forced to update, amend, and retract what turned out to be an erroneous story, took the time to add that, you know, like trying to cover themselves a little bit while, yes, whoops, this didn't actually happen, it was still an entirely possible and even likely scenario.  Except of course on toothbrushes that only had Bluetooth it actually wasn't.



And of course that may also account, you know, the fact that we're prepared for this, that could account for the fact that everyone rushed to submit the story.  You know, even though it was not true, it carried the ring of truth for any tech publication since, as everyone listening to this podcast knows, routers and security cameras and IoT devices of all makes, models, and functions are indeed being compromised and enlisted in botnets daily.  It's not science fiction, even though this particularly intriguing story was pure fiction.  So anyway, Leo, again, no harm, no foul.  And I may have picked up on it if my newsgathering had been a little later in the day than it turned out to be.  Because this, as you said, just happened as we were beginning the podcast.  So there.



Okay.  So I got a kick out of the blog post headline posted at the VulnCheck website.  It read "There Are Too Many Damn Honeypots!"  So here's what the VulnCheck guys explained.  They wrote:  "Determining the number of Internet-facing hosts affected by a new vulnerability is a key factor in determining if it will become a widespread or emergent threat.  If there are a lot of hosts affected, there's a pretty good possibility things are about to pop off," as they put it.  "But if only a few hosts are available for exploitation, that's much less likely.  But actually counting those hosts, turns out, has become quite a bit more challenging."



They said:  "For example, take CVE-2023-22527."  So that's last year.  "This affected the Atlassian Confluence servers."  They said:  "At the time of writing, Confluence has appeared on CISA's KEV" - K-E-V, the Commonly Exploited Vulnerabilities list - "nine [yes, nine] times."  They wrote:  "That's a level of exploitation that should encourage everyone to get their Confluence servers off the Internet.  But let's look for ourselves.  There are a number of generic Confluence Shodan queries floating around, but X-Confluence-Request-Time might be the most well known.  This simply checks for an HTTP response header being returned."



In other words, okay, so breaking from them for a second, as we know, the Shodan Internet search scanner is constantly scanning the 'Net and aggregating the presence of hosts on the Internet.  Who's listening to what port on what IP?  And in the same way that Google indexes the Internet so that it's easy to find a site by search terms, Shodan indexes the Internet so that you're able to find vulnerable or at least present services by IP and type of service.  So it's a search engine for stuff that's listening on ports.



So Shodan can make an HTTP query to Confluence's service port; and if the reply coming back from that port contains the reply header "X-Confluence-Request-Time," that strongly suggests that there's a running Confluence server answering queries at that IP and port.  So the VulnCheck guys then show a Shodan screen capture showing, get this, 241,702 occurrences of that reply header being returned from queries across the Internet.  Then they point out one particular thing.  They say:  "241,000" - it's a little more than that - "hosts," they said, "is a great target base for an emergent threat.  But on closer examination, there's something off about the listed hosts.  For example, this one" - and they select one - "has the Confluence X-Confluence-Request-Time header, but it also has an F5 favicon," you know, as in the well-known security firm F5 Systems.  Uh-huh.



LEO:  Uh-huh.



STEVE:  And, they say:  "It also claims to be a QNAP TS-128A."  You know, a NAS device.  They say:  "This is a honeypot," you know, because it's arranging to look like a bunch of things in order to attract flies.



LEO:  I've got to tell you this is something that our sponsor would never have done.  They have so much, so accurate, and they don't put their little logo in it, and they don't impersonate more than one device.  So this is not the Canary, obviously.  This is some other...



STEVE:  Right, right.  Well, and I was thinking about this, too.  Canaries are not meant to be publicly exposed.



LEO:  That's right.



STEVE:  They're for your LAN in order to detect intrusion.



LEO:  That's what you want.  You don't - yeah, exactly, yeah.



STEVE:  Yeah.  There was no reason you would stick it out there just to take incoming from the Internet.



LEO:  We know there's bad guys out there.  We don't have to attest to that.



STEVE:  Yeah.  What we want is to find out if any of them get inside.



LEO:  Mm-hmm.



STEVE:  So the VulnCheck guys say:  "Whoever created this honeypot was somewhat clever.  They mashed together the popular Shodan queries for Confluence, F5 devices, and QNAP systems, to create" what they described as "an abomination that would show up in all three queries.  To avoid throwing exploits all over the Internet, and thus getting quickly caught, some attackers use Shodan or similar to curate their target lists.  This honeypot is optimized for this use case."



LEO:  Oh, interesting.



STEVE:  "Which is neat.  But it blocks our view of what is real."



LEO:  Right.



STEVE:  "Can we filter them out of our search?"  They say:  "At this point, it's probably useful to look at what a real Confluence server HTTP response looks like.  The server has a number of other useful headers to key off of, but we'll try to filter by adding in Set-Cookie: JSESSIONID=.  That update brings the host count down."  Okay, so now they're saying - so they modify their Shodan query so that they want it to have both that very popular X-Confluence-Request-Time header, and to be setting a cookie named JSESSIONID=.  So they're doing an AND on those two requirements.  And they write:  "That update brings the host count down from 241,702 to just 37,964, so just shy of 38.  And they call that 'probably-actual Confluence servers publicly exposed to the Internet.'"



But is that number real?  They say:  "It still seems high because most of those do not respond with an actual Confluence landing page.  A simple way to capitalize on that is to also search for a snippet from the Confluence login page in our search criteria."  So they add another term to the Shodan query looking for the returned HTML to contain the phrase "confluence-base-URL."  And they say:  "Ah, now we're down to 20,584," a little over half as many as before they added that additional term.  And they write:  "This knocks off 17,000 hosts, and things are looking more Confluency.  But there seems to be a whole bunch of entries without favicons.  Let's drill down into that one and see."



So they do that, looking for the presence or lack of any favicon for the site.  And at one point it occurs to them to examine the value being returned in the Confluence JSESSIONID cookie settings reply header.  And what do you know.  A great many of those across the Internet have identical values.  Meaning they're not being generated dynamically.  They're part of some fixed Confluence simulating honeypot, and the simulation took some shortcuts, that is, the simulation of the honeypot took some shortcuts, for example, randomizing the JSESSIONID which gives it away when it's examined closely enough.



By applying this spoofed JSESSIONID filter, the number now drops to 4,187 probably authentic publicly exposed Confluence servers.  So again they write and conclude.  They said:  "A quick investigation suggests that this could be the complete set of real Confluence hosts, or just very, very good honeypots."  They say:  "That's a reduction from around 240,000 hosts all the way down to just 4,200.  That means there are approximately 236,000 Confluence honeypots on the Internet, or more than 50 times the actual number..."



LEO:  Of Confluence users.



STEVE:  "...of real Confluence servers."



LEO:  I'm thinking, well, that's interesting.  Why do people want to do public honeypots?  I don't get that.



STEVE:  Right.  Just, you know, just probably to see.  Anyway, they say:  "A vulnerability that only impacts 4,000 hosts is much less concerning than a vulnerability that impacts 240,000.  Understanding the scale of an issue, and therefore being precise about the number of potentially impacted hosts is important, too.  Those who copy overinflated statistics or haven't done their due diligence are making vulnerabilities appear more impactful than they truly are."  Uh, three million toothbrushes, anyone?



"Anyway, while we focused on Confluence," they said, "this particular problem has been repeated across many different targets.  Honeypots are a net good for the security community.  But their expanding popularity does make understanding real-world attack surfaces much more difficult for defenders, not just attackers."  And Leo, I really think you raise a good point.  You know, we're talking a quarter of a million.



LEO:  That's a lot of them.



STEVE:  Bogus Confluence servers.  What?  You know, you're right, that's - I don't know that there are that many bad Russians.



LEO:  Not as much fun to be a hacker as it used to be.  I just...



STEVE:  So anyway, this will be a very good rule of thumb for us to keep in mind moving forward.  Academically, it's interesting that the explosion in honeypot use and population is this large.  I mean, it's like, what?  Who are all these people?  You know, that's sort of astonishing.  But this means that the tendency to immediately rely upon and believe the results of a simple, not-very-critical Shodan search for a given open port - assuming that that means there's a truly vulnerable service running there - needs to be significantly tempered.  And it also suggests that future Internet vulnerability scanners will themselves need to do a better job of filtering out the honeypots since the problem has obviously become nothing less than massive.



LEO:  And it might be worse even than that because these were not well configured honeypots.  I mean, any hacker worth his salt would have immediately noticed the Fortinet or the F5 icon and the fact that it was both a QNAP and, I mean, that's a little bit, you know, the whole thing doesn't ring true.  And I would think most bad guys, except for script kiddies, would be sensitive to that and watching out for that.  There are probably many, many, many more that they can't see because they're well configured.  They look just like a real Confluence server.



STEVE:  Yup.  Yup.  Leo, let's take our next break, and then we're going to plow into some user-driven, really interesting discussions.



LEO:  And again, I'm sorry about the toothbrushes.  They're just Bluetooth devices.  They can't...



STEVE:  Leo, I'm glad to know you're taking good care of your teeth.  Teeth are very important.



LEO:  Yes, they are.  I should have just paid more attention.



STEVE:  You've got a hi-tech toothbrush.  Hopefully it hasn't been hacked to have its running time reduced because...



LEO:  We will get to him.  Someday we will kill him with tooth decay.  It may take a few years.  And now on with the show, Steve.



STEVE:  So Dextra tweeted:  "Hello, Steve.  Thank you for introducing me 13-plus years ago to the world of being security-minded from a tech perspective.  I travel a lot, and over the years I've been working on trying to come up with a solution where I can appear on my home network so I can access and watch content on my cable provider's app while being secured with the least amount of possibility of opening my home router up to external threats.  I've a Synology RT2600ac router at home.  I've recently started to travel with a Beryl travel router.  I do have an extra Synology RT2600ac router that I've traveled with in the past.  Do you have any suggestions on how to go about appearing to be on my home network in a secure manner so I can access my cable provider's catalog and live TV?  Signed, RM."



Okay.  So this has changed over the years.  Ten years ago the standard generic answer would have been to arrange to set up a VPN server at home, and then VPN into your home network from afar.  That's no longer the optimal solution.  Among other things, it's often more easily said than done, and it requires opening a static port through your home router, which is then visible to anyone on the Internet, like Shodan, not just you.  While there are ways to do this safely, it's no longer necessary thanks to the widespread availability of many free and terrific overlay networks.



The very early such network we talked about many years ago was Hamachi.  It was originally free, then it went paid, and then it was purchased by LogMeIn.  It's still possible to use LogMeIn's Hamachi for $50 per year.  But many free solutions exist, and they're just as good:  Nebula, which was done by the Stack people; Tailscale; and ZeroTier are three of the very popular ones.  Since I didn't know anything about Synology's RT2600ac router, I went over to Michael Horowitz's astoundingly useful and comprehensive "RouterSecurity.org" site.  It's, you know, as the name sounds, RouterSecurity is one word, dot org.  There's just so much stuff there.  His site allowed me to quickly learn that the router has some possible use as a VPN client, it builds in a VPN client, but it doesn't appear to be general purpose enough to host - the router itself doesn't appear to be general purpose enough to host an overlay network, which any Raspberry Pi can do, for example.



So this would mean that, when traveling, some machine inside your home network would need to be left running.  But, as I said, that could just be a Raspberry Pi serving as a quiet, always-on, fanless network node to anchor the overlay network.  Then you'd run another node on your laptop, and all of these things are multiplatform, so whatever OS you're carrying it'll be compatible.  And then you'd be all set.  Essentially, your laptop and your cable provider's catalog and video streaming would see that you were connected to them from home, and this is just trouble-free.



Now, as I've mentioned before when I've talked about overlay networks and these various ones, I get people saying, okay, well, which one do you recommend?  I can't recommend one because I have not had the need nor the chance to do this myself since I've not been traveling.  But the next time I'm going to be out and about, I will make time to check out the various overlay network solutions.  I can say, however, that the response from our listeners who have bitten the bullet and set up overlay networks has been, like, gob-smacked positive.



I mean, they can't believe, they just, you know, they can't believe that it is that simple to obtain world-class security in cross-device networking through the public Internet, which is anything but secure.  So, you know, the day has truly arrived when it no longer needs to be difficult in order to do that.  You just have to poke around.  There's, you know, YouTube is full of how-to videos on overlay networks.  Again, Nebula, Tailscale, and ZeroTier are top of the list.



LEO:  Seems like Tailscale is very popular.



STEVE:  Yes.  Yes.



LEO:  So that would be my first guess.



STEVE:  So Evan, wow, I can't pronounce his name, Phyillaier - sorry, Evan.  Anyway, he said:  "Hey, Steve.  Love the show.  I run an ecommerce site, and my customers have been asking for an easier way to log in.  I was wondering if there are any security considerations for going passwordless via email only.  The system I would like to set up is registration and login via email, i.e., customer just enters their email and then receives a six-digit code in their email to authenticate and log in.  Is this just as secure as email plus password authentication?  Thanks."  So I thought that was a really interesting and intriguing question.



LEO:  Yeah.



STEVE:  So let's answer the last question first:  "Is this just as secure as email plus password authentication?"  At first we might be tempted to answer, "No, it cannot be as secure since we've eliminated the 'something you know'" factor from the login.  But of course that's a red herring; right?  Since, as I've often noted, every login everywhere on the planet always and without fail has the obligatory "I forgot my password" link.



LEO:  Right, right.



STEVE:  And, sadly, we're now also seeing "I can't use my authenticator right now."  Like, oh, my god, that annoys me.  It's like, what?  So you don't need that really yet, either.  It's kind of like, well, yeah, how about if you have it?  Wow.  And, you know, and I've even made this notion of the ever-present email link into a joke.  You know, where someone explains that they don't need no stinking password manager while they're creating an account by just mashing on their keyboard to fill-in their password field.  And when they're asked, "But, but, but how do you login again later?" they glibly explain that they just click on the "I forgot my password" link, then click on the link in their email that they receive, and they're logged in.



The point, of course, is that so long as all username and password logins include the "I forgot what I was supposed to remember" get out of jail free link, our ownership over and control of our email is the only actual security we have.



LEO:  Sad to say, yeah.



STEVE:  The rest is just "feel good" security illusion.  This in turn means that the service the password and the password manager are actually performing is only "login acceleration."  If your password manager is able to supply the password quickly and painlessly, then the much slower "I forgot my password" login process, which is always available using an email loop, can be bypassed.  So it's login acceleration, which is good.  As Bruce Schneier would probably describe it:  "The password is just security theater."



LEO:  Oh, god.



STEVE:  So calling passwords a "login accelerant" is the perfect context to put them in.



LEO:  This is so important.  Please, everybody, clip that paragraph, that previous paragraph, and send it to everybody.  Because we've said this many times.  The weakest link is always the real determinant of how much security you have.  And if there is a "forgot my password," that's the weakest link.  That's the security you've got.



STEVE:  Yup.  So let's return to Evan's question:  Is emailing a one-time passcode to someone who wishes to login just as secure as using a password?  It should be clear that the correct and defensible answer is yes.



LEO:  It's identical, actually.



STEVE:  Yes.  If the users of his ecommerce site do not wish to be hassled for a password, there is no reduction in security to eliminating passwords entirely and just using an email loop.  However, there's also no need for even a six-digit code, since that does not provide any additional security, and it's more hassle which Evan and his users are wishing to avoid.  What Evan wants to verify is that someone who is wishing to login at this moment is in control of their previously registered email account.  Remember, that's the same fallback test that's being used by every login challenge in the world.  This means that all Evan needs to do is email this user a direct login link which contains a one-time passcode as a parameter.  And since the user no longer needs to transcribe it, the passcode can be as long as Evan wishes.  32 digits?  No problem.



The only requirement for security is that the code must be unpredictable and only valid the first time it is used.  Okay.  So how do we do that?  Let's design the system for Evan.  We'll start with a monotonically increasing 32-bit counter.  That'll be good for 4.3 billion logins before it wraps around.  Now, you can make it 64 bits if you like, so that the most significant 32-bit counter is incremented if the lower 32-bits should ever overflow, even though that would seem to be quite unlikely.  And actually, since we're going to put a timestamp in this design also, even if you did have 4.3 billion, and it finally came around to the same, you would not have a valid timestamp in any event.



Okay.  So we have a binary value which will never repeat since it's a simple counter that only ever counts upward.  And it's stored non-volatile by the server so that it takes like in the registry or in a file so that it writes it back and always starts, even after a reboot, with the next count from where it had left off.



Okay.  So we can do several things with that, always incrementing binary value.  It could be fed into the AES Rijndael Cipher which is keyed with a random secret and unchanging key.  That secret's known only to the server.  It's also, you know, it might be coded into it or also written somewhere so that it's non-volatile, it never changes.  Then the Rijndael is a 128-bit block.  So the 128 bits that comes out of the cipher, basically we have a random secret key which is going to encrypt our 32-bit counter into a 128-bit result.  That you run through a Base64 converter, those are available in every language, which produces 22 ASCII text characters.



Since the encryption key will never be changed, and the input to the cipher is an upward-counting counter, the output will never repeat, and it will be cryptographically unpredictable.  So we've met several of our conditions.  Unpredictable.  It never occurs again. 



So, just to explore the territory, you could take a salted hash with a secret salt.  The counter value would be hashed, and then the hash's output would be similarly converted into text using Base64.  Now, it's true that there's an infinitesimally small chance of a hash collision where two different counter values might produce the same output, but any good hash will be cryptographically secure.  And remember that any single bit which changes in the hash's input will, on average, change half of its output bits.  So collisions there would really not be a problem.  But no reason not to use Rijndael.  That's kind of cool anyway.



Okay.  So now we have a 22-character one-time token.  Evan's ecommerce system should append that token to the link that's sent in the email to the individual who has just asked to log into his system.  The instructions in the email are to simply click the button in the email.  They do that.  This confirms that someone who provided the email address is receiving email at that address, and they are instantly logged in.



At Evan's end, when the token is obtained and the email is sent, those two items along with a timestamp are added to a "Pending Login" list, a list in the sense of a linked list in programming terms.  Anytime someone clicks a link, the list is scanned searching for a matching token.  The objects on this "Pending Logins" list should use a timestamp so that they are self-expiring.  And the way I've organized this on my own expiring lists, of which I have many over in GRC's server, managing all the DNS stuff and ShieldsUP! and everything, and of course this is technically called a queue, is that as I'm traversing the list from its start, I'm also checking the timestamps for every object that I encounter, whether or not they match the one I'm looking for.



If that object's timestamp has expired, I delete it from the list right then so that the list is self-pruning.  When I get to the object whose token matches, and if its timestamp has not expired, this confirms the login.  I accept the inbound link and log this person in and remove that little object from the list.  It would remove itself after it timed out anyway, but might as well, you know, keep it clean.



So anyway, this simple system gives us everything we want.  We have unpredictable self-expiring single-use tokens - oh, and that's the other reason to remove it from the list.  As you're honoring it and the login, you delete it from the list so that anyone who might capture it somehow is unable to log in again using that token, which is meant to be single use.  Evan's users no longer need to mess with a password.  They simply go to a login page, enter their previously registered email address, click the "email me" button, open the email that they received, click the button, and they're in.  No passwords to worry about, and every bit as secure, actually, as if a password were being used.



If you have a password manager, then you have - you're able to use, on sites that support passwords, you're able to use that as an accelerant to logging in.  But it doesn't make you any more secure.  And you could argue, if it's a poor password, it could make you even less secure.  And that's the danger; right?  Passwords that are bad allow bad guys to brute force.  If you don't have a password, there's nothing to brute force.  So you can make the argument that a passwordless login is even more secure than a system that did have passwords.  Yikes.  But a really great question, I thought.



LEO:  Yeah.  I mean, yeah, really you've got me thinking.  A lot of...



STEVE:  It's counterintuitive; isn't it.



LEO:  Yeah, Medium uses that.  They don't have passwords.  It's sort of annoying because it means I have to go to my email every time I want to log in.



STEVE:  Exactly.  And Evan is suggesting that his users would rather do that than have to remember a password. 



LEO:  So I'm seeing that more and more often on sites like Medium where you just don't set up a password.  You just say "email me."  Micro.blog does that, too.



STEVE:  Well, and we're about to encounter that because what you're describing is the unintended consequence that was last week's topic of sites asking for your email because they want to replace first-person tracking because third-person tracking is going away.



LEO:  Right, right, right, right.



STEVE:  Anyway, we'll get there in a second.  Margrave said:  "Hey, Steve.  I've been a loyal listener since the early days.  And though I'm not a security expert, I work in Software Quality Automation and have found the Security Now! podcast incredibly helpful several times.  I recently created a LinkedIn article and was given the option to share it on social media.  When I chose Facebook, I encountered an interesting situation.  I remembered changing my password, but it struck me as odd that Facebook would notify me about it."  And in his message to me he included a screenshot of what he encountered where it's a Facebook popup, says log into your Facebook account to share.  And then it says "You entered an old password.  Your password was changed about two weeks ago.  If you don't remember making this change, click here."  And then it prompts him for his current password.



So then he continues:  "I'm not entirely sure if this is a positive or a negative feature for Facebook.  Sure, Facebook is often filled with a lot of random stuff like pictures of cats in sunglasses, chickens wearing hats, breathtaking sunsets from someone's backyard, and other equally ridiculous images.  But this made me ponder the implications of such notifications. I'm curious to hear your thoughts, as well as those of other listeners, on this feature Facebook is offering.  



"I'm also eagerly awaiting SpinRite 6.1.  It's been a fantastic tool, and I appreciate all the other facets of your podcast, including your involvement with Vitamin D3.  Best regards, Tom."



LEO:  Part of this is because Facebook, which was originally for college kids - exactly 20 years ago, by the way, it launched - is now primarily for old folks, people like you and me, who forget our passwords, who change our passwords and forget we changed our passwords, things like that.  And who are often, often hacked.  I think Facebook accounts are most often hacked.  I mean, very, very common.



STEVE:  So I don't see, to answer Tom's question, any downside to this.  And given that Facebook, exactly as you said, Leo, caters to the people who are taking and posting those images which do not impress Tom, I can see the merit in reminding someone when their password was changed, and then for whatever reason they entered their earlier password.



LEO:  I think more sites are going to be like this, to be honest with you, as we age.



STEVE:  Yeah.  Yeah.  And in fact, you know, to demonstrate that, in Tom's case this was useful to him.  He did recall having changed his password several weeks before, but for whatever reason he entered his earlier password.  The alternative to having Facebook helpfully saying, hey, you entered an old password, would be "Sorry, that password is incorrect."  This would be more confusing than having Facebook recognize and helpfully report that the password was the user's attempted use of an earlier password.  And I don't know whether multiple people in a household routinely share a single Facebook account.



LEO:  Oh, that's a good point, yeah.



STEVE:  But if so, one of them might have changed their shared password and failed to inform the others.  So this would be a huge help in that instance.  The only problem I can see would arise if Facebook were to honor Tom's use of his retired password, but that would obviously or hopefully never happen.  So I don't see any downside.  And we know that those really annoying systems require their users to periodically change their passwords for no reason, and then also refuse to allow any recently used password to be reused.  And, you know, they are - so that means they are similarly storing previous password hashes.  So the practice of remembering previous password hashes is not new.  I think this amounts to a useful and user-friendly feature.



LEO:  And secure, which is what he absolutely worried about.



STEVE:  Yes, exactly.  I don't see any problem for security.



LEO:  Good.



STEVE:  Gimix3, he says:  "Hey, Steve.  I've been thinking about this thing that now we'll be able to choose our browser in iOS.  And whilst I'm excited to be able to run Firefox in my iPhone, I'm feeling a bit uneasy.  Safari, by being imposed on iOS and the default on macOS, has gained popularity over the years, and has been 'too big to ignore' until now.  Are we going back to the days of the hegemony of Chrome, and websites that can only be used on Chrome?"



So I thought about this for a while, and I would say that it's really up to the other browsers.  All of the standards that Chrome, obviously and currently the global dominant browser for here and for the foreseeable future, the standards that Chrome is using are open, open source, and available for adoption by anyone.  It may indeed be that if they wish to retain what market share they can, they will need to adopt the same set of open standards that Chrome has.  These next few years are going to be really interesting.  The only place where Apple is being forced to allow third-party browser engine cores is the EU.  And we know that Apple is infuriated by this interference with their sovereignty over their own platform.  So it seems unlikely that Apple will similarly be opening their devices to other browsers elsewhere.



Also, the Internet as a whole appears to finally be maturing, waking up and sobering up a bit.  We're seeing things tightening up everywhere.  Advertising is pulling back.  Sites that never had a clear and justifiable reason for their own existence, yet were carrying a huge overhead with a plan to, well, make it up in volume, they're disappearing.  What a shock.  So in today's climate, I cannot see anyone willfully turning away visitors who come surfing in from any platform.  Perhaps internal corporate sites might force their employees to use some specific browser in order to run their poorly designed software that will only run on a specific browser.



But that's their fault.  That is never going to happen as a general rule.  No matter what happens on the platform side, especially with the web standardization process so well established today, I doubt we're ever going to see any public sites, certainly none that plan to survive, telling their users that they must go get another browser.  That's - I think those days are over.  And really those were written by, or those days were largely back when browsers were incapable of doing everything.  And so it was go get Flash, download Flash if you want to use this site.  And as we know, entire sites were once written in Flash, which, you know, was crazy.  So any browser that wouldn't run Flash, wouldn't be able to run that site.



Barbara says:  "It occurs to me that the third CISA recommendation might address Universal Plug and Play issues.  If UPnP is on, and malware tries to open ports, the user would be notified; right?"



Okay.  So Barbara's referring to CISA's third recommendation which we discussed last week, about configuration changes to the router or network device requiring a manual - oh, changes which affect the security requiring a manual intervention by the user of some kind, like them going over and pressing a button saying, you know, enable me to make changes to this router.  And she raises a very good point about UPnP, which we know is a real security problem.  But I'm afraid that's not what CISA was referring to, and there's really no good way to deal with that particular problem.  UPnP is so ubiquitous that all of today's routers enable it by default out of the box.  Otherwise things break.  And since it's not the router's fault when UPnP is abused, there's no downside for the router to default to having it enabled, as they all do.



The last thing any router manufacturer wants is for some online reviewer to write up that they swapped in this router, and a bunch of things that were working before broke.  You know, the fact that it broke because the router's more secure will be lost on the audience.  So the value of UPnP for providing hands-free connectivity, which is what it does, is that it needs no management interface.  That, you know, it's just magic.  Unfortunately, its magic is black, and it is certainly prone to abuse because it allows anything on the internal network, without any authentication barriers of any kind, to create static incoming port mappings to whatever devices are chosen.  Because of UPnP's totally freewheeling nature by design, there's no way to require any sort of manual intervention.  Today's networked devices just expect UPnP to be there, and for their network traffic to be able to come and go as they please.  And unfortunately, secure it is not.



Guillermo Garca said:  "Hey, Steve.  Listening to SN-960 and your explanation on the reaction and workaround to Google's Protected Audience solution.  I have two comments.  If this registration requirement is widely adopted, I'm wondering how that will affect the indexing spiders that index the web for us.  And then I wonder what kind of password reuse nightmare will emerge if a login is required for every website on the web."



I thought those were two good points.  And the second of those two questions occurred to me last week.  If we're being asked to create what are essentially throwaway accounts just for the privilege of visiting websites, then why not use a throwaway password?  Come up with something that probably meets modern password requirements and reuse it for sites that just don't matter.  The problem, of course, is that there will probably be some tendency to keep using that password even on sites that are not throwaway.  So this reuse for convenience is instilling a very bad habit, which, you know, we spent the last decade training everyone out of.  And this would also render our password manager's "web checkup" features useless since they would be freaking out over all of our deliberate password reuse.



As for spiders, I hadn't considered that.  And I wonder how that works today, since news sites behind paywalls appear to be indexed.  One thought would be that the user-agent header which identifies a spider might be checked by the site.  But of course that would be easy for anyone to spoof in order to get past the paywall, just like the spider does.  I suppose that the IP address blocks from which spiders crawl are likely well known and fixed.  Or you could do reverse DNS on the IP to see if like it's coming in from Google, from a Google.com property.  And of course IP addresses cannot be spoofed.



So it would be possible to admit incoming requests from a set of previously well-known IP ranges without requiring a gratuitous login first.  But having said that, there's really no reason why spiders could not just log in like everyone else.  I'm sure that, assuming this comes to pass, the problem of keeping the web indexed will be solved somehow.  And what we're about to learn is that it turns out no password required.  And that's the solution, just like you mentioned, Leo, for Medium.



Earl Rodd tweeted:  "Regarding 'Unintended Consequences' and websites requiring an account to view their content.  I first encountered this a few weeks ago and wondered why.  It clearly was not a paywall.  Now I understand why."  Referring to last week's podcast.  He said:  "In fact, no password is needed since it's not an account, but merely a way to track me.  They did verify that my email was a real one.  So the friction for a user is minimal.  Really nothing to remember except my junk email which I have for such purposes."  He said:  "P.S.:  The site was foxnews.com," he says, "one of the several entertainment sites I look at to see the going narratives related to the news."



Okay.  So first of all, it's very interesting that no password is needed.  And Earl is correct, the only thing they really want and need is our email address.  That's what they're trying to get.  I went over to Fox News and poked around a bit, and I was not initially prompted for anything.  I noticed in the URL bar that Firefox was saying that I had given the Fox site some special permissions of some sort.  It turned out that I had disabled autoplay, and audio was blocked on that site.  So I cleared any cookies that Firefox might have been carrying.



And then, sure enough, I got the same thing Earl reported.  I grabbed a picture of it by myself for the show notes.  And it's a box that says "Join Fox News for access to this content."  It says:  "Plus get unlimited access to thousands of articles, videos and more with your free account."  Then there's a form to fill in, just a one-liner, "Enter your email," and then a Continue button.  And then in the fine print below it says "By entering your email, you're agreeing to Fox News Terms of Service and Privacy Policy, which includes our Notice of Financial Incentive."  That's bold.  "To access the content, check your email and follow the instructions provided."                       



Okay.  So they do that page fade effect where you can see the top of the story, but it fades to white and so that it becomes unreadable while this box appears.  I was curious about the notice.  So in other words, you can't really continue reading the story until you've entered your email, clicked a button, gone to your email, clicked the link there to confirm your email address.  All of that gives your browser a cookie which is now tied to your email address.  So every time you come back in the future they know who you are.  So I was curious about this "Notice of Financial Incentive" they referred to.



LEO:  Yeah, what could that be?



STEVE:  So I followed the link which brought me to the following disclosure.  Under Notice of Financial Incentive it says:  "This notice applies to our offers or programs (each an 'Incentive Program') that link to this section of our privacy policy."  Okay.  And of course the page blocking that email that brought me here linked to this, so it applies to what we just did; right?  "And which California may consider to be a financial incentive.  You can opt-in to participate in an Incentive Program by providing your email address or other personal information.  In exchange for providing your personal information and depending on the incentive program in which you participate, you may be able to access certain content, features, or events, receive a discounted price on an applicable subscription, or receive special news alerts or other entitlements.  We will, in turn, use your personal information for the purposes set forth in this privacy policy, such as sending you alerts and marketing messages and personalizing your experience, including providing advertising you may find more relevant and interesting.



"To the extent we can estimate the value of your personal information to us, we consider the value of the offer such as special content or features, the cost to us associated with providing the offer" - in other words, right, it's a net zero, it's a net equal - "and the potential benefit to us in the form of additional advertising or other revenue we may receive as a result of you using our services.  The value to us, if any, will depend on the extent to which you engage with our services."  Which, boy, you know, some attorneys made a bunch of money putting those couple paragraphs together.  Basically what this amounts to is you've given us your email address, which we're going to use to enrich ourselves.  And the more time you spend here, the richer we get.



LEO:  Yes.



STEVE:  And, you know, all the sites that are doing this say the same thing.  It's very clear that this is exactly what Earl, who first encountered this, suggested it was.  I don't visit the Fox News site often enough to have appreciated this as a change of behavior recently, but apparently Earl does, and it changed for him.  This is new.



LEO:  Yeah, it's probably the - the CCPA I bet is that financial, the California Privacy Act is the financial one.



STEVE:  Right.



LEO:  But yeah, it makes sense.  You know, we don't need a pass - you don't need no stinkin' password.



STEVE:  Nope.



LEO:  Just give us your email.  That's all we ask.  I see that a lot, by the way.



STEVE:  Yup.  As he put it, "I first encountered this a few weeks ago and wondered why."  You know, it's not obnoxious, and the lack of any request for a password makes it much less obnoxious.  So it looks like we have a perfect example of last week's topic, the unintended consequences of trying to take tracking away from an industry that does not want to let it go.



LEO:  Exactly.



STEVE:  Everyone who fills out these "Join our site" online forms, aside from subjecting themselves to an ever-increasing torrent of spam, will be receiving a completely legal and legitimate first-party browser cookie to uniquely identify them to the site and tie it to their email address.  So long as their browser returns that cookie during that and subsequent visits, they will be seen as a "member" of the site so they won't be bothered again.  This is a one-time deal.  However, yes, the site - with members come advantages; right.  The site will in turn forward the visitor's email address to all partners, including all advertisers on that site...



LEO:  You bet, you bet, yup.



STEVE:  ...who will effectively be paying them, be paying the site for that information.  Before I had switched away from the site, by the way, uBlock Origin's blocked access attempt count was up to 98 different domains.



LEO:  Oh, my god.  That has to be a record.



STEVE:  98.



LEO:  Holy cow.



STEVE:  98.



LEO:  Oh, boy.  You're going to start getting - there's more ads that'll sit in your email, I think.



STEVE:  Yup.  There's more evidence of this.  As I was researching the title story for today, the Trusted Platform Module BitLocker decryption story, I scrolled down on the PCGamer.com site, and I encountered exactly the same thing:  "PC Gamer Newsletter.  Sign up to get the best content of the week, and great gaming deals as picked by the editors."  And there it is.  And then there's two checkboxes that were not default checked, which I at least appreciated.  That was "Contact me with news and offers from other future brands" and "Receive email from us on behalf of our trusted partners or sponsors."  Yikes.



And then same fine print:  "By submitting your information, you agree to the Terms and Conditions and Privacy Policy and are aged 16 or over."  So one thing I didn't mention last week during our discussion of this is that, if anyone doesn't yet have a throwaway junk email account, now would certainly be a good time to establish one.  No site to whom we provide this email address will be respecting our privacy.  That's the entire point of obtaining our email address.  It's so that our privacy can be more explicitly ignored than ever before.  And note that we are also implicitly agreeing now to every such site's Privacy Policy, which should be renamed their "Lack of Privacy Policy."



LEO:  Yeah, and you know, even if you use a burner email, they don't care.  It's a fingerprint.  So, you know [crosstalk].



STEVE:  Yup, exactly.  It all ties back to you.



LEO:  It's marginally better, I guess.  Now, I have to ask you one thing.  In the screen shots, I see a LastPass icon.  Are you still using LastPass?  I thought you stopped using LastPass.



STEVE:  That's a good point.  On this computer I think I must not have done it.



LEO:  You forgot to install it.  Okay.



STEVE:  Yeah.



LEO:  All right.  Oh, that's your screenshot computer.  It's probably not your main machine.  Okay.



STEVE:  Right, right, right, it's off the 'Net.



LEO:  On we go with TPM, Mr. Gibson.



STEVE:  Okay.  So Tom Walker tweeted:  "Hi, Steve.  Years ago you mentioned that you leave your phone plugged in all the time.  Do you still do that?  Just curious if, in your experience, that has kept the phone battery healthy?"



LEO:  Ah.



STEVE:  Now, I do keep my phones charged up all the time.  I have an iPhone X that is stuck to an electromagnetic charging stand at either my day or evening location.  Otherwise it's in my pocket when I'm out or between locations.  But the moment I return home I walk right to the charger, and it docks.  Separately, I also keep an older iPhone 7 that my wife retired right here next to me as my desk phone.  And as we can see in the video, it is never unplugged.  It is essentially a corded phone.  And I have three iPads which I use daily.  Each, similarly, is always plugged in.



Now, I can't claim to have any clear experimental evidence that this helps the batteries to live longer.  The science all says that today's lithium-cycle batteries do not like to be deep discharged.  But neither do they like to be overcharged.  No, they don't like that at all.  They much prefer to be kept nearer to their fully charged state.  And I assume that Apple understands all of this and is doubtless very careful not to overcharge their devices, so leaving them connected is safe.



One thing I can say is that my devices always outlive their batteries.  That is, I never have batteries die on any of my things.  So there's one data point.  Another is that I have a friend of many years who used to allow his Apple devices to discharge fully before plugging them in.  He was remembering the previous Nickel-Cadmium battery admonishments to always deep-discharge that type of battery chemistry, NiCad, to avoid the famous "memory effect" where NiCads that were only partially discharged a little bit before they were recharged would start thinking that they were empty at that point where they had been recharged.



Anyway, he killed one Apple device after another until I noticed - I mean, and he complained to me.  He says, "These darn Apple - these batteries are no good."  And I noticed that his battery symbol was red and, like, screaming for his attention.  It was in pain.



LEO:  Oh.



STEVE:  And I explained that plugging them in at every opportunity is the way to treat Lithium-cycle batteries.



LEO:  I think also you can trust companies these days to manage their batteries very well, especially Apple.  Because Apple actually has traditionally smaller batteries than some of the other companies in their phones.  And so they're very - they're constantly tweaking everything to make sure they get max [crosstalk].



STEVE:  Well, yes.  But the problem is, if you refuse to plug it in, and you insist on using it, there's nothing Apple can do.



LEO:  There's no way around that.  Nothing they can do about that, if you're going to discharge all the way, yeah.  I guess what I'm saying is my advice is generally to people just let the - don't worry about it.  Let the phone do its thing.  Devices these days are pretty good about all that stuff.



STEVE:  I would say, I mean, it took a long time to train my wife to plug her phone in if there was no reason not to.



LEO:  Right.



STEVE:  That is, it's better to have it on power than not because, you know, if you don't have the habit, it's easy to leave it lying around.  Then you grab it when you're running out the door, and it's already low, and it's not going to last long.  And it's just not good for it.  So anyway, for what it's worth, I just, you know, I keep everything charged up.



Mark Jones said:  "Hey, Steve.  I know you get no spam, but would like your advice on email deliverability.  I too am an old-timer and maintained websites with email for decades.  You have not commented on how hard email deliverability is in the age of SPF, DKIM, and DMARC.  You also haven't offered advice about maintaining your own email server.  February 2024" - that's now - "marks changes for both how Google and Yahoo regard appropriate settings.  What's your take?  Costs continue to escalate for services that interpret delivery failure events.  EasyDMARC was free for multiple sites at one point, now only free for one.  And paid plan is more than I pay per month for shared hosting.  Is it time to give up running email off my own domains?"



Okay.  So I've not commented upon the difficulty of email delivery in the age of SPF, DKIM, and DMARC because I've not yet tried ramping up GRC's rate of mail delivery.  I do run my own email server, and it fully supports all three sender verification standards.  They are all configured and running, and GRC has been trickling email in and out of its domain for years with never a hint of any trouble.  So it occurred to me there's some chance that I may have already established more of a positive reputation than I was worried might be needed.  You know, it's not as if the email that will begin coming from me will emerge from some never-before-seen domain, and suddenly bulk email starts going out.  So anyway, we'll see how it goes.  And I will absolutely 100% share everything I encounter along the way.



But Mark concluded his note with the question, "Is it time to give up running email off my own domains?"  And I think that's a question only he can answer.  But from what he mentioned of escalating costs for something called EasyDMARC, for example, it doesn't sound as though he's running his own email server.



LEO:  Yeah.



STEVE:  So he's incurring additional service costs.  I am running my own email server, so I have zero cost associated with hosting email environments.



LEO:  Well, and I'm going to, you know, our sponsor Fastmail will do all DKIM, DMARC, and SPF for you on your domains, by the way.  I have my own - all my email comes to my own personal domain.  I don't have Gmail or Yahoo or Outlook, anything like that.  It's all Leoville or whatever. 



STEVE:  Right.



LEO:  And I do all the - the MX records are all through Fastmail.  They do all of the authentication.



STEVE:  Nice.



LEO:  I don't see any reason - I think if it's asking the question should I run my own server, only if you're Steve Gibson.  You've got to be nuts to run your own email server.  That's just, for one thing, you don't use - you're not using consumer IP addresses.  Anybody who has an ISP-based IP address, forget your email ever getting through.



STEVE:  Yeah.



LEO:  You lease Level 3 addresses; right?



STEVE:  Right.



LEO:  You have commercial addresses.  And you've been using them...



STEVE:  I have a block of 24.



LEO:  And you've been using them for so long that they've never been used for spam.



STEVE:  Right.



LEO:  So you're not on any blacklist or, I mean, this is - Steve is an unusual case.  Very few people should be running their own servers.  Domain's different.  Servers, don't do that.  Don't do that.  That's a great [crosstalk].



STEVE:  I like it.



LEO:  That's great.  Well, you're fine now; right?  Because for whatever reason, you know, those addresses are safe.  And you're doing all the right authentication, so you're fine.



STEVE:  Right, right.  Max Feinleib said:  "Thank you so much for sharing @abrenty's tip about checking iOS app sizes."  He says:  "I just deleted over 10GB off my phone..."



LEO:  Yeah, these sizes are giant.



STEVE:  "...in what seems to be nothing but cruft."



LEO:  It's terrible.



STEVE:  So for anyone who's interested, remember, you know, go through, look at the sizes of the data.  And if it doesn't make sense to you, delete the app and reinstall it.  And none of that crap will come back.  Really, Leo, it really is wrong.  I'm surprised that Apple doesn't have like a space cleaner.  On the other hand, they don't mind selling you larger memory for more money.



LEO:  Yeah, I mean, a lot of the other stuff is stuff like attachments in your messages and things.  And those get big.  And, you know, they're not going to delete those willy-nilly.  They're going to, you know, presume that you want them until you delete them.  And I wish they did have a way of doing it, but they don't, yeah.



STEVE:  Yeah.  Andre Arroyo said:  "@SGgrc" - this was a public tweet.  He said:  "SpinRite 6.1 Release Candidate 6 running directly on my old iMac and booting off USB."  He said:  "I couldn't do this before.  Now it's easy.  Thanks for SpinRite  and Security Now."  And I put a big picture in the show notes just because it was very cool to see SpinRite sitting there proudly on his iMac screen.



LEO:  Now, how does he do that?  That's an advanced tip.



STEVE:  Yeah, you know, if you're able to boot from USB, that's all it takes.



LEO:  Okay.  And run - this is obviously an Intel iMac.  It's not...



STEVE:  It's got to be an Intel iMac, yes, yup.  



LEO:  Because you have to be able to run this - you're still using FreeDOS right now; right?



STEVE:  Yup.  Yup.



LEO:  But next time it'll be this other DOS that you own, practically.



STEVE:  The RTOS.



LEO:  As the last user.



STEVE:  I bought it as the sink was shipping.  As the ship was sinking.



LEO:  As the ship was sinking.  Hey, I'll take it off your hands.



STEVE:  Wait a minute.  I'll buy it.  I'll buy it.  So speaking of SpinRite, I am, as I had hoped, at work on SpinRite's documentation now while SpinRite's paint continues to dry.  For example, one user in GRC's forums who had a dying SDHC SD card with a large non-critical file, wanted to experiment with its recovery.  So here's what he wrote.  He said:  "Hi, Steve.  I'd like to know if there is a way to have SpinRite perform an operation like a Level 2 scan multiple times.  The reason I ask is that I have a Samsung 32GB SDHC card that has a couple of spots it cannot read or write to.  I was able to copy all the files except one large one off it."  He says:  "(An MP4 phone video I took that's not important).  And I've decided to play with it to see if it's recoverable.  The card passes the SpinRite Level 2 test, but does not pass Level 3 in two areas where I get a 'Device Fault' error.



"The really interesting thing about this is that in running Level 2 a number of times, I've been able to 'heal' some of the bad spots and increase the amount of the file being copied using Windows from 60% to 86%.  My thought is, if I was able to have SpinRite do the Level 2 scan overnight multiple times, it might just heal any remaining bad spots."



Okay.  So the first thing I explained in my reply to him was that SpinRite can now be completely controlled from its command line.  So it would be possible to start it with a command that will bypass any user interaction, select the proper drive and processing level, run SpinRite over the drive, then exit back to DOS once that's done.  At that point it's a simple matter to create a DOS command script, which of course DOS refers to as BATCH files, that jumps back up to loop to repeat the command over and over until it's interrupted.  So it would just be running a Level 2 over and over and over, which, you know, is apparently good for that drive.



The reason I'm mentioning this is that SpinRite's user can interrupt anything SpinRite is doing at any time.  But if the user then manually exited to DOS in this situation, the batch file will still be in control and would immediately restart SpinRite.  It would be possible to exit SpinRite, then frantically hit CTRL-C over and over and over to attempt to get DOS's attention and break out of the loop.  But that's certainly inelegant.



So when programs exit, this is all programs everywhere, a nearly universal convention is that they will return an "exit code" to whatever invoked them.  This code can signify whatever the program wishes, which is typically the program's sort of generic success or failure.  Today, SpinRite exits with a "0" exit code unless it's unable to parse its command line, in which case it exits with a "1."  So what occurred to me while answering his question is that when SpinRite is exiting automatically due to the "exit" verb on its command line, and not because of a manual intervention, it could exit with an error code of "2."  This would allow for much more graceful "infinite loop" termination by using the DOS line "if errorlevel 2 goto scan" at the bottom of the batch file.  Anyway, that way it would loop.  And when you used the ESCAPE key to get out of SpinRite, it would drop back out and break out of the loop elegantly.



So anyway, at some point when my eyes are crossing from writing documentation all day, I'll take a break from that to add this tiny little additional convenience feature.  And this is the great advantage of having some time to let the paint dry.  SpinRite is done.  It's working perfectly.  No one is encountering any new errors.  And again, it's like, it's done.  But there's still time for some minor touch ups, and history has shown that once I finally do release it as SpinRite v6.1 and have started working on its successor, I'm going to be extremely reluctant to mess with it any further.  So now is the perfect time for those last little tweaks, while I'm working on the documentation and getting it ready for the world.



LEO:  Nice.  How exciting.



STEVE:  That is, really.  Okay.  So, BitLocker:  Chipped or Cracked?  The number one most sent to me news item of the past week - wow, it was like everybody, seen this, seen this, seen this?  Oh, yeah - was the revelation that PCs whose secret key storage Trusted Platform Module functions are provided by a separate TPM chip outside of the main CPU are vulnerable to compromise by someone with physical access to the machine.



This came as a surprise to many people who assumed that this would not be the case, and that their mass storage systems were more protected than they'd turn out to be by Microsoft's BitLocker.  During system boot up, the small unencrypted startup portion of Windows sees that BitLocker is enabled and configured on the machine, and that the system has a TPM chip which contains the decryption key.  So that pre-boot code says to the TPM chip, "Hey there, I need the super-secret encryption key that you're holding."  And the TPM chip replies, "Yeah, got it right here.  Here it comes, no problem," and then sends it over to the processor.



The only glitch here is that anyone with a hardware probe is able to connect the probe to the communicating chips of the processor or the TPM chip, or perhaps even to the traces on the printed circuit board which interconnect those two, if those traces happen to lie on the surface.  Once connected, the computer can be booted, and that entire happy conversation can be passively monitored.  Neither end of the conversation will be any the wiser, and the probe is able to intercept and capture the TPM chip's reply to the processor's request for the BitLocker decryption key.



These are the sorts of tricks that the NSA not only knows about, but has doubtless taken advantage of, who knows how many times.  But it's not made more widely obvious until a clever hacker like this "StackSmashing" guy, that was his handle, comes along and shines a very bright light on it.  So it's a wonderful thing that he did.  And I should note that this is not the first time this has come to light.  It happened a few years ago and a few years before that.  So it's the kind of thing that surfaces every so often, and people go, "What?  Oh my god."



Okay.  The fundamental weakness in the design is that the TPM's key storage and the consumer of that stored key are located in separate components whose communication pins are readily accessible.  And the obvious solution to this dilemma is to integrate the TPM's storage functions into the system's processor so that their highly sensitive communication remains internal and inaccessible to casual eavesdropping.  And as it turns out, that's exactly what more recent Intel and AMD processors have done.



So this inherent vulnerability to physical attack occupies a window in time where discrete TPM modules exist and are being maybe overly depended upon for their security, and before their functions had been integrated into the CPU.  It's also unclear, like just broadly, whether all future CPUs will always include a fully integrated TPM, or whether Intel and AMD will only do this for some higher-end models.  Or perversely, it turns out, some lower-end models.



Anyway, all of this created such a stir in the industry that yesterday, on Monday the 12th, Ars Technica posted a very nice piece about the whole issue.  And under the subhead "What PCs are affected?" the Ars guy wrote:  "BitLocker is a form of full-disk encryption that exists mostly to prevent someone who steals your laptop from taking the drive out, sticking it into another system, and accessing your data without requiring your account password."  In other words, they're unable to start up your laptop, so they just take the hard drive out and stick it in a different machine which they know how to start up.



"Many modern Windows 10 and 11 systems," they write, "use BitLocker by default.  When you sign into a Microsoft account in Windows 11 Home or Pro on a system with a TPM, your drive is typically encrypted automatically, and a recovery key is uploaded to your Microsoft account.  In a Windows 11 Pro system, you can turn on BitLocker manually whether you use a Microsoft account or not, backing up the recovery key any way you see fit."  They say:  "Regardless, a potential BitLocker exploit could affect the personal data on millions of machines.  So how big of a deal is this new example of an old attack?  For many individuals, the answer is probably 'not very.'



"One barrier to entry for attackers is technical.  Many modern systems use firmware TPM modules, or fTPMs, that are built directly into most processors."



LEO:  I think all AMD systems do that; right?



STEVE:  Right.



LEO:  Yeah.



STEVE:  "In cheaper machines," he writes, "this can be a way to save on manufacturing.  Why buy a separate chip if you can just use a feature of the CPU you're already paying for?  In other systems, including those that advertise compatibility with Microsoft's Pluton security processors, it's marketed as a security feature that specifically integrates these kinds of so-called 'sniffing' attacks.  That's because there is no external communication bus to sniff for an fTPM.  It's integrated into the processor, so any communication between the TPM and the rest of the system also happens inside the processor.  Virtually all self-built Windows 11-compatible desktops will use fTPMs, as will modern budget desktops and laptops.  We checked four recent sub-$500 Intel and AMD laptops from Acer and Lenovo.  All used firmware TPMs.  Ditto for four self-built desktops with motherboards from Asus, Gigabyte, and ASRock.



"Ironically, if you're using a high-end Windows laptop, your laptop is slightly more likely to be using a dedicated external TPM chip, which means you might be vulnerable.  The easiest way to tell what type of TPM you have is to go into the Windows Security Center, go to the Device Security screen, and click Security Processor Details.  If your TPM's manufacturer is listed as Intel (for Intel systems) or AMD (for AMD systems), you're most likely using your system's fTPM, and this exploit won't work on your system.  The same goes for anything with Microsoft listed as the TPM manufacturer, which generally means the computer uses Pluton.



"But if you see another manufacturer listed, that is, not Intel, AMD, or Microsoft,  you're probably using a dedicated external TPM."  He said:  "I saw STMicroelectronics TPMs" - that's a very popular one - "in a recent high-end Asus Zenbook, Dell XPS 13, and midrange Lenovo ThinkPad.  StackSmashing" - the guy who publicized this again, you know, reminded everybody of this, "also posted photos of a ThinkPad X1 Carbon Gen 11 with a hardware TPM and all the pins someone would need to try to nab the encryption key, as evidence that not all modern systems have switched over to fTPMs - admittedly something I had initially assumed," he wrote.  "Laptops made before 2015 or 2016 are all virtually guaranteed to be using external hardware TPMs when they have any.



"That's not to say fTPMs are completely infallible.  Some security researchers have been able to defeat the firmware TPMs in some of AMD's processors with 'two to three hours of physical access to the target device.'  Firmware TPMs just aren't susceptible to the kind of physical, Raspberry Pi-based attack that StackSmashing demonstrated."



Okay.  So there is some good news here, at least in the form of what you can do if you really need and want the best possible protection.  It's possible to add a PIN to the boot-up process even now so that the additional factor of "something you know" can be used to strongly resist TPM-only attacks.  Microsoft provides a couple of very good and extensive pages which focus upon hardening BitLocker against attacks.  I've included links to those articles in the show notes.  But to give you a sense for the process of adding a PIN to your system right now, Ars explains under their subhead:  "So what can you do about it?"



They say:  "Most individual users don't need to worry about this kind of attack.  Many consumer systems don't use dedicated TPM chips at all, and accessing your data requires a fairly skilled attacker who is very interested in pulling the data off your specific PC rather than wiping it and reselling or stripping it for parts."  He says:  "This is not true of business users who deal with confidential information on their work laptops, but their IT departments hopefully do not need to tell anyone to do that."



Okay.  "So if you want to give yourself an extra layer of protection, Microsoft recommends setting up an enhanced PIN that is required at startup, in addition to the theoretically sniffable key that the TPM provides.  IT admins can enable this remotely via Group Policy.  To enable it on your own system, open the Local Group Policy Editor, using Windows+R to open the run, and then type gpedit.msc, hit ENTER.  Then navigate to Computer Configuration > Administrative Templates > Windows Components > BitLocker Driver Encryption > Operating System Drives.



"Enable both the 'require additional authentication at startup' and 'allow enhanced PINs for startup.'  Then open a Command Prompt window as an admin and type manage-bde-protectors -add c: -TPMAndPIN.  That command" - and this is all in the show notes, of course - "that command will immediately prompt you to set a PIN for the drive."  I would think of it as a password.  Anyway, he says:  "Once you've done this, the next time you boot, the system will ask for a PIN before it boots into Windows."  He says:  "An attacker with physical access to your system and a sufficient amount of time may be able to gain access by brute-forcing this PIN, so it's important to make it complex, like any good password."  And again, I would make it really good if you're taking the time to do it at all.  Why not?



He finishes:  "A highly motivated, technically skilled attacker with extended physical access to your device may still be able to find a way around these safeguards.  Regardless, having disk encryption enabled keeps your data safer than it would be with no encryption at all, and that will be enough to deter lesser-skilled, casual attackers from being able to get at your stuff."



So, ultimately, we're faced with the same tradeoff as always:  convenience versus security.  In the absence of a very strong PIN password, anyone using a system that is in any way able to decrypt itself without their assistance should recognize the inherent danger of that.  If the system escapes their control, bad guys might be able to arrange to have the system do the same thing for them.  That is, decrypt without anything that they don't know.  Requiring "something you know" is the only true protection.  Maybe something else that you have, if that could be arranged.  That's what I did when I did my little European trip to introduce SQRL is I had my laptop linked to my phone, and my iPhone had to be present.  At the same time, BitLocking, or BitLockering, a drive is certainly useful since it will very strongly prevent anyone who separates the drive from the machine from obtaining anything that's protected in any way.



So BitLocker:  Yes.  PIN:  Yes.  And as we've seen, it's possible to add a PIN after the fact.  And if your PIN is weak, you can still strengthen it, and you should consider doing so.



LEO:  Do we still like VeraCrypt?  Would you prefer VeraCrypt to BitLocker?  BitLocker's so convenient.



STEVE:  It's convenient.  And VeraCrypt is 100% strong.  I was thinking the same thing.  BitLocker suffers a little bit from the monoculture effect of everybody having it, and it just being built in.  On the other hand, its convenience means that it won't get in anyone's way.



LEO:  Right.  Yeah, you just log into the computer as normal.



STEVE:  Yeah.



LEO:  Yeah.  But if you wanted really better security, I think VeraCrypt is - we still - that's still is our choice, now that - what was it, its predecessor?  I've forgotten now.



STEVE:  TrueCrypt.  TrueCrypt.



LEO:  TrueCrypt.  TrueCrypt is gone.  Yeah, yeah, yeah.



STEVE:  Yup.



LEO:  All right.  If your PIN is weak, you can still straighten it.  The motto of the day.



STEVE:  If it is weak, you can still straighten it.  I like it, Leo.



LEO:  That's Calia, who is a textile worker.  Thank you very much, Steve Gibson.  You are the best.  Steve lives at GRC.com.  That's where SpinRite 6 also lives, soon to be 6.1.  Like a butterfly, it's coming out of the chrysalis and emerging into the...



STEVE:  There's movement, folks.  There's movement.



LEO:  There's movement.  The wings are fluttering.  If you get 6.0 now, you get 6.1 automatically free.  Well, not completely automatically.  You have to download it.  But it's worth getting 6.0 now so you can have it, and 6.1 the minute it's available.  You can also get the beta version now if you are an owner.  You can also get this show at the website, GRC.com.  And that's free.  He has two unique versions, a 16Kb version for the bandwidth-impaired, and the very well done transcripts by Elaine Farris so you can read along as you listen, or search, or that kind of thing.  All that's at GRC.com, along with SpinRite and ShieldsUP! and ValiDrive and all the great stuff Steve does in assembly language in the middle of the night.  What are your coding hours?  You're not a late-night coder, I don't think.



STEVE:  No.  I'm 68.  I'm not a late-night coder.  When I remodeled my home, and I was 38, it had blackout drapes installed in the master bedroom.  You know, I have normal cloth drapes, and then behind it is opaque, like, thick, I don't know, vinyl so that - because I would be coding, and I'd be looking out the window and noticing the sky was getting lighter.



LEO:  Yeah.



STEVE:  It's like, "Oh, no."



LEO:  Yeah.  Oh, that feeling.



STEVE:  And I always, afterwards, I chastised myself.  I never wanted to stop.  But I was useless the next day.  I mean, it just screwed everything up.  



LEO:  For at least a day, yeah.



STEVE:  So what you needed to have is the self-control back then to make yourself go to sleep.  Now I don't need self-control because I'm tired.  And so I'm, like, looking forward to hitting the sack and being fresh in the morning.



LEO:  Well, it is fresh, almost 6.1, almost fully cooked.  We have the show at our website.  What?  What?



STEVE:  Lorrie does comment, when I mention that, she says, "Well, yes, you're tired.  You just coded for 18 hours straight."



LEO:  It's amazing.



STEVE:  So there is that.



LEO:  6:00 a.m. till 10:00 p.m. or something like that; right?



STEVE:  Yup.  Yup.



LEO:  Yeah, very nice.  He's a hard-working guy.



STEVE:  I love to code.



LEO:  Yeah.  I mean, it's fun, isn't it.



STEVE:  Yeah.



LEO:  I am completely stuck.



STEVE:  Better than anything I've ever found.  Well, except one thing, but you can't do that all the time.



LEO:  Second best thing, yeah.



STEVE:  That's right.



LEO:  A lot of endorphins, though.  Very good for the endorphins.  Thank you, Steve.



STEVE:  I will be back in a week.  Bye-bye. 



Copyright (c) 2024 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#962

DATE:		February 20, 2024

TITLE:		The Internet Dodged a Bullet

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-962.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  What's the worst mistake that the provider of remotely accessible residential webcams could possibly make?  What surprises did last week's Patch Tuesday bring?  Why would any website put an upper limit on password length?  And for that matter, what's up with no use of special characters?  Will Canada's ban on importing the Flipper Zero hacking gadgets reduce car theft?  Exactly why didn't the Internet build in security from the start?  How could they miss that?  Doesn't Facebook's notice of a previous password leak information?  Why isn't TOTP just another password that's unknown to an attacker?  Can exposing SNMP be dangerous?  Why doesn't email's general lack of encryption and other security make email-only login very insecure?  And, finally, what major cataclysm did the Internet just successfully dodge?  And is it even possible to have a "minor cataclysm"?  Today, we'll be taking a number of deep dives after we examine a potential solution to global warming and energy production as shown in our terrific Picture of the Week.  Some things are so obvious in retrospect.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  Coming up, we're going to talk about the webcams that accidentally put everybody's video in everybody else's house.  Patch Tuesday is here.  Well, it was last week.  Steve has some notes on that.  Why the Flipper Zero is being banned in Canada.  And a nightmare scenario with DNSSEC that could have brought the whole Internet to its knees.  Steve explains, next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 962, recorded Tuesday, February 20th, 2024:  The Internet Dodged a Bullet.



It's time for Security Now!, the show you wait for all week long.  I know it, I know it, one of the nine best security shows in the world, with Steve Gibson, the man in charge.



STEVE GIBSON:  Not what I do now.



LEO:  What number are we on that list, by the way?  Number one, I hope.



STEVE:  No, no.  I think maybe seven.  But it wasn't ranked in, like, order of goodness.  It might have been alphabetical.  I think it was alphabetical, actually. 



LEO:  Oh, all right.  Well, then we would come in towards the end, yeah.



STEVE:  I think "A" was like Appleby's something or other.



LEO:  Security show, yes.



STEVE:  That's right.



LEO:  Which is a wonderful, wonderful show, by the way.



STEVE:  Yeah.  So, oh boy.  No one is going to believe this, literally, because - well, you will once I'm done.  But this has got to be the most underreported event that I can remember.  And this sounds like hyperbole, and you know me, so it could well be.  But the whole Internet was at risk for the last couple months.



LEO:  What?



STEVE:  And a group of researchers silently fixed it.



LEO:  What?



STEVE:  Because if it had been discovered, the Internet could have been shut down.



LEO:  Wow.



STEVE:  It turns out there was a problem that has been in the DNS specification for 24 years.  It's deliberate.  And so it wasn't a bug, but it was a feature that a group of German researchers figured out how to exploit.  And one receive - I'm stepping on my whole storyline here because I'm so excited by this.  It's like, wow.  Okay.  So this is Episode 962 for February 20th, titled "The Internet Dodged a Bullet."



LEO:  Not the first bullet, though; right?  Didn't Dan Kaminsky save the Internet back in 2008?



STEVE:  No.



LEO:  Oh.



STEVE:  Dan is a great publicist.



LEO:  Oh.  Was a great publicist, yeah.



STEVE:  I mean, and yes, the issue of DNS queries not having sufficient entropy was important.  I mean, I wrote the, what the hell's it called, I forgot now, the DNS...



LEO:  Cache poisoning.



STEVE:  The spoofability.  No, the spoofability test.  A lot of work in order to allow people to measure the entropy of their own - of the DNS servers that are issuing queries on their behalf.  This blows that away.  I mean, here I am doing it again.  Anyway.  Okay.  So [crosstalk] questions. 



LEO:  Wow, I can't wait to hear this.



STEVE:  It is something.  And Leo, be careful not to look at the Picture of the Week because this is a great one.  So we're going to answer some questions.  What's the worst mistake that the provider of - and I've not had too much coffee - of remotely accessible residential webcams could possibly make?  Like when Lisa said, "We don't want any cameras in our house," what was she worried would happen?



LEO:  Oh, yeah, yeah.



STEVE:  Yeah.  What surprises did last week's Patch Tuesday bring?  Why would any website put an upper limit on password length?  And for that matter, what's up with the "no use of special characters" business?  Will Canada's ban on importing the Flipper Zero hacking gadgets reduce their car theft?  Exactly why didn't the Internet build in security from the start?  Like what was wrong with them?  How could they miss that?  Doesn't Facebook's notice of a previous password being used leak information?  Why isn't TOTP just another password that's unknown to an attacker?  Can exposing SNMP be dangerous?  Why doesn't email's general lack of encryption and other security make email-only login quite insecure?  And, finally, what major cataclysm did the Internet just successfully dodge?



LEO:  Yow.



STEVE:  And is it even possible to have a "minor cataclysm"?



LEO:  All this and more from one of the best 17 cybersecurity podcasts in the world.



STEVE:  No, that's a different one.  Mine was nine.  We were in the top nine.



LEO:  Oh.  Well, SANS Institute says we're in the top 17.



STEVE:  Well, but where?  And are they alphabetical?



LEO:  They're in random order.  But we're on there.



STEVE:  Or in order of love.



LEO:  That's the most important thing.  And, I mean, SANS Institute's pretty credible.



STEVE:  SANS is good.  But I'd rather be in the top nine.  So today we'll be taking a number of deep dives after, Leo, after we examine a potential solution to global warming and energy production - no, this is serious - as shown in our terrific Picture of the Week.



LEO:  Oh, I can't wait.



STEVE:  And Leo, some things are just so obvious in retrospect.



LEO:  Oh, wow.  How could we not have known?



STEVE:  This is a podcast for the age.  Thank god we got it in before 999.



LEO:  Oh, my goodness.



STEVE:  Otherwise, you know, we'd have to wait into four digits.



LEO:  Now let's see if Steve has overhyped his Picture of the Week.  What is this going to do for us?  It's going to save the world.



STEVE:  Solution to global warming and energy production.



LEO:  Well, of course.  You put light on the solar panels, and infinite electricity.



STEVE:  That's exactly right, Leo.  So what we have for those who are not looking at the video is a picture of a rooftop with some solar panels mounted on it, as many are these days.  What makes this one different is that it's surrounded by some high-intensity lights pointing down at the solar panels because, you know, why not?  [Crosstalk] electricity.



LEO:  This way it generates day and night, yeah.  Perfect.



STEVE:  And my caption here, I said, "When you're nine years old, you wonder why no one ever thought of this before.  Adults are so clueless!"



LEO:  I bet you even knew better when you were nine.



STEVE:  Well, you know, it was interesting because this put me in mind of the quest for perpetual motion machines.



LEO:  Exactly, yeah.



STEVE:  Remember those back in our youth?



LEO:  Yeah, yeah, yeah.



STEVE:  I mean, and like even Da Vinci was a little bit obsessed with these things.  There was one really cool design where you had an inner - you had a wheel with marbles riding tracks where the marbles could roll in and out, and the tracks were canted so that on one side of the wheel the marbles rolled to the outside.  Therefore their weight was further away from the center so pulling down harder.  And on the other side, the way the fixed tracks were oriented, the marbles would roll into the center, into the hub.  So their weight was pulled - well, there it is.  You just found the perpetual motion machine, exactly.



LEO:  Never stops turning.



STEVE:  And, I mean, there were, again, I was, you know, five and interested in physics already because I was wiring things up with batteries and light bulbs and stuff when I was four.  So I was, you know, I spent some length of time puzzling that out.  The good news is now as an adult I don't give it a second thought.



LEO:  You're saying you believe Newton's law of the conservation of energy.  You just believe that to be true.



STEVE:  Well, the problem we have with our Picture of the Week is that lights are not 100% efficient in converting electricity into light.  For example, they get warm, so you have some energy lost in heat.  And the physics of the conversion are also not completely efficient.



LEO:  Oh, yeah.  Solar panels are no more than 5 or 10% efficient at the very most.



STEVE:  And there you go.  So the idea would be you hook the output of a solar panel to the input of the lights.  And then, you know, when the sun goes down, this just keeps going. 



LEO:  Keeps going.



STEVE:  Yeah.



LEO:  Somebody has a good point, though.  Maybe those lights are hooked up to the neighbor's electricity.



STEVE:  Well, and the only thing I could think when I was, like, trying to find a rationale, was that they might turn the lights on at night because for some wacky reason, whatever they're powering from the solar panels needs to be on all the time, or maybe they turn it on on cloudy days because, again, for the same reason.  So it's sort of a sun substitute because of, you know, dumbness, because it is dumb.  As you said, solar panels are way inefficient.  So you're going to get much less juice out of the solar panels than you put into the array of lights which are busy lighting them up.  But...



LEO:  We're in a golden era for scammers.  You're just going to see endless variations of this on YouTube and on Twitter and, you know, using water to power your car.  And this stuff just never dies.  It never dies.



STEVE:  Wait, does that work?  I've got to try that.



LEO:  No.  No.  No.  No.



STEVE:  Okay.  So in reporting the following story, Leo, I'm reminded of your wife Lisa's "wisely," so to speak, because this story's about Wyze, forbidding cameras of any kind inside your home.  9to5Mac's headline read:  "Wyze camera breach let 13,000 customers view other people's homes."



LEO:  Oh, boy.



STEVE:  Tom's Hardware:  "Wyze security failure let 13,000 customers see into other users' homes."  GeekWire:  "Wyze security cam incident that exposed images to other users impacts more than 13,000 customers."  And even our good old BleepingComputer:  "Wyze camera glitch gave 13,000 users a peek into other homes."  Now, one of our listeners, a user of Wyze monitoring cameras, was kind enough to share the entire email he received from Wyze.  But BleepingComputer's coverage included all of those salient details and added a bit more color, as you might expect.



Here's what Bleeping just wrote yesterday.  They said:  "Wyze shared more details on a security incident that impacted thousands of users on Friday and said that at least 13,000 customers could get a peek into other users' homes.  The company blames a third-party caching client library recently added to its systems, which had problems dealing with a large number of cameras that came online all at once after a widespread Friday outage.  Multiple customers have been reporting seeing other users' video feeds under the Events tab" - I bet there were some events - "in the app since Friday, with some even advising other customers to turn off the cameras until these ongoing issues are fixed.



"Wyze wrote:  'The outage originated from our partner AWS and took down Wyze devices for several hours early Friday morning.  If you tried to view live cameras or events during that time you likely weren't able to.  We're very sorry for the frustration and confusion this caused.  As we worked to bring cameras back online, we experienced a security issue.  Some users reported seeing the wrong thumbnails and Event Videos'" - yeah.  Whose hot tub is that?



LEO:  That's not Mama walking around.



STEVE:  "'We immediately removed access to the Events tab and started an investigation.'"  We bravely did that.  Okay.  "Wyze says this happened because of the sudden increased demand" - and I'll get to my skepticism on that in a minute - "and led to the mixing of device IDs and user ID mappings" - you don't ever want that to happen with your camera system - "causing the erroneous connection of certain data with incorrect user accounts.  As a result, customers could see other people's video feed thumbnails and even video footage after tapping the camera thumbnails in the Wyze app's Events tab.



"In emails sent to affected users, Wyze confessed:  'We can now confirm that as cameras were coming back online, about 13,000 Wyze users received thumbnails from cameras that were not their own; and 1,504 users tapped on them.  We've identified your Wyze account as one that was affected.  This means that thumbnails from your Events were visible in another Wyze user's account and that a thumbnail was tapped.'"  That is a confession.  Somebody was looking at your video, baby.  "'Most taps enlarged the thumbnail, but in some cases it could have caused an Event Video to be viewed.'"



LEO:  Let me zoom in on that one.



STEVE:  What is that in the corner over there?  Yeah, that's right.  "Wyze has yet to share the exact number of users who had their video surveillance feeds exposed in the incident.  The company has now added an extra layer of verification" - oh, you betcha - "for users who want to access video content via the Events tab to ensure that this issue will not happen in the future."  Is that really your video you're about to look at?  "Additionally, it adjusted systems to avoid caching during user-device relationship checks until it can switch to a new client library" - get rid of that old cache - "capable of working correctly, which would be convenient, during 'extreme events'" - they had in quotes - "like the Friday outage."



Okay, now, I like Wyze, and their cameras are adorable little beautifully designed cubes, you know, they look like something Apple would produce.  But at this stage in the evolution of our understanding of how to do security on the Internet, I cannot imagine streaming to the cloud any content from cameras that are looking into the interior of my home.  You know, maybe the backyard.  But even then, you know, who knows?  The cloud and real-time images of the contents of our homes do not mix.  I understand that for most users the convenience of being able to log into a Wyze camera monitoring website to remotely view one's residential video cameras is difficult to pass up, you know, seductive, if you weren't listening to this podcast.  And the whole thing operates with almost no setup.



You know, and Wyze's response to this incident appears to be everything one could want.  You know, they've really - they've been honest, which could not have been easy.  The letter that our listener shared with me, unlike the letter that BleepingComputer quoted, said that his account had not been affected.  So you know, Wyze was on the ball.  And they've clearly got logging working because they knew that 1,504 people did click on a thumbnail, like what is that?  That doesn't look like my living room.  Click.  Oh, it's not my living room.  Wow.  Who's that?



Anyway, I should add, however, that I'm a bit skeptical about the idea that simply overloading a caching system could cause it to get its pointers scrambled and its wires crossed, thus causing it to get its users confused.  If it really did happen that way, it was a crappy cache.  And I'm glad they're thinking about, like, revisiting this whole thing because there was a problem somewhere.  Anyway, I do like the cameras.  I'm just  not going to let them be exposed to the outside world.  That makes no sense.  Okay.



LEO:  By the way, Jason on MacBreak Weekly recommended a new Logitech camera that's HomeKit enabled, which means it doesn't send video outside your house.



STEVE:  Nice.



LEO:  Or I guess it does probably to Apple encrypted iCloud, something like that.



STEVE:  And again, done right, probably.



LEO:  Done right, of course.



STEVE:  So, yeah.



LEO:  So HomeKit and security and Apple, that seems like a good choice.



STEVE:  Yeah.



LEO:  It's too bad because the Wyze stuff is very inexpensive.  I've been recommending it for years, and using it, too.



STEVE:  Oh, I think, in fact, you order one, they send you money.



LEO:  Amazingly, yeah.  It's so inexpensive.



STEVE:  And they're just beautiful little things.



LEO:  Yeah, yeah.  That's too bad.



STEVE:  Okay.  We haven't checked in on a Microsoft Patch Tuesday for a while.  Last week's update was notable for its quiet inclusion of a mitigation for what the industry's DNS server vendors have described as "the worst attack on DNS ever discovered."  It seems to have slipped under much of the rest of the industry's radar, but not this podcast's.  Since it could have been used to bring down the entire Internet, it is today's topic, which we will of course be getting back to later.



But first we're going to take a look at Patch Tuesday and then answer some questions and have some interesting deep dives.  The DNS attack didn't happen, so the Internet is still here.  Thus Microsoft doesn't get off the hook for fixing another round of problems with its products.  Last week's patches included fixes for a pair of zero-days that were being actively exploited, even though they were not the worst from a CVSS rating standpoint.  That goes to a different pair which earned 9.8 CVSS ratings.



Overall, last Tuesday Microsoft released patches to repair a total of 73 flaws across its product lineup.  Of those 73, five are Critical, 65 are Important, and the last three have Moderate severity.  Fixed separately were 24 flaws in Edge which had been repaired in the intervening month between now and last month's updates.



The two zero-days are a Windows SmartScreen Security Bypass carrying a CVSS of only 7.6 and an Internet Shortcut Files Security Feature Bypass with a CVSS of 8.1.  The first one of those two, the Windows SmartScreen Security Feature Bypass, allowed malicious actors to inject their own code into SmartScreen to gain code execution, which could then lead to data exposure.  The lack of system availability could also happen, or both.  Now, the reason it was only rated at 7.6 is that, for the attack to work, the bad guys needed to send the targeted user a malicious file and convince them to open it.  So it wasn't like, you know, just receive a packet and it's the end of the world, which actually is what happened with this DNS business we'll get to.



The other zero-day permitted unauthenticated attackers to bypass displayed security checks by sending a specially crafted file to a targeted user.  But once again, the user would somehow need to be induced to take the action of clicking on the link that they'd received.  So not good.  Was being actively exploited in the field.  Both of these were zero-days being used.  Still, this one rated an 8.1, and it's fixed as of last Tuesday.



The five flaws deemed Critical, in order of increasing criticality, were a Windows Hyper-V Denial of Service Vulnerability that got itself a score of 6.5, so Critical, but not a high CVSS; the Windows Pragmatic General Multicast, which is PGM, Remote Code Execution Vulnerability, scored a 7.5; Microsoft Dynamics Business Central/NAV Information Disclosure Vulnerability came in with an 8.0; and finally the two biggies, both getting a 9.8.  To few people's surprise, we have another Microsoft Exchange Server Elevation of Privilege Vulnerability and a Microsoft Outlook Remote Code Execution Vulnerability, both of those 9.8s, both of those easy to do, both of those now resolved as of last week.



A senior staff research engineer at Tenable, the security firm, said in a statement that this was very likely to be exploited, and that exploiting the vulnerability could result in the disclosure of a targeted user's NTLM (NT LAN Man) v2 hash, which could be relayed back to a vulnerable Exchange Server in an NTLM relay or pass-the-hash attack to allow the attacker to authenticate as the targeted user.  So it was a way of getting into Microsoft Exchange Server through this 9.8, basically backdoor vulnerability.



And believe it or not, last Tuesday's updates also fixed 15 - I had to, like, what, really?  Yes, 15 remote code execution flaws in Microsoft's WDAC OLE DB provider for SQL Server that an attacker could exploit by tricking an authenticated user into attempting to connect to a malicious SQL server via Windows OLEDB.  So 15 remote code execution flaws.  It must be that someone found one and said, wow, let's just keep looking.  And the more they looked, the more they found.  So Microsoft fixed all of those.



And rounding off the patch batch, as I mentioned, is a mitigation, not a total fix, for a 24-year-old fundamental design flaw - not an implementation flaw, a design flaw - in the DNSSEC spec which, had they known of it, bad guys could have used to exhaust the CPU resources of DNS servers to lock them up for up to 16 hours after receiving just a single DNS query packet.  We'll be talking about this bullet that the Internet dodged as we wrap up today's podcast.



But first, Ben wrote:  "Hey, Steve.  Love the show.  I hated the security courses I took in school, but you make it way more interesting.  I haven't missed a podcast from you in three years.  My question was:  I was recently going through my password vault and changing duplicate passwords.  I encountered a lot of sites with length and symbol restrictions on passwords, for example, no passwords longer than 20 characters, or disallowing certain symbols.  My understanding of passwords is that they all get hashed to a standard length regardless of the input, so it can't be for storage space reasons.  Why the restrictions?  Even if I input an eight-character password, the hash will end up being 128 bits, or whatever.  I would love some insight because it makes no sense to me.  Thanks, Yuri."  So that's his real name.



Okay.  So, Yuri, you're not alone.  When you look closely at it, none of this really makes any sense to anyone.  I think the biggest problem we have today is that there are no standards, no oversight, and no regulation; and everything that is going on behind the scenes is opaque to the user.  You know, we only know in detail, for example, what LastPass was doing, and the entire architecture of it, because it really mattered to us, and back then we drilled down and got questions from the guy that wrote that stuff.  And so, you know, we really understood what the algorithms were.  But on any random given website we have no idea.  There's just no visibility.



LEO:  You can also kind of feel how antiquated that is when you say the words "the guy who wrote it."  Besides your software, SpinRite and all of that, nothing's written by one person.  That's nuts.  Right?



STEVE:  You're right.  You're right.  You're right.



LEO:  Yeah.  The fact that Joe Siegrist wrote LastPass 30 years ago by himself is amazing.



STEVE:  Yeah.  So everything is opaque to the user.  We hope that passwords are stored on a site's server in a hashed fashion.  But we don't know that.  You know?  And assuming that they are hashed, we don't know whether they're being hashed on the client side in the user's browser or on the server side after they arrive in the clear over an encrypted connection.  That much we do know because we can see that the connection is encrypted.



A very, very long time ago, back at the dawn of Internet identity authentication, someone might have been asked by a customer support person over the phone to prove their identity by reading their password aloud.  I'm sure that happened to a few of us.  I'm sure I was asked to do it like at the very beginning of all this.  The person at the other end of the line would be looking at it on their screen which they had pulled up for our account to see whether what we read to them over the phone matched what they had on record.  That's the way it used to be.  Of course, as we know, this could only be done if passwords were stored as they actually were given to the server, in the clear, as indeed they originally were.



And in that instance, you can see why the use of special characters with names like "circumflex," "tilde," "pound," "ampersand," "back apostrophe," and "left curly brace" might be difficult for some people at each end of the conversation to understand.  Tilde?  What's a circumflex?  What's a circumflex?  What?  So restricting the character set to a smaller common set of characters made sense back then.  And we also know that in those very early days a web server and a web presence was often just a fancy graphical front end to an existing monstrous old school mainframe computer, you know, up on an elevated floor with lots of air conditioning, and that computer's old account password policies predated the Internet.  So even after a web presence was placed in front of the old mainframe, its ancient password policies were still being pushed through to the user.



Today we all hope, all, that none of that remains.  But if we've learned anything on this podcast, it's to never discount the power and the strength of inertia.  Even if there is no longer any reason to impose password restrictions of any kind - well, other than minimum length, that would be good, you know, because the password "a" is not a good one - restrictions may still be in place today simply because they once were.



And hopefully all consumers have learned the lesson to never disclose a password to anyone at any time for any reason.  We see reminders from companies which are working to minimize their own fraud, explicitly stating that none of their employees will ever ask for a customer's password under any circumstances.  And we're hoping that's because they don't have the passwords under any circumstances.  They were hashed a long time ago, and they're just being stored.



The gold standard for password processing is for JavaScript or WASM running on the client, that is, the user's browser, to perform the initial password qualification and then its hashing.  Some minimum length should be enforced.  All characters should be allowed because why not, and requirements of needing different character families - upper and lower case, numbers and special symbols - also make sense.  That will protect people from using "123456" or "password" as their password.  And those minimal standards should be clearly posted whenever a new password is being provided.  It's really annoying, right, when you're asked to change or to create an account or change a password, and then after you do, it comes up and says, "Oh, we're sorry, that's too long."  Or, "Oh, you didn't put a special character in."  It's like, why didn't you tell me first?



Anyway, ideally there should be a list of password requirements with a checkbox appearing in front of each requirement, dynamically checked as its stated requirement is met.  And the "Submit" button should be grayed-out and disabled until all requirements have checkboxes in front of them, thus those requirements have been met.  And a password strength meter would be another nice touch.



Once something that has been submitted from the user arrives at the server, then high-power systems on the server side can hash the living daylights out of whatever arrives before it's finally stored.  But since we also now live in an era where mass data storage has become incredibly inexpensive, and where there's very good reason to believe that major world powers are already recording pretty much everything on the Internet, all Internet transactions, in the hope of eventually being able to decrypt today's quantum-unsafe communications once the use of quantum computers becomes practical, there's a strong case to be made for having the user's client hash the qualifying password before it ever leaves their machine to traverse the Internet.



Once upon a time we coined the term PIE, P-I-E, Pre-Internet Encryption.  So this is like that.  This would be I guess PIH, Pre-Internet Hashing.  But, you know, JavaScript or preferably WASM should be used to hash the user's qualifying input.



Much of that gold standard that I just described is user-facing, and its presence or absence is obvious to the user.  You know,  unfortunately, we rarely see that going on today.  It would be necessary to reverse engineer individual web applications if we wished to learn exactly how each one operates.  Since avoiding the embarrassment of breaches and disclosures is in each company's best interest, and since none of the things I've described is at all difficult to deploy today, we can hope that the need to modernize the user's experience while improving their security will gradually overcome the inertia that will, you know, always be present to some degree.  So we'll always be dragging forward some of the past.  But at some point, you know, everything should be catching up.



LEO:  I like it.



STEVE:  Yup.  Felipe Mafra said:  "Hello, Steve.  First of all, I'd like to thank you and Leo for the great show.  I'd like to bring you something very interesting that recently happened on this side of the border.  The Canadian Industry Minister Franois-Philippe Champagne proudly tweeted on February 8th that they are banning the importation, sale, and use of hacking devices, such as Flipper Zero, that are being widely used for auto theft.  This is an attempt to respond to the huge increase in auto thefts here in Canada.  Even if I believe it's good that the government is trying to address this issue, I found myself, rather than blocking the usage of such devices, it would be better if the industry was required to make things right by design.



"This pretty much aligns with last week's Security Now! Episode 960 regarding security on PLCs" - you know, Programmable Logic Controllers - "as we see no commitment from those industries to make their products safe by design.  Anyways, I wanted to share this with you and get your perspectives.  Thank you again for the show.  Looking forward to 999 and beyond.  Best regards, Felipe."



Okay.  In past years we've spent some time looking closely at the automotive remote key unlock problem.  What we learned is that it is actually a very difficult problem to solve fully, and that the degree of success that has been obtained by automakers varies widely.  Some systems are lame, and others are just about as good as can be.  And we've seen even very cleverly designed systems, like as good as they could be, fall to ingenious attacks.  Remember the one where a system was based on a forward rolling code that was created by a counter in the key fob being encrypted under a secret key, and the result of that encryption was transmitted to the car.



This would create a completely unpredictable sequence of codes.  Every time the unlock button was pressed, the counter would advance, and the next code would be generated and transmitted.  And no code ever received by the auto would be honored a second time.  So anyone snooping and sniffing the radio could only obtain code that had just been used and would no longer thus be useful again.



So what did the super-clever hackers do?  They created an active attack.  When the user pressed the unlock button, the active attack device would itself receive the code while simultaneously emitting a jamming burst to prevent the automobile from receiving it.  So the car would not unlock.  Since that happens when we're too far away from the car, and it's not that uncommon, the user would just shrug and press the unlock button again.  This time, the active attacking device would receive the second code, emit another jamming burst to prevent the car from receiving the second code, then itself send the first code it had received to unlock the car.  So the user would have to press the button twice.  But they just figured the first one didn't make it.  The second one unlocked the car.



By implementing this bit of subterfuge, the attacker is now in possession of a code that the key fob has issued, thus it will be valid, but the car has never seen it.  And it's the next key in the sequence from the last code that the car did receive.  It is diabolically brilliant, and I think it provides some sense for what automakers are up against.



From a theoretical security standpoint, the problem is that all of the communication is one-way, key fob to auto.  The key fob is issuing a one-way assertion instead of a response to a challenge.  What's needed to create a fully secure system would be for the key fob's unlock button to send a challenge request to the car.  Upon receiving that request, the car transmits a challenge in the form of an unpredictable value resulting from encrypting a counter.  Again, the counter is monotonic, upward counting 128 bits, and it will never repeat during the lifetime of the universe, let alone the lifetime of the car or its owner.



So upon receiving that unique challenge code sent by the car, the key fob encrypts that 128-bit challenge with its own secret key and sends the result back to the car.  The car, knowing the secret kept by its key fobs, performs the same encryption on the code it sent and verifies that what the key fob has sent it was correct.  Now, I cannot see any way for that system to be defeated.  The car will never send the same challenge, and the key will never return the same response.  And no amount of recording that challenge and response dialogue will inform an attacker of the proper responses to future challenges.  If some attacker device blocks the reception, the car will send a different challenge.  The key will respond with a different reply.  And once that reply is used to unlock the car, the car will no longer accept it again.



So the only problem with this system is that now both endpoints need to contain transceivers capable of receiving and transmitting.  Previously, the fob only had to transmit, and the car only had to receive.  So transceivers add some additional cost, though not much in production since both already contained radios anyway.  But what this does mean is that a simple software upgrade to the existing hardware install base will not, and cannot, solve this problem.  I doubt it's possible to create a secure one-way system that's safe against an active attacker while still reliably unlocking the vehicle without unduly burdening its user.



The system I've just described is not rocket science, it's what any crypto-savvy engineer would design.  And since this problem is also now well understood, I would be surprised if next-generation systems which fix this in this way once and for all were not already on and off the drawing board and headed into production.  But that doesn't solve the problem which exists, and will continue to exist, for all of today's automobiles.



So now let's come back to Felipe's point about Canada's decision to ban the importation of devices such as the Flipper Zero.  We know that doesn't solve the problem.  But will it reduce the severity of the problem?  Yeah, probably somewhat.  Kits will spring up to allow people to build their own.  Canada is a big place.  There's nothing to prevent someone from firing up manufacturing and creating homegrown Flipper Zeros or their like.  It's an open-source device.  I mean, like the design is all there.  What we keep seeing, however, is that low-hanging fruit is the fruit that gets picked and eaten.  And many people won't take the time or trouble to exert themselves to climb a tree to obtain the higher hanging fruit.  Hand 'em a piece?  Sure.  Work for it?  Perhaps later.  So I would argue that making car theft even somewhat more difficult will likely be a good thing.  And the Flipper Zero is, at best, a hacker's gadget.  It's not as if it has huge non-hacker applications.



LEO:  No, but it's a lot of fun.



STEVE:  It is a lot of fun.



LEO:  And I was going to use it on my car, and then Russell said don't because you could actually lock yourself out of the car because the car's security features will see that you're doing it and will prevent you from using your regular fob after that.  So I declined.  But I was able to get into the office.  I was able to clone our key fob and use it.



STEVE:  Oh, yeah.  It is a - I did a little bit of brushing up on it yesterday.  It is a very cool device.



LEO:  I gave mine to Father Robert.  So it's now in Italy.



STEVE:  You gave it a good home.



LEO:  I think he's really going to get a lot of use out of it.



STEVE:  And actually, you know, from a hardware-hacking standpoint, all of the little GPIO pins along the back...



LEO:  Oh, it's really cool.



STEVE:  It's very, very cool.



LEO:  It's a great device.  I think you could duplicate it with an Arduino or any - a variety of other devices, as well.



STEVE:  But as we've seen, packaging counts.



LEO:  Sure.



STEVE:  Like remember the picture of that TPM buster that we talked about last week, where it had the little row of pogo pins along one side.



LEO:  Right.



STEVE:  It just looked adorable.  And it's like, wow, that's very cool.



LEO:  It's cute.



STEVE:  So Leo, let's take a break, and then what are we going to talk about next?  We've got, oh, we're going to talk about why the Internet didn't start off having security in the first place.



LEO:  Oh.  You know, I interviewed, I guess it was Vint Cerf, the Father of the Internet, back in the day.  And I asked him, you know, why didn't you think of putting crypto in?  And he said no one knew.  We just didn't - he said we would now.  We now know how people use the Internet.  But at the time - anyway, I'm very curious to what you have to say about this one.  All right, Steve.  On we go.



STEVE:  So M-Scott tweets:  "Steve, I'm wondering about your thoughts.  The cybersecurity community seems to bemoan the lack of security baked into the original Internet design and ceaselessly encourages designers of new technology to bake in security from the get-go."  Well, we certainly agree with the second half of that.  "Several books I'm reading for a cyber and information warfare class suggest that government regulation to require security is the answer and should have been placed on the Internet in the initial design.  However, I suspect, if security had been a mandate on day one, the robust cyber community we have today would not exist.  I see the Internet as more of a wicked problem where solutions and problems emerge together, but cannot be solved upfront.  Your thoughts?  Thank you for your continued service to the community."



Okay.  I suppose that my first thought is that those writing such things may be too young to recall the way the world was when the Internet was being created.  I'm not too young.  I was there, looking up at the IMP, the Interface Message Processor...



LEO:  Oh, wow, yeah.



STEVE:  ...a big imposing white cabinet sitting at Stanford's AI lab in 1972, as the thing known at the time as DARPANET was first coming to life.  The problems these authors are lamenting not being designed-in from the start didn't exist before the Internet was created.  It's the success of the Internet and its applications that created the problems, and thus the needs, we have today.  Also, back then we didn't really even have crypto yet.  It's nice to say, "Well, that should have been designed in from the start."



But it wasn't until four years later, in 1976, that Whit Diffie, Marty Hellman, and Ralph Merkle invented public key crypto.  And a huge gulf exists between writing the first academic paper to outline a curious and possibly useful theoretical operation in cryptography, and designing any robust implementation into network protocols.  No one knew how to do that back then, and we're still fumbling around finding mistakes in TLS decades later.  And one other thing these authors may have missed in their wishful timeline is that the applications of cryptography were classified by the U.S. Federal government as munitions.



LEO:  Oh, yeah.



STEVE:  In 1991, 19 years after the first IMPs were interconnected, Phil Zimmermann, PGP's author, had trouble with the legality of distributing PGP over the Internet.  Today, no one would argue that security should always be designed in from the start, and we're still not even doing that very well.  We're still exposing insecurable web interfaces to the public-facing Internet, and that's not the Internet's fault.  So anyone who suggests that the Internet should have been designed with security built in from the start would have to be unaware of the way the world was back when the Internet was actually first being built.  Some of us were there.



LEO:  Yeah.  And, you know, Vint Cerf did say the one thing he would have added had they been able to was encryption.  That's exactly right.



STEVE:  Yeah.



LEO:  Yeah.  But they just couldn't at the time.



STEVE:  No, I mean, it didn't exist.  It literally, like, you know, sure, I mean, we knew about the Enigma machine.



LEO:  Right.



STEVE:  But, you know, you're not going to have gears and wires on your computer.



LEO:  You don't have time to do that, yeah, yeah.



STEVE:  You know, it's like, what?  No.



LEO:  Yeah, yeah.



STEVE:  So, I mean, so we were playing around with these ideas.  And Leo, remember also when HTTPS happened, when Navigator came out, and SSL was created, there was a 40-bit limit on any cipher that would leave the U.S.



LEO:  Right.



STEVE:  So, you know, we had 128 bits, as long as it didn't try to make a connection outside the U.S.  I mean, it was a mess back then.



LEO:  It was.  It was.



STEVE:  Yeah.  Jonathan Haddock said:  "Hi, Steve.  Thanks for the podcast, having been listening from very early on.  Was just listening to Episode 961 [last week] and the section about Facebook telling users they entered an old password.  A downside of this practice is that an attacker knows the incorrect password was one that the person has used.  Given the prevalence of password reuse, the attacker is then in possession of a confirmed previous password that could still work for that user elsewhere.  A middle ground would be to simply say when the password was last changed.  That way, the user experience is maintained, but the previous validity of the password is not exposed.  Thanks again.  Jonathan."



And I think he makes a very good point.  It's not as useful to tell a user that their password was recently changed, but the fact is that it could be useful to an attacker.  That is, you know, just telling the user your password was changed last Tuesday, well, okay, that's definitely more useful.  But the difference that Jonathan is pointing out is that, if a bad guy is guessing passwords, and is told by Facebook, well, close, you got close, this is the password from last week, well, now the attacker knows he can't get into Facebook with that password, but he may be able to get into something else this user logs into with that password if they haven't, you know, if they were reusing that password elsewhere.



So it's weird because something was bothering me a little bit when we talked about this last week.  Like why isn't this a problem?  It turns out there is some information leakage from Facebook telling their users that.  Probably it's worth doing still, but I'm glad that Jonathan brought this up.



LEO:  Yeah.



STEVE:  Shawn Milochik said, and while we're sharing - oh, I said:  "And while we're sharing brilliant observations from our listeners, here's one from Shawn Milochik."  He said:  "On the topic of passwordless email-only login, I think the largest benefit to the companies is that this is" - that is, using email links for login, which we talked about last week.  He said:  "The largest benefit to the companies is that this practically eliminates password sharing."



LEO:  Oh, you're right.



STEVE:  Isn't that cool?  Yes.  He says:  "It's one thing to give someone your password for a news or a streaming site.  It's quite another to give them access to your e-mail, and probably the ability to reset your banking password, among other things."  So Shawn, brilliant point.  Thank you.



Lars Exeler said:  "Hi, Steve.  Love the show and just became a Club TWiT member."  He said:  "I'm wondering about TOTP as a second factor.  So a password is the 'something you know' factor.  A TOTP is the 'something you own' factor.  My question is, isn't the TOTP just based on a lengthy secret in the QR code?  So in my mind it's just like a second password, with the convenience of presenting itself as six digits because it's combined with the actual time.  What am I missing here?  Regards from Germany, Lars."



Okay.  A password, right, is something you know.  But the way to think of the TOTP, which is based upon a secret key, is that it's "transient and time-limited proof of something else you own or know."  But transient and time-limited proof.  And as we know, a password can be used over and over.  But in any properly designed TOTP-based authentication, the six-digit token not only expires every 30 seconds, but each such token is invalidated after its first use.  This means that even an immediate replay attack which still fits inside that 60-second expiration window will be denied.  It's those two distinctions that make the TOTP a very powerful and different form of second factor.



LEO:  Yeah, you're not - if you were sharing the secret every time, then it would just be another password.  But you're not.



STEVE:  Right.



LEO:  Right?  That's not floating back and forth.



STEVE:  You are proving that you know the secret.  



LEO:  Exactly, yeah.



STEVE:  Without sharing it.  And that's a key.



LEO:  That's one step better.  A little bit better.



STEVE:  Yes.  Gavin Lanoe wrote - and this is a long one, but this needs to be shared.  "Dear Steve.  I first came across GRC in the early 2000s for recovering hard disks and your ShieldsUP! scanner, just as I was at the start of a career in IT and IT Security.  About a year ago, I rediscovered you on the Security Now! podcast and also discovered the wider TWiT network.  The Security Now! podcast over the last year is now the highlight of my week and even goes so far as to have a diary note in my calendar..."



LEO:  Nice.



STEVE:  "...so I can try to listen to the creation live at what is late night where I live."  Gavin hails from Germany.  He said:  "This week I discovered an issue with routers provided by one of our local ISPs, and I thought if there was ever a reason to drop you a message worthy of your time, this was it."  And I would add, even worthy of our listeners' time.



He said:  "Over my 30 years or so in IT, I, as I'm sure we all have, gathered a number of people who we look after for home user/family and friends IT support.  I noticed over the last week or so some of my 400 or so 'private clients' were reporting strange issues.  They reported to be getting warnings when browsing that sites were dangerous and were getting a lot of 404s from streaming services which weren't working, and connections to email services via IMAP secured with SSL were not working, connection and certificate errors and such.



"When I sat back and thought about it, all the users were on the same ISP and were all using the ISP-provided router."  He said:  "Side note:  I usually recommend replacing the ISP's router with something more substantial.  But when it's working fine, or doesn't need anything better, like a single elderly home user with an iPad and nothing else, it's hard to justify the spend."



So he says:  "I got access to one of the users' routers, and after some investigation I found that the DNS servers listed just didn't look right.  A trace route showed the server was in the Netherlands, and we're in Guernsey, Germany.  The listed owner of the IP had nothing to do with the local ISP.  Reverting the DNS Servers to the ISP's addresses resolved all the issues."  And he says:  "Later I reset them to Quad9."  And here it is.  "I also found that the ISP had left SNMP enabled on the WAN interface with no restrictions, and the username and password to gain access to the router were both 'admin.'"



LEO:  No.  That's for their convenience, not yours.  Oh, boy.



STEVE:  That's exactly right.  He said:  "I have rotated through all of my other customers that have the same router and am locking them down, checking the DNS servers, disabling SNMP and resetting the password.  I may even go so far as replacing the routers in their entirety because they may have also been compromised in some other, yet undiscovered way."  Yes, I would say that he's been listening to this podcast.



He said:  "I wrote to the ISP yesterday to explain the issue and received a call back today.  It did take me some time to explain that any suitably skilled bad actor can connect to the routers via SNMP" - which, you know, Simple Network Management Protocol.  And boy, is it dangerous - "with the default credentials, admin/admin."



LEO:  Unbelievable.  Oh, my god.



STEVE:  "And reconfigure" - I know.  "And reconfigure the router settings.  The engineer I was speaking to had trouble comprehending how this was possible without the web management enabled on the WAN side, but he eventually understood.  We contacted another customer while we were talking, and he got to see the issue firsthand and is taking this up with their security team internally."



LEO:  Good.



STEVE:  Lord, I hope so.  He said:  "My understanding for their reason" - here it is, Leo, get this.  "My understanding for their reason for this configuration is" - believe it or not, it was on purpose - "they want to be able to remotely connect to a customer's router to assist in fault finding."



LEO:  Yeah.



STEVE:  And, hah.  They won't have to look very far.



LEO:  Yeah.



STEVE:  "They have a script that connects to the routers via SNMP and enables web management on the WAN interface..."



LEO:  Yup, there you have it right there.  Jesus.



STEVE:  "...for a period of time, and then later it removes the web-based access again."



LEO:  Uh-huh.



STEVE:  "They didn't consider that the open SNMP management interface with default credentials and an easy way to guess community string could be exploited in this way."



LEO:  We never thought of that.



STEVE:  Yeah, wow.



LEO:  What?



STEVE:  Yeah, we're accessing them remotely.



LEO:  Yeah, but that's us.



STEVE:  You mean bad guys can, too?



LEO:  Geez.



STEVE:  Wow.  "The engineer," he says, "the engineer did seem quite disgruntled" - get this - "disgruntled that I often replace the ISP-provided routers."



LEO:  How dare you?



STEVE:  Yes.  "But as I explained to him, if it has a vulnerability on it, I'm going to replace it without any consideration for their convenience of remote diagnosis."



LEO:  Absolutely.  Geez.



STEVE:  He said:  "Thank you, Steve.  I hope this may get a mention on the show."  And how.  "But regardless, I really do look forward every week to your deep dives into detail behind the week's security-related news.  Warm regards, Gavin from Guernsey."



So wow, Gavin.  Thank you for sharing that.  Apparently this ISP believes that SNMP stands for "Simply Not My Problem."  But they would be wrong.



LEO:  They would be wrong.



STEVE:  And this deliberate unprotected SNMP exposure with default credentials is breathtaking.  It is a horrific breach of security.



LEO:  Yeah.



STEVE:  SNMP, for those who don't know, is a funky, non-user-friendly, UDP-based protocol.  It was originally and is still present in contemporary devices.  But, like, very old original networking gear which allows for both the remote over-the-wire monitoring and control of that gear.  Unfortunately, it also comes from an era before security was being given the due that it deserves.  On the one hand, it's a good thing that this ISP has the wisdom to keep their customers' web management interfaces disabled by default.  But that good intention is completely undone by their persistent exposure of the router's SNMP service, which I'm still shaking my head over.



So one remaining mystery is what was going on with the DNS rerouting.  Given that most of today's DNS is still unencrypted in-the-clear UDP, this would be part of the requirement for site spoofing, to be able to spoof DNS.  But with browsers now having become insistent upon the use of HTTPS, it's no longer possible to spoof a site over HTTP.  So someone would need to be able to create web server certificates for the sites they wish to spoof.  The fact that it's so widespread across an ISP's many customers tells us that it's probably not a targeted attack.  That means it's somehow about making money, or intercepting a lot of something.



Having thought about this further, though, Leo, he mentioned lots of email problems when people are using secure IMAP.  Well, not all email is secure.  And so you could spoof email MX records in DNS in order to route email somewhere else, in which case clients would have a problem connecting if the DNS was spoofed over a TLS connection.  So basically it's really only HTTP in the form of HTTPS where we've really got our security buttoned down tight with web server certificates.  Other things that use DNS, and there are lots of other things that use DNS, they're still not running as securely as the web is.  So that could be an issue.



Jeff Zellen said:  "Steve, listening to SN-961 and thinking about the discussion on passwordless logins and using email as a single factor.  Is my recollection correct that email is generally transmitted in the clear?  Can't some unscrupulous actor sniff that communication?  I'm especially concerned if the email contains a link to the site.  This provides all the necessary info to anyone who can view the email.  A six-digit code without any reference to the site where it should be used would be more secure as it is both out of band, but also obfuscated from the site.  Of course this is all based on my possibly antiquated recollection of the lack of privacy in email."



Okay.  Now, the email transit encryption numbers vary.  I saw one recent statistic that said that only about half of email is encrypted in transit using TLS.  But Google's most recent stats for Gmail state that 98 to 99% of their email transit, both incoming and outgoing, is encrypted.  And it's a good feeling to have that.  GRC's server fully supports all of the transit encryption standards, so it will transact using TLS with the remote end whenever it's available.  And it's a good feeling knowing that when Sue and Greg and I exchange anything through email, since our email clients are all connecting directly to GRC's email server, everything is always encrypted end to end.  



But it's not encrypted at rest.  Jeff's point about the possible lack of email encryption in transit is still a good one, since email security is definitely lagging behind web security now that virtually everything has switched over to HTTPS.  On the web, we would never even consider logging into a site that was not encrypted and authenticated using HTTPS.  For one thing, we would wonder why the site was doing that; and, like, alarm bells would be going off.  And even back when almost everything was still using HTTP, websites would switch their connections over to HTTPS just long enough to send the login page and receive the user's credentials in an encrypted channel, and then drop the user back to HTTP.



So by comparison to the standard set by today's web login with HTTPS, email security is sorely lacking.  If we revisit the question then of using email only to login to a site, where that email carries what is effectively a login link, or even a six-digit authentication token, what we've done is reduce the login security of the site to that of email, which today we would have to consider to be good, but not great, and certainly far from perfect.



Another difference worth highlighting is that a web browser's connection to the website it's logging into is strictly and directly end-to-end encrypted.  The browser has a domain name, and it must receive an identity asserting certificate that is valid for that domain.  But this has never been true for email. Although modern email does tend to be point-to-point, with a server at the sender's domain directly connecting to the server at the recipient's domain, that's not always, nor is it necessarily true.  Email has always been a store-and-forward transport.  Depending upon the configuration of the sender and receiver's domains, email might make several hops from start to finish.  And since the body of the email doesn't contain any of its own encryption, that email at rest is subject to eavesdropping.



Last week I coined the term "login accelerator" to more clearly delimit that value added by a password.  And the whole point of the email loop authentication model is the elimination of the "something you need to know" factor.  Yet without the "something you need to know" factor, the security of email loop authentication can be no better than the security of email,  which we've agreed falls short of the standard set by the web's direct end-to-end certificate identity verification with encryption.



As a result of this analysis, we might be inclined to discount the notion of email loop authentication as being inferior to web-based authentication with all of its fancy certificates and encryption, except for one thing.  The ubiquitous "I forgot my password" get out of jail free link is still the party spoiler.  It is everywhere, and its presence immediately reduces all of our much-ballyhooed web security to the security of email, imperfect as it may be.  Even without email loop authentication, if a bad guy can arrange to gain access to a user's email, they can go to any site where the user maintains an account, click the "I forgot my password" link, receive the password recovery link by intercepting their email, and do exactly the same thing as if that site was only using email loop authentication.



What this finally means is that all of our identity authentication login, either using all of the fancy web technology, or just a simple email loop, is not actually any more secure than email, and that simple email loop authentication without any password is just as secure as web authentication, so long as web authentication includes an "I forgot my password" email loop bypass.  So the notion of a password being nothing more than a login accelerator holds, and the world should be working to increase the security of email since it winds up being the linchpin for everything.  So, really interesting thought experiment there.



Two last quickies.  Alex Neihaus, a long-time friend of the podcast and early advertiser, I think, what, our first advertiser...



LEO:  I think our first, yeah, with Astaro, yeah.



STEVE:  Yeah.  He said:  "Regarding 961" - last week's episode - "and overlay networks, pfSense has a killer Tailscale integration.  With this running, you can set up pfSense to be an exit node, combining safe local access and VPN-like move-my-apparent-IP-address.  That's useful for services that are tied to your location.  Second, some cable providers' apps require UPnP, which you can, at your own risk, enable in pfSense-Tailscale."



So Alex, thank you for that.  Given that I'm myself a pfSense shop, as Alex knows because he and I sometimes talk about pfSense, and that Tailscale integrates well with pfSense's FreeBSD Unix, Tailscale will certainly be a contender for a roaming overlay network.  So thank you, Alex.



And finally, just a reminder from a listener.  He said:  "Hi, Steve.  Given your discussion of throwaway email addresses during last Security Now! podcast episode, I'd like to bring your attention to DuckDuckGo's Email Protection Service.  This free service is designed to safeguard users' email privacy by removing hidden trackers and enabling the creation of unlimited unique private email addresses on the fly.  This feature allows users to maintain a distinct email address for each website they interact with.



"What makes this even more compelling is DuckDuckGo Email Protection's integration with Bitwarden.  Bitwarden has included DuckDuckGo among its supported services, alongside other email forwarding services.  This integration allows Bitwarden users to easily generate unique email addresses for each website they interact with.  Take a look at" - and he provided the links which I've got in the show notes.  So I thank our listener very much for that.



LEO:  Okay.  This I've been waiting all day for, Steve.  What the heck?  Are we in trouble?



STEVE:  No.



LEO:  Oh, good.



STEVE:  The bullet was dodged.  But the story is really interesting.  The vulnerability has been codenamed "KeyTrap," and if ever there was a need for responsible disclosure of a problem, this was that time.  Fortunately, responsible disclosure is what it received.  What the German researchers who discovered this last year realized was that an aspect of the design of DNS, specifically the design of the secure capabilities of DNS, known as DNSSEC, could be used against any DNSSEC-capable DNS resolver to bring that resolver to its knees.  The receipt of a single UDP DNS query packet could effectively take the DNS resolver offline for as many as 16 hours, pinning its processor at 100%, and effectively denying anyone else that server's DNS services.



It would have therefore been possible to spray the Internet with these single-packet DNS queries to effectively shut down all DNS services across the entire globe.  Servers could be rebooted once it was noticed that they had effectively hung; but if they again received another innocuous-looking DNS query packet, which is their job after all, they would have gone down again.  Eventually, of course, the world would have discovered what was bringing down all of its DNS servers.  But the outage could have been protracted, and the damage to the world's economies could have been horrendous.  So now you know why this podcast is titled "The Internet Dodged a Bullet."  We should never underestimate how utterly dependent the world has become on the functioning of the Internet.  I mean, it's like, what don't we do with the Internet now?  Like all of our entertainment, all of our news, you know, do I have an FM radio?  I'm not sure.



Okay.  The detailed research paper describing this was just publicly released yesterday, though the problem, as I said, has been known since late last year.  Here's how the paper's Abstract describes what these German researchers discovered, to their horror.  They wrote:  "Availability is a major concern in the design of DNSSEC.  To ensure availability, DNSSEC follows Postel's Law, which reads:  'Be liberal in what you accept, and conservative in what you send.'  Hence, nameservers should send not just one matching key for a record set, but all the relevant cryptographic material, in other words, all the keys for all the ciphers that they support and all the corresponding signatures.  This ensures that validation will succeed; and hence availability, even if some of the DNSSEC keys are misconfigured, incorrect, or correspond to unsupported ciphers, will be maintained."



They write:  "We show that this design of DNSSEC is flawed.  By exploiting vulnerable recommendations in the DNSSEC standards, we develop a new class of DNSSEC-based algorithmic complexity attacks on DNS, which we dub KeyTrap attacks.  All popular DNS implementations and services are vulnerable."  This is them writing.  "With just a single DNS packet, the KeyTrap attacks lead to a two million times spike in CPU instruction count in vulnerable DNS servers" - remember, that's all DNS servers - "stalling some for as long as 16 hours.  This devastating effect prompted major DNS vendors to refer to KeyTrap as 'the worst attack on DNS ever discovered.'  Exploiting KeyTrap, an attacker could effectively disable Internet access in any system utilizing a DNSSEC-validating resolver.



"We disclosed KeyTrap to vendors and operators on November 2, 2023.  We confidentially reported the vulnerabilities to a closed group of DNS experts, operators and developers from the industry.  Since then, we have been working with all major vendors to mitigate KeyTrap, repeatedly discovering and assisting in closing weaknesses in proposed patches.  Following our disclosure, an umbrella CVE was assigned."



Okay.  So believe it or not, all that just actually happened, as they say, behind closed doors.  Google's Public DNS and Cloudflare were both vulnerable, as was the very popular, the most popular and widely deployed BIND 9 DNS implementation, and it was the one - we'll see why later - that could be stalled for as long as 16 hours after receiving one packet.



So what happened?  As the researchers wrote, months before all this came to light publicly, all major implementations of DNS had already been working on quietly updating because had this gotten out, it could have been used to bring the Internet down.  With Microsoft's release of patches which included mitigations for this last week, and after waiting a week for them to be deployed, the problem is mostly resolved, if you'll pardon the pun.



I say "largely" because this is not a bug in an implementation.  And apparently even now, as we'll see, some DNS servers will still have their processors pinned, but they will still at least be able to answer other DNS query functions.  It sounds as though thread or process priorities have been changed to prevent the starvation of competing queries.  And we'll actually look in a minute at the strategies that have been deployed.  Characterizing this as a "big mess" would not be an exaggeration.



KeyTrap exploits a fundamental flaw in the design of DNSSEC which makes it possible to deliberately create a set of legal, but ultimately malicious, DNSSEC response records which the receiving DNS server will be quite hard pressed to untangle.  Once the researchers had realized that they were onto something big, they began exploring all of the many various ways DNS servers could be stressed.  So they created a number of different attack scenarios.



I want to avoid getting too far into the weeds of the design and operation of DNSSEC, but at the same time I suspect that this podcast's audience will appreciate seeing a bit more of the detail so that the nature of the problem can be better appreciated.  The problem is rooted in DNSSEC's provisions for the resolution of "key tag" collisions, the handling of multiple keys when they're present, and multiple signatures when they're offered.



I'm going to quote three paragraphs from their research paper.  But, you know, just sort of let it wash over you so that you'll get some sense for what's going on without worrying about understanding it in any detail.  You will not be tested on your understanding of this.  Okay.  So they explain:  "We find that the flaws in the DNSSEC specification are rooted in the interaction of a number of recommendations that in combination can be exploited as a powerful attack vector."



Okay.  So first, key tag collisions.  They write:  "DNSSEC allows for multiple cryptographic keys in a given DNS zone."  Zone is the technical term for a domain, essentially.  So when they say "zone," they mean a DNS domain.  "For example," they say, "during key-rollover or for multi-algorithm support."  Meaning you might need multiple cryptographic keys.  If you're retiring one key, you want to bring the new key online, right, before the old key goes away.  So for a while you've got two or more keys.  Or multi-algorithm support.  Might need different keys for different algorithms if you want to be more comprehensive.



So they said:  "Consequently, when validating DNSSEC, DNS resolvers are required to identify a suitable cryptographic key to use for signature verification."  Because, you know, the zone is signed, so you want to verify the signature of the zone to verify it's not been changed.  That's what DNSSEC is all about is preventing any kind of spoofing.  So they said:  "DNSSEC uses key tag values to differentiate between the keys, even if they are of the same zone and use the same cryptographic algorithm."  So they just could be redundant keys for some reason.



They said:  "The triple of zone, algorithm, and key tag is added to each respective signature to ensure efficiency in key-signature matching."  Again, don't worry about the details.  "When validating a signature, resolvers check the signature header and select the key with the matching triple for validation.  However, the triple is not necessarily unique."



And that's the problem.  "Multiple different DNS keys can have an identical triple."  That is to say an identical tag.  "This can be explained by the calculation of the values in the triple.  The algorithm identifier results directly from the cipher used to create the signature and is identical for all keys generated with a given algorithm.  DNSSEC mandates all keys used for validating signatures in a zone to be identified by the zone name.  Consequently, all DNSSEC keys that may be considered for validation trivially share the same name.  Since the collisions in algorithm-id and key name pairs are common, the key tag is calculated with a pseudorandom arithmetic function over the key bits to provide a means to distinguish same-algorithm, same-name keys."  Again, just let this glaze over.



"Using an arithmetic function instead of a manually chosen identifier eases distributed key management for multiple parties in the same DNS zone.  Instead of coordinating key tags to ensure their uniqueness, the key tag is automatically calculated.  However" - here it comes - "the space of potential tags is limited by the 16 bits in the key tag field.  Key tag collisions, while unlikely, can thus naturally occur in DNSSEC.  This is explicitly stated in RFC-4034, emphasizing that key tags are not unique identifiers.  As we show, colliding key tags can be exploited to cause a resolver not to be able to uniquely identify a suitable key efficiently, but to have to perform validations with all the available keys, inflicting computational effort during signature validation."



Okay, now, just to interrupt this for a second, cryptographic keys are identified by tags, and those tags are automatically assigned to those keys.  Work on DNSSEC began way back in the 1990s, when the Internet's designers were still counting bits and were assigning only as many bits to any given field as would conceivably be necessary.  Consequently, the tags being assigned to these keys were, and are still today, only 16 bits long.  Since these very economical tags only have 16 bits, thus one of 64K possible values, inter-tag collisions, while unlikely, you know, one of 65536, and we've got the birthday paradox which makes collisions more, you know, happen more often than you'd expect.



If DNSSEC were being designed today, tags would be the output of a collision-free cryptographic hash function, and there would be no provision for resolving tag collisions because there would be none.  The paragraph I just read said "the key tag is calculated with a pseudorandom arithmetic function," in other words, something simple from the '90s that scrambles and mixes the bits around, but doesn't do much more.



LEO:  Like a salad spinner or something.



STEVE:  Right, exactly.  It's still salad.  It's just better now.  Consequently, servers need to consider that key tags are not unique.  And so what the attack is, is it deliberately  makes all the key tags identical, forcing the server to check them all.



LEO:  Oh, brilliant.



STEVE:  Yes, yes.



LEO:  How many key tags can you put in there?



STEVE:  We're getting there.  We're getting there.  But you make them all the same, and the server can't use it to select the key.  It's got to try them all.



LEO:  Right.



STEVE:  So the first attack is key tag collisions.  Literally, they all collide.  Okay.  On to the next problem, multiple keys.  "The DNSSEC specification mandates that a resolver must try all colliding keys until it finds a key that successfully validates the signature, or all keys have been tried."



LEO:  Of course.



STEVE:  Right.



LEO:  What could possibly go wrong?



STEVE:  That's right.  "The requirement is meant to ensure availability, meaning DNSSEC will try as hard as it can to find a successful signature.  Even if colliding keys occur, such that some keys may result in failed validation, the resolver must try validating with all the keys until a key is found that results in a successful validation.  This ensures that the signed record remains valid, and the corresponding resource therefore remains available."



However, what they call eager - they say:  "However, this 'eager validation' can lead to heavy computational effort for the validating resolver, since the number of validations grows linearly with the number of colliding keys.  So, for example, if a signature has 10 colliding keys, all with identical algorithm identifiers, the resolver must conduct 10 signature validations before concluding that the signature is invalid.  While colliding keys are rare in real-world operation, we show that records created to deliberately contain multiple colliding keys" - meaning all the keys are colliding - "can be efficiently crafted by an adversary imposing heavy computation upon a victim resolver."



Okay.  And the third and final problematic is multiple signatures:  "The philosophy," they said, "of trying all of the cryptographic material available to ensure that the validation succeeds also applies to the validation of signatures.  Creating multiple signatures for a given DNS record can happen, for example, during a key-rollover.  The DNS server adds a signature with the new key, while retaining the old signature to ensure that some signature remains valid for all resolvers until the new key has been propagated.  Thus, parallel to the case of colliding keys, the RFCs specify that in the case of multiple signatures on the same record, a resolver should try all the signatures it received until it finds a valid signature or until it's used up all the signatures."



Okay.  So we have the essential design features which were put into the DNSSEC specification in a sane way, I mean, all this makes sense, with the purpose of never failing to find a valid key and signature for a zone record.  Their term for this, of course, is "eager validation."  They write:  "We combine these requirements for the eager validation of signatures and of keys, along with the colliding key tags to develop powerful DNSSEC-based algorithmic complexity attacks on validating DNS resolvers.  Our attacks allow a low-resource adversary to fully DoS a DNS resolver for up to 16 hours with a single DNS request."



LEO:  Holy cow.



STEVE:  Yeah.



LEO:  Wow.



STEVE:  One request, and the server goes down for 16 hours.  They wrote:  "Members from the 31-participant task force of major operators, vendors, and developers of DNS and DNSSEC, to which we disclosed our research, dubbed our attack 'the most devastating vulnerability ever found in DNSSEC.'"



Okay.  Now, the researchers devised a total of four different server-side resource exhaustion attacks.  And I have to say, Leo, I was a little bit tempted to title today's podcast after three of them.  Had I done so, today's podcast would have been titled "SigJam, LockCram, and HashTrap."



LEO:  I would have done that.



STEVE:  I know.



LEO:  That's good.  That's good.



STEVE:  I know.  SigJam, LockCram, and HashTrap.



LEO:  It's a little like a law firm.



STEVE:  And while I certainly acknowledge that would have been fun, I really didn't want to pass up, you know, I didn't want to lose sight of the fact that the entire global Internet really did just dodge a bullet.  And we don't know which foreign or domestic cyber intelligence services may today be silently saying "Darn it.  They found it.  That was one we were keeping in our back pocket for a rainy day, while keeping all of our foreign competitor DNS server targeting packages updated."  Because this would have made one hell of a weapon.



Okay.  So what are the four different attacks?  SigJam utilizes an attack with one key and many signatures.  They write:  "The RFC advises that a resolver should try all signatures until a signature is found that can be validated with the DNSKEY.  This can be exploited to construct an attack" - we're going to answer your question, Leo, about how many - "using many signatures that all point to the same DNSSEC key.  Using the most impactful algorithm" - meaning the most time-consuming to verify - "an attacker can fit 340 signatures into a single DNS response, thereby causing 340 expensive cryptographic signature validation operations in the resolver until the resolution finally fails by returning SERVFAIL response to the client."



LEO:  That shouldn't take 16 hours.



STEVE:  Ah, it gets better.



LEO:  Oh.  Because it's more.



STEVE:  Because that's linear.  We're going to go quadratic in a minute.



LEO:  Oh, good.



STEVE:  "The SigJam attack is thus constructed by leading the resolver to validate many invalid signatures on a DNS record using one DNS key."



Okay.  The LockCram attack does the reverse, using many keys and a single signature.  They write:  "Following the design of SigJam, we develop an attack vector we dub LockCram.  It exploits the fact that resolvers are mandated to try all keys available for a signature until one validates or all have been tried.  The LockCram attack is thus constructed by leading a resolver to validate one signature over a DNS record having many keys.  For this, the attacker places multiple DNS keys in the zone which are all referenced by signature records having the same triple - name, algorithm, key tag.  This is not trivial, as resolvers can de-duplicate identical DNSKEY records, and their key tags need to be equal.



"A resolver that tries to authenticate a DNS record from the zone attempts to validate its signature.  To achieve that, the resolver identifies all the DNS keys for validating the signature, which, if correctly constructed, conform to the same key tag.  An RFC-compliant resolver must try all the keys referred to by the invalid signature before concluding the signature is invalid for all keys, leading to numerous expensive public key cryptography operations in the resolver."



And the next attack, the KeySigTrap, combines the two previous attacks by using multiple keys and multiple signatures.  They say:  "The KeySigTrap attack combines the many signatures of SigJam with the many colliding DNSKEYs of LockCram, creating an attack that leads to a quadratic increase in the number of validations compared to the previous two attacks."



LEO:  SigLock JamCram is so much worse.  Wow.



STEVE:  Yeah, you don't want one of those.



LEO:  No.



STEVE:  "The attacker creates a zone with many colliding keys and many signatures matching those keys.  When the attacker now triggers resolution of the DNS record with the many signatures, the resolver will first try the first key to validate all the signatures.  After all the signatures have been tried, the resolver will move to the next key and again attempt validation of all the signatures.  This continues until all pairs of keys and signatures have been tried.  Only after attempting validation on all possible combinations does the resolver conclude that the record cannot be validated and returns a SERVFAIL to the client."



Okay, now, elsewhere in their report, when going into more detail about this KeySigTrap, they explain that they were able to set up a zone file containing 582 colliding DNSSEC keys and the same 340 signatures that we saw in SigJam.  Since the poor DNS resolver that receives this response will be forced to test every key against every signature, that's 582 times 340, or 197,880 expensive and slow public key cryptographic signature tests.  And that was caused by sending that DNS server a single DNS query packet for that domain.



Now, interestingly, the researchers discovered that not all DNS servers were as badly affected.  For some reason, several took the request much harder.  Upon investigating they discovered why.  For example, the Unbound DNS server is DoSed approximately six times longer than some of the other resolvers.  The reason is the default re-query behavior of Unbound.  In its default configuration, Unbound attempts to re-query the nameserver that gave it this malicious zone five times after failed validation of all signatures.  Therefore, Unbound validates all attacker signatures six times before returning a SERVFAIL to the client.  Essentially, Unbound is being penalized for being a good citizen.  Disabling default re-queries brings Unbound back to parity with the other servers.



But BIND, the famous, I mean, it's the most-used server on the Internet, BIND - in fact, that's where Unbound got its name, right, it's a play on BIND.  BIND was the worst.  That's the one with the 16-hour DoS from a single packet.  And they explained why:  "Investigating the cause for this observation, we identified an inefficiency in the code, triggered by a large number of colliding DNSSEC keys.  The routine responsible for identifying the next DNSSEC key to try against a signature does not implement an efficient algorithm to select the next key from the remaining keys.  Instead, it reparses all keys again until it finds a key that has not been tried yet.  This algorithm does not lead to inefficiencies in normal operation where there might be a small number of colliding keys.  But when many keys collide, the resolver spends a large amount of time parsing and reparsing the keys to select the next key, which extends the total response duration of the DoS to 16 hours."



So what all of this should make clear is that these potential problems arise due to DNSSEC's deliberate "eager to validate" design.  You know, the servers are trying as hard as they can to find a valid key and signature match.  DNS servers really want to attempt all variations, which is exactly, unfortunately, what gets them into trouble.  The only solution will be something heuristic.  We've talked about heuristics in the past.  They can be thought of as a rule of thumb, and they usually appear when exact solutions are not available, which certainly is the case here.



As we might expect, the researchers have an extensive section of their paper devoted to "what to do about this mess," and they worked very closely for months with all the major DNS system maintainers to best answer that question.  I'll skip most of the blow-by-blow, but here's a bit that gives you sort of a sense and a feeling for it.



Under the subhead "Limiting all validations," they wrote:  "The first working patch capable of protecting against all variants of our attack was implemented by Akamai.  In addition to limiting key collisions to four, and limiting cryptographic failures to 16, the patch also limits total validations in any requests to eight."  In other words, they basically just said it is Akamai patched their DNS to say, okay, it's unreasonable for anyone to expect our server to work this hard.  There are not really going to be that many key collisions in a DNSSEC zone.  There aren't, you know, it's just not going to happen in real life.  And we're not going to constantly be failing.



"So let's just stop after we've got four key collisions.  We're just not - we're just going to say, sorry, times up, you know, we're not going to go any further.  And let's just stop after we've had 16 cryptographic failures.  No matter what kind and what nature, that's all we're going to do because it's unreasonable for any valid DNSSEC zone to have more.  And we're also going to cap the total limit of validation requests to eight."



So then they wrote:  "Evaluating the efficacy of the patch, we find the patched resolver does not lose any benign request" - meaning DoS is avoided - "even under attack with greater than 10 attacking requests per second."  In other words, it doesn't fix the problem, it just it mitigates it.  It gets it under control.  "The load on the resolver does not increase to problematic levels under any type of attack at 10 requests, 10 malicious requests per second, and the resolver does not lose any benign traffic.  It thus appears that the patch successfully protects against all variations of KeyTrap attacks.



"Nevertheless," they said, "although these patches prevent packet loss, they still do not fully mitigate the increase in CPU instruction load during the attack."  You know, it's still an attack.  "The reason that the mitigations do not fully prevent the effects of the KeyTrap attacks is rooted in the design philosophy of DNSSEC.  Notice, however, that we are still closely working with the developers on testing the patches and their performance during attack and normal operation."  "Still" as in today still.



Okay.  So the disclosure timeline, you know, the responsible disclosure timeline for this is extra interesting since it provides a good sense for the participants and the nature of their efforts and interactions over time.  So in the following, they wrote:  "We describe the timeline of disclosure to indicate how the vulnerability was reported and how we worked with the experts from industries to find solutions for the problems we discovered."



Okay.  So November 2nd of 2023, the initial disclosure to key figures in the DNS community.  They said:  "Both confirm that KeyTrap is a severe vulnerability, requiring a group of experts from industry to handle."



Five days go by.  Now we're at November 7th.  "Confidential disclosure to representatives of the largest DNS deployments and resolver implementations, including Quad9, Google Public DNS, BIND9, Unbound, PowerDNS, Knot, and Akamai.  The group of experts agree that this is a severe vulnerability that has the potential to cut off Internet access to large parts of the Internet in case of malicious exploitation.  A confidential chat-group is established with stakeholders from the DNS community, including developers, deployment specialists, and the authors."  That is, you know, the researchers here.  "The group is continuously expanded with additional experts from the industry to ensure every relevant party is included in the disclosure.  Potential mitigations are discussed within the group."



Two days later, November 9th:  "We share KeyTrap zone files to enable developers to reproduce the attacks locally, facilitating the development of mitigations."



November 13th, after four days:  "Akamai presents the first potential mitigation of KeyTrap by limiting the total number of validation failures to 32."  That doesn't work.



November 23rd, 10 days later:  "Unbound presents its first patch, limiting cryptographic failures to a maximum of 16, without limiting collisions."  Also a non-starter.



Next day:  "BIND9 presents the first iteration of a patch that forbids any validation failures."



Okay, now we jump to December 8th, so a couple weeks go by.  The CVE is assigned to the KeyTrap attacks, although nothing is disclosed.  It's just an umbrella CVE to encompass them all.



Now we move to January 2nd this year, 2024, beginning of the year:  "After discussions with the developers, we find some have problems recreating the attack in a local setup.  We thus provide them an updated environment with a DNS server to ease local setup and further facilitate testing of patches.  In other words, the researchers actually put this live on the Internet so that the DNS developers could DoS their own servers."



March 1st.  A month goes by, or two months go - no, I'm sorry, the next day.  I'm getting the dates are in an odd, well, you know, day/month/year.  Anyway, so January 3rd:  "BIND9 presents the second iteration of a patch, limiting validation failures."



Same day, they said:  "Our newly implemented DS-hashing attack proves successful against all mitigations which do not limit key collisions, including BIND9 and Unbound, and is disclosed to the group."  So, whoops, patch again, everybody.



January 16th:  "Our Anytype attack circumvents the protection from limiting colliding keys and limiting cryptographic failures."



And on January 24th:  "The first working patch is presented by Akamai.  Other resolvers are implementing derivatives of the countermeasures to protect against the attacks."



And so now we get to the reports and the German discoverers of this final conclusions.  They write:  "Our work revealed a fundamental design problem with DNS and DNSSEC.  Strictly applying Postel's Law to the design of DNSSEC introduced a major and devastating vulnerability in virtually all DNS implementations.  With just one maliciously crafted DNS packet, an attacker could stall almost any resolver, for example the most popular one, BIND9, for as long as 16 hours.



"The impact of KeyTrap is far reaching.  DNS evolved into a fundamental system in the Internet that underlies a wide range of applications and facilitates new and emerging technologies.  Measurements by APNIC show that in December of 2023, 31.47% of web clients worldwide were using DNSSEC-validating resolvers.  Therefore, our KeyTrap attacks have effects not only on DNS itself, but also on any application using it.  An unavailability of DNS may not only prevent access to content, but risks also disabling other security mechanisms, like anti-spam defenses, Public Key Infrastructure, or even inter-domain routing security like RPKI or ROVER.



"Since the initial disclosure of the vulnerabilities, we've been working with all major vendors on mitigating the problems in their implementations, but it seems that completely preventing the attacks will require fundamentally reconsidering the underlying design philosophy of DNSSEC, in other words, to revise the DNS standards."



LEO:  Wow.



STEVE:  So as we can see, as I titled this podcast, the Internet really did dodge a bullet.  And I've got to say it's also terrific to see that the technical and operational level of all of this, at that level we have the ability to quickly gather and work together to resolve any serious trouble that may arise.  Fortunately, that doesn't happen very often.  It just did, and it's like, whew, exactly, everybody is breathing a deep sigh.



LEO:  It was never exploited by anybody.  Do we know?



STEVE:  Nope.



LEO:  No.



STEVE:  No.  Far as we know.  I mean, had it been, it would have been discovered.  These guys...



LEO:  You would know, yeah, yeah.



STEVE:  Basically they reverse engineered this from the spec, kind of going, well, what if we were to do this?  And what if we were to do that?  And when the servers went offline, they thought, uh, whoops.



LEO:  Yeah.  Yeah.  What if we did the - oh.  Hello.



STEVE:  Yeah.  And how can we make it worse?  And how can we make it worse?  And how can we make it worse?



LEO:  Well, good research.  And let's hope everybody fixes their - it's ironic because there was a huge push to get everybody to move to DNSSEC for a while.  We talked about it a lot.



STEVE:  Yup.



LEO:  Oh, well.



STEVE:  Yup.



LEO:  Steve, another fabulous show in the can, as they say.



Copyright (c) 2024 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.






GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#963

DATE:		February 27, 2024

TITLE:		Web portal?  Yes, please!

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-963.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  What U.S. state is now trying to ban encryption for minors?  What shocking truth did a recent survey of IT professionals reveal?  What experimental feature from Edge is Chrome inheriting?  Are online services really selling our private data?  And what about browser add-ons?  Should we be paying extra to obtain cloud security logs?  Now that the dust has settled, what happened with LockBit?  What new features just appeared in Firefox v123?  And what lesson have we just received another horrific example of?  I have news on the GRC software front, and we have a bunch of interesting feedback from our terrific podcast listeners.  So another jam-packed episode of Security Now!.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  We've got a great show planned for you.  Steve's going to talk about that web-based login that's supposed to be secure.  Turns out it's not, not even close.  I hope you're not using it.  We'll talk about the state of Nevada.  Their attorney general wants to ban encryption on Facebook Messenger, but just for kids.  That'll make them safer; right?  And Steve has a new app he just made just for you.  Then a whole lot more coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 963, recorded Tuesday, February 27th, 2024:  Web Portal?  Yes, Please!



It's time for Security Now!, the show where we cover the latest security news with this guy right here, everybody's favorite geek, Mr. Steve Gibson.  Hello, Steve.



STEVE GIBSON:  Yo, Leo.



LEO:  How are you today?



STEVE:  Good to be back with you for the - I just realized we've got a leap year.  I was looking at the calendar.



LEO:  Yes, we get an extra day.



STEVE:  I said, hey, February's got 29 days.



LEO:  Aren't we excited.  What are you doing to do on your free Thursday?



STEVE:  How often does that happen?  Well, we know exactly how often.



LEO:  We know exactly.		



STEVE:  Because many of us have written, what was the linear date...



LEO:  Oh, god, it's hard to do that code.



STEVE:  It is a mess, yes.  It is, like, who came up with this calendar?  Certainly not any programmer because no one would do that to themselves.



LEO:  The 29th, I know, well, but they have to because the actual clock is a little bit off from the calendar.  But what is it, it's the 29th day every year that ends in four unless it ends in zero zero.



STEVE:  Like 100 and 400 are also exceptions, I think.



LEO:  100 and 400, yeah.



STEVE:  And, boy, whenever there's any talk about, like, well, we're going to get rid of this daylight savings time because it's a real mess, we're just going to stay on one, I think all the technology now that knows when the time changes would get broken; right?  I mean, like, there's a lot of different things like clocks that have that built into them now.



LEO:  Oh, yeah.



STEVE:  That's like, okay, well, there's more problems.



LEO:  For the longest...



STEVE:  Anyway, speaking...



LEO:  Go ahead.



STEVE:  Speaking of problems, Leo, our listeners come here to find out about problems. 



LEO:  Yes.



STEVE:  And, boy, have we have got some problems for them today.  I titled this "Web Portal?  Yes, Please!" for a reason that we'll be getting to.  But we've got a lot of interesting questions to answer.  What U.S. state is now trying to ban encryption for minors?  Which has, like, got a lot of people wound up, as you can imagine.  What shocking truth did a recent survey of IT professionals reveal?  Things are not good for them.  What experimental feature from Edge is Chrome inheriting?  Are online services really selling our private data?  And just how big a problem is that?  And what about browser add-ons?  Them, too?  Should we be paying extra to obtain cloud security logs, is the question.  And now that the dust has settled somewhat, what happened with LockBit?  What new features just appeared in Firefox 123?  And what lesson have we just received about another horrible instance occurrence?  We'll be getting to that.  And I have some news on the GRC software front.



LEO:  Ooh.



STEVE:  And we also have a bunch of interesting feedback from our terrific podcast listeners.  So another jam-packed episode of Security Now.  And there were a couple things I couldn't get to, but I'll tell everybody about those at the end because we'll be getting to a couple of them next week.



LEO:  Oh, good.



STEVE:  So I think a lot of fun in store.



LEO:  You're busy again, all week long, preparing for this show.  We also have a Picture of the Week that's tied to the headlines.  It's ripped from today's headlines.



STEVE:  That's right.



LEO:  All right.  Well, you know what else doesn't get old, your Picture of the Week.  Tell us about this one.



STEVE:  Yes.  Now, somebody repurposed this picture for the news, as you mentioned.  First of all, it started off just as a great picture all by itself.  We see a yellow painted cinderblock wall that has sort of an exterior because, you know, cinder block is you can't, you know, you're not going to install an outlet in the cinderblock.  So it's a steel, two-plug, AC outlet box which is mounted on the outside of the cinderblock.  And it's got a black cord and an orange cord plugged into it.  And they're sort of running off the screen.  But taped next to what we learn is a very important set of cords, it says, "Do not unplug!!  Magic cord runs entire company!!"



So, you know, clearly the lesson here is, whatever you do, just don't, you know, like if you need to vacuum the floor, don't unplug one of these to plug the vacuum cleaner in temporarily.  Just go plug your vacuum cleaner in somewhere else.  Well, repurposing this picture for today's news, it says down below, "Live Look at AT&T Network Security."



LEO:  Oh, boy.



STEVE:  Where of course somebody tripped over a cord somewhere.  And you know, AT&T's been curiously unsatisfying in what little they've said.  I mean, we heard that, you know, that DHS, our U.S. Department of Homeland Security, and CISA, and the FBI were going to roll up their sleeves and get to the bottom of this major 12-hour outage that happened last week.  I'm sure anybody with AT&T probably knew, and even people trying to call people with AT&T knew.  And then AT&T just sort of said, well, it's an update that we were doing.  The software went sideways.



LEO:  They didn't even say software.  I mean, their statement was completely...



STEVE:  Opaque.



LEO:  Opaque is a good word. 



STEVE:  Yes, yes.  It is the word of the day, actually.  I'll be using "opaque" several more times by the time we're done.  There's a lot of opacity in today's podcast.  I'm not sure why that all landed today.  But, yeah.  It's like, what?  You know?  And it's not like they're some little nothing company, right, that doesn't matter.  It's like, this is important.  People want to know, you know, what happened?  But AT&T's like not really telling us yet.



LEO:  I hope we find out something.



STEVE:  I imagine they have to tell their shareholders or something, I would think.



LEO:  You know, after you talked so many years about BGP routing mistakes, I thought could be that.  Then some wag on Mastodon wrote, you know, probably a certificate expired on some server in a closet somewhere that the only guy who knows it's there was fired eight years ago, and so they can't - it took them, it literally took them 12 hours to get back up and running. 



STEVE:  And it could be that embarrassing; right?



LEO:  Yeah, it must be.



STEVE:  Where like a company like AT&T cannot say, well, we had an expired certificate.



LEO:  And we fired Joe, and he was the only guy.



STEVE:  Yeah, it took us a while to figure out what - it's like, oh, no.  Heads would roll.



LEO:  They implied they were doing some sort of network upgrade.  But I think that was self-serving, too, like we're expanding our network.  And, well, things sometimes go wrong when you expand.



STEVE:  For our listeners' benefit, we've had a 12-hour outage.



LEO:  Yeah.



STEVE:  Wait, what?



LEO:  What?  Yeah.



STEVE:  No.



LEO:  We did it for you, kids.



STEVE:  So Kim Zetter's Zero Day blog had the best coverage I've seen of this surprisingly aggressive move.  I edited what Kim wrote down for length and readability.  But here's the gist of the news, and then two other outlets weight in.  Nevada's attorney general filed a motion to prevent Meta from providing end-to-end encryption to users under 18 who reside in the state of Nevada.  And it's like, what?  The request explains that its intention, of course, is to combat predators who target minors for sexual exploitation and other criminal purposes - now, there's always that extra clause in there, right, like, oh, wouldn't it be nice if we got some terrorists while we were at it - and, they say, to allow law enforcement to retrieve communication between criminals and minors from Meta's servers during investigations.



Now, what's interesting about this language is that suggests that there's some retention on Meta's part, and we'll get in a minute here to what Matt Blaze says about that.  But anyway, Kim said in his reporting:  "Last Tuesday, AG (attorney general) lawyers filed a partially redacted brief in Las Vegas federal court seeking a temporary restraining order" - so they're asking for a TRO - "and a preliminary injunction against Meta to prevent it from offering" - well, okay, but they have been.  But okay, we'll get to that, too - "offering end-to-end encryption on Messenger for anyone residing in the state whom Meta believes may be a minor.



"In its request to the court, the Nevada Attorney General's office claims that Meta's decision to enable end-to-end encryption by default is 'irresponsible' and 'drastically impedes law enforcement's efforts to protect children from heinous online crimes, including human trafficking, predation, and other forms of dangerous exploitation.'"



The AG requests an immediate hearing on the matter two days from then.  And this was, I think, at the beginning of the week last week, or maybe like Tuesday, and so - yeah.  I think this was Tuesday, and they wanted a hearing on Thursday.  And it's like, wait.  Okay, again, what?  And it would have been, yes, last Thursday, citing the "extreme urgency" - again, exact quotes - affecting "the safety and well-being" of children in Nevada who use Messenger.  The court scheduled the hearing for February 26th, so they didn't quite have it as quickly as they wanted to.  So that was yesterday.



In its response to the filing, Meta said that the - so Meta, of course, responded, saying wait a minute, saying that the request makes no sense since it and other messaging services have been offering end-to-end encryption to minors and other users, which is to say anyone who wants it, for years; and law enforcement, as acknowledged in the Nevada AG's own filing, can still obtain such messages from the devices used by criminals or minors.



Meta wrote:  "The State cannot properly assert that it requires emergency injunctive relief, on two days' notice, blocking Meta's use of end-to-end encryption, when that feature has been in use on Messenger for years and began to be rolled out for all messages more than two months ago."



So a legal expert and a research scholar at the Stanford Internet Observatory calls Nevada's request "bizarrely aggressive" - that was her quote - and says the timing of it is perplexing, writing:  "It seems to come out of nowhere, and what's the motivation for this to happen now?"  The expert cited it as being the biggest attack on encryption in the U.S. since 2016.  And of course we know what that date was, which was a reference to the FBI's attempt to force Apple to undermine the encryption on its iPhones so the agency could access a phone used by the suspect in the San Bernardino terrorism case.  As we recall, of course, the FBI wound up gaining access through another means and so dropped its push on Apple.



Meta has made end-to-end encryption available to Messenger users since 2016.  But last December the company promoted it to the default setting for all Messenger communication, it being the application used for private messaging between users on Facebook and Instagram.  As we know, law enforcement investigators can still read the messages, even if they were encrypted in flight, if they obtain the device used by either party to the communication and are able to access the device with the password or by bypassing it using forensic tools.  This has been true since 2016, when any user, including minors, opted to enable end-to-end encryption.  The only thing that has changed recently, and this was a couple months ago, is that Meta is now encrypting all messages by default.



But Nevada's Attorney General appears to be asking the court not just to prevent Meta from enabling end-to-end encryption for minors by default, but also to prevent the company from providing the option to use end-to-end encryption for all minors who reside in the state, even though they've been able to use end-to-end encryption for eight years.



In its response opposing the request for a restraining order and injunction, Meta points out that end-to-end encryption has been available by default for Apple iMessages since 2011, and is also available to users of Signal and other similar applications, you know, Telegram and so forth.  End-to-end encryption has been considered essential for protecting communications for years, Meta notes.  And they said:  "Indeed, Nevada law recognizes the value of encryption, requiring data collectors to encrypt personal information."



The Stanford Observatory expert noted that, if the court were to grant the restraining order and injunction, it would actually be making minors less secure than other users of Messenger, writing:  "It's bizarre for the state to be saying that the AG wants to ensure that only children in Nevada receive less privacy and security protection than any other user of Messenger."  And of course there's the danger that this could set a precedent with other states then following.



As the basis for its request to obtain a restraining order, the AG's office claims in its filing that in providing end-to-end encryption for minors, Meta is violating Nevada's Unfair and Deceptive Trade Practices Act, which seems like a stretch, which prohibits the violation of laws in the course of selling or leasing goods or services.  Nevada law prohibits the use of encryption to commit a criminal offense or conceal a criminal offense or obstruct law enforcement, the Attorney General states.  Therefore Meta is directly and indirectly aiding and abetting child predators - boy - by providing them with end-to-end encryption.  The Attorney General also states that Meta further violates the Unfair and Deceptive Trade Practices Act by misstating the risks minors face - what? - in using Messenger.



LEO:  It's ridiculous.  It's just ridiculous.  There's no way to logic it out.  It makes no sense.



STEVE:  Oh, wow.  The Attorney General states that Meta presents Messenger as a safe application for minors to use, but fails to inform them that in using Messenger with end-to-end encryption they are putting their safety at risk.  Wow.  The Attorney General's document actually states:  "Meta represented that Messenger was safe and not harmful to young users' wellbeing when such representations were untrue, false, and misleading."



Wow.  Well, I sure hope that the Attorney General will be required to back that up with some clear evidence rather than just waving their arms around.  The Attorney General also says that there would be "minimal or no cost to Meta in complying with such an injunction, and therefore the burden on the company is light."  Meta, of course, disagrees, saying in its response that its ability to identify users based in Nevada is limited and is based on IP addresses and the users' self-disclosure about their location, both of which are not always accurate.  And we talked about this before, that like IP addresses, Internet routing is not constrained by state borders.  Maybe by national borders, but not within the United States.  We don't route based on state.



So "To ensure compliance with the temporary restraining order,  as a result, Meta may have to attempt to disable end-to-end encryption on Messenger for all users."



LEO:  Oh, well, that will make us all safer.



STEVE:  That's right.



LEO:  According to that logic.



STEVE:  And that's like, why should only the kids be made safer, Leo?



LEO:  Right.  We all need safety.



STEVE:  Let's have - exactly.  That's good.  They said:  "Due to the truncated timeline here, Meta has not yet been able to assess the feasibility and burdens of doing so."  Oddly, the Attorney General asserts in its filing that the request for a restraining order is tied to a complaint that it sent Meta at the end of January.  But, Meta notes, that complaint is based on claims that Meta's services are addictive to users - so, right, save the children - to users and contribute to mental health issues in teenagers.  The complaint barely mentions end-to-end encryption and doesn't reference at all the Nevada Unfair Practices law, which the Attorney General cites as the reason for the court to grant the restraining order.  Wow.



Of course The Register picked up on this and had a field day with it, you can imagine.  I just grabbed one little piece of it.  They quoted Georgetown University's Professor of Computer Science and Law, Matt Blaze.  And Matt said:  "It's worth noting that it's not actually the encryption that they seem to object to, which would only hinder real-time interception.  It's the failure to make a surreptitious, permanent third-party record of otherwise ephemeral communications for the potential future convenience of a law enforcement investigation."  Yikes.



And The Register also quoted the Stanford Internet Observatory expert, saying:  "Prohibiting Nevadan children, and only Nevadan children, from having end-to-end encryption for their online communications would not help children's safety, it would undermine it.  Banning children in Nevada from having end-to-end encryption means giving some of the state's most valuable residents" - I'm sorry, most vulnerable, well, yes, and valuable residents - "less digital privacy and cybersecurity than everyone else."



And, she said, "The FTC and other state attorneys general, such as California's, have long been clear that it is a consumer protection violation for companies not to give users adequate digital privacy and security.  A strong encryption is the gold standard means of doing that.  It's therefore puzzlingly backwards," she wrote, "for the Nevada attorney general to argue that Meta is violating Nevada consumer protection law here."



Okay.  So, now, I went looking for the outcome of yesterday's hearing because that Thursday request got bumped to yesterday, the following Monday.  I found a mention in the Las Vegas Review Journal which noted that a follow-on hearing was now scheduled for some time next month.  So that would be March.  So we can hope that, whatever happens, this establishes a stronger precedent for encryption rather than one against it.  It is, as you said, Leo, it is just nonsense.  You know, and based on what Matt Blaze said, one has to wonder whether the ban on end-to-end encryption will then be followed by a mandatory requirement for the archiving of the communications of Nevada minors from some period of time.  And then what?  AI scanning them?  Help us.



LEO:  I guess the theory is, well, predators could be having encrypted conversations with children that we wouldn't be able to see.  But it's already illegal for the predators to be using the encryption.  So I don't know exactly...



STEVE:  That's exactly the point that occurred to me, too; right.



LEO:  Yeah.  What are they gaining here?



STEVE:  Nevada makes it clear that bad guys can't use encryption.  So, okay.  Let them do it and then convict them for that.  Nuts.



LEO:  I just - it makes no sense.



STEVE:  And who knows what is going on?  I mean, it would be interesting to know why Nevada?  Like, what?  But, you know, the good news is this will probably get smashed, hopefully, and set a precedent so other states won't even bother.



Okay.  So what's it like out in IT land?  Cybereason conducted a survey of more than 1,000 enterprise IT professionals, asking them about ransomware.  How's that going?  The survey found that all respondents, all more than 1,000 IT professionals suffered at least one security breach over the past two years.  84% of the respondents admitted ended up paying a ransom to attackers, 84%.  But only 47% - so just over half - said they got their data and services back and running uncorrupted.  So that's interesting.  And 82% of the respondents were hit again within a year.



Okay.  So it's difficult for me to imagine being responsible for the security - I've said this before - of a sprawling enterprise with complex networking requirements, people needing access everywhere all the time, with employees receiving a stream of email and needing to click on links in order to get their job done.  Although all of that is required for the business to function, it's also all a nightmare to secure.  I can't imagine how you even do that.  And the job of making all of that work securely, which these survey results suggest is mostly not possible, is also mostly thankless.



So I just wanted to take a moment, having seen these results, you know, to say to all of the IT professionals who, you know, are literally on the front lines of cyber defense, that I salute you, and I sincerely wish you the best of luck.  I'm sure the job is both or all fascinating, frustrating, infuriating, and certainly challenging.  So, you know, more power to you.  And god bless because...



LEO:  I wouldn't do it.



STEVE:  I hope you can sleep at night.



LEO:  We couldn't do it.



STEVE:  No.



LEO:  We couldn't do it.



STEVE:  No, no.



LEO:  It's the hardest work ever.



STEVE:  Yeah.  And, you know, make sure you're getting paid enough money because, you know, you're going to need it for your health coverage later in life.



LEO:  Yeah.  Whenever I talk to these guys, though, mostly what they complain about is not lack of money for them - I'm sure they'd like more - but lack of budget to do the job they need to do.



STEVE:  Lack of resources.



LEO:  Constant pressure to do it for less money, without the tools they need, et cetera, et cetera.  



STEVE:  And the problem is it doesn't look like a profit center; right?  It looks like a...



LEO:  That's exactly right.



STEVE:  It looks like a profit sink.



LEO:  Right.



STEVE:  And so it's, you know, it's like, well, but if we invest, you know, in a new Chromax 9 on the assembly line...



LEO:  Exactly.



STEVE:  You know, we'll be able to spit out twice as many widgets.  So, but, wow, you know.  One, you know, look at the reputation damage that we're seeing sprinkling across the industry as major company after major company gets themselves zapped.



LEO:  82%, wow.



STEVE:  I know, Leo.  Yeah.



LEO:  It's really stunning.



STEVE:  Okay.  So we talked about this little goodie three years ago, back in 2021.  And I did, and who wouldn't, love the name.  How could anyone not love something called "Super-Duper Secure Mode"?



LEO:  Yes, I remember we talked about it.



STEVE:  It's wonderful.  You know?  And the surprise was that it came from stodgy old Microsoft, you know, the IBM of the PC industry.  Back then Johnathan Norman, who was leading Edge's Vulnerability Research team at the time, explained that an important performance-versus-security tradeoff had been noticed because more than half of all prior Chrome/Chromium engine zero-days exploited in the wild turned out to be issues directly related to the V8 Just In Time (JIT) compiler.



What he and Microsoft were proposing for Edge was that with computers having grown so much more powerful than they were in yesteryear, back when Just In Time compilation was added for its performance benefit, that extra edge in performance today had become much less important that having an extra edge in security, and that the most obvious way to increase security was just to turn off Just In Time code compilation.  Super-Duper Secure Mode did just that.



The idea proved to be a total success, and it eventually went from being an experiment to being incorporated into Edge.  Sadly, however, in the process Microsoft's stodginess did win out, as it was bound too; right?  There's no way Super Duper Secure Mode would actually end up in the Edge UI.  No.  It became Enhanced Security Mode, you know, much less fun.  



But anyway, last week we saw the release of Chrome 122.  The Chrome browser in the process inherited the result of Microsoft's pioneering.  If you put the address into your Chrome URL, chrome://settings/content/v8, you'll be taken to a page titled "V8 Optimizer."  And there you will find two radio buttons.  The first one, which is on by default, sites can use the V8 Optimizer.  The other one, which I would argue is worth exploring, click it and you get "Don't allow sites to use the V8 Optimizer."  Now, as for getting there, I did try searching from the top level of settings for "V8 Optimizer," but that didn't get me there.  So again, chrome://settings/content/v8, you know, numeral 8.  And this page - so as I said, this page allows you to flip the default from yes, everybody gets to use V8, to no, don't want V8 because it's dangerous.



So my advice to Chrome users would be to give it a try and see whether you notice any difference.  I'm guessing that for most sites, maybe all, the probably minor difference in performance would end up being masked by the site's own performance and the network overhead of stuff getting between you and them.  And if that's not the case, that is, if a site should actually like be noticeably slower, that page also allows for per-site overrides.  So you could just disable the use - so you could globally disable the default use of the V8 Just In Time compiler.  But then if you end up with a site that does benefit from having it, just whitelist it for that one site.



And I should also note that also with Chrome 122 they added some experimental AI features.  And I'm not going to roll my eyes.  We have a long way to go.  This is the very beginning of the AI What Is It journey.  So if you click the three dots in the upper right of Chrome, you know, the Chrome of Chrome, and choose Settings at the bottom of the dropdown menu, over on the left, in that list on the left, about a third of the way down, you'll find "Experimental AI."  If you flip the switch, which is off by default, to "On," the box there expands to show you three items:  "Help Me Write," a tab organizer, and "Create themes with AI."



I've not gone any further since I'm not using Chrome any longer as my default browser.  I'm happily back using Firefox.  But, for example, under "Help Me Write," it says:  "Helps you write short-form content for things on the web, like reviews."  Great.  Oh, boy.  "Suggested content is based on your prompts and the content of the web page.  To use this feature, right-click on a text box."  So anyway, they gave us an example, you know, where you, like, I don't know, I want to ask for a refund on my airline tickets, you know, and it wrote it for you.  So okay.  Anyway, it's built into Chrome now.  Have fun.  And we'll see where AI takes us.



On the topic of how much is apparently continuously going on behind our backs, without our knowledge or awareness, I noted in passing that the home delivery service DoorDash has agreed to pay, not a crushing fine, $375,000, still attention-getting, in civil penalty for violating California's privacy laws.  California's Attorney General sued DoorDash for selling customer data without notifying its users or providing a way to opt out. The company sold customer data such as names, addresses, and transaction histories to, like, what was brought to your door that you were dashing to get, to a marketing cooperative.



Now, more and more we're all using these services.  COVID drove a significant upswing in the use of home delivery services of all sorts.  And many people use Uber, Lyft, or something similar.  And all of these services are being managed through online apps that need to know a lot about us in order for them to function.  You know, we give them the information that we understand that app needs based on the service it's providing.  But when we're not being told that that information, which could be significant about us, is going to be used to create further profit for this company, that seems wrong.



You know, along comes a marketing firm and offers these companies real money in return for sharing everything they know about us, in many cases never giving us any permission or any opportunity to say whether that's all right with us or not.  So, you know, and then where does it go from some marketing cooperative?  It's being resold to, you know, information brokers and who knows what else.  So it's a hidden privacy cost of participating in today's connected economy.



And speaking of which, the United States Federal Trade Commission, the FTC, has just fined the cybersecurity firm Avast a somewhat larger sum, gulp, $16.5 million for selling its users', okay, its users' web browsing data.



LEO:  Oh, I remember this.  I'm glad.  Good.



STEVE:  Yes.  Yes, the finally got a number.  And you're right.  If that sounds familiar to people, it's because we talked about this when it first became news.  Essentially, Avast was functioning as a spy in our browser.  The FTC accused the security firm of using bait-and-switch tactics by offering browser extensions that blocked Internet tracking, but then selling browsing data behind its users' backs.



LEO:  Brilliant.



STEVE:  Yeah, we're going to block that tracking for you because you don't want to be tracked.



LEO:  Let us do it.



STEVE:  Yeah.  We'll be one-stop shopping for tracking.  So between 2014 and 2020, Avast - get this, Leo - sold browsing data to more than 100 third parties, everywhere their users went, through its Jumpshot subsidiary.  The FTC has banned Avast from engaging in similar practices - I wish they would ban them from doing business on the planet.



LEO:  No kidding.



STEVE:  And has ordered the company to notify - oh, that'll be fun - notify all users whose data was sold.  That'll be...



LEO:  [Crosstalk] sorry note.



STEVE:  That'll be an interesting written - okay, attorneys.



LEO:  I'm sorry.



STEVE:  Earn your keep, you attorneys.  Make this - or I guess the attorneys meet with the PR people.



LEO:  Yeah.  Then they fight.



STEVE:  And probably, yeah, and probably the - I forgot the department name that - Human Resources.  Because we would all like to keep our jobs also.



LEO:  Yeah. Yeah.



STEVE:  So you guys figure out what we have to write in order to send this.  Okay.  One more, and then we'll take our next break.  We know how beneficial logging can be for monitoring a network environment's security.  And to that end, Microsoft has taken some heat and come under the gun for charging their enterprise cloud customers extra money if they wanted logging services that would better protect them from security threats which were Microsoft's fault.  Ouch.  So in a move that CISA has greeted happily, after noting that Microsoft should do it, Microsoft has finally made previously extra pay security logs free to use for  its enterprise customers.  Thirty-one logging categories have just been moved from the premium tier of the Microsoft Purview Audit service into the standard offering.



LEO:  Oh, good.  Wow.



STEVE:  Yes.



LEO:  Great.



STEVE:  Yes.  This was something Microsoft had promised last year in the aftermath of its Storm-0558 hack.  So it's a welcome move in the right direction.  On the other hand, given the precipitating events and the pressure it was under, I wouldn't go so far as to suggest that this represents any actual change in philosophy within Microsoft; but this was definitely the right thing to do, regardless.



LEO:  We'll just charge for something else, that's all.



STEVE:  That's right.  We'll just, you know, we'll make up for the lost profit by increasing the price of, what, maybe security patches.  How would that be?



LEO:  All right.  Let's take a little break.  And then you and I shall return with more.  And I'm looking forward to hearing this news that you mentioned, that you referred to about GRC.  But all that's still to come.



STEVE:  I've got another laugh-out-loud app title for Leo.



LEO:  Oh, good.  I love those.



STEVE:  Okay.  So in a little bit of, I don't know that this is exactly schadenfreude, but while the politicians in the EU consider reducing browser security by forcing EU member country root certificates into our browsers, and consider the imposition of limits on the use of end-to-end encryption for their citizens, the European Parliament's IT service has found traces of spyware on the smartphones of its security and defense subcommittee members.



LEO:  Oh-ho. 



STEVE:  Who needs that encryption?  The infections were discovered after members went in for a routine checkup.  The EU Parliament has sent a letter urging its members to have their devices scanned by its IT department.



LEO:  Wow.



STEVE:  So, yeah, maybe it's good to be running with security set to max on your smartphones.



Law enforcement agencies, as there's been a lot of coverage of this and then some brief mention here, I titled this "LockBit gets bitten."  Law enforcement agencies from 11 countries disrupted the LockBit RaaS, the Ransomware as a Service operation in which was the most thorough and coordinated takedown of a cybercrime portal service to date.  During the operation, which was codenamed Operation Cronos (C-R-O-N-O-S), officials seized LockBit server infrastructure, froze cryptocurrency wallets which were still holding past ransoms, released decryption tools, arrested members and affiliates, filed additional charges, and imposed international sanctions.



Operation Cronos began several months ago and was led by the UK's National Crime Agency, their NCA.  The agency infiltrated the gang's servers, mapped out their infrastructure, collected their truly secret master encryption keys, and accessed the LockBit backend, where admins and affiliates collected stats about attacks and negotiated with their victims.



The takedown occurred last Monday the 19th and was announced the following day, one week ago on February 20th, by the UK's NCA, Europol, and the U.S. Department of Justice in a coordinated disclosure.  In total, officials say they seized 34 LockBit servers; identified and closed more than 14,000 online and web hosting accounts used in past LockBit attacks; seized more than 200 cryptocurrency accounts holding past ransoms; detained two affiliates in Poland and Ukraine; and indicted two other Russian nationals.



Lockbit affiliates who logged into their LockBit backend accounts on Monday were greeted by a special message from the NCA blaming the takedown on "LockBitSupp" - who's the big cheese of Lockbit, the Kingpin - "LockBitSupp (S-U-P-P) and their flawed infrastructure."  The message urged affiliates to rat on their former boss, which tends to confirm the belief that law enforcement has yet to identify LockBit's creator.  And actually there's even some news since then.  And you might imagine that he's gone into hiding, whoever he is.



And as was done in other recent cases of the Hive and the AlphV disruptions, the cybercrime officials didn't just take down servers.  They also collected the coveted Ransomware-as-a-Service backend  the encryption keys that were used to lock victim files.  Officials say the keys were handed over to a technical unit inside the Japanese national police, who created a decryption, a master decryption utility that is able to recover all files from Windows systems that had previously been locked with LockBit.  The utility is available now through Europol's No More Ransom project.



The long-term impact of this takedown is still unknown.  As we've seen before, ransomware operations that met a similar fate might relaunch under a new name.  On the other hand, for example, the Hive gang never did return after the FBI hacked its servers and released decryption tools a year ago January; whereas the operators of the AlphV Ransomware-as-a-Service did pop back online and start launching attacks from a new infrastructure a month after the FBI took down their servers.  And in even more recent news, just as we were getting ready to start the podcast, I saw that LockBit has reemerged already under new infrastructure and has posted the news about its first 12 new victims.



LEO:  Didn't take long.  Wow.



STEVE:  Did not take long, no.



LEO:  Amazing.



STEVE:  Firefox v123.  That happened last Tuesday.  And they wrote three things that might be of interest to our Firefox users.  They said:  "We've integrated search into Firefox View.  You can now search through all the tabs on each of the section subpages - Recent Browsing, Open Tabs, Recently Closed Tabs, Tabs from other devices, or History."  And actually that's kind of cool, to be able to search like recently closed tabs.  Sometimes when I am busy and closing things, I go, ooh, what was that thing that I had before?  And so being able to search through that content would be very cool.



Also, well, okay, two other things, and they had a lot to say.  They wrote:  "Having any issues with a website on Firefox, yet the site seems to be working as expected on another browser?  You can now let us know via the Web Compatibility Reporting Tool.  By filing a web compatibility issue, you're helping us detect, target, and fix the most impacted sites to make your browsing experience on Firefox smoother."  And finally they said:  "Address bar settings can now be found in the Firefox Settings Search section."



Okay.  So the web compatibility issue was something I recently encountered.  But, and it bugs me, I don't now recall where because I would like to go back.  And I've seen it more than once.  The page attempted to load, and it looked like it was going to, but then it just remained blank.  The first thing I tried was to disable uBlock Origin for the site and then reload it, but that didn't help. The same thing happened.  So I turned uBlock Origin back on.  And then I tried the site under Chrome, where the site did work correctly.  And so I just did whatever it was I was doing and then came back to Firefox. 



In researching this further for the story I found that Firefox's "Enhanced Tracking Protection," which I do have enabled for all sites, is the most likely cause of this kind of trouble.  But I didn't think to try that, and I should have.  So next time this happens with Firefox I will.  You click on the little shield icon to the left of the URL bar and, assuming that "Enhanced Tracking Protection" is on, you turn it off.  This will cause an automatic page reload which may fix the problem.  Now the shield will have a slash through it, since "Enhanced Tracking Protection" has been disabled for the site.



If you click on it then, you'll see the question "Site fixed? Send report."  And if you click that, you'll be able to add some optional comments and send a report to Mozilla, with a single click, about the site containing the information that they will need so they can see what's going on and work on fixing Firefox's "Enhanced Tracking Protection" compatibility so that it works better.  So the next time that happens, that's what I'm going to remember to do.



But that's not what just changed here in Release 123.  There's now an explicit "Report Broken Site" option always present now under that Shield icon.  For that to show, you need to have "Allow Firefox to send technical and interaction data to Mozilla" enabled on your main "Privacy & Security" page, but that's now the default for new installs.  I just tried it to verify that, and it is on.  And I would imagine all of the listeners of this podcast have that turned on.



And doing this, figuring this out, brought me back to the Privacy & Security page in Firefox, and I think it's definitely worth going just to that page and scrolling through it just from time to time; but, you know, but do it soon because it's got many friendly settings, and you might well find something that is off that you thought was on, or that you'd like now to be doing differently.  But anyway, for all of our Firefox listeners, and I know we have many, if a site misbehaves, click on the little shield, and you'll be able to easily and quickly send news of that misbehavior to Mozilla so that they will be able to keep Firefox working well.  And of course, as we know, unfortunately its continued existence in the world may be a little endangered, so it's worth doing that, I think, to keep it going.



The last thing I've been wanting and intending to mention for a while is that I had become annoyed by Firefox's apparently pointless division of the URL bar into two separate fields, with the URL on the left and a separate search box on the right. There are some instances where what I'm searching for looks like a domain name, and that might be confusing to Firefox where it's trying to figure out should I search for it or go there?  So placing that into the right-hand search field would make that clear.  But just enclosing the term in quotation marks solves that easily enough.  The single unified field is now the default for new installations of Firefox.  But I've been using Firefox for so long, from before that was changed, so my top-of-screen still had two separate fields.



LEO:  Wow.  I turned that off, like, 30 years ago, I feel like.



STEVE:  Yeah.  Yes.  And I did it, like, two months ago.



LEO:  Wow.



STEVE:  So anyway, I just wanted to say, if like me you still have separate fields, if you go to - just open Settings and search for Address, you know, A-D-D-R-E-S-S.  The option will immediately be at the top of the page.  Just flip it to a unified field and, yes.



LEO:  You're okay with it now.  You get used to it.



STEVE:  Oh, no, I mean, I was using Chrome for a while where it's a unified field.  Or Edge, where it's a unified field - nobody else still does that except me on Windows 7.  So I just wanted to let everybody know, Firefox lets you easily turn that off.



LEO:  The unibrow, I think, is what we call it.  No, no.  That's not right.  



STEVE:  The which?



LEO:  The unibrow.  But I don't think that's right.



STEVE:  Yes.  It's the uni something.



LEO:  They do have a name for it, yeah, yeah.



STEVE:  Yeah.  Okay.  So this one is the reason I titled the podcast "Web Portal?  Yes, Please."  Last Monday the 19th, the industry was informed of yet another horrific web authentication bypass in a widely used and popular product known as ConnectWise ScreenConnect.  Unfortunately, this allowed bad guys to trivially connect to an enterprise's screens and network by completely sidestepping their need to identify themselves as an authorized party.



LEO:  Holy cow.



STEVE:  I know, Leo.  It's just astonishing.  And connect they did, in large numbers and almost immediately, wasting no time.  I'm not going to go too far into this because, you know, it's like, really?  Again?  But Huntress Labs wrote about what they found, and it's worth giving this a little more color.  The title of their posting two days later, last Wednesday, was "A Catastrophe for Control:  Understanding the ScreenConnect Authentication Bypass."



They wrote:  "On February 19th, 2024, ConnectWise published a security advisory for ScreenConnect v23.9.8, referencing two vulnerabilities and software weaknesses.  The same day, Huntress researchers worked to understand this threat and successfully recreated a proof-of-concept exploit demonstrating its impact.  This write-up will discuss our analysis efforts and the technical details behind this attack, which we're coining as 'SlashAndGrab.'  The ConnectWise advisory indicated that in all versions of ScreenConnect below 23.9.8, there were two vulnerabilities.  In other words, it's always been there, folks:  an authentication bypass using an alternate path or channel, and improper limitation of a pathname to a restricted directory."  In other words, a path traversal mistake.



They wrote, Huntress wrote:  "The first vulnerability was disclosed with a critical base CVSS score of 10 - that is, right, 10 out of 10, the highest possible severity."  Which is basically where a system just says, please, come on in, whoever you are.  Username, password, ah, don't bother.  Just click, you know, Submit.  The authentication bypass would ultimately open the door for the second vulnerability.



They wrote:  "ConnectWise made a patch available and expressed that all on-premise versions of ScreenConnect 23.9.7 and below must be updated immediately.  At the time of release, the ConnectWise advisory was very sparse on technical details."  Uh-huh.  "There was not much information available as to what these vulnerabilities really consisted of, how they might be taken advantage of, or any other threat intelligence or indicators of compromise to hunt for."  You know, basically ConnectWise just was saying, holy crap, please, please, please, everybody update to 23.9.8.  Don't ask any questions.  Just do it now.



Huntress said:  "Once we recreated the exploit and attack chain, we came to the same conclusion.  There should not be public details about the vulnerability until there had been adequate time for the industry to patch.  It would be too dangerous for this information to be readily available to threat actors.  But," they wrote, "with other vendors now" - two days later - "publicly sharing the proof-of-concept exploit, the cat is out of the bag.  We now feel that sharing our analysis shares no more threat than what is readily available.  So we're ready to spill the beans."  And they finished with their intro saying:  "The 'exploit'" - and they had that in quotes because it's not - "is trivial and embarrassingly easy."



Anyway, further details are unimportant to further establishing the point.  Everyone gets the gist.  We have yet another example of the truth that we do not yet fully understand as an industry how to do web authentication interfaces securely.  Oh, yes, we want to, since they're so friendly, colorful, attractive, and appealing.  Look at that, you just go there with any web browser, and you're logged into the enterprise's network.  It's magic!



LEO:  Oh.  Yeah.



STEVE:  And the bad guys love it just as much.  They love how easy we've made it to log into enterprise networks.  Web portal? Yes, please!



LEO:  It wasn't as easy as just leaving the fields empty and pressing Submit; right?



STEVE:  No.  I didn't even go any further.



LEO:  Okay.



STEVE:  Because, you know, because it was immediately picked up by bad guys.



LEO:  Yeah, of course.



STEVE:  And every enterprise that hadn't updated by the time it was reverse engineered, which took apparently minutes to reverse engineer, they were then being compromised.



LEO:  I wonder how many people use ConnectWise.



STEVE:  It's a big deal, apparently.



LEO:  Is it?  It's for managed service providers.



STEVE:  It's, yes, because they're so easy and so powerful.



LEO:  Yeah, wow.  Oy.  Well, I hope our MSP - I don't think Russell uses it.  I think we'd know if he did.



STEVE:  Yeah.  Well, I do have some good news.  I'm very pleased to finally be able to announce that SpinRite 6.1's code is no longer a release candidate; it has graduated to its official release.



LEO:  Yay.



STEVE:  So I announced that on Sunday.



LEO:  What does that mean?



STEVE:  It means that, well, it means that I thought I was done.  It turns out that something I did with probably conditional assembly, when I switched it around out of release candidate stage, caused the SpinRite executable, which is written to the diskette image, to have the attribute of the volume label.  I have no idea, I mean, and I learned about it yesterday morning when I was already started in on the podcast production.  So I haven't looked at it.  But it makes USB, it makes bootable USBs just fine.  And that's what almost everybody uses now.  So it's not a big problem.



But the diskette image is used both for creating a bootable diskette and the ISO and the IMG images.  So they don't boot right now.  This evening I'll fix it.  And it'll be SpinRite 6.1, Release 2, rather than Release 1, as it is right now.  But so you might want to wait till tomorrow, if anybody's been waiting.  But it's done, is what this means.  And all the bugs, all the features, all the bell-and-whistles, blah blah blah, it's done.



LEO:  It's more a typo than a bug is really what you're saying.



STEVE:  Yes.  It is, exactly, it is something so dumb, I mean, intellectually I am so curious.  It's like, what the heck?



LEO:  Where did I put that?



STEVE:  How did that happen?  Yeah.



LEO:  Yeah.



STEVE:  So, okay.  But I learned something very cool that I wanted to share with our listeners as a consequence of this.  Sunday evening I had submitted SpinRite's final code to Microsoft's threat detection system, as everyone's been hearing me talk about.  Prerelease users have been driven to grief by their Windows systems immediately deleting their copy of SpinRite, where they were unable to run it because it would immediately be quarantined and deleted, thinking that it's some random trojan which obviously it's not.  But it's a false positive.  And if you google that particular trojan, turns out like it misfires for a lot of people.  A lot of people are doing something that, like, freaks out Windows.  Anyway, so that's why I spent a month doing code signing, right, hoping that if I signed my code, then I would get the benefit of the doubt.  But it didn't seem to be happening.



Okay.  So Sunday evening I submitted this final code to Microsoft Threat Detective System because, yes, it was generating false positive detections and creating a problem.  Yesterday morning I checked on that, and here's the reply that I received from Microsoft.  They said:  "The warning you experienced indicates that neither the application nor the signing certificate had established reputation with Microsoft Defender SmartScreen services at the time.  We can confirm that the application 'sr61.exe' has since established reputation, and attempting to download or run the application should no longer show any warnings."



Then they said, and this was what warmed my heart:  "Please note, the signing certificate thumbprint" - and then they gave the hex, which I checked is the signing certificate on the server - "thumbprint is still in the process of establishing a reputation.  Once completed, all applications that are signed with that certificate should have a warn-free experience from the start."



LEO:  Oh, nice.  Interesting.  So you somehow have to establish reputation, even for a signing certificate.



STEVE:  Yes.  And that last bit is the best news I've received in a very long time.  You know, as I've mentioned before, I've been despairing over this because there've been times in the past few months - there was some guy a couple days ago who wanted a refund from a recent purchase because he was unable to run SpinRite.  He was all excited, but he couldn't run it  because his Windows 11 kept deleting it out from under him.  So, you know, every time they tried, SpinRite would just immediately quarantine it, I mean, Windows would immediately quarantine SpinRite and remove it from their system.



So hoping that a signature might mean something, and by the way, this was - all of these have been signed; right?  The signing system's working perfectly now, and beautifully, never having a hiccup.  But it wasn't helping.  So that's why I had spent that month figuring out how to get Microsoft's less well-documented code signing APIs to work remotely on GRC's server with a hardware security module because I have it, you know, the EV code signing cert is in an HSM, and you have to use an HSM for EV code signing.  So I did all that, and all I had was hope.



It wasn't until yesterday morning when I received Microsoft's note that it finally became clear that it would actually be possible for GRC's EV certificate to eventually protect these individual downloads, and SpinRite's users, from unwarranted harassment.  And, I mean, and the reason is that every copy a SpinRite user downloads has their licensing information embedded in it.  So it's brand new.  It's never been seen by Windows, which is why it's always freaking Windows out.  It's like, what?  What's this?  And because the signing certificate doesn't yet have reputation, Windows quarantines it.  



So what's interesting is that the reputation of that single SpinRite executable which I sent to Microsoft for analysis only took a few hours to obtain, that is, reputation.  But GRC's code signing certificate still hasn't.  Since I wanted to obtain the longest run time possible for this new signing technology and the certificate that it would be using, right before I deployed it in the middle of January I asked DigiCert for an update.  EV certs are good for three years, max.  So on January 16th that new certificate was created, and I immediately placed it in the certificate.  That is exactly six weeks ago today.  And over the course of those six weeks, thousands of copies of SpinRite's code have all been signed by that new certificate and downloaded and run when users are able to.  And Microsoft's note exactly identified that certificate by its thumbprint.



So, you know, we know that Microsoft has been watching this certificate for six weeks, and it still says "The signing certificate is still in the process of establishing reputation."  What this suggests is that it takes quite a bit longer for a code signing certificate to establish a reputation.  Even an Extended Validation code signing certificate, it was more difficult to obtain, and it can only be used from a hardware security module.  And really, when you think about it, that makes sense, since a fully trusted code signing certificate would be a very potent source of abuse if it were ever used to actually sign malicious code, since Microsoft just confirmed what I had been hoping, which is that code that gets signed by it gets a green light by default.



So, you know, at the same time I'm quite certain that a reputation that was long and hard earned would be instantly stripped, obviously, if Microsoft were to ever confirm that a piece of true malware was bearing that certificate's signature.  So anyway, this is all good news on the SpinRite front.  I'm done with the product.  I'm working on the documentation.  I'll fix the little bug in the executable being flagged with a volume label attribute so it won't run, I'll fix that, and Release 2 will be available later this evening, I'm sure.  And it looks like, you know, this certificate is on its way to establishing reputation.  I have no idea who to ask or how long it's going to take.  I think what I'm going to do, though, is the GRC Benchmark is now being downloaded about 1,600 times per day.  And interestingly, ValiDrive has turned out to have some legs.  It's now - it's been steadily increasing...



LEO:  Oh, I'm not surprised, yeah.



STEVE:  ...in popularity.  It's now at more than 1,200 downloads a day.  So I think I'm going to cosign both of those with this same cert so that it gets way more downloads, and Microsoft sees it a lot more.  And then, I mean, I don't know if it's time or it's number of downloads or I just don't know what their metric is for what it takes to establish reputation.



LEO:  If anybody from Microsoft's listening, can you do a solid for our man here and run down the hall and say, can you push that through?  There must be somebody who can help you listening to the show.



STEVE:  Leo, if they're listening to the show, they're upset with me more than they're wanting to help me.



LEO:  Oh, no, they know.  They work for Microsoft.  They're used to it.



STEVE:  I guess that's true.  I guess that's true.  So I also have a new piece of GRC freeware to announce.  It's a Windows app called "BootAble" because it creates any sort of boot media - USB, CD, ISO, IMG or diskette - for the purpose of allowing its user to freely confirm, and/or figure out, how to get any given PC-compatible machine to boot DOS.  And, Leo, you know how I am with my naming programs.  I still vividly remember you laughing out loud when I first told you about "Never10."



LEO:  Yes.



STEVE:  You thought, what a name.



LEO:  Great name.



STEVE:  Anyway, so I was sorely tempted to name this "DOS Boot."



LEO:  I wish you had.  Oh, I wish you had.



STEVE:  I know.  I know.  It would be so good.  The reason I didn't is that SpinRite 7 will boot on either BIOS or UEFI machines, and it will no longer be bringing DOS along for the ride.  So "BootAble," which is more generic, would be the better choice for the long run.  So anyway.



LEO:  So, does this - I wonder, maybe this will be useful for people who want to install Linux, too, because Linux has trouble with secure boot and in some cases UEFI.



STEVE:  Yes.  Yes.  I mean, so the idea would be you would have to, well, actually Linux will install on UEFI, and this won't test that yet.



LEO:  Yeah, most Linuxes will, yeah.



STEVE:  Yeah.  This won't test that yet.  So you need BIOS or a CSM, you know, the compatibility, the software module on UEFI.  But this allows people - so I wanted something so that people weren't buying SpinRite and then getting upset that they wanted to run it on a laptop that's UEFI-only.  Or that, you know, or they were concerned about whether or not they would be able to boot SpinRite on any given machine.  This is freeware, and it has all the same boot technology that SpinRite 6.1 has, and it's free.  So you're able to just easily create a USB thumb drive and play around with, like, you know, do you hit F12 or F2 or delete, you know, because you have to intercept the normal boot, right, in order to get it to, like, boot [crosstalk].



LEO:  And I can never remember those keys.



STEVE:  Yeah.  And there's no standard.  Every machine is different.  You know, I think they randomize it at the factory.  So anyway, just another little simple piece of freeware for everybody.



LEO:  Very nice, thank you.



STEVE:  And Leo, let's take our last break, and then we're going to do a bunch of feedback from our listeners.



LEO:  Great, let's do it.



STEVE:  Astralcomputing tweeted:  "Cox is transitioning its email service to Yahoo Mail for its users.  Customers will be moved to Yahoo email while still retaining their email address and password.  However, the POP/IMAP/SMTP settings for Outlook will change.  My main concern," he writes, "is the security hassles this is going to create for users due to the Password Reset issues you've been talking about lately.  Thinking of moving my 86-year-old mom off Cox before this happens, but it's going to be a nightmare to change all those email addresses for every utility, bank, et cetera.  Keep up the good work past 999.  SN listener from day one and proud SpinRite Enterprise supporter.  Signed, W."



Which brought me to note, just for anyone who's interested, a SpinRite Enterprise supporter is rare, but I always like to see it.  It's nice.  For those who don't know, we offer three levels of license.  The standard SpinRite end-user license allows it to be run on any machines that the user personally owns.  And as I've often noted, I would never complain about someone coming to the rescue of a friend or a family member in need.  If a company wishes to use SpinRite on any or all of their machines at a single location, we ask them to maintain four licenses for the version of SpinRite they're using.  And if a large multilocation enterprise wishes - and we call that a "site license" if you have four SpinRite licenses.  If a large multilocation enterprise wishes to use SpinRite across their entire enterprise, you know, wherever, then maintaining 10 licenses officially allows for that.  So, again, Astralcomputing, thank you.



I did a bit of poking around, and I've confirmed that 86-year old moms everywhere will not be disturbed by this change.



LEO:  Oh, good.



STEVE:  Oh, yes.  Although Yahoo!'s network and servers will be the ones handling everything for Cox in the future, none of Cox's email addresses, which all end in "Cox.net," will be changing.  In their announcement about this, Cox wrote:  "To ensure the best email experience possible for our customers, we have decided to transition the email service and support of your Cox.net email to Yahoo Mail.  This transition lets you keep your email address, messages, folders, calendar, and contacts.



After the move, Yahoo Mail will become your email provider, and Cox will no longer manage or support your email services.  We realize how important your Cox.net email address is to you and have carefully selected Yahoo Mail because we believe they are a trusted provider that will continue to offer the advanced support and enhanced protection for your email account that you've had at Cox.  We'll work with Yahoo to provide a seamless transition for our Cox.net email customers."



So anyway, no need to change anything related to the email addresses themselves.  Your email client login domain will apparently need to move to Yahoo!, but that change should be minimal; right?  You just change a couple settings for POP or IMAP or SMTP, and you're good to go.  But Mom will not need to change any of her email addresses.



Eric Mann asked, he said:  "Hey, Steve.  I was just at my local grocery store and had a thought.  In this day and age, why do credit cards have the number, expiration date, and CVV code printed/embossed on them?  Everything a thief needs is right on the card.  Simply not necessary for in-person transactions.  All the info can be stored somewhere else, say BitLocker.  Still loving the show.  Eric."



So that's an interesting question, actually.  You know, especially, Leo, the embossed part.  You know, it's obviously all a holdover from the manual credit card processing days.



LEO:  Remember that?  You had to go shump-shump.



STEVE:  Yup, where a card would be placed in a manual credit card machine, a multipart carbon slip would be placed on top, and then the roller would be rolled back and forth across the slip and over the card underneath, you know, to basically transfer the card's data, the credit card number and the expiration date, onto the carbon.



Now, I can't recall the last time I saw that being done, but it does remain a possible fallback in the event, for example, of a power outage, where credit cards still need to be processed, or if there was some Internet connectivity outage where you were not able to do it, like your credit card processing terminal wouldn't work even though you had power.  And, you know, as with an increasing number of things, like phone books and even going to a library or, sadly, a physical book store, I imagine there are young people who have never encountered a manual processing of a credit card.  But anyway, just sort of interesting that it's still, you know, they are still embossed, just like in the old days.



LEO:  Yeah.  Although my latest American Express card does not have that on.



STEVE:  Oh, really.  So they finally have given that up.



LEO:  Yeah.  I think that a lot of cards are giving that up.



STEVE:  I mean, it does make sense.



LEO:  Yeah.



STEVE:  You know, because you could just manually transcribe the number onto the same carbon, if that was actually necessary.



LEO:  Yeah.  If you needed one, yeah.



STEVE:  Yeah.



LEO:  I don't know who's - you're right.  It's if the power goes out or some station in Nevada or somewhere, some gas station somewhere doesn't have - yeah.



STEVE:  Right.  So Matsumura Fishworks, I guess that's the guy's company name...



LEO:  I hope so.



STEVE:  He said:  "Hello, Steve."



LEO:  Be a terrible personal name.  But go ahead, okay.



STEVE:  He said:  "Hello, Steve.  I've been a Security Now! listener for many years and can't thank you enough for all the security and computer science education you've given out so freely.  Also my kids are on a daily vitamin D regimen because of you."  That's great.  He said:  "I had a question about one of the items from SN-962" - that's last week - "the gold standard of client-side hashing for password creation."



He said:  "In a scenario where the client submits their own hashed password, and the adherence to password requirements is governed only by client-side controls, would there be any way to prevent a malicious party, like a pen tester for example, from swapping out the hash in-transit and supplying the server with a valid hash of a non-conforming password?  This would be admittedly counterproductive for the user, but it would seem that the server would lose the ability to make strong assertions about the hashes that it was accepting.  Am I thinking about this correctly?  I'd love to hear any thoughts you have on this, and thanks again for all you do."



Okay.  So the essence of this listener's question is whether the receiving server is able to determine anything about the quality of the user's password from its hash, and the answer of course is no.  Assuming that the user's browser employs a strong local PBKDF, you know, a password-based key derivation function, the result will be a completely opaque fixed-length blob of bits from which absolutely nothing about the original source password can be reverse engineered.  Hopefully, that PBKDF will also be salted so that it's not even possible to compare the results of that PBKDF function with previously computed passwords.  So it's due to the total opaqueness of the result that we now depend upon the user's browser to enforce password complexity requirements right upfront before the PBKDF function is applied because that's the only time it can ever be done.



And Efraim, he said:  "Hi, Steve.  Thank you for the great show.  I'm a long-time listener and excited for the opportunity to continue listening for many more years.  In regards to passwordless login by way of a link sent to a user's email and the concern over email security," he said, "episodes 961 and 962, I was wondering if there would be a way to construct the magic link from a cookie or the like from the user's browser session?  That way, the link would only work from the same browser session where the login request originated.  Looking forward to hearing your take."



Okay.  At one point the same thought had occurred to me, but I was in the middle of assembling the podcast, so I didn't pursue it.  But the answer is absolutely and unequivocally yes.  Now that I've thought about it, here's a far stronger solution.  In fact, it's absolutely strong.  Even without being logged in, the user's browser will have obtained at the very least a session cookie from the site they wish to log into.  That cookie will be valid until the browser is completely closed.



And in fact the cookie's probably persistent and long-living, but it wouldn't have to be.  And a bunch of information can be encoded and encrypted into the link beyond a one-time token, the link that's emailed to the address the user provides.  So the emailed link could include the time of day, the user's IP address, and the value of the unique cookie that their browser has just received from the site.  When the user then clicks on the link, it will open a new page at the domain they're wishing to authenticate to.  In opening that page and sending the URL to the site's server, the server will be obtaining all of that information, which is totally opaque because it's been encrypted before it was added to the link and sent to the user.



So it first decrypts the information and verifies that a reasonable amount of time has passed since the link was created using the link's embedded timestamp.  It verifies that the IP address encoded into the link matches the IP address of the browser's query.  So the user hasn't moved.  And that the first-party cookie the browser just returned with its query also matches the cookie value that was encrypted into the link.  So it's the same browser.  I don't see any way for that system to be compromised.  You need no email security.  You know, you could hand the link around to people, and it wouldn't matter.  The IP address provides strong verification about the location and connection.  The browser cookie verifies that it's the same browser at that same IP.  That link would be totally useless to anyone else who might be able to intercept it as a result of email's less than totally perfect security.



So thank you for posing the question, Efraim.  I am very glad that we were able to revisit this once again.  That makes it three weeks in a row since it's an intriguing idea.  We've just made the email-only login system utterly bulletproof.



LEO:  I like it.



STEVE:  Yup.  It's a win.



LEO:  So there is a rule with cookies that only the site that created the cookie can read the cookie.  Right?  So that's what...



STEVE:  Correct.  I mean, that is rigorous.



LEO:  Yeah.  That's what's protecting you.  So you get the email, or rather you get the link.  Click in the email, click the link, it would open your browser.  Now you're in that session, and you are theoretically with that first party; right?  So the cookie then could work.



STEVE:  Right.  You're back at that site domain where...



LEO:  Right, it's first-party.



STEVE:  Which is where you want to log in.  So it's first-party.  So, and that link also could have encoded your IP address, which would not change from, like, minute to minute, right, because, you know, you have a connection to the site, and you say I want to log in here.  Send me a link.



LEO:  Right.



STEVE:  So it sends you a link.  You open your email.  You click on the link.  And clicking on the link opens your browser back to that site.  Well, your IP address hasn't changed.  It's like, you know, 15 seconds went by.



LEO:  Do we know that, I mean, people must be - I would think people are using that, in fact.  Do you know if they are?



STEVE:  I don't know.  But they certainly, I mean, so you...



LEO:  That would be good.



STEVE:  Yes, you encode the timestamp, the user's IP, and their browser cookie.



LEO:  Right.



STEVE:  And that locks that link to them.



LEO:  It would only work with them.



STEVE:  Right now, where they are.  Yeah.



LEO:  I should ask, you know who uses that is Microblog.  Let me ask him in Microblog if he's doing that because that's the only way to login, as far as I can tell, is you click a link, and it sends you an email, and you click the link in the email to reopen the site.  I bet, he's smart, he's doing that.



STEVE:  It can be made - it can really be locked down.



LEO:  Yeah.



STEVE:  And be made super secure.



LEO:  Yeah.  Oh, I'm going to ask.  That's a good idea.



STEVE:  So Mykel, spelled M-Y-K-E-L, Mykel Koblenz, he said:  "Steve, just listened to your commentary again on auto keys and the banning of the Flipper Zero.  What you and the Canadian government have missed" - and Mykel is 100% correct - "is that this is only the access to the inside of the car.  All cars from about year 2000 have used a, let's call it an RFID chip to simplify it, in the key that needs to be physically present for the car to start.  Typically, the remote function is a separate system to the RFID chip in the car, so fixing the remote feature is not going to prevent the car from being stolen.



"And don't think that a remote is the only way to get into a car.  Getting physical access to the inside of a car is easy.  Break a window, use any number of methods" - like the slim jim, you know - "of unlocking a door when keys are locked inside, et cetera.  Banning the Flipper Zero will have no impact on the number of cars being stolen, not unless it is able to replicate the RFID function of the key.  If the car has a CAN bus, then that is another avenue for attack and theft.  There are videos of a Lexus having its headlight popped out to access the CAN bus at the back of the headlight, and then the car is opened and started using an injection technique that fools the ECU into thinking that the key is present and the start signal has been given.  Cheers."



And of course Mykel is 100% correct.  My entire conversation about this was effectively off-topic last week, since I was only thinking about unlocking the car, not about starting it and thus stealing it.  And you cannot steal a car merely by unlocking its doors, as he points out.  So thank you.  And you're right, having the Canadian government as a consequence banning Flipper Zeros will obviously have no impact whatsoever upon auto theft.  I would imagine that it's the how-to TikToks and the YouTube videos that provide the greatest impetus and explanation for the rise in Canadian auto theft.  But, you know, what is any politician going to do about that?



LEO:  Right.



STEVE:  ViperXX said:  "Hi, Steve.  Hello from Germany.  Long-time listener, SpinRite license holder.  The router topic."  He said:  "The company AVM, a very popular German router brand, actually does what you say.  They require you to confirm security sensitive changes by pressing a button on the router or via a connected phone.  And in the last release they added a one-time password, an OTP token which lets you add it to your authenticator app."



So I just wanted to share that with the world.  The company is AVM, a German router brand.  And that is very cool.  Let's hope that this heightened level of configuration security spreads since it might help to crimp the trouble that we are seeing with routers.  And as we know, something really needs to be done.



Raed Iskandar, he said:  "Hello, Steve.  I was just listening to your response on our new Canadian ban" - he must be Canadian - "of the Flipper Zero."  He said:  "Your challenge system is a good method to strengthen the car-to-key communication.  However, the current Canadian car thefts are not relying on the jamming method.  The thefts have been recorded by victims' security cameras using a signal extender to allow the attacker to unlock and start the car from the owner's driveway while their key is in the house."  And of course we've covered this, too.



He says:  "Once the car has started, the attacker just drives off with it.  And as long as they don't turn it off before reaching their destination, they got what they came for.  This is not even a capability that the Flipper Zero can currently perform.  In my opinion, this type of attack requires a redesign of how the key and car communicate.  Perhaps a shorter communication field would be required, like NFC, in order to make the key's signal not audible by a radio location outside of a victim's house.  Or perhaps a physical kill switch on the car key itself so that when an owner is inside their house and are not expecting their key to be used to actively unlock the car, they can disable its radio."  He says:  "I keep my car keys inside an RF sleeve, which creates one extra step to unlocking my car, but completely blocks all the current attacks that have been occurring in my neighborhood."



LEO:  Yeah.



STEVE:  "Looking forward to hearing your thoughts on this."  Okay.  So I'm very glad for the additional information, and our long-time listeners will recall that we extensively covered exactly this attack some time ago, the use of signal extenders for car theft which serve to trick the car and the key into believing that they are much closer to each other than they actually are.  Keys normally not working from a distance is a feature, not a bug.  Right.  And signal boosters defeat that somewhat weak security.



At the time, we talked about adding "time of flight" to the security, though that becomes tricky when an active agent must respond to a ping, since its own response time might be long compared with the speed of light, though there might be something that could be done using phase shifting or interferometry to determine distance separately from signal strength, which is what you would want.  Again, I presume that there's a lot of work being done along those lines.  But, once again, targeting the Flipper Zero as the culprit is way off the mark.



LEO:  So I was going to show you this.  This is the card key for my car.  And newer BMWs use Apple's Car Key, they call it.  So this is an RFID card.  And you can see there's even instructions, you know, tap it on the NFC.  It is not, I'm sorry, RFID.  It's NFC.  So it doesn't work at longer distance; right?  You have to tap it on the door.  And then but the phone also has an unlock.  My card key is in my Apple Wallet.  And it's using UWB, the ultra-wideband.



STEVE:  Wideband.



LEO:  Right, which is basically directional radar.  And so it is, I think, immune to those kinds of replay attacks; right?



STEVE:  Very nice.  Very...



LEO:  Because it's UWB,  yeah.



STEVE:  Oh, definitely, yes.



LEO:  So I think this is - and by the way, it works so much better than the old Bluetooth car key in my Ford Mustang, which would fail all the time.  This is infallible.  In fact, it also works on my watch, which is nice.  If I don't have my phone, my  watch will get me in.



STEVE:  Very cool.



LEO:  And I can drive.  Although with the card key you have, because it's NFC, you actually have to put it in the location in the car because it has to be proximate to the NFC reader.



STEVE:  Ah, right, right, right.



LEO:  So you put it in the phone charging tray, and it works.



STEVE:  So they provide a solution for people who have no additional Apple technology.



LEO:  That's why the key.



STEVE:  Is there also something for Android?



LEO:  Yes, it works in Android.  I don't know if Android is as secure.  I presume it is.  I don't know how it works.



STEVE:  I don't think Android has ultra-wideband.



LEO:  Has UWB?  So it may not.  May not be as secure.  They also offer a fob for people, like old people like me, who don't understand how all this stuff works.



STEVE:  And who know what a fob is.



LEO:  Yeah, I have two of them.  Amazing.



STEVE:  So Emma Sax said, well, she provided some useful thoughts about meeting the need for throwaway email.  Emma wrote:  "I have a few comments regarding the email signups for tons of different throwaway websites.  I started moving to an email alias service about a year ago.  It's been a game changer for me.  Due to Bitwarden's integration with my choice service," and she says, "Bitwarden currently integrates with SimpleLogin, AnonAddy, Firefox Relay, Fastmail, DuckDuckGo, and Forward email.  It makes it super easy to generate email aliases on the go.  So now, I no longer mind if I need to provide an email address to a random website.



"As Leo said," she said, "even if you use a single throwaway email address, it's still a fingerprint, and it's still trackable across different websites.  And if you use a personal domain with multiple email addresses, all emails with that domain are a fingerprint.  With these alias services, there is no fingerprint.  There's no tie between the different email addresses.  I'm not saying whether these email address services are the best, or whether Bitwarden is the best password manager" - we think it is here - "but I chose a provider I trust for both my email alias service and my password manager, and I have not been disappointed with them yet.  And their integration with each other is invaluable.  Thanks for all you do.  I'm so happy to hear you're going past 999 on Security Now!."  And so forth.



Anyway, thank you, Emma.  We're glad to have you, too, as a listener - she said, you know, that she was happy that we were continuing - and everyone else who finds this podcast to be worth their time.  I really do understand how valuable everyone's time is.  We've talked about Bitwarden's integration before; so I thought it was worth sharing Emma's experience to perhaps give our listeners a little bit of a nudge in the direction of considering email integration.  Since more and more listeners are reporting encountering the "Join our website to access our valuable content" notices, I have the feeling that throwaway email is going to become increasingly necessary for anyone who would prefer not to be providing explicit tracking data.



DH said:  "Hey, Steve.  One remark about the 'click link in email to login to your account without password' feature mentioned in Episode 962.  As mentioned during the episode, one could see it as a password-sharing prevention mechanism because no one in their right mind would give access to their main email account.  Nevertheless, you still could use a shared, separate email account specifically created for login to specific services you intended to share.  Signed, Daniel."



And that's a great point.  I love that.  Instead of not wanting to share your email address, create a deliberately shared email account which you share with those who you wish to share login access with.  Then the email loop actually makes all of that easier.  You don't have to, like, keep a password synchronized among yourselves because you're just - you've already got email which is serving as your one-way of logging in.  And of course it could be used for multiple accounts which all being shared among that group of users.  Very nice.



Christopher Ursich, he wrote:  "Steve," and he says:  "Chris from Cleveland here, a listener since the days of The Onion Router, SecurAble, Jungle Disk, and the Astaro Security Gateway."  He said:  "In SN-962 you gave a recommendation for client-side password quality enforcement.  We need to deprecate website passwords entirely, but in the meantime" - and of course we know that's never going to happen - "in the meantime I think I have a better idea that is even easier for sites to implement."  He said:  "It should not be difficult to define a declarative microformat," and he says, "a.k.a. microformats.org, that sites can use to machine-readably inform browsers and password managers what password constraints the site requires.  Bitwarden or Mozilla could even write the standard.  This would allow sites that don't actually handle passwords properly to at least avoid burdening the user with cumbersome rules.  Regards, Chris."



Okay.  So I agree strongly with part of what Chris has suggested, and I think it's brilliant.  Okay.  So first, I doubt that the microformats.org that Christopher refers to as an example would be adopted in a world that's pretty much settled on JSON, J-S-O-N, JavaScript Object Notation, as its textual representation for structured data.  Microformats date from 2004, so that's 20 years now, and it worries about counting and minimizing character counts because that was sort of its deal back then.  That doesn't pack the same punch today as it would have back when the '90s were only a few years removed.



But the representation format of the data is really beside the point and doesn't matter.  The brilliance is the idea that there could be a very simple means for our password managers to obtain a website's more or less arbitrary password rules and constraints without any human intervention.  When you're using a password manager, as I'm sure now everyone listening to this is, you know, and you know you're never going to need to remember any site's password, the longer the password the better; right?  So 32 characters with all possible character classes mixed together would be perfect.



But then you hit upon some annoying site that says, "Your password is too long.  20 characters maximum."  So, okay, you dial the length down to 20.  Then it says, "You must have some upper-case characters."  What?  And sure enough, by the luck of the draw, that shorter 20-character password happened to be all lowercase, numbers, and special characters.  So you need to make your password manager regenerate another password.  So you do that, and now you're told that it must also have at least four non-consecutive numeric characters.  Okay, perhaps I've created a worst-case example, but everyone gets the idea, as I'm sure we've all needed to adjust at least the length of our password manager's automatically generated passwords in the past.



We already have the well established, you know, it's in the root of a server, it's /.well-known/ directory which is used for locating website information in specific directories off the root.  So we've got that in place.  The industry could define a /.well-known/ directory named "password-rules," and that directory could contain a JSON file which succinctly describes the site's acceptable password policy.  A configuration option in our password manager would be to poll - we could turn it on - to poll any site's acceptable password policy whenever our password manager is about to present a password recommendation, and design the password it offers to match the most secure password allowed under that site's policies.  Gone, then, permanently, would be the need to constantly change the details of the password our password manager creates.  It could always be set to maximum, and it would drop down to what a site said it was willing to accept, if necessary.



Anyway, I know it would be a heavy lift to get this adopted industry-wide.  Remember, I'm the guy who spent seven years on SQRL.  But not all sites need to do it, and those that did would be encouraging the use of the strongest possible passwords for their account holders.  So it would be beneficial to the site.  And it would also make automatic password rotations, which are sometimes necessary, you know, where you want to change all your passwords, much more automatic because your new password wouldn't be violating that site's rules.  We know that even with the adoption of Passkeys, passwords will not be disappearing.  They'll be with us for the foreseeable future.  So automating the selection of the strongest possible password for a site seems like a useful feature.



Okay.  We're at page 19 in the show notes, which typically means that I've been trying everyone's patience long enough for the week.  Even so, there were three additional stories that I ran out of time to cover the way I wanted to.  The first one was a story that I thought was going to be the most exciting, generating actually some quite frightening, well, the story itself generated some quite frightening headlines about a new side-channel attack on fingerprint biometrics.  For example, Tom's Hardware coverage was headlined "Your fingerprints can be recreated from the sounds made when you swipe on a touchscreen."  It continued, "Chinese and U.S. researchers show new side channel can reproduce fingerprints to enable attacks."



Okay, now, what?  The only problem with that is that it's not even remotely true.  It turns out that within the fingerprint biometrics research community there are two generic fingerprint templates, one called "MasterPrint," and the other is "DeepMasterPrint."  By themselves, these templates have a 1.88% and 1.11% chance of fooling any fingerprint sensor that's been trained on some specific individual's actual fingerprint.  Okay?  1.88%, like this freaky MasterPrint that the industry has created turns out to sort of like be a generic fingerprint, and it'll work 1.88% of the time.



LEO:  This may be more of a flaw in fingerprints.  They're not unique.  I mean, that's a...



STEVE:  They're not unique enough, exactly.



LEO:  Yeah, yeah.



STEVE:  Exactly.  Our fingerprint - well, and we know.  You look at it, and you can decide, like oh, yeah, that looks like a fingerprint.



LEO:  [Crosstalk], right.



STEVE:  That doesn't look like, you know, dust or entropy or something.



LEO:  Or even a QR code; right.



STEVE:  Exactly.  Okay.  So that itself alone is interesting, the idea that there is this thing called a MasterPrint, which is known.  Okay.  And that it's a generic template for fingerprints.  But what these researchers found was that they were able to slightly better inform those very low performance generic MasterPrint templates by listening to the sound of a finger moving across a touchscreen.  I suppose it should not be surprising that something might be learned from that, but it should also not be surprising that it's not very much, and that it's certainly not, as the breathless headlines claimed, "Your fingerprints can be recreated from the sounds made when you swipe on a touchscreen."



You know, it turns out it barely helped at all, although it took me a long time to figure that out because I had to read the research paper.  But anyway, so much for that.  If you saw that, and you wonder why I didn't talk about it, it's because it's nonsense.



I also wanted to have time to check back in on the state of our intrepid Voyager 1 spacecraft since it appears that it may have finally lost its battle with time and entropy.  I will make some time for a more detailed look at that next week.



And finally, the story that's probably going to be next week's main topic, so I definitely didn't have time to fit it in today, is Apple's announcement last week of PQ3, where "PQ" stands for Post-Quantum.  The blog posting from Apple's Security Engineering and Architecture group contains sufficient detail to make for a terrific main topic.  So stay subscribed, and we'll be back next week with all of the interesting details about Apple's PQ.



LEO:  Yeah.



STEVE:  Adding post-quantum crypto to messages.



LEO:  I was very curious what you thought about that.  So, good.  I suppose, I mean, PQ3 is not one of the NIST protocols.  So I suppose they're using one of the NIST protocols.  But we'll find all that out.



STEVE:  Right.  Yeah, PQ3 is their own...



LEO:  Their name for it.



STEVE:  ...encapsulation of, you know, how to do - and you've got to do key distribution and key rotation, I mean, basically what NIST is giving us are some ciphers.  But as we know, there's a long distance between a cipher and an entire working protocol.



LEO:  Yeah, yeah.



STEVE:  That has all the bells and whistles that Apple will need.



LEO:  Pretty cool, actually.  That's great. 



STEVE:  So I think that next week's topic may just have three letters.  Or characters.



LEO:  I hope so.  I hope so.  Steve Gibson, GRC.com.  By the way, our Discord, our wonderful Club TWiT members tell me that at least some Android phones do have ultra-wideband, including those new Samsung Galaxy S24s.  Or, sorry, no, he has a Note 20.  Wojo has a Note 20.  So it's been around for some time.



STEVE:  Oh.



LEO:  Maybe even predates Apple.



STEVE:  Maybe Apple's catching up, yes.



LEO:  Yeah, yeah.  I haven't tried the car key feature in Android.  I have a Pixel whatever, the latest.  I should try it.  I'll get back to you.  It sure is nice when your car walks up and recognizes you, and you just hope in, drive off.  I love that.  I love that feature.  Steve is at GRC.com.  Hop in.  Drive off with a brand new copy of SpinRite 6.1.  You'll be glad you did.  It's official, and you can get your copy at GRC.com, the world's best mass storage maintenance and recovery utility.



Copyright (c) 2024 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.






