GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#897

DATE:		November 15, 2022

TITLE:		Memory-Safe Languages

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-897.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we have another event-filled Patch Tuesday retrospective.  We look at a newly published horrifying automated host attack framework which script kiddies are sure to jump on.  We have a welcome new feature for GitHub, crucial vulnerabilities in the LiteSpeed web server, a spiritual successor to TrueCrypt and VeraCrypt for Linux, Australia's announcement of their intention to proactively attack the attackers, a controversial new feature in iOS 16.1.1, a couple more decentralized finance catastrophes, some miscellany and listener feedback.  Then we'll finish by looking at a just-published advisory from U.S.'s National Security Agency, our NSA, promoting the use of memory-safe languages.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  It's another event-filled Patch Tuesday retrospective.  We'll talk about a new automated attack framework the script kiddies are going to just love, a replacement for TrueCrypt for Linux that gives you plausible deniability, and then he'll take a look at the NSA's proposal for memory-safe languages.  This is one everyone should pay attention to, next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 897, recorded Tuesday, November 15th, 2022:  Memory-Safe Languages.



It's time for Security Now!, the show where we talk about your security and privacy online with our guy right here, this cat, Mr. Steven Gibson of the GRC.



STEVE GIBSON:  We will protect you whether you want it or not.



LEO:  Hi, Steve.  Happy Tuesday.  We survived Election Day, and we are now on to other things.



STEVE:  Well, we've got tonight's announcement from Mar-a-Lago.



LEO:  Oh, that's right.  It's 'uge.  A 'uge announcement.



STEVE:  It will be at 6:00 p.m. Pacific time, 9:00 p.m. Eastern for those who are interested.  I'm certainly a spectator.



LEO:  I will be watching, yeah.



STEVE:  I will be glued with my popcorn, and we'll see what happens.  It's a lot of fun.



LEO:  Yes.



STEVE:  Also fun is today's topic.  This is another one of these weeks where there was no, like, major cyber research that was - I always like to, you know, to track cyber research things.  Nothing really happened except a bunch of news.  Except that the NSA published a real interesting sort of an appeal to everyone to seriously consider switching to the use of memory-safe languages.



LEO:  Yes.  Yes.



STEVE:  Yes.  Thus the title of today's episode of Security Now! 897 for November 15th, "Memory-Safe Languages."  We're going to do a little retrospective on the event-filled, more eventful than Microsoft would have wished, Patch Tuesday.  We're going to look at a newly published horrifying automated host attack framework, which script kiddies are sure to be jumping on.  We've got a welcome new feature that's been introduced into GitHub.  Also three critical vulnerabilities in the - it's only sixth in the world, but that gives it 1.9 million instances, the LiteSpeed web server.



LEO:  Oh.



STEVE:  We've also got something I think is going to be of interest to our Linux users, the spiritual successor to TrueCrypt and VeraCrypt for Linux.  And then we're going to do a little segment, 2.5 minutes we're going to play of Australia's announcement of their intention to proactively attack the attackers.



LEO:  Oh, that doesn't sound good.



STEVE:  I don't know how you could do that legally.



LEO:  Yikes.



STEVE:  It's like, oh, okay.  And boy, she's pissed off.  We've also got a controversial new feature that was added to iOS 16.1.1.  Also I just have to touch on a couple more decentralized finance catastrophes, believe it or not, Leo, other than the collapse of FTX last week, or weekend before last.  We've also got some miscellany and listener feedback, and then we're going to wrap up by taking a look at this National Security Agency promotion of the use of memory-safe languages.  So I think another great podcast for our listeners.  Oh, and because of last Tuesday's election, they put up a chart on the screen at one point where I just said, okay, stop everything, hold on.  This is just so wrong.  And we probably talked about this last year.  But again, I just can't resist because this just - it just makes my blood boil.  It's like, who's in charge?



LEO:  You know, I'm looking at it.  I'm trying to figure out what you don't like about it.  We'll find out in just a little bit.  Oh, I see.  Yes, I do see.



STEVE:  It's green.  Green is like way bigger than orange.



LEO:  Yes.  You're right.  I see.  We'll show you in a second.  Okay.  Now that I look at it, yes.  Interesting, huh.  It's a perspective thing.  You just wouldn't understand, Steve.  It's all about TV production.



STEVE:  I lack a perspective, that's right.



LEO:  All right.  I've got a chart.



STEVE:  So the whole point of a pie chart...



LEO:  Yes.



STEVE:  ...is to show you visually...



LEO:  Yes.



STEVE:  ...the angles of the various slices of the pie.  I mean, the reason aircraft instrumentation and even our cars still use dials, still have pointers, and a clock face, an analog clock face is so powerful, is that it requires no interpretation, no figuring out what the numbers mean in order to get meaning from this.  They tried to go all digital in cockpits years ago, and it turns out that was not a good idea because a pilot can instantly see the angle of needles as opposed to needing to interpret numbers.



LEO:  Yeah.  Yeah.



STEVE:  So what happened last Tuesday during the election is that the various outlets could not resist doing this 3D thing to their pie charts.  And in order to make it more fancy 3D, they accentuated the third dimension, creating this perspective.  Well, so today's Picture of the Week shows a random exit poll with five different values.  And the largest one is smaller looking than the next largest one.  In this chart green looks like it's way more than 90 degrees, when in fact it's not.  I mean, compared to the orange thing.  Anyway, it just - I know I've mentioned this before.  But I saw this, and I thought, oh, you know, come on, guys.  Just give us angles which are a flat pie chart that we're looking down on from above, rather than like, oh, we're going to make this 3D and fancy.



LEO:  But it's pretty, Steve.



STEVE:  Yeah.



LEO:  It may be inaccurate, but it's pretty.



STEVE:  Yeah, that's right.



LEO:  Yeah.



STEVE:  Okay.  Anyway, I'm done with my rant on that.  Last Tuesday was November's, not only was it the midterm elections in the U.S., it was November's Patch Tuesday.  We had security updates being released by Adobe, Microsoft, SAP, Android, VMware, Citrix, and others.  Everybody sort of piled on, probably hoping to like not be seen as obviously, if everyone does it on the same day.  In the case of Microsoft, 67 security flaws were fixed.  So, you know, on the high side, but not 167.  And among those were the two lingering so-called "ProxyNotShell" zero-day vulnerabilities which have been continuing to plague those running onsite Exchange Server installations.  There were also another four actively exploited zero-days fixed by last week's update.  There was a remote code execution in the JScript9 scripting language.  The Google TAG team found and reported that to Microsoft.  And again, zero-days; right?  They were being exploited when they were discovered.



There was a Mark-of-the-Web bypass known as "ZippyReads."  An Elevation of Privilege in the Windows Print Spooler service, we're still discovering problems with that.  Microsoft found that one being exploited.  And also an elevation of privilege in the Windows cryptographic number generation key isolation service which, again, clever people are finding these problems and using them to elevate their privileges, typically obviously bad people.  So Microsoft found that being happening.



However, amid all this, we did not get past this month's Patch Tuesday without the induction of some new headaches for enterprise admins.  Since last Tuesday, Microsoft has been investigating the cause behind many reports, and no doubt many quick rollbacks of the month's "improvements" to Windows, which affected enterprise domain controllers, which started experiencing Kerberos sign-in failures and other authentication problems.



Well, we now know since Microsoft said, well, we're investigating this, we know what's going on.  A little bit about Kerberos.  After Windows 2000, Kerberos replaced the creaky and never quite ready for prime time homegrown NT LAN Manager protocol.  And we've discussed NTLM's many security problems through the years on this podcast.  Kerberos was designed by MIT and was first released, interestingly, the year after I first published SpinRite.  Yes, way back in 1988 was when Kerberos happened.  So it's been around for 39 years.  It's a bulletproof client server mutual authentication protocol that works.  Well, at least it did for Windows until last week.  They're going to get it fixed, I'm sure.



They've acknowledged the trouble by saying - this is Microsoft:  "After installing updates released on November 8th, 2022 or later on Windows servers with the domain controller role, you might have issues with Kerberos authentication."  Which is to say, you know, pretty much all authentication that the domain controller's using.  They said:  "When this issue is encountered, you might receive a Microsoft-Windows-Kerberos-Key-Distribution-Center Event ID 14 error event in the System section of event log on your domain controller with the below text.



"And as a consequence, here's what happens.  Domain user sign-in might fail.  This also might affect Active Directory Federation Services authentication.  Group Managed Services Accounts used for services such as Internet Information Services" - their IIS web server - "might fail to authenticate.  Remote Desktop connections using domain users might fail to connect."



Of course, all of these "mights" are actually "wills."  Right?  "You might be unable," and we read this as you will be unable, "to access shared folders on workstations and file shares on servers.  Printing that requires domain user authentication might fail," or will.  Anyway, so, you know, this is the typical collapse that we are with increasing frequency seeing following the application of a monthly round of patches and improvements.  Thanks to a listener, Jonathan, I'm sorry, Johan Mellberg, who tweeted about this, there's a Reddit thread that explains what happened.  Now, the current best practice is to disable the use of the very old RC4 cipher for Kerberos.  But November's trouble has hit those enterprises which did.  So this suggests that Microsoft did not test their November changes under what is now well-established current best practices.



I've got a link in the show notes to this Reddit thread for anyone who's interested and who would be affected by this.  It documents the fixes to this which have worked for all of those who have implemented them.  So that might be useful to some of our listeners.  We tell everyone, of course, that patching as soon as possible can often be critical.  We know that it can be.  But it's also true as we seem to keep seeing that doing so might also cause major systemic failures throughout an organization.  So it's no wonder that those whose job this is to deal with this are notoriously high stressed.



Anyway, so a bunch of problems fixed, a bunch of zero-days fixed, a regression which Microsoft will fix, I'm sure, and some workarounds.  So your typical Patch Tuesday.



This next bit of news falls under the category labeled "Not all things that can be done should be done."  And we occasionally encounter those on the podcast.  Unfortunately, a new, smart, host attack automation framework - yes, a host attack automation framework - was open sourced a week ago, last Tuesday, on GitHub.  My first thought was, "Please don't make attacking hosts any easier for the script kiddies."  But it's too late.



A cybersecurity engineer by the name of Mazin Ahmed explained that back in 2019 he and a fellow researcher, Khalid Farah, participated in a security competition for developing offensive security tools.  Not defensive, offensive.  And as we'll see a bit later, these might be now of interest to Australia, but we'll get there in a minute.



So Mazin wrote, he said:  "I enjoy building security tools, and this competition was funded by HITB" - that's Hack In The Box - "with a reward of $100,000 for the winners."  He says:  "I thought it would be an interesting challenge to work on as a side project.  I met my friend Khalid.  He was also interested in winning this competition.  We signed up and, once accepted, we started meeting regularly to build this project."



Okay.  So the Hack In The Box CyberWeek AI Challenge, as it was called, had two categories, Host Exploitation or Malware Evasion.  And unfortunately they chose the Host Exploitation option.  So he wrote:  "Host exploitation sounds more relevant to our experience.  The goal was to build a host exploitation framework using AI, based on the concept of DeepExploit.  The winning team should ideally prove the accuracy of the model, the improvement of the training and execution speed, and the technical features that have been added to the framework.  We started experimenting with DeepExploit and how it works, and we decided to start a new project based on the ideas we had on how to improve that tool.



"This eventually ended up having us develop [something that they  named] Shennina (S-H-E-N-N-I-N-A), a host exploitation framework."  He said:  "The project is four times faster than DeepExploit.  We were excited about the results.



"Shennina comes with a deception detection capability that detects if the machine being exploited is a Virtual Machine or a Container, and then terminates post-exploitation once it's detected.  This feature is powered by Metasploit modules.  The Shennina Framework has qualified for the top five projects out of 40,"  that is, in that competition.  He said:  "We worked on developing the tool further to prepare for our final demo that will be live at HITB Abu Dhabi 2019."



Okay.  So that was, what, three years ago.  He said:  "Unfortunately, the rules of the competition and the judging criteria changed during the demo day."  Which I guess meant that they didn't do as well as they expected to.  He said:  "We enjoyed HITB CyberWeek 2019.  It was an amazing journey, and I enjoyed building the Shennina Framework.  I also presented my research on JWT hacking at the time."  JWT is the abbreviation for JSON Web Tokens.  And he says:  "It was a busy week.  Finally," he said, "We are planning to open-source the project and the experiment.  There are no plans for further maintaining Shennina in the near future."



And that open sourcing of Shennina just came to pass.  This powerful Shennina Host Exploitation framework is now up on GitHub.  They do comment under the FAQ question, they ask themselves:  "Why are we solving this problem with AI?"  And they reply:  "The problem should be solved by a hash tree without using AI.  However, the HITB CyberWeek AI Challenge required the project to find ways to solve it through AI."  So that's what they did.



Okay.  So extracting some of the more interesting bits from the project's abstract, they explain:  "Shennina is an automated host exploitation framework.  The mission of the project is to fully automate the scanning, vulnerability scanning and analysis, and exploitation using artificial intelligence.  Shennina is integrated with Metasploit and Nmap for performing the attacks, as well as being integrated with an in-house command-and-control server for exfiltrating data from compromised machines automatically."



And I'm thinking, "And why post this tool publicly?"  Wow.  Then they continue:  "Shennina scans a set of input targets for available network services, uses its AI engine to identify recommended exploits for the attacks, then attempts to test and attack the targets.  If the attack succeeds, Shennina proceeds with the post-exploitation phase.  The AI engine is initially trained against live targets to learn reliable exploits against remote services.  Shennina also supports a heuristics mode for identifying recommended exploits.  The documentation can be found in the Docs directory within the project."



And then they enumerate their features:  automated self-learning approach for finding exploits; high performance using managed concurrency design; intelligent exploits clustering; post exploitation capabilities; deception detection; ransomware simulation capabilities; automated data exfiltration; vulnerability scanning mode; heuristic mode support for recommended exploits; Windows, Linux, and macOS support for agents; scriptable attack method with the post-exploitation phase; exploits suggestions for Kernel exploits; out-of-band technique testing for exploitation checks; automated exfiltration of important data on compromised servers; reporting capabilities; coverage for 40-plus TTPs - that's of course Tactics, Techniques, and Procedures - within the MITRE ATT&CK Framework; and supports multi-input targets.



Well, that's just great.  It's bad enough - really.  I mean, do we have to be doing this, making this so easy for the script kiddies?



LEO:  This is the excuse they always give.  It's for pen testing.  You know, it's to simplify your pen testing.



STEVE:  That's right.  It's bad enough that the quite sophisticated and powerful Nmap scanner and Metasploit exploit frameworks, both which have been developing and evolving over many years, already allow sophisticated hackers to scan network regions and exploit them.  But now this process has been automated with a somewhat smart frontend that incorporates Nmap and Metasploit to create a turnkey, all-in-one automated system that can be launched by junior hackers who would never have the ability to create such a tool themselves.  So buckle up.



LEO:  You think it's meant for malicious use or - obviously you do.



STEVE:  I just think, like, you know, it's one of those things that just because you can doesn't mean you should.  This will be of vastly more interest to script kiddies who say, oh, look, we download it, and I run it on my Linux box, and it just attacks everything and exploits them.



LEO:  Oh, look.  It's so easy.



STEVE:  Yeah.



LEO:  Yeah.  But, I mean, you can also see how red teams need tools or would like tools like this.



STEVE:  Yes.  So I'm sure that a red team could point it at their own network and discover useful things.  Unfortunately, it also accepts the command-line star dot star.



LEO:  Oh, ow.



STEVE:  Which, you know, put it on the Internet and let it loose.



LEO:  Yeah.  Yikes.



STEVE:  So, and the fact that it runs on Linux means that if you compromise routers, routers are all Linux-based now.  So just host this and start scanning and exploiting.  Wow.  Yeah.



So again, we've seen where there's a problem with people posting proofs of concept to GitHub now because they're immediately weaponized.  Well, now we've just weaponized Nmap and Metasploit so you don't even have to read the manuals for them.  You just press Go.  And unfortunately, people will.  Wow.



LEO:  Ah, well.



STEVE:  You know, one thing we've seen, Leo, over and over, is the only way to drive change is to force it to happen.  So the upside is, after a period of cataclysm, we may end up with stronger software as a consequence, just because we didn't have a choice because, you know, these AI bots are now roaming around intelligently learning how to attack.  Skynet, anyone?  Wow.



Okay.  In happier news, GitHub has just added a new private vulnerability reporting feature.  It allows security researchers to report vulnerabilities which they have found which exist in public repositories to their respective owners via a private channel for the first time.  So in other words, this new feature will allow sensitive security-related reports to be filed to repo owners without having to file a publicly viewable entry in a repo's issues tracker.



LEO:  Probably should have had that a long...



STEVE:  That was my first thought was, oh, you're celebrating that in 2022?  Why is this a new feature today?  Yeah, this should have been done long ago.  But I guess maybe it wasn't in keeping with the spirit.  Again, this is - we need some evolution of the spirit; right?  And we're going to get that here before long.



As I mentioned, there are presently 1.9 million unique instances of the LiteSpeed web server providing services on the Internet.  LiteSpeed is ranked sixth most popular web server, which gives it about a 2% share of all web servers globally.  So it's not, you know, not a big name, but it's there.  The problem is, after some research by Palo Alto Networks' Unit 42, any editions of either the free and open source version, which is known as OpenLiteSpeed, or the enterprise version, which is just LiteSpeed, which have not been updated in the past month, need to be patched immediately.  So I'm bringing this to our listeners' attention because among them, I mean, I remember looking at LiteSpeed when I was considering servers.  It's a very - it's a recently written, fast, fleet little server.  But it's got some problems.



The Unit 42 research team took a close look at the source code for OpenLiteSpeed, which of course as you are often telling us, Leo, and you're right, a big advantage of open source, right, is there it is.



LEO:  Yeah.



STEVE:  Researchers, if they're so inclined, can look at it and go, uh, wait a minute, that doesn't look like a type safe cast there and so forth.  Anyway, the Unit 42 guys from Palo Alto Networks took a look at it and discovered three different vulnerabilities in the web server.  The three vulnerabilities have been confirmed to similarly affect the non-open source enterprise version, which obviously is based on the same code base.  By chaining and exploiting the vulnerabilities, adversaries can compromise the web server to gain fully privileged remote code execution.  Meaning remote, like over across the Internet.  So the three vulnerabilities, there's a Remote Code Execution which is rated high severity, with a CVSS of 8.8; and a Privilege Escalation, same, high severity, 8.8; and a Directory Traversal which is given a medium severity of 5.8.  



A little over a month ago, Unit 42 responsibly disclosed their discovery of these three vulnerabilities to the server's publisher and their maintainer, known as LiteSpeed Technologies, and even provided suggested remediation.  Okay.  That was on October 4th.  Two weeks later, on the 18th, LiteSpeed Technologies released their updates.  So anyone using the open version from 1.5.11 through 1.7.16 needs to immediately update to 1.7.16.1.  And LiteSpeed versions 5.4.6 through 6.0.11 need to immediately move to 6.0.12.



Again, those were released on October 18th, not quite a month ago.  So if this affects you, if you've got LiteSpeed running, and you haven't, and you don't know that you've updated in the last month, do it.  There are 1.9 million of these in use.  And since the fixes will be reflected in the open source version, bad guys will have no trouble seeing what was fixed and designing an exploit chain that Unit 42 warned of.  Since the location of all LiteSpeed web servers are known to Shodan and other Internet scanners, these servers will be found.  So don't delay.



Okay.  A really cool bit of news and of interest for Linux users.  Last Thursday, the guys at Kudelski Security Research open sourced an interesting utility that I think will be of interest to our listeners.  Here's how Kudelski explains their creation.  They said:  "Today we are excited to release Shufflecake, a tool aimed at helping people whose freedom of expression is threatened by repressive authorities or dangerous criminal organizations, in particular whistleblowers, investigative journalists, and activists for human rights in oppressive regimes.  Shufflecake is FLOSS (Free/Libre Open Source Software).  Source code in C is available and released under the GNU General Public License 3 or superior."



They explain:  "Shufflecake is a tool for Linux that allows the  creation of multiple hidden volumes on a storage device in such a way that it is very difficult, even under forensic inspection, to prove the existence of such volumes.  Each volume is encrypted with a different secret key, scrambled across the empty space of an underlying existing storage medium, and indistinguishable from random noise when not decrypted.



"Even if the presence of the Shufflecake software itself cannot be hidden, and hence the presence of secret volumes is suspected, the number of volumes is also hidden.  This allows a user to create a hierarchy of plausible deniability, where 'most hidden' secret volumes are buried under 'less hidden' decoy volumes, whose passwords can be surrendered under pressure.  In other words, a user can plausibly lie to a coercive adversary about the existence of hidden data by providing a password that unlocks decoy data.  Every volume can be managed independently as a virtual block device, in other words, partitioned, formatted with any file system of choice, and mounted and dismounted like a normal disk.  The whole system is very fast, with only a minor slowdown in I/O throughput compared to a bare LUKS-encrypted disk, and with negligible waste of memory and disk space."



And I love this.  They said:  "You can consider Shufflecake a spiritual successor of tools such as TrueCrypt and VeraCrypt, but vastly improved.  First of all, it works natively on Linux, it supports any file system of choice, and can manage up to 15 nested volumes per device, making deniability of the existence of these partitions highly plausible."  So for anyone who's interested, look at Shufflecake.net, S-H-U-F-F-L-E-C-A-K-E dot net.  And you'll find all the information there.



LEO:  Yeah, it looks really good.  For plausible deniability, I love it.



STEVE:  Yeah, they did a beautiful job.



LEO:  Yeah.  I have your video ready, Steve.



STEVE:  So, good.  Earlier I mentioned that Australia might have an interest in offensive as opposed to purely defensive cyber weaponry.  As we've previously noted here, Australia has been recently having more than their share of problems with cyberattacks.  They've had a slew of them, and they're apparently getting a bit tired of them.  So the straw that may have broken the camel's back is the most recent ransomware cyberattack on an Australian private insurance provider named Medibank.  Medibank said that attackers broke into their network, stole internal files - including sensitive personal and healthcare details on 9.7 million Australians - then encrypted their files and demanded a ransom.



When Medibank refused to pay, the ransomware gang, going by the name of BlogXX, although believed to be a spinoff of REvil, began leaking some of Medibank's patient records to intimidate Medibank and create public pressure to pay the ransom.  One of the group's first leaks was a file named "abortions."  Not surprisingly, the BlogXX group's actions have galvanized the Australian government, and Australia has now vowed to go on the offensive.  The group's activities have been met with outrage and significant political attention from the highest levels of the Australian government.



Okay.  So Clare O'Neil is an Australian Minister for Home Affairs, and she's the Minister of Cyber Security.  She was interviewed last week by ABC Insider and posted her 2.5-minute discussion to Twitter.  So here's what Clare had to say.



[BEGIN CLIP]



CLARE O'NEIL:  Mark Dreyfus and I announced with the Deputy Prime Minister yesterday that we're setting up a permanent standing operation, a partnership of new policing between the Australian Signals Directorate, which are the [indiscernible] of the Australian Public Service and the Australian Federal Police.  This is an entirely new model of operating for these two organizations.  What they will do is scour the world, hunt down the criminal syndicates and gangs who are targeting Australia in cyberattacks, and disrupt their efforts.  This is Australia standing up and punching back.  We are not going to sit back while our citizens are treated like this way and allow there to be no consequences for that.



INTERVIEWER:  Okay.  Just to be clear, though, this task force has been in place for while, though, hasn't it?



CLARE O'NEIL:  No, that's incorrect.  This is a new operation, a permanent standing force of 100 of the best, most capable cyber experts in this country that will be undertaking this task for the first time, offensively attacking these people, David.  So this is not a model of policing where we wait for a crime to be committed and then try to understand who it is and do something to the people who are responsible.  We are offensively going to find these people, hunt them down, and debilitate them before they can attack our country.



INTERVIEWER:  What's your expectations here of what they'll be able to achieve?  Because, you know, we know whether it's in this new standing operation or in the previous taskforces, they've been trying for a while to go after the attackers.  The Americans have for years.  They did manage to arrest a few of them about a year ago.  What's your expectation of what they'll realistically be able to achieve?



CLARE O'NEIL:  Yeah.  Well, I think there's a perception in the community that it's hard to do anything about cyberattacks, and that's actually wrong.  There's an enormous amount that we can do.  I think we need to shift away from the sense that the only good outcome here is someone behind bars because that can be hard when we've got people who are essentially being harbored by foreign governments and allowed to continue this type of activity.  But what we can do is two really important things.  The first thing is hunt these people down and disrupt their operations.  It weakens these groups if governments like ours collaborate with the FBI and other police forces and intelligence agencies around the world.  But the second important thing that we need to do is stand up and say that Australia is not going to be a soft target for this sort of thing.  And if people come after our citizens, we are going to go after them.  



[END CLIP]



LEO:  I don't think that's legal; right?  I mean, you can't pursue people before they commit the crime, for one thing.



STEVE:  I know.  I mean, this is like, what?  



LEO:  Very Australian.  Very Australian, yeah.  



STEVE:  Yeah, well, so her statement was released the same day that the Australian Federal Police issued a statement formally identifying the Medibank hackers as being located in Russia, which, you know, comes as a surprise to no one.  Australian Federal Police Commissioner Reece Kershaw said in his statement:  "We believe we know which individuals are responsible, but will not be naming them.  What I will say is that we'll be holding talks with Russian law enforcement about these individuals."



And, you know, despite an explosion in ransomware attacks, as we noted last week, more than $1.2 billion in payments made in 2021 alone, the cases where law enforcement agencies fought back and hacked the hackers have been rare.  There have been a few success stories, such as the DOJ, the U.S. Department of Justice, managing to recover the Colonial Pipeline payment that was made to the DarkSide group.  And both U.S. CYBERCOM and the FBI hacked REvil's servers following the Kaseya-based attacks.  In both cases, DarkSide and REvil's operations were shut down following these proactive, potentially, I guess, offensive operations.  And these effective responses did appear to give attackers at least a bit of pause.  Yet the problem obviously persisted, and persists.  So it's going to be very interesting to see what might come from Australia's new "we're not going to take this lying down anymore" stance.



But exactly as you said, Leo, until now the bad guys have had the advantage that they don't care about breaking cyber intrusion laws, whereas global governments have been hampered by their need to play by the rules and act within the law.  So, you know, what does Australia exactly mean when they say they're going to be going after the bad guys and disrupt them?  Are they going to break cyber intrusion laws to get that done?  I guess we'll see.  Wow.



LEO:  She may have misunderstood exactly what they were going to be doing.  Well...



STEVE:  I don't know because, I mean, he sort of pushed back, and she pushed back on him.



LEO:  Yeah.  Yeah.  Wow.



STEVE:  So they did celebrate Medibank's decision to not capitulate to the demands of its attackers.  And separately, O'Neil suggested that the government might look into a law which formally bans payments made to ransomware and data extortion attackers altogether, with the hope that this would strangle the financial incentive behind those attacks.  Like you're not going to get any money out of anyone you attack in Australia, so go attack other people.  Again...



LEO:  I mean, I understand the outrage.



STEVE:  Oh, yes.



LEO:  You know, it's infuriating.  But...



STEVE:  I know.



LEO:  But.



STEVE:  You can't be a vigilante.  



LEO:  Yeah, you can't, no.



STEVE:  So Apple iOS.  What observers are saying looks like Apple capitulating to demands from China, the latest iOS 16.1.1 and also in the next beta of 16.2, have added a new timeout to the AirDrop sharing feature which limits the "Share with Everyone" option to 10 minutes.  The reason Chinese influence is suspected is that, for the time being at least, this only affects iPhones purchased in China.  This new restriction is tied to the hardware.



On the other hand, it's also worth noting that many people outside of China have celebrated the addition of this restriction as an extremely useful security improvement, since inadvertently leaving "Everyone Sharing" on, you know, enabled, could represent a significant security risk.



And interestingly, this is not the first time Apple has customized some aspects of their iPhone offerings for China.  In researching this a bit, I learned, believe it or not, that the Taiwanese flag emoji is not available on iPhones sold in China.  Talk about petty.  Anyway, Apple also uses the hardware tied method to limit the volume level of its devices within the EU, as is required by the laws there.



So in the case of this 10-minute time limit, the motivation appears to be due to the fact that Chinese protesters had been using the feature to spread posters in opposition to Xi Jinping and the Chinese government.  And for those who think that this is a useful feature, Bloomberg reported that Apple does have plans to make this new AirDrop option available globally next year, though presumably optional as opposed to mandatory, where it is now in China.



Again, you know, we've seen that these global companies, whether it's Apple or Google or whomever, you know, if they want to operate within other countries that have different regulations, they need to alter their behavior, if they want to stay there.



Okay.  A couple of Decentralized Finance notes that I just, I couldn't help with because these are just, again, it must be that by now our listeners understand where not to put their  money.  The DeFi platform Pando said it was the target of a hack Saturday, actually Saturday before last, when a threat actor attempted to steal more than $70 million worth of cryptocurrency from the platform's wallets.  Pando said that it managed to hold onto $50 million of the attempted stolen funds, but the attacker successfully stole more than $21.8 million of its funds.  They said that the hacker used an Oracle attack against one of its protocols, and that they're still hoping to negotiate with the attacker to return some of the stolen funds.  To which apparently the proper response is, well, you can hope.



I wondered whether that had happened, since now that's been 10 days.  So far it doesn't appear the case.  I found Pando's Twitter thread where they had announced the hack of their system.  Then over the course of several days they followed the stolen funds as they were anonymously moved from one currency or exchange to another.  You know, this is just a mess.  Imagine watching nearly 22 million of your dollars moving around and being able to do nothing about it.  Such is the world we're in today.



Also, the DeFi platform DFX Finance suffered a crypto heist, reporting the loss of $4.185 million worth of cryptocurrency assets following an attack on its platform last week.  The company said the incident was identified as a - here's a new one - a reentrancy attack, which sounds like another of those increasingly common, "Oh, shoot, we failed to consider that possibility when we were designing our platform.  There went more than $4 million."



So anyway, as I said, I'm sure everyone who follows the podcast by now knows that no one should rely upon what over and over again appear to be half-baked, poorly conceived, decentralized finance systems.  And it clearly doesn't matter how well intended the creators and the operators of these systems are.  Remember the famous Willie Sutton quote, you know, where he explained that the reason he robbed banks was because that's where the money was.  For many people, money is the great motivator.  That's the singular thing that's driving the ransomware attacks today.  They could care less about anyone's data.  That's of no interest to them.  They want money.  And these DeFi systems are apparently sloshing around in way more money than they are responsible enough to manage.  So, yeah, don't put yours there.



A little bit of miscellany.  It turns out that a product which was a past sponsor of TWiT and of the Security Now! podcast, The Helm, was unable to survive changes resulting from COVID-19.  They recently announced that they would be unable to continue operating past the end of the year.  So they provided their users with 60 days' notice, so November and December, and have already stopped accepting new subscriptions.  They explained that in 2019 they had relocated their production operations from Mexico to China in order to improve their scalability, and began to work on a v2 system.  But then COVID hit, and their scheduling times more than doubled.



I'm bringing this up here for two reasons:  First, anyone who had obtained their own domain that was hosted at Helm's registrar, you may wish to move it to somewhere else if you want to hold onto it.  You know, like I would suggest Hover, my own registrar before they also became a TWiT sponsor.  So current users will have only until the end of this year to get their domain moved.  But The Helm is fully supporting that movement and will work with any of their current users to do so.



The second reason I'm mentioning this is that on their FAQ page, in response to the question "What can I do with the Helm server after you shut down?" they wrote:  "We are working on a firmware update for Helm servers to be converted to Linux servers running Armbian.  We will provide documentation for this conversion, along with pointers to some guides for running mail and file-sharing services if you would like to continue using your Helm.  We expect this firmware update to be available in early to mid-December."  So I've got a link here in the show notes to the support page about this happening, about Helm shutting down.  So, you know, just a heads-up to anyone listening that, if you are a Helm user, some action will need to be taken between now and the end of the year.  I would imagine you'll want to keep your eye out for this firmware update to update to your device that would make it freely useful elsewhere.



LEO:  Yeah, I think they're doing the best they can.



STEVE:  Yeah, yeah.



LEO:  They were hit by supply chain shortages and so forth.  I know the guys.  They're great people.  And they're just trying to do the best thing they can.  It won't be an email server anymore because serving your own email is not something for the faint of heart.  It's very difficult to do from a home network.  Even for us it would be difficult to do because of the fact that Gmail, the big mail companies just don't want to hear from you.  They'll just blackhole you like that.



STEVE:  Yeah.



LEO:  But at least you'll be able to use it for something.  And it's a cool - I still have mine.  It's a cool little thing.  I'll put this, what is it, ArmDebian?



STEVE:  Armbian.



LEO:  Armbian.  Not Ambien.  Armbian.



STEVE:  Right.  Okay.  So Elon meets Twitter.  There's no way for a weekly podcast to possibly follow the insanity that has befallen Twitter ever since Elon has taken the reins.  But one of our listeners, Ed Ross, put me onto a Twitter thread from yesterday that gave me a chuckle, which I thought our listeners would appreciate.  So Elon says, he announced yesterday:  "Part of today will be turning off the microservices bloatware."  He says:  "Less than 20% are actually needed for Twitter to work!" 



LEO:  And then of course the engineers who said, "Elon, you don't understand how it works," were fired immediately.



STEVE:  Uh-huh.  Yes, exactly.  And after the results of pulling those plugs resulted in a new catastrophe du jour, someone named Zach Silberberg tweeted.  He said:  "Apparently they didn't turn off two-factor authentication, but they did turn off the service that sends you the two-factor authentication code.  So if you log out and try to log in with an authentication code, you simply won't receive one."



LEO:  Well, first of all...



STEVE:  Thank you, Elon.



LEO:  You should have turned off SMS authentication a long time ago. 



STEVE:  Yes.



LEO:  Unfortunately, the way two-factor, you know, I went through this a couple of years ago on Twitter.  The way two-factor works on Twitter, you have to turn it on first, but they give you two other methods, an authenticator app like Google Authenticator or hardware key.  And once you get the others set up, you can uncheck, as I did, the SMS.  So if you didn't do that, if you only had SMS, I don't know what you're going to do now.  Don't log out.  The other thing, Ed Bott did a good article on what to do now because Twitter security is clearly going to fail.  He suggested, and I think he's right, to immediately go to your Twitter account and turn off all the third-party apps you've enabled.  Most of us over the years have turned on a variety of tools, maybe even Login with Twitter and stuff.  For instance, I get into Medium by logging with Twitter, so Medium shows up there.  Given that we don't know what the security status of Twitter's going to be going forward, disconnect those.  Right?



STEVE:  Very, very good point.



LEO:  Just disconnect all of those.  So I did that.  Turn off two-factor with SMS, but make sure you have two-factor on because, you know.  And then others are saying maybe you want to delete your DMs.  Those are not encrypted.  And if you had anything in there that you thought was private, maybe you should get rid of it now.  It's inevitable at this point that it's going to slowly start to crumble.  And the first thing that's going to go is security.  So you'd be wise, I mean, if you want to stay on there, that's fine.  But you'd be wise to give it nothing you don't want to lose.



STEVE:  What a mess.



LEO:  Ugh, such a mess.  It's sad.  It's really sad.



STEVE:  Yeah, it is.  Daniel Smith, a listener, he said:  "This could be a new SN" - as in Security Now! - "saying."  And then he quoted somebody on Twitter named Lewis Lovelock who replied to someone else.  And I do love this quote:  "There's nothing more permanent than a temporary solution."



LEO:  It's true.  You know that.  You had a fix that you forgot you did; right?  Ages ago to your server.



STEVE:  That's right.  That's exactly right.  You're right.  I overrode DNS for my ecommerce.  And then when they changed their IP, I stopped being able to accept payments.  It's like, uh, whoops.  Yeah.



Dennis Keefe, financial coach.  He tweeted:  "Hi, Steve.  Wanted to let you know the sponsor link for Drata seems to send you to a 404.  I'd hate for y'all to lose money.  Love the show."  And so I went, I mean, I tried Drata.com/twit.  And sure enough, it 404s.  So I just thought, Leo, mostly for your benefit, that I would note that.



LEO:  Yeah, they're a sponsor.  Go to TWiT.tv/sponsors, and there should be a link on there to any sponsor.  And if the one that we told you doesn't work - let me click Drata.com/twit.  Error 404.  So I will - whoops.  Wait a minute.  No, yeah, yeah.  It says:  "Things got out of control there.  Automate compliance and risk management control past find this page control failed."  Okay.  I'll go, I'll be taking a walk.  Well, no, after I do the next ad because I know you're getting close.



STEVE:  Yeah, yeah.



LEO:  I will take a walk down to continuity and alert them.  And thank you, Dennis, yeah.



STEVE:  Thank you for the pointer.



LEO:  Yeah, I appreciate that, yeah.  And by the way, if you want to tweet me, don't do it on Twitter.  Do it on TWiT.social, our Mastodon instance.



STEVE:  There we go.



LEO:  Yeah.



STEVE:  Neil Baldridge tweeted:  "Hi, Steve. Just a quick thank you for passing along your recommendation for the Silver Ships series."  He says:  "I'm just into book 6, and finding the stories to be a great read.  I'm trying to pace myself, but without much success.  It's so nice to find an extensive series that's so enjoyable."



And so I'll just take this opportunity to note that I totally agree.  The series author, Scott Jucha, suggests that after finishing book 13, the reader should switch over to the four-book "Pyreans" series.  So that's what I did.  I'm nearly finished with those four books.  I'm more than halfway through the fourth of that four-book series.  So I can attest that they are every bit as delicious as the Silver Ships series.  Those four books trace the history of an entirely different Earth-descended colony, and its characters are every bit as interesting and alive as those in the Silver Ships.



And what's intriguing is that by this point the reader will have grown to intimately know two very different groups of human colonists, and Scott has explained that the two threads are going to be merging once the reader resumes with the Silver Ships series in book 14.  I've come to know this diverse set of characters so well that I cannot wait to see what he has in store for them when they meet.  It promises to be quite cool.  So anyway, I'm glad that a lot of our listeners have picked up and are enjoying that series.



Tiemo Kieft tweeted:  "Hey, Steve.  I just wanted to respond to the whole OpenSSL debacle.  You mentioned not many web servers accept client certs.  This is not the case, though.  Web servers serving big sites accept client certs because CDNs and DDOS protection services like Cloudflare use a client cert to assert their identity to web servers they proxy.  This feature is used to ensure that the server only responds to requests provided through Cloudflare.  Cloudflare calls this feature 'Authenticated Origin Pulls.'"



So thank you, Tiemo.  That certainly makes sense as a reason for web servers to solicit and accept client certs.  I guess I still doubt that the number is huge, but I agree that it would definitely be another reason for some increased vulnerability to the recent OpenSSL v3 flaw.  And of course it would be those servers that would be accepting client certs because they were being served by CDNs and DDOS protectors that are probably worth attacking.  So that's a very good point.



And finally, Nariman Aga-Tagiyev, sorry about the way I pronounced your name, he said:  "Dear Steve.  After listening to Episode 289, whoa, sorry, Episode 892..."



LEO:  You're just a little dyslexic, it's okay.



STEVE:  Dyslexic much?  Yeah.  Episode 892.  He said:  "I decided to replace my Raspberry Pi home VPN server with a ZimaBoard, which I ordered online right away.  Following getting started instructions from the CasaOS, I created a new user and a complex password using its WebUI on port 80.  But I was quite concerned when I noticed that the credentials set are only for the Casa web application.  The board came with more than 10 default users on its Debian OS which still had their default passwords after the initial configuration steps."  He says:  "I could log in over SSH using casaos casaos for username and password.  The root's default password is also casaos.  There are no warnings or instructions on the ZimaBoard and CasaOS websites about how to remove default users and passwords from the board."  He said:  "I'm very concerned that if those boards are installed as a router according to this article" - and he provided a link - "anyone will be able to SSH to it if owners don't proactively delete all Debian users.  Thank you for your podcasts.  Your active listener for a few years already."



So I wanted to share that because that's definitely a big heads-up that would be important if somebody were using a ZimaBoard, not as I am, to run SpinRite and to develop SpinRite, but using it as a pfSense router and exposing SSH publicly where it's got the default casaos/casaos username and password.  So thank you very much for sending that to my attention.



LEO:  Yeah, that's nasty.



STEVE:  Yeah.  And, boy, they really absolutely should make a note, or like the setup system should change username and password for all of those other users and not leave them - not only change them for the login to the WebUI and leave all of the other ones alone.



In a quick update about SpinRite, it's all but finished.  I'm getting very, very close to the first pre-release alpha of 6.1.  I ended up rewriting a bunch of the command-line parser to make drive selection extremely flexible and powerful.  Then, as I had  planned, I implemented the final verification for SpinRite by arranging to take a hash of an entire drive over which SpinRite had been run in order to absolutely verify that not a single bit had been changed across all of SpinRite's various machinations with the drive.  And I discovered to my surprise that my AHCI driver would, in a very low but non-zero percentage of 16MB transfers, occasionally not properly read the drive's data.



To make sure this wasn't, like, I had just written this code.  So to make sure that it wasn't the brand new hashing system that was at fault for this, I tried the same thing with an IDE drive using SpinRite's also new Bus Mastering DMA driver, and that worked perfectly.  So I know that my confidence testing code is working and that the new Bus Mastering driver is also working perfectly.  But not so yet the new AHCI driver.  This just happened on Sunday evening and also a little bit yesterday morning, you know, before I switched to working on the podcast.  So I haven't had a second yet to dig into any of this.



My guess is that something is going on with my end-of-transfer status testing where I might be shutting down the controller before it's completely finished having transferred everything into RAM.  Since it's transferring data asynchronously in the background using DMA, it's definitely necessary to make sure that it's finished.  The good news is, thanks to now being able to take a hash of everything SpinRite reads, I'm able to test that quickly and verify any fixes that I make, and changes.  So anyway, I'm anxious to get back to it there, as I will tonight, to see what's going on.  And I expect to be able to say next week that I am finished with this round of SpinRite development, and that it's currently in alpha testing.  So very, very, close.



LEO:  Cool, cool.



STEVE:  And I'm very excited.



LEO:  Yay.



STEVE:  And Leo?



LEO:  Yes?



STEVE:  I'm going to wet my vocal cords, and then we're going to talk about memory-safe languages.



LEO:  I'm very interested in this because, as you know, I like garbage collection.  It's just me, but I like it.  All right, Steve.  Let's talk about memory.



STEVE:  Okay.



LEO:  It is the number one problem, isn't it, in...



STEVE:  It is, absolutely.  Like 70% of the vulnerabilities are caused by mismanagement of memory.



So what I'm going to share, verbatim, is what our National Security Agency, the NSA, published in an effort to improve security across the board.  So they said - at the beginning there's a short executive summary.  They said:  "Modern society relies heavily on software-based automation, implicitly trusting developers to write software that operates in the expected way and cannot be compromised for malicious purposes.  While developers often perform rigorous testing to prepare the logic in software for surprising conditions, exploitable software vulnerabilities are still frequently based on memory issues.  Examples include overflowing a memory buffer and leveraging issues with how software allocates and deallocates memory.



Microsoft revealed at a conference in 2019 that from 2006 through 2018, 70% of their vulnerabilities were due to memory safety issues.  Google also found a similar percentage of memory safety vulnerabilities over several years in Chrome.  Malicious cyber actors can exploit these vulnerabilities for remote code execution or other adverse effects, which can often compromise a device and be the first step in large-scale network intrusions.



"Commonly used languages, such as C and C++, provide a lot of freedom and flexibility in memory management while relying heavily on the programmer to perform the needed checks on memory references.  Simple mistakes can lead to exploitable memory-based vulnerabilities.  Software analysis tools can detect many instances of memory management issues, and operating environment options can also provide some protection; but inherent protections offered by memory-safe software languages can prevent or mitigate most memory management issues."



Okay.  So that sets it up.  Here's how they elaborate on this:  "NSA recommends using a memory-safe language when possible.  While the use of added protections to non-memory-safe languages and the use of memory-safe languages do not provide absolute protection against exploitable memory issues, they do provide considerable protection.  Therefore, the overarching software community across the private sector, academia, and the U.S. government have begun initiatives to drive the culture of software development towards using memory-safe languages.  



"How a software program manages memory is core to preventing many vulnerabilities and ensuring a program is robust.  Exploiting poor or careless memory management can allow a malicious cyber actor to perform nefarious acts, such as crashing the program at will or changing the instructions of the existing program to do whatever the actor desires.  Even unexploitable issues with memory management can result in incorrect program results, degradation of the program's performance over time, or seemingly random crashes.



"Memory safety is a broad category of issues related to how a program manages memory.  One common issue is called a 'buffer overflow,' where data is accessed outside the bounds of an array.  Other common issues relate to memory allocation.  Languages can allocate new memory locations as a program is executing and then deallocate the memory, also called releasing or freeing the memory, later when the memory is no longer needed.  But if this is not done carefully by the developer, new memory may be allocated again and again as the program executes.  Consequently, memory is not always freed when it is no longer needed, resulting in a memory leak that could cause the program to eventually run out of available memory.  Due to logic errors, programs can also attempt to use memory that has already been freed, or even free memory that has already been freed.



"Another issue can arise when languages allow the use of a variable that has not been initialized, resulting in the variable using the value that was previously set at that location in memory.  Finally, another challenging issue is called a 'race condition.'  The issue can occur when a program's results depend on the order of execution of two parts of the program accessing the same data.  All of these memory issues are much too common occurrences.



"By exploiting these areas of memory issues, malicious actors, who are not bound by normal expectations of software use, may find that they can enter unusual inputs into the program, causing memory to be accessed, written, allocated, or deallocated in unexpected ways.  In some cases, a malicious actor can exploit these memory management mistakes to access sensitive information, execute unauthorized code, or cause other negative impacts.  Since it may take a lot of experimenting with unusual inputs to find one that causes an unexpected response, actors may use a technique called 'fuzzing' to either randomly or intelligently craft multitudes of input values to the program until one is found that causes the program to crash.



"Advances in fuzzing tools and techniques have made finding problematic inputs easier for malicious actors in recent years.  Once an actor discovers that they can crash the program with a particular input, they examine the code and work to determine what a specially crafted input could do.  In the worst case, such an input could allow the actor to take control of the system on which the program is running.



"Using a memory-safe language can help prevent programmers from introducing certain types of memory-related issues.  Memory is managed automatically as part of the computer language.  It does not rely on the programmer adding code to implement memory protections.  The language institutes automatic protections using a combination of compile time and runtime checks.  These inherent language features protect the programmer from introducing memory management mistakes unintentionally.  Examples of memory-safe languages include C#, Go, Java, Ruby, Rust, and Swift.  Even with a memory-safe language, memory management is not entirely memory safe.



"Most memory-safe languages recognize that software sometimes needs to perform an unsafe memory management function to accomplish certain tasks.  As a result, classes or functions are available that are recognized as non-memory safe and allow the programmer to perform a potentially unsafe memory management task.  Some languages require anything memory unsafe to be explicitly annotated as such to make the programmer and any reviewers of the program aware that it is unsafe.  Memory-safe languages can also use libraries written in non-memory-safe languages and thus can contain unsafe memory functionality.  Although these ways of including memory-unsafe mechanisms subvert the inherent memory safety, they help to localize where memory problems could exist, allowing for extra scrutiny on those sections of code.



"Languages vary in their degree of memory safety instituted through inherent protections and mitigations.  Some languages provide only relatively minimal memory safety, whereas other languages are very strict and provide considerable protections by controlling how memory is allocated, accessed, and managed.  For languages with an extreme level of inherent protection, considerable work may be needed to simply get the program to compile due to checks and protections.



"Memory safety can be costly in performance and flexibility.  Most memory-safe languages require some sort of garbage collection to reclaim memory that has been allocated, but is no longer needed by the program.  There is also considerable performance overhead associated with checking the bounds on every array access that could potentially be outside of an array.



"Alternatively, a similar performance hit can exist in a non-memory-safe language due to the checks a programmer adds to the program to perform bounds checking and other memory management protections.  Additional costs of using non-memory-safe languages include hard-to-diagnose memory corruption and occasional program crashes along with the potential for exploitation of memory access vulnerabilities.  It is not trivial to shift a mature software development infrastructure from one computer language to another.  Skilled programmers need to be trained in a new language, and there is an efficiency hit when using a new language.  Programmers must endure a learning curve and work their way through any "newbie" mistakes.  While another approach is to hire programmers skilled in a memory-safe language, they too will have their own learning curve for understanding the existing code base and the domain in which the software will function."



Now, here's something that we've never talked about before.  But I think it's an important thing to mention here.  They say:  "Several mechanisms can be used to harden non-memory-safe languages to make them more memory safe.  Analyzing the software using static and dynamic application security testing" - abbreviated SAST for Static Application Security Testing and DAST for Dynamic Application Security Testing - "can identify memory use issues in software.  Static analysis examines the source code to find potential security issues.



Using SAST allows all of the code to be examined, but it can generate a considerable number of false positives through identifying potential issues incorrectly.  However, SAST can be used throughout the development of the software, allowing issues to be identified and fixed early in the software development process.  Rigorous tests have shown that even the best-performing SAST tools only identify a portion of memory issues in even the simplest software programs, and usually generate many false positives.



"In contrast to SAST, dynamic analysis examines the code while it is executing.  DAST requires a running application.  This means most issues will not be identified until late in the development cycle, making the identified problem more expensive to fix and regressively test.  DAST can only identify issues with code that is on the execution path when the tool is run, so code coverage is also an issue.  However, DAST has a much lower percentage of false positives than SAST.  Issues such as a memory leak can be identified by DAST, but the underlying cause of the memory issue may be very difficult to identify in the software.



"Neither SAST nor DAST can make non-memory-safe code totally memory safe.  Since all tools have their strengths and weaknesses, it's recommended that multiple SAST and DAST tools be run to increase the chances that memory or other issues are identified.  Working through the issues identified by the tools can take considerable work, but will result in more robust and secure code.  Vulnerability correlation tools can intake the results from multiple tools and integrate them into a single report to simplify and help prioritize analysis."



Couple last points.  Anti-exploitation features.  "The compilation and execution environment can be used to make it more difficult for cyber actors to exploit memory management issues.  Most of these added features focus on limiting where code can be executed in memory and making memory layout unpredictable."  Which of course we've talked about extensively before.  They said:  "As a result, this reduces a malicious actor's opportunities to use the exploitation tradecraft of executing data as code and overwriting a return address to direct program flow to a nefarious location.



"Leveraging options such as Control Flow Guard (CFG) will place restrictions on where code can be executed.  Similarly, Address Space Layout Randomization (ASLR) or Data Execution Prevention (DEP) add unpredictability to where items are located in memory and prevent data from being executed as code.  Bypassing ASLR and DEP is not insurmountable to a malicious actor, but it makes developing an exploit much more difficult and lowers the odds of an exploit succeeding.  Anti-exploitation features can help mitigate vulnerabilities in both memory-safe and non-memory-safe languages."



So they conclude:  "Memory issues in software comprise a large portion of the exploitable vulnerabilities in existence.  NSA advises organizations to consider making a strategic shift from programming languages that provide little or no inherent memory protection, such as C and C++, to a memory-safe language when possible.  Once again, some examples of memory-safe languages include C#, Go, Java, Ruby, Rust, and Swift.  Memory-safe languages provide differing degrees of memory usage protections; so available code hardening defenses, such as compiler options, tool analysis, and operating system configurations should be used for their protections, as well.  By using memory-safe languages and available code-hardening defenses, many memory vulnerabilities can be prevented, mitigated, or made very difficult for cyber actors to exploit."



So, okay.  I just wanted everyone to hear and to consider that.  And for those of our listeners who may be in positions where a choice of implementation language can be made for applications where the program's security and integrity would be important, you know, consider perhaps breaking with the status quo that's been established and choose to bite the bullet and make a switch to a more secure programming language and environment.  Take the relatively new Rust language, for example.  Since its first release in January of 2014, Rust has been adopted by companies including Amazon, Discord, Dropbox, Facebook, Mozilla, Google, and Microsoft.  And it's a systems implementation language.  It does not slow things down.



Should changing to a memory-safe language not be feasible for whatever reason, at least consider looking into the idea of using those static and dynamic analysis tools against your existing code base.  You don't have to treat what they say as gospel, but check out everything that they flag to make sure that it's not a problem.  You may find that a chunk of those things are problems and be glad that you just ran this thing over your code to see what it thinks.  All of that should really pay off.



LEO:  Are all garbage-collected languages memory safe?  In theory they would trigger an error at runtime if you addressed memory that didn't exist; right?  They'd crash.



STEVE:  I think so, unless you used a pointer that was still available to you that no longer pointed to anything.



LEO:  A compiler should catch that.



STEVE:  And we often see those use-after-free problems.



LEO:  Yeah, right, right, right.  Yeah.  I mean, Rust actually doesn't do garbage collection, does something a little more complex.  But Rust is very cool.  Not a great language if you don't want to spend a lot of time learning it.



STEVE:  Yeah.  I think Rust uses usage counting, as I recall.



LEO:  Exactly.  Yeah, reference/dereference, yeah.



STEVE:  Yes, yeah.



LEO:  Count by reference.  Sort of.  It's kind of - it's really clever.  It's unique, I think.



STEVE:  And we do hear that Rust has a steep learning curve.



LEO:  Oh, it does.  I tried.  If you use Java, it should be pretty straightforward.  If you're already using one of those languages that has a lot of declarative stuff, it should feel fairly comfortable to you.



STEVE:  Well, again, you know, C is showing its age.



LEO:  Yeah.



STEVE:  C++ is a mixed blessing.  Even its developer agrees that it sort of didn't work out the way...



LEO:  If your language has malloc or calloc, that's a pretty good sign you should not be using it; right?



STEVE:  Yeah.



LEO:  That's for sure.  Very good.  I think this is great.  So did you write - it's almost like a column.  Did you write this like a column?  You should submit this to somebody.



STEVE:  Well, the NSA submitted it to the world.



LEO:  Oh, it's the NSA.  Ah, that explains it, yeah, yeah, yeah.



STEVE:  Yeah.



LEO:  Because that's what it feels like.  It feels like a column.



STEVE:  Yup. 



LEO:  I missed that one.



STEVE:  Yup, just wanted to share it because I thought it made a bunch of really good points.



LEO:  Yeah, it's good, yeah.  People forget the NSA has a couple of missions, one of which is to secure us at home through proper practices, whether it's hardening our machines or writing safe code.  So that's a big part of the job, as well as spying on people and stuff like that.



STEVE:  Oh, that.



LEO:  Sometimes, though, they're incompatible; right?  You know, if you want to spy on them, you don't want them to use any secure.  That's wrong.



Steve, great stuff, as always.  Steve Gibson is at GRC.com.  That's his website.  Now, there's a lot there.  So let me point you to a couple of things.  I mean, it's just fun to browse around.  But of course that's where you'll find SpinRite, which is Steve's bread and butter, the world's best mass storage maintenance and recovery utility, currently 6.0, soon to be 6.1.  He's very close now.  You'll get a free upgrade if you buy now.



STEVE:  Getting nervous, yeah.



LEO:  Getting nervous?



STEVE:  Yeah, I'm getting nervous.



LEO:  Do you have a button that you push that's like, Release the Kraken?



STEVE:  Well, there's just like so many little details; right?



LEO:  I know.



STEVE:  But exciting.  So I'm excited.



LEO:  And, see, Steve - this is good, and you should understand this.  Steve isn't one of those people who releases it, sees what the bugs are, and then fixes them.  Steve wants to release a perfect piece of software from v1.0.  So that would make me nervous, too.



STEVE:  In 2004 SpinRite 6 was released, and there are no known bugs.



LEO:  Haven't patched it since.  That's kind of amazing.  Really amazing.  So that's one thing, SpinRite.  Probably GRC.com/SpinRite would probably take you there.  You also should take a look at the podcast because he has two copies of the podcast that we don't have on our TWiT site.  One is the 16Kb audio.  It's a little scratchy.  But if you have limited bandwidth, it's the smallest version.  Actually, it's the second smallest version because the smallest version I'm sure is text only.  That's the transcriptions Elaine Farris does, which are great.  You might want them separately.  You might want them together with the audio so you can read along as you listen.  And it's certainly a utility you can use to search for part of any of the shows, any of the 897 shows, and find those comments right in the show.  So it's really, really a great thing.



He also has 64Kb audio, the full quality audio.  We have that and video at our website, TWiT.tv/sn.  You can also find Security Now! on YouTube.  There's a YouTube channel.  Probably, I'm not sure, but I think it's YouTube.com/securitynow.  If you go to YouTube.com/twit, you'll find, besides all the TWiT bits and the shorts and stuff, you'll find a list of all the shows, and you can subscribe from there, as well.



Of course the easiest thing to do probably for most people is to find a podcast player you like and subscribe to Security Now! on that.  That way you'll get it automatically as soon as it's available.  It downloads automatically.  You can listen whenever you want.



If you want to watch us do it live, get the very first pressing, the EVOO Security Now!, you can - I never really thought of it that way, but I guess so - you can go watch us live at live.twit.tv every Tuesday, right after MacBreak Weekly.  That time will vary, somewhere around 1:30 or 2:00 p.m. Pacific, 5:00 p.m. Eastern, 22:00 UTC.  Live.twit.tv.



Copyright (c) 2022 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#898

DATE:		November 22, 2022

TITLE:		Wi-Peep

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-898.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we note that Firefox moved to v107 and that Google recently reached a nearly $400 million user-tracking settlement.  Red Hat has started cryptographically signing its ZIP distributions, the FBI purchased the nefarious Pegasus spyware, and Greece paid 7 million euros for the similar Predator spyware.  Passkeys have a directory listing sites where they can be used, the OMB has decreed a quantum decryption deadline, and 33 U.S. state attorneys general have asked the FTC to get serious about online privacy regulation.  We have some engaging listener feedback, and SpinRite is finally a day or two away from starting its final testing.  And we're going to wrap



up by examining some chilling research which allows the physical location in space of every WiFi device within range to be accurately determined by someone walking past or flying a tiny drone.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here with lots to talk about.  Red Hat cryptographically signing its ZIPs.  How do you do such a thing?  We'll talk about the FBI.  Apparently they tried to use Pegasus.  How legal is that?  And then we're going to talk about Wi-Peep, a new way to map WiFi access points or, more threateningly, to track people using WiFi devices.  All that and more coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 898, recorded Tuesday, November 22nd, 2022:  Wi-Peep.



It's time for Security Now!, the show where we cover your security, your privacy, your online exploits, your offline deploits, with this guy right here, Mr. Steven Gibson.  Hi, Steve.



STEVE GIBSON:  Leo, great to be with you again for...



LEO:  As always, good to see you.



STEVE:  ...what is this, the pre-Thanksgiving episode.



LEO:  It is.



STEVE:  Yeah.



LEO:  And we're almost in the 900s, which is a scary place to be.



STEVE:  Actually, it was interesting because when I got Elaine's transcript last week she said this was 897.  And she reminded me, she said, okay, that means that we are 102 episodes from 999, and there are 51 episodes per year because we skip one for the holidays.



LEO:  She's paying attention.



STEVE:  Which means exactly, precisely two more years of Security Now!.



LEO:  Wow.  I'll put that in my calendar.



STEVE:  Oh, honey, I'm not going to let you forget.



LEO:  By then you might say, oh, I'd like to keep doing this.



STEVE:  You know, Leo, I may have the hang of it by then.  So it'd be like not that big a deal.



LEO:  Well, as somebody who just quit the radio show after 19 years of doing that, I can kind of understand.  After a while you get to a point where it's like, you know, I've done everything I'm going to do.



STEVE:  And imagine, now, I would say you could sleep in on Saturday except that the show didn't start till 11:00.  So if this really changes your sleeping habits, then we have a different problem.



LEO:  I get to do stuff on Saturday, which is, I mean, I've worked weekends for 19 years.  That's a long time.



STEVE:  Well, yes.  In fact, what was happening was you were only working six hours, two days, Saturday and Sunday for three hours.  And then you and I were meeting once a month up in Toronto.  You were spending four days up there to record 15 Call for Help shows.



LEO:  I'm getting PTSD just hearing about it.  That's just crazy.



STEVE:  And you had three unfilled weekday weeks.  And so you said, you know...  



LEO:  Let's do some shows.



STEVE:  I've got a little time on my hands here.



LEO:  What was I thinking?



STEVE:  Well, aren't you glad now that you have a podcast network?



LEO:  I am.  I've been telling people this is the first time I've not been working for anybody in my whole working life.  I'm working for myself for the first time ever, something you know a lot about.



STEVE:  Well, except now you have a wife.  So, you know.



LEO:  Well, as Patrick Norton once told me, because I said, "I don't want to work for the man," he said, "Leo, there's always a man."  And in this case the man is a woman, but still.  No.  We're partners.  But it is kind of interesting that I've been an employee, a W-2 employee since I was 16 years old.  So that is a big change.  I think the podcast thing might work out, that's all I'm saying.



STEVE:  It might turn out to be something.



LEO:  I don't need to keep this job anymore, yeah.



STEVE:  It might.  So we're going to note this week many things.  We've got a new version of Firefox.  Google recently reached a nearly $400 million user-tracking settlement.  We've got some interesting legislative things to talk about during this next couple hours.  Red Hat has started cryptographically signing its ZIP distributions.  It's like, what?  You can sign a ZIP?  Well, not really.  The FBI purchased, turns out, the nefarious Pegasus spyware, just to kind of see what it's about.  Uh-huh.  Greece paid 7 million euros for a similar spyware called Predator.  Passkeys has a directory listing the sites where they can be used, so that will be exciting.  The OMB, the U.S. Office of Management and Budget, has decreed a quantum decryption deadline.



LEO:  Oh.



STEVE:  Oh, yes.  And of course we're all going to pay attention to that.  Also 33, speaking of paying attention to the FTC, 33 U.S. state attorneys general have asked the FTC to get serious, my friends, about online privacy regulation.  We'll see how that turns out.  We've got some engaging listener feedback.  And SpinRite is finally a day or two away from its final testing to begin.



LEO:  What?



STEVE:  Yeah, it's done.  I'll explain.  I'll explain.  I have a couple drives here.  There's three drives which are weird, and you should not write to these drives.  So I'll explain about that.  And then we're going to wrap up by examining some chilling research which allows the physical location in 3D space of every WiFi device within its range, like within a multistory building or whatever facility, to be accurately located within a meter or so by someone simply walking past or flying a tiny drone, for about 20 bucks.  So that's the Wi-Peep thing.  So we're going to talk about all this.  And we have a Picture of the Week that had you almost falling off your chair.



LEO:  It was pretty funny.



STEVE:  Yeah, it's a pretty good one. 



LEO:  It was cute.  I liked it.



STEVE:  So I think a good podcast for our listeners.



LEO:  All coming up on this fine 898th edition of Security Now!.  That's kind of amazing, isn't it.



STEVE:  898!



LEO:  Agh.  Well, it's funny because the last Tech Guy show is December 18th, and it's going to be I think Episode 1955.  I'm one shy of my birth year.  And I thought, if I can just do - or actually it'll be 1954, and then the Best Of will be 1955.  If I could just do one more.  That's okay.  That's okay.  I'm ready, my friend, for the Picture of the Week.



STEVE:  Okay.  So for those who are not video-connected here, as always I have to explain this.  We have a flatbed transport vehicle, a flatbed trailer sort of thing.  And it looks like there's on the left is a sort of a rust-colored red container where someone probably said, hey, we need you to pick up some dirt, so bring a container, and we've got some dirt for you.  Well, apparently the container that they brought was too small.



LEO:  Yeah, couldn't fit.



STEVE:  Because it's about one third of the back of this trailer, this flatbed.  And the rest of it has been piled up with the overflow dirt that didn't fit in the container.  Now, in a sane world they would throw a tarp over this whole thing, right, and lock the tarp down.  But maybe they didn't have a tarp?  Anyway, some apparent rocket scientist here decided, well, you know, I've got to do something, right, because I've just got this exposed dirt, the big pile on the back of this trailer.  So they used what they had.  They threw about a two-inch diameter belt across the top of the pile of dirt which is about, I don't know, it covers maybe 4% of the pile, the rest of it exposed to the air.  Now, you can sort of see also on one side, Leo, facing us, there is like, looks like the strap was somewhere else initially because you can sort of see that some of the dirt was flattened on the side there.



LEO:  Oh, yeah.  It's moved a little bit, yeah.



STEVE:  So it looks like, well, it looks like maybe the strap was originally anchored on the slot in the trailer one notch further forward.



LEO:  I think you've spent too much time looking at this.



STEVE:  So there was, yes, there was - are you telling me there's a reason SpinRite took three years to get...



LEO:  No, I think Logan 5 in our chatroom may have come up with something.  It's not to prevent slippage.  It's to prevent theft.



STEVE:  Oh.  It's brilliant.  You wouldn't want someone to steal the dirt.  



LEO:  Steal your dirt.  Don't, don't steal my dirt, man.



STEVE:  So this is like that pole that we saw that had the bike lock around it.



LEO:  Right, yeah.



STEVE:  Where it didn't - it indicated an intention without actually providing any enforcement.



LEO:  I love it.  Oh, my.



STEVE:  Anyway.



LEO:  Oh, my.  Oh, my.



STEVE:  Once again, we seem to be drifting here a little bit off of the security-related topics recently.



LEO:  But it's secure dirt.  No, no, it's secure.



STEVE:  Well, part of the goal of podcasts is to have some fun.  And so we're providing some entertainment.



LEO:  We do that, yes.



STEVE:  Okay.  With Firefox v107, which was released last Tuesday, a week ago, nothing was earth shattering.  There were no critical security fixes, but there were a very large and welcome collection of high-severity things fixed.  No zero days that were noted.  There were also a couple moderate severity repairs.  So, you know, it appeared to primarily be released just to fix those things, since there were not otherwise even a large number of new features.  A couple little developer things, you know, they're continuing to push the standards which Firefox supports forward because, you know, the web people can't keep their hands off of, like, ooh, how about if we added the ability to read your mind?  That would be good.  It's like, well, okay, we don't have that technology.  Or no, but let's develop an API for that so that when we do, web pages will be, I mean, this is what's going on.  So a little bit more of that is happening.  Nothing else to see.



It was interesting to me to see that Google recently settled something that we discussed four years ago.  This was a suit brought against Google by 40 states attorneys general.  They settled for $391.5 million.  Where that number came from only the attorneys know.  As I said, we talked about this four years ago, back in 2018, when these offices of those 40 states attorneys general sued Google, alleging that Google had been lying and misleading users into thinking that they had disabled location tracking in their account settings.  The lawsuit followed some reporting that was produced by the Associated Press, which found that Google was continuing to track its users even after they had enabled the account privacy setting that claimed to turn off location tracking.



So in that settlement Google agreed to pay this $391.5 million in restitution and also, of course, to change the way it handled location tracking in the future.  The first thing we're reminded of is that the wheels of justice, when they don't completely fall off the wagon, do tend to turn slowly, at least in the United States.  So it took us four years to get to this point.  The other thing we learned is that, thanks to Google's posting about this, their own posting, we've learned what has changed since then.



So their posting last week was titled "Managing your location data."  And it brings new meaning to the phrase "putting on a happy face."  They wrote:  "Location information lets us offer you a more helpful experience when you use our products.  From Google Maps driving directions that show you how to avoid traffic, to Google Search surfacing local restaurants and letting you know how busy they are," like all the benefits, right, of Google knowing where you are.  They said:  "Location information helps connect experiences across Google to what's most relevant and useful."  And, okay, yeah, that's certainly the case, or can be.



They said:  "Over the past few years" - right, while this lawsuit was in the works - "we've introduced more transparency and tools to help you manage your data and minimize the data we collect.  That's why we" - and then they have three things - "launched auto-delete controls, a first in the industry, and turned them on by default for all new users, giving you the ability to automatically delete data on a rolling basis and only keep three months', 18 months', or 36 months' worth of data at a time."  And if that sounds familiar to our listeners, it's because, yes, we covered this when this was happening.  Second thing they did:  "Developed easy-to-understand settings like Incognito mode on Google Maps, preventing searches or places you navigate to from being saved to your account."  And third:  "Introduced more transparency tools, including Your Data in Maps and Search, which lets you quickly access your key location settings right from our core products."



And they said:  "These are just some ways that we have worked to provide more choice and transparency.  Consistent with these improvements, we settled an investigation with 40 U.S. state attorneys general based on outdated product policies that we changed years ago."  Okay, you know, in addition to the $391.5 million, "outdated product policies that we changed years ago.  As well as a financial settlement, we will be making updates in the coming months to provide even greater controls and transparency over location data."  So things to come.



These updates include three things:  "Revamping user information hubs.  To help explain how location data improves our services, we're adding additional disclosures to our Activity controls and Data & Privacy pages.  We're also creating a single comprehensive information hub that highlights key location settings to help people make informed choices about their data."  Okay, so more transparency.



Second thing:  "Simplified deletion of location data.  We'll provide a new control that allows users to easily turn off their Location History and Web & App Activity settings and delete their past data in one simple flow.  We'll also continue deleting Location History data for users who have not recently contributed new location data history to their account."  And third:  "Updated the account set-up.  We'll give users setting up new accounts a more detailed explanation of what Web & App Activity is, what information it includes, and how it helps their Google experience."



So they finished:  "Today's settlement is another step along the path of giving more meaningful choices and minimizing data collection while providing more helpful services."  So it seems clear that what was going on during these four years, I mean, lots of back-and-forth, was some negotiation about the things that Google was being asked to do proactively in order to make what they were doing, make this tracking behavior which initially got them into such trouble that these 40 attorneys general decided to gang up and say, look, this needs to change.



So stepping back from this a bit, it must be truer than I guess I'm able to understand that the more information an advertiser has about someone, the more revenue is generated by showing that person advertisements.  I mean, as our listeners know, I've always been somewhat skeptical about that, I mean, that it can mean that much.  But it seems to me that advertisers would not be trying so hard if it didn't really make them more money, since they also know that no one wants to be profiled and tracked across the Internet.  So they wouldn't be risking our wrath to the degree they are if it really wasn't valuable to them.  So anyway, we've got, in a minute, we'll be talking about a different issue with some more attorneys general and the FTC.



I caught wind of a mention that Red Hat had started cryptographically signing its deployment ZIP files.  Okay, now, that made me curious since I'd never heard of ZIP files being cryptographically signed.  We're always talking about executables being signed.  And we know that web assertions of their identity are signed.  But that was new for me for ZIPs.  And with all the problems that we've been seeing with supply chain poisoning, obtaining verifiable assurance of an archive's unmodified authenticity, that would be great.  So a cryptographic signature could do that.



And cryptographic signing makes way more sense than the old-school practice of publishing the hashes of files on the same site where the files are being hosted for download.  Doing that never made any sense to me since, if a bad guy was able to compromise a web server to alter the files being downloaded from that site, what would keep them from also updating the hashes shown at the same site as proof of a file's authenticity?  Talk about a false sense of security.  So anyway, this is a lot better than that.  



So I looked into what was going on, and I found a posting by Red Hat titled "Cryptographic signatures for zip distributions."  I've paraphrased what they posted to remove a lot of their over-simplified descriptions for our audience.  So they wrote:  "Our build system, Brew, produces our RPM and zip distributions and automatically hashes the archives it makes.  The hashes are used to validate that the files have not changed before they're uploaded to our CDN and made available to customers.  We've taken advantage of this aspect of our build process and extended it by combining all of the hashes for a particular release and packaging them into an SHA256SUM file, so S-H-A-2-5-6-S-U-M file.



"This file is in a standard format that lists the hash and the corresponding filename of the particular file."  "Artifact" is the term they use.  "It is commonly used across the industry to provide integrity to binary files.  However, it's not limited to that.  The SHA256SUM command on Red Hat Enterprise Linux, other Linux distributions, and macOS natively support this file format."



They said:  "Once our software production team has completed their verification procedures, they sign off on the release from both a process and technical perspective.  The SHA256SUM file they created is signed by our latest release key, which produces a .asc file. This file is an ASCII-Armor formatted detached signature file that proves the integrity and provenance of the SHA256SUM file and, transitively, the zip file artifacts enumerated within that file.  The GPG command on Red Hat Enterprise Linux, other Linux distributions, and macOS supports the file format natively.  



"Due to the potential damage that a lost or stolen private key could cause, we have taken additional steps to add assurance to the signatures we produce.  The primary technology behind this is our signing server.  To sign these files we use a high-strength, 4,096-bit private key, and our public keys are available on our website and the MIT (Massachusetts Institute of Technology) public key server."



Okay.  So that's what they posted.  Red Hat's mention of a detached signature simply means that the signature itself resides in a separate file.  The signature is just an SHA-256 hash of the file it's signing, which is then encrypted under Red Hat's super-secret, and in this case very long, 4,096-bit private key, which they're careful not to let loose.  Just like my GRC code-signing keys, it probably resides in an HSM, a hardware security module, where it literally cannot be extracted.  It can only be used.  So there's no reason for that signature file not to stand alone, that is, again, it's just - so there's this composite file which contains the hashes and the files that they were hashed from.  That's just a listing, a textual listing, an ASCII file.  That file is then SHA-256 hashed.  That's the file whose integrity you want to verify.



That SHA-256 hash is then signed with their signing server, which is to say that the SHA-256 hash is encrypted with the private key.  So that creates an encrypted blob which is the signature.  And it's a freestanding file.  So somebody who then wants to verify that uses Red Hat's private key, which is available from several sources so you don't have to worry about that being screwed with, in order to decrypt the blob.  That will bring - decrypting that blob restores the SHA-256 hash which you can then use to verify that the file of the hashes that you've got matches and has not been tampered with.



So this is a welcome move as a deterrent to the abuses that we are now seeing and talking about more and more of today's supply chain.  And it's probably where the broader open source community will need to go.  The glitch here, the glitch to doing that, is that Red Hat Enterprise Linux Corporation, you know, Red Hat Corporation, has no problem maintaining a signing server and buying a certificate that asserts their identify.  But the open source world has always had a problem with the need to pay for certificates.



As we know, Let's Encrypt solved this problem by making TLS certificates free for web servers.  But the challenge here is not the same.  Let's Encrypt offers no guarantees about the identity of a site.  It provides domain validation certificates where the only requirement is for the certificate to match the server's domain name.  Specifically, it does not offer, that is, Let's Encrypt does not offer OV (Organization Validation) certificates.  In order to issue OV certificates, any certificate authority must by universal agreement perform some significant reconnaissance to positively verify the identity of the entity requesting the certificate so that the OV-ness means something.  And what's more, of course, many open source projects are just some guy working alone without any organization to be validated.



So maybe the solution will be, for example, to come up with a secure means for submitting repositories to GitHub for its signing with its signature, then using some much stronger means for asserting the identity of the individual requesting the signing service.  For example, that process might require much more rigorous multi-factor authentication.  Something, again, you're really wanting to put it out of the reach of bad guys to get in there and screw this up so that it means something.  So it's a problem that needs to solved.  But one way or another, we need a solution to this current supply-chain pollution problem.  And, you know, the application of a bit of crypto might be a place to start.  So hats off to Red Hat for doing a little pioneering here in that way.



Okay.  Now, the FBI purchased Pegasus, you know, that's the NSO Group's infamous smartphone spyware platform.  They said it was for "research and development purposes."



LEO:  Yeah, what are they developing, I wonder?



STEVE:  Uh-huh, yeah.  Last week The New York Times ran a story with the headline "Internal Documents Show How Close the FBI Came to Deploying Spyware."  Now, I have a little bit different take on this, but we'll get to that in a second.  The New York Times reported that, last December, FBI director Christopher Wray (W-R-A-Y) told Congress, this was closed-door testimony, that the Bureau purchased - "bureau" as in, you know, Federal Bureau of Investigation - the Bureau purchased the infamous Pegasus phone hacking tool for "research and development purposes."  Well, it turns out that FOIA, the U.S. Freedom of Information Act, can be quite handy for figuring out things that really happened.



Here's how the Times explained what they found.  They wrote:  "During a closed-door session with lawmakers last December, Christopher A. Wray" - spelled W-R-A-Y - "the director of the FBI, was asked whether the Bureau had ever purchased and used Pegasus" - so, like, directly asked - "the hacking tool," writes the Times, "that penetrates mobile phones and extracts their contents.  Mr. Wray acknowledged that the FBI had bought a license for Pegasus, but only for research and development, 'to be able to figure out how bad guys could use it, for example,' he told Senator Ron Wyden, according to a transcript of the hearing that was recently declassified.



"But dozens of internal FBI documents and court records tell a different story," writes the Times.  "The documents, produced in response to a Freedom of Information Act lawsuit brought by The New York Times against the Bureau, show that FBI officials made a push in late 2020 and the first half of 2021 to deploy the hacking tools  made by the Israeli spyware firm NSO  in its own criminal investigations."  That is, in the FBI's own criminal investigations.  "The officials developed advanced plans to brief the Bureau's leadership, and drew up guidelines for federal prosecutors about how the FBI's use of hacking tools would need to be disclosed during criminal proceedings."  Like, okay, how did you get this information?  Uh, well, it came to us.  Uh-huh.



So the Times writes:  "It's unclear how the Bureau was contemplating using Pegasus, and whether it was considering hacking the phones of American citizens, foreigners, or both.  In January, the Times revealed that FBI officials had also tested the NSO tool Phantom, a version of Pegasus capable of hacking phones with U.S. numbers.  The FBI eventually decided not to deploy Pegasus in criminal investigations in July of 2021, amid a flurry of stories about how the hacking tool had been abused by governments across the globe.  But the documents offer a glimpse at how the U.S. government, over two presidential administrations, wrestled with the promise and peril of a powerful cyberweapon.  And despite the FBI decision not to use Pegasus, court documents indicate the Bureau remains interested in potentially using spyware in future investigations."



And of course the Times reporting brings up the question of Christopher Wray's apparently misleading testimony in front of Congress.  Senator Ron Wyden is not happy about that.  In a statement from his office, it read:  "It is totally unacceptable for the FBI director to provide misleading testimony about the Bureau's acquisition of powerful hacking tools, and then wait months to give the full story to Congress and the American people."  So the Times revealed in January that the FBI had purchased Pegasus in 2018 and, over the next two years, tested the spyware at a secret facility in New Jersey.  Since the Bureau first purchased the tool, it has paid approximately $5 million to the NSO Group.



Now, it seems to me that the issue with Pegasus is less about its use than its potential for misuse and abuse.  The worry is that, once they have it, repressive governments would be unable to resist the temptation of using it to spy on political rivals - we'll see an example of that here in a moment - and, of course, dissidents and other non-criminal actors.  And of course Pegasus doesn't respect geopolitical boundaries.  So anyone who has it can aim it at anyone else, anywhere.  But in the United States we have a system for obtaining court orders for searching and for making legal, within bounds, what would otherwise be illegal reconnaissance.



So as long as the FBI would only be using Pegasus within our constitutional protections, I think that it would be a useful tool to empower their criminal investigations.  And yes, they would be required to tell a judge that this is what we want to do.  This is how we're going to do it.  And we have probable cause and all the other requirements of getting a court order to pursue things like a wiretap and so forth.  So it seems to me, yes, it is problematical because it can be abused.  But if we're going to have systems that are otherwise not prone to be subject to court order search, then maybe this is the way it happens.



LEO:  Yeah, I mean, we allow wiretaps under court order.



STEVE:  Exactly.  Exactly.



LEO:  Is Pegasus somehow too dangerous to be used?



STEVE:  I think the concern is just - it's control.  All the reports we have suggest that it is a zero-click tool which it is possible to target at an individual smartphone, and it goes in against all of the attempts by Apple and Google, you know, iOS and Android to keep it out.  There are enough ways in that it gets in, and then it's able to provide the entity that deployed it with information the equivalent of someone unlocking their phone and also being eavesdropped on.  It's able to, you know, it is a surveillance tool.



LEO:  I guess the question always is, is it - first of all, it's going to be very expensive.  It's a million-dollar surveillance tool; right.  It's very, very expensive.



STEVE:  Yeah, multi.



LEO:  Multi.  Because it can't be used too often or it loses its usefulness.  Because as soon as the companies find it, they'll defend against it.  So these zero-days are very, very expensive, especially if it's no-click.



STEVE:  And it also might very well be that it is tightly tethered.



LEO:  Well, that's actually, and this may have been the problem, as I understand it, the NSO Group is responsible for the hack.  You don't just give the FBI Pegasus and say, "Have fun with it, guys."



STEVE:  Yes.  



LEO:  It doesn't work that way; right?



STEVE:  Right, right.



LEO:  So that's another problem is that some international company, Israeli company, would then be privy to what you're doing.



STEVE:  Right.  



LEO:  Yeah, that might be a bigger problem.



STEVE:  And other entities may not care, but that may be something that we can't get over.  And in fact, maybe that was the beginning, you know.  In testimony like this there's typically some piece of truth.  So probably, yeah, the FBI said maybe we need to be empowered with this tool.



LEO:  Right.



STEVE:  Because we're unable to get in any other way.  So let's buy a copy, and let's learn how it works.  Let's have the Pegasus experience so that we can decide if this is something that we can sell to the greater government.



LEO:  It's my understanding that what you're buying really is the NSO Group...



STEVE:  Access.



LEO:  They trigger it on - let's say they want to spy on my phone.  The NSO Group gets into my phone, triggers it, and then hands control over to the FBI.



STEVE:  Right.



LEO:  So I'm sure that's illegal in the U.S. because that's an Israeli company, not even a governmental entity, but a business that the FBI says, okay, we want to hack Leo's phone, here's his phone, hack it for us.  It can't be legal.  And you're right, it literally was research.  They just wanted to know, well, how, you know, let's understand it a little bit better.  But I can't imagine the NSO giving the keys to the kingdom to the FBI either.  That's why they do it that way; right?



STEVE:  No.  Yes.  And in fact I have another related story that sort of speaks to that.  Greece, the Athens government, Greece, bought a related program, Predator, for 7 million euros.



LEO:  Wow.



STEVE:  A recent report in the Greek press claimed that Greece's government paid 7 million euros to Intellexa, I-N-T-E-L-L-E-X-A, Intellexa, for access to the Predator surveillance and spyware platform, and an additional $150,000 euros for the ability to rotate 10 new targets per month.  So that says, yes, they were not given carte blanche.  They had to - it is tightly tethered under Intellexa's control.  So this little bit of accounting news follows the massive scandal of the Greek government having been caught using the spyware to go after not only rival political parties, but also journalists and prosecutors investigating government corruption.



So this is the double-edged sword is that it seems to be impossible for governments that purchase this to behave themselves.  Again, I would hope that, if it were made possible for the FBI to acquire this technology, it would be done aboveboard.  It would be done within the constitutional protections of the government.  I'm sure there are those who, you know, Edward Snowden, who don't believe it could be possible.  But we do have - we've set up a situation where the technology that our private citizens and corporations are using is not subject to court orders.  And thus the tension that we're currently under.



So anyway, again, as I said, it seems to me the problem is less about the tool than how it's used.  It is technology.  It already exists, and it's going to exist.  So it makes more sense to me to properly regulate and control its use than to attempt to deny it completely, which, you know, just forces its use underground.



LEO:  And maybe it's old-fashioned, but I also feel like we're the United States.  We should be better than those other guys.  You know?



STEVE:  I agree.



LEO:  You know, we should have higher standards.  Just because other countries use these tools doesn't mean we have to.



STEVE:  Yeah.  I agree.



LEO:  All right, Steve.  On we go.



STEVE:  The password manager 1Password has added support for passkeys to its offering.  And in a nice promotion of passkeys, they've created a community-supported online directory listing online services currently supporting passkey authentication.  I've been waiting for this because I want to play with passkeys.  I've got iOS 16.1.1, I think, now.  And it's supposed to support passkeys, but I've never tried it.  So now we can.



So this directory is at passkeys.directory.  I didn't know "directory" was a TLD.  Really, they've just gotten out of control, Leo.  Is there a .leo?  There probably is.  Anyway, so, again, passkeys.directory takes you to this listing.  It currently has 43 companies listed with their URLs, although some are flagged as MFA so, you know, multifactor authentication, so I suspect that they might not be pure passkeys login.  They may be passkeys plus another factor, which would be annoying.  So anyway, some notable names on the list, which do appear to be pure passkeys authentication without the MFA tag, include a 1Password Passkeys Demo page.  Of all things, Leo, Best Buy.



LEO:  Whoo, about time.



STEVE:  Yeah, it supports passkeys.  Carnival Cruises.



LEO:  Good.



STEVE:  eBay.



LEO:  Good.



STEVE:  Kayak, you know, the travel site.  Microsoft.com.  Again, Nescafe?  Like what?



LEO:  Sure.  Why not?



STEVE:  Nvidia, PayPal, and Robin Hood.  So anyway, I just discovered this as I was putting the podcast together, so I have not made any time to experiment with and explore.  But I am an avid buyer on eBay.



LEO:  Oh.



STEVE:  Often buying, like, old hard drives that I need to make sure that SpinRite works with.  Or in fact I'll be talking about SpinRite in a few minutes here because I actually did just buy four drives from eBay which were specific drives that I needed.  So anyway, I ought to be able to give logging into eBay on passkeys a try.



LEO:  I think I'm seeing it.  Let me log in, and I'll show you.  I'm going to log into - I'll go to Carnival Cruises.  And it says "Create an account."  Here, let me show you this.  I'm going to make it bigger.  And see, that "Login with your phone's Face ID or fingerprint," that's passkeys.



STEVE:  Ohhh.



LEO:  It may not say "passkeys."  Right?



STEVE:  Right.  Right, right, right.



LEO:  So scan this QR code - all right, let me try it - with your phone's camera.  So this is - yeah.  That's cool.  I'm so glad.  This is the first time I've seen it.  All right.  I scanned it with my camera.  I'm logging in.  Enter your email.  Okay.



STEVE:  Ooh, and the site knows, Leo.  Look what it's doing.



LEO:  Oh, it does.  It knew I did something.



STEVE:  Yeah.



LEO:  How would it know that?



STEVE:  Oh, because you're...



LEO:  I'm going to a special URL.  That QR code is...



STEVE:  Yes.



LEO:  Okay.  Connection lost.  Something went wrong.  Try again.  Oh, crud.  Hah.  Well...



STEVE:  Well, after all, it is Carnival Cruises, so...



LEO:  We're working, working on it.  So now what do I do?  Now what do I do?  Do I take another picture?  Let's do it again.  No, I can't.



STEVE:  Do you have an account at Carnival?



LEO:  Not Carnival, no.



STEVE:  Okay.  How about Kayak?



LEO:  Well, I think the idea is you would have to - oh, should I go somewhere I already have an account?  You want to see what that looks like?



STEVE:  I don't know.  I don't know.



LEO:  Let me just - I didn't do it quickly enough, probably.



STEVE:  Let me try.  What happens if I log out of eBay?  If I'm, like, statically...



LEO:  Yeah.  So now I'm pressing Continue.  Oh, here it is.  Do you want to allow Carnival.com to use Face ID?  Continue.  I'm using Face ID.  It worked.  And look at this, on the phone it now says - let's see if I could find that - passwordless sign-in enabled.  



STEVE:  Enabled.



LEO:  Fast login by own ID.  But this is passkeys.  Right?



STEVE:  It's got to be passkeys.



LEO:  Yeah.  So that's cool.  So now it wants, you know, complete your profile, blah blah blah.  But now I presume from  now on I can just use my phone.  I love it.  Yay.



STEVE:  Yes, yes.



LEO:  Now I have a Carnival Cruise Line login.  Not sure I want that.  Oh, actually the cruise line that we do go on is owned by them, so I guess that's - one of the cruise lines we like to go on.



STEVE:  Again, this is the weirdest list.  Like Best Buy.  Carnival Cruises.



LEO:  Don't you think I should trust these people because they're at least on top of it?



STEVE:  Chase is not there.  BofA is not there.



LEO:  No, banks is going to be a higher standard.



STEVE:  You know, Nescafe, but not Starbucks.  It's like, okay.  I don't know what's going on.  But anyway...



LEO:  I think it's going to be lower stakes companies, don't you think, initially?  A bank, that's going to be problematic, probably.



STEVE:  Yeah, I guess Microsoft has become lower stakes; you're right.



LEO:  Would I like to receive emails?  No.  Do I accept their terms and conditions?  Yes.  Have you already booked a cruise?  No.  Okay.  Now I guess the next time I go there - let's go on another computer.  This is the first time I've ever used this.  It's cool.  So now I'm going to see "Login," and it's going to say "Login in with your phone's Face ID or fingerprint."  I'm going to click that.  Scan.  Oh, I have to scan it again.  Is that right?  Is that what it should be doing?



STEVE:  Yes, yes, because you don't have - you haven't transferred your passkey into that computer.



LEO:  And then it says do you want to login using a saved account?  Yes.  Logging in.  Bingo.  "Hi, Leo.  When's your cruise?"  It works.  It's a little onerous.  So will I always have to scan my QR code to get in?



STEVE:  Well, so what you're doing is you're using your phone's passkey in order to authenticate across to a different device.



LEO:  Right.



STEVE:  And this was the problem that I talked about is that SQRL would - there would only be one.  But so you need to create another passkey in your laptop.  And so there is, there should be a way to - you can't export the passkey, but you can link them.  You can create another passkey and then link them so that they're identified as the same.



LEO:  Yeah, see, I don't see - I already have a Microsoft account, but I don't see any way to log in, if you haven't set up passkeys, with passkey; right?  I'm just going to the Microsoft site.  Now, I do have an account, and I could sign in, but I'm going to say could I do this with my passkey.  No.  But maybe if I go into my account I could set that up.



STEVE:  What's that little thing down at the bottom?



LEO:  Sign-in options, but that's - I already looked at that, and it just gives me GitHub or forgot my username.  That's not passkey.  I bet you I have to go to the Microsoft account, log in normally, and then say "and I would like to establish passkeys with this account."



STEVE:  Probably, yeah.



LEO:  That would make sense.  I'll try it while you're talking.



STEVE:  Anyway, all of our listeners now, again, passkeys.directory.  You can check back there, and maybe eventually some more interesting sites will be available.



LEO:  I think it's hysterical that Robin Hood is using it.



STEVE:  Yeah, yeah.



LEO:  I bet FTX would have.



STEVE:  Go to passkeys.directory and see what it...



LEO:  Oh, there's more than just this.  Okay, yeah, yeah, yeah.



STEVE:  Oh, yeah, yeah, because there was a bunch of things that also had MFA tags for some reason.  So you could see that the little green dots are just...



LEO:  Sign-in.



STEVE:  Sign-in.



LEO:  And here's Cloudflare MFA.



STEVE:  Yeah.  So I didn't know what that meant.



LEO:  It probably means I need a password to log in and passkeys.  Like it's two-factor.  Yeah?



STEVE:  That's what I'm thinking, too.



LEO:  Use DocuSign.  I can sign in, or I can - interesting.  Well, I have a GitHub account.  Let me play with that a little bit and see. 



STEVE:  Ah, okay, cool.



LEO:  Yeah.



STEVE:  In other news - again, passkeys.directory, our listeners.  Okay.  So from the Having Fun with Bureaucracy department comes an edict from the OMB.  The U.S. Office of Management and Budget has ordered federal agencies to scan their systems - oh, yes, scan those puppies carefully.



LEO:  Scan them, man.



STEVE:  Scan them, and provide an inventory of assets containing cryptographic systems that could be cracked by quantum computers in the coming years.



LEO:  Just how would you know?



STEVE:  Well, Leo.  Okay.  First of all, there is probably not a single computer in the government that doesn't use and depend upon some public key crypto.  And none of the currently deployed public key crypto...



LEO:  There's no way.



STEVE:  ...is quantum resistant.



LEO:  Yeah.



STEVE:  So the OMB could have simply said, give us a list of all  your computers.



LEO:  That's a good point.



STEVE:  And by the way, stop using them.



LEO:  Yeah.



STEVE:  Okay.  So the next point worth noting is just a reminder that no one has come near to building a quantum computer anywhere, so far as anyone knows, that could even begin to think about breaking actual public key crypto.  Oh, yes, factoring the number 27, we can do that.  It's magic.  But the number 35?  Uh, we're not quite there yet.  Give us another 10 years or so, and we'll be able to factor 35.



Okay.  Now, that said, I'm on the record agreeing that there's absolutely no reason not to move us to quantum-safe crypto sooner rather than later.  You know, let's not wait till we need it because we know how slow and painful these moves can be.  So just as soon as we are absolutely sure that we're not going to be making a big mistake because that's possible.  Remember that one of the candidates that had already been chosen, already selected, was recently cracked by conventional computers.  So it would be a lot better for us to stay where we are, where we know we can't crack today the algorithms we're using, before moving prematurely to something that we presume some future non-existent mythical quantum computer should also be unable to crack.



So the OMB edict stated that federal agencies had until May 4th, 2023.  So like this coming May 4th.  I don't know why May 4th, but that's it.  And the NSA ordered that all government agencies handling classified information must use quantum-resistant encryption by 2035.  Okay.  So that's 13 years from now.  By then we ought be up to factoring 45.  So good to be switching over to quantum computers, you know...



LEO:  Any minute now.



STEVE:  Before we need them, yeah.  Okay.  So this other piece of attorneys general news that I wanted to share, one of the developing themes of this podcast is the observation that we're still in the Wild West stage of the creation of the Internet.  It remains an unregulated or only very loosely regulated medium.  And of course globally it's an uncoordinated total disaster.  The idea that we've linked our fundamentally insecure networks to those of openly hostile nations should give anyone pause.  Yet that's what we've done.



Chinese, Russian, and Iranian cybercriminals under the protection of their nation states, who have no love for the U.S., are able to openly attack the networks of U.S. corporations and its private citizens.  And yes, there's reciprocity.  You know, the U.S. is able to do the same to them, and presumably that's happening, too, although there seems to be a surprising lack of information about that.  You know, but reciprocity doesn't make any of this sane.  It's like, you know, mutually assured destruction.  So we can only hope that the Internet our grandchildren will use as adults 30 years from now will be much different from the one we've been watching being born through these past 30 years.



I bring this up because various democracies around the world, notably the EU and the U.S., among others, are inching forward cautiously in an attempt to provide their citizens with some legally enforceable rights to privacy and personal information.  At the moment, we have clear statutes outlawing overt network intrusion and attack.  And when those laws are broken, people lose their freedom for doing so.  But nothing yet prevents or regulates the passive collection of as much Internet user data as possible.  Google was sued by those 40 states attorneys general, not for tracking, but for tracking after they said they weren't.  As long as a company doesn't say that they won't do something, they can do pretty much anything they want.



So how do we get this to change?  Here's a hopeful example:  Last Thursday a coalition of 33 state attorneys general co-signed a letter formally urging the U.S. Federal Trade Commission, our FTC, to pass legislation which would regulate online data collection practices.  Might not happen, but it's a good start.  These AGs said they are "concerned about the alarming amount of sensitive consumer data that is amassed, manipulated, and monetized."  And they also said that they regularly receive inquiries from consumers in their states about how their own data is being hoarded and abused.



Okay.  Since we've still got a bit of time, and I think this is extremely important, I'm going to first share just the introduction in the letter which was submitted to the FTC and signed.  It's really pretty, like different colors of ink on the signatures.  I don't know how they pulled this off.  But it was like, you know, signed by 40 states attorneys general.



So in the beginning of this letter they said:  "We, the Attorneys General of Massachusetts" - I'm not going to read them all because they didn't list them all, but they did some - "Massachusetts, Connecticut, Illinois, New Jersey, North Carolina, and Oregon, joined by the respective Attorneys General of the undersigned states, write to the Federal Trade Commission in response to the August 22, 2022 Advanced Notice of Proposed Rulemaking on Commercial Surveillance and Data Security."  So this was something that the FTC put out there and asked for comments.  So that was an Advanced Notice of Proposed Rulemaking on Commercial Surveillance and Data Security.  That all sounds great.



So they said:  "As the chief consumer protection officials in most of our respective states, we hope to inform the Commission as it contemplates new trade regulation rules governing commercial surveillance and data security.  The State Attorneys General commend the FTC for its comprehensive review of corporate surveillance and data security in preparing the Notice.  We, too, are concerned about the alarming amount of sensitive consumer data that is amassed, manipulated, and monetized.  Our offices frequently receive outreach from consumers concerned about the privacy and security of their information.  Research supports that consumers are worried about commercial surveillance and feel powerless to address it."



LEO:  Oh, really.



STEVE:  Imagine that.



LEO:  That is interesting.



STEVE:  We're just going on the record here.  "Many consumers believe that tracking by companies is inevitable, yet often do not even know what is being recorded.  These fears intensify when they learn more about the commercial surveillance economy, and in particular consumers fear falling victim to identity theft and data misuse.  A majority doubt that their data can be kept secure.  Contributing to these concerns is the fact that companies are often collecting more data than they can effectively manage or need to perform their services.



"Our consumer privacy-related enforcement actions and investigations have resulted in settlements" - like Google - "that have provided significant business practice changes to strengthen data security and privacy going forward, but there is still more work to be done.  Our submission highlights the heightened sensitivity of certain categories of consumer information, the dilemma of data brokers and how they surveil consumers, and how data minimization can help mitigate concerns surrounding data aggregation."



Okay, then the letter goes on at quite some length detailing five general categories of abuse.  Unfortunately, in an effort to be very clear and to drive their points home, that part is too long to share.  But I found a separate release about this action from New Mexico's Attorney General Hector Balderas.  It addressed each of these five points by reference quite succinctly, so those I want to share because it's good stuff.



So first, so there's five categories.  Location data, he said, or his office said:  "According to the letter, many consumers are not even aware that their location information is being collected; and, when a consumer wishes to disable location sharing, their options are quite limited.  The attorneys general recognize the sensitive nature of this information, which can reveal intimate details of daily life such as where they live and work, their shopping habits, their daily schedule, or whether they visited the doctor or pharmacy.  Laws passed in states like California, Connecticut, and Virginia that restrict the use and collection of location data can provide a framework to inform the FTC through the rulemaking process."  So this is him saying, or his office saying, for location data things, look at what California, Connecticut, and Virginia have done.  Use that, you know, consider using that as a framework.



Biometric data:  "The coalition urges the FTC to consider the risks of commercial surveillance practices that use or facilitate the use of facial recognition, fingerprinting, or other biometric technologies.  Many consumers provide this information to companies for security purposes or to learn about their ancestry.  But consumers are not always made aware of when their data is collected, how it is used, or if it is resold for purposes to which they never meaningfully consented."



Medical data:  "The FTC should also consider the risks of practices that use medical data, regardless of whether the data is subject to the Health Insurance Portability and Accountability Act of 1996" - popularly known as HIPAA - "and the Privacy Rule.  Medical data not necessarily covered by HIPAA is referred to as 'health adjacent data,' which can be collected by many devices, for instance, smartwatches, heart monitors, sleep monitors, and health or wellness phone applications.  The letter also highlights medical information risks through examples such as the storage of health-related Internet searches, or appointment scheduling information being passed to others through online tracker tools."  In other words, you get a sense for how comprehensive this letter was that the 40 states attorneys general submitted to the FTC.



Two more to go.  Data brokers:  "The attorneys general reiterated to the FTC the persistent dangers of data brokers.  Data brokers profile consumers by scouring social media profiles, Internet browsing history, purchase history, credit card information, and government records like drivers' licenses, census data, birth certificates, marriage licenses, and voter registration information.  Data brokers use this information to create profiles of certain consumers which can be purchased by almost anyone based on susceptibility to certain advertising or likelihood to buy certain products.  This scale of aggregation of anonymously gathered information can identify consumers and put consumers at risk of scams, unwanted and persistent advertising, identity theft, and lack of consumer trust in the websites they visit."



And lastly, data minimization:  "The attorneys general say that it is vital that the FTC consider data minimization requirements and limitations.  With respect to data collection and retention, the letter encourages the FTC to examine the approach taken in California, Colorado, Connecticut, Utah, and Virginia consumer privacy laws which mandate that businesses tie and limit the collection of personal data to what is 'reasonably necessary' in relation to specified purposes.  Limiting the collection and retention of data by businesses will improve consumer data security as businesses will have less data to protect and less data potentially available to bad actors."



Okay.  So I think, if nothing else, this is a useful start.  In the United States, where we exalt capitalism, no one wants to strangle innovation.  But we all know that we're a long way from being in danger of that.  Much of what is going on today is only able to happen under the cover of darkness, because consumers are blissfully unaware.  You know?  What did Apple discover when they started requiring their apps to proactively obtain cross-application tracking permission?  They found that nearly everyone who was asked, declined.  No thanks, and no surprise.  So we can expect any improvements to be slow going.  As I always say, change is slow.  But the pressure is there, and it's not going to go away.  At least I think we're moving in the right direction.  And 40 states getting behind this, you know, one wonders why it's not 50.  Well, who knows?  Some presumably buckled to some pressure.



Okay.  Some closing-the-loop things that I think are interesting.  Vincent Stacey shot me a note that I wanted to share regarding - we were talking about the concern that was raised by a different listener about the ZimaBoard and how, when he changed its credentials, it was only away from the logon of casaos/casaos, it was only for the web portal logon, and all of the other credentials remained the same.  He was concerned that the lack of changing of other credentials was unknown to ZimaBoard users, and that they might get themselves in trouble, for example, if they turned this thing into a router.



Anyway, Vincent Stacey tweeted:  "Hi, Steve.  pfSense installs its own version of Linux and won't have the default users of another distribution."  And that's a very good point for anyone who's interested in using a ZimaBoard as a pfSense router, though just for the record it's actually FreeBSD Unix that pfSense runs on top of and brings along with it.  But the main reason why a ZimaBoard would not be my first choice as a router is that, unless a network expansion board were to be plugged into its PCIe x4 slot, it only has a pair of LAN NICS built in, and I would expect a router today, certainly one that any of our listeners would be using, to have a few more network interface controllers, a few more NICs for implementing useful multi-network isolation.  So I can't see it being really popular as a router.  There are some better fanless solutions like that, what is it, the SG-1000, I think, that I've talked about before.



Charles Turner tweeted:  "As possible fodder for a Listener Feedback section in a future episode of Security Now! podcast, I have a question arising from the discussion you and Leo had on Tuesday" - okay, he says November 15th, that was last Tuesday - "during Security Now! Episode 897, Memory-Safe Languages."  Yup, last podcast.  He says:  "With the future of Twitter in doubt, what is your prediction on the long-range fate of Mastodon?  The cynical part of me gives Twitter a 50/50 chance of either, A, rebounding back to its former glory and beyond; or, B, becoming a $44 billion version next iteration of MySpace and FTX."



Okay.  So it's clear to us all that Twitter is currently in turmoil.  And I don't have any firsthand sense for just how fragile Twitter's technology is internally.  And it seems to me that matters a lot.  If the previous regime engineered really solid bulletproof systems, then it ought to be able to withstand Elon's shaking of its foundation.  But overall I'm a big believer in inertia and in things generally changing much more slowly than we expect.  Now, of course, Elon could trip over the main power cord, and Twitter could go dark until someone plugged it back in.  And I suppose I'm interested in what Elon is doing there; you know?  He's an interesting character, and somehow he's managed to get other people in the past, at least, to do some truly amazing things.  I'll never forget the sight of those twin booster rockets returning to and landing on that floating platform for reuse.



LEO:  Oh, yeah.  That was something, yeah.



STEVE:  That was truly astonishing technology.



LEO:  Inspiring, yeah.



STEVE:  And it's Elon's SpaceX Starlink technology, which actually works, that's enabling Ukraine to survive Russia's increasingly aggressive attacks against its infrastructure.  Again, thanks, Elon.  Mostly, though, my take is that I think Elon is just having fun with his life, as is his right.  Right?  You know, I hope he's having fun.



LEO:  Expensive fun.



STEVE:  You know?



LEO:  But what about our lives?



STEVE:  He doesn't care.



LEO:  He doesn't care, no.



STEVE:  No, he doesn't.



LEO:  He thinks we're simulations, that's why.



STEVE:  Because it's his life.  And he's not a guy who likes to make small waves.  Elon's waves are big.  And let's not forget that Twitter made him do it.  They insisted that he honor his wildly overpriced purchase offer.  He didn't want to buy Twitter.  They made him buy it.  So it seems to me that Twitter is getting what it deserves:  the Elon treatment.  He's showing them that he can do anything he wants to with it.



LEO:  Yeah.



STEVE:  So all of this made me curious about what he is doing with it.  You know, I pick up little bits here and there, but I don't follow news feeds, or even Twitter, because they interrupt my work and my train of thought.  So it was with some joy that I stumbled upon a site, which I figured had to exist somewhere.  The site's called TwitterIsGoingGreat.com.



LEO:  In the spirit of Molly White, yup.



STEVE:  And yes, of course, it's offering up its share of schadenfreude.  So keep in mind that it's naturally going to biased.  But it's still a lot of fun.  The site hosts a simple timeline of Twitter's Elon-related happenings.  So now I can check in from time to time whenever I want to get a sense for what's going on over there.  I mention it because I imagine that some of our listeners would also appreciate knowing about this nicely distilled timeline event resource.



LEO:  It's hysterical because it's all tweets.



STEVE:  Yes.



LEO:  I guess that's a best source of what's going on at Twitter, I guess, yeah.



STEVE:  TwitterIsGoingGreat.com.



LEO:  I'll show you another one that you should read.



STEVE:  Okay.



LEO:  This is from a Twitter reliability, site reliability engineer.  I think former.  Matthew Tejo, he's on Substack.  And I think you would enjoy this.  I barely understood it, but he talks about all of the redundancies, all of the automation.  He says:  "When I came in, the list of servers was on a spreadsheet.  Now, of course, it's a much better system."  And he did a really good job.  It sounds like he and his team did a really good job of making it run.  He was in charge of the cache, the cache team, which is a pretty big deal because everything you're getting is served from cache.  None of it's served from the source.



STEVE:  Leo, I didn't want to interrupt you, but has anyone stopped to think about what it does?



LEO:  Oh, it's phenomenal, yeah.



STEVE:  It is un-be-frigging believable.  



LEO:  This is just a fraction of it.



STEVE:  What Twitter actually does.



LEO:  Yeah, yeah.



STEVE:  I can't imagine building this system.



LEO:  Oh, yeah.



STEVE:  It just astonishes me.



LEO:  Well, read this.  I think you'd enjoy it.  And it's just a fraction of what is going on.  And but his point is these things are designed to run unattended.  We automated everything we could.  And so it should, unless something, you know, nobody's going to kick the plug out of the socket.  I hope there's more than one plug.  But it could have a...



STEVE:  You piss off Elon, he might...



LEO:  Well, that might - yeah.



STEVE:  He just might pull the plug.



LEO:  So, but you wouldn't expect it to all fail all of a sudden.  There may be bugs here and there and stuff.  And the real problem is there may not be somebody to solve that problem which cascades to another one, et cetera.  I've read a number of articles.  We had Phil Libin on, who was the founder of Evernote.



STEVE:  Very, very well rounded.  I was very impressed with Sunday's...



LEO:  He's a smart guy.  And he was saying, you know, give Elon, as you do, give Elon some credit.  There was a good article by a former Tesla engineer that says Elon did exactly the same thing in 2018 to Tesla.  He was firing people.  He was spending the nights there.  He was bemoaning they might be bankrupt.  This was all in the lead-up to the Type 3, the Model 3 of the Tesla.  They said this is kind of how Elon works.  Obviously, for some people, not the ideal situation.  That's why so many have left Twitter voluntarily, as well as involuntarily.  But I've also read articles that say, you know, this is how he - he's reinventing Twitter.  You have to get rid of almost everybody and then build a team of people who believe in your vision.  He hasn't really communicated that, apparently, but who believe in your vision first.



STEVE:  And so he's still making it up.  He's making it up as he goes along.



LEO:  Nobody, you know, I don't - I'm confused.  I see stuff that looks crazy.  He says we're going to have a committee to approve who comes and goes.  And then he just says, no, I'm going to bring them back.  And, you know, it's just - it seems chaotic.



STEVE:  There was one piece there that said he sat down and explained to the core team how advertising should be tweets.  And they said, uh, they are, Elon.



LEO:  He said native, yeah, they should be native.  It is, yeah, that's exactly my problem with the advertising.  You know, so he's coming somewhat from ignorance.  But you're right, he's also a pretty interesting, apparently...



STEVE:  Probably sleeping there.



LEO:  He says he is.



STEVE:  He's there, you know, 24 hours a day.  And, you know, he'll figure this thing out.



LEO:  He's a weirdo.  And some of the things he's tweeted, I'm not thrilled about some of the pictures and stuff.  But this is from 1:20 a.m. at Twitter he - this is when he, you know, came, had everybody come in Saturday, or Friday night and Saturday morning, to explain how Twitter works.  And these are the skeleton crew.  There he is sitting with them.  But this is his picture of what they drew on the whiteboard.  This is not a code review.  This is explaining in rudimentary fashion to somebody who doesn't know how this stuff works, how it's working.  I get, you know what, we don't know yet.  He may - this may be Twitter 2.0 he's inventing.  And maybe this is how he works.  I would never want to work for him.  But people will, and we'll see what happens.



STEVE:  There was an interesting moment.  I was watching a press conference when Biden was off in the East.  And it was that awkward press conference where he meant to say "Cambodia," and he said "Colombia" three times.  It's like, oh, Joe.



LEO:  Oh, Joe.



STEVE:  But someone in the press pool asked him about Elon.  And so understand that our relationship, the government has a relationship, right, with Elon because he's now SpaceX, and we've got all these contracts. 



LEO:  Right.



STEVE:  So Biden just locked up.  He didn't know what to say because it's like, oh, you know, I don't dare piss off Elon, or we're going to be in real - we're not going to have any missile launches.



LEO:  He says we're looking into it, though.  You know, it's very complicated because Elon has relationships with not just the U.S. government, but many other governments.  Tesla sells a lot of cars and builds them in China.  It's a complicated system.  And he's kind of a bull in a china shop, but we'll see.



STEVE:  I just think he's a - he's a character.



LEO:  It's fascinating.



STEVE:  And I think he's having fun with his life.



LEO:  Yeah.



STEVE:  And, you know?  And we're all just sort of observers.



LEO:  Too bad, though, because Twitter is a valuable resource.  It's not a public resource.  It's not even a publicly held company anymore.  And he's...



STEVE:  It's incredibly valuable.



LEO:  But it's a shame if he crashes it; you know?



STEVE:  And that's why I think - well, I mean, he's let a bunch of loons back on recently.  And, you know, but I don't ever see tweets from loons.  I have a very quiet experience with Twitter.  I just talk to our listeners.



LEO:  Right.



STEVE:  And they talk to me.  And it's just a great little channel.  So I don't care who says that vaccines are garbage.  Who cares?



Okay.  Leslie MacFarland said:  "Hi, Steve."  Uh-oh.  "If Twitter implodes, are you going to Mastodon or somewhere else?  Your Security Now! podcast is top-notch security and quality."  Well, thank you, Leslie.



So, okay.  In order to get the word out to 18 years' worth of SpinRite owners, I will shortly, and I mentioned this before on the podcast, be setting up an old-school email facility.  One of the several lists that I'll be maintaining will be for Security Now! listeners who would like to subscribe to the weekly links and the show notes and a description of each week's podcast, which I post to Twitter.  And it'll be nice to have more than 280 characters for that.  So that will be a possibility.  And, you know, as for Mastodon, I have no idea.  I'm not...



LEO:  Folks, remember, it took me 10 years to get Steve on Twitter.



STEVE:  Right.



LEO:  Patience.



STEVE:  Thank you, Leo.



LEO:  We'll get him.



STEVE:  I'm really not looking - I'm not looking for more connectivity.  We'll see how Twitter goes.  As it is, I spend most of my time in GRC's quiet newsgroups.



LEO:  There you go.  That's the best place.



STEVE:  Getting actual work done.  And now we have GitLab for managing SpinRite bugs and feature requests.  And I have GRC's web forums, which will soon be quite active since that's where SpinRite's tech support will be hosted.  And a lot of new users are going to be using SpinRite 6.1 and have questions.  Or maybe not because it's pretty much the same as it was, it just works a lot better.  So anyway, I just don't have any additional bandwidth available for new conversation opportunities.  I doubt that Twitter can actually implode.  It's, as you said, Leo, it's too big and too important.  You know, I doubt that even Elon can or will kill it.  You know, and I have an alternative means for communicating my and GRC's events to anyone who cares through good old email.



LEO:  And I will extend this offer after Episode 999.  You can always use us to tell the world.  I would bet a lot of SpinRite users and owners listen to various other things we do.  And we have a lot of different channels, including Twitter channels.



STEVE:  And Leo, we still have two years.  Who knows, two years from now, what'll be going on.



Okay.  Someone said, where did his name go?  I didn't have his name here.  Shoot, I think it was Walt.  Anyway, he said:  "Steve, did you see there's a 'Project Hail Mary' in IMDB?"  He said:  "Crossing my fingers."  Anyway, indeed there is.  A "Project Hail Mary" movie is in the works.



LEO:  Well.



STEVE:  It is currently flagged as in "in development."



LEO:  Well, if you had listened to our interview with Andy Weir some months ago when it came out, he had already optioned it.



STEVE:  Wow.



LEO:  And he told me, and I wasn't too thrilled, I don't know how well I hid my discomfort, that Ryan Gosling had signed on for the lead.



STEVE:  Yes, I saw that, too.  I saw that, yeah.



LEO:  And I went, oh.  Okay, yeah.  But yeah, we're going to, you know, Andy was going to be on some months ago, but he'd just had a baby.  We'll get him back on.  And by the way, Daniel Suarez has a new book.  The sequel to his "Delta" book is coming out soon, I think next January.



STEVE:  We had a lot of fun reading those.



LEO:  Those were great.



STEVE:  Yeah.



LEO:  So we'll get him on, too.  So, yeah, we'll keep an eye.  I'll have Andy on long before a movie gets made.  We'll get the latest on that one.



STEVE:  Okay.  So speaking of books we've loved.  So many people have written to me, telling me that they're loving the Silver Ships series, that I want to share a tweet I received two days ago from the first person I know, or we know, who has finished the entire 24-book series.  I was horrified, as I started to read the tweet, that he might have written something of a spoiler, but that concern was misplaced.



So here's the content of the DM that Bob Grant sent.  He wrote:  "Wow, wow, wow.  Superb ending to the series.  There was enough great writing and new intrigue in the first part of this final book in the Silver Ships series to be a great book in and of itself.  However, the wrapping up of all the various storylines from the previous 23 books," and he says, "(20 Silver Ships and the related four Pyrean books) at the end was superb.  There were joyful and poignant endings to each of the major characters from the books.  I have to say that this is the best series I've ever read.  Not to take away from Weber's Honorverse" - and of course he's talking about David Weber's Honor Harrington series that was one of the early series that we talked about on this podcast - "or Ryk Brown's Frontiers series," he says, "both of which I've enjoyed.  But these 24 books have been a joy to read from beginning to end."



And then he said:  "After a little break to catch up on some other reading, I plan to start the new Scott Jucha series called 'Gate Ghosts' whose first book is 'Axis Crossing.'"  And as I mentioned to you, Leo, there are six more in that series after these 24.  So anyway, obviously Bob has been following along with my previous reading discoveries.  He knows of and read David Weber's Honorverse series and Ryk Brown's work in progress, Frontiers Saga series.  And for what it's worth I'm in complete agreement with him about this being the best series I've ever read.  I'm at the start now of book 19 of those 24 so I have six to go.  And having already made this large investment in this series, I'm delighted to learn in advance that it ends wonderfully.  So anyway.



One last piece.  Simon, he said:  "Hi, Steve.  Persistence paid off.  I was able to disable one-time code 'feature,'" he has in quotes.  He's talking about PayPal.  He said:  "You can call PayPal and ask to 'unconfirm' your phone number.  It may impact use of the PayPal app.  But as long as you do not confirm phone number, it will not text security codes."  So that's very cool.



LEO:  Wait a minute.  Which is less secure?  Having no two-factor or having SMS two-factor?



STEVE:  Oh.  No, no, no.  You can still use...



LEO:  Oh, you still have Authenticator or a YubiKey.



STEVE:  Yes.



LEO:  Oh, oh, oh.



STEVE:  You still - yes, yes, yes.



LEO:  No, I did that on Twitter, too.  You had to have SMS to enable 2FA on Twitter.  But once you set up a key or an Authenticator, you could then disable it.  So you're saying you did the same on PayPal.



STEVE:  Yes, although there is no UI for doing it.



LEO:  Oh, interesting.



STEVE:  You need to contact them.  You have to contact them and say please unconfirm the phone number.  And that makes sense, right, because the phone number can go to somebody else.



LEO:  I don't have it anymore, right.



STEVE:  Right.  Anyway, it was Simon who originally noticed and communicated that it was always possible to cause PayPal to send an SMS code for account/password recovery.  However, I should note, someone else sent me a note, and I apologize to that person for letting his name slip, but he sent me a note that if users set up their own personal account recovery questions, you know, those like who was your favorite high school teacher and what was the name of your first dog or whatever, if you set those up, they cannot be bypassed.



LEO:  Ahhh.



STEVE:  So that's another solution:  Deliberately choose impossible to guess, no matter how well someone knows you, account recovery questions, and assuming that that information is correctly provided, then you will be safe from hijacking because nobody else will know what it was that you set up.



LEO:  It's just like three more passwords, basically.



STEVE:  Yeah, yeah.



LEO:  Yeah.



STEVE:  Okay.  Finally, I mentioned last week that I thought SpinRite's new AHCI driver was not working correctly.  I was wrong about that.  It was working correctly.  It was the location in my code where I was taking the hash of SpinRite's results that was causing a false positive detection.  So I found and fixed that and made some other final improvements.  Then, as planned, I updated GRC's server to get it ready to manage all subsequent downloads of the pre-release testing versions of SpinRite that will be forthcoming.  That work is finished, and the server has been restarted and is now standing by to make SpinRite available.



I have one final feature to add which came up about 10 days ago.  SpinRite 6.1 has four levels, or degrees, of its operation.  The first level never performs any writing to a drive under any circumstances.  It's strictly read-only.  I'm not sure why, but it always seemed like it ought to offer that, so it always has.  The second level is allowed to perform data recovery, so it will selectively rewrite only those regions of the media that are in need of repair.  Level three goes further.  Since refreshing any drive's data is generally good for it, and that's because latent and evolving soft errors are completely hidden by all modern drives, level three always rewrites the drive's data as it's moving through the drive.  And level four goes even further, writing inverted data, reading it back to verify it, then rewriting the original data and reading it back to make sure that it was written correctly.



Okay.  I mention this because there are three classes of drives that I refer to as being "write-hostile," and should only be used under SpinRite's first two "read mostly" levels.  Those drives are SSDs whose media we know is incrementally fatigued by writing to it, hybrid drives which incorporate an SSD on their front end to serve as a non-volatile cache, and SMR drives where SMR stands for Shingled Magnetic Recording.  Shingling, exactly like it sounds, refers to the deliberate overlapping of adjacent tracks in order to push track density to insane levels.  If you picture a shingled roof, you cannot change an embedded shingle without pulling up the shingle above it, and then the shingle above that one, and the shingle above that one, and so on.  The same is true for SMR drives, which makes writing to them something you want to do as little as possible.



As I mentioned, this issue just came up in SpinRite's newsgroup discussion a couple of weeks ago.  Since I want SpinRite to continue doing everything possible for its user, in this case warning them if they are about to perform a level 3 or 4 scan on any drive which should not be written to needlessly, I need to be able to detect that.  But I didn't own any hybrid or SMR drives.  So I immediately tracked some down on eBay, and those four drives have all arrived.  The last two just came in  yesterday's mail.  So after today's podcast I'll be adding detection of those drive technologies to SpinRite so that it can take responsibility for warning its users if they're about to do something that they probably don't want to do.



And then, with that last bit of technology in place, as far as I know, SpinRite 6.1 will be ready to start its final stage of pre-release testing.  And as for that I'm absolutely certain there are things I've missed, things I just can't see because I'm their author.  But that's why we test.  What I am confident of is that at this point so much testing has already been done, by far the bulk of the work, that there are no showstoppers remaining.  It should be a matter of cleaning up debris.  So by next week's podcast it will have been under test for - I'm hoping that this is a Thanksgiving present for our testers.  So I should have a good calibration on where we stand.



LEO:  Nice.  Incidentally, "Project Hail Mary" is the Book of the Month for Stacey's book club in January.  If you have read it or want to read it, that's a good book to read, and discussion in the club.



STEVE:  And we all read it and loved it.  It was great.



LEO:  Oh, it's a great book.



STEVE:  Yeah.



LEO:  And if you can listen to the audiobook, there's some features the audiobook has that the written page cannot that makes it kind of fun, too.  Anyway, it's good either way.



Now, whatever it is, I want to know, what is Wi-Peep?  Little Wi-Peep.



STEVE:  Okay.  Little Wi-Peep.  So imagine a technology that allows someone walking past a multistory building or a drone flyby to accurately locate and pinpoint within that building or any other similar space, closed or open, with a positional accuracy of about a meter, the location of every WiFi device such as security cameras, and locks and switches, and anything else on WiFi.  That capability, which jumps off the pages of science fiction movie scripts, is not only here now, but it costs about $20.  The two researchers who figured out how to make this WiFi mapping technology real named it Wi-Peep.  They presented their research during the recent ACM MobiCom '22 which was held last month, in October, in Sydney, Australia.



Here's how they described what they accomplished.  They said:  "We present Wi-Peep, a new location-revealing privacy attack on non-cooperative WiFi devices.  Wi-Peep exploits loopholes in the 802.11 protocol to elicit responses from WiFi devices on a network that we do not have access to.  It then uses a novel time-of-flight measurement scheme to locate these devices.  Wi-Peep works without any hardware or software modifications on target devices and without requiring access to the physical space that they're deployed within.  Therefore, a pedestrian or a drone that carries a Wi-Peep device can estimate the location of every WiFi device in a building.



"Our Wi-Peep design costs $20 and weighs less than 10 grams.  We deploy it on a lightweight drone and show that a drone flying over a house can estimate the location of WiFi devices across multiple floors to meter-level accuracy.  Finally, we investigate different mitigation techniques to secure future WiFi devices against such attacks."



Okay.  So this has never been done before.  The key components here are the non-cooperative nature and the fact that this is being done by a probe which is not on the WiFi network.  So they set this up and framed the problem, explaining the problems they encountered and how each such problem was solved.



They said:  "We live in an era of WiFi-connected TVs, refrigerators, security cameras, and smart sensors.  We carry personal devices like smartwatches, smartphones, tablets, and laptops.  Due to the deep penetration of WiFi devices into our lives, location privacy of these devices is an important and challenging objective.  Imagine a drone that flies over your home and detects the location of all your WiFi devices.  It could infer the location of home occupants, security cameras, and even home intrusion sensors.



"A burglar could use this information to locate valuable items like laptops and identify ideal opportunities when people are either not at home or away from a specific area, for example, everyone is in the basement, by tracking their smartphones or smartwatches.  The promise of pervasive connectivity has been to merge our physical and digital worlds, but the leakage of such location information brings arguably the worst aspect of the digital world, pervasive tracking, into the physical world.



"In this paper, we show that there are fundamental aspects of the WiFi IEEE 802.11 protocol that leak such location information to a potential attacker.  We demonstrate that it is possible to reveal accurate location of all WiFi devices in an indoor environment, A, non-cooperatively, without any coordination with WiFi devices or the access points; B, instantaneously, without waiting for devices to organically transmit packets; and, C, surreptitiously, without any complex infrastructure deployment in the surrounding.  Our goal is to expose the security and privacy vulnerabilities of the 802.11 WiFi protocol by demonstrating a first-of-its-kind non-cooperative localization capability.  We hope that our work will inform the design of next-generation protocols."



So they said:  "We note that there's been much past work in WiFi-based positioning.  However, such past work does not enable non-cooperative, surreptitious localization of WiFi devices.  First, most of this work relies on cooperation from end devices, for example, the client needs to switch channels or physically move or share inertial sensor data.  Second, state-of-the-art techniques, such as ArrayTrack, rely on antenna arrays with multiple antennas, that are typically bulky and cannot be easily carried by a person or a small drone.  Deploying multiple such antenna arrays near a target building makes the attack less practical and easier to detect."  And I don't know if they said, but way more expensive, obviously.



"Third, RSSI-based" - and remember that's Received Signal Strength Indicator - "RSSI-based techniques rely on fingerprinting or trained models that require physical access to the target space.  Finally, most of these need client devices to continuously transmit WiFi packets or share their received WiFi packets by installing an application, an access we cannot assume for such privacy-revealing mechanisms."



So they say:  "We present Wi-Peep, a system that is quick, accurate, and performs non-cooperative localization.  It does not require any access to target devices or the network access points.  It does not even need the attacker to connect to the same WiFi network.  In our attack, the attacker, a lightweight drone or a pedestrian, passes by the house carrying a small WiFi capable device and estimates the location of all WiFi devices in the target environment.  We exploit the design of the 802.11 protocol to first generate WiFi traffic from non-cooperative clients, then use a novel time-of-flight based-technique to locate these devices.  Wi-Peep solves the following challenges."



Okay.  The first challenge, generate WiFi traffic without cooperation.  They explain:  "We must, A, identify all devices in the network quickly at the start of the attack; and, B, generate WiFi traffic continuously from such devices to perform location estimation.  A simple solution to identifying devices is to passively wait for WiFi devices to transmit a packet.  This approach is problematic because it requires the attacker to linger around for a long time.  Instead, we exploit the 802.11 power-saving mechanism, which is available in all 802.11 standards from 11a and b up through 11ax by injecting a fake beacon imitating the access point that tells all connected WiFi devices to contact the access point to receive buffered packets.  This beacon elicits a response from all devices on the target WiFi network.



"Once we've identified all devices, we use targeted packets to each of these devices.  To perform time-of-flight measurements on these devices, the attacker requires exchanging packets directly with target devices.  Therefore, natural traffic from a target device cannot be used.  Recent work has shown that 802.11 devices always respond to packets with an ACK, even when the packets emerge outside the WiFi network and are unencrypted or incorrectly encrypted.  We use this flaw to perform time-of-flight measurements to any target device.  The challenge in using WiFi is that WiFi devices are in the sleep mode most of the time, and their radios turned off.  We have designed a technique that allows an attacker to keep the radio of target devices on during the attack so that they keep sending ACKs."



Okay.  So basically what these guys did was to recognize there was a way to, after learning about the beacon in a residence or a corporate facility or wherever, to simulate a broadcast from the beacon which will induce all WiFi devices to respond.  When they respond, they're going to get each device's MAC address.  That then allows them to individually target those devices selectively and in real time, so basically they get an instant inventory, and then they switch into an active tracking mode where they're spewing out packets, measuring roundtrip time which they call "time-of-flight," in order to determine an instantaneous distance they are away from each of the devices.



And of course as they move, all of those various vectors are changing length.  And by changing their path, they're able to infer where the device must be in order for its vector to have changed as it did over time.  So then they explain the second problem they had was localization in the face of noisy, what they call SIFS, which is short for Short Interframe Space.



So they explain:  "In 802.11, ACKs are sent at a fixed interval after receiving a data packet.  This interval is called Short Interframe Space or SIFS as illustrated in" the figure that they have in their notes.  They said:  "Wi-Peep measures the roundtrip time between a packet transmission and an ACK reception and subtracts the SIFS.  This allows Wi-Peep to estimate the time-of-flight and hence the distance between the attacker and the target device.  Unfortunately, our experiments reveal that even though the WiFi protocol mandates SIFS to be 10 microseconds, in practice this delay can vary from 8 to 13 microseconds.  Such errors can randomize the location estimation process.  We build a new algorithm to correct for such variations in time-of-flight estimates."



And finally, dealing with multipath effects.  They explain that the time-of-flight measurements are error-prone because multiple copies of a signal arrive at the receiver from multiple paths, you know, reflection of signals within an environment.  They said:  "The strongest path may not necessarily be the direct path.  Since the attacker is far away and obstructed from the target, this problem is further exacerbated.  Indeed, our measurements reveal that Wi-Peep's individual time-of-flight measurements are error prone for this reason.  To counter this challenge, we take [what they call] the 'wisdom-of-the-crowd' approach.  Even though each measurement is noisy, Wi-Peep involves quick packet-ACK sequences at the millisecond level."  So they're doing thousands per second.



"Therefore, we can collect hundreds of measurements as the attacker flies by, or walks by, the target.  We exploit the spatial diversity of these measurements to get an accurate position estimation of our targets."  So, you know, that's a brilliant and completely workable solution.  Individual measurements are noisy, but the truth can be found by sorting through thousands of measurements made over time from different positions.



And then they talk about their implementation.  They said:  "We've implemented our design on an ultra-light DJI mini 2 drone" - you probably have one, Leo - "using off-the-shelf..."



LEO:  Well, I have the mini 3, but okay.  Is there something finally I can do with it?



STEVE:  Yeah.



LEO:  Yeah.  There's a picture in their paper of it.  It's kind of cool, yeah.



STEVE:  Yeah, it's neat.  Yeah, it's sort of like stuck on the front of it.



LEO:  Yeah, I don't know how well it'd fly with that on there, but I guess it's not too heavy.



STEVE:  They managed to do it.  Anyway, they said:  "Using off-the-shelf ESP32 and ESP8266 WiFi modules.  Our hardware weighs 10 grams and costs less than $20.  It can be deployed on lightweight drones or carried by a person.  Our evaluations in a real environment shows that Wi-Peep finds the location of target devices in an 802.11ax WiFi 6 network on three different floors of a house with a median error of 1.2 meters in around two minutes.



"The contributions of this paper are:  We present a new way for 802.11 protocol features to perform time-of-flight-based positioning of WiFi devices without having any control over target devices.  We find that many devices deviate from the standard time for SIFS which creates a challenge for localization.  We design a localization technique that finds a target device without knowing the exact SIFS used by the device.  We present a solution for future WiFi chipsets that allows authenticated devices to perform localization, while disabling non-cooperative attacks."



So consider these fact which they then enumerate:  "The Wi-Peep attacks work with any WiFi device without instrumentation, in other words, without any application or firmware-level changes.  It does not need physical access to the enclosed physical space and does not need to break the encryption of the WiFi network.  Once the target MAC address is obtained, the target device doesn't even need to be connected to WiFi.  Due to the ease of attack, Wi-Peep has many privacy and security limitations," they write.  "We list some example implications below.  In these scenarios, we assume that it is common for a person to carry a WiFi-capable device such as a smartphone or a smartwatch.  Also note that the type of device - iPhone versus smart sensors - can be identified through various means like the vendor specific information in the MAC address."



Okay.  So, and they give us four examples, one impacting security.  "An attacker can track the location of security guards inside sensitive buildings, for example, banks, if they carry a smartphone or a smartwatch.  And notice that this is real-time.  So moving targets are fine.  They will get real-time feedback as things move within the area that they're surveilling.  A privacy implication, an eavesdropper can fly a drone over a hotel to find the number and types of rooms currently occupied.  This can be done by a rival hotel trying to find detailed information of how target business is performing. WiFi devices that belong to a room such as smart TVs can be filtered based on MAC addresses.  If other devices such as tablets and laptops are found in a room, it can be considered occupied.  And this can be done in the middle of the night when most guests are in their rooms."



Or a privacy/security implication.  "If the MAC address of a device that belongs to a person of interest is known, Wi-Peep can track that person individually in a crowd..."



LEO:  Oh, that's scary.



STEVE:  Uh-huh, "...or inside a building like a shopping center or an airport, even when their device is not connected to any WiFi network."



LEO:  So this is - so you could tail somebody with one of these in your pocket.



STEVE:  Yup.



LEO:  Oh, that's interesting, yeah.



STEVE:  Security:  "Wi-Peep could be used by burglars to find out the occupancy status of specific parts of a building.  For example, the burglar can find out all the people are on the second floor, and the basement is empty.  Wi-Peep can also be used for positive use cases."  And I like this.  "For example, in a hostage situation, the police can fly a drone over the building to find out where the hostages are kept because many hostages might have smart devices on them, and they would be collected together in a dense group and not moving.  It might also be possible to track the attackers, as well."



Okay.  Anyway, through the balance of their paper, which is lengthy, they proceed to deal with every aspect of their system and present its solution.  So my point is, the method to do this today is now in the public domain.  So anyone who wants to do it, and has the skill set to replicate their work, can.  You know, I could do that.  Many of our listeners could do that.  And I would not be surprised if we didn't eventually see an off-the-shelf turnkey Wi-Peep mapping system that would allow anyone with only a few dollars to spare to obtain this potentially powerful remote WiFi mapping capability, very much the way script kiddies are using scripts that they were unable to write.



Until now, we've had a general sense that the goings on inside our homes and offices were at least moderately private.  The idea that someone standing outside in the middle of the night could first take a complete inventory of all WiFi devices within the area - non-cooperatively, without connecting to or knowing our network's password - and then determine the approximate location of every one of those devices, whether they are upstairs or downstairs, and generally where, might not be unsettling to some people.  But there are likely some situations and installations where having such knowledge in real time could be very valuable to the wrong people.



The authors spend some time near the end of their paper talking about possible future mitigations.  And the overall outlook there is bleak.  The bad news is that since this is a hardware-level attack which only leverages standard WiFi features which are implemented in the core WiFi silicon, nothing can be done in firmware or software.  All WiFi chips today will and do respond to the probe request packets sent during the use of this technology.  It will take a future generation of WiFi chips to deliberately break the WiFi specification or the spec to be updated in order to sanction this by not replying within a microsecond or two, but by deliberately randomizing the Short Interframe Space interval so that time-of-flight information cannot readily be determined.  Doing that will allow WiFi to work, while still making location impossible.



LEO:  That is why Apple randomizes MAC addresses on its iPhones, though.  I wonder if that is effective as a countermeasure.



STEVE:  Actually, that's different than this.  This doesn't need MAC addresses.



LEO:  But if I were following you around, I would need your MAC address to know it's you.  I'm not thinking about the mapping feature.



STEVE:  Ah, that is true, yes, yes, yes.  That is true.  MAC addresses, as we know, are fixed when the phone is attached to a network. 



LEO:  Right.



STEVE:  They're only randomized when it's not - when it hasn't joined a network.  Once it has, then it uses its actual MAC address.  But you're right, following you around, the MAC address - I forgot exactly what the algorithm is.



LEO:  I think they change it every 15 minutes.



STEVE:  Okay.



LEO:  And I wonder if, you know, since you know it's him for 15 minutes, and then the MAC address changes, there might be some way to say, ah, yeah, he's just changed his MAC.  I don't know.



STEVE:  Not in a crowd.



LEO:  Not in a crowd.



STEVE:  Because you would be, yeah, because you would be getting - you would be - so first of all you would be - you'd be only pinging that, and then suddenly there would be no reply.



LEO:  Yeah, yeah, right.



STEVE:  So it would then go dead, and you'd have to go back...



LEO:  You would have lost it by that point, yeah.



STEVE:  Yeah, you have to go back into broadcast mode in order to get replies from everybody in the neighborhood.



LEO:  I'm less concerned about somebody mapping my house.  



STEVE:  I knew you wouldn't be.



LEO:  For WiFi access points.  But the tracking thing is concerning.  I think others, I think there's Android phones that also randomize MAC addresses.



STEVE:  Well, remember that it's not the WiFi access point that they're locating, it's all your security cameras.



LEO:  Right, right.  Anything WiFi; right.



STEVE:  Anything WiFi.



LEO:  Yeah, yeah, yeah.  Again, less worried about that.  But the tracking thing is a real cause for concern.  But it must have been a threat anyway.  That's why they're randomizing MAC addresses, I would think.  There must be other reasons; right?



STEVE:  Apple did that for privacy.



LEO:  For privacy; right.



STEVE:  Yeah, yeah.



LEO:  Yeah.  It's interesting.  Really clever.  The multitask thing is what I find most interesting.  The algorithm to get around that.  Fascinating, yeah.  Wi-Peep.  Little Wi-Peep.



STEVE:  Wi-Peep.



LEO:  The paper is in the show notes, if you want to read it.  It actually makes pretty good reading.  Mr. Steven "Tiberius" Gibson, again we have come to the end.  And every time we come to the end of a show, I think we're one step closer to Episode A00.  But it's okay, man, it's okay.  Because I'll survive.  We will survive.



STEVE:  Yeah, Leo, we've got two years.



LEO:  Two years.



STEVE:  Let's not worry about it yet.



LEO:  Two years is a lifetime.



STEVE:  Lots can happen.



LEO:  For a mayfly.  Steve is at GRC.com.  That's his website.  That's, of course, where SpinRite lives, the world's finest mass storage maintenance and recovery utility, GRC.com.  It's also where you'll find copies of this show.  It's one of the places, but it is the place to find the 16Kb version for the bandwidth-impaired, and the beautifully handcrafted, human-crafted transcripts.  He also has a 64Kb audio.  You can leave feedback at the website, GRC.com/feedback.  Those SpinRite forums are there.  Lots of other stuff, including ShieldsUP.  There's so much great stuff.  GRC.com.  We have the show at our site, as well, of course, TWiT.tv/sn.  There's a full-time YouTube channel dedicated just to Security Now!, all 898 episodes.  And of course you can subscribe, and that way you'll get it automatically whenever there's a new one, like right now.



Copyright (c) 2022 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#899

DATE:		November 29, 2022

TITLE:		Freebie Bots & Evil Cameras

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-899.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  What happens when you run a Caller ID spoofing service?  Or when you mislist and underprice online goods?  Or click on a phishing link for a cryptocurrency exchange?  Or consider working for a underworld hacking group?  Use a web server from the Dark Ages in your IoT device?  Or rattle your sabers while attempting to sell closed networking systems to your enemies?  Or decide whether or not to continue to suspend your Twitter ad buys?  Or log into Carnival Cruises with a Passkey?  Or use hardware to sign your code?  This week's podcast answers all of those questions and more.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here with a potpourri of security stories:  the end of a famous Caller ID spoofing service taken over by the feds now; a funny little scam involving misplaced decimal points; a web server from the Dark Ages that's unfortunately still being widely used; and when Passkey is not really Passkey.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 899, recorded Tuesday, November 29th, 2022:  Freebie Bots and Evil Cameras.



It's time for Security Now!, the show where we cover your security and privacy online with the hero of the hour, Mr. Steve Gibson.  Hello, Steve.



STEVE GIBSON:  This is the podcast which just has a boring start every week because everything works.  We're not spending half an hour trying to get, like, stuff onscreen or the lighting right.	



LEO:  Oh.  You just don't see that part.  In the old days we did that all the time, didn't we.



STEVE:  Yeah, I mean, well, and I mean, like with different hosts and juggling things and all that.  But it is easier to do a one-on-one podcast than the thing you're doing with everybody else.



LEO:  Oh, I see.  You saw the rocky start of our previous program.



STEVE:  Yeah, and it's generally like half an hour before things...



LEO:  Yeah, I know, it's a little weird.



STEVE:  ...get going.



LEO:  It's a little weird, I know, I know.



STEVE:  Speaking of which.



LEO:  Yes.



STEVE:  This is Episode 899 for...



LEO:  Oh, dear.  Oh, dear.



STEVE:  Oh, I know.  And this is the birthday episode, for those who don't know.  Leo is celebrating number 66.



LEO:  Look at that.  Route 66.



STEVE:  Route 66 on his - for those who don't have video, he just held up an old sign that won't mean anything to anyone  much younger than us.



LEO:  "Kookie, Kookie, lend me your comb."  That's all I have to say, yes.



STEVE:  So this was one of those weeks where nothing really stood out, but a lot of interesting things happened.  So I grabbed two of the items we're talking about as the title, basically taking from the typical naming of your other podcasts, Leo, where you think, okay, what did we talk about?  Let's come up with something about that.  So this is Freebie Bots and Evil Cameras for 899.  And during this podcast we're going to answer a few questions.  What happens when you run a Caller ID spoofing service?  Or when you mislist and underprice online goods?  Or click on a phishing link for a cryptocurrency exchange?  Or consider working for a underworld hacking group?  Oh, no, this is a great podcast.  Or use a web server from the Dark Ages in your IoT device?



LEO:  This is not all one story.  These are multiple stories.



STEVE:  Oh, yes.  Oh, yes, yes, yes.  Good point.  Otherwise that would be one hell of a story.



LEO:  Really confusing, yes.



STEVE:  Yeah.  Or rattle your sabers while attempting to sell closed network systems to your enemies?  Or decide whether or not to continue to suspend your Twitter ad buys?  Or log into Carnival Cruises with a Passkey?  Or use hardware to sign your code?  This week's podcast answers all of those questions and more.



LEO:  Now, that's a tease.  You are absolutely now, finally, after 899 episodes, conforming to the TWiT way.



STEVE:  It only took 17 years, my friend.



LEO:  We were heading your way, and you headed our way.  So we've met in the middle.



STEVE:  And then we're going to defrag a zebra.  Just, you know...



LEO:  Oh, wait'll you see this.  That is our Picture of the Week.



STEVE:  That's pretty good.



LEO:  And a good one it is.



STEVE:  That's right.



LEO:  I feel like this is almost a Dad joke, this Picture of the Week.



STEVE:  Well, and I think we've used it before [Episode 637].  I mean, it looks familiar to me.  But, if so, it's kind of fun anyway.  So for those who are not seeing our video stream, the caption on this...



LEO:  [Laughing]



STEVE:  I know, it's really good.  It says "I defragged my zebra."  And what we have is what looks a horse with the front half black and the rear half white.



LEO:  It's defragged.



STEVE:  It's defragged, yeah.  All of the in-use clusters got pushed to one end, and the free space is on the other.  And anyway, it's just very clever.



LEO:  I love it.  It got us talking about defragging.  You don't really do that anymore; right?  Most modern operating systems that's handled.



STEVE:  Correct.  Well, actually, the argument that Microsoft has always made, although it's not really as true, is that there is no - they were always saying there was no need to defrag NTFS file systems.  It was clear that over time FAT32 file systems became fragmented.  And what we were saying before we got on the air was I was posing rhetorically the question, how many user centuries of time were lost...



LEO:  Watching.



STEVE:  ...with us just staring at the defrag screen, while the little squares jumped around.  It was just wonderful.  And, I mean, it served no constructive purpose whatsoever.  But, you know...



LEO:  We loved it.  It was fun.



STEVE:  It just really - maybe it was a way for a geek to have a timeout.  It's like, okay.



LEO:  It was a zen.  It was meditative, exactly.  Yeah, yeah.



STEVE:  Yeah. Yeah.



LEO:  But not necessary anymore.  That's right; right?



STEVE:  Well, okay.  So Windows says that it defrags, like, automatically...



LEO:  In the background, yeah.



STEVE:  ...in the background.  Which may be the case.  The one place it can be useful is for data recovery.  If your files have been defragmented, and you lose somehow, some catastrophe, the entire metastructure of your file system, and you really desperately have to have some file back, but basically if you've lost all of the metadata, there's no directory hierarchy, no directories, anything, somewhere out on your drive is a blob of space that a file occupies.  And if it's contiguous, if it is defragmented, you can find it.



LEO:  Right.



STEVE:  I mean, it's there in whole.  But if it itself is scattered all over the place, and it was dependent upon the file system's pointer structure in order to reconstruct that file on the fly, you're really not going to be in such great shape.  So, you know.  But in the old days the reason we of course defragmented was because seek times were so long that, if pieces of a file were scattered physically around a drive, the drive's head would have to go jumping back and forth around, in and out on different tracks, grabbing little pieces of the file in order to get the whole thing.  If the file was defragmented, the head would just go to the beginning and just maybe tick over sequentially a few tracks, depending upon how large the file was, but so it was less wear and tear on the drive because it wasn't having to jump all over the place just to get one file read, and it was a lot faster because you weren't embedding all these seeks in the middle of a file read.



LEO:  Of course there's zero seek time on SSDs, so you...



STEVE:  Right.  And so that's what changed is when we went to solid-state, suddenly all of that head-seeking disappeared, and it made no difference in terms of performance.



LEO:  Although Microsoft quite cleverly, I think, instead of defragging SSDs, if you issue the defrag command, because you still have a defrag, I believe you still have defrag...



STEVE:  Yeah.



LEO:  Microsoft says, yeah, well, we'll just trim the SSD.  It's a way to invoke trim.  And so Allyn Malventano always said, you should still be defragging because you're now trimming your SSDs.  Although I think modern SSDs do trim, as well, in the background.  It's kind of...



STEVE:  Yeah.



LEO:  That's kind of necessary to keep the speed up.



STEVE:  Yeah, well, it's actually an OS-level thing because the SSD has no knowledge...



LEO:  Oh, it doesn't know.



STEVE:  ...of the date - right.



LEO:  But it could be in the controller.  I thought it maybe was in the controller.



STEVE:  No, it's got to be in the OS.



LEO:  No?  Oh, okay.



STEVE:  And so the idea is that the drive itself has no knowledge of the file system.  It's file system agnostic.  But all of the operating systems now, Linux does it, and Windows does it.  In fact, it came up relative to SpinRite recently because, if you were to do a write-level, which is level 3 or 4 in SpinRite 6.1, that leads the drive to believe, the SSD to believe, that its entire space is now in use because when you write to something, basically it flags that area as in use.



So what you can then do is, under Windows, there is a way to say, please trim this drive.  And under Linux it sort of does it more easily, but you're also able to force it.  And so that is one sort of a power user tip that we'll be getting to at some point with SpinRite is, once you do something on an SSD that writes to the whole thing, you then need to put it back into the operating system to let the OS say, okay, calm down here.  These are the areas that are actually in active use, and all the rest of this, no.  That's just completely free.  And the point is...



LEO:  It's hard drive garbage collection.  We've been talking about memory garbage collection.



STEVE:  Yes, very much.



LEO:  It's smart drive garbage collection.



STEVE:  Yes, yes.



LEO:  Yeah, yeah.



STEVE:  Okay.  So I asked a question at the beginning of the show:  What happens if you run a commercial Caller ID spoofing site?  Well, your site turns into the top of this podcast, it's on the second page here.  Yes, and anybody who's interested can go there now, I went there yesterday, I presume it hasn't changed.  iSpoof.cc is the domain name.  I-S-P-O-O-F dot C-C.  And what you get is a big page that says, "This website has been seized," and the various emblems of global law enforcement.  And it says:  "This domain has been seized by the Federal Bureau of Investigation and the United States Secret Service in accordance with," blah blah blah blah blah.  Anyway, and then we've got Europol and London City Police and Cyber Police and all kinds, you know, everybody's involved.



LEO:  Wow, yeah.



STEVE:  So, okay.  Get a load of this interesting bit of happening:  Europol and law enforcement agencies from several countries, including the FBI, have seized the servers and web sites of iSpoof, which was a service that allowed users to make calls and send SMS messages using spoofed identities.  And Leo, if you were curious - oh, actually I have a link on the page below to the web archive Wayback Machine of iSpoof from before it was seized.  And it's quite interesting.



Anyway, so the service launched in December of 2020 and advertised itself as a way for users to protect their phone numbers and identities online.  But Europol said that iSpoof was widely abused - yeah, no kidding - for fraud because it allowed cybercrime gangs to pose as banks and other financial organizations.



An investigation into iSpoof began in 2021 after Dutch Police identified the service during one of its fraud investigations.  The Dutch Police said they linked the service to a web host in Almere, where they deployed a wiretap that allowed them to map the site's reach and learn the identities of its registered users and administrators.  Officials said iSpoof had more than, get this, 59,000 registered users before it was taken down just earlier this month.



U.K. Metropolitan Police said that 142 suspects were detained throughout the month of November.  So they did a big sting operation globally, with more than 100 individuals detained in the U.K. alone, including iSpoof's administrators.  Europol said iSpoof was being used to place more than one million spoofed calls each month, that administrators made more than 3.7 million euros, and that the service has been linked to fraud and losses of more than 115 million euros worldwide.



The U.K. police said they plan to notify all U.K. users who received spoofed calls made through iSpoof, which is nice of them.  So anyway, as I said, I was curious to see what the site looked like before the global takedown which displayed that site seizure page, above.  So I turned to the Internet Archive Project's Wayback Machine.  And what I found was just sort of, you know, headshaking.  



The top of the site's very modern-looking home page, which sort of has a floating iPhone there on the right, proclaims:  "Protect Your Privacy with Custom Caller ID."  And it says:  "You can show any phone number you wish on call display, essentially faking your caller ID."  And then down in their features they said:  "Get the ability to change what someone sees on their caller ID display when they receive a phone call from you.  They'll never know it was you.  You can pick any number you want before you call.  Your opposite will be thinking you're someone else.  It's easy and works on every phone worldwide!"



So, yeah, you can imagine that all kinds of bad people with ill intent would be abusing this thing.  I mean, like, you know, ex-boyfriends or stalkers or spouses or whomever, you know, whose calls you are not accepting would just figure out whose call you were accepting and then spoof it in order to get you to answer the phone.  I mean, it's awful.  Anyway, we've talked a lot about how insecure all of this is.  The, what is it, SS7, the current Signaling System 7, is still allowing this to go on.  I finally gave up and disconnected, actually I had three, I had a fax line and two landlines, because all I was ever getting was just junk calls.  They were just, you know, it was awful.



So for me, the most disturbing thing about this story is that the site was up and running for nearly two years before it was brought down.  You know, that was a ton of damage to be done.  And, you know, you can imagine how the word-of-mouth of this was no doubt spread among the world's shadier types as this thing was allowed to continue.



So for what it's worth, I hope there are not alternative sites that are already up and going.  I would be surprised, frankly, if there weren't.  I should have done a google and looked around.  It didn't occur to me until just now.  But still, just sad that it took that long to get this down.  And we're hearing about the encryption and the tightening of the intercarrier communications.  It's one thing for a carrier to be secure within itself.  But it's the gap between carriers where we need security.  And, you know, they're just not in a hurry.  It's like, why, you know, we have to make them do this.  And so far that hasn't happened.



Okay.  What is a Freebie Bot, you ask?  A new class of bot has been identified.  And this one does something that would be difficult to predict; but, once you hear what it does, you think, huh, is that illegal?  Last Tuesday, the anti-bot research and security provider Kasada, whom we've spoken of before, shared the results of their latest threat intelligence, which detailed the growing prevalence of so-called "Freebie Bots."  Freebie Bots automatically scan and scrape retail websites searching for and purchasing mispriced goods and services, purchasing these discoveries at scale before the error is found and fixed.



Get a load of this.  Kasada research has found more than 250 retail companies recently being targeted by Freebie Bots, with over seven million messages being sent monthly - monthly - within freebie communities.  Okay.  Now just to be clear...



LEO:  But this isn't illegal.  Right?  No.  This is capitalism, baby.  You screwed up.



STEVE:  That's right.  So just to be clear, these are not Furry communities, these are Freebie communities.  Nor are they Furby communities, but that's something else.  Members within one popular freebie community used Freebie Bots to purchase nearly 100,000 products in a single month with a combined retail value of $3.4 million. But Kasada's research revealed that, due to significant underpricing, the total purchase cost of the goods for the Freebie Bot users was $882.  This allowed some individuals to realize a monthly profit of over $100,000.  Top items purchased using Freebie Bots during this period of time included off-brand sleeveless halter neck mini dresses; get this, Apple MacBook Air laptops; and deep cleansing facial masks.



LEO:  It's an interesting Venn diagram.  I don't know.



STEVE:  That's right.  What's your overlapping customer matrix?  Many pricing errors were the result of a decimal point misplacement, granting discounts as large as 99%.  Using the speed and scale of a bot attack to rapidly purchase as much stock of these erroneously priced goods as possible, actors then turn around and resell the goods at the price they should have been, reaping a large profit.



So you can see how this could happen; right?  Someone keying in a new item's retail listing gets into the habit of entering a decimal point before the last two digits of the price.  But then they encounter a price formatted as a whole integer number of dollars without any cents, and without thinking they place a decimal point before the last two digits, thus inadvertently reducing the listing's price by a factor of 100.  It turns out that, at scale, across the entire Internet, these mistakes happen enough to have spawned the creation of a new class of bot, automated retail mistake-finding bots, which will instantly purchase as much of something that's been mispriced as they're able to.  So human ingenuity knows no bounds.  I suppose that while this might not be technically illegal, it certainly is unethical and dishonorable.



LEO:  Is it?  Is it?



STEVE:  Well, you know...



LEO:  No.  I'm buying it at the listed price.



STEVE:  You know when the MacBook Air is offered for 50 bucks...



LEO:  Not my problem.



STEVE:  There's something wrong.



LEO:  Not my problem.  That's a good deal.  I'll take it.



STEVE:  How many can I have?



LEO:  I guess it depends.  If this is happening to, you know, your local Goodwill Store, that's terrible.  And that's probably more likely where it is.  Apple probably never makes a mistake like this because they have good software.  But still.



STEVE:  You're right.  It's probably taking advantage of people who can't afford...



LEO:  Of small retailers, yeah, yeah, yeah.  I mean, Apple's never going to misprice its Apple gear on its site.



STEVE:  I have seen oddly priced things on Amazon.  You probably have, too, Leo.



LEO:  Sure, all the time, yeah.



STEVE:  Where it's just like, what?  That can't be right.  You know?  And I just, you know, I mean, it's for a left-handed screwdriver, so I don't need one.  But still.



LEO:  I mean, I'm the kind of guy, and I know you are, too, that probably would go, oh, that's a mistake.  I'm not going to take advantage of that.  So maybe it is unethical.  I wouldn't do that.  But still, depends I guess on the size of the company.  The problem is...



STEVE:  As I said, once you hear the idea, no one is surprised.



LEO:  Oh, it happens all the time, yeah, yeah.



STEVE:  Well, no, I mean, that a bot has been created to go scan for these mistakes in real-time.



LEO:  Oh, yeah.  Absolutely.



STEVE:  And buy up the inventory.  Wow.  Okay.  We have the anatomy of a real-time cryptocurrency heist.  The group PIXM Security, whose business is to protect end users from credential fraud, recently blogged about the details of an attack group they've been monitoring.  The lengths this group will and does go to, to circumvent, like, one of the newer protections, the deliberate "authorized device" protections we're beginning to see more and more, where like if use a new device, you log in with somewhere you haven't logged in before, there's like, whoa, we haven't seen this device before, so we're going to jump you through some extra hoops.



So, okay.  What's interesting here, and I think you're going to find this really interesting, Leo, is their report in detail of what's behind a true real-life phishing exploit.  So, okay.  And just to give you a hint, scammers will use in-browser chat to initiate a remote desktop session on a victim's device, approve their own device as valid to access the user's account, then drain the cryptocurrency from their wallet or wallets.



So, okay.  Here are the details behind this.  When PIXM's Threat Research team first started tracking the group, they were only targeting Coinbase, right, like the premier exchange.  Then over the past month the group has increased their coverage, that is, the bad guys have increased their coverage to add "support," if you call it that, for MetaMask, Crypto.com, and KuCoin (K-U-C-O-I-N), in addition to Coinbase.  So now four.  The spoofed domains are the typical slightly misspelled, in this case subdomains of azurewebsites.net.  So that's the hub of where they are.  And so it'll be like, you know, conbase.azurewebsites.net or something like that.



The group employs working effective second-factor relay interception when a user is spoofed into going to a lookalike site.  Regardless of the credentials the user enters, whether they're legitimate or not, since the spoofing site cannot determine that initially, the user will be moved to a two-step verification page after clicking "Login," where depending upon the platform in question they'll get what they're expecting, which is either prompted for a second-factor code or their phone number is prompted and used then to receive a two-factor code.



The criminal group will first attempt to relay the credentials they've been given and second-factor codes to the legitimate login portal which is associated with the platform they're spoofing.  Once the user clicks "verify," they will be presented with a message, no matter what happens, telling them unauthorized activity has occurred on their account.  Well, it turns out it's true, actually, but this is the bad guys trying to reel them in further.  As with the original Coinbase attack which this group started with, this will initiate a chat window to keep the user on the phishing page in the event the two-factor code should fail, which of course the bad guys don't know  yet because they'll get prompted for that after they attempt to log in, and should the threat actor need to start a remote desktop session with the victim to continue with this attack.



PIXM wrote that, in their experience, regardless of whether the victim enters legitimate credentials or not, the group will "chat" with the victim to keep them in contact should they need to resend the code or proceed to the second phase of the attack.  The criminal gang's willingness to do this significantly increases, I'm sad to say, end-user engagement, you know, and their belief that, like, they're talking to the real guys, right, because there's someone there.  For the majority of the attacks which this group carries out, they engage in direct interaction with the user.  Their spoofed login and verification portals will, by default, return a login error, as I mentioned, regardless of the actual standing of the user's account on the actual exchange and the wallet.



Of course this process is intended to initiate a chat session with a member of the criminal group posing as a customer support representative from the exchange.  The criminals will use this interface to attempt to access the users if their initial credential relay failed or if time expired, you know, because we know that these one-time passwords only are limited to 30 seconds, and then they change, so it may have expired.  If so, they'll prompt the user for their username, password, and second-factor authentication code again directly in the chat window.  The criminal will then take this directly to a browser on their machine and again try to access the user's account.



Should this also fail for any number of reasons, most common of which is that the device the attacker is using to access the victim's account or wallet is not, as I mentioned before, an "authorized device" in the user's profile, which probably means unknown IP or it doesn't have a persistent cookie, which the user's browser would have, even if they've said I don't want to remain logged in, they would still have a, you know, that would be a session cookie.  Separately they'd have a persistent cookie which says this browser has logged in in the past.  In that case, the attacker will proceed to Phase 3 with the victim.  The group uses the "tawk.to" (T-A-L-K dot T-O) chat plugin on all the sites, each with the same customer support representative named Veronica.  So be wary if Veronica is talking to you.



If the previous efforts have not succeeded in giving the criminal group access to the victim's wallet, they will instruct the victim to download the "TeamViewer" remote access control app.  They instruct the victim that this is to help them diagnose the issue with their account directly on the user's machine.  Once the victim has installed TeamViewer on their device and entered the code provided by the group, right, to initiate the session, the criminal now has full control of this poor user's device and will guide them through the steps required to authorize their device, that is, you know, their own machine, wherever they are, to the victim's account and hijack their session.



The criminal has the user navigate to their email inbox associated with the crypto exchange or wallet account.  They'll instruct the user to log into their account on the exchange or wallet site.  While the user is logging in, the attacker, who has control of the victim's device, will enter a random character while the victim is entering their password, right, like interject a character midstream which will force it to fail.  The attacker will then click into the TeamViewer chat box without the victim's knowledge and ask them to enter their password again, which is just of course sending the password now to the criminal in plaintext.



When the user re-authenticates, the attacker will simultaneously log into the user's account on their own device, which will prompt a "new device confirmation" link to be sent to the user.  The criminal then takes over the user's desktop session and sends themselves, via the TeamViewer chat feature, the device confirmation link.  They can now use this link to validate their own device to access the user's account.



The final draining of the user's cryptocurrency funds may then be initiated, will be initiated during any of the previous attack phases as soon as the bad guys have access to the wallet.  It's of course only contingent upon the attacker finally being able to successfully authenticate to the victim's account from their own machine being recognized as an authenticated machine, if it hasn't already been.  And of course once the criminal is in the victim's account, they'll immediately begin transferring the cryptocurrency held in any of the victim's wallets to their own.  And they keep the victim engaged and waiting as they steal their funds in the background on their own machine in the event that the service they're draining funds from might require some sort of email or additional phone confirmation of funds transfer.  If that's the case, the attacker will assure the victim that this is normal and expected activity related to their account restoration.



Once all the funds have been sent from the victim to the criminal's wallet, they end the communication with the victim, having emptied the target's wallet.  So that should give everyone a sense for how much effort bad guys in some sort of big cyber farm, you know, cryptocurrency exchange farm are willing to do to phish people who have cryptocurrency and relieve them of that burden.  Amazing.



LEO:  I wonder if they'll move on now that crypto's gotten less and less valuable. 



STEVE:  I don't know.



LEO:  It's nicely anonymous.  It's a great thing to steal because it's hard to track.



STEVE:  Yes.  And toward the end of the podcast I'm going to talk briefly about my own experience with having an open web server where anyone is able to create an account.



LEO:  Yikes.



STEVE:  Leo, the Internet has become a sewer.  And I know from my experience in trying to prevent that that there are - and in fact from talking to some of the anti-forum-spam people who I struck up a dialogue with, that there are rooms full of people sitting at screens and keyboards who do nothing but that all day long.  And there are different rooms full of similar people who do nothing but respond to phishing cryptocurrency link clicks and then perpetrate all of this, draining people individually of their crypto currency.  So, you know, it cost, I mean, if they're willing to do that to create an account against all odds on a web forum, they are certainly willing to do something not that much more in order to get a hold of someone's cryptocurrency wallet that may have a bunch of money in it.  Unbelievable.



If any of our listeners are looking for something to do, the Karakurt group, with known ties to former Conti gang members, and known for its hack-and-leak extortion operations, announced this week that they are recruiting people to breach networks, code malware, socially engineer people, and extort companies for payments.  And of course I'm not serious about any of our listeners wanting a job there.  But their online posting was wonderful.



So just to back up a little bit, Karakurt (K-A-R-A-K-U-R-T) gets its name from a type of black widow spider.  It's not a ransomware gang.  They don't bother with encryption.  They're known for extortion and for demanding ransoms between $25,000 and as much as $13 million, payable in Bitcoin.  They don't target specific sectors or industries.  They're an equal opportunity denizen.  The gang backs up their claims of stolen data using screenshots and copies of exfiltrated files as proof that they've been in someone's network, and they threaten to sell or leak the data publicly if they don't receive a payment.



And they're not very patient.  Karakurt typically sets a one-week deadline to pay.  Until they're paid, they bully their victims by harassing their employees, business partners, and customers with emails and phone calls all aimed to pressure the company into paying the ransom.  So not nice people.



Okay.  Their site on the dark web is a Tor hidden service; so, you know, it's a .onion domain.  It contains several terabytes' worth of previous victim data, along with press releases naming organizations that had not paid up in terms of getting ransom, and instructions for buying victims' data.  The site surfaced in May.  The miscreants usually break into networks by either purchasing stolen login credentials; using third-party Initial Access Brokers that we've spoken about extensively previously, you know, of course, those are brokers that sell access to compromised systems; or by abusing security weaknesses in the network's infrastructure.



Okay.  So this brings us to their so-called "Great Recruitment" posting recently, last week, on the dark web.  Since it was interesting and somewhat entertaining, I thought it would be worth sharing.  Now, they're Russians.  But I found myself thinking, wow, okay, they're not having a translation problem into English in this instance.  The posting is well translated into English.



They wrote in this posting:  "The Karakurt team is glad to announce some news.  More than a year in private mode, but now we open the great recruitment.  You can join our honorable mission to make companies pay for the existing gaps in their cybersecurity and for the inaction of their IT staff.  So, our dear hack lovers, what we have for you:



"Are you an experienced pen tester and for some reason do not want to work with ransomware operators?  We can find a better place in our team."  Meaning they don't do ransomware.  Otherwise they're every bit as evil.  "Do you work for a company that you hate with all your heart?  Or maybe your boss fired you, but forgot to turn off your network access?  You can find solace in our arms.  You are a bearer of a sacred knowledge of malware coding?  Disassembling?  Exploit developing?  The Karakurt team is ready to set interesting and non-trivial tasks for research, implementation of specialized software, and modification of toolkits.



"Are you from the financial industry?  Do you know how to make money on quotes of companies whose shares are in poor condition?  Know how to sell data in a specific market?  We will hug you and love you more than anyone has ever loved you before.  Are you from a data recovery company and know us?  Let's be friends.  Maybe even best friends.  Do you have social engineering experiences?  There is also a vacancy.  Want to take revenge on capitalism through cyberspace?  We will find you both a vacancy and a psychologist.  Perhaps you're a crazy researcher.  We are really waiting for you, bro.  The best hacker group, Karakurt, is waiting for you, our dear hack lover."



So the good news is that's not being seen by most people who are not visiting the dark web.  And I assume if you're visiting the dark web you're either a security researcher who is not interested, or you're a bad guy who might be.  Anyway, now you know.  Karakurt has their arms wide open, ready to love you  more than you've ever been loved.



Okay.  And speaking of job offers, over the summer the U.S. government held what they called a Cybersecurity Apprenticeship Sprint.  As a result of that, 7,000 apprentices were hired in official cybersecurity roles with around 1,000 of the new hires being sourced from the private sector.  The sprint was launched in July by the White House and the Department of Labor as a way to boost the government's cybersecurity workforce.



Okay.  I mentioned a web server from the Dark Ages.  The security firm Recorded Future found that a Chinese Advanced Persistent Threat actor had leveraged a vulnerability in an IoT device to gain access to an electrical grid operator in India.  And in a report last week, Microsoft said that they had identified the entry point for the attack.  It was a tiny, somewhat obscure web server known as Boa.  It's www.boa.org.  And actually I was surprised that there was a three-letter dot org.  Those are rare.  And it's only due to the fact that it's been around for a long time.  Boa, which is said to be widely used across the IoT and ICS (Industrial Control Systems) space.



Okay.  As we all know, it can be very handy to have a nice simple and tight little web server, so tiny that it could even be considered a component.  Although Boa is written for Unix-like operating systems, it doesn't use the traditional Unix fork and spawn approach of creating multiple instances of itself to handle individual incoming connections.  I didn't study Boa long enough to determine whether it's multithreaded, thus spawning a new thread for each request.  It might be purely serializing.  Since the Unix/Berkeley sockets TCP/IP stack supports a queue of waiting connections, Boa might simply accept one connection after another using a single thread of execution.  That would indeed make it quite lean.  And apparently Boa is also quite fast.  Of course you get that until you overload it by an HTTP server that is so simple.



Okay.  All of that is okay.  But here's the problem.  It's not that Boa was first written and released 27 years ago in 1995.  That's fine.  The problem is that the last attention its source code received was 17 years ago, back in February of 2005.  In looking through Boa's development history, I noted with some...



LEO:  Is this the website?



STEVE:  Yes, my friend.



LEO:  It looks very...



STEVE:  That makes mine look modern.



LEO:  It's very not - last updated February 2005.



STEVE:  Uh-huh.



LEO:  And it's, you know, I couldn't pull it up because it's not HTTPS.  I had to just...



STEVE:  Oh, no.  Nor is the web server, Leo. 



LEO:  Yeah.



STEVE:  Uh-huh.  Okay.  So if you click on "News," that first link there, and then if you scroll down to the 2002 Developer's Conference...



LEO:  Oh, yeah, the big Boa Developer's Conference, who could forget that?



STEVE:  Well, yeah, in fact I have a picture of the Developer Conference attendees.



LEO:  What a party.



STEVE:  In the show notes.  I noted with some interest on...



LEO:  There's just two of them.



STEVE:  On October 4th and 5th of 2002, the Boa Developer's Conference was held.  The official minutes of the event noted:  "Larry and one of his sons stayed at Jon's house October 4th and 5th, 2002.  While the reasons were unrelated to Boa development, and in fact Larry and Jon spent only a few hours discussing Boa, computers, and the Free World, it seemed appropriate to refer to the event as a Developer's Conference.  Here is a picture..."



LEO:  Of the team.



STEVE:  Here is, yeah, the entire team in one location.  "Here is a picture of Larry and Jon at Jon's house. (Left to right:  Jon, Larry)."  Now...



LEO:  Oh, my goodness.



STEVE:  This web server is in an IoT device which is being used by the grid operators of, what was it that I said, Israel?  No.



LEO:  India.  India.



STEVE:  India, right.  So, you know...



LEO:  Well, the price was right, I guess.



STEVE:  Oh, it certainly was.  I have no doubt that these two have their hearts in the right place, if they're still beating.



LEO:  If they're around, yeah.



STEVE:  But a web server they wrote 27 years ago and last tweaked 17 years ago, which has no support for secure connections, is currently in use, and apparently widely so because it's apparently very popular.



LEO:  Wow.



STEVE:  Among other places, the operation of an electrical grid operator in India.  Lord only knows where else this Boa Constrictor might be lurking.



LEO:  There are a lot, I mean, you know, there are a lot of mini specialty web servers.  That's a simple thing...



STEVE:  Yeah, it takes about an afternoon.



LEO:  ...to do a lot of languages.



STEVE:  It takes an afternoon to write one these days.



LEO:  Yeah, simple.  Yeah, yeah.  But wow.  Why they chose this one is baffling.



STEVE:  Well, it's tiny.  Right?  So it's like, well, we're going to put it in ROM.  Who's got the smallest server?  Oh, look, Boa.  Oh, and you didn't bring up their logo page on that site, Leo.  It's pretty good.  These are if you want to put a logo on your home page when you've used the Boa Constrictor server in order to serve your pages.  You can pick from any of these...



LEO:  I'm going to put this on my website just for fun.  Powered by Boa, the high-perform - when you feel the need for speed.



STEVE:  Yeah, I like the one with the colored scales.



LEO:  Oh, yeah, yeah.



STEVE:  That one's good.



LEO:  That's nice.  That'd look good on my site.  Ooh.



STEVE:  Anyway, unfortunately, IoT devices on the 'Net are powered by Boa.  And Microsoft didn't specify the way in, but China found a way in.  And it's not surprising.  I did a search on their errata page for null.  And I found lots of null pointer problems in the past.  So presumably not all of them.  Yikes.



LEO:  But good news, it's Y2K-compliant.



STEVE:  Yes, yes.  Your concerns from 22 years ago about Y2K have been addressed.  Larry and Jon did it by phone.  They decided not to have a developers conference for that because - and there actually is.  They go on at some length on their explanation page about Y2K.  And while the underlying OS may have a problem with it, at least their code doesn't.  So rest assured, if your clock is set wrong, you'll be okay.



LEO:  I notice they copied their Y2K statement from the Apache Project.  So I guess they were aware of that other little web server out there.



STEVE:  Yeah, no need to reinvent the wheel there.



LEO:  No, that's right.



STEVE:  Unfortunately, they didn't copy their TLS support from Apache.  So they don't have any.



LEO:  Wow.



STEVE:  Wow.  Okay.  So the dilemma of closed-source Chinese networking products.  I dislike the idea of, and I know you do, too, Leo, of banning foreign companies from selling their products to whomever wants to purchase them.  And the idea that networking and surveillance cameras of Chinese origin might incorporate designed-in trojan capability, it does seem a little bit farfetched to me.  Presumably, such cameras are not phoning home to China but are networked locally.  So the first instant that unexplained data was caught transiting the wire, there would be hell to pay.  But at the same time, we cannot prove the negative; right?  We have no way of proving that there isn't any backdoor trojan capability present in Chinese network and surveillance cameras.  So I suppose that the recent actions from the U.S. and the U.K. are understandable.



Last Friday, November 25th, both the U.S. and U.K. governments banned the use of Chinese networking and surveillance equipment, citing national security-related fears as the grounds for their decisions.  The U.S. Federal Trade Commission has banned the import and sale of networking and video surveillance equipment from Chinese companies Dahua, Hikvision, Huawei, and ZTE.  And I know that at least Dahua and Hikvision are state-owned companies.  And we talked about Hikvision not long ago with regard to some badness that they were caught with.



So in the U.K., the Parliament has instructed government departments to cease the deployment of security cameras from Chinese companies on "sensitive sites" such as government buildings and military bases.  British officials said the Chinese-made security cameras should not be connected to core networks, and that government departments should also consider removing and replacing existing equipment even before "scheduled upgrades."



U.S. and U.K. bans come after both countries' intelligence agencies warned against the use of equipment from Chinese companies, cautioning that Chinese equipment could be used for digital surveillance, digital sabotage, and economic espionage.  Again, of course, they're not wrong.  But we already do lots of even dumber things, like deploying proprietary design, closed-source voting machine technology in critical elections.  You know, how do we know what those machines are doing?



Both Dahua and Hikvision had already lost a large chunk of their market in the U.S., after the U.S. Treasury department sanctioned the companies for providing the Chinese government with facial recognition and video tagging solutions in the government's efforts to oppress the Uyghurs.  And I recall, as I mentioned, that Hikvision was on our radar separately for something that they were doing maybe six months ago or so.



We've talked about this a lot in the past.  I noted that it was hard to believe that Russia was still using the American-made closed-source Windows OS when hostilities between the U.S. and Russia have been so aggravated.  And it's also amazing that, until now, the U.S. has been deploying Chinese-made networking gear while having absolutely no idea what's inside the box.  In the past we've even discussed the existence of counterfeit Cisco networking gear.  Since Cisco equipment is all manufactured in China, both the real and the clearly counterfeit equipment all comes from the same place.  How do we know what the counterfeit systems are going to do?



And the burden of trust is really not symmetrical.  Due to Chinese massive manufacturing and fabrication capability, they receive Western technology from us, and the West purchases the resulting Chinese products from the East.  Thus, more trust is required from the West than is from the East.  So I suppose my point is we cannot discount such concerns as being purely hyperbolic and inflammatory.  Our dependence upon our networks and digital infrastructure has slowly but surely been growing through the last several decades.



So it's only natural that at some point, someone at the national government level is going to wake up one morning and pose the big "but what if" question to their staff.  You know, it's that "but what if" that was the driving factor behind the recent decision to "just say no" to Chinese networking and video equipment.  And unfortunately, the protectionism that results I think is both sane and rational, even if you can't prove that anybody's doing anything wrong.  You know, what if?



And, you know, the equipment we're buying is just a black box.  We plug it in, and we assume it's going to be okay, but we have no ability to prove that that's the case.  It really is a dilemma that we've gotten ourselves in.  And all I can see is that over time, between countries where there are clear hostilities, we're just not going to be able to trust equipment from each other.  And, you know, I think that's what has to happen until and unless open source ultimately wins, as I argue, and I know you agree, Leo, it ultimately should.



LEO:  Oh.  I didn't realize you were a complete fan.



STEVE:  Oh, yeah, yeah.



LEO:  Good.  I am, too, yeah.



STEVE:  Yeah.  I absolutely think that's...



LEO:  I think we're really learning that lesson over and over and over, frankly, yeah.



STEVE:  Yes, yes.  MIT recently published its rankings of national cyber defense by nation.  Interestingly, at the top of the list for the best defense, cyber defense, is Australia.  In second place is the Netherlands.  Third place goes to South Korea.  And we here in the U.S., we just eke out Canada a little bit.  We're in fourth place, with Canada in fifth.  So those are the top five:  Australia, Netherlands, South Korea, U.S., and Canada.  Then the way MIT - so they did the top 20.  So the way they organized it is top five is green.  Then the middle 10 they lumped together.  That's Poland, the U.K., France, Japan, Switzerland, Italy, China, Germany, Spain, and Saudi Arabia in descending order.  And then the bottom five, they set them out separately as red, and that's in order of descending security:  Mexico, India, Brazil, Turkey, and Indonesia.



So anyway, just sort of an interesting ranking.  And it's interesting that Australia is solid.  And they got a 7.83.  This was all ranked out of 10.  So they got a 7.83.  The U.S. is 7.13.  So a bit of a drop.  Although Indonesia at the very bottom of this 20 is 3.46.  So it's possible to be doing a bad job.



I just wanted to make a quick note for our listeners to be careful about Docker Hub images.  It turns out that the security firm Sysdig scanned the official Docker Hub portal and identified 1,652 malicious Docker images which have been uploaded, as I said, on that official Docker Hub portal.  More than a third contained cryptomining code, you know, making somebody some money, if you just run that Docker and don't pay any attention to what it's doing, while others contained hidden secret tokens that the attacker could later use as a backdoor into a server that was running a Docker and exposed publicly.  Other Docker images contained proxy malware or dynamic DNS tools.



So anyway, just be careful.  They are seductively easy to grab and deploy.  They're very cool.  But not everyone who's creating and making them available for everyone is doing so out of the goodness of their heart.  So a word of warning.



We've been tracking zero-days for a while.  I wanted to note that Google just fixed Chrome's eighth zero-day of the year.  So they're doing better than they were last year.  They updated Chrome to eliminate CVE-2022-4135 which, no surprise, was a heap buffer overflow.  It was found and exploited in Chrome's GPU component.  The vulnerability was discovered by one of Google's TAG researchers and is now history.  So eight for Chrome, eight zero-days for 2022, and I imagine they'll get through the rest of the year.  We'll see.



CISA, the Cybersecurity and Infrastructure Security Agency, is now on Mastodon, Leo.  After a fake account was spotted for CISA's director, Jen Easterly, on Mastodon, CISA now has an official account on the platform.  The account is at the very popular infosec.exchange server which is turning out to be where most of the industry's security researchers have been hanging out and hanging their hat.  So infosec.exchange/@cisacyber is the handle, C-I-S-A-C-Y-B-E-R.



LEO:  They need to add an icon and some verification.



STEVE:  Yeah, they didn't...



LEO:  I'm not going to follow them till they put a little more effort into their account.



STEVE:  They didn't do very much.



LEO:  Yeah.  One of the nice things about Mastodon, by the way, 1,400 people already do follow them, is that it's very easy to verify that you are who you say you are.  All that CISA has to do is put a Mastodon link in the CISA home page, even can be hidden, doesn't have to be visible, and they would be verified.  



STEVE:  Oh.  Very cool.



LEO:  But they have so far not posted anything.  They're not following anybody.  They haven't put in an icon, nor are they verifying their links.  But I'll take your word for it they're the real deal.  You've seen this posted at CISA's site or something?



STEVE:  No, I picked up a news blurb about it in the infosec community.



LEO:  Yeah, yeah.  That is a good server, by the way.  If you're in infosec, it's a good one to follow.  So cisajen is not real.



STEVE:  Correct.



LEO:  That account has been suspended.



STEVE:  Good.



LEO:  But CISA, which is cisacyber at infosec.exchange is apparently real.



STEVE:  The real guys.



LEO:  I'll follow them.  I'll let you know if anything more happens.



STEVE:  And you're right.



LEO:  Let's wait.



STEVE:  Let's hope that they go to the next step because, come on, guys.



LEO:  Yeah, c'mon.



STEVE:  That's sloppy.



LEO:  Just all you have to do is follow one person.



STEVE:  It's very cool.



LEO:  Yeah.



STEVE:  Very cool.



LEO:  It's good that they're there.  You know, infosec.exchange has a lot of really good people on it.  And I should mention that Alex Stamos, speaking of infosec, will be on TWiG tomorrow.



STEVE:  Oh, cool.  Very cool.



LEO:  Yeah.  He of course was in charge of infosec at Yahoo! and then at Facebook.  Left over the Cambridge Analytica scandal.  Not his fault.  He left because they weren't doing the right thing.  And he is part of the Krebs Stamos Group.  He's working with Chris Krebs now doing cybersecurity.  So he'll be a great guest tomorrow on TWiG.



STEVE:  Yeah, Alex was first, and then they added Chris to the group.



LEO:  Yeah, yeah, it's really good.



STEVE:  And in fact he was involved with Zoom in the early COVID-19.



LEO:  That's right.  He was the first person they went to when people got mad at them for not doing it right, doing encryption right, or kind of misrepresenting their encryption.  He's also a professor at Stanford.  So I think he will be a good guest.



STEVE:  Neat.  Tomorrow. 



LEO:  Yeah, yeah.



STEVE:  I have one piece of miscellany.  It's not directly security-related, or privacy.  But everyone's talking about Twitter and its uncertain future under the reign of Elon.  I stumbled upon something that I thought our listeners might find interesting, and I think you might, Leo, as I did, because it appears to contain some actual facts.  This is a note written by an unnamed executive director at an unnamed business-to-business organization.  But it looks authentic.  I presume it's anonymous because he would prefer not to have Elon Musk retaliate against his firm.  The title of his posting was "I told my team to pause our $750,000 per month, so three quarters of a million dollar per month, Twitter ads budget last week."



So here's what he wrote.  He said:  "I've seen a lot of technical and ideological takes on Elon Twitter."  And I got a kick out of that.  I wondered whether it was a play on "Tim Apple."  Anyway, he said:  "But I wanted to share the marketing perspective.  For background, I'm a director at a medium-size B2B tech company, not in financial services anymore, running a team that deploys about $80 million in ad spend per year.  Twitter was 8-10% of our media mix, and we have run cost-per-engagement, i.e. download a white paper, register for an event, et cetera, campaigns successfully since 2016.



"I had my team keep our Twitter campaigns live for two weeks post-takeover on the bet that efficiency would improve with fewer advertisers, and that the risks were managed and probably overblown.  I was wrong, and I think the things we saw in these last two weeks means many more advertisers will bail on the platform in the coming weeks," and he says, "(for non-ideological or virtue-signaling reasons)."



So then he has four bullet points.  He says:  "Performance fell significantly.  CPMs didn't drop" - meaning same number of eyeballs - "but our engagement went way down.  Maybe it's a shift in users on the platform.  Maybe it's ad serving-related."



Second point:  "Serious brand safety issues."  He said:  "Our organic social and CS teams got dozens of screenshots of our ads next to awful content.  Replies to our posts with hardcore anti-Semitism and adult spam remained up for days even after being flagged."  Third:  "Our entire account team at Twitter turned over multiple times in two weeks.  We had multiple people," he said, "AE, AM, analyst, creative specialist, supporting our account, and they all vanished without so much as an email.  We finally got an email with a name for an AM" - I guess that means account manager - "last week, but they quit, and we don't have a new one yet."



And, finally, he said:  "Ads UI is very buggy, and login with single-sign-on and two-factor authentication broken.  One of my campaign managers logged in last week and found all our paused creatives from the past six years had been reactivated.  Campaign changes don't save.  These things cost us real money."  Anyway, I thought that was...



LEO:  I wonder if they put any prices with the decimal point in the wrong place up by accident.  Now, that could cost you.



STEVE:  Since I hadn't encountered anything as substantive as that, I thought that it was interesting to see.  And I understand a bit about what's going on from the perspective of one of Twitter's advertisers who views the service dispassionately.  He doesn't care one way or another who's doing what, except he dislikes the idea of their ads appearing to endorse horrific content which it's now appearing next to or in the comments that follow an ad.  You know, for him, Twitter is just either an ends to a means - wait.  A means to an end.  Or maybe not.  So anyway, I thought that was interesting.



LEO:  Yeah, he's just a businessperson; right.



STEVE:  Yeah.  Oh.  And in a related piece in a security newsletter I recently scanned, the statement was made:  "Some threat intelligence companies are telling their customers that they can no longer guarantee takedowns of malicious or reputation-damaging content from Twitter as there is nobody in Twitter's abuse team to respond to requests anymore."  So another data point from a different direction.  And for what it's worth, TweetDeck is behaving weirdly now.  You know, I always go in in order to pull feedback from largely my DMs, although I scan the public feed, you know, the @SGgrc postings.  And it was definitely not working the way it used to, and not in a way that I liked.  So something is changing or has changed.  And, you know, I don't know, I don't care to know what that is.



Okay.  So KerryOnAnon is his name.  MrIndigo is his Twitter handle.  He said:  "Hi, Steve.  Finally listening to the latest Episode 898, and I started wondering, is quantum computing going to be just a faster way to guess passwords, or is there another attack vector?  In other words, is it just going to be a faster way to brute force attack passwords?"



Okay.  Interestingly enough, once we get quantum computing, assuming that we ever get quantum computing, it won't be any faster at brute forcing passwords.  In fact, it would likely be far slower and vastly more expensive than conventional hardware-accelerated, hash-based password brute forcing.



LEO:  Oh, how interesting.  That's not the problem.



STEVE:  No.  There's just a class of things it's good at.  The rest it's really crappy at.  You know, it's like weather prediction.  It can do that, but it can't tell you where a specific drop of rain is going to land, and that's what you need for symmetric crypto and hashing is that kind of exact operation.  The important thing to understand here is that some of today's crypto, but only some of it, depends upon the traditional, time-proven difficulty of factoring a very large number into its two half-as-large prime number components.



That's it.  That's all that the fervor surrounding quantum computing is about, the ability to do a couple of things quickly that are entirely insurmountable, that is, this factorization problem.  But it's only the asymmetric key crypto that quantum computing might be able to someday weaken.  None of the other crypto that we also depend upon today will be affected.  Symmetric key crypto, like our beloved AES ciphers or today's strong hashing algorithms, will not be affected at all.  And they don't need to be changed.



I was thinking about quantum computing after I read this guy's note, and I was looking for a good analogy of the effort, its promise and the difficulty that it presents.  And what popped into my head as being in almost every way similar was power generation, at scale, via nuclear fusion.  It's a useful analogy.  It requires crazy, way out there, new physics and new materials and new technologies.  And like quantum computing, fusion has been chased for decades, driven by the promise of "what if," just like quantum computing has.  And incredible amounts of ingenuity and money have been sunk into it.  Many different approaches have been tried and discarded.



And yes, we are creeping forward little by little, inch by inch, tantalizingly, just enough to keep the investment cash flowing.  But, boy, is fusion a difficult nut to crack.  In order to fuse matter, we must create, contain, and compress the hottest plasmas humans have ever handled - hotter, it turns out, than the sun.  And at this point it's as much art as science.  Will we get there someday?  Maybe.  Maybe not.  It's still not clear.  But as with quantum computing, we do appear to be making some progress year after year, learning as we go.



So as for quantum computing, my feeling is that there's no reason not to replace that small but crucial portion of our large crypto library of algorithms, which are believed to be currently unsafe if quantum computing ever happens.  We can replace it with algorithms which are believed to be quantum safe.  We just don't want to make any mistakes with our replacements, and there's no reason to believe that there's any big hurry.  We might well have free electricity, once we figure out how to burn water, before quantum computers threaten our current dependence on today's asymmetric crypto.  So not to worry.



Another listener who requested anonymity, and I'll explain why in a second, he said:  "Hi, Steve.  In the last episode of Security Now! you talked about passkeys.directory, which lists web applications that support Passkeys.  I wanted to share my observations with you.  First, the website owner chose to manage it with no transparency.  When I saw it, I thought there must be a Git repo where I could open an issue for a change request.  Surprisingly, they choose to use Google Forms, which masks all the review and approval process."  And he's talking about passkeys.directory.



"Second," he said, "I've noticed that many companies in this list are also customers of OwnID, which is listed as the authentication provider, including Carnival Cruises."



LEO:  Oh, interesting.



STEVE:  Yes.  Yes.  They did not do it natively.  And he says:  "And then investigating the OwnID flow."  He said:  "When Leo pressed the fingerprint button, the QR code encoded a URL that sent his iPhone to passwordless.carnival.com with a session identifier.  Then he performed a WebAuthn authentication on his iPhone.  Once completed, the session got updated on the server, and the browser on his laptop logged in.  The flow is using WebAuthn's Passkeys, but not likely the way it was designed to be used.  WebAuthn phishing resistance mechanism works in a way that a Javascript API called on the browser triggers the underlying library and matches the domain a key was registered in and the domain asking to authenticate.



"By implementing WebAuthn as it is in Carnival, the phishing resistance mechanism suffers from a flaw.  As an attacker, you can spoof Carnival's login page.  So the user sees the same page, only a different domain.  When you click the Biometrics button, the attacker's backend will send a request to Carnival to get a QR code which encodes the passwordless.carnival.com.  Then the phone would ask you for your face or fingerprint to authenticate with a Passkey, which will update the session on the backend, and the attacker gets in.  Actually this is a thing that I spent a lot of time on SQRL solving completely, and it's crucial."



He says:  "The right way to implement Passkeys is by calling the WebAuthn API on the laptop's browser," he says, "instead of presenting the QR that will open a browser on the mobile phone, and letting the browser do its job, presenting native WebAuthn screens, including a QR which is scannable from a mobile phone.  This way, the domain you're authenticating to is passed in a side channel" - that is, you know, Push versus BLE, Blue Tooth Low Energy, from the browser to the phone - "to the mobile phone directly from the browser, and a phishing site will be blocked as the credential on the phone was registered under the original domain."



Okay.  So first of all, our listener who wrote this to me is 100% correct.  And by the way, he's a developer for an authentication provider who asked for anonymity.  Another way to say this is that, rather than doing the work of upgrading their own servers to become a first-party Passkeys provider, Carnival Cruises, and unfortunately a lot on that list, has outsourced their authentication responsibility to a third-party provider, in this case OwnID.  But in doing so, by punting in this way, they've bypassed Passkeys phishing protections.  This gives their visitors the false belief that they're getting the hack-proof benefits of Passkeys without actually getting them.  This could be transient.  We can hope not.



But on the other hand, OwnID is in the business of doing this.  So they're going to presumably keep selling their "instant onboarding" services, and most websites will simply want easy login without really caring about their visitors' security.  So we've seen the first way that Passkeys will fail.  And that is, when implemented like this, you can be phished.  And that was a big deal.  It was supposed to be anti-phishing.  Well, it's only anti-phishing if you don't turn the responsibility over to a third party.  And if you do, and this page of people have, you're not getting the benefit of Passkeys.



LEO:  Oh, that's disappointing.



STEVE:  All you're getting is - yeah.



LEO:  But of course to be predicted.  



STEVE:  Yeah, exactly.



LEO:  Yeah.



STEVE:  Christopher Ursich, he said:  "SN topic request:  Hardware Security Modules."  He said:  "You said you had one.  Besides the technical crypto, can you describe how you interact with it in practice to sign your code?"



Sure.  Just as there are EV (Extended Validation) TLS certificates for web servers, there are EV code-signing certificates.  I have no idea whether they are any better or more trusted than non-EV code-signing certificates.  But I'll take every advantage I can get.  And one requirement of EV code signing is that they must, without exception, be protected by a hardware security module so that the EV private key can only ever be used for signing, and cannot possibly escape into the wild.



The EV code-signing key which I purchased from DigiCert was packaged in a Gemalto USB dongle which is paired with the SafeNet Authentication Client.  Somehow, when I use the same Authenticode code-signing command in Windows as I've always used, that SafeNet client is invoked, the hash of the file I'm signing is sent to the key and signed inside there, and it returns a signed blob.  So it's just a matter of having a free USB port and installing a hardware interface client.



Part of the effort which I'll be engaged in toward the end of the work to publish the final SpinRite 6.1 code, which will be like 6.0 is, a hybrid DOS and Windows app, will be automating this code-signing process server side.  Since each owner's copy of SpinRite embeds their license information, which makes their executable unique, each one needs to be individually code-signed on the fly by the server as it's downloaded.



What's going to be really annoying is that Windows Defender will always be complaining, for every single user, that the user-specific custom SpinRite file is not commonly downloaded, thus needlessly warning and alarming its users.  We've seen that no degree of reputable signing is able to bypass this alarm.  I discovered that when I signed the final version of SQRL, and I updated the DNS Benchmark.  You know, people said, "Hey, Windows Defender's not happy."  And I said, "I know."  Doesn't matter if you sign, and those were EV certificate signed.  Windows Defender says, "Oh, haven't seen this a lot before."



And you could understand, it's going to take a hash of the things that you want to download, and it's obviously sharing those in the cloud.  And when it sees enough of those and no complaints, then it goes, okay, it must be okay, and stops bringing up warning messages.  Unfortunately, SpinRite's users are just going to have to get used to that because every one of those that they download is going to be unique.



Two people.  Dangard asked:  "Steve, how can I get access to test the pre-release version of SpinRite 6.1?  Feel free to email me or just respond here.  Thanks so much for your work on SpinRite. I have drives waiting for 6.1."



And sdholden asked:  "Hey, Steve.  Not sure the best way to reach you about the Git server for SpinRite, so I thought I'd start here.  When I try to create an account, I get a dialog box asking me to sign in instead of allowing me to create a registration," he says, dot dot dot question mark.



Okay.  To both listeners and everyone else, in case some of you hadn't noticed, the Internet has, sadly, become a sewer full of both bots trolling constantly and even human labor farms being paid for creating accounts online.  I've been running two web forum servers for years.  Despite having all manner of entrance barriers erected, like even requiring the correct answer to the question "What software is Steve best known for?" in order to create an account, five out of six of the account registrations were bogus in those forums.  Like, how does a bot know?



LEO:  How hard is that?  How hard is that?  Yeah, a bot wouldn't know, but...



STEVE:  No.  I know.  At one point we had 6,500 users registered in GRC's forums.  And I was thinking, wow, I haven't even talked about it that much.  Okay.  Now that number is a bit over 1,100, after I spent several days working to get that under control.



LEO:  Yeah.



STEVE:  5,500 of those were registered in Afghanistan and Turkey and Indonesia.  I mean, just like - and Russia.  And, you know, just it was so annoying.



LEO:  Spammers love forums.  They really do.



STEVE:  Oh, my god, yes.  So I've erected much tougher barriers since, and I've mostly gotten it under control.  And since I erected those stronger barriers, 20,204 additional account creation attempts have been thwarted.  So I have an additional 20,000 bogus users on top of the 5,500 I had before.  The reality is that, today, as you said, Leo, running any sort of open web service results in a torrent of bogus registrations.  And even with all that in place, the wonderful volunteer moderators I have, who make time to read everything, are still removing users who attempt to subtly pollute our content.



So here's the problem.  GRC's forums need to be open.  So I have no choice other than to erect the strongest account creation barriers I can, then apologize to those whom we mistakenly reject as false positives, and also weed out those who do slip past the barriers due to false negatives.  But GRC's GitLab server has no need to be open.  So it's closed.  Its account creation page is protected by a magic incantation which must be provided before the troll that guards the bridge will allow newcomers to pass.  It requires insider information which can only be obtained by participating in GRC's old-school, blessedly wonderful text-only NNTP newsgroups.  Once someone shows up there and is able to post, they can ask how to satisfy our cantankerous GitLab troll.  But also note that we're not using GitLab for any social interaction.  We're only using it for issue management.



At this point, what I need is feedback from people who are testing SpinRite 6.1.  Since we have a handful of known issues to fix - I'll get to that in a moment - it's best for newcomers to join and catch up on all the various threads in the newsgroup in order to eliminate duplicate postings of already-known problems.  So if anyone is really and truly interested in participating in SpinRite 6.1's testing, you're invited to head over to GRC's Discussions page - that's the page at GRC, if you google "GRC.com discussions" it'll take you there - and create a connection to our news server.  Find the grc.spinrite.dev group and say hi.



And speaking of SpinRite, it's working.  As I planned, I updated GRC's primary server to handle downloading of pre-release versions of SpinRite, and last Friday morning, after Thanksgiving, I posted the information in GRC's spinrite.dev newsgroup about where any existing SpinRite owner could go to grab their own copy.  I'll share three newsgroup anecdotes which I've edited just a bit for podcast clarity.



A few hours after my first release announcement, someone whose handle is DarkWinX posted on Friday at 2:44 p.m.:  "Well I can already report success with a USB.  In my race to find something to eagerly test on, with the short time I had, I grabbed an old USB I received with the purchase of StarCraft II.  I figured I'd reformat it with InitDisk and run SpinRite from there.  So I put it in the computer and started InitDisk.  It waited, and waited, for about 30 seconds.  Eventually the USB was recognized by Windows and showed up so I could nuke it.  I tried it again and it still took around 30 seconds to load.  So I figured maybe not the best USB to run SpinRite from.  So I found another.  I thought, why not run SpinRite on the problem USB as a target, so that's what I did.  After a Level 2 scan, without finding anything wrong, I rebooted, plugged it in, and instant success.  That USB now loads inside Windows instantly every time.  Looking forward to testing some more."



Second comment, Saturday morning, 8:39, Mark Ping posted:  "Finished the Level 2 in two hours for a 1TB.  Then ran Level 4, and it took nine hours, 37 minutes for 1TB, compared with 150 hours before."  And then he finished:  "SpinRite is back, baby."



And finally, Dale F., Saturday evening at 10:12 p.m., posted:  "I have a 500MB laptop drive that I put in a SABRENT portable enclosure.  After I dropped it about two years ago, it could not be recognized by any PC, or by SpinRite 6.0.  So I said to myself, 'Just have to wait for 6.1.'  On Friday, I ran a Level 2 with SpinRite's first alpha release, and one hour later it was good as new.  Thanks, Steve."



Okay.  So frankly, SpinRite's first functional pre-release debut could not have gone much better, and it went far better than it might have.  Over the weekend, using the feedback provided by the large group of avid testers, we moved SpinRite through three more releases to its fourth alpha release by mid-afternoon on Sunday.  And with only a few exceptions, it is now working well for everyone.  Overall, it's 100% functional in every way that matters.  There are a number of things that I need to fix, like SpinRite's various clocks are not continuing to operate while it's deep into data recovery.  I recently rewrote that entire data recovery system, and I just forgot to periodically update the clocks while I was in there.  So actually I'm going to change the entire way that works so that it's much better.



Another example is that SpinRite's predictions of its remaining time to run is not working right when it's started midway into a drive rather than at the beginning.  You could start it wherever you want to.  Anyway, it was working once, and something I did broke that.  So I'll fix that.  So right now the newsgroup gang is continuing to pound away on the fourth alpha release, logging everything they encounter in our GitLab instance.  While that's underway, my own now highest priority is to make a decision about that next operating system that I'm considering purchasing and moving to.  Its licensing deadline, as I mentioned before, is the end of the year.  It's either by then or never.



So I expect that to take - that's what I'm going to be doing this evening.  I'll start that.  I only think it'll take a couple days.  I just want to make sure that I can boot something, you know, the classic "Hello World" app, both from a BIOS and from a UEFI-based machine.  Then that says yes, I'm going to go with this OS.  Then I'll return and get SpinRite's DOS executable completely finished.



I should mention, I told you this, Leo, before we began recording today.  One thing happened this morning that completely caught me off guard.  I hired Greg, who everyone has heard me refer to through the years, 32 years ago tomorrow.  Tomorrow is his 32-year anniversary of employment with GRC.  That means that tomorrow he will have been providing technical support for SpinRite for 32 years.  Yesterday he fired up the latest SpinRite 6.1 alpha.  He'd never seen it before.  He's seen nothing until, you know, I've been keeping him and Sue appraised of what was going on.  I sent them both an email saying, "Well, it works, somewhat to my amazement."



So he fired up the latest SpinRite 6.1 alpha, ran it on a bunch of drives he had around.  He said that he ran it on a 1TB spinner which took about two hours.  That's about right.  Remember I've thought about half a terabyte per hour is good performance for a spinning drive.  And that certainly beats two weeks.  And still it wasn't instantaneous because it was a spinning drive.  Then, he said, he scanned a 128GB SSD in five minutes, and he was stunned.



So he told me on the phone this morning that he knows the number one question he is certain people are going to be asking, once SpinRite's previous users start using 6.1, is how SpinRite 6.1 could possibly be so much faster?  It was like, it's like the difference is too much to believe.  You know, either 6 was like way slow, or is 6.1 actually doing anything?  On the other hand, I should also mention that a number, a whole bunch of people in the newsgroup have actually had it recovering data, recovering drives, things that could never be copied before.  We're seeing green R's on the map showing data was problematical and was recovered.  So anyway, I'm very excited that I will be able to soon stop talking about it and have it in everybody's hands.



LEO:  Woohoo.



STEVE:  Yeah.



LEO:  Very, very good news.  Thank you for the hard work.  



STEVE:  Well, thank everybody for their support.  I really appreciate it.



LEO:  Yeah.  So you said NNTP, your newsgroups are NNTP.  I thought it was XenForo.  Or does XenForo use NNTP?  Is that why?



STEVE:  No, XenForo is the web forums.



LEO:  Oh, you have newsgroups in addition to the web forums.  I get it.



STEVE:  Yes. 



LEO:  I get it.



STEVE:  Yes.  Newsgroups I've had forever, and I love them.  They're little backwater...



LEO:  Yeah, they sure are.



STEVE:  We get real serious work done.



LEO:  How do you read a newsgroup these days?



STEVE:  Thunderbird is a really good newsgroup reader.



LEO:  Okay.



STEVE:  It does a good job of it.  On the discussions page, I list - I asked the question of everybody, like six months ago, and there's like a list of maybe 30 different NNTP clients.  There's only one for iOS which is called NewsTap.  It's a great little newsreader for iOS.  There's a bunch of newsreaders for Android, and a bunch of Linux and Mac and Windows.



LEO:  And then you host it.  It's on your GRC site.



STEVE:  Right.  It's news.grc.com.



LEO:  Nice.



STEVE:  And that's been one of the things I've had, well, okay.  So here's the reality.  SpinRite 6.1 will ship.  It will be perfect.  The newsgroups are why it will be perfect.  In this day and age, once upon a time, you know, back when we had DOS 2 or 3 or 4, I could write a program, and it would work everywhere.  Those days are gone.



LEO:  Yes.



STEVE:  I could never...



LEO:  You need testers nowadays



STEVE:  I could never do this if it weren't for the guys in the newsgroup.  As I said before, I've got, like, all these motherboards around now, and all these old hard drives, because it was like, uh, Steve, the ASUS Cranox 327 isn't working.  So I'd go onto eBay, ASUS Cranox 327.  Oh, yeah, there it is.  And I buy it.  You know?  So Lorrie is saying, do we still need all these?  I go, no, honey, just a little bit longer.



LEO:  A little bit longer.



STEVE:  A little bit longer.



LEO:  Little bit longer.  Yeah, used to be that all the browsers could handle newsgroups.  But they've slowly stripped that out of every browser.



STEVE:  And of course FTP is gone now, too.



LEO:  It's gone, too, that's right.  They take all - and reasonably so.  If nobody uses it, it's just a security...



STEVE:  But a good generic newsreader is Thunderbird.  It's multiplatform, and it's pretty good for getting the job done.



LEO:  Good.  I'll have to check out the newsgroups.  For some reason I spaced that you have a newsgroup.  I thought it was all forums.  Which forums are fairly old-fashioned.  Newsgroups are positively antediluvian.  That's good.  I like it.



STEVE:  Yes.  And the forums are where support will be for SpinRite. 



LEO:  Right.



STEVE:  I'm going to engage community support.  But I'm never going to allow, I mean, like the newsgroups are my sanctum sanctorum.  Is that the right term?



LEO:  Do you still - does you use UUCP and send it off and everybody in the world gets to see it?  Or is it just hosted on your site?



STEVE:  Actually, we block it going anywhere else.  



LEO:  Yeah.  Yeah, yeah, okay.



STEVE:  Because Google Groups would like to be pulling from an NNTP server.  The problem is people were responding to postings that Google had sucked out, and nobody was ever seeing their responses.



LEO:  Right, right.



STEVE:  So it is closed.  I actually have a technology where the IP address of the entity which pulls the article is added to the headers.  So if we ever see postings out in public, we can look at the headers and see the IP address that is pulling them, and then I block them.



LEO:  Oh, so smart.  So there.  Wow.  So it's really, I mean, to call it a newsgroup is really not exactly right because the whole idea was newsgroups were federated, and they would be copied every night from university to university.



STEVE:  Oh, and we've got - I've written a whole bunch of extra code.



LEO:  You just use the NNTP protocol for your server. 



STEVE:  Yes.  We have something called a CECIL-ID, which is also added to a posting, which is a hash of the person's username and password, which allows the postings to be owned by them.  Nobody else can delete them, but they can delete their own.  



LEO:  Perfect.



STEVE:  And there's a whole bunch of other, you know, benefits that we've added over time.



LEO:  Yeah, yeah.  Very interesting.



STEVE:  I just, you know, I will - that's what I'll be using, like...



LEO:  Forever.



STEVE:  When somebody comes along to turn off the servers after I'm gone, they'll be shutting down the newsgroups.



LEO:  Oh, that'll be sad.  All right, Steve.  Always a pleasure.  He does it the old-fashioned way.  He does it the old way.  But the old ways are often still the best.  Steve Gibson is at GRC.com, along with his newsgroups.  That is the Gibson Research Corporation.  You'll find SpinRite there, the world's best mass storage recovery and maintenance utility, now faster than ever.  It's really working.  It is.  It's really doing something, honest.  If you don't have a copy, get 6.0 now, you'll have a free upgrade to 6.1 when it comes out.  You can also participate in the development and all of that, as he said.  GRC.com.



While you're there, you can get a copy of this show.  Security Now! is hosted at TWiT.tv but also at GRC.com.  Steve has two unique versions, a 16Kb audio version for the bandwidth-impaired.  He's always done that from day one.  And for his transcriptionist, actually, Elaine Farris, because she writes this all out, and she's living in the country with a lot of horses, doesn't have a lot of bandwidth.  You can get the transcripts there, as well, GRC.com, as a 64Kb audio file.  We have audio and video at our website, TWiT.tv/sn.



There is a YouTube channel for Security Now!.  That's a great way to introduce somebody to it, or if you hear something on here you want to share with other IT professionals, your boss or friends, your spouse, then just clip it at YouTube.  That's probably the easiest way to do it.  They make that a fairly simple thing to do.  Of course subscribing in your podcast client might even be the best way to get it.  That way you'll get it automatically the minute it's available.  You can build your collection of all 899 episodes.  Whew.  That's a lot of episodes.



Steve, we will be back here next Tuesday, 1:30 Pacific, 4:30 Eastern, 21:30 UTC.  Had to do the math.  You can watch us live, live.twit.tv.  Chat with us live at irc.twit.tv.  Or if you are fortunate enough to be in the Club, you can do it in the Club TWiT Discord.  Actually, you should join the Club, if you're not already a member.  It supports Steve's efforts plus everything we do here.  $7 a month for ad-free versions of the show, access to the Discord.  You also get stuff that we don't put out in public, like Hands-On Macintosh, Hands-On Windows, the Untitled Linux Show and all of that.  Thank you, my friend.



STEVE:  And Leo?



LEO:  Yes.



STEVE:  Happy Birthday again.



LEO:  Thank you.



STEVE:  For your 66th.  I want you to hold onto that sign so that in 33 years you can turn it upside down and celebrate 99.



LEO:  99 etouR.  Good thinking, Steve.  I'll save that.  I bet you save old calendars, too, don't you.



STEVE:  No.



LEO:  Thank you, Steve.  Have a great week.  We'll see you next time on Security Now!.



STEVE:  Bye.



Copyright (c) 2022 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.










GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#900

DATE:		December 6, 2022

TITLE:		LastPass Again

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-900.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we answer a few questions.  What if an Australian company doesn't secure their own network?  Has Ireland NOT levied fines against any major Internet property owned by Meta?  What's in REvil's complete dump of Australia's Medibank data disclosure?  We finally answer the question, is nothing sacred?  (It turns out it's not rhetorical.)  Also, whose root cert just got pulled from all of our browsers, and how did a handful of Android platform certs escape?  What U.S. state has banned all use of TikTok?  What country is prosecuting its own ex-IT staff after a breach?  How has memory-safe language deployment actually fared in the wild?  Are last August's Black Hat 2022 videos out yet?  And which brand of IoT security camera do you probably NOT want to use or purchase?  Which podcast had the most amazing guest last week?  What happened when SpinRite was run on an SSD?  And what does LastPass's announcement of another hacker intrusion mean for it and its users?  Answers to those questions and more coming your way during this week's Security Now! podcast.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  You've got questions?  Steve's got answers.  A lot of questions.  Australia's stiff fines for not disclosing a break-in.  What country is prosecuting its own ex-IT staff for a memory breach?  Which podcast had the most amazing guest last week?  I wonder.  And what happened when SpinRite was run on an SSD?  Plus an analysis of LastPass's recent revelation of an attacker intrusion.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 900, recorded Tuesday, December 6th, 2022:  LastPass Again.



It's time for Security Now!, the show where we cover your security, your privacy.  We explain things.  We talk about the world as it is with this fantastic fellow right here, Mr. Steve Gibson.  Hello, Steve.



STEVE GIBSON:  Hello, Leo.  You noted, and I had previously seen, that we are now at Episode 900.	



LEO:  Wow.



STEVE:  Yeah.  So for the beginning of December.  And there was little doubt that I had to title today's podcast LastPass Again.



LEO:  Yeah.



STEVE:  Yeah, boy.



LEO:  Holy cow.  Yeah, this was breaking news at the end of the episode last week.  And now we've had - you had time to look at it.



STEVE:  Yup, yup.  So...



LEO:  And I still don't know if we know exactly what happened, but I'm looking forward to hearing you.



STEVE:  In fact, yes, we will get there.  We're going to answer a few questions.  And when I looked at the word "few," I thought, wait a minute, we're doing more than a few.  What if an Australia company doesn't secure their own network?  Also, has Ireland not levied fines against any major Internet property owned by Meta?



LEO:  No.



STEVE:  What's in REvil's complete dump of Australia's Medibank data disclosure?



LEO:  Oh.



STEVE:  We finally answer, Leo, the question...



LEO:  Yes?  Yes?



STEVE:  Is nothing sacred?  It turns out it's not rhetorical. 



LEO:  Oh.



STEVE:  Also, whose root cert just got pulled from all of our browsers?  And how did a handful of Android platform certs escape?  What U.S. state, I kid you not, has banned all use of TikTok?  



LEO:  What?



STEVE:  Uh-huh.  What country is prosecuting its own ex-IT staff after a breach?  How has memory-safe language deployment actually fared in the wild?  Are last August's Black Hat 2022 videos finally out yet?  And which brand of IoT security camera do you probably not want to use or purchase?



LEO:  Oh.



STEVE:  Which podcast had the most amazing guest last week? 



LEO:  Hmm.



STEVE:  Uh-huh.  What happened when SpinRite was run on an SSD?  And what does LastPass's announcement of another hacker intrusion mean for it and its users?  Answers to those questions and more coming your way during this week's Security Now! podcast.



LEO:  You've got questions.  Steve has answers.  Inquiring minds want to know.  Well, that's why we tune in the show, right, every week.



STEVE:  And why is nothing sacred?



LEO:  I can't wait to hear that one.  Some of those I think I know the answer to, but we'll see.  Go ahead, sorry.



STEVE:  Our Picture of the Week...



LEO:  Oh, I haven't looked yet.



STEVE:  ...is a great one.  So the caption that came with it, it was perfect.  So it reads:  "If you've ever messed up a dimension or a hole position on something you're building, don't be too hard on yourself.  At least you're not the Cisco design engineer..."



LEO:  Oh, dear.



STEVE:  Un-huh, "...who caused an entire product line recall by placing the mode button directly above an RJ45 port."



LEO:  Oh, man.



STEVE:  "That button resets the switch to its factory default settings when it's held down."



LEO:  What happens when you plug it in Ethernet with a lock?



STEVE:  Yes, well, yeah.  So for those who can't see the picture, what we see is an RJ45 plug in the first port of the switch.  Now, if it were just a minimal RJ45, that would be fine.  That was what the engineer was thinking.  But if you put one of those rubber boots on it, where you have that rubber flap that comes over and protects the...



LEO:  The lock, yeah, yeah.



STEVE:  ...plastic lock; right?  Then what happens is that thing is perfectly positioned over the factory reset button.



LEO:  Oh, my god.



STEVE:  So if you were then to lift the cord up, that would rotate the plug so that the protective boot pushes the factory reset button down.



LEO:  And holds it, holds it down.



STEVE:  And holds it down, returning the switch to its, like, where the login and password, or cisco/cisco...



LEO:  I don't understand why every time I set up our Cisco router, it goes back to factory settings.  I don't understand it.



STEVE:  Yeah, what could be wrong?  Oh, goodness.  Anyway, great Picture of the Week.  Thank you, listener who sent that to me.



Okay.  So continuing our recent Australia watch, recall from last week that a recent cybersecurity country ranking which was published by MIT gave Australia the number one slot for Cyber Defense, followed by, in decreasing order, the Netherlands, South Korea, U.S., and Canada.  So Australia number one.  We recently covered Australia's proactive declaration of cyberwar, which they'll be waging against the world's perpetrators of cybercrime, not waiting for them to commit another crime; but, you know, going after them.  You know, it would seem that these high-profile attacks on Optus, Telstra, Medibank that we'll be talking about a little bit later, Woolworths, and EnergyAustralia really woke up the sleeping bear and galvanized Australia into action.  They've decided to go active.



Last week saw another facet of this campaign, with the creation of new legislation to replace Australia's creaky 34-year-old Privacy Act of 1988.  The new legislation ups the ante when Australia's own internal attack targets, like those companies just mentioned, turn out to be willfully negligent.  The legislation bears the cumbersome name "Privacy Legislation Amendment (Enforcement and Other Measures) Bill 2022."  It grants the Office of the Australian Information Commissioner, the OAIC, the power to levy hefty fines on companies  and not only Australian companies, which is interesting, we'll get to that in a second  which ignore security best practices to needlessly expose their customers' data through cybersecurity breaches.



Under this bill, which is expected to receive royal assent shortly to place it into force, companies that fail to safeguard their data face fines of up to the greater of 50 million AUS or 30% of the company's adjusted revenue, whichever is greater.



LEO:  Wow.



STEVE:  Yeah.  So if you have more than 30 - if 30% is greater than 50 million AUS, that's what you've got to pay.  So, now, this is a huge change.  The existing fines impose only 2.22 million AUS as a fine as a result of security breaches.  So we're going from 2.22 million to 50 million, or 30% of revenue if 30% is more than 50 million.



Okay.  So steep as this is, the updating of Australia's antiquated legislation has, not surprisingly, been greeted with positive feedback from Australian cybersecurity experts, who view it, and I think reasonably, as the incentive needed to get local companies to pay the attention that they must to the state of their IT systems.  Right?  I mean, it's like, oh, we're sorry, like when a breech happens.  It's like, well, if you'd worked to fix this in the first place proactively, then you wouldn't have to be sorry.  And Australian citizens wouldn't have had all of this data lost.



So given the historical reticence that we keep seeing to getting ahead of this problem - nobody does - I can't see any other way to bring about the changes we need.  It does still feel a bit wrong for the suppliers of the too-often buggy systems which enterprises rely upon to continue to be held harmless.  But that's a bridge we're not prepared to cross yet.  So at this point we're going to say, well, were you patching?  And that's the other problem, too, that is uncomfortable here, is how is this decision about negligence reached because it's a thorny problem.



I mentioned before that this law wasn't applicable only to Australian companies.  According to the bill's text, its provision and fines will also apply to any non-Australian company who's doing business in Australia, even if they're headquartered outside of Australia.  And that one promises to prove interesting.  You know, Australia fining some other company who's doing business in Australia, who exposes the private information of Australian citizens, 50 million AUS.  Wow.



Okay.  While we're on the topic of fines, Ireland's data protection agency fined Meta 285 million euros due to Facebook's data breach a year and a half ago, in April of 2021.  The Irish Data Protection Commission said that Meta failed to safeguard its Facebook platform from data scraping.  Which, as we know, data scraping is just like bots, spiders, get in and scrape all of the pages of Facebook.  Anyway, this Irish Data Protection Commission alleges that this allowed a threat actor to compile details on more than 530 million Facebook users.  This data was later sold on an underground cybercrime forum, thus bad guys profited.  Facebook told TechCrunch that following the incident they rolled out protections to detect scraping operations.  To which I would ask, again, why not before this?  I mean, it really does seem that we're having to, like, severely punish these tech companies in order to get their attention and get them to change their behavior, crazy as it seems after the fact.



Since Ireland had previously fined Meta's Instagram 405 million euros in September of also this year, and WhatsApp got a fine of 228 million euros in the previous September, this now rounds out the fines so that Ireland's data protection agency has now fined all or each of Meta's three main platforms.  So that answers the question we posed at the beginning:  Is there any major property of Meta that Ireland has not yet fined?  The answer now, no.  They've got them all.



And speaking of Medibank and Australia, okay, remember Medibank, as I mentioned, was one of the several ransomware embarrassments that Australia-based organizations recently suffered.  Well, Medibank stood up to the REvil gang, as everyone thinks they should, if you can, refusing to pay or to buckle under to REvil's extortion threats.  They threatened to release the entire contents of what had been illegally stolen from Medibank.  So Medibank is Australia's largest private health insurer.  And what's known is that the significant personal data for Medibank's 9.7 million current and past clients was stolen during REvil's original intrusion and data exfiltration.



So last Thursday Medibank released a lengthy statement.  I won't bother with it all, but it began:  "We're aware that stolen Medibank customer data has been released on the dark web overnight.  We are in the process of analyzing the data, but the data released appears to be the data we believed the criminals stole.  Unfortunately, we expected the criminals to continue to release files onto the dark web.  While our investigation continues, there are currently no signs that financial or banking data has been taken.  And the personal data stolen in itself is not sufficient to enable identity theft" - although when you hear what it is, you'll maybe question that - "and financial fraud.  The raw data we have analyzed today so far is incomplete and hard to understand."  Okay.  We'll get back to that because I thought that was an interesting thing that they said.



A bit later in their released statement, they start quoting Medibank's CEO David Koczkar.  And so they wrote his quote, saying:  "Anyone who downloads this data from the dark web, which is more complicated than searching for information on a public Internet forum, and attempts to profit from it, is committing a crime.  The Australian Federal Police have said law enforcement will take swift action against anyone attempting to benefit, exploit, or commit criminal offenses using stolen Medibank customer data.  We continue to work closely with the Australian Federal Police, who are focused, as part of Operation Guardian, on preventing the criminal misuse of this data.  Again," he says, "I unreservedly apologize to our customers.  We remain committed to fully and transparently communicating with customers, and we will continue to contact customers whose data has been released on the dark web."



Okay.  At the end of the statement, Medibank then - and the statement goes on and on and on, like at great length - they enumerate the sobering details of what they believe the REvil gang both obtained, and a little bit of what they did not obtain.  So on the daunting bit that they did get, they said, the name, date of birth, address, phone number and email address for around 9.7 million current and former customers, and some of their authorized representatives.  This figure represents around 5.1 million Medibank customers, 2.8 million ahm customers, and around 1.8 million international customers.  



Also, Medicare numbers for ahm customers, though not the expiration dates; passport numbers; and the travel visa details for international student customers, though again not expiration dates.  Health claims data for around 160,000 Medibank customers, around 300,000 ahm customers, and around 20,000 international customers.  This includes service provider name and location, where customers received certain medical services, and codes associated with diagnosis and procedures administered.



Additionally, around 5,200 My Home Hospital (MHH) patients have had some personal and health claims data accessed, and around 2,900 next of kin of these patients have had some contact details accessed.  Also health provider details, including names, provider numbers, and addresses.  So, you know, wow.  A huge breach.



On the flipside they said that REvil did not access primary identity documents such as drivers' licenses for Medibank and ahm resident customers.  Medibank did not collect primary identity documents for resident customers except in exceptional circumstances.  REvil also did not access health claims data for extras services such as dental, physio, optical and psychology.  And they did not access credit card and banking details.



Okay.  But, wow, they still got a lot; right?  Lots of stuff on 9.7 million individuals.  What most caught my attention was their statement that "The raw data that we have analyzed today so far is incomplete and hard to understand."  What occurs to me is that a raw unorganized dump of data concerning 9.7 million current and past customers is far less actionable than an organized searchable online database containing the same information.  In other words, it's almost entirely the structure of the data, I mean, that much data, that gives it meaning and makes it useful.



If REvil grabbed raw data files without formatting templates and indexes into the data, if the database is highly relational in nature and deeply depends upon the interrelationships of pieces of it and the indexes into it in order to pull it together into something coherent, then the release of a massive blob of raw and disorganized data, where there's nothing to make clear what pieces goes with which, might be much less damaging than it at first appears.  Which is interesting.  It sort of says that, you know, while the data is in place, it's actually useful to the organization that owns it, and that knows how to interpret it.  But if you just grab a static blob, a static file, then there may not be, I mean, it's all there.  But if you can't, like, find all the little bits and pieces which get pulled together by having this data understood by the database and databases that contain it, then it's probably not so useful to anyone.  Anyway, just interesting to have this actually happen.  Okay.  Leo?



LEO:  Yes?



STEVE:  I couldn't resist giving this story, this short bit of news, the heading "Is Nothing Sacred?"  



LEO:  Okay.



STEVE:  Because the official website of the Vatican was pushed offline last Wednesday.



LEO:  Oh.



STEVE:  By a DDoS attack.



LEO:  Is nothing sacred?



STEVE:  Exactly.



LEO:  I'm sure Father Robert had a late night that night, I would bet.



STEVE:  Yeah.  It was pro-Russian hacktivists.  And I had to look this up.  CNA, all right, the Catholic News Agency.



LEO:  Oh, well, there you go.  They're serious.



STEVE:  CNA, the Catholic News Agency, pointed out the attack came a day after Moscow criticized Pope Francis's latest condemnation of Russia's invasion of Ukraine.  So, no, nothing is sacred.  And if the Vatican says, "Putin, you are bad," well,  get ready to be blasted off the Internet for a while.



Okay.  Mozilla yanks a no longer trusted root.  As we know - actually, not only Mozilla, but everybody else - because we've covered this through the years, web browsers are extremely reticent.  Okay, it's not the web browsers, right, it's the people who manage them.  But web browser teams, I should have said, are extremely reticent to remove root certificates - actually, Leo, maybe one day web browsers themselves will be sentient and will then decide whether they should trust a root certificate.



LEO:  I'm sorry, Dave.  I can't go there.



STEVE:  I don't trust that website any longer, and you shouldn't either.  Okay.  So they are reticent to remove root certificates from their trusted root stores because doing so immediately renders invalid and not trusted any and all outstanding certificates which have been previously signed by that certificate's authority using their matching private key.  Pulling the trust from the root effectively puts the certificate authority out of business overnight.  And as we know, this has happened a few times since the start of the podcast, and it's always interesting.



In this case, the certificate authority in question is an apparently shady Panamanian firm called TrustCor.  Nearly a month ago, long simmering questions about TrustCor were brought to a boil by a piece in the Washington Post whose headline was "Mysterious company with government ties plays key Internet role.  TrustCor Systems vouches for the legitimacy of websites, but its physical address is a UPS Store in Toronto."



LEO:  Oh, that's not good.



STEVE:  Whoa.



LEO:  Oh, boy.



STEVE:  No.  That'll get your attention.  Here's just a sampling of the juicy bits from The Washington Post's reporting.  And for what it's worth I've got the link in the show notes.  I really recommend any of our listeners who enjoy gossip based in facts, this thing is just - this reads like you couldn't make this up.



Okay.  The Washington Post wrote:  "The company's Panamanian registration records show that it has the identical slate of officers, agents, and partners as a spyware maker identified this year as an affiliate of Arizona-based Packet Forensics, which public contracting records and company documents show has sold communications interception services to U.S. government agencies for more than a decade.



"One of those TrustCor partners has the same name as a holding company managed by Raymond Saulino, who was quoted in a 2010 Wired article as a spokesman for Packet Forensics.  Saulino also surfaced in 2021 as a contact for another company, Global Resource Systems, that caused speculation in the tech world when it briefly activated and ran more than 100 million previously dormant IP addresses assigned decades earlier to the Pentagon. The Pentagon reclaimed the digital territory months later, and it remains unclear," wrote the Post, "what the brief transfer was about, but researchers said the activation of those IP addresses could have given the military access to a huge amount of Internet traffic without revealing that the government was receiving it."



And our listeners may recall that we talked about this weird event at the time, noting how odd it was that this previously dormant and DoD-reserved block of IPv4 address space was suddenly being routed and tied to some random private company no one had ever heard of.  Anyway, the Post continues:  "TrustCor's products include an email service that claims to be end-to-end encrypted, though experts consulted by The Washington Post said they found evidence to undermine that claim.  Researchers said that a test version of the email service also included spyware developed by a Panamanian company related to Packet Forensics.  Google later banned all software containing that spyware code from its app store.



"A person familiar with Packet Forensics' work confirmed that it had used TrustCor's certificate process and its email service, MsgSafe, to intercept communications and help the U.S. government catch suspected terrorists.  Speaking on the condition of anonymity to discuss confidential practices, the person said, "Yes, Packet Forensics does that."  And come on. The name "Packet Forensics" should be an obvious enough tell.



LEO:  It should tell you everything you need to know; right?  Yes.



STEVE:  Yes, about the company's intentions.  Remember, any device that's holding a certificate which is able to sign other end certificates is thereby able to intercept any and all TLS-secured traffic bound for any remote web server.  It accepts the connection, the TLS connection; examines the domain being requested; creates and signs a TLS certificate on the fly; and returns it to the browser.  In this case, so long as all web browsers contained the TrustCor CA root certificate, they would happily accept that on-the-fly signed certificate.  So the connection to the intercept middlebox, as they're called, would be encrypted where the middlebox would decrypt the TLS data for completely in-the-clear analysis.



LEO:  It's a man-in-the-middlebox attack.



STEVE:  That's exactly what it is.  The middlebox would then initiate its own connection to the actual destination server so that its interception was invisible, while it continued to surveil all of the intercepted browser's traffic.



LEO:  Oh, my god.  Oh, dear.



STEVE:  So what is really, I mean, it's heartwarming to see how long the thread was in the Google group's back and forth, while they explained very patiently to the TrustCor representative, who kept trying to rebut all of their evidence that, you know, we're going to pull the plug on you.



LEO:  Yeah.  It's pretty obvious, yeah.



STEVE:  Because this is not okay, yeah.



LEO:  How did he get in in the first place?  That's my question.



STEVE:  Well, I mean, apparently if you have enough money - there was a quote somewhere later in this long Washington Post article.  If you had enough money, basically you could buy yourself certificate authority privileges.  And, you know, essentially they don't want to deny a company just out of hand who, like, passes all the requirements and certifications and looks like they're going to be a reputable and reliable certificate authority.  It's like, well, you know, why should we say no to them?  So...



LEO:  I would have hoped there was a better way to vet certificate authorities given the power that it gives them.



STEVE:  I know.  And, you know, Mozilla currently recognizes 166 root certificates, but no longer the three from TrustCor.



LEO:  Oh, good.



STEVE:  Our really long-time listeners may recall that episode...



LEO:  The Hong Kong Post Office Episode.  You don't have to - say no more.  I know exactly where you're going.



STEVE:  That followed my chance discovery of what was then the explosion in certificate authorities in Firefox's root store.  The time before when I had looked, there were like seven or eight certificate authorities.  Now there appear to be hundreds.  And as I said at the time this is inherently not good.  All web browsers are trusting any certificate signed by the owners of any of these root certs.  It makes it an inherently unstable system.  But in fairness, you'd have to say that things have gone much better, certainly than I expected.  The industry has been amazingly effective at policing itself.  And the events, you know, of these trusted root store abuses have been very few and far between.  You know, it's an obvious privilege to be granted certificate authority rights.  It's a license, essentially, to print money, but only so long as the certificate authority's signature means something.



Okay.  So the Washington Post's story is not behind any paywall, and it reads, as I said, like someone's imaginative fiction, being well researched and backed by facts.  I've got a link, as I said, in the show notes for anyone who's curious to know more.  And also I have a link to the Google group's discussion, which...



LEO:  It sounds pretty good.  I've got to read that.



STEVE:  Oh, yeah.  All of the industry's participants, including TrustCor's, you know, TrustCor's representative was there, trying to rebut this.  But ultimately they lost the argument because the evidence was just there.



LEO:  Yeah.



STEVE:  You know, it just - it went on and on and on.  And they said, look, sorry, but we can no longer trust you to sign anything.



LEO:  This initial - the kickoff post has 34 references, footnoted references.  That's a pretty good start.



STEVE:  Yeah.  Yeah.



LEO:  That's not just something off the cuff.  I think this is a problem.



STEVE:  Again, you have to take your hat off to these guys.  They do not yank this privilege casually.



LEO:  Yeah.



STEVE:  I mean, you really, you know, they fret and worry and make sure that this, you know, that it wasn't a one-off sort of event.



LEO:  In fact, there's almost a smoking gun connecting TrustCor to spyware.



STEVE:  Yes, yes.



LEO:  I mean, it's pretty bad.  I mean...



STEVE:  Yes.



LEO:  A UPS post box in Toronto.



STEVE:  Oh, my god.



LEO:  Holy cow.  Holy cow.  Oh, here's the TrustCor response, yeah, yeah.  Oh, I'm going to read this.  That's some good late-night reading.  Thank you.



STEVE:  Yeah, it really is good.



LEO:  Yeah, yeah.



STEVE:  Okay.  So one more, and then we'll take our next break.  While we're on the subject of crucial certificates and certificate management, last Wednesday the 30th, an internal Google report which was originally created on the 11th of November was made public.  And two days later, on Friday, the security firm Rapid7 pulled the pieces together.  Google's report is titled "Platform certificates used to sign malware."



And under Technical Details of Google's report they said:  "A platform certificate is the application-signing certificate used to sign the 'android' application on the system image.  The 'android' application runs with a highly privileged user ID - android.uid.system, basically it's like the root - and holds system permissions, including permissions to access user data.  Any other application signed with the same certificate can declare that it wants to run with the same user ID, giving it the same level of access to the Android operating system."  In other words, it's a full penetration of Android security.



Digging a bit deeper, we find that the Android Security Team discovered several malware samples in the wild that were signed by platform certificates issued by major vendors including Samsung, LG, MediaTek, and Revoview.  After discovering the incident, the Android Security Team worked with the affected companies to revoke and rotate the leaked platform certs.



I liked what Rapid7 had to say about this because what they said made a lot of sense about what didn't make a lot of sense about the whole escapade.  Here's what they wrote.  They said:  "On November 30, 2022, a Google report initially filed on November 11th was made public.  The report contained 10 different platform certificates and malware sample SHA-256 hashes, where the malware sample had been signed by a platform certificate, the application signing certificate used to sign the 'Android' application of the system image.  Applications," they wrote, "signed with platform certificates can therefore run with the same level of privileges as the 'Android' application, yielding system privileges on the operating system without user input.  Google has recommended that affected parties should" - I be they've recommended.  "Affected parties should rotate their platform certificate.  However, platform certificates are considered very sensitive," Rapid7 wrote, "and the source of these certificates is unknown at this time."



They said:  "This use of platform certificates to sign malware indicates that a sophisticated adversary has gained privileged access to very sensitive code-signing certificates.  Any application signed by these certificates could gain complete control over the victim device.  Rapid7 does not have any information that would indicate a particular threat actor group as being responsible; but historically, these types of techniques have been preferred by state-sponsored actors."  Meaning, right, like those like at the top of the food chain.  "That said," they wrote, "a triage-level analysis of the malicious applications reported shows that the signed applications are adware, a malware type generally considered less sophisticated.  This finding suggests that these platform certificates may have been widely available, as state-sponsored actors tend to be more subtle in their approach to highly privileged malware."



Okay.  So some low-end malware adware was somehow signed by like the most closely guarded private keys belonging to some of Android's largest and most reputable vendors.  Either those closely signed private keys escaped, or somehow those still-resident keys were used to sign the malware.  Either way, the fact that malware was signed means that something went wrong.  What's weird is that any agency that somehow obtains the ability to get any malware signed by major platform keys is not going to waste that awesome privilege on easily discovered adware.  They would treasure that capability and hold it close, choosing to reserve its use for only highly targeted infiltration specifically so that it never was discovered because, as soon as it is, it's going to be rendered, you know, it's going to be neutered by having the keys rotated.



Now, thanks to the casual misuse of a collection of certificates that somehow escaped from something, whoever or whatever gained the ability to sign those certs has almost certainly lost those rights.  None of the signatures of those certificates will be trusted going forward.  Given what we know, none of this makes any sense.  So we have a mystery.  But, you know, it's been dealt with, thank goodness.  Wow.



LEO:  Interesting.  



STEVE:  And after we tell our listeners why we're here, Leo, we're going to find out what state has banned the use of TikTok.



LEO:  I think I know the answer.  But I'll wait and hear.



STEVE:  Oh, my goodness.



LEO:  I think it's only for government employees, not for you and me.



STEVE:  That is true.  Well, yeah, you can't ban it statewide, of course. 



LEO:  You couldn't.  You couldn't.



STEVE:  No.



LEO:  Yeah.



STEVE:  Thank god.



LEO:  Yeah.  What would my son do without all his millions of fans on TikTok?  I ask you.



STEVE:  So here's one for you.  Last week South Dakota's Governor Kristi Noem...



LEO:  Oh, good old Kristi.  Ah, yes.



STEVE:  ...signed Executive Order 2022-10, which bans all use of the Chinese social media platform TikTok by state government agencies, employees, and contractors.  The Executive Order's news release stated that the order is in response to the growing national security threat posed by TikTok due to its data-gathering operations on behalf of the Chinese Communist Party.  You know, Leo, you've got to keep your eye on those commies.



The press release quoted Governor Noem saying - wow.  "South Dakota will have no part in the intelligence-gathering operations of nations who hate us.  The Chinese Communist Party," she says, "uses information that it gathers on TikTok" - apparently from watching Hank make things - "to manipulate the American people."



LEO:  Yeah.



STEVE:  They're being manipulated by Hank's salt.



LEO:  Yes.



STEVE:  And they gather data off the devices across the platform.  She says:  "Because of our serious duty to protect the private data of South Dakota citizens, we must take this action immediately.  I hope that other states will follow South Dakota's lead; and Congress should take broader action, as well."  The order took effect immediately and applies to all employees and agencies of the State of South Dakota - no more TikTok for you.



LEO:  No.



STEVE:  Including persons and entities who contract with the state, Leo, commissions and authorities or agents thereof.  And thinking about that, you know, I thought, I really do wish that I would still be alive in another hundred years to see what the Internet has become by then.  You know, will it have succeeded in pulling the world together?  Or will the world's fearful leaders have established borders and regional controls just as they have everywhere else?



LEO:  Wouldn't that be a terrible thing for the Internet?  I mean, that's just what we don't want; right?



STEVE:  Yes.  And it seems to be happening.  It's getting chopped up and fragmented and regulated.  And now unfortunately everybody is like suing everyone because they're not happy with the outcome of using it.  Speaking of which, Albania has blamed its IT staff.  Remember the drama that we covered back in July, where Iran retaliated against Albania by melting down their government networks.  Then Albania retaliated back, which I guess is redundant, by severing diplomatic ties with Iran and sweeping into the just-closed Iranian Embassy looking for anything that Iran might have not sufficiently destroyed before leaving.



Also recall that it turned out that Iran had been rummaging around in Albania's networks since April of 2012, so for more than 15 months without ever being detected.



LEO:  They need a Thinkst Canary.



STEVE:  I thought that as I was pulling this together.



LEO:  They really could use that, yeah.



STEVE:  I thought, you know, someone needs to give those Albanians a clue.  Well, who is to blame for all this?  It must be someone's fault; right?  And we can't blame the vendors of the buggy systems.  After all, they provided patches.  For some of the problems.  Usually.  Eventually.  Again, we've got to blame someone.  So Albania has decided that it was all the fault of the IT staff, and so now they're in trouble.



LEO:  Oh.  Oh, boy.



STEVE:  Albanian prosecutors have charged and asked for the house arrest of five government employees.  The prosecutors say the five accused failed to apply security updates to government systems and also failed to detect the hackers that had been wandering around inside their network as far back as April 2021.



Okay.  So maybe the IT guys were seriously negligent.  But we know that's not necessarily the case.  If I may segue for a moment, a perfect example of Albanian-scale negligence not being necessary is the news that the U.S. Department of Homeland Security's Cyber Safety Review Board recently said that it intends to review attacks carried out by the Lapsus$ extortion group and will publish a report detailing how Lapsus$ managed to bypass a broad range of security measures without the use of advanced malware and managed to breach a large number of high-profile targets including Cisco, Microsoft, Nvidia, Samsung, Uber, Rockstar Games, and others.  These companies are not firing their IT department staff because they recognize that it's possible to do nothing wrong and still be breached.



Okay.  In Albania's case, it could just as easily have been His Excellency the President of the Republic of Albania who clicked a link in a phishing email to invite those crafty Iranian cyberwarriors to come for a visit.  And who knows what managerial opposition or budgetary constraints the intrepid five might have faced in their department?  IT departments are notoriously understaffed, overworked, and unappreciated.  And IT people are just like everyone else; right?  There are good ones, and there are bad ones.  Which are they?  We don't know.  What I wonder, though, is who they're going to get to fill those vacated jobs?  With the risk of prosecution...



LEO:  Oh, good point.



STEVE:  With the risk of prosecution for attempting to do a job that might be impossible, and knowing what happened to the last five guys, I would not be surprised to learn that those IT staff positions are difficult to fill.  So, you know, I would be careful, you know, how you deal with problems like this in the future.  Wow.



We do have some good news on the memory-safe languages front.  Since the August release of Android 13, which was the first Android where a majority of the new code added to the project was written in memory-safe languages including Rust, Java, and Kotlin, Google noted that since shifting its focus to memory-safe languages, the number of memory safety vulnerabilities reported in the Android OS has dropped to less than half of comparable counts.  So that's good news for memory-safe languages.  You know, I've always been saying that we're never going to get our systems fixed if we keep messing with them.  You know, this is of course the big problem with Windows is Microsoft refuses to stop.  And they just keep doing stuff.



Well, when you do new stuff, you're going to have new problems.  And so the only thing you could do would be to start using, I mean, high-quality, memory-safe languages for all the new stuff you do.  And that's what Google's been doing with Android, and they're seeing a precipitous drop.  I have a chart in the show notes that shows successive years of Android releases - 2018, '19, '20, '21, and 2022.  And, I mean, it is really looking good.  So something has to change in order for these problems to change.  And empowering programmers with languages that help them makes all kinds of sense.



LEO:  I'm kind of surprised Kotlin is so low because, boy, everybody's so excited about Kotlin.



STEVE:  Yeah.



LEO:  It is still a tiny fraction of the overall development.



STEVE:  I think it's because it's just the very start.



LEO:  It's new, yeah, yeah.



STEVE:  Yeah.  And isn't Kotlin the one that runs on top of the Java VM?  I think it...



LEO:  Ultimately, at some point, Java's got to be - I think has to be in there, although I see Rust and C and C++.



STEVE:  Yeah.



LEO:  But the Google, the last time I looked at Android development, the underneath, underlying stuff is Java.  So I don't know, yeah, Kotlin would make sense to a virtual machine for Java VM, or a frontend for a Java VM.



STEVE:  And I think it is a different language on top of the Java VM.



LEO:  Oh, it's a wonderful language.  I mean, it's a very, yeah, it's for the JVM, that's right.



STEVE:  Right.



LEO:  It's a great language, and I would guess probably more memory-safe, has null safety and stuff like that.  So, yeah.



STEVE:  Right.  It's pretty-looking.



LEO:  Yeah.



STEVE:  For me, the prettiest-looking language I ever did any serious work in was Pascal.  It was just - it was just - it was pretty.  And you could come back later, and it made sense to you.



LEO:  Yes.  It was very concrete.  



STEVE:  Yeah.  And the least pretty was Forth.  Forth is a write-only language.



LEO:  Forth is fun.



STEVE:  You could stare at that and have no idea what the hell is going on.



LEO:  Huyen Tue Dao is one of the hosts on All About Android, an Android developer, loves Kotlin.  And I remember when they announced that they were going to support Kotlin first-class, and this is four years ago or five years ago at Google I/O, the developers cheered.  So I have high hopes.



STEVE:  Good.



LEO:  Don't want to see all that C.  Look how big the C slice is.



STEVE:  I know.



LEO:  Talk about not memory-safe.



STEVE:  And C++ is, like, together they're almost half.



LEO:  Yeah.  Yeah, the other half is Java.  Big slice for Rust, though.  That's also good, good news.



STEVE:  Yeah, yeah.  Okay.  We're going to answer another question.  Have those Black Hat USA 2022 talk videos, which were recorded back in August, finally been published?  Why am I asking, you might wonder?



LEO:  Probably because...



STEVE:  Because the answer is yes.



LEO:  Oh, they have.



STEVE:  They have been, yes.



LEO:  Woohoo.



STEVE:  I have a link in the show notes for anyone who's interested, right below that graphic you were just showing.  It brings you up to a playlist of all the Black Hat 2022 videos.  And those are always interesting for hackers.



LEO:  And they're all on YouTube, which is great.



STEVE:  Yup.  We do have another Chrome zero-day biting the dust, which brings the total up to nine for Google.  It was a type confusion bug in Chrome's JavaScript V8 engine.  It was discovered internally by one of Google's tag researchers.  But being a zero-day it was found because somebody was using it.  So, wow.  There's a lot of pressure to get into Chrome, it being the majority browser now.  And this was another way that's now been foreclosed.  At some point over the weekend I restarted Chrome, and it came up with an announcement of, yay, you've got a new version.  It's like, oh, okay, good.



Okay.  The Verge's coverage of Anker's Eufy, spelled E-U-F-Y, IoT cameras did not pull any punches.  Their headline read "Anker's Eufy lied to us about the security of its security cameras."  And then the subhead said "Despite claims of only using local storage with its security cameras, Eufy has been caught uploading identifiable footage to the cloud.  And it's even possible to view the camera streams using VLC."  Okay.  Since I can't improve on The Verge's coverage and reporting, here's what they wrote, the beginning of it.  It was long, but this will give you the idea.



They wrote:  "Anker" - a company we all like - "has built a remarkable reputation for quality over the past decade," said The Verge, "building its phone charger business into an empire spanning all sorts of portable electronics, including the Eufy home security cameras we've recommended over the years," said The Verge.  "Eufy's commitment to privacy is remarkable.  It promises your data will be stored locally, that it 'never leaves the safety of your home,' that its footage only gets transmitted with end-to-end military-grade encryption" - okay, at this point you start to have to worry, right, when someone says "military-grade encryption."



LEO:  I never want to see that phrase again.



STEVE:  No.



LEO:  Every advertiser puts it in there.  I just take it out and say AES-256 or something.



STEVE:  It is frightening, yeah.  And they said that "It will only send the footage 'straight to your phone.'  So," The Verge wrote, "you can imagine our surprise to learn you can stream video from a Eufy camera from the other side of the country with no encryption."



Okay, now, The Verge's coverage of this might seem somewhat harsh.  But they then show a snapshot of the marketing for the Eufy camera which makes all of these claims quite clear, which then makes the reality of what Anker is doing somewhat stunning.  So in the show notes I have this snapshot, which is right off of the Eufy marketing page.  They said:  "Our Technology Keeps Your Privacy Safe."  Okay, now, I'm not sure that privacy can be kept safe.  So the wording of the headline...



LEO:  Yeah, you're not safe, but your privacy is.



STEVE:  Yeah.  So, but they got two, they got the words in there that they wanted; you know.



LEO:  That's the main thing, yeah.



STEVE:  You know, "your," "privacy," and "safe."  So, okay.  So maybe actually whoever wrote this didn't even understand what they were saying.  I don't know.  But that might be their way out of this corner.  So they've got three big icons.  Local Storage, and we've got kind of your house, and there's a server with a power symbol on it.  And it says:  "For Your Eyes Only.  Home is where your data belongs.  With secure local storage, your private data never leaves the safety of your home and is accessibly by you alone."  Okay, now, consider that in the context of the fact that you can stream it from the other side of the world with VLC.



Okay, second icon:  "End-to-End Encryption.  Peeking Prohibited.  All recorded footage is encrypted on-device and sent straight to your phone, and only you have the key to decrypt and watch the footage.  Data during transmission is encrypted."  None of that is true.  "On-Device AI."  Oh.  "Everything In-House.  Our super smart AI" - apparently much better than their super dumb crypto - "is built into every Eufy device.  It analyzes your recorded footage without the need to risk your privacy by sending it to the cloud."  Okay?  Like all of this is untrue, stunningly.



LEO:  Wow.  Wow.



STEVE:  I mean, it's just, it's, like, incredible.  None of it's true.  So get ready for the lawsuits.  And how.  The Verge continued, okay, The Verge said:  "Worse, it's not yet clear how widespread this might be because, instead of addressing it head-on, the company falsely claimed to The Verge that it wasn't even possible.  On Thanksgiving Day, infosec consultant Paul Moore and a hacker who goes by Wasabi both alleged that Anker's Eufy cameras can stream encryption-free through the cloud, just by connecting to a unique address at Eufy's cloud servers with the free VLC Media Player."  So, I mean, there shouldn't even be cloud servers; right?  What?  It never leaves your house.  It goes straight to your phone.  What do you need the cloud for?  But apparently there's a cloud, and all your video is there.  And you don't even need an app.  You just use VLC and give it the URL.



"When we asked Anker point-blank to confirm or deny that, the company categorically denied it.  'I can confirm that it is not possible to start a stream and watch live footage using a third-party player such as VLC,' said Brett White, a senior PR manager at Anker."



LEO:  Oh, well, he knows, yeah.



STEVE:  And of course that's exactly whose opinion you want regarding anything potentially damaging.



LEO:  I couldn't do it.  I tried, but I couldn't do it.  What is this VLC?



STEVE:  Yeah, I clicked the link, and it just said hello.  Wow.  They wrote:  "But The Verge can now confirm that's not true."  That is, what Brett said.  "This week we repeatedly watched live footage from two of our own Eufy cameras" - which of course they had been recommending in the past, so they probably had some - "using that very same VLC media player from across the United States, proving that Anker has a way to bypass encryption and access these supposedly secure cameras through the cloud."



They said:  "There is some good news.  There's no proof yet that this has been exploited in the wild."  Oh, great.  Now everyone's going to jump on that.



LEO:  I don't know how you would know.  I mean, you couldn't - there's no way to prove or disprove.



STEVE:  Right, right.  You know?  Suddenly the cameras are getting hot.  I wonder why.



LEO:  I do think you need to know the serial number of the camera.



STEVE:  That is true.



LEO:  So that's some protection; right?



STEVE:  They said:  "The way we initially obtained the address required logging in with a username and password before Eufy's website will cough up the encryption-free stream."  Again, none of this, none, I mean, like what they're doing completely belies what they said they were doing.  I mean, the fact that you have, like you log into the cloud, well, then the video must be there.  It's not in your house.  I mean, this, like, this should make your head explode.  If you think about this, nothing they're claiming matches the services that they're offering.



So they said:  "But it also gets worse."  They said:  "Eufy's best practices appear to be so shoddy that bad actors might be able to figure out the address of a camera's feed because that address largely consists of your camera's serial number encoded in Base64, something you can easily reverse with a simple online calculator.  The address also includes a Unix timestamp you can easily create, plus a token that Eufy's servers don't actually seem to be validating - we changed our token to 'arbitrarypotato' and it still worked [thank you, The Verge] - and a four-digit random hex whose 65,536 combinations could easily be brute forced."  And I'll note that other people have already done this, and they did it.



So a Mandiant vulnerability engineer, Jacob Thompson, tells The Verge:  "This is definitely not how it should be designed."  Yeah, no kidding.  For one thing, serial numbers don't change, so a bad actor could give or sell or donate a camera to Goodwill and quietly keep watching the feeds.  But also he points out that cameras don't tend to keep their serial numbers secret.  Some stick them on the box and sell them at Best Buy.  Yes, including Eufy.



On the plus side, Eufy's serial numbers are long at 16 characters and aren't just an increasing number.  We've seen that done before.  Not here.  So:  "You're not going to be able to just guess at IDs and begin hitting them," says Mandiant Red Team consultant Dillon Franke, calling it a possible "saving grace" of this disclosure.  "It doesn't sound quite as bad as UserID 1000, then you try 1001, 1002, 1003," and so forth.



Anyway, I'm reminded of the fact that I don't have a single connected video camera anywhere within my environment, and that your wife Lisa, Leo, early on intuited the inherent dangers of having unknowable video capture technology, which is what all of this is, lurking around the house.  In a TNO (Trust No One) world, the simple though impractical truth is:  "Unless you designed it yourself, you don't know what it does."  And I should add that due to the crazy complexity of the things we design today, even if you did design it yourself, you may still not know that it does what you think it does.



LEO:  Right.  What you said, man.



STEVE:  Yeah.



LEO:  Right on.  The point is, maybe it isn't an easy to exploit, but they completely misrepresented what was going on.



STEVE:  Oh, my god, yes.  Nothing that they said about it was the truth.



LEO:  And they must have known better.  I mean, it's not an accidental mistake disclosure.



STEVE:  The one out I had was when you have a situation like Anker was apparently in, where everybody loved their power supplies, which they probably themselves actually did create.  There's a tendency to go buy other companies.



LEO:  Right.



STEVE:  In order to expand yourself.



LEO:  That may well have happened, yeah, yeah.



STEVE:  Yeah.  And so it probably is like, well, we've got all this money from power supplies, who looks good?  Oh, let's get Eufy.



LEO:  Right.  They make everything, too.  They make headphones.  They make all kinds of stuff.  Anker has a sound core division. 



STEVE:  Yeah.



LEO:  They, exactly as you say, they found success, and they then expanded.



STEVE:  Yup.  And so they, the Anker people, unfortunately are tying their good name to products that they can't actually vouch for, but which are making them money now, and they've got Brett out there on the front lines saying, what?  What link?  I don't have a link.  Where did you get that link?  That's illegal for you to have that link.



LEO:  I tried, but I couldn't do it.



STEVE:  Okay.  So this is moderately random, but not too far afield for this podcast.  Everyone knows of my passion for coding.  But I predate electronic computers.  And before computers was electronics.  Although coding has taken over, electronics will always be my first love.  So in addition to coding, I occasionally do a bit of tinkering, hacking and designing with electronics.



At some point in the past, some googling must have taken me to a place called Seeed Studio.  That's S-E-E-E-D, spelled with three E's, SeeedStudio.com.  I purchased something from them, I don't now remember what, and as a consequence was promptly added to their periodic mailing list.  In this case, I don't mind the spam because the mail contains photos of the stuff they're promoting, and my jaw spends most of its time hanging down with my mouth open over the insanely low cost of the technology that's currently available from China.  It is truly astonishing.



For example, a recent mailing showed the "Seeed Studio XIAO ESP32C3."  It's a tiny module about the size of a quarter with 14 electrical connections, seven on either side, and what appears to be two tiny buttons and an LED.  It also has a tiny  USB-C connector, presumably for programming this little thing.  And all of the software for doing so is open source.  Its description says:  "Seeed Studio XIAO ESP32C3 adopts new RISC-V architecture..."



LEO:  Ooh, ooh.



STEVE:  "...supporting both WiFi and BLE wireless connections."  On this little thing.



LEO:  On that thing?



STEVE:  WiFi and Bluetooth.



LEO:  What?



STEVE:  "For Internet of Things applications, you will find it is flexible and suitable for all kinds of IoT scenarios."  Okay.  I was curious, so I looked into the chip this uses.  The ESP32C3 is a 32-bit RISC-V microprocessor, which includes a whole host of I/O peripherals in addition to WiFi and Bluetooth 5.  It has cryptographic hardware accelerators that support AES-128/256.



LEO:  What?



STEVE:  SHA hashes, RSA, HMAC, digital signature, secure boot...



LEO:  What?



STEVE:  And has a hardware random number generator.  And how much is it if you purchase just one?  $4.99.



LEO:  Oh, my god.  Oh, my god.



STEVE:  Five dollars for that.  And that is just typical of what this Seeed Studio has for sale.  Anyway...



LEO:  Just don't try to bring it into South Dakota, that's all I'm saying.



STEVE:  No, no, no, no.  That's outlawed.  It might have TikTok embedded on it.  Anyway...



LEO:  That is so cool.  And RISC, everybody's very interested in this RISC-V.  This is the newest kind of open source digital architecture.



STEVE:  Well, yes.  And license-free; right?  The reason there's no ARM on this is you have to pay ARM for that.  And you're not going sell something for $5 that has this and everything else it has if you have to pay some ARM licensing fee.



LEO:  And it shows you what the ARM tax is, really, if you think about it.



STEVE:  Yeah.  And so, I mean, RISC-V is - it's a beautiful architecture.  It's been, like, moving along for years, and it's evolving.  And it has an absolutely mature open source free tool chain for doing stuff.  But none of that is why I've brought this up today.



LEO:  Although, Steve, you could put, you know, it could be a SpinRite hardware device.  Does it have room for software?  You could put SpinRite on it.  You wouldn't have to use DOS or anything.  You just plug it in and boot to it.



STEVE:  Wonderful.



LEO:  Five dollars, I'm telling you.  I'm just telling you.  All right.  All yours.



STEVE:  Okay.  That's not why I'm telling anyone about this.  I'm telling anyone about this because a month or two ago, maybe three, something in one of those mailings brought me up short because it was similarly stunning, and I thought you guys, our listeners, all needed to at least know about it.  It got away from me when I went back to try to find it.  I didn't know where it went.  But when their most recent mailing mentioned it again, I thought, okay, this time it's not getting away from me.



Okay.  Get a load of this.  It's called the LinkStar-H68K-1432 multimedia router.  It has WiFi 6, 4GB of RAM, 32GB of eMMC flash storage onboard, with an SD card slot for more.  It's powered by a quad-core 64-bit Cortex-A55 chip, an ARM G52 2EE GPU.  There's a GPU because it can output HDMI 4K video at 60 frames per second.



LEO:  What?



STEVE:  By the way, Leo, it's 2.5 by 3.5 inches.  That's the size of that little thing that you're looking at.  It has a USB 3 port, two USB 2 ports, a USB Type-C that can be attached to a SATA 3 drive.  On the router side, aside from its dual-band 1200 Mbps WiFi 6, it also has four Ethernet ports, individual interfaces, two running at up to 2.5GB, another two at 1GB.  It comes with Android 11 preinstalled...



LEO:  What?



STEVE:  ...but also supports Ubuntu, Debian, Armbian, OpenWRT, and Buildroot, which is used to build embedded Linux systems.



LEO:  I just thought of a new geek game we could play:  Geek Price Is Right.  Try and guess.



STEVE:  So what will this little pocket-sized fanless WiFi 6, four Ethernet interface router set you back?  How about $119?



LEO:  Unbelievable.  Wow.



STEVE:  That's what got my attention.  That little Netgate SG-1100 router that I love and use and have recommended, it's 189, and it only has three Ethernet interfaces and no WiFi.  This thing has four separate interfaces and WiFi 6, and a ton more.  The fact that you can drop OpenWRT onto it and have an operating, state-of-the-art router with four ports, all isolated individual subnets, and WiFi 6?  For $119?



LEO:  Could you put pfSense on it, do you think?



STEVE:  That I don't know.  That's a question I have, and maybe one of our listeners will be interested to try.  Again, it's hard to imagine this thing from the picture.  It's 2.5 inches by 3.5 inches.  And it's fanless.  It's got a little heat sink on the bottom, multiple USB ports, 4K HDMI, and an SD card slot.  It's just incredible.  For 119 bucks.



LEO:  In the palm of your hands.



STEVE:  I want to be clear, I don't own one.  I don't have time to own one, and I'm not vouching for it in any way.



LEO:  Do not buy it.  We're going to get mad at you.



STEVE:  That's right.  So I'm not vouching for it in any way.  Unlike the ZimaBoard, which I was happy to vouch for since I had several, and I loved them, you're on your own with this thing if you should decide to take the plunge.  For the right hardware tinkerer, this could be so much fun, and it's not very expensive.  I have the link in the show notes, and it is Episode 900, this episode's GRC shortcut of the week, so grc.sc/900.  That will take you to this thing's web page, where you can see for yourself.  Anyway, I just - it was so cool, so inexpensive, it could be the perfect home router.  Four ports, and WiFi 6 for $119.



LEO:  That's really cool.  Boy, we live in amazing times, Steve.  Can you imagine if you were a young guy, you know, a teenager, at the time building the Portable Dog Killer, if you'd had something like Seeed Studio available to you?  You might have...



STEVE:  Unfortunately, I'd probably be bringing Elon's satellites down if I had this.



LEO:  It's a good thing.  It's a good thing you didn't have it.  Wow.  Wow.



STEVE:  I've figured out how to fire the retrorockets.  Cool.  Okay.  Speaking of Elon, one last piece of lunacy.  When asked during a scheduled Twitter Space chat this past Sunday why he bought Twitter, Elon explained his decision as follows, and I'm not making this up:  "I can't exactly say why because it's one of those things where it's like my biological neural nets said, 'It is important to buy Twitter.'  And just like with a digital neural net, you can't really exactly explain why the neural net is able to understand an image or text.  The collective result of the neural net says this is an important decision, or this is the right action.  And my biological neural net concluded that it was important to buy Twitter; and that if Twitter was not bought and steered in a good direction, it could be a danger for the future of civilization.  And so that's why I bought it."



LEO:  Wow.  Wow.  Clear as mud, Elon.



STEVE:  Yeah, Elon.  So okay, you're passing the responsibility off to your brain, whose operation you don't understand.



LEO:  I don't know what I'm doing or why I'm thinking it, but I'm going to do it anyway.



STEVE:  Yeah, because, you know, I'm a biological neural net just like those image recognizers, and we don't know how they work either.



LEO:  It's a puzzle whether he knows what he's saying is moronic, and he's saying it to confuse and distract you, or if he actually believes it, which I do not know which is worse.  It's amazing.



STEVE:  So I did not have time to make a comprehensive scan of my DMs this week.  Frankly, the DMs channel is becoming quite popular, and there's a lot to go through.  I'm only going to share my own tweet from last Wednesday for those of our listeners here who don't follow me on Twitter, and there are many.  I tweeted on Wednesday:  "To all Security Now! Listeners:  I'm currently listening to Alex Stamos on Wednesday's 'This Week in Google.'  Alex has not let anyone get a word in edgewise because he has so much amazing information to share.  Without reservation, I RECOMMEND, all caps, listening to this.  It's FANTASTIC! in all caps, exclamation point."  That tweet received about three times as many likes as any of my weekly Security Now! notes posting tweets do, as well as 13 replies and 12 retweets.  Alex was amazingly wonderful.



LEO:  Thank you, yeah.



STEVE:  And I just wanted to make sure, I wanted, like, this is a listening assignment for all of our listeners.  Last week's, so what was that, it'd be November 30th, This Week in Google.  It was amazing.



LEO:  If you just go to TWiT.tv/twig will take you to the TWiG page.  It's Episode 692.  So it'll be the first episode if you go right now.  But even in a few weeks it'll still be TWiT.tv/twig, Episode 692.



STEVE:  And you don't even have to wade through a bunch of crap in the beginning.  I mean, like...



LEO:  No, there's no crap.



STEVE:  Alex said:  "Is this microphone on?"  And that's all it took.



LEO:  It was very - rich density of information.  He was great.  He's a wonderful guy.  And I really enjoyed him, and I'm hoping we can get him back because he had a lot to say.



STEVE:  So on my end, to say that things are going well with SpinRite's alpha release testing would be an understatement, considering how poorly things could have easily gone.  I'm still somewhat in shock that we're very close to having a final release.  I have things to fix, but nothing major so far.  Mostly people who are now really engaged and involved are, like, keep asking for new features.  And so it's like, oh, oh, oh, you know, that's really not what we should be doing now.  So it's really looking good.



There was one posting to the newsgroup last week that I wanted to share because it makes a point that I need to drive home, not only for everyone's safety, but it's part of the reason why I am so fired up about SpinRite's potential long-term future.  This person posted this.  He said:  "I have a ThinkPad Helix, and the SSD is a Samsung EVO 1TB mSATA."  So a high-end Samsung EVO.  That's all, that's the only brand I buy now.  He said:  "When the SpinRite pre-release starts, it estimates 31.7 minutes for processing.  However, a Level 2 pass, with no errors detected, takes two hours and 56 minutes."  Okay, so just shy of three hours.  He says:  "So that's more than five times longer" - it's actually almost six times longer; right?  It estimated 31 minutes, it actually took three hours, so six times - "than the estimated time."  He asks:  "Is that normal?"



Okay.  So I replied to him in the newsgroup:  "We've found that whereas the fronts of spinning drives tend to be the fastest regions because they contain more sectors around their longer outer tracks, the fronts of many SSDs are conspicuously slow.  We posit that this is due to the presence of much more on-the-fly error correction and data recovery.  We first saw this using the ReadSpeed benchmark tool.  One of my future plans is to locate these slow-to-read spots and selectively rewrite them to restore their speed by eliminating this unseen error correction and data recovery which results in a significant reduction in SSD performance."



I said to him:  "If you do discover that the front of that drive is quite slow, you could identify the slow region and run Level 3, which does a rewrite, over just that region, and it might very well speed it back up."  I said:  "And that would likely also increase its reliability by solidifying those sectors which might be on the verge of transitioning from very slow to unrecoverable."



He replied:  "I just did a Level 3 on the entire drive with Alpha 4, and now SpinRite estimates the 1TB drive will require 29.7 minutes, and a full Level 2 scan completed in 29:50, which is almost six times faster than it scanned a couple of days ago."  He said:  "Thank you so much for creating 6.1."



So what that means is, to summarize this, he first did a simple read pass on that SSD in his ThinkPad.  It took him three hours just to read all the sectors.  He ran Level 3, a SpinRite Level 3 across the entire drive.  Now that same process of simply doing a read scan takes 30 minutes, just shy of 30 minutes.  So running a Level 3 SpinRite on an SSD that had nothing technically wrong with it increased its speed by a factor of six, on average, across the entire SSD.  That's what we're seeing.



So this is the reason I am very excited.  What's happening is we are - SSDs are having trouble reading their contents, but still able to; yet it's revealed by a significant slowdown which is going unnoticed.  But how many times have we heard that an SSD-based machine, which being solid-state you would think, right, it's solid-state, that an SSD-based machine doesn't seem to be as fast as it was when it was new.  That wouldn't seem logical, but this might be what's going on.



So as I said, one of the things I have in store once SpinRite 6.1 is launched, and we start working immediately on 7, is to profile the performance of mass storage media to locate and selectively repair sluggish spots.  But for what it's worth, what anyone can do, or will be able to do as soon as 6.1 is out, is to give such drives a single Level 3 pass, as this person who just posted did, which very well could significantly improve both the system's performance and its reliability by rewriting the drive's sectors to recharge those leaky storage capacity cells.  So anyway, a cool outcome from the 6.1 work.



LEO:  And now the continuing LastPass saga.  And, you know, we still use LastPass for our enterprise access...



STEVE:  I do.



LEO:  ...and [indiscernible] management.  So let me know if we should stop.  I mean, I use Bitwarden at home, but TWiT uses LastPass.



STEVE:  Like many other LastPass users, last week I received another note from LastPass.  The note is short, so I'll read it into the record.  We got a note from Team LastPass, is how it was signed.  "Dear Valued Customer:  In keeping with our commitment to transparency, we wanted to inform you of a security incident that our team is currently investigating.  We recently detected unusual activity within a third-party cloud storage service, which is currently shared by both LastPass and its affiliate, GoTo.  We immediately launched an investigation, engaged Mandiant, a leading security firm, and alerted law enforcement.



"We've determined that an unauthorized party, using information obtained in the August 2022 incident, was able to gain access to certain elements of our customers' information.  Our customers' passwords remain safely encrypted due to LastPass's Zero Knowledge architecture.  We're working to diligently understand the scope of the incident and identify what specific information has been accessed.  As part of our efforts, we continue to deploy enhanced security measures and monitoring capabilities across our infrastructure to help detect and prevent further threat actor activity.



In the meantime, we can confirm that LastPass products and services remain fully functional.  As always, we recommend that you follow our best practices around the setup and configuration of LastPass, which can be found here."  And then they provided a link.  "As is our practice, we will continue to provide updates as we learn more.  Please visit the LastPass blog for the latest information related to the incident.  We thank you for your patience while we work through our investigation."



Okay.  So there's no follow-up yet.  Last time we waited precisely three weeks from the first announcement.  The first announcement came on August 25th, and three weeks later we got the update on September 15th.



For me, the two most relevant pieces of information in this first of presumably two disclosures are where the person, the team, wrote:  "We've determined that an unauthorized party, using information obtained in the August 2022 incident, was able to gain access to certain elements of our customers' information."  So that.  And "Our customers' passwords remain safely encrypted due to LastPass's Zero Knowledge architecture."



Okay.  So as usual from all such partial disclosures, we're left wanting more.  Of course.  We don't know, at the moment, which "certain elements" of their customer information was inadvertently made available.  But they apparently know.  They're not saying yet.  We know from the follow-up note from the first intrusion that the bad guys were rummaging around in some developer network that was not connected to their production systems.  But now it appears that those who were doing the rummaging managed to get sufficient information...



LEO:  They found something.



STEVE:  Right.



LEO:  They found some keys in there.



STEVE:  Right, whether in the form of source code that disclosed, like, some other way to get into something else, or maybe in the form of credentials that were bound into whatever it was they got, which were used to perpetrate this latest breach and information exfiltration.  If LastPass lost control of their customers' billing data  names, credit cards, street addresses and so on  that would not be good.  But at this point we're just speculating.  Presumably in another two weeks, or three, whenever, we'll be told more.  You know, let's hope.



Last week, after this happened, I popped on for the first 15 minutes of Tech News Weekly with Jason and Mikah to talk about this latest breach.  I made the same point that I always make, which is that none of the passwords and other secret data that's being stored by any of the many competing password managers, you know, LastPass and all of them, should ever be vulnerable to any breach of the data that's being stored on our behalf in the cloud.  You know?  That's thanks to what we once called - we coined the abbreviation or an acronym PIE, which stood for Pre-Internet Encryption, which the industry now calls end-to-end encryption, though that term is becoming less useful as non-end-to-end systems abuse it.  I mean, you know, even Apple says that iMessage is end-to-end encrypted, except that, well, it's not in the cloud because they have the keys to that.



So again, unfortunately the definition of "end-to-end" is being abused.  But that's going to happen when any popular phrase gets into the marketing department.  But the point is, if it's done correctly, we're really just using a password manager's cloud service to keep our various devices synchronized.  It's simply a synchronization mechanism.  And in fact my annoyance with the passkeys system which is becoming, hopefully will become popular, is that there isn't an all-in-one cloud sync for them.  This is an opportunity for Microsoft to keep their devices separate from Apple, to keep their devices separate from Google, to keep their devices separate, unfortunately.  But anyway, maybe that'll change when password managers start supporting passkeys.  Maybe we'll get sync back.  That's a possibility.



The threats we face to our stored secrets, and this is the point, are only on our end.  In order to do its work, at some point any password manager must have at least the user's username and password decrypted for the site that they're visiting and want to fill in the form for.  I don't know whether the entire password archive is decrypted as a whole, or whether sites can be decrypted individually, which would seem safer to me.  But either way, at some moment in time, the data must exist in the clear in the user's browser.  Way back at the start of this podcast we noted the inherent impossibility of protecting encrypted DVD video content because the player itself needed to be able to decrypt the DVD in order to play it for its owner, and the DVD's publisher ultimately had no control over the DVD player and what it did.



So all they could do was the best they could do.  If the password manager's browser add-on were to be adulterated in some way to break its security design, or if something was able to somehow intercept its operation in the client, that would prove devastating.  But it's difficult to see how any breach of LastPass or any other password manager's cloud synching facility could ever endanger a user's always-encrypted secrets.



Now, of course, none of this prevents reputational harm to LastPass.  You've got to know they're not happy about this.  But they're also, like, the biggest target in town.  So that's what comes with being the big target, like Chrome is the big browser target.  And most users will have no idea what it means for all of their data to always be encrypted before it leaves their browser.  They don't see anything leaving their web browser.  They have no concept of the cloud or of client-side encryption or what any of that means.  They just know that they're using this or that password manager, and this or that password manager suffered a breach, and that the press is now able to say that this is, in the case of LastPass, the second breach in a little over four months.



If we assume that the decision to change password managers is unwarranted  and I'm not suggesting whether it is or it not, it's a personal decision  then one huge advantage any password manager has is inertia.  It's much easier to change search engines than it is to change password managers.  You know, certainly it can be done.  And the password managers have provided means to lower the bar to doing so, offering various importers of other password managers' content, you know, and archives.  But it seems most likely that, until users learn that someone's passwords were actually stolen, inertia will reign.  The statement "Our customers' passwords remain safely encrypted" I think matters a lot, especially when changing password managers is a pain to do.



The listeners of this podcast, as knowledgeable and sophisticated as any, anywhere, you know, they're able to make an informed decision.  I don't know what it means for them.  I'm continuing to remain with LastPass because I have no interest in punishing them for making a mistake, and there is no indication that the security I actually require from them has ever been endangered.  You know?  That said, I'll be interested to learn more in the next couple weeks when they're able to tell us more.  I think their fiduciary obligation is to immediately engage law enforcement and an outside firm and tell us that this has happened.  So it's reasonable for us to give them a few weeks for them to tell us more.  And I'm sure we'll get something in a week or two.



LEO:  Good.  Nothing to fear yet, anyway.



STEVE:  Yeah.  I mean, I just - I do think the fact that something that happened, some information that escaped from the first breach got used in some way, it's like, well, okay, that makes sense.



LEO:  What it wasn't is a leak of - they don't know what your master password is.  They can't unencrypt your vault.



STEVE:  Correct.



LEO:  So there was nothing that could be leaked in that regard.



STEVE:  Right.



LEO:  Another thing maybe to worry about, and this was something that Tavis Ormandy brought up, is the way that that JavaScript Chrome extension or Firefox extension works, the code in there, because at some point it does have to see it in the clear.



STEVE:  Yes, that is the concern is what happens on the client side.



LEO:  Right.



STEVE:  But not on the cloud side.



LEO:  Right, right.



STEVE:  Yeah, and in fact, you know, one of my favorite expressions back when I was spending time doing SQRL stuff was that I used to say "SQRL gives websites no secrets to keep."



LEO:  Yes.  That's the key.



STEVE:  And that's the key.



LEO:  That's Trust No One.



STEVE:  Exactly.  It's Trust No One.  Not even - you don't even have to trust your password manager.



LEO:  Right, right.  And somebody's saying, well, didn't Steve review the code?  Well, you reviewed the code, but like 10 years ago.  I mean, I don't think any of that code survives.



STEVE:  And it wasn't their code in their cloud.  What it was was it was the algorithm that Joe was using, which is this concept of client-side blob encryption.



LEO:  Right, right.



STEVE:  That's what it was.



LEO:  Right.



STEVE:  And in fact he even provided - he provided me a web page  where you could go and, like, see a simplified JavaScript and understand it and see what it was doing.



LEO:  Yeah.  My guess is that all the major password companies do basically the same thing.



STEVE:  They'd be insane not to.



LEO:  Yeah, E2EE encrypted block.



STEVE:  You have to do this.  The moment a password company actually compromises their users' passwords, they might as well just declare Chapter 11.  It's over.



LEO:  Right, right.



STEVE:  Roll up the carpet.  Nobody will ever use them again.



LEO:  The other good side of this, LastPass has been spun off Log Me, which it's a complicated long corporate chain.  But the equity capital company that bought Log Me is spinning LastPass off as a standalone company, which if it isn't already, it will be any day now.  And in a way that's good because of course equity capital...



STEVE:  It helps them keep focus.



LEO:  Exactly.  And equity capital is always looking to make their investment back.  They're usually highly leveraged.  So just getting that off in its own corner where nobody's going to tell them what to do means that, I mean, they're good people.  They know what to do, and they will keep doing the right thing as long as they're left to their own devices.  And I think that's sort of what this is.



STEVE:  And, you know, this hurt them.  There's no doubt.  This hurt them.  You know?  No one wants to say whoops, we were breached.  But, you know, nothing to see, move along.  There's going to be a bunch of people that go, okay, that's two in one year, you know, I'm not waiting for three strikes.



LEO:  Yeah, no, that's true.



STEVE:  I'm going with two strikes.



LEO:  But you've reassured us, I hope.  You know, I think everybody says...



STEVE:  I'm not changing.  I'm...



LEO:  And if you're in Australia, just, you know, don't you [laughter].  No, they had - they did responsibly disclose it; didn't they.  They were the letter of the law in Australia, and GDPR has a responsible disclosure clause.  I don't know if we do in the U.S.  Feels like we ought to.  But since...



STEVE:  CISA is definitely, certainly among government agencies, you know, thou shalt disclose.



LEO:  Yeah.



STEVE:  I think it was within, was it 48 hours or 24 hours?  I mean, it was...



LEO:  It was a short time, yeah.



STEVE:  Yeah, there's a short window, at least.  But I don't know if...



LEO:  But they don't have the $50 million AUS penalty to back it up.  That's the problem.



STEVE:  Ooh, boy.  



LEO:  That does get the job done, I must say.  I must say.  Steve Gibson, you get the job done each and every Tuesday with Security Now!.  Thank you so much for Episode 900.  What a milestone, huh?



STEVE:  Wow.



LEO:  We would never have thought, way back in what was it, 2007, 2006...



STEVE:  We were still using cranks to start our cars, Leo.



LEO:  It's a lifetime ago.  But we're doing it, and we're going to do it again next Tuesday, and we're going to keep doing it for at least two more years.  Go to GRC.com.  That's Steve's website, the Gibson Research Corporation.  Couple of things you can do there.  Of course, first thing you should do is get SpinRite, the world's best mass storage maintenance and recovery utility.



STEVE:  Looking better every day.



LEO:  You're getting 6.0, but we're minutes away from the release of 6.1.  And anybody who buys now will get 6.1 for free as an automatic upgrade.  You'll also get to participate in the development, which is kind of a fun thing to do.  There's wonderful forums going on all the time.  Steve's got his own newsgroup.



STEVE:  Yeah, don't ask for any new features.  No new features.



LEO:  No, no, too late.  It's locked.



STEVE:  Please, yeah.



LEO:  Solid.  Locked in.  While you're there, you also should check out all the other freebies that Steve offers.  ShieldsUP is world famous.  Things to test your network, router and so forth.  And you can get a copy of the show while you're there, too, save yourself a step.  He has 16Kb audio for the bandwidth-impaired; 64Kb audio, that's the standard version.  And transcripts, written by Elaine Farris, so they're very well done.  So you can read along as you listen, or you can search to find a part of it.  All that's at GRC.com.  Feedback forms are open at GRC.com/feedback.  Steve's also on Twitter, although it sounds like it's a little busy now.  Your DMs might be a little full.



LEO:  Don't DM him there.  But you can follow him.



STEVE:  I have some catching up to do.



Copyright (c) 2022 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#901

DATE:		December 13, 2022

TITLE:		Apple Encrypts the Cloud

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-901.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we answer the following questions and more:  What browser just added native support for Passkeys and where are they stored?  What service have I recommended that suffered a major multi-day service outage?  How can you recognize a totally fake cryptocurrency trading site?  Which messaging platform has become cybercrime's favorite, and how would you go about monetizing desirable usernames?  What's the latest in TikTok legislative insanity, and is it insane?  Which two major companies have been hit with class-action lawsuits following security breaches?  Was Medibank's leaked data truly useless?  And Apple has finally given us the keys to our encrypted data in the cloud, holding none for themselves - or have they?



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  And it's again questions and answers.  What browser just added support for Passkeys?  How can you recognize a totally fake cryptocurrency trading site?  And has Apple finally given us the keys to encrypted data in the cloud?  Steve analyzes it.  All the security news next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 901, recorded Tuesday, December 13th, 2022:  Apple Encrypts the Cloud.



It's time for Security Now!, yay.  You've been waiting all week, haven't you.  Well, good news, it's finally here.  So is Steve Gibson from GRC.com, our host, the man of the hour.  Hi, Steve.



STEVE GIBSON:  Yo, Leo.  Great to be with you.  Episode 901.  And I was originally going to talk about something else, but this news hit, and it was like, okay, you know, actually it was by far the most tweeted question that I've had in a long time about any news.  And that of course is Apple's announcement that they're going to begin encrypting iCloud.  So Apple Encrypts the Cloud is today's topic.  But that's just what we're going to wrap with.  We're going to answer a bunch of questions as we are now poised to do for the last few weeks.  What browser just added native support for Passkeys, and where are they stored?  What service have I recommended that suffered a major multiday service outage?



LEO:  I love, by the way, I love this question format.  This is great.



STEVE:  It works.  



LEO:  Yeah.



STEVE:  Yeah.  How can you recognize a totally fake cryptocurrency trading site?  Which messaging platform has become cybercrime's favorite?  And how would you go about monetizing desirable usernames?  What's the latest TikTok legislative insanity, and is it insane?  Which two major companies have been hit with class-action lawsuits following security breaches?  Was Medibank's leaked data truly useless?  And Apple has finally given us the keys to our encrypted data in the cloud, holding none for themselves.  Or have they?



LEO:  Oh, boy.  That's a good one.  That's a great question.  Can't wait to hear that.  I knew, you know, you texted me earlier that you were going to cover this, and I told everybody on MacBreak Weekly.  Apple just rolled this out.  I just turned it on in my phone.  Well, actually I didn't because it said, before you turn it on, you have to get updates on all of your Apple devices, of which there are more than a dozen.  Which makes sense.



STEVE:  Yeah, I'll talk about that.  You cannot have any older versions.  They all have to know about this technology.  



LEO:  So it's going to take me - I have to go around to a lot of stuff and update it before I can do it, including my watch.  Picture of the Week time, Mr. G.



STEVE:  So rarely does a picture, I think, really work as an analogy as well as this thing does.  As a coder, and you're a coder, we know the danger of, as you're writing code, just like not making sure that what you're writing is correct.  There is this sense of building a foundation.  And you sort of - you create part of it, and then you add to it, and it grows.  Well, you know, you want the beginning of that process and all the various stages between the beginning and the end to be correct.  Anyway, so the caption on this picture is "Just keep coding, we can always fix it later."  And this is so perfect because - so what you and I are laughing at is two bricklayers, I guess maybe this is not...



LEO:  They started okay.



STEVE:  Well, except look at that vertical one. 



LEO:  Yeah, maybe not.



STEVE:  I love that vertical one.  It's like, so you have to ask, what is the story here?  I mean, this is - they kind of know that bricks are supposed to be horizontal.



LEO:  Kind of.



STEVE:  Kind of.  But first of all, they seem to have a problem with their brick sizing.  You know, these are not all the same size bricks.  And so, and there's like some wedgies in there, and there's, I mean, it's just - it's a disaster.  But the beauty of the picture is like way down to the bottom is a brick that it's never going to get fixed because it was four days ago in the process of building this wall that this brick, someone stuck it in vertical because, well, they had a gap to fill or something.



LEO:  Eric in our chatroom says that's a go-to statement.  Once you see it, you can never unsee it.  That's hysterical.



STEVE:  Yeah.  So anyway, I just - it's a great analogy because, you know, these guys just said, oh, well, you know, what the heck, just keep going.  And this is never going to get fixed.  That brick down there, it will be vertical as long as that wall is standing, just like some bug that got written in the beginning, and they just said, oh, just, you know, we'll fix it later.  No, bad idea.



Okay.  A good idea was covered by Ars Technica's Friday headline, which read:  "Passkey support rolls out to Chrome stable."  They said - this is Ars Technica.  "With a huge list of caveats, initial Google Passkey support is here."  They wrote:  "Google's latest blog says:  'With the latest version of Chrome'" - that's 108, I have it, probably everybody does - "'version of Chrome, we're enabling Passkeys on Windows 11, macOS, and Android.'"  And Ars wrote:  "The Google Password Manager on Android is ready to sync all your Passkeys to the cloud.  And if you can meet all the hardware requirements and find a supporting service, you can now sign into something with a Passkey."



So they then in their coverage take some time explaining stuff that we all know about Passkeys that we've talked about.  And they get most of it right in kind of a watery down sort of way.  Then they talk about compatibility by writing:  "Today Passkeys essentially require a portable device, even if you're logging into a stationary PC.  The expectation is that you'll use a smartphone for this, but you can also use a MacBook or iPad.  The first time you set up an account on a new device, you'll need to verify that your authenticating device, your phone, is in close proximity to whatever you're signing into.  This proximity check happens over Bluetooth.  All the Passkey people are really aggressive about pointing out that sensitive data is not transferred over Bluetooth.  It's just used for a proximity check.  But you'll still need to deal with Bluetooth connectivity issues to get started," they say.



"When you're signing into an existing account on a new device, you'll also need to pick which device you want to authenticate with, probably also your phone.  If both of these devices are in the same big-tech ecosystem, you'll hopefully see a nice device menu; but, if not, you'll have to use a QR code.



"Second big issue," they said, "did everybody catch that OS listing at the top?  Google supports Windows 11 with Passkeys, not Windows 10" - what? - "which," they write, "is going to make this a tough sell.  Statcounter has Windows 11 at 16% of the total Windows install base, with Windows 10 at 70%.  So if you happen to make a Passkey account, you could only log in on newer Windows computers."  Okay.



So they continue:  "Passkeys get stored in each platform's built-in keystore, so that's Keychain on iOS and macOS, and Google Password Manager or a third-party app on Android, and 'Windows Hello' on Windows 11.  Some of these platforms have key syncing across devices, and some do not.  So signing in on one Apple device should sync your Passkeys across to other Apple devices via iCloud, and the same goes for Android via a Google account, but not Windows or Linux or Chrome OS.  Syncing," they write, "by the way, is your escape hatch if you lose your phone.  Everything is still backed up to your Google or Apple account.  Google's documentation mostly doesn't mention Chrome OS at all, but Google says:  'We're working on enabling Passkeys on Chrome for iOS and Chrome OS.'  There's also no support for Android apps yet, but Google is also working on it."  Which makes me wonder, like okay, a lot of these limitations seem significant and weird.



Anyway, they wrap up this news of Chrome's emerging support by writing:  "Now that this is actually up and running on Chrome 108 and a supported OS, you should be able to see the Passkey screen under the 'autofill' section of Chrome settings."  So, you know, of a Chrome browser's settings.  So you can go to chrome://settings/passkeys.  Put that into the address bar.  And they said:  "Next, we'll need websites and services to actually support using a Passkey instead of a password to sign in."  They said:  "Google Account support would be a good first step.  Right now you can use a Passkey for two-factor authentication with Google, but you can't replace your password."



And they finished:  "Everyone's go-to example of Passkeys is the Passkeys.io demo site, which we have a walkthrough of," and then they do that.  So, you know, I've got Chrome 108.  So I put "chrome://settings/passkeys" into Chrome's address bar, and I was greeted with a little thing that said - okay, this was on Windows 10.  It just said:  "To manage Passkeys, use a newer version of Windows."



LEO:  Oh, god.



STEVE:  And it's like, wow, really?  I mean, that's what you're going to get on Windows 10?  So presumably it's the Windows Hello which, I mean, it's very cool that the browser is not storing your Passkeys, that the browser manages this process, but it's in the substrate.  It's in Windows itself where the Passkeys are stored.  That seems like a much better idea than having them in the browser.  But wow.  You're not letting people with Windows 10 have it?  Good luck with that.



So, you know, 70% of the world won't be able to use it.  These feel like arbitrary limitations.  Lack of syncing among competing platforms and devices feels like the attempt to  create walled gardens.  It feels to me as though Passkeys, the way this is going, Passkeys, like passwords, may also become the domain of our existing password managers.  We know that any password manager that has a pulse has got to be racing as fast as possible to getting support of Passkeys up and going.  And of course they will provide cross-client synchronization.  And none of this, sorry, you can't use it on Windows 10 nonsense.  So, you know, we can hope that that's coming.  We still have the chicken-and-egg problem, of course, of getting it to actually work and then having places where you can use it to sign in.  But you and I, Leo, are old enough to remember when people thought that the web would never happen because it's like, well, you know, why is anyone going to put a web page up...



LEO:  What do we need that for?



STEVE:  ...when there's only five people who are using the Internet?



LEO:  I've got Gopher.  I don't need a website.



STEVE:  Yeah, exactly, exactly.  Okay.  So while we're talking about synchronization, I thought I should mention that my favorite cloud synchronization platform, Sync.com, had something happen to it. The problem first surfaced last Wednesday, apparently after a scheduled maintenance somehow went wrong, and things didn't appear to be fully restored until just yesterday.  I utterly depend upon Sync.com, and the outage had me revisiting the wisdom of that dependency.  I'm not a typical user since I also run a pair of NAS boxes, you know, Network Attached Storage boxes, at each of my two locations.  And as we know, there are many other syncing alternatives.  Reading between the lines of Sync's frequent online status updates through these slowly passing days, you sort of got the sense that this was causing a major problem for their customers.  You know, there was a lot of, like, we really understand how much you need this back.  And believe us, we're working on this as hard as we can.  Yet they never really told us what was going on.



Anyway, I came away - first of all, I thought it was my problem.  I didn't immediately - I didn't even know that there was like a status log page until I dug in.  So one of the things that I discovered, I know that a lot of our listeners are now Sync.com users, probably as a consequence of me having said, yeah, it's been years, and I love it.  I'm still using it.  Anyway, what I discovered was that the client, the desktop clients do not update themselves.  Or, if they do, mine weren't.  Like they got stuck or something.  Anyway, the world, the Sync.com world has gone to 2.1.7.  What am I saying?  2.1.4.  Sorry.  Don't go crazy.  2.1.4.  I had to manually get that and run the latest version in order to bring my clients current.  They were, you know, I've had Sync for years; and, like, they were really old.  So everything seems to be like working really well now that I actually have clients that were written in this century.



The good news is when you run it, it seamlessly installs itself and, like, removes the old one, puts in the new one, gets us all set up and going.  So I just wanted to say, yup, if you were affected by this Sync.com update, I feel for you.  I was, too.  They were saying, oh, some of our users are experiencing problems.  Really?  Okay.  I would love to hear, for example, if there are Sync.com listeners of this podcast who had no problems at all for the last week.  That would be interesting to me because maybe it was just some people.  I don't know.  Okay.



LEO:  By the way, you use, you mentioned Syncthing.



STEVE:  Yes.



LEO:  And of late, since Syncthing is open source and completely local, I feel like I don't know if I need a cloud because I have everything on every computer.



STEVE:  Yup.



LEO:  And then I was thinking, well, one advantage of the cloud is it's always on.  So you use a NAS, as I do, a Synology NAS.  You could put Syncthing on your Synology NAS.  So I'm basically treating my Synology NAS as the canonical copy of all that stuff.



STEVE:  Yup.



LEO:  And it's pretty fast.  I use Syncthing to update source code.  I know that's one of the things you do is so that you can work at Lorrie's and at the office.  So it's pretty fast.  I will finish a project, close a window, go over to my other computer, and it's almost always there by then.



STEVE:  So I'm glad you mentioned it because I should have.  I have Syncthing on both of my NASes, both the Drobo and the Synology.  I use Syncthing to synchronize some directories between them.  And Syncthing is what we use with Lorrie's fleet of laptops which she has out to her clients to keep them all synchronized.



LEO:  Smart.  So they all have the same stuff on them.  That's smart.



STEVE:  Yeah, yeah.  It is absolutely a win.



LEO:  It's free.  It's open source.  It works.  It does NAT traversal.  No, I just - I love it.  It's really, really good.  In fact, it's really made me rethink how I back stuff up since I have a copy of everything everywhere.



STEVE:  Yup.



LEO:  Without being in the cloud at all.



STEVE:  Yup.  Okay.  So I titled this piece "Medibank Reboot" because that might literally be what happened this past weekend.  We've been recently following the drama with Australia's latest private medical insurer, Medibank.  That's the one that exposed a huge number, well, a huge number of its current and past customer data.  9.7 million clients got exfiltrated on the dark web, and then it got released into the public when they refused to pay.  Well, on Friday the Sydney, Australia Morning Herald had an interesting bit of news.  Their coverage began:  "Private insurer Medibank's app, stores, contact center, and IT systems will all go dark this weekend as it overhauls its cybersecurity following the nation's worst data breach in corporate history."



They said:  "From 8:30 a.m. Australian Eastern Daylight Time on Friday" - I'm sorry, p.m., 8:30 p.m., end of the day in Australia on Friday, p.m., Eastern Australia.  They said:  "Australia's largest health insurer will shut down its IT systems followed by retail store and customer contact center closures on Saturday to 'further strengthen systems and enhance security protections,'" was the official line from Medibank.  They said:  "The company expects normal activity to resume by Sunday at the latest."



They also said:  "Microsoft IT security experts from the Asia-Pacific region will travel to Medibank's Melbourne headquarters to assist with the operation."  And then I kind of quipped to myself, I guess Microsoft is going to show them where the Update button is located on their server.  And they said:  "This was said to have been planned over several weeks and will be Medibank's first shutdown of such scale."  Well, yeah, you can imagine.  This is, like, for a massive private insurer to shut down everything.  So in other words, you know, shut down, update software and firmware and everything else, then turn everything back on again.



So the Herald finished by saying:  "The overhaul is part of a series of maintenance strategies, termed 'Operation Safeguard'" - so, right, they gave this a big banner as part of this - "established after the personal information," they said, "of up to 10 million current and former Medibank customers was breached in a cyberattack.  The data was released on the dark web when Medibank refused to pay a 15 million" - we never had numbers before - "a $15 million ransom demand by the hackers, who police have said were based in Russia.  The company said the damaging cyberattack will cost the firm at least $35 million in initial recovery costs, though that is likely to grow as law firms and regulators circle.  A Medibank spokesperson said, although there had been 'no further suspicious activity' detected inside its systems since October, the insurer was carrying out further maintenance to strengthen its online security."



Now, okay.  If we can read between those lines, what might be happening is a complete wide-scale coordinated reinstallation of system software.  I mean, not just an update.  As we've noted before, and Leo, you and I have talked about the problem after something has been compromised is, in a complex system, you can never really be certain that something isn't still hiding somewhere.  So, you know, imagine you're in their shoes, and there was - and we don't know, right, what the forensic examination actually found.  It might have left them horrified or terrified, you know, and feeling that they had no choice other than to just wash everything clean and reinstall.  And boy, if so, what a nightmare.



Oh, and they did say:  "Since the hack, Medibank has bolstered monitoring, added detection and forensics capability across its system, and scaled up analytical support via specialist third parties."  Right, so they brought a bunch of people in.  And they said:  "It also recently introduced two-factor authentication" - oh, imagine that - "where access is granted only after providing a code sent to one's email or SMS."  Oh, okay, so not very good two-factor authentication, but better than none.  Anyway, so being completely down and offline for as many as two days sure does sound like a major sweep cleaning of all mission-critical systems, and they probably had no choice.



It was P.T. Barnum who is credited with the saying "There's a sucker born every minute."  I was reminded of that when I came across this bit of news about a cybercrime group that's been named "CryptosLabs." The cybercrime research group known as Group-IB, which we talk about from time, identified a new cybercrime operation which they named CryptosLabs.  Okay, get this.  Since 2018, so four years, this CryptosLabs group has operated a network of more than 300 scam websites posing as fintech, you know, financial technology, and cryptocurrency trading platforms.



Group-IB says the group used search engine ads and social media posts to trick French-speaking users across Europe into investing more than 480 million euros, okay, so nearly half a billion euros, in these scam websites by leading them to believe that they would get to trade in stocks and crypto assets.  But researchers say that once users put money into their accounts - their, you know, bogus accounts - the crooks either asked for more, you know, get as much as you can; right?  Or ignored their customers before shutting down platforms and moving to a new domain.



Group-IB said it named this group CryptosLabs after the kit they used to automate the deployment of fake trading portals.  Right.  Since we're going to be doing this a lot, let's create a kit that makes it real quick to set up a new fake trading portal because, you know, we're not going to be there long.  We're going to have to do it again soon.  Okay.  It typically, these portals typically mimicked 40 different popular banking, fintech, and crypto brands.  So, you know, these guys were pretending to be Coinbase, and set up one fake cloned Coinbase site after another.



People, you know, had heard of Coinbase.  Their friends were talking about it.  Then they encountered an advertisement in a search engine or in social media, and they thought, hey, I just got paid, now's the time.  They didn't really know what Coinbase's domain name was, so they just clicked the advertisement and went to Coinbases.org.  And since everything looked quite official, they never thought that it might be an illegitimate ad and site.  People would transfer money in; and, at some point, once enough had, or someone wanted their money back, the fake domain would be shut down and another would be set up in its place using this crypto slab tool that just, you know, just spits out these fake platforms.



So this is not the Internet that Tim Berners-Lee envisioned back in 1989 when he originated the concept of an Internet full of interlinked HTML documents that anyone could create and publish on their own.  It's not as if crime hadn't existed before.  It's just flowed into this new medium.  So that's going to happen.



Two interesting pieces of news about Telegram.  First, malware on Telegram.  The Russian security firm Positive Technologies published a report on Telegram's budding cybercrime ecosystem.  According to the company's scans, Telegram has slowly replaced hacking forums and is currently being used for advertising a wide spectrum of hacking services and malware.  The sale of remote access trojans, corporate network account credentials, and cash-out services are among some of the most popular topics on Telegram now.



Okay.  So there's one tidbit.  But get a load of this one.  Telegram, which now we know is generally becoming the favored hangout of the crime underworld, has decided to further expand their subscriber base by allowing users to sign up without needing one of those pesky SIM cards to anchor their identities.  Telegram wrote:  "Today starts a new era of privacy.  You can have a Telegram account without a SIM card, and log in using blockchain-powered anonymous numbers available on the Fragment platform."



Okay.  So, what?  I thought, what's Fragment?  So I followed a link, https://fragment.com, and I was told:  "Oops.  This service is not available in the United States."  Okay.  That's interesting.  I wonder why not?  So I thought that Wikipedia might know about Fragment, and perhaps it does.  But the word "fragment" is so common that I wasn't able to find it there among all of the other fragments.  Googling turned up an abbreviated reference that was more tease than anything else.  Google said:  "Buy and sell usernames.  Secure your name with blockchain in an ecosystem of 700-plus million users and assign it as a link for your personal account," dot dot dot.  And that's where the little summary cut off on Google Search.



I thought, buy and sell usernames?  What?  Now, I know that Kevin Rose might be willing to sell you an icon of a zombie.  But what's Fragment?  So I dug in some more, and I found some news about Fragment over at Crypto.news, where their coverage had the headline "Telegram now allows users to buy and sell usernames via auction."  And then it goes on to explain:  "Telegram releases new feature, transforms usernames into digital assets."  Then it says:  "Popular cloud-based instant messaging app Telegram has just launched a new feature to allow users to buy and sell short, recognizable '@usernames' for personal accounts, public groups, and channels.



"Telegram has commenced an auction for the best usernames on Fragment, a free collectible trading platform.  With this new feature, Telegram usernames have become digital assets that can be secured and sold between parties.  According to the innovative platform's unveiling note, ownership of the collectible usernames is secured in the immutable ledger of TON" - (T-O-N), and it goes on - "a fast and scalable blockchain network," which no one's ever heard of before.  "Interestingly, the new feature allows owners to add multiple username aliases to their personal accounts, group, or channel.  Also, each collectible name can be accessed with its @username on Telegram or outside Telegram using links such as username.t.me and t.me/username.



"To acquire usernames on Telegram, buyers visit Fragment, search for their desired" - unless you're in the U.S., I guess.  For whatever reason you can't have a Fragment in the U.S., but here a VPN might be your friend - "search for their desired username and click on auction if that username is still available."  So, okay.  Telegram is like tied in with Fragment somehow.  And now you have to buy your Telegram username on the blockchain.  So then it says:  "Buyers will then be redirected to a page which shows the highest bid along with the bid setup and minimum bid."



Okay.  So earlier this said that TON was an immutable ledger.  Apparently it's also a currency.  I went over to CoinMarketCap.com/currencies/toncoin, and I learned that a TON has a current value of $2.10 U.S.  And it was fluctuating as I was there at that page, couldn't make up its mind between $2.10 and $2.11.  Also it's got a 24-hour trading volume of $44 million.



LEO:  Whoa.



STEVE:  Yeah, $44,628,950.  That was yesterday, 24-hour cycle.  And I saw that there was a TON.org.  So I went over there, and I discovered that TON stands for The Open Network (TON).  And from the TON homepage we learn that:  "TON is a decentralized layer-1 blockchain designed by Telegram to...



LEO:  Ah.



STEVE:  Uh-huh, so the loop closes.  "Designed by Telegram to onboard billions of users."  They hope.  "It boasts ultra-fast transactions, tiny fees, easy-to-use apps, and is environmentally friendly."



Okay.  So let's get this straight.  Telegram noticed that they had a lot of users and a popular platform.  So they decided that they wanted to monetize the ownership of Telegram usernames.  They wanted to create a marketplace which would allow Telegram usernames to be bought and sold.  So they created TON, their own cryptocurrency, anchored it with their own blockchain.  They then established an auctioning system which uses the TON as its exchange currency to allow their users to bid for, purchase, and sell Telegram usernames.  The rest of the coverage of this, the first part of which I already shared, tells us how this is going.



Under the heading "Massive instant adoption of new feature; millions of TONs earned in username sales," we have less than - this is the reporting of this.  "Less than six hours after the launch, thousands of usernames featuring international brands and celebrity accounts have been put up for sale.  Still on auction are @nike, @king, @esport; while others such as @auto, @avia, @fifa, et cetera, have been sold for as much as 900,000 TON."  Now, we don't know what the TON was worth when it sold.  But the TON is now $2:10.  So that's, what, $1.8 million, thereabouts.



Judging by data on the Fragment platform, millions of TONs, you might say a ton of TONs, have been earned by Telegram users from the sales of their short usernames.  So users who were on Telegram early, got in, got a short name, those are desirable, they're now able to cash in on their short username by selling them for TONs, and then liquidating TONs for cash.  There's still more to be made as there are still lots of usernames currently on auction, the report says.  "An example is the popular shoe brand @nike, which has over 300,000 TON bid for it at the moment.  Telegram is affording its users full ownership of their usernames, and they are embracing the idea."  So think about that.  There's no trademark protection, I guess.  So anybody could buy @nike on Telegram who is willing to pay enough.  And that's got to make Nike a little nervous; right?  Because Telegram, you know, it is a happening place right now.



LEO:  So I think it's more complicated because I don't think they actually own TON.  Telegram - I love Telegram, by the way.  And, you know, you can look at what you just described in, as you did, kind of...



STEVE:  Askance.  Askance.



LEO:  Askance.  But also if you're trying to solve this issue of, and this is an issue, it's an issue with Signal, having to tie your account to an actual phone number...



STEVE:  Yeah.



LEO:  ...you'd need to do some sort of - I think blockchain is actually, this might be one good application, blockchain, some sort of decentralized authentication system.  My short name is @leolaporte so, you know, I own that.  But I didn't buy it with TON.  I just always have had it.



STEVE:  Right.



LEO:  So when you started talking about this, I said, well, wait a minute, doesn't Telegram have its own crypto?  And they did.  They started something called the Gram in 2018, which the SEC...



STEVE:  That's kind of a good name.  I like that.



LEO:  It was a great name.  SEC halted it because they were doing an ICO.  Remember when that was the big thing, the Initial Coin Offerings?



STEVE:  Right.



LEO:  And they had registered the ICO.  So the SEC halted it.  So Telegram abandoned TON, which stands for The Open Network.  And it's open source.  So developers have kept it going.



STEVE:  Ahhh.



LEO:  According to The Verge, Pavel Durov, the owner of Telegram, said he supported the project a year ago, saying I'm proud that the technology we created is alive and evolving.



STEVE:  Cool.



LEO:  So it is a third-party effort.  And I agree, anytime I hear crypto I go, okay, what's - and especially if you're selling usernames.  You know, this is a - you know.  But if you think about it, if you wanted to replace phone numbers, you need some sort of unique fingerprint; right? 



STEVE:  Yeah. 



LEO:  Could do what Threema does and, you know, meet in person and do it.  But there's some sort - Threema generates basically a private and public key chain.



STEVE:  Correct, correct, correct.



LEO:  Key pair.  But how do you get your key out there is the problem.  So this is an interesting solution.  I wish Signal would do something that didn't require a phone number because I think that's a problem.



STEVE:  Yeah.  And it definitely is a privacy concern, right, because you've got to have something that is, you know, anchored to a SIM.



LEO:  Plus Signal might - any Signal app I run is attached to a one phone.  I can't put it on another phone without...



STEVE:  Right.  You're only able to have it, like you're able to sync a desktop, but not another phone.



LEO:  Exactly.  And with Telegram I can.  I have Telegram everywhere.  So, you know, different strokes.  I agree there's, you know, potential for misuse, obviously.



STEVE:  So I just - I wanted to mention, because this is in the news, in the technology, that the TikTok banning has continued.  Texas has now joined the ranks.  And I was going to say that Texas was the latest, but that was yesterday.  So who knows what's happened since then.  Texas Gov. Greg Abbott has banned the use of TikTok on the devices of state employees, and in doing so becomes the fourth, happens to be Republican-led state  to ban TikTok on employee devices.  That follows Maryland, South Carolina, and South Dakota.  Remember that last week it was South Dakota that I noted was first, with their Gov. Kristi Noem saying that she hoped other states would be following suit.  I guess her wish is coming true.  Greg Abbott also ordered state agencies - this was interesting - to come up with plans to govern the use of TikTok on state employees' personal devices, not owned by the state.



LEO:  Yeah.  Good luck.  You can't do that.



STEVE:  So yikes.



LEO:  That's just performative.  You can't do that.



STEVE:  Yeah.  He wrote in a letter to state agency leaders that there are "growing threats posed by TikTok" to the state's sensitive information.  "TikTok harvests vast amounts of data from its users' devices - including when, where, and how they conduct Internet activity - and offers this trove of personally sensitive information to the Chinese government."  Okay.  But wait, there's more.



Indiana's attorney general brought a pair of lawsuits against TikTok, accusing the company of deceiving users by claiming that their data was protected from the Chinese government and for exposing Indiana children to adult content.  The lawsuits claim that the Chinese-based social media giant has deceived and harmed Indiana residents.  Indiana's first lawsuit alleges TikTok has marketed its video-sharing platform as safe for teens, even though its algorithm "serves up abundant content" depicting drugs, sexual content, and other inappropriate themes.  The second lawsuit asserts that TikTok has deceived consumers by suggesting their personal information is protected from the Chinese government and the Communist Party.



Okay.  So the attorney general said in a statement:  "The TikTok app is a malicious and menacing threat unleashed on unsuspecting Indiana consumers by a Chinese company that knows full well the harms it inflicts on users.  With this pair of lawsuits, we hope to force TikTok to stop its false, deceptive, and misleading practices, which violate Indiana law."  So the attorney general is asking for emergency injunctive relief against the company and is seeking monetary penalties for every time TikTok violated Indiana's Deceptive Consumer Sales Act.  Wow.



LEO:  How do you violate a sales act if it's free?



STEVE:  Yeah.  I was wondering if maybe there was something I had missed.  So I have it in the show notes.  I'm not going to drag our listeners through it.  But Kaspersky has a very useful, factual, piece-by-piece walkthrough, and the upshot is there is no smoking gun here.  There's nothing that TikTok is doing that any of the other social media platforms, most notably Facebook, being the largest and here in the U.S., isn't also doing.  I thought, okay, what's on the other side of this?  How about a balanced look?  So I found some coverage that NPR offered from less than a month ago about the FBI's raising of concerns, which apparently is what started all of this, over what TikTok might be capable of doing.  And what's interesting is the quotes from FBI Director Christopher Wray during a Homeland Security Committee meeting.  So NPR's story says:  "The FBI alleges TikTok poses national security concerns."  Right?  Okay.  So, okay.  That's a concern.



So NPR says:  "The head of the FBI says the bureau has 'national security concerns'" - and they even quoted that - "about the U.S. operations of TikTok, warning that the Chinese government could potentially use the popular video-sharing app to influence American users or control their devices."  Gee, like what?  Facebook?  Like Twitter?  Like everything; right?  Anyway, sorry.



"FBI director Christopher Wray told a House Homeland Security Committee hearing about worldwide threats on Tuesday" - this was middle of last year, it was like the 17th, I think, of November - "that the FBI has 'a number of concerns' just days after Republican lawmakers introduced a bill that would ban the app nationwide."  How do you do that?  Okay.  Anyway, Wray said, I mean, this is sounding like, right, some other country we talk about somewhere else.  Wray said:  "They include" - the concerns, that is - "include the possibility that the Chinese government could use it to control data collection on millions of users or control the recommendation algorithm, which could be used for influence operations if they so chose, or to control software on millions of devices, which gives it an opportunity to potentially technically compromise personal devices."



And then NPR reminds us:  "TikTok, which hit one billion monthly active users in September 2021, is owned by the Chinese company ByteDance.  Chinese national security laws can compel foreign and domestic firms operating within the country to share their data with the government upon request, and there are concerns about China's ruling Communist Party using this broad authority to gather sensitive intellectual property, proprietary commercial secrets, and personal data."  Blah blah blah.



So, you know, concerns.  And based on concerns and China and communism, what we see to my reading is a bunch of grandstanding by governors and attorneys general who want to make a big deal about this because it's owned by a Chinese company.  We've seen misbehavior on the part of our domestic firms, who are looking at things that they shouldn't.  There was some BuzzFeed news had some audio.  Because I spent some time digging into this, wondering what the heck.  There was some audio that BuzzFeed's News found of some TikTok employees clearly looking at the data of some TikTok users, much like Facebook has been caught looking at their own users, and Twitter was caught doing the same.  So anyway, to me this looks like a bunch of nonsense.  We'll see where it goes.  I would be surprised if we end up with legislation banning TikTok.



LEO:  Did Kaspersky go into what information TikTok knows about you?  I mean, because that's I guess the fundamental question.



STEVE:  Yes.



LEO:  There's two parts to this.  A, is TikTok giving information - let's assume the Chinese government has a pipeline into ByteDance.  I mean, that's a big assumption, but let's assume they do.  Is TikTok gathering - what kind of information is it gathering from my phone that it's sending to the Chinese government?  That's problem number one.  And then problem number two is people say, well, you know, they could use the algorithm to propagandize us, you know, to convince us of something.  Which is legit, although the Chinese government is not unhappy to use Twitter and Facebook and YouTube to do that, as well.



STEVE:  Right, they don't need TikTok.



LEO:  They don't really have to have some other way to do it.  But what did Kaspersky say about - is it any different from any other app on the phone?



STEVE:  No.  Kaspersky directly addresses the question of privacy concerns.  They wrote:  "One of the most viral aspects of TikTok has been privacy concerns, with questions like 'What data does TikTok collect?' and 'Does TikTok steal your information?' regularly circulating online."  They wrote:  



"Like many other social networking platforms such as Facebook, TikTok collects a lot of information about its users.  This includes every TikTok video watched, and for how long; the entire contents of every message sent through the app since messages are not encrypted; the user's country location, Internet address, and type of device being used.  And with the user's permission, TikTok also captures its user's exact location, rather than just their IP address; their phone's contacts and other social network connections...



LEO:  But you have to give them that because...



STEVE:  ...in order to build a graph.



LEO:  It's always asking for my contacts, and I always say no.



STEVE:  Okay.



LEO:  I think I'm assuming that Apple blocks it if I don't say yes.  I mean, that they're not sneakily, underhandedly...



STEVE:  Oh, I bet Apple would.



LEO:  Yeah.



STEVE:  And finally, their age, phone number, and payment information.  Again, if you say yes, you allow it.



LEO:  You provide it.  Unless you say you're 98 and you live in Muncie, Indiana.  Which you could, yeah.



STEVE:  Yeah.  So Kaspersky says:  "This information can be used to assemble a detailed profile for advertisement targeting - by understanding who its users are, who their friends and family are, what they like and find entertaining, and what they have to say to their friends.  To use the app, users grant access to their microphone and camera.  If they create videos, the app captures close-ups of their face.  Potentially..."



LEO:  And by the way, again, you have to do that only if you're going to use it to create videos.  I don't.  I use it - and most people don't.  They use it to look at videos.



STEVE:  To watch, yes.



LEO:  And every time it says you want the camera access, I say no, you can't - no.  I don't want you to have that.  So that, again, that's something you turn on.



STEVE:  Right.  So they said:  "Potentially, this provides biometric data which could be used in conjunction with other images which exist online.  TikTok also uses technical measures to encode its activity.  This means that some of what it does is hidden from external researchers, which all the apps do.  TikTok says this is to disrupt hackers and other malicious actors.  There's been extensive media coverage of TikTok privacy concerns.  However...



LEO:  Yeah, think how mad we'd be if TikTok was sending all that information in the clear.  You know?  That's not good either.



STEVE:  The way all of our web pages used to be.



LEO:  Right, right, yeah.



STEVE:  And Kaspersky said:  "However, most social media platforms worldwide collect, use, analyze, and ultimately profit from users' personal data.  TikTok argues that it collects less data than platforms such as Facebook or Google since it does not track user activity across devices."  So again, to me this is all just, you know, the FBI saying, well, we have concerns because maybe, you know, "an opportunity to potentially technically compromise personal devices."  Right.  Because you loaded an app.  All apps have that opportunity to potentially technically compromise personal devices.  Anyway, to me this is protectionism; right?  It's xenophobia and protectionism and an opportunity for so far it's all been Republican political people.



LEO:  It's performative.  It's performative.



STEVE:  To say, yeah, the Commies are going to get our kids.  Okay.  And while we're on the subject of nonsense, filed two days after LastPass's November 30th disclosure, password management company LastPass has been hit with a class-action lawsuit after experiencing two data breaches in the past three months, which we've talked about.  In fact, it was the title of our podcast a couple weeks ago.  The plaintiff in this case, DebtCleanse Group Legal Services LLC, filed the class-action complaint against LastPass US LP on December 2nd in a Massachusetts federal court, alleging negligence and breach of contract.  The Chicago-based debt relief firm said it used LastPass to manage its passwords.  However, it says LastPass was negligent in its duties, evidenced by the fact that it has experienced two data breaches in the past three months.



The class-action lawsuit alleges that LastPass was negligent with data security, stating that LastPass used ineffective data security measures to protect its customers' information.  The lawsuit states:  "There is a strong probability that entire batches of stolen information have been dumped on the black market, or are yet to be dumped on the black market, meaning plaintiff and class members may be at an increased risk of fraud and identity theft for many years into the future."



So DebtCleanse responded to news of the breach by changing all of its employees' passwords for accounts that used LastPass, which took considerable resources, it said.  It seeks to represent all LastPass users whose information was accessed in the breach.  Well, that's a very small number.  It seeks certification of the class action, damages, fees, costs and a jury trial.



So, okay.  The geniuses over at DebtCleanse freaked out and totally unnecessarily changed all of their passwords for everything.  They noticed that LastPass made an announcement of a secondary breach following on from the first one.  But they somehow failed to heed the statement - which was also right there, both times - that no user information or passwords were at any time at risk or exposed as a consequence of either of those two breaches.



Now, Leo, I know you and I feel quite similarly about class-action lawsuits.  You know, from time to time I'm awarded something like 92 cents from something I purchased once that faded too quickly if it was left out in the sun or whatever.  Somewhere some lawyers made some money.



LEO:  Yeah, that's really it, yeah.



STEVE:  The individuals in the class on whose behalf the action was taken netted 52 cents, you know, or whatever.



LEO:  There is one secondary value to class-action lawsuits.  Companies still have to pay that money.  And it certainly has sometimes a salubrious effect on the company, maybe stopping them from doing that again, if you know what I mean.



STEVE:  Yeah.



LEO:  So there is that.



STEVE:  Well, and unfortunately, I mean, I can't even imagine what a trial would look like because this is technical stuff.  I mean, you would like it just to be dismissed when under deposition LastPass techie says nothing that happened could have possibly compromised anyone's passwords.



LEO:  Yeah, and how do you prove that to a judge and jury who are not technically sophisticated?



STEVE:  Oh, oh, oh.



LEO:  They bring in you, somebody like you, to say, well, I'm an expert, and in my judgment it's safe.  And what else are they going to do?  And just, you know.  You used to do that, didn't you, testify?



STEVE:  I did.  I did.  And when I tried to explain to a judge who literally had - I will never forget this - to his right was a green oxygen tank.



LEO:  Oh, geez.



STEVE:  And he had the nasal cannula up his nose and a mask in case he needed a little extra hit, you know.



LEO:  Holy cow.



STEVE:  I mean, this is who was, you know, judging this.  And it went the wrong way.  And I thought, okay, I'm done here.  This is dumb.



Okay.  But we're not done.  A tiny piece of Rackspace's overall cloud hosting business hosting Microsoft Exchange email services, was hit 11 days ago by ransomware.  Things have not been going well for them ever since.  An article published yesterday in IT World Canada summed things up and offered some interesting perspective on the cloud industry overall.  Here's what IT World Canada wrote.



They said:  "On December 2nd, Rackspace experienced an outage for its Hosted Exchange environment.  The company blamed a 'security incident.'"  Unfortunately, first mistake was they didn't say what it was.  "A status update issued by the company noted:  'We proactively shut down the environment to avoid any further issues while we continue work to restore service.'"  Okay, now, that really stretches the phrase "putting a good face on the problem."  What, they were too embarrassed?  I guess.  They should have just said "We got hit with ransomware" right from the start because that happens now, unfortunately.  Anyway, back to what IT World Canada said.  They said:  "One week later, the outage continues."



LEO:  Wow.



STEVE:  Yes.



LEO:  Sounds like that Medi story, wow.



STEVE:  "And the company has confirmed that it is due to a ransomware attack.  Rackspace has not indicated how much data might be lost, whether it will pay the ransom, or when the managed exchange service will resume.  This is the only information from the section of the website dealing with the press.  However, in an announcement on its investors page, the company notes that the hosted exchange business accounts for less than 1% of the company's revenue."  Which, by the way, is a hefty $3 billion per year.  And on the page they reassure investors that the company has cyber insurance.  And I guess they're going to be needing it.  We'll get to that in a minute.



But World IT said:  "But the attempt to reassure investors may not be working.  In an article on December 10th" - so that would be last Sunday - "investment blog MarketWatch criticized the company for being "frustratingly closed mouthed" about the incident, and noted that the company's stock price had declined.  The article notes:  "Since the incident came to light, Rackspace shares have tumbled by a third.  This is a relatively small part of the company's business, only about $30 million a year in revenue," right, against 3 billion in total, so yeah.  That's 1%.  "But the bad blood that Rackspace is generating will leave a lasting stain."



The stinging critique of the company's communication is significant, but another quote from the article raises an issue that could extend beyond Rackspace to the entire cloud industry.  The writer notes:  "While I remain a big believer in cloud computing, the Rackspace attack is an urgent reminder of the risks in relying on cloud for mission-critical applications if your provider isn't keeping up with software patches and paying attention to security risks."  So the use of cloud computing, even for mission-critical applications, has grown rapidly for years, but that growth has accelerated in the past year and is predicted to further accelerate in the next 24 months.



And, you know, when we had Paul and Mary Jo on Windows Weekly, they were often talking about how Microsoft is like, Microsoft doesn't really care about desktop anymore; right?  They just - it's all Azure.  And we know that AWS is a massive thing for Amazon.  So sure enough.  And it's funny because I so clearly recall the looks I received from the IT guys who attended DigiCert's Customer Advisory Board meeting six years ago that I was also invited to.  I made some offhand comment about the rack of gear I had over at Level 3 which brought all discussion to a halt.  I said, "What?"  And one of them replied, apparently for the entire group:  "Nobody does hardware anymore."  Oh.  Well, I do.  You know, I like hardware with lots of little blinky lights.



But anyway, this piece ends:  "Senior management has bought into cloud in a big way.  But could investor nervousness from the Rackspace outage have an impact on attitudes in the boardroom?  When a service that gives you 1% of your revenue leads to a 30% drop in your market share, cloud proponents may, to quote Ricky Ricardo," says the article, "have some 'splaining to do."



And that was just the first shoe to drop.  The second shoe was, yes, Rackspace is now facing not one, not two, but at least three class-action lawsuits.  Reports are that Rackspace will need to defend themselves in so far at least three different class-action lawsuits related to this recent ransomware attack from which they're still recovering and may never be able to fully recover from.  The attack left countless companies - unfortunately it was tons of small and medium-size businesses that had all outsourced their email to Exchange Server hosted by Rackspace, and they just got wiped.  So it's not clear that it's coming back.



In an interview last week, Rackspace suggested they may not be able to recover all their customers' data, which they referred to as "legacy data."  The company also appears to have given up on hosting Exchange email servers in its cloud - yeah - and said it was migrating all its existing customers to Microsoft 365.  Right, give it to Microsoft.  Which, according to documents Rackspace filed with the SEC will cost the company 30 million, right, because that's that 1% of its revenue.  But who cares?  It's certainly not worth the headache.  I imagine that hosting Exchange Server was more of a means to an end for Rackspace, you know, a way of establishing a relationship with an enterprise and then, over time, probably moving more of their non-email business into the cloud.  You know?  That didn't work out so well.



Okay.  The trend appears to be, what we're seeing broadly, is everyone is getting very tired of the consequences of these apparently never-ending attacks.  Governments are going on the offensive, saying that they're no longer going to be waiting to be attacked.  They're going to go proactive.



LEO:  Australia.



STEVE:  Yes.  Other governments are calling their own employees criminally negligent and bringing them up on criminal charges.



LEO:  I don't know if that's fair.



STEVE:  Right.  And the class-action ambulance chasers now only take a day or two to file their lawsuits.  Why is all this happening all of a sudden?  Well, we know, because criminal organizations, apparently some with state-level sponsorship and protection, have realized that the cryptocurrency boom, coupled with the presence of exchanges to local fiat currencies, provides them with a means of being anonymously paid.  That provided the motivation.  Clever hackers provided the means.  And the final piece, endless opportunity, was readily supplied by the industry's historically lax cybersecurity.



So today everyone is furious, beside themselves, pointing fingers and suing.  But who's still never being held to account?  By a perversion inherent in the system we've built, the suppliers of the buggy, brittle, and breakable systems are never to blame.  I would say stay tuned.  That's going to be next.  Juries are made up, after all, of end users, and they're not going to be sympathetic after everything this industry has been putting them through lately.



Oh, and one last piece.  Speaking of losing patience, another country goes on the offensive.  On December 11th it was reported that Japan, kind of quiet, peaceful Japan, intends to establish a legal framework that will allow strengthening of defense measures in cyberspace, including the ability to attack preemptively.  The Japanese government intends to change the rules so that it can start tracking potential attackers and breaking into systems, the attacker systems, as soon as there's a potential threat.  Current regulations make it extremely difficult to apply such measures unless there's an emergency situation that requires the mobilization of defense forces after a military attack.  The plan is reflected in a proposal to amend the National Security Strategy that has just been submitted to the governing coalition.  The draft amendment is expected to be approved by the cabinet before the end of this month.  So we're clearly seeing a changing of attitudes across the board.  Wow indeed, my friend.



Okay.  Get this, Leo.  The following was sent to me via Twitter DM, and I'm not sure this individual would want his name published so I'm not sharing it.  But he's the Australian cybersecurity person who was tasked with finding and downloading the Medibank data from the dark web for analysis.  He sent the following.  He said:  "Hi, Steve.  I just wanted to drop you a line about the Medibank data.  You mentioned in Security Now! 900 that you didn't know what use the data would be if it wasn't formatted properly.  I'd like to offer one way it could be useful in the wrong hands.



"I work in cybersecurity for a law enforcement government department in Australia, and I was tasked with finding and downloading the Medibank and Optus data.  I absolutely agree, the data held within the extracted data was not formatted and was of limited use in its raw format," he says, "(all CSV files with different structures presumably depending upon the database it was extracted from)."  He said:  "It's true that the data is of limited use as the medical information about the patients was all in code format which meant nothing without the application to match the code to the treatment the patient received.



"However, my job was to search through the data to see what law enforcement officers' details were leaked.  Can you imagine how bad it would be if an individual used their government-issued email address, which has their law enforcement division within the domain of the email address, to sign up with Medibank?  Suddenly you would be able to match a name and home address of a person, and verify it was a person working for the law enforcement division thanks to their email address.  In the hands of the wrong people, such as criminal gangs, this information could put these law enforcement officers in serious danger.  Sadly, data of this nature was leaked, and our organization now has to help protect these individuals.  I'm sure you've already considered this scenario with leaked data.  Really, any leaked data could be used this way.  But considering the high-profile nature of this leak, we're having to take it very seriously.  Thank you for such a wonderful podcast."



And thank you for the very interesting note from the field.  You know, it's abundantly clear that this podcast has aggregated quite an amazing group of listeners.  I am continually humbled and flattered by the people who take the time to listen to these ramblings every week.  So thank you all.



Paul Jolley wrote:  "I know you've spoken of UTF-8 in past episodes, but was left asking myself what could possibly be the legitimate purpose of some characters after I encountered [and then he has a] U+200B [so that's a Unicode character] recently."  He said:  "I appreciate English isn't the only language used in the world, so I understand why this extended character set is useful; but, as previously described on the podcast can also see how it can be abused - typo-squatting in domain names, for example - by using characters that look like their ASCII equivalents.



"Getting back to my recent experience," he said, "I received an email where the sender name would not sort alphabetically in my email client.  I didn't think much of it at first, but then decided to investigate.  It turns out that they had inserted a 'Zero Width Space' character..."



LEO:  Wow.  Wow.



STEVE:  ...before the first letter of their name, making it appear at the top of my sorted list.



LEO:  That's very sophisticated.  Holy cow.



STEVE:  Yeah.  "The three bytes could be seen in a hex editor as E2, hex 80, and then hex 8B; but would not visibly appear in Notepad."  He said:  "I've never encountered this kind of whitespace before and thought it would be worth mentioning because I expect something like this will only be used for mischief.  Try it yourself by renaming a folder in Windows and putting this special character at the beginning to make your folder appear at the top of a sorted list without any visible whitespace, or paste it in Notepad a few times and see the file size increase when you save the file but there's nothing to select or highlight on the screen.  The world doesn't need this."  So anyway, another lesson here is that for every useful thing we create, there are clever people who will subvert that use into abuse.



LEO:  Yeah.



STEVE:  A SpinRite note.  SpinRite is currently at alpha release 5 with many further improvements already implemented for its next alpha release.  Everything is going quite well.  Thanks to a truly gratifying level of engagement - we now have exactly 500, when I looked this morning, registered users in GitLab - we've been uncovering mysteries around the edges that I've been working to solve, and also things that I didn't anticipate.  And I should also mention that the people testing are also solving these mysteries, as well.



For example, in 6.1 it's possible to select a drive from the command line by its model number.  But Western Digital's model numbers contain spaces, and spaces are used as a delimiter to separate command line entities.  So the answer is simple; right?  Allow tokens to be quoted to have their contents parsed literally.  When I wrote the command line parser I forgot to add support for quoting literals.  So that's an example of something that needs to get fixed.  Someone booted SpinRite from a CD, and it didn't notify them that their request for logging to the CD could not be honored.  So those sorts of things.  They're things I want to take the time to get correct now so that I don't have to correct them later.



And we do have a couple of mysteries.  One guy who has two HP All-In-One PCs, if he warm boots from Windows 10 into SpinRite, SpinRite locks up as soon as it starts running, trying to actually run on the drive.  But if he cold boots the machine, everything works fine.  Okay, now, we might say, okay, so don't warm boot.



LEO:  Yeah, there you go.



STEVE:  Doctor, it hurts every time I raise my arm like this.



LEO:  Well, don't raise your arm like that, you silly boy.



STEVE:  Exactly.  So that might be all that, you know, we may end up saying, okay, you're going to have to cold boot.  But it could be a symptom of something more important.  So I found one of those machines on eBay for $47.



LEO:  Oh, my god, I can't believe you.



STEVE:  And it's in my car's trunk right now.



LEO:  Wow.



STEVE:  I'll figure out what's going on, and I'll answer the question that's best expressed using one of my favorite SpinRite development abbreviations which now comes up frequently.  The abbreviation is WTF.



LEO:  There's a lot of that.  I've been doing a radio show for 19 years that's basically filled with that.



STEVE:  Wow.  I have one piece of miscellany to share, and then we'll take our last break.  The ZimaBoard fanless single-board computer that I stumbled upon and told everyone here about has apparently been a big hit.  I keep seeing kind of casual, offhand mentions of it in the postings among those who are testing SpinRite, and also in Twitter messages.  Okay.  So given how long it takes to obtain one after ordering it from Hong Kong, and the fact that shipping is not free, I was surprised and delighted to learn a few hours ago that Amazon has all of them in stock and available for astonishing same-day arrival, not even next-day, today, at least in my location.  So as an Prime subscriber, the cost is $10 more than ordering the ZimaBoard directly from its source.  But there's no shipping cost.  So $129 versus 119, and you can have it today.  So anyway, I know that not everyone bothers being a Prime subscriber, but for what it's worth it's on Amazon.  So I just wanted to share that bit of happy news.



LEO:  All right.



STEVE:  I love when you talk about ExpressVPN, and you disconnect the VPN as [whisking sounds].



LEO:  It's gone [whisking sounds].  It's out of RAM [whisking sounds].  Complete with sound effects.  All right.  I really - so this was huge news last Wednesday, after the show, when Apple announced, yes, we're going to turn on end-to-end encryption in the cloud.  I just got it, iOS 16.2.  Everybody did.  MacOS had an update.  You can turn on advanced data protection, and it includes cloud backups.  But I knew I should not assume unless I listen to Steve and find out what he has to say about this.



STEVE:  Okay.  So last Wednesday Apple posted the news, which generated the most feedback, as I said at the top of the show, of anything that's happened recently.  So I knew we needed to address that today.  Before that, I was planning to talk about the technology behind a clever new generic bypass of web application firewalls.  Okay, nerdy, but cool.  And don't worry, we're going to get to that next week.  Today it's Apple time.



So Apple's headline reads:  "Apple advances user security with powerful new data protections."  The subhead, which introduces three new terms, reads:  "iMessage Contact Key Verification, Security Keys for Apple ID, and Advanced Data Protection for iCloud provide users with important new tools to protect their most sensitive data and communications."



Okay.  So we have three new things.  Apple explains that iMessage Contact Key Verification allows users to verify they're communicating only with whom they intend.  We'll see what that means in a minute.  Then there's Security Keys for Apple ID where users have the choice to require a physical security key to sign into their Apple ID account.  Okay, so that's going to be very cool.  And Advanced Data Protection for iCloud, which Apple explains users will be able to obtain what Apple says is end-to-end encryption to provide Apple's highest level of cloud data security where users will have the choice to enhance the protection of important iCloud data, including iCloud Backup, Photos, Notes, and more.



Okay, now, before we go any further I should note that this just became available, Leo.  What was said in Apple's announcement was that it was available for beta now and would be available for non-beta by the end of the year.



LEO:  Yeah, one week later.  Less than a week, yeah.



STEVE:  Okay, right, okay.  But that's only the iCloud backup.  The other two are early 2023 rollout globally.  And iCloud encryption also, the so-called Advanced Data Protection from iCloud, that's globally in 2023.  As far as I know it's only in the U.S. for now.



LEO:  I'm sure there are all sorts of issues of rolling it out globally.  So I understand, yeah.



STEVE:  Yeah.  Okay.  Let's look at each of those three security improvements.  The good news is that most of the information is available about the thing that people are most excited about, which is that not even Apple will be able to get in, type of true user encryption of user data stored in the cloud.  But because that's where the most information is, I'm going to save that for the last.  The bad news is that neither of the other two security improvements is explained anywhere that I've been able to find in much detail, but at this point maybe there's  not much detail to be had.



Of their iMessage Contact Key Verification, Apple only said:  "Conversations between users who have enabled iMessage Contact Key Verification receive automatic alerts if an exceptionally advanced adversary, such as a state-sponsored attacker, were ever to succeed in breaching cloud servers and inserting their own device to eavesdrop on these encrypted communications.  And for even higher security, iMessage Contact Key Verification" - which I think is where it gets its name - "users can compare a Contact Verification Code in person, on FaceTime, or through another secure call."



Okay.  And that's all we know presumably until it's rolled out early next year.  They do provide a photo showing a little emergency triangle icon, underneath which is written:  "An unrecognized device may have been added to" - and then whatever the account's name is, you know, Steve, Jenny, whatever, account.  And then they have a clearly clickable, in blue in their screenshot, "Options," dot dot dot, but we don't know what one's options might be.



Okay.  So this idea of a foreign device being added to an account covers one of the two theoretical ways that we know iMessage can be subverted.  Since iMessage conversations are broadcast to all of an account's registered and logged-on devices, if an adversary were to somehow get their own additional device added to a user's account, that additional device would automatically be privy to all of the attacked account's messaging.  So this seems like a useful feature.  Somehow Apple has arranged to be able to assure that in the event of that happening, this aspect of some sort of integrity checking won't be fooled, and that message would appear informing you that an unknown device was now sharing your account.



But since Apple handles all key management for its users, the iMessage attack we've always wondered and worried about is law enforcement serving Apple with a wiretap subpoena which would compel Apple to surreptitiously add an additional key into an iMessage conversation.  Since iMessage can already handle multi-party messaging, adding a hidden key seems eminently plausible.  And it's unclear under what grounds Apple could refuse a legal order for something they could probably do.  But that's all speculation for which there's no evidence.  The issue arises, however, any time any third-party manages keying on behalf of its users.  That's the convenient way to do it, but it's less secure.



The other piece of this announcement which we'll apparently need to wait to see, is this means for somehow comparing and confirming what Apple calls this Contact Verification Code, like in person, on FaceTime, or through another secure call.  So you're like reading it off of the screen to the other person.  This, I'll call it the CVC, is presumably, and hopefully, a hash of the user's public key which matches their private key, which is then converted into an ASCII code.  So it's not just binary gibberish or hex.  And note that this is exactly what my favorite messaging agent, Threema, which you mentioned earlier, Leo, has understood was important and has always done from the start.  So that's all we know for now about iMessage Contact Key Verification.  I'm sure we'll talk about it again next year once it's rolled out.



Next up is "Security Keys for Apple ID."  Apple explains it by writing:  "Apple introduced two-factor authentication for Apple ID in 2015.  Today," they say, "with more than 95% of active iCloud accounts using this protection" - that's nice to know, and that's a big number - "it is the most widely used two-factor account security system in the world that we're aware of."  And they broke their arm patting themselves on the back.  I should clarify that what we're talking about here is just the use of some other device to receive a six-digit code to authenticate a user.  So, yeah, everybody has that.  Okay.  Then they said...



LEO:  Although Apple, I don't know if you've experienced it, does it nicely by showing you the location of the person asking for the code on a map, and then you say "Allow," and then it shows you the code, I mean, they've done a nice job of that, I think.



STEVE:  Right, right.



LEO:  They're not texting you a six-digit code on your SMS number.



STEVE:  Right.  So they said:  "Now with Security Keys, users will have the choice to make use of third-party hardware security keys to enhance this protection."



LEO:  Woohoo.



STEVE:  Yeah.  "This feature is designed for users who, often due to their public profile, face concerted threats to their online accounts, such as celebrities, journalists, and members of government.  For users who opt in, Security Keys strengthens Apple's two-factor authentication by requiring a hardware security key as one of the two factors.  This takes our two-factor authentication even further, preventing even an advanced attacker from obtaining a user's second factor in a phishing scam.



LEO:  Woohoo.  That's the key; right?  Because you can phish that six-digit number.



STEVE:  Yes.



LEO:  But you can't phish a YubiKey because it's mine.



STEVE:  Nope.



LEO:  Yeah.



STEVE:  So we're getting hardware authentication dongles.  That's good.  And I imagine that Stina and the team over at Yubico is happy to see third-party hardware dongles becoming far more mainstream.



LEO:  They've offered YubiKeys with Lightning for a long time.  I have the one that has Lightning on one end and Type C on the other.  They also have NFC YubiKeys that work.  So, yeah.



STEVE:  So what I'm wondering, Leo, is what stage of authentication.  Is this unlocking your device?  Or is this like when you need to like put in your passcode, like after the first time you turn it back on, I would imagine.



LEO:  I would guess it's that second, you know, you're not going to do it every time you unlock your device.  I mean, maybe you could set that.  But it's whenever the two-factor pops up anyway; right?



STEVE:  Right.



LEO:  But I do wonder, and you taught me this, what's the fallback method?  Because the weakest link is always the fallback method; you know?  So they might be great and say, well, you could use a YubiKey, or we'll text you a message.  And that wouldn't be quite as useful.



STEVE:  That's a very good point.



LEO:  Yeah, yeah.



STEVE:  Okay.  Now...



LEO:  I'll have to learn about that.



STEVE:  Encrypting the cloud.  Most of the content of that first announcement page is glitz and self-congratulatory hype.  The good news is that there's a second page available containing much more detail. But since it leaves out some of the broad strokes, we'll cover them here first.



Okay.  Ivan Krstic, is that how you pronounce his name?  Krstic?  He needs some more vowels, just like ExpressVPN.  Ivan Krstic, who's Apple's head of Security Engineering and Architecture, is quoted in this first broad overview, saying:  "Advanced Data Protection is Apple's highest level of cloud data security, giving users the choice to protect the vast majority of their most sensitive iCloud data with end-to-end encryption so that it can only be decrypted on their trusted devices."  For users who opt in, Advanced Data Protection, which you'll hear me just abbreviating to ADP because they keep saying it over and over and over, keeps most iCloud data protected, even in the case of a data breach in the cloud.



Now, this is what's interesting.  For users who opt in, Advanced Data Protection keeps most iCloud data protected, even in the case of a data breach in the cloud.  They're big on this data breach in the cloud here.  I have some comments about that later.  So they're naturally not saying that this keeps their hands off any of their users' data, which is of course the main reason everyone wants this, but rather they're saying it's to protect their users in the event of an iCloud security breach.  Okay, fine, whatever, you know, as long as we can have it.



They explain, and here's the interesting bit.  They said:  "iCloud already protects 14 sensitive data categories using end-to-end encryption by default, including passwords in iCloud Keychain and Health data.  Meaning that Apple does not have access to that stuff, unlike most famously the iCloud backup.  For users who enable Advanced Data Protection, the total number of data categories protected using this end-to-end encryption rises from that 14 to 23, which then includes iCloud Backup, Notes, and Photos, and more.  The only major iCloud data categories that are not covered are iCloud Mail, Contacts, and Calendar because of the need to interoperate with the global email, contacts, and calendar systems.



But iCloud Backup, that's the biggie; right?  Remember that back in 2016 after the San Bernardino shootings, it was Syed Farook's phone whose iCloud Backups, had they been available, and had the FBI not changed the password on the account, could have been decrypted by Apple to provide the FBI with the vital evidence that they say they needed at the time.  So this was the big deal about iCloud backup; you know?  We learned that under some situations it could be made available.



LEO:  Yeah, Apple even said, yes, see, we cooperated.  All they had to do was go back to his house.



STEVE:  Right.  Okay.  So just to wrap up the overview, Apple is expending some significant effort, in my opinion, to spin this additional encryption as user protection in the event of a breach, presumably to deflect some of the government's and law enforcement's annoyance over more stuff being encrypted.  Apple went out of their way to write.



They said:  "Enhanced security for users' data in the cloud is more urgently needed than ever before, as demonstrated in a new summary of data breach research, 'The Rising Threat to Consumer Data in the Cloud,' published today.  Experts say the total number of data breaches more than tripled between 2013 and 2021, exposing 1.1 billion personal records across the globe in 2021 alone.  Increasingly, companies across the technology industry are addressing this growing threat by implementing end-to-end encryption in their offerings."  So again, this is Apple saying we're doing this to protect our users from data breaches because that's a big problem.  And of course the users want it because Apple has the keys.



Okay.  So that's the broad strokes.  There is more good stuff in a secondary document.  "Advanced Data Protection for iCloud," they write, "is an optional setting that offers Apple's highest level of iCloud data security.  When a user turns on Advanced Data Protection" - I'm going to now call it ADP - "their trusted devices retain sole access to the encryption keys for the majority of their iCloud data, thereby protecting it with end-to-end encryption.  For users who turn on ADP, the total number of data categories protected using end-to-end encryption rises from 14 to 23 and includes iCloud Backup, Photos, Notes and more."  But that stuff all seriously encrypted.



"Conceptually," they said, "ADP is simple.  All CloudKit Service keys that were generated on device and later uploaded to the" - what's called the "available-after-authentication," you'll hear that phrase a couple times more - "to the available-after-authentication iCloud Hardware Security Modules (HSMs) in Apple data centers" - get this.  Okay.  I'm going to back up again because this is important.



"All CloudKit security keys that were generated on device and later uploaded to the available-after-authentication iCloud Hardware Security Modules in Apple data centers are deleted from those HSMs and instead kept entirely within the account's iCloud Keychain protection domain."  Which again, Apple has no access to.  Apple does have access to that available-after-authentication iCloud hardware security modules where the keys have up until now or up until the user turns that on, that's where they've resided.  So they were in hardware.  They were protected as well as Apple could.  But available after authentication.  Now, not.  They are deleted.  When the user turns that on, those keys are deleted from the HSMs that Apple had.  They're handled, and they're moved into the iCloud Keychain protection domain, meaning Apple can't get there.  That stuff is sent down to the user for them to decrypt locally.  They are not decrypted at Apple's end.  They're handled like the existing end-to-end encrypted service keys, which means Apple can no longer read or access them.



Okay.  They wrote:  "ADP also automatically protects CloudKit fields that third-party developers choose to mark as encrypted, and all CloudKit assets.  When the user turns on Advanced Data Protection, their trusted device performs two actions:  First, it communicates the user's intent to turn on ADP to their other devices that participate in end-to-end-encryption.  It does so by writing a new value, signed by device-local keys, into its iCloud Keychain device metadata.  Apple servers cannot remove or modify this attestation while it gets synchronized with the user's other devices.



"Second, the device initiates the removal of the available-after-authentication service keys from Apple data centers."  So that's what I was just talking about before.  Now we're getting more into the detail.  "The device initiates the removal of the available-after-authentication service keys from Apple data centers.  As these keys are protected by iCloud HSMs, this deletion is immediate, permanent, and irrevocable.  After the keys are deleted, Apple can no longer access any of the data protected by the user's service keys.  At this time, the device begins an asynchronous key rotation operation, which creates a new service key for each service whose key was previously available to Apple servers.  If the key rotation fails, due to network interruption or any other error, the device retries the key rotation until it succeeds."



Okay.  This is very cool and very serious.  Apple is saying, since we once had those keys, we want all of those keys, not only to be deleted, but then rotated out of service so that even though we've already deleted the keys, you're now using new keys that we have never seen and never had anywhere in our possession...



LEO:  Excellent.



STEVE:  ...in an unencrypted form.



LEO:  Excellent.



STEVE:  Yes.  Yes.  It is really serious.  So they said:  "After the service key rotation is successful, new data written to the service cannot be decrypted with the old service key."  Right, because key rotation means replacement.  "It's protected with the new key, which is controlled solely by the user's trusted devices, and was never available to Apple," they wrote.  So notice what's been quietly acknowledged here.  This is has nothing to do with breach protection anymore.  This is all about Apple strongly selling the truth that they no longer have access to their users' iCloud device backups, photos and notes.



LEO:  Wow.



STEVE:  Okay.  Then they say:  "When a user first turns on ADP" - now, here's some interesting caveats because, you know, they've got some things that they needed, they offer some things that they have to have the keys for at their end if you want those things.



LEO:  Yeah.  We've talked about that.  There is a limitation on what you can do if you don't have access to the data.



STEVE:  Right.  They said:  "When a user first turns on ADP, web access to their data at iCloud.com is automatically turned off."



LEO:  Oh, that's a big one.



STEVE:  "This is because iCloud web servers no longer have access to the keys required to decrypt and display the user's data."



LEO:  Wow.



STEVE:  "The user can choose to turn on web access again, and use the participation of their trusted device to access their encrypted iCloud data on the web."



LEO:  Ah.



STEVE:  "After turning on web access, the user must authorize the web sign-in on one of their trusted devices each time they visit iCloud.com."



LEO:  Good.



STEVE:  "The authorization 'arms' the device for web access.  For the next hour, this device accepts requests from specific Apple servers to upload individual service keys, but only those corresponding to an allow list of services normally accessible on iCloud.com.  In other words, even after the user authorizes a web sign-in, a server request is unable to induce the user's device to upload service keys for data that isn't intended to be viewed on iCloud.com, such as health data or passwords in iCloud Keychain.  Apple servers request only the service keys needed to decrypt the specific data that the user is requesting to access on the web.  Every time a service key is uploaded, it is encrypted using an ephemeral key bound to the web session that the user authorized, and a notification is displayed on the user's device showing the iCloud service whose data is temporarily being made available to Apple servers."



Now, what this really means is, if you really don't want Apple ever to have access, you have to stop using iCloud.com.  iCloud.com is a backdoor.  I mean, they've done everything they can, all the due diligence possible.  They've proscribed their access.  They've limited it.  But if you're saying you want your data to be shown on a web page, that's got to come from our server.



LEO:  Right.



STEVE:  So you have to give us transient access to that.



LEO:  And presumably, if they wanted to, they could cache that key.



STEVE:  Right.



LEO:  So you're giving them a key again, and suddenly, yeah.  So if you really want to be private, you wouldn't use the web.



STEVE:  And given that turning ADP on forces a key rotation, maybe turning it off and back on again would reinitialize those keys.



LEO:  Ah, rotate that.  Yeah, yeah.



STEVE:  Yeah.  So they said:  "ADP and iCloud.com web access settings can be modified only by the user.  These values are stored in the user's iCloud Keychain device metadata and can only be changed from one of the user's trusted devices.  Apple servers cannot modify these settings on behalf of the user, nor can they roll them back to a previous configuration."  Again, not about protection from data breaches.  It's all obviously about Apple going well out of their way to demonstrate exactly how they no longer have access.



Then they said:  "In most cases, when users share content to collaborate with each other - for example, with shared Notes, shared Reminders, shared folders in iCloud Drive, or iCloud Shared Photo Library - and all the users have ADP turned on, Apple servers are used only to establish sharing, but don't have access to the encryption keys for the shared data."  Again, they've really done everything they could.



LEO:  So an encrypted blob would go through Apple, but never be unencrypted at Apple.



STEVE:  Right.



LEO:  And then presumably I'd have to share a key.  That's part of the sharing.  I'd have to share a key with the recipient.



STEVE:  Right.  So it would actually be - it wouldn't be the encrypted blob.  It would be the key would get shared to the other device, and then the device would do the decryption of the blob.



LEO:  So the data is device-to-device shared?  It's peer-to-peer?



STEVE:  No, no.  The keys.



LEO:  Right.  But the data - so I have some data on my phone I want to share with somebody else.  It's still going to go through Apple.  It has to go through Apple.



STEVE:  Exactly, because it comes from your iCloud Keychain that they don't have access to, to the other account's iCloud Keychain.



LEO:  So I have - my photos are on Apple's iCloud.  I want to share an album with Lisa.  I would send her the key, and then she could get the data from Apple, encrypted as it is with Apple, download it, and unencrypt it locally.  Yeah.



STEVE:  Right.



LEO:  That makes sense.



STEVE:  Right.  So they said:  "The content remains end-to-end encrypted and accessible only to participants' trusted devices.  For each sharing operation, a title and representative thumbnail may be stored by Apple with standard data protection to show a preview to the receiving users."



Okay.  Also, other little gotchas here.  Like this is why they had to work to do this.  "Selecting the 'anyone with a link' option when enabling collaboration will make the content available to Apple servers under standard data protection, as the servers need to be able to provide access to anyone who opens the URL.  iWork collaboration and the Shared Albums feature in Photos don't support Advanced Data Protection.  When users collaborate on an iWork document, or open an iWork document from a shared folder in iCloud Drive, the encryption keys for the document are securely uploaded to iWork servers in Apple data centers.  This is because real-time collaboration in iWork requires server-side mediation to coordinate document changes between participants.  Photos added to Shared Albums are stored with standard data protection, as the feature permits albums to be publicly shared on the web."



Okay.  So they also said - so those are the caveats.  Basically they've done everything possible.  The typical user who isn't using iCloud.com and doesn't want to do like dynamic document and photo sharing, they're locked down tight when they turn this on.  They said:  "The user can turn off ADP at any time.  If they decide to do so, two things.  First, the user's device first records their new choice in iCloud Keychain participation metadata, and this setting is securely synchronized to all their devices.



Second, the user's device securely uploads the service keys for all available-after-authentication services to the iCloud HSMs in Apple data centers."  In other words, you turn off ADP, here are the keys, Apple, that you're going to need to return us to previous, this is the way we've always encrypted your stuff before, mode.  And they said:  "This never includes keys for services that are end-to-end encrypted under standard data protection, such as the iCloud Keychain and Health."



They said:  "The device uploads both the original service keys, generated before ADP had been turned on, and the new service keys that were generated after the user turned on the feature.  This makes all data in these services accessible after authentication and returns the account to standard data protection, where Apple can once again help the user recover most of their data should they lose access to their account."



And now this is really interesting.  "The requirements to turn on Advanced Data Protection for iCloud include the following."  In other words, what you have to do to get this on in the first place.  And Leo, your mentioning of what happens if you lose your YubiKey is what put me in mind of this because again, Apple can't help you.  So they said:  "The user's account must support end-to-end encryption.  End-to-end encryption requires two-factor authentication for their Apple ID, and a passcode or password set on their trusted devices."  In other words, that 5% globally who don't have what they consider two-factor authentication, they can't turn on ADP.  They've got to first set better security on their device.  So that has to be there.



"Second, devices where the user is signed in with their Apple ID must be updated to iOS 16.2, iPadOS 16.2, macOS 13.1, tvOS 16.2, watchOS 9.2, and the latest version of iCloud for Windows.  This requirement prevents a previous version of iOS, iPadOS, macOS, tvOS, or watchOS from mishandling the newly created service keys by re-uploading them to the available-after-authentication HSMs in a misguided attempt to repair the account state."  Right?  You wouldn't want, I mean, it used to be that all of these previous iOS versions were using, you know, if they couldn't access something in the iCloud, they'd go, uh-oh, Apple must have lost the keys, and it would send them again.  Well, that's exactly what we don't want now.  So you've got to have the latest version of everything.



"Finally, the user must set up at least one alternative recovery method, one or more recovery contacts or a recovery key, which they can use to recover their iCloud data if they lose access to their account."  And Leo, I have not had a chance to play with this.  I didn't realize it was already there.  So we'll find out as we turn on ADP what this alternative recovery method, one or more contacts, recovery key and so forth, is all about.



LEO:  Yeah.  I think somebody said you should store the recovery keys in your password vault.  So somebody's turned it on in our chatroom, and they said that's what they could see.  So you might get, you know, you usually do get with these things five "use once" recovery keys, that kind of thing.



STEVE:  Yeah, yeah, yeah.  I mean, you know, you'll want that when you've got a no-way-out scenario.



LEO:  Yeah, print it out and put it in your safe or something, yeah.



STEVE:  And they said:  "If the recovery methods fail" - here it is.  "If the recovery methods fail, such as if the recovery contact's information is out of date, or the user forgets them, Apple cannot help recover the user's end-to-end encrypted iCloud data."  Period.



LEO:  Wow.  Good.  Good.



STEVE:  They finished - yup.



LEO:  If they said anything else, it wouldn't be good.



STEVE:  Right.  "Advanced Data Protection for iCloud can be turned on only for Apple IDs.  Managed Apple IDs and child accounts, which varies by country or region, are not supported."



So I am officially 100% impressed.  We heard what you need to hear, which was that if you lose your device or forget your password and are unable to authenticate, and none of the emergency recovery methods we made you set up when you turned on Advanced Data Protection available after we made you jump through all those hoops and acknowledge that your first born might be up for sacrifice, there is truly nothing we can do to help.  You turned it on.  We made you jump through hoops. You acknowledged the risks.  And now it's on you.  So, yay.



LEO:  Good.



STEVE:  You know, it sure took us a long time to get here, but we are here at last.  Everything that Apple is storing for us is encrypted in our devices before it ever leaves them.  They're storing keys in keychains, but they cannot decrypt those keychains because that's the one ultimate key that's being held by the user's device.  So Apple has finally been willing to move the rest of the keys that they were holding into the Keychain to which only its users have access.



LEO:  Very good.  Very, very good.  Right?  Can you see any holes in that?



STEVE:  No.  No.  The only thing might be that Apple could say to the FBI, well, if the user logs into their iCloud.com, then we could get a snapshot.  But so in that sense, you know, they're having to, in order not to lose the feature entirely, they're saying, okay, well, if you still want iCloud.com, we'll give it to you, and we double pinky swear that we're not going to do anything with the data that we shouldn't.



LEO:  We've talked about this before.  It's why Dropbox and OneDrive and Google Drive, none of them do this end-to-end encryption because you lose capabilities when you do it.  And I think what Apple's done is actually kind of interesting.



STEVE:  It's why I like Sync.  Sync.com does have true end-to-end encryption.



LEO:  Fully encrypted.  But then it's hard for them to de-dupe.  I can't remember what all the features were, but there were features that you lose if they can't see the data; right?



STEVE:  Yup.



LEO:  Chiefly de-duping, which is only for their benefit so you don't waste their storage.



STEVE:  Needlessly.



LEO:  Yeah.  But I think the ability to give them a temporary one-hour key so that you can look at the stuff, you just have to trust that they'll delete that key.



STEVE:  Yeah.



LEO:  Or as you said, you could rotate the keys.  We should play with that and see how long it takes, for instance, to rotate the keys.



STEVE:  Yeah.  Yeah, yeah.



LEO:  You know, if it takes an hour, well, that's no good.  If it's instant, then you could do that all the time.



STEVE:  I think it'll be instant.



LEO:  It should be; right?  Yeah.



STEVE:  Yup.  Yup, absolutely.



LEO:  Yeah.  This is why you listen to this show.  I don't need to say this, but I say it every week, this is the best stuff.  And when I see stories like Apple is adding end-to-end encryption, all I can think is, oh, I can't wait to hear what Steve has to say about this.  Well, now we know.  Now we know.  That's why we count on him, Mr. Gibson.  You'll find him at GRC.com, the Gibson Research Corporation.  Somebody was - you mentioned this Apple Light Pen video that was going around.



STEVE:  Uh-huh.



LEO:  Somebody posted it, I can't remember where, probably in Mastodon, and said:  "That's not Steve Gibson, is it?"  Yeah, yeah, it is, it's Steve Gibson, a long time ago.  It's great.  That's I guess where GRC came from, the Gibson Research Corporation.  These days the research is mostly around SpinRite, getting your solid-state or hard drive back to normal, working fine.  It is the world's finest mass storage recovery and maintenance utility.  It's a good thing for maintenance, as well.  And he is working hard on 6.1, as you've probably heard.  6.0 is the current version.  Don't be put off by the fact that it's been there since 2004.  Bug free since 2004.  How about that?  Bug free.



STEVE:  And that's what we're trying to do on 6.1.



LEO:  Yeah.  You will get a copy of 6.1 if you buy 6.0 right now.  You'll also participate in the development.  It's just a matter of, well, I won't make any promises for Steve.  But it's just around the corner.  How shall we put that?  By the end of next year for sure, 6.1.  No, no, I didn't mean to say that.  Poor Steve.  He's, you know what, god bless you, you're just putting in the effort to make sure it's absolutely robust.  You've got a $40 all-in-one in the trunk of your car just so you can check it.



STEVE:  Actually, while Lorrie was down last week with the 'flu, I worked 18 hours a day on SpinRite.



LEO:  Oh, man.



STEVE:  Because I couldn't get near her, and we couldn't have any fun doing anything.



LEO:  Didn't have anything else to do, yeah.



STEVE:  So, and she was really worried.  By like the end of the fifth day she said, "Are you okay?"



LEO:  You're self-sufficient.  She should know that.



STEVE:  Yeah, she does.



LEO:  You've spent most of your life self-sufficient.



STEVE:  That's true.



LEO:  It's the same thing.  You have to balance, a life-work balance.  And it's not easy for us geeks because we get tunnel vision, and that's all we can think about.  I'm looking at some code right now thinking, I could probably simplify that.  But all you have to do is go to GRC.com, pick up a copy of SpinRite.  That's Steve's bread and butter.  While you're there, take a look at all the other free things he offers like ShieldsUP! and on and on and on.



Plus of course the podcast.  He's got 16Kb audio versions.  That's for people who really want a small download.  He made that for Elaine Farris who is in a bandwidth-constrained environment.  She downloads it and makes a human, beautifully written, crafted transcription.  Somebody said the 900's not up yet, but it takes a few days.  Takes a few days.  So as soon as it's done it'll be up on the website there.  He also offers 64Kb versions for those who have ample bandwidth.  Go to the GRC site to get that and lots of other great stuff.  You can also leave feedback there at GRC.com/feedback.



He's also a Twitterer.  He's still on the Twitters, @SGgrc.  And his DMs are open.  Talk about brave.  So you can leave him a message there, @SGgrc.  And he posts a link to the show notes there every week, so that's another place you can get those.  Those are good.  If you listen to the show, get the show notes.  They're either on his website, or you can get them from Twitter.  Because that, you could read along, but it's got links, it's got pictures.  The show notes are a really great resource that Steve puts a lot of effort into.



Copyright (c) 2022 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#902

DATE:		December 20, 2022

TITLE:		A Generic WAF Bypass

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-902.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we answer another collection of burning questions:  Is there no honor among thieves?  What was discovered during this year's Toronto Pwn2Own competition?  What did we learn from last Tuesday's patch fest?  Whose fault was the most recent Uber data breach?  What happened when Elon tried to block all the bots?  What's the first web browser to offer native support for Mastodon?  What exactly is "Coordinated Inauthentic Behavior," and why is it such a problem?  What will happen to GitHub submitters at the end of next year?  What measure could every member of the U.S. Senate possibly agree upon?  Exactly what applications are there for a zero-width space character?  And finally, what larger lesson are we taught by the discovery of a serious failure to block a problem that we should never have had in the first place?  The answer to all those questions and more await the listeners of today's Security Now! Podcast #902. 



SHOW TEASE:  It's time for Security Now!.  Last show of the year!  Steve Gibson is here.  We're going to talk about the results of the Pwn2Own competition.  Everybody got hacked.  Everybody.  Then it's the latest on last week's Microsoft Patch Tuesday.  And, finally, what exactly is coordinated inauthentic  behavior?  You'll find out.  Nothing but authentic behavior next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 902, recorded Tuesday, December 20th, 2022:  A Generic WAF Bypass.



It's time for Security Now!, the show where we cover your security, your privacy, your online agenda, online with this guy right here, the king of security, Mr. Steve Gibson.  Hello, Steve.



STEVE GIBSON:  And Leo, you'll be glad to know, actually Paul Thurrott will be very glad to know that I left my Grinch costume...



LEO:  You were so good.  Lisa and I - so we're talking about the show that's not yet aired.  It's going to be a Christmas Day version of TWiT.  Steve, Jeff Jarvis, Doc Searls, Paul Thurrott and I were the old guys talking about the year's news and so forth.  But Steve really dressed it up with his Grinch costume.  And the thing that I was so impressed with is you didn't just do it, like, token Grinch.  You kept it going the whole 2.5 hours.  He was doing his hands.  You were impressive.  Lisa and I both were really impressed, I have to say.



STEVE:  Well, now everybody listening is thinking, okay, I didn't know if I was going to make time to actually watch a podcast on Christmas Day.



LEO:  You have to watch this.



STEVE:  But maybe...



LEO:  But you can watch it whenever you want.  You can watch it next year.



STEVE:  I've got to see Gibson being the Grinch.



LEO:  No, I thank you.  Steve goes all in.  He put his heart into that.  And it's one of the reasons, you know, I think that this show does well, and you've really helped the network.  I'm very grateful to you, and I always will be.  Steve, we don't have a show next week.  We have a Best Of.  You can take next week off.



STEVE:  Oh, thank goodness.  Oh, what did I say?  No.



LEO:  No, that's okay.  Steve...



STEVE:  I get to work on SpinRite.  It's going to be a major SpinRite work...



LEO:  If you wanted to take six months off, it wouldn't be me  that you'd have to worry about.  Or Lisa.



STEVE:  Oh, I know.



LEO:  It'd be the fans.



STEVE:  You can't know how many messages I receive about new ways to number the shows so that 999 is not a problem.



LEO:  That's going to be a bad day in Blackrock when that happens.



STEVE:  Oh, it's not going to be good.



LEO:  But we'll think about it.



STEVE:  Actually, on Episode 900, it was only two weeks ago, when I said, yeah, we have 100 episodes left, I got immediately scolded.  Gibson, where is your off-by-one map.  That would be 99 shows left.  It's like, oh, that's...



LEO:  He had an overflow, a math overflow, buffer overflow.



STEVE:  One has to be very careful when talking to our audience, Leo.  But I think that's why they appreciate my being careful as I can be.



LEO:  Absolutely.  Absolutely.



STEVE:  This week we're going to answer another collection of burning questions.  First, is there no honor among thieves?  What was discovered during this year's Toronto Pwn2Own competition?  What did we learn from last Tuesday's patch fest?  Whose fault was the most recent Uber data breach?  What happened when Elon tried to block all the bots?  What's the first web browser to offer native support for Mastodon?  What exactly is coordinated inauthentic behavior, and why is it such a problem?  What will happen to GitHub submitters at the end of next year?  What measure could every member of the U.S. Senate possibly agree upon?



LEO:  Oh, god.



STEVE:  Exactly what applications are there for a zero-width space character?  And, finally, what larger lesson are we taught by the discovery of a serious failure to block a problem that we should have never had in the first place?  The answer to all those questions and more await the listeners of today's Security Now! Podcast 902.



LEO:  See, now, this is exactly why you've got to do more than 999 shows.  You've come up with the perfect way to introduce the tease to the show.  It's perfect.  All those questions and more will be answered.  I like it.



STEVE:  You know, and my theory - it matches my theory of life.  Just as you're taking your final dying last gasp of a breath, you figure it out.



LEO:  Oh.  I thought you were going to say, you say "Is that all there is?"



STEVE:  It's like, oh, I finally get it.



LEO:  Oh, I get it.  Eureka.



STEVE:  And of course the kids never want to listen to Gramps.  He has no idea what he's talking about.



LEO:  No.  Yeah.



STEVE:  So I just keep it to myself.



LEO:  We yell at the cloud so you don't have to.



STEVE:  That's right.



LEO:  We'll take a little break, and then we're going to get the answers to all those questions and more with Steve Gibson.



STEVE:  And of course I forgot to mention that the title of today's podcast is...



LEO:  Oh, what is it?



STEVE:  It's "A Generic WAF Bypass."



LEO:  Is that the answer to any of those questions?



STEVE:  Happy Holidays.



LEO:  A Generic WAF Bypass.  All right.



STEVE:  A Generic WAF Bypass is involved with the larger lesson that we're taught by the discovery of a serious failure to block a problem that we should have never had in the first place.



LEO:  So that's what I thought.  Okay.  So that last one...



STEVE:  That's right.



LEO:  ...is the laugh.  Okay.



STEVE:  Boy, what a - yes.



LEO:  I don't even...



STEVE:  We're going to have a buildup to that one.



LEO:  That's why you listen to this show; right, folks?  That's exactly right.  I am ready for the Picture of the Week.



STEVE:  And this one...



LEO:  Ooh.



STEVE:  ...really is interesting.



LEO:  Yes.



STEVE:  So I titled this "Old School Message Routing."  And it'd be difficult to adequately describe this, but it's a fascinating picture.  And if nothing else, Leo, it shows us how far we've come because what we have is...



LEO:  I'm old enough to remember going to a department store, buying something with my mom, and the clerk would roll up the slip, put it in a tube...



STEVE:  Like a cylinder.



LEO:  A cylinder, and shove it into one of these holes.  That's a pneumatic delivery system.



STEVE:  And my earliest memory of that was when you did a car, you know, auto ATMs.  I remember that the very earliest...



LEO:  Yeah.  Oh, yeah, we still have one in town, yeah.



STEVE:  Yeah, where you'd actually, like, stick your checks and things in this plastic cylinder and then stick it in this tube, and it would go [sucking sound].



LEO:  It was so satisfying.



STEVE:  Oh, it was wonderful.  So, okay.  So what we have, you know, if people could imagine a bunch of tubes that have sort of like a catcher at the end, so that this cylinder is going to come flying out of this tube and, like, stop.  This is the switch room, which involves all of the ends of these tubes, and some poor guy who's standing there, I guess like picking tubes up from one, like picking cylinders up that have arrived in one tube and then sticking them up in another tube and off they go.  You know, on to their destination.  This looks like a message routing or switching room for pneumatic tube transfers.  And but now what's interesting is that I don't see any labels on these things.  There's like, I don't know, what, 30 or them or 20 of them that we can see.  And then over on the left are a whole bunch, like other rows of them.  This is just fascinating.



LEO:  This is the inbound.  This is your inbox.  I don't know what you would do for outbound.



STEVE:  Yeah.



LEO:  I guess you have to remember where it came out of.  These are called, according to the chatroom, Lamson tubes.  And there's a website dedicated to it, pneumatic.tube.



STEVE:  Oh, that's so good.  That's so good.  And also I love the one there along the ceiling at the top.  Like it looks like it starts to come down, and then it changes its mind.  It goes, oh, nope, we're not going to end here.  We're going to go across and go back up somewhere else.  So it's like, okay.  And also when you think about it, you know, these things have some length to them; right?



LEO:  Oh, yeah.



STEVE:  Like it's actually a canister that you're able to put documents in.  So there's a minimum radius for the bend of these things, or the canister's going to get stuck trying to go around a curve.  So it must be that it had larger ends and a thinner body, or maybe even like a concave body that would allow this thing to navigate around a corner because it's got - we can see some corners.  They're not sharp.  And they couldn't be.  Anyway, just, you know, completely not about packet, well, it is kind of about packet routing, I guess.



LEO:  Oh, yeah.  This is a website all about it.  Here's a Scientific American article about the pneumatic tube system of New York City.  Now, it was published a couple of years ago.  But the original article was from 1897.



STEVE:  This is why the Internet is so good.



LEO:  There's nothing you can't find.  I mean, pneumatic.tube.  There's a website, for crying out loud.



STEVE:  Love it.



LEO:  Amazing.  Amazing.



STEVE:  I love it.



LEO:  Yeah.  And they're all over the world, I guess, these pneumatic - there's a pneumatic railway.  I'm not sure I'd want to ride that.



STEVE:  Just lie flat and tuck your arms in, and we're just going to close this little lid on this round coffin and send you on your way.



LEO:  Yeah.  I think actually that's - isn't that Elon Musk's idea for - what did he call that, that tube system that he wants?  Not the boring company, but he actually wanted to do a high-speed tube.



STEVE:  Hyperloop; right?



LEO:  Hyperloop, thank you.



STEVE:  That was last week.  Who knows what tomorrow will bring.



LEO:  He's a little busy now, yeah.



STEVE:  Yeah.  So a malware operation known as URSNIF, which we've noted a few times, they've kind of crossed our radar, it's the fourth one this year to suffer from internal squabbles which end up surfacing in the public eye.  Disagreements over Russia's invasion of Ukraine and in some cases strong pro-Russian sentiments which have divided previous groups were the nominal triggers in those first three instances.  But for number four it appears that it's just about greed.  You know, that's all the motivation we need.  And this is heading toward the answer to the question, you know, is there no honor among thieves.



Through a Twitter account which was @URSNIFleak, which may still be suspended, it was a while ago at least, an ex-member of this group, this URSNIF group, announced his intention to leak the real world identities of the top leaders of the group unless he received a significant payout.  To prove his willingness to do so, in a succession of tweets @URSNIFleak leaked various pieces of internal dialogue, some of the group's source code, and the names of three low-level group members.  That was enough to get this person paid.  After that he tweeted, he said:  "I just made more money in a single week than I have made in years.  Pay workers right, and they won't have a reason to leak stuff."



And I couldn't help but note that it's interesting that this person considers this had been an act of "making money."  Wow, what a different culture.  Apparently, the motivation to extort was heightened by something that the head of the group said in an interview with the VX-Underground project, though it's not clear what about that was upsetting.  The URSNIF leaker tweeted:  "The interview angered me.  He has been a bad boss for a long time.  I've been waiting for the right time to release."  And of course, remember that this is a bad boss of a Russian ransomware group.  So, yeah, "bad" kind of goes with the territory.



LEO:  That's hysterical.  I just wanted to collect my paycheck and go home, but no.



STEVE:  Yeah.  So I had to extort money in order to get paid, or extort the boss for money.  That's right.



So anyway, I don't think we can count on all of the major groups to implode, but there's probably a little extra tendency for that to happen within an organization comprised of people who must be aware that what they're doing is not earning an honest day's living.  At least I guess we can hope so.



Okay.  Pwn2Own Toronto 2022 just happened.  And it's always interesting to see what hackers wearing white hats are able to do to today's fully patched and up-to-date systems, right, because those are the targets is in every case these things are 100% patched.  And we've seen instances in the past where a group will get all ready to demonstrate a vulnerability that they've very cleverly crafted in something and, like, the day before their demo the publisher patches.  And not because they told them; right?  I mean, they will end up telling them.  All of the things that are done during these Pwn2Own contests end up being communicated to the publisher of the thing that was compromised, but not beforehand.  Anyway, so the point is, this is state-of-the-art, fully patched as good as we know how to make it, products that these guys are going after.



So in the past we've taken people through a blow-by-blow.  And sometimes I think that ends up getting a little long.  So I'm going to summarize this a bit.  The recently concluded Toronto 2022 hacking contest focused upon hacking routers, smartphones, printers, and other smart devices.  So it was sort of an IoT-esque, you know, smartphones, printers, routers, and other stuff.  It was a four-day contest that ended up getting won by DEVCORE, which is the now well-known Chinese/Taiwanese penetration testing group.



Okay.  So to give everyone some sense for this, I'm just going to quickly scan down, and I abbreviated these, just the bullet points which briefly describe the attacks.  So, and this is just day one; okay?  Day one of the four-day contest:  A stack buffer overflow attack against the Canon imageCLASS MF743Cdw printer.  A two-bug authentication bypass and command injection attack against the WAN interface of a TP-Link AX1800 router.  A command injection attack which caused a Lexmark MC3224i printer to serenade the audience with a well-known Mario Brothers tune.



We had a command injection attack against the WAN interface of the Synology RT6600ax router.  A stack-based buffer overflow against an HP Color LaserJet Pro M479fdw printer.  An improper input validation attack against the Samsung Galaxy S22.  A command injection root shell attack against the LAN interface of the Synology RT6600ax router again.  Another improper input validation attack against the Samsung Galaxy S22.  A two-bug attack, SQL injection and command injection, against the LAN interface of the Netgear RAX30 AX2400 router.  A SQL injection on a router, that's interesting.



Anyway, two different stack-based buffer overflow attacks against the MikroTik router and a Canon printer.  Three bugs, two missing auth for critical function and an auth bypass attack, against the Synology DiskStation DS920+ NAS.  Two bugs, including a command injection, in an attack against the HP Color LaserJet Pro M479fdw printer.  Five different bugs leveraged in an attack against the LAN interface of the Netgear RAX30, again, AX2400 router.  And three different bugs against a Netgear router and an HP printer.  Now you know why I'm only doing day one.  And remember, these were all 100% up-to-date devices, all cut through.  All of that on only the first day.  And it kept going like that throughout the entire event.



As we know, LAN-side attacks on routers and NAS devices are much less concerning than attacks that can be launched against the WAN interface.  But this contest revealed plenty of both of those.  And the number of printer vulnerabilities that still exist, well, I suppose we shouldn't be surprised.  But obtaining well-hidden persistence inside a network is an overriding goal of anyone who penetrates an enterprise's perimeter.  And printer protocols by their design loudly broadcast and advertise on networks because their goal is to be found.  Unfortunately, this results in highly vulnerable printers shouting their presence and creating a perfect and often unsuspected place for malicious post-intrusion malware to set up shop and wait, thus becoming an advanced persistent threat.



So anyway, I just, sort of as a reality check, here's, yes, these guys are at the top of their game; right?  They're the world's best hackers.  Yet it appears that all they have to do is look at some device, make that the target of their scrutiny, and they can find a way in.  So, you know, we need to - I guess anyone listening to this podcast long enough will have lost any sense that there's anything that's invulnerable to somebody who is serious about finding a way in.  And in fact that is the story behind today's main podcast story at the end.



Okay.  And speaking of getting into networks, it's not just lower-end IoT devices that are permitting bad guys to get into networks.  Both Citrix and Fortinet, who are two of today's largest providers of enterprise networking equipment, recently released security updates to patch zero-day vulnerabilities, one in each of their devices, that were being exploited in the wild against them.  In the case of the Fortinet zero-day, which created an unauthenticated remote code execution in the FortiOS, which is what runs the company's SSL-VPN devices, it was the way some ransomware was managing to crawl inside enterprise networks.  Which is never what you want.  And it was so bad that Fortinet did the right thing by also offering down-version patches for their older, out-of-support devices which were still running their also-vulnerable 6.0 firmware.



The zero-day was first spotted being used in the wild by a French security firm Olympe last week, and to Fortinet's credit they patched it over the weekend in just three days.  So props for getting it fixed quickly.  But, boy, you know, what this French security firm watched were ransomware groups gaining entry to an enterprise network through this vulnerability.  So, wow.



And I said, you know, two zero-days, one each, Fortinet and Citrix.  Citrix's is the other.  And it's also an unauthenticated remote code execution exploit.  Interestingly, this one was spotted by the NSA - yeah, our National Security Agency.  In their security advisory, the NSA wrote that they saw the Chinese cyberespionage group designated APT5 leveraging that Citrix zero-day, but the NSA offered nothing further about what was being done with the obtained leverage.  So again, high-end gear also vulnerable, not just low-end stuff.



Last Tuesday was the industry's increasingly well-attended final monthly patch event of the year.  And those offering up incrementally more secure improvements in their code and products notably included Adobe, Android, Apple, Microsoft, Mozilla, SAP, and VMware.  Microsoft fixed 72 security flaws this month across their range of offerings, and that included a zero-day that was being used to circumvent Microsoft's SmartScreen and Mark-of-the-Web detection which was - I got myself tangled up.  The zero-day was being used to bypass that to allow standalone JavaScript files to execute because modern Windows will execute JavaScript natively.  And of course we covered this trouble recently, so it's very good that it's been fixed.



The other issue Microsoft addressed was a problem that we also noted before here, which was that somehow malicious Windows drivers were being used by the Hive and the Cuba ransomware strains or groups, and those malicious drivers were being trusted by Windows because they were carrying valid Microsoft signatures.  Whoops.  Okay.  In this month's advisory, Microsoft wrote:  "We were notified of this activity by SentinelOne, Mandiant, and Sophos" - so everybody was watching - "on October 19th of 2022, and subsequently performed an investigation into this activity."  And I should just mention that SentinelOne, Mandiant, and Sophos, they've all got clients, and their technology is on those clients' networks offering protection over and above what Microsoft is providing.



So the reason all three of those companies all notified Microsoft on October 19th of 2022 is that's when all three of their technologies' alarms went off when drivers were acting maliciously.  They immediately thought, wait a minute, how is a driver getting into the kernel and acting this way?  So they yanked those, looked at them, found that they were all validly signed by Microsoft, and immediately notified Microsoft that that's what was happening.  So that's the good thing about the way this industry is evolving with third parties who are offering real-time detection services for people's networks is they're able to close the loop and let Microsoft know when something bad has happened.



Microsoft said:  "This investigation revealed that several developer accounts for the Microsoft Partner Center were engaged in submitting malicious drivers to obtain a Microsoft signature."  In other words, there were some bad partners there.  They said:  "A new attempt at submitting a malicious driver for signing on September 29th, 2022, led to the suspension of the sellers' accounts in early October."  So, okay.  So early October, yet the drivers appeared on the 19th of October, which suggests that drivers were signed.  Microsoft caught this happening at the end of September, yet there were still drivers out there that Microsoft wasn't aware because they hadn't invalidated them.  So then they appeared in use toward the end of October on the 19th, and that's when they got notified of something that basically they already knew about.



Anyway, that was all good.  And not to be left out, Apple also updated WebKit to fix a zero-day that was being used in targeted attacks against iOS users.



Uber has been having a rough time recently.  Recall that about four months ago the Lapsus$ gang breached Uber's security and caused them trouble.  What's interesting about last week's second breach, which resulted in unfortunately the leaking of the personal details of more than 77,000 Uber employees, and also some source code and credentials for some of the company's internal IT network.  And I should mention Uber confirmed the authenticity of that leaked data.  What's interesting is that this wasn't directly Uber's fault.  The breach occurred in the network of an Uber-contracted IT service provider whose name suggests, or suggested to me at least, that all the good names were already taken.  This company chose to name itself Teqtivity.  It's T-E-Q-T-I-V-I-T-Y.



Anyway, the day after Uber outed Teqtivity as being the actual proximate cause of this latest leak, Teqtivity themselves disclosed the breach last Thursday.  Uber may have been Teqtivity's biggest customer.  Actually, I did some looking around, and they've got a bunch of them.  But I mention this because other notable companies may also have had their data stolen since a breach of one large service provider can potentially expose the data belonging to all their clients.  We saw this of course a couple years ago when all of those dental offices were in trouble because they were all outsourcing their dental records management to one single provider, the so-called MSP; right?  Managed Service Provider.



As an industry these days, we're really sort of facing a conundrum.  Do you run your own in-house shop where you are solely responsible for your company's security and IT and everything?  Or do you decide that running networks and servers and points of presence and dealing with the constant need to focus upon security is not your mainline business?  It isn't what you should be spending your cycles on.  And also that it's just become too complicated to do it right.  You know, that's a valid consideration.  So you farm it out to someone who promises to you that it will be their mainline business because that's all they're going to do.  It is their business.



And, you know, I think today that's a tough call.  I think it can work out and be extremely cost effective to do this subcontracting so long as everything goes well.  On the other hand, when something doesn't go well, you know, if it's a big breach at a major service provider, potentially the damage can be huge because so many individual clients of theirs can be affected by a single attack.  So again, a tough call; but increasingly I can see that it makes sense.  And this sort of goes back to the comment I made, I think it was last week, where the guys at the DigiCert customer advisory board meeting looked at me like I was nuts for still doing it myself, saying, "Gibson, nobody does their own hardware anymore."  



Okay.  I don't know, Leo, if every podcast on TWiT mentions Twitter and Elon.  Probably.  But he keeps doing things that are interesting, certainly for us.



LEO:  That's one way to put it.



STEVE:  From the outside looking in, it's difficult to understand the mechanisms at play inside Elon Musk's Twitter reign.  From the outside, anyone would get the sense of things lurching back and forth inside Twitter, presumably as Elon's, as he described it himself, his "biological neural net fires off whimsical edicts," which Twitter's remaining employees apparently quickly implement without any buffering in a desperate effort to hold onto their own paychecks in this chaotic and fragile work environment which has been created.



You know.  One moment we're done with layoffs.  Then we have more layoffs.  No, now we're really done with layoffs.  Then entire departments disappear.  Collections of press accounts are suspended for an interval of seven days, until the next day they're reinstated.  A new policy states that anyone tweeting a link which points to another social network will have their account suspended, until a few hours later when that policy ends.  You know, it really has been quite something to watch.  And as I'm assembling the notes and details of this podcast, when I follow links to online events that would once have linked to Twitter, I'm increasingly being taken to Mastodon.



Well, last week something else happened as a result of an apparent misfiring of Elon's biological neural net.  He decided that he was going to block all of the bots.  This, of course, was something that had endlessly bedeviled all of the pre-Elon Twitter engineers, how to block the bots.  Elon, it turns out, had the answer.



So he declared publicly that he had a surprise for all of the bot farms, and last Monday Twitter blocked entire IP address blocks which were used by, it turns out, approximately 30, three zero, mobile carriers across Asia.  According to Platformer...



LEO:  [Making noises]



STEVE:  I know, this included the primary telecom providers for all of India and all of Russia, as well as Indonesia's second-largest telecom.  Of course, there were vastly more legitimate users in every one of those address blocks than there were bots.  So three countries' worth of legitimate Twitter users, who all shared the same IP address blocks as a few bots, were completely cut off from Twitter.  And you have to, like, wonder, how could anyone not anticipate that happening?  It's - I don't know.  Again, it's just incredible to me.  What I can see is that Elon wants to own Twitter, but Twitter is not technology.  It is enabled by technology.  Twitter is a community.  A community can be enabled and nurtured and encouraged.  The one thing it cannot be is owned.  Nobody owns Twitter's community.  No one can.  Not even Elon.



LEO:  You're always welcome over at TWiT Social.  You could have your own Mastodon account.  I promise not to ban you.



STEVE:  Well, we're going to see because, you know, he famously held a poll over the weekend.



LEO:  Oh, that's all silly, silly.



STEVE:  I know.  He said if this poll says I should no longer be CEO, I will resign.



LEO:  Yeah.  It did.



STEVE:  Of course the poll said please resign.



LEO:  We're waiting.



STEVE:  Well, yeah, go.  We're done.  We're done.



LEO:  Well, and then he said be careful what you wish for, which is probably true because god knows who would take over from him.



STEVE:  And the problem is I think he's destroyed.  I mean, you know, one of the things that I'm seeing, in fact we're going to get to this in a minute, this is the "coordinated inauthentic behavior," which is just this wonderful phrase.



LEO:  I love that phrase, yeah.



STEVE:  It is difficult to do this, Leo.  It is difficult to be in an ownership or, you know, catbird position with any large social media network.  You are going to be constantly fighting abuse.  On the one hand, you want to open your gate and allow everybody in the world to come in and participate.  Unfortunately, we know that the world has a whole bunch of bad people in it.  And, you know, bots are a thing.  And so it's just this is really hard to do.  And I would argue Twitter was doing the best job they could.  And of course then they got all - they ran afoul of all of these issues of, well, you know, should we allow people to scream fire in a burning building or not?  And ad infinitum.  Anyway, Elon appears to have badly broken it.  And it's not all clear to me that him disappearing is going to suddenly fix it.  I don't know how you do that.



LEO:  Yeah.



STEVE:  It's sad.  Anyway, the good news is, speaking of Mastodon, Vivaldi recently became the first browser to have its own Mastodon instance, Vivaldi Social.  Now, the new version on the desktop is also the first to integrate Mastodon natively into the browser itself, along with the ability to pin tab groups and other UI improvements.  They said:  "We believe in providing alternatives to Big Tech while putting your privacy first, and launched Vivaldi Social, our Mastodon instance.  And today we are integrating Vivaldi Social into the sidebar of our desktop browser, becoming the first browser to offer this functionality."  So anyway, I just wanted to give a tip to Vivaldi and note that it's interesting that this has happened.



On the topic of governments recognizing the growing dangers of known vulnerabilities in the networks of the enterprises within their own borders, remember we've talked about a couple governments, I don't think it was the Dutch government, and I meant to go find out which one we'd referred to before.  But there was another note of some government that, like, announced they were going to start scanning their own citizens.  It might have been the U.K.



Anyway, in this case the Dutch government has been doing it.  And they just said that since the beginning of this work, which was the summer of 2021, so about a year and a half ago, and about a year and a half worth of this, they have sent more than 5,200 warnings to Dutch companies concerning security vulnerabilities within their networks.  Officials said that around 76%, so three out of four, of these warnings were for sensitive systems being accessible via the Internet:  RDP, SMB, LDAP, and so forth.  The other 24% of the warnings regarded malware infections, leaked credentials, or unpatched systems.  So presumably they're scanning the Internet and seeing a version number in the greeting of something and saying, whoops, that's not the latest version.  And they send the company a note saying, hey, you know, maybe you ought to update your email because you're running an old one which has some known vulnerabilities.



So anyway, this is not the first time we've encountered this, and it seems like an entirely sane thing for governments to do in the interest of helping to protect their national interests and those of all of their citizens and the enterprise operating within their borders.  So I expect that we're going to be seeing more announcements of this sort in coming years.



Okay.  CIB, that's the abbreviation for Coordinated Inauthentic Behavior, a term that I love.  A recent report from Facebook's parent company, Meta, introduced me to this term, Coordinated Inauthentic Behavior.  And I love it because it's such a wonderfully neutral and politically correct term to describe the behavior of organizations and countries that have figured out that they can use fraudulent postings and replies on Facebook to influence beliefs and behavior through massive coordinated campaigns.  Facebook's report, which they published on Thursday, last Thursday, was titled "Recapping Our 2022 Coordinated Inauthentic Behavior Enforcements."



They noted that since they began focusing upon the explicit abuse of Facebook services for what they term "covert influence operations," they've disrupted 200 identifiably separate global networks that were the source of these campaigns.  Those networks were based in 68 countries - but far from evenly, as we'll see - and operated in at least 42 different languages.  Two thirds of the campaigns, I thought this was really interesting, two thirds of the campaigns were targeting their own local audiences in their home countries, and only one third were aimed at audiences outside the country, so abroad.



In terms of targets, more than 100 different countries, from A through Z, Afghanistan through Zimbabwe, have been targeted by at least one CIB network, foreign or domestic, with the U.S. being the most targeted with 34 of those operations; followed by Ukraine, and I'm sure that's only in the most recent year,  targeted by 20 CIB networks; and then the U.K. targeted by 16.  So 34 for the U.S., 20 for Ukraine, 16 for the U.K.  And a single covert network might often be simultaneously targeting multiple countries at once.  In one case, for example, a network running from Iran was simultaneously targeting 18 countries on four different continents.



Okay.  As for the originators of the campaign networks, perhaps not surprisingly, Russia leads the list of the originating sources of these networks having 34 networks identified, closely followed by Iran with 29.  And then the next highest, with fewer than half of Iran's 29, interestingly, was Mexico, which surprised me as the third largest source of these influence networks at 13.  Interestingly, those are the top three, right, Russia, Iran, Mexico.  China is not among them.  Russia and Iran are the biggest perps in this game.



And as I said, I was surprised about Mexico, so I went looking for some more information about them.  As I suspected, most of the CIB networks originating in Mexico have focused primarily on regional or local audiences to Mexico, often in the context of regional elections.  Those networks tended to be less tactically sophisticated, and many were linked to PR or marketing firms, including instances where - I love this - one network simultaneously supported rivals in the same electoral post.  The report noted that this illustrates the danger of using covert "Influence Operations for Hire" that might be providing inauthentic support to, not just the highest bidder, but to multiple bidders at once.  So again, we have this wonderful term "Coordinated Inauthentic Behavior."  And now we have some sense, thanks to Facebook's work on this, about the spread and nature of these networks.



Okay, SHA-1.  We might say that we hardly knew ye, but as it turns out we knew ye quite well.  The NIST has formally announced what many of us have been assuming for some time.  The aging original SHA-1 cryptographic hashing function is officially being retired.  In its place is either SHA-2 or SHA-3, both which have existed for quite a while and have been in use for a long time.  But I did a bit of a double take when I saw that companies now have, as of the NIST's announcement, companies have until the end of 2030, in other words until the beginning of 2031, so another entire eight years from now, to make that replacement.  The end of NIST's announcement said:



"Modules that still use SHA-1 after 2030 will not be permitted for purchase by the federal government.  Companies have eight years to submit updated modules that no longer use SHA-1.  Because there's often a backlog of submissions before a deadline, we recommend that developers submit their updated modules well in advance, so that CMVP has time to respond."



Okay.  Now, a cryptographer might have been a bit more explicit and careful in the wording of that mandate.  I'd have written "Modules that are still capable of using SHA-1 after 2030," dot dot dot.  The reason for the added clarity is that, as we've often talked about, many cryptographic systems obtain robust interoperability by comparing acceptable protocol suites that both ends understand and then negotiating the best and hopefully the most secure among those.  But through the years of this podcast we've examined a great many "downgrade attacks" where a malicious endpoint identifies that the other end is still offering a no-longer-considered-safe weak cryptographic protocol.  So the sneaky end pretends that it cannot use any of the stronger systems, thus tricking the agreeable other endpoint into establishing a potentially vulnerable connection.  So what we want is for all systems to immediately eliminate SHA-1 from their collection of acceptable hashing functions.  Absolutely it should no longer be offered.



And, you know, it is a fine point, but for the record there are still some things you could use SHA-1 for safely if you chose.  It would make a fine hash for use in a PBKDF (password-based key derivation function) where a hash is iterated a great many times.  But given that its presence might allow its misuse, removing it altogether would be best.



Okay.  And one last little tidbit for any of our listeners who are using WordPress.  Last Wednesday WordFence, the very useful third-party WordPress web application firewall people who have been identifying troubled WordPress add-ons, they launched a free and very useful-looking vulnerability database for WordPress add-ons.  I poked around it a bit, and I'm impressed.  So I wanted to give our WordPress users a heads-up about it.  It is at wordfence.com/threat-intel.  Again, www.wordfence.com/threat-intel.  Looks like a very comprehensive listing of dangerous add-ons for WordPress.  I would say worth taking a look and making sure that you're not using any of those and might be unaware of the problems.



And Leo, time for me to catch up on my caffeine at the moment.



LEO:  Ketchup on your caffeine?  That doesn't sound very tasty.



STEVE:  Catch up on my - and it really is actually very tasty.  I'm loving it.



LEO:  Oh, good.  We will not be here next week.  We're going to be doing reruns.  Well, we like to call them the "Best Of" shows.  We carefully edit them, craft them, to be the best material from the year 2022.  That'll be a week from today, December 27th.  And then Steve and I will be back with a live show in two weeks, January 3rd.  This is our last show of 2022; January 3rd the first show of the brand new year.  Steve, let's continue on.  Soldier on, as you...



STEVE:  And you know, you're right, Leo, ketchup on my caffeine.



LEO:  You got it now.



STEVE:  I did.  Took me a little while.  I got it.



LEO:  It wasn't a very good joke.



STEVE:  I also don't put ketchup on my eggs, for what it's worth.  That's just not where...



LEO:  Oh.  How about hot sauce?



STEVE:  Oh, yeah.  A little Sriracha maybe, or some Tapatio. 



LEO:  Yeah.  Now you're talking.  See?  See?



STEVE:  There we go.  Okay.  So a bit of Closing the Loop feedback from our listeners.  Michael Lawley, he said:  "Please @SGgrc, it's pronounced meddy bank."  And, okay, I'm glad to know that.



LEO:  We didn't know.  How would you know, med-i-bank vs. meddy.



STEVE:  Right, Medibank and Meddy Bank.  And, you know, that does sound more Australian, doesn't it.



LEO:  Meddy Bank, yeah.



STEVE:  Yeah, in that kind of an accent.  So thank you Michael.  Glad to know.  SKYNET tweeted me:  "Question about ADP," referring to Apple's new encryption that was the topic of last week's podcast.  He says, or asks:  "Once it's turned on, and the keys are sent down to your device, is it stored in hardware or software?  Because what happens when you get a new iPhone in the future, how do you get the keys over to your new iPhone?  You can't set up the new phone and restore an iCloud backup once you log on, so it would have to be by the method where you move your old phone close to your new phone; correct?	Thanks."



Okay.  And I received a number of questions that are sort of related to this.  The primary concept that I guess I want to get across is similar to the familiar pattern that LastPass and presumably all other password managers use, at least I hope they would.  In all of those cases, they are simply storing an encrypted blob on our behalf.  They have no visibility into the blob.  But by making that blob available across devices, devices are able to share a common set of passwords or, as in the case of Apple, a common set of decryption keys.



So the process of Apple relinquishing the keys to iCloud is that Apple sends the current Keychain blob, which it is never and has never been able to decrypt, and the current iCloud keys, which until now it has held in its data centers' HSMs, to the user's device.  The device uses its local private account key, which never leaves the device, to decrypt the Keychain blob on the device, and then adds the current iCloud key into the Keychain.  In this way, the keys that Apple was holding are moved where Apple could get at them, into the user's account Keychain, where Apple can never get at them.  The device then instructs Apple to delete the iCloud keys that it just sent from all of its data centers' HSMs.  Now, only the device has the old iCloud keys in its Keychain.  Then, wanting to be thorough, the device performs a key rotation, changing the key that encrypts the iCloud data to one that Apple has never had in its possession.



But again, since we're all quite familiar with the notion of Trust No One and Pre-Internet Encryption, which is the technology that all password managers holding encrypted blobs that they're unable to decrypt use, I think that's the clearest way to think about this, and the best analogy.  Basically, Apple is holding this stuff for us, provides the synchronization service among devices, but is only able to hand the devices these blobs which are then decrypted locally on the device in order to give devices the keys which it's then able to use to go further.  You know, one of the things that I've been saying for years is that we've got all these very cool crypto components which we can assemble in any manner of different ways.  



Walt Stoneburner said:  "Steve, you have warned several times that pixelation is not a safe redaction technique.  Someone just wrote a beautiful GitHub project that visually brings home the point," he said, "as you see unredaction being performed."  And it's funny, I don't know why this started circulating again.  I got a whole bunch of tweets about it.  And I thought, oh, cool, something new.  But it was 10 months ago when we first talked about this and showed this.  So not something, turns out, that was new.



Michael Brodsted said:  "Hi, Steve.  Love your show.  Read this article and thought it might be interesting for you."  Okay.  So what Michael sent, and I appreciated it, was The Verge's follow-up on their story about those Eufy cameras that we talked about a few weeks ago.  Remember, those were the cameras that promised that all of their storage was local, and that nothing ever left the user's home, and that it was all transmitted directly to their phone.  Then of course in some reporting, following up on some news that that was not the case, The Verge was able to monitor their own Eufy cameras from the other side of the country.



And Leo, you and I talked about it at the time.  This was the company that was owned by Anker, and it was our conjecture that, you know, the way this could happen, because we like Anker, and we thought they were a reputable company, was that after having launched their successful power supply product line, they perhaps purchased the Eufy camera line in order to grow their business.  Anyway, we don't know.  But I guess I'd want to forgive them a little bit for making such a mess.



Anyway, The Verge checked back, and when did they find?  Their follow-up story is headlined "Anker's Eufy deleted these 10 privacy promises instead of answering our questions."  And the subhead reads "Two weeks after getting caught lying to The Verge, Anker still hasn't sent us any answers about its security cameras.  Instead, it's nerfed the Eufy 'privacy commitment.'"



So one of the things on The Verge's page, they have this wonderful mouse-based sliding divider where you can slide the divider with your mouse back and forth, and it reveals either - it's like a shutter, revealing either the old or the new privacy claims.  And if you pull it to the - I think you pull it to the right.  You see the original claims where nothing leaves your facility, it's all kept locally, blah blah blah.  You pull it to the other side, and you get then the updated claims which are dramatically toned down.  So anyway, The Verge makes a very good point.  And it's sad.  But on the other hand, all of these systems are out there.  They can't change the way they operate.  And I'm sure they never operated the way they said they did.  Someone just got a little maybe overenthusiastic or carried away when they were writing the marketing material for this.  Or, you know, maybe they did add features later where like they began to offer cloud things and never updated the page in order to make it correct.  So they've done that now.



Alain, he tweeted, @Alain_Gyger, he said:  "Thanks for another excellent episode.  I do have one question about TikTok.  Do you see a difference between the bans on ZTE and Huawei versus TikTok?"  He said:  "The FCC has labeled all three as 'unacceptable risk.'"  He said:  "Also, I just saw that there is a bipartisan bill that would 'end all commercial operations of TikTok in the U.S. and other social media platforms that are sufficiently controlled or influenced by America's foreign adversaries, including China, Russia, and Iran.'"  He said:  "It'll be interesting to see where this goes."



So, okay.  I wanted to include Alain's tweet to give me the opportunity to note that last Wednesday the entire U.S. Senate voted unanimously, passing a bill which would bar the installation of TikTok from any government-owned devices.  So, yes, whereas initially a handful of Republican governors and an attorney general may have been first during the previous week or two, now we have unanimous and obviously completely bipartisan agreement on this, which is astonishing to me.  Wow.  But it happened.



But to Alain's question, I do regard these selective bans such as on ZTE and Huawei, and even on TikTok, as mostly ridiculous theater because we are so intimately, deeply, and inexorably enmeshed with Chinese technology products.  You know, I look around, and everything in my home, all of the electronics that I own, and the electronics in what I drive, was fabricated in China.  Every bit of it.  And I'm sure that's the case for all the people listening to this podcast.  And speaking of listening to this podcast, this podcast is literally brought to your ears thanks to networking chips and processors and transistors all made in China by Chinese citizens, and much of it was designed there.



So to me, none of this posturing and saber-rattling makes any sense.  It must be that some sort of geopolitical kabuki is transpiring at a level that's far above my pay grade.  I'm just a simple technologist who does understand networking and processors and transistors, so I know that if China did actually want to be evil, the West would be in deep trouble because in the interest of economy we've allowed ourselves to become utterly dependent upon products which we need from China.  I don't want that to be a bad thing.  I hope it's never going to be a bad thing.  But if it is going to be a bad thing, then the problem is way bigger than a couple of wayward Chinese companies.



So maybe I don't get it, but it's sort of similar to me being amazed that Russia was still using Windows, like for the last many years, and still is.  They've finally said that they're thinking about moving to something Linux-based, presumably.  And there have been some rumblings of the same thing from China.  Just to me it seems crazy that that would be the case.  But here we are, utterly dependent upon another country and now saying we don't trust them.  Well, we're unable not to trust them, frankly.  So that's what I think.



And finally, David Ruggles.  He said:  "Reaching out regarding the zero-width space mentioned in Security Now! last week," he said, "I use it to fix stupid programs.  For example, if you want to reference an account on Twitter without tagging them, enter the at sign, then a zero-width space, and then the account name, and it won't get tagged."  And then he gave some examples of things that were not tagged, were and weren't tagged, in his tweet to me.  And so he said @TheRealRuggles vs. @TheRealRuggles.  They looked identical.  Only one was lit up.  He said:  "Similarly in Excel it defaults to adding a hyperlink when you enter anything that looks like a URL or email address.  You can use the zero-width space to prevent that behavior without changing the look of the text."



So that was cool.  I think that's very clever.  The problem is, I mean, and I should say I can see many applications for that, as well, but it leaves the question, how do you enter a zero-width space through the keyboard?  I asked the Google, and I was told:  "The zero-width space is a Unicode character U+200B," which is also HTML &#8203; right?  And Google said:  "It's remarkably hard to type.  On Windows you can type ALT and then 8203."  Well, I tried that, and I got, what is that, the symbol for maleness, I think.  Anyway, that didn't work, ALT+8203.  So if anyone can figure out how to type these, how to enter the zero-width character through our keyboard, I think that seems like a useful thing to be able to do.



LEO:  People were also using it to post their Mastodon link on Twitter because it didn't look the same.  I mean, it looked the same, but...



STEVE:  Without triggering the thou shalt not post...



LEO:  Yeah, their filters, yeah.



STEVE:  Oh, cool.  Okay.  Briefly I'll note that SpinRite is looking quite good.  By the end of this past weekend we were at the eighth alpha release, every known weird data recovery behavior that we were seeing appears to have been resolved, and SpinRite is now cruising through even the most damaged and troubled drives.  While my focus was on getting SpinRite to properly perform those primary functions, I had also been accumulating a list of less critical but still necessary to-do items.  And our testers that have been getting a little restless have been suggesting new features that they'd like to have.  You know, nothing big, but there are some convenient things that make sense.



So by the end of the day, Sunday, two days ago, I told the group that I would be retrenching now and disappearing for a while, while I worked my way through everything that was on the wish list and the things to be fixed.  After today's podcast, that's what I'll be doing.  When I return with alpha release nine it should be very close to finished.  I'm sure there'll be a few loose odds and ends.  That's the nature of such a complex project.  But I have to say with some pride and relief that everybody who has been testing 6.1 has been very impressed by this new SpinRite's speed and capabilities.  So we're getting there.  It's not going to be a Christmas present.  It's not going to be a New Year's present.  But it's going to be early in 2023 that we finally have this 6.1 for everybody.



Okay.  A Generic WAF - that's W-A-F - Bypass.  As an industry, we've matured to the point where vulnerabilities are being discovered only in specific implementations of some specific solution, and only typically in specific versions of those implementations.  In other words, whereas once upon a time the entire industry would realize that an established standard could be abused in an unexpected way, and everyone's implementation would need to be changed.  That's where we were.  A perfect example was everyone's DNS servers which were emitting queries from ports sequentially assigned by their underlying operating system and often emitting those queries with sequential identifiers.  When that came to light, those who were focused on DNS realized that this would allow for successful DNS spoofing at scale, and the entire industry repaired DNS overnight.



These events stand out because, thankfully, they've become so rare.  These days, as we know, problems have generally become much more obscure and specific.  For example, it might be that if you're still using the out-of-support version 2.029.472 of JimmyCrack's Query Reflector, you need to update it to at least version 2.426.327 in order to avoid problems with query reflection back flush, and you should do so immediately.  Those are the kinds of things we're often seeing now.



LEO:  It's not real, folks.  Don't go looking for Jimmy's back crack.



STEVE:  Oh, don't worry.  Unless you actually do have JimmyCrack's Query Reflector, in which case you've got other problems.



Today's rarity of big generic protection bypasses has made their existence extremely interesting.  And a group known as Team82 recently discovered just such an industry-wide mistake.  They discovered an attack technique that acts as the first generic bypass of multiple web application firewalls being sold by industry-leading vendors including at least Palo Alto Networks, F5, Amazon Web Services, Cloudflare, and Imperva.



Okay.  So before we proceed, we need to briefly revisit another one of those "Holy Crap!" events which hit the entire industry many years ago, and which due to its difficulty the industry continues to grapple with, and that's SQL injection.  Stated succinctly, SQL injection can occur when there is some way for user-provided input to be passed to a SQL database for its interpretation.  A SQL database is driven by strings of characters which express commands and queries.  Simply by typing commands, new database tables can be created.  They can be populated with data, queried for their data, and deleted when they're no longer needed.  New users can be instantiated, passwords can be changed, privileges can be granted, all through simple text commands.  And further increasing the system's power, the simplicity of this interface allows SQL databases to be queried over networks.  The simplicity and the power of this interface explains SQL's success.



But the simplicity and power of this interface has also been at the heart of one of SQL's longest running vulnerabilities.  Wikipedia tells us that the first known public discussion of SQL injection appeared around 1998, and cites an article in Phrack (P-H-R-A-C-K) Magazine, long since discontinued.  SQL injection has been the bugaboo of web applications from the start.  The first web apps gleefully presented a form asking their user to please enter their full name to look up their record in the site's database.  The designer of this form assumed that that's what anyone would do.  So whatever string they provided as their name would be added into a SQL query string to access the site's database.



And all was well until it occurred to some clever individual that the website had inadvertently given them direct access to that site's SQL database back end.  Rather than simply inputting their name, they could, for example, input a string which closed the open query and started another entirely separate SQL command of their choosing.  This allowed a remote visitor to directly issue SQL commands to the site's database.  If the web designer had assumed that no one else could ever access the database, which of course is what they assumed, the SQL account behind the website's form might even have admin rights.  This would allow remote visitors to do anything they might wish.



This has been such a common and persistent problem because the fundamental architecture of this system is fragile.  It is not inherently secure and resilient.  It is inherently insecure.  We need to take user-supplied input, like some personal details, and embed them into a database query so that we can look up their record.  We have to do that; right?  The trouble with SQL is its power.  That same query channel is also SQL's command and control channel.



This has been such a longstanding and well-understood problem that it found its way into one of XKCD's brilliant comics, and we've talked about it in the past.  The first frame shows somebody holding a cup of coffee, listening over the phone.  This is Mom, who's received a phone call.  And over the phone she hears:  "Hi.  This is your son's school.  We're having some computer trouble."  Mom replies:  "Oh, dear.  Did he break something?"  And we hear the voice of the distraught principle saying:  "In a way.  Did you really name your son Robert '); DROP TABLE Students;-- ?"  And Mom says:  "Oh, yes.  Little Bobby Tables, we call him."  And then the principal says:  "Well, we've lost this year's student records.  I hope you're happy."  To which Mom replies:  "And I hope you've learned to sanitize your database inputs."



LEO:  Such a classic.  Such a great comic.



STEVE:  Perfect.



LEO:  Yeah.



STEVE:  And so what XKCD is telling us is like exactly this.  And so here's, I mean, stepping back from this a bit, the biggest problem is, through all these years, since it was first understood near the birth of the web, no one has fixed this.  Instead, we just keep patching it.  We focus upon each mistake in isolation, rather than recognizing that the entire architecture is wrong for this application.  SQL was not created for the web.  No one would have done that.  It was first designed in the early 1970s.  Leo, we were just talking about when we graduated from high school.



LEO:  Yeah.



STEVE:  Yeah, back then.



LEO:  Yeah.



STEVE:  Yeah.  That's when this, you know, IBM came up with this before there was an Internet or websites or web apps.  Unfortunately, the web found it, and it's been a troubled marriage ever since.  The problem is every newly created web app creates another new opportunity to make a mistake in the parsing of user-supplied input that would give a remote attacker access to the site's backend database.  That's why I say that the systems we've built around this architecture are inherently brittle and fragile.  That's why, still today, SQL injection attack scans are constantly sweeping the Internet looking for that newly created, newly vulnerable web app, and SQL injection remains at the top of the OWASP Top 10 list of web application vulnerabilities.



So what do we do if there's no sign that we're going to fix the underlying problem?  Well, the universal solution to protecting our networks from external hostility is to place a firewall in front of those networks and force all external traffic to be inspected and to pass through that gauntlet before it's permitted to reach our interior, presumably vulnerable networks.  And thus was born the idea of the Web Application Firewall, or WAF for short.



The fundamental concept of a web application firewall is detailed traffic inspection.  Whereas packet-level firewalls generally look no deeper than packet headers, which specify the source and destination IPs and ports for the purpose of monitoring packet flows, a web application firewall examines in detail the content of all web application traffic transiting its boundary in order to detect and block malicious attacks.



So in XKCD's example above, a WAF would spot and block a form's input field data that contained suspicious characters for a user's name such as closed parentheses and semicolons, so that they would go no further.  With a web application firewall positioned upstream of an organization's web application servers, that malicious data and intent would never reach any web applications that might not be adequately providing for their own protection.  Again, you wouldn't need this if mistakes still weren't being made freshly, but they are because this is all being done wrong.  But it's what we've got.



So, okay.  With this background, here's what Team82 had to say about their recent discovery.  They wrote:  "Web application firewalls (WAFs) are designed to safeguard web-based applications and APIs from malicious external HTTPS traffic, most notably cross-site scripting and SQL injection attacks that just don't seem to drop off the security radar."  Gee, imagine that.  I wonder why.



They said:  "While recognized and relatively simple to remedy, SQL injection in particular is a constant among the output of automated code scans, and a regular feature on industry lists of top vulnerabilities, including the OWASP Top 10.  The introduction of WAFs in the early 2000s" - okay, note that time, note that date - "early 2000s was largely a counter to these coding errors.  WAFs are now a key line of defense in securing organizational information stored in a database that can be reached through a web application.  WAFs are also increasingly used to protect cloud-based management platforms that oversee connected embedded devices such as routers and access points.  An attacker able to bypass the traffic scanning and blocking capabilities of WAFs often has a direct line to sensitive business and consumer customer information.  Such bypasses, thankfully, have been infrequent, and one-offs targeting a particular vendor's implementation.



"Today, Team82 introduces an attack technique that acts as the first generic bypass of multiple web application firewalls sold by industry-leading vendors.  Our bypass works on web application firewalls sold by five leading vendors:  Palo Alto Networks, F5, Amazon Web Services, Cloudflare, and Imperva.  All of the affected vendors acknowledged Team82's disclosure and implemented fixes to their products' SQL inspection processes.  Our technique relies first on understanding how WAFs identify and flag SQL syntax as malicious, and then finding SQL syntax the WAF is blind to.



"This turned out to be JSON, JavaScript Object Notation.  JSON," they write, "is a standard file and data exchange format, and is commonly used when data is sent from a server to a web app.  JSON support was introduced in SQL databases going back almost 10 years.  Modern database engines today support JSON syntax by default, including basic searches and modifications, as well as a range of JSON functions and structures.



"While JSON support is the norm among database engines, the same cannot be said for WAFs.  Vendors have been slow to add JSON support, which allowed us to craft new SQL injection payloads that include JSON, and that completely bypassed the security WAFs provide.  Attackers using this novel technique could access a backend database and use additional vulnerabilities and exploits to exfiltrate information via either direct access to the server or over the cloud.  This is especially important for OT and IoT platforms that have moved to cloud-based management and monitoring systems.  WAFs offer a promise of additional security from the cloud.  An attacker able to bypass these protections has expansive access to systems."



Okay.  So what happened?  History has shown that no one is able to always get SQL injection protection correct because it's so much easier for it not to be correct.  So the notion of a web application firewall is created to move the burden from individual input forms, fields, and web apps to the perimeter, where a single comprehensive web application firewall will be able to protect all of an organization's applications at once.  That happened about 20 years ago, in the early 2000s.  Now, remember, though, only for those organizations that deploy them.  A web application firewall is like your big iron box.  It's expensive.  It needs to be constantly maintained.  It needs to be licensed.  Smaller organizations aren't going to have them.  But the big guys do for the last 20 years.



The problem, of course, is that now it's become less imperative for those individual web applications, which are now safely ensconced behind their protective application barrier, to be quite so worried about their own input form field content.  After all, there's a big mean web application firewall at the front gate that's going to keep little Bobby Drop Tables safely out of reach.  So all is well.



But then a decade passes, and a particular syntax for describing the features and details of objects becomes popular.  It outgrows its own modest origins and is adopted by other languages and applications because it does the one thing it was designed to do cleanly, minimally, and efficiently.  And so the JavaScript Object Notation, JSON, grows increasingly prevalent.  Perhaps it was inevitable that SQL databases would eventually choose to add their own support for JSON.  And they did.  Here's what Team82 had to say about that.



They said:  "In modern times, JSON has become one of the predominant forms of data storage and transfer.  In order to support JSON syntax and allow developers to interact with data in similar ways to how they interact with it in other applications, JSON support was needed in SQL.  Currently, all major relational database engines support native JSON syntax by default.  This includes Microsoft SQL, PostgreSQL, SQLite, and MySQL.  Furthermore, in the latest versions, all database engines enable JSON syntax by default, meaning it is prevalent in most database setups today.



"Developers have chosen to use JSON features within SQL databases since it became available for a number of reasons, starting with better performance and efficiency.  Since many backends already work with JSON data, performing all data manipulation and transition on the SQL engine itself reduces the number of database calls needed.  Furthermore, if the database can work with the JSON data format, which the backend API most likely uses as well, less data pre- and post-processing is required, allowing the application to use it immediately, without the need to convert it first.



"By using JSON in SQL, an application can fetch data, combine multiple sources from within the database, perform data modification and transform it to JSON format, all within the SQL API.  Then the application can receive the JSON-formatted data and work with it immediately, without processing the data again.  While each database chose a different implementation and JSON parser, each supports a different range of JSON functions and operators.  Also, they all support the JSON data type and basic JSON searches and modifications."



And here's the key underlying what Team82 discovered:  "Even though," they wrote, "all database engines added support for JSON, not all security tools added support for this comparatively new, though decade-old feature, which was added as early as 2012.  This lack of support in the security tools" - meaning the WAFs - "introduced a mismatch in parsing primitives between the security tool - the WAF - and the actual database engine which is implementing SQL, and caused SQL syntax misidentification."



They said:  "From our understanding of how a WAF could flag requests as malicious, we concluded that we needed to find SQL syntax the WAF would not understand.  If we could supply a SQL payload that the WAF would not recognize as SQL, but the database engine would parse, we could actually achieve the bypass.  As it turns out, JSON was exactly this mismatch between the WAF's parser and the database engine.  When we passed valid SQL statements that used the less prevalent JSON syntax, the WAFs did not flag requests as malicious.



"The JSON operator '@>' which checks whether the right JSON is contained in the left one, threw the WAFs into loops and allowed us to supply malicious SQL payloads and allowed us to bypass the WAFs.  By simply prepending simple JSON syntax to the start of the request, we were able to exfiltrate sensitive information over the cloud."



So this forms a very interesting story.  We start with a fundamentally insecure design when a powerful database system from the '70s which was never designed to allow malicious users to access its command input stream is used as the backend database for websites, thus inadvertently giving malicious users access to its command input stream.  Rather than recognizing that using SQL in this way is fundamentally a horrific mistake, every individual website must patch their input field parsers in an attempt to prevent SQL command and query syntax from being submitted by the visitors to every site.  SQL injection becomes a meme, and XKCD captures its essence.



In an extension of the firewall concept, web application firewalls are created to centralize and concentrate the SQL syntax filtering challenge.  And all seems fine for a time.  Then SQL syntax undergoes a fundamental extension as all SQL servers implement support for the increasingly popular JavaScript Object Notation.  But despite this extension, some of the industry's web application firewalls fail to update their protection logic to incorporate an awareness that JSON can now be used to encapsulate and issue SQL queries.  Fortunately, a team of white hat security researchers stumble upon this tidbit while they're working to discover just such a bypass, and they quietly inform the many vendors of those vulnerable web application firewalls of their discovery.  And all is well again.



Or is it?  Because SQL is still powering virtually all web applications, and the fundamental problem of now an even more powerful SQL syntax existing still remains.  If JSON could be used to slip past web application firewalls to reach the SQL database behind, how many websites, individual websites that are not being protected by a big iron web application firewall, might now be vulnerable today to exactly the same JSON bypass?  Happy New Year.



LEO:  A lot of the apps that I've used either use MySQL, which isn't the - it's SQL syntax.



STEVE:  That's it, MySQL.



LEO:  So it counts.



STEVE:  No, it's exactly the same.  It supports JSON.  



LEO:  Yeah, as long as it supports JSON.  How about SQLite?  Same thing?



STEVE:  Yup.



LEO:  Okay.



STEVE:  SQLite, MySQL, PostgreSQL, and MS SQL.



LEO:  As long as it supports the SQL language, which that's the IBM, is the SQL Server.  That's the original.



STEVE:  Right.  Right.



LEO:  But these all support that language.  So they're all...



STEVE:  Yeah, and even MariaDB is a...



LEO:  Really.  Maria also supports it.



STEVE:  Yeah.



LEO:  Yeah, because everybody knows SQL and knows that language; right?



STEVE:  Yup.  



LEO:  So why would you get a new one?



STEVE:  And unfortunately, exactly, why would you invent a new one?  It's horrible to use it as a backend for the web.  But it's the one we've got.



LEO:  Well, I mean, you could use it as a backend.  You just don't want to expose it.



STEVE:  The problem is, if you say, you know, look up the username that the user inputs, you're taking the - you're inherently taking the string they gave you.



LEO:  And that's going to be a SQL string, yeah.



STEVE:  And inserting it into a query.



LEO:  Yeah, yeah.



STEVE:  I mean, so the problem is that that query is not just a query.  It's also command and control, account creation, table deletion, I mean, it is - it was never meant to be exposed to arbitrary input.  But, oh, look, we got SQL.  Let's use it as our backend.



LEO:  And sanitizing your inputs merely, I mean, it requires you to be clever enough to catch all the...



STEVE:  Perfect.  Perfect.



LEO:  You have to be perfect.



STEVE:  Perfect every single time.



LEO:  Yeah, yeah.



STEVE:  That's why I say this is inherently broken, inherently bad.



LEO:  Right, right.  What would be the alternative?  A new language?  Well, any database query language is going to have - is going to be prone to this problem; right?  



STEVE:  Well, no, because a database query language should not let you delete the database that you're querying.  That's not a query language.  That's a command and control language.



LEO:  Right.  So separating the queries from the control and command would be the solution.



STEVE:  Yes.  And, you know, it's reminiscent of the printf we talked about Apple getting tripped over, where the problem was they - the printf inherently mixes control with text.



LEO:  Right, right.



STEVE:  And that's a bad idea.



LEO:  You should see what the format string in Lisp can do.  I mean, it's printf on steroids.  It predates printf because it's Lisp.



STEVE:  Right.



LEO:  And it is crazy the things you can do with that.  It's a programming language in and of itself.



STEVE:  Right.



LEO:  And that's probably not a good thing.  I would never - I could never imagine opening your website to a random format string.  So I guess I can see the inherent problem here, yeah.



STEVE:  Yeah.  You know, I don't know how we fix it.  You could preserve, you could reengineer it so that the query was fundamentally limited through that channel, you know, so that you could...



LEO:  Yeah.  You could say I won't accept commands of any kind, only search queries.



STEVE:  Right.  And...



LEO:  And I guess that's what sanitizing your inputs means.  But it's hard to do that perfectly, especially since they're probably using regular expressions to parse it or something.  I don't know.  I wonder what the current best practices is.



STEVE:  Well, and that's just it.  The problem is, you know, how many times, Leo, have we encountered, for example, a TCP/IP stack where security researchers figured out, ooh, you know, we can't do things this way.  We have to do them that way.



LEO:  Right.



STEVE:  You know, a classic example is packet fragmentation.  It turns out it's bizarrely difficult to deal with fragmented packets.  Yet new people come along and reimplement the TCP/IP stack and make the same errors that we fixed 30 years ago all over again.



LEO:  Yeah.



STEVE:  Because there are some things that are just hard to get right.  And the problem is, like SQL is what everyone uses as their backend, and it's a bad idea.



LEO:  Well, you need a database of some kind.  I think the bad idea is to allow commands to - you would think the permission structure would say, look, unless you're logged in as a permissioned user, you shouldn't be able to execute commands.  And then just keep the privilege level of the web server and the web queries low.  Seems like that would be solvable.



STEVE:  Unfortunately, the programmers who put this together never think that.,



LEO:  Well, and they also want the power.



STEVE:  They're like, well, you know...



LEO:  They want to be able to do it.



STEVE:  I developed this code.  It's great.



LEO:  Yeah, yeah.  Yeah, I don't think it's insoluble.  And you do need a database on the backend.  I mean, that's the modern web.  You don't want flat files.  I bet you're all flat files, though.  You don't have a database on your backend; do you?



STEVE:  Actually, one of the things that has been really heartening is that when you go to GRC, and you put your SpinRite serial number in, in order to get a link for the pre-release, it's shocking how fast it is.



LEO:  Did you write the program yourself?  Of course you did.  In assembler?  Of course you did.



STEVE:  It's a super lean embedded database that's being accessed in assembler, of course.



LEO:  Yeah.



STEVE:  And it's just - it's a simple indexed database, and it is like, it is amazing.  And it took me a while to realize everything else I use, I click the button, and it's like, okay, wait a minute, you know, it's still spinning.  And then it comes up.  But not GRC, it's just pow.



LEO:  Nice.  Very interesting.  Of course this is why you have to listen to the show; right?  The best, most interesting stuff.  It's been a great year, Steve.  I think you'll enjoy the Best Of.  We found some really fun bits to put together.



STEVE:  Oh, neat.



LEO:  That's next Tuesday.  And then the following Tuesday, January 3rd, we're back again with Episode, what'll that be, 903.  Yeah.



STEVE:  903, baby.



LEO:  Wow.  We're getting close to the end.



STEVE:  Don't say that.  Don't say that.



LEO:  It's like walking off a cliff.



STEVE:  We've got a whole two years.



LEO:  We've got a long walk off a short pier ahead of us.



STEVE:  That's right.



LEO:  As my father used to say.	



Copyright (c) 2022 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.






GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#904

DATE:		January 3, 2023

TITLE:		Leaving LastPass

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-904.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week, since a single topic dominated the security industry and by far the majority of my Twitter feed and DMs, after a brief update on my SpinRite progress we're going to spend the entire podcast looking at a single topic:  LastPass.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here, and the topic of the day, really the topic of the whole show, is the topic everybody has wanted Steve to comment on since the news broke late last year about the LastPass breach.  What happened, what does Steve think, and what's he going to do?  That's coming up next.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 904, recorded Tuesday, January 3rd, 2023:  Leaving LastPass.



It's time once again, first show of the new year for Security Now!.  Here he is, well rested, relaxed, hasn't had a security problem in weeks - not so - Steve Gibson.  Hi, Steve.



STEVE GIBSON:  Yo, Leo.  Great to be with you for this first podcast of 2023.



LEO:  Wow.  Did you have a good holiday?	



STEVE:  I did.



LEO:  Good.



STEVE:  I got a lot of work done, which is for me that's a good holiday.  And I apologize to my lovely wife, who just said, "You're a machine."



LEO:  Yeah.



STEVE:  And I said, well, you know, when you love what you do, you do what you love.



LEO:  You kind of have to, though, to be a good coder.  You kind of keep - you get rusty quickly, so you have to keep going; right?  That's been my experience.



STEVE:  On CNN, Fareed Zakaria had a series of interviews of, like, well, people we all know.  And the one that I thought was interesting was Elton John.



LEO:  Yeah.



STEVE:  And, you know, you hear about these guys who are, like, just amazing.  But when you look at the back story, like in his 20s and 30s he didn't think he was going to succeed.



LEO:  No.  Little Reginald Dwight, the Beatles used to mock him.  But he knew the Beatles, on the bright side.



STEVE:  Yeah.  Anyway, so I guess my point is that, yeah, you know, I've been programming computers since I was 14, and because I love it.



LEO:  Yeah.



STEVE:  And I have a plan that's going to keep me coding until I'm finally, like, okay, where did the ENTER key go?  What?  Where?



LEO:  Has it gotten harder or easier for you as time goes by?



STEVE:  I suspect that - I guess it would be a change in characteristics.  Certainly the way I approach solving problems in code has matured so that things are easier because I'm not painting myself into corners.  When surprises occur, it's like, oh, look.  Oh, in fact I had that happen a couple weeks ago because I rewrote SpinRite's logging system completely.  Well, it turns out what I had written was pretty good.  And so it allowed me, you know, it didn't fight back very much when I was like trying to change the way it worked.  And it's like, wow, this was a pleasant experience.  I didn't think I was going to have that happen.  So, you know, but when you're starting out, you can sort of force the computer to do what you want, as opposed to it just sort of gracefully agreeing.  And that's just sort of a consequence of the way you approach code.



LEO:  Somehow I think you get it in your mind, and you're speaking a language that is second nature.  And so now it's a question of just conversing; right?



STEVE:  Yeah.  And I think in the beginning I was - because I remember I was editing a series of books in my late teens called "Teach Yourself Basic."



LEO:  Oh, yeah.



STEVE:  Bob Albrecht was the author.  



LEO:  You were 19 and editing Bob Albrecht's books?



STEVE:  Yeah.



LEO:  Wow.  Wow.



STEVE:  Anyway, but I remember coding back then, because I didn't have a lot of experience, when the program didn't do something, you'd go, oh, and you'd like stick in a go-to; right?  



LEO:  Right.



STEVE:  And you're like, ooh, I need an if statement here to make it go over here.



LEO:  Yeah, a Band-Aid over it, yeah, yeah.



STEVE:  It was like, it was just like, oh, not right.



LEO:  And it got worse and worse and worse.



STEVE:  Exactly, because you end up just chasing yourself around.  Oh, wait.  Now it needs to be this, unless it's this, in which case we've got to go over here.  So anyway.



LEO:  You don't do that anymore.  You know, because you have enough experience now.  You kind of know the consequence of that kind of behavior.



STEVE:  Yeah.  And I will tell you, though, the same problems or the same issues always exist, which are those off-by-one problems.  That's like something fundamental to computers.  



LEO:  It's universal, yeah.



STEVE:  It just, you know, do I mean greater than, or do I mean greater than or equal to?  And it's like, oh.  And so now, you know, I'm just - so I think what you learn is to be very careful about those.  You're not going to get rid of them because, you know, you can't.  But so, but what you can do is appreciate that there's a landmine every time you're checking to see whether, wait, do I mean that the carry bit is on, or the carry and the zero flag are on? 



LEO:  Yup, yup.



STEVE:  So, you know.



LEO:  Yup, yup, I totally understand it, yup.



STEVE:  Okay.  So.



LEO:  Let's get to the show.  Here we go.



STEVE:  Our listeners will be glad, those who have already left, to know that the title of today's podcast is "Leaving LastPass."



LEO:  Yikes.  I have to say I left almost two years ago, well, maybe more, for Bitwarden.  And then about a year ago I finally said, "I'm going to delete my LastPass vault."  Which as it turns out I'm thanking goodness for.  I do hope they didn't back it up somewhere, and that that wasn't the backup that was downloaded.



STEVE:  Yeah.



LEO:  There's no assurance on that.



STEVE:  So, you know, this past couple weeks a single topic has dominated the security industry, and by far the majority of my Twitter feed and DMs, you know, it just exploded.  So today, after a brief update on how my holidays went with SpinRite, we're going to spend the entire podcast looking at a single topic, which is LastPass.  And I think everyone is going to appreciate this because, you know, there's no hyperbole.  There's no hyperventilation.  There was plenty of that on the Internet, and we'll talk about some of that, because that's part of what fueled everyone's frenzy.  So I'm going to do a little bit of taking down of the takedowns.  So the question is, what happened?  What does it mean?  How worried should you be?  What are the consequences?  And what's next?  And of course we have a great Picture of the Week.



LEO:  Very important subjects.  And we will get to those in just a second.  Picture of the Week time, Mr. G.?



STEVE:  So this is another one of those.  I looked at the picture, and I gave it the caption "There are no words."



LEO:  And I immediately burst out laughing when I saw it.



STEVE:  Yeah.  I looked carefully at the road behind where the arm is to see if maybe somebody had photoshopped it to like exclude the arm from the picture because how else do you explain this?  Okay.  So for those who are not seeing the photo or don't have the show notes, we've got your automotive gate, you know, with an arm that comes down in order to block a car from passing.  And in fact you can see the sensor loops that are in the asphalt that have the wires going over to the little motor stand that moves the arm up and down.  So it all looks real.  The only problem is the arm extends about, like, a foot into the roadway. 



LEO:  Yeah.



STEVE:  And it's a two-lane road.



LEO:  There's a lot of room to drive around the arm, let's put it that way.



STEVE:  Yeah, I just, you know, somebody went to a lot of trouble.  This is a bright yellow pedestal with a blue-and-red striped, like visible arm.  You know, I was going to say so you don't hit it, but it'd be hard to hit it because it's like not in the road.



LEO:  Yeah, that's a good point.  It's not even like you could hit it.  You'd really have to work at it, yeah.



STEVE:  Maybe if your passenger door was open when you were driving by.



LEO:  It's just a suggestion.  It's a security suggestion.



STEVE:  Oh, wow.  Anyway, thank you whomever sent this.  I appreciate it.  Okay.  As I said, if there was any security news that happened since our last podcast, I never had time to track it down since I was digging deep into today's topic, which exploded my Twitter feed.  So next week I'll be looking back over everything that has transpired in the security world - other than LastPass - since our last podcast.



LEO:  Oh, good.  Oh, good, okay.



STEVE:  And we'll catch up.  So, you know, nothing is getting by me.  Next week we'll be brought current from the last time we talked.  Also the last time we talked I mentioned that the eighth alpha release of SpinRite had been put out on the prior weekend before the podcast.  There has been no release since.  We're still on eight.  And the testers are getting anxious.  They're like, hey, we were having a lot of fun here.  What happened?



Okay.  So mostly those releases were intended to resolve a couple of mysterious behaviors that people were reporting on seriously damaged drives when they were testing SpinRite against what it turns out they have literally - our testers have boxes of old hard drives that they've been holding onto in anticipation of this day.  So I've got pictures of them, like boxes.  And they're like, they have scoreboards of SpinRite has recovered these 11, and it's got problems with these four, and we're not sure about a couple others.  It's sort of - it's a lot of fun.



Anyway, because I was focused on resolving those few mysteries, it was just difficult for me conceptually to move on because there were some problems that I didn't understand.  So I wasn't fixing the growing number of non-mysterious things that people were noting.  Those were things like during intense data recovery SpinRite was not updating its onscreen clocks.  They were just froze while SpinRite was like grinding away on a hard drive, trying to recover a damaged sector.  Or if the system hung or crashed, the permanent log of all the work that had been done up to that point would be lost because SpinRite's log was deliberately being kept in RAM until SpinRite's graceful exit.  But unfortunately, if there was not a graceful exit, you'd get no log.  So anyway, thanks to all of the feedback during that first flurry of testing, I saw that SpinRite could be much more aware of drive trouble now that it was no longer isolated from the drive by the system's BIOS, and more.



Essentially, the feedback from that extensive initial testing showed me some ways in which I could improve upon my whole first take.  You know, I want 6.1 to be as good as it can possibly be, not only because we're going to be living with it for a while during the work on SpinRite 7, but because every way in which I can make SpinRite 6.1 better today will be inherited by SpinRite's future.  So none of this work is going to get tossed.



So I've reworked some significant portions of SpinRite over the holidays.  Logs are now being incrementally written to non-volatile media so that everything that has happened so far will be saved in the event of a system hang or crash.  And if the user wishes to write SpinRite's log back to the same drive SpinRite's running on, then it will be spooled to RAM and written at the end, and they're notified that's going to happen so they could change their mind if they want to.  I've also completely rewritten SpinRite's clock management and completion estimation system.  It's now also providing much more feedback about the state of critically ill drives, which it turns out we have in abundance, thanks to the testers that we have pounding on SpinRite.



So I'm left now with a bunch of to-do list items.  Our GitLab has a lot of feedback.  So I'm going to continue working on this ninth alpha release, that is, when I release it that's what it'll be, because my goal is to basically finish it again, get everything done that I know of that can be finished so that people are not reporting things that I already know I need to do.  I'm going to do all those first.  And then release nine, this alpha release nine, ought to be like as close to beta as it can be.  I'm sure there'll be a few more iterations.  I suspect that these really weird drives are still going to give SpinRite some heartburn.  But, you know, now's the time to take an antacid.



So, okay.  We last talked about LastPass following the November 30th disclosure of the second breach.  So that was, you know, in each of these breaches, the one back in August and now this one in November, there have been a pair of disclosures.  There's the immediate acknowledgment that something happened, and then about three weeks later we get a more fulsome explanation of exactly what that was that happened.  After the first announcement in August I noted that their follow-up occurred exactly three weeks, 21 days later, with the results of that forensic analysis.



So when we talked about this on November 30th, our listeners may remember that I noted that we might expect to receive an update on the second breach three weeks after that.  Three weeks after Wednesday, November 30th was Wednesday, December 21st, and the follow-up arrived one day after that on December 22nd.



Now, the "likes" and the clickbait-driven tech press and some security researchers' postings suggested that LastPass had deliberately timed their bad news for release shortly before Christmas in some attempt to have it swept under the rug.  Given the timeline we've seen LastPass adhere to previously, I think that's not the case.  And, you know, it's the kind of junk that causes any careful reader when they see that, if they understand that that is actually where the timing came from, to wonder whether what follows will be objective reporting of facts or a subjective smear.



Two days after Christmas, a researcher named Jeremi Gosney, who is a widely recognized expert in password cracking, wrote a lengthy and inflammatory takedown of LastPass over on his new Mastodon account.



LEO:  Yes, I read that, yes.



STEVE:  Yeah.  He recently moved there from Twitter.  And in fact over on Twitter he said:  "I won't be posting anything new here.  I'm over on Mastodon."  So while Jeremi made some excellent points that I will be touching on here in a few minutes, he also suffered from the "piling on" syndrome.  You know, for example, he wrote, quoting him:  "LastPass's claim of 'zero knowledge' [he has in quotes] is a bald-faced lie.  Nearly everything in your LastPass vault is unencrypted."  He says:  "I think most people envision their vault as a sort of encrypted database where the entire file is protected.  But no.  With LastPass, your vault is a plaintext file, and only a few select fields are encrypted."



Okay, well, you know, what does one say to that?  There's nothing about that that accurately conveys the truth, or even a sense of the truth.  As evidence of that, another well-known researcher, Wladimir Palant, the creator of Adblock Plus, who blogged a lot - he blogs from his site titled "Almost Secure," and he had four blogs over the holidays about this incident.  The day before Christmas, Wladimir posted an article titled "What data does LastPass encrypt?"  And - this is important - he provided a snippet of JavaScript code that could be dropped into any web browser's developer console, which we all have, to retrieve the logged-on user's data blob, you know, their vault, directly from LastPass's cloud server.



And I'm going to suggest that everybody listening to this podcast do this because there are some fun things we have in store.  In the show notes I have this little three-line snippet of code.  So any current LastPass user can obtain their encrypted vault data to examine it for themselves.  Open any browser.  I used Chrome for this, and I also tested on Edge.  Log into LastPass, assuming you still have an account there, so that you're looking at your vault page, which is what you get when you log into LastPass.  Press F12 to open the Developer Tools.  I think there's also some other way of getting there, probably under the main menu.  I'm sure you can go to Developer Tools.  F12 is what I use.  And your screen will basically split into your web page on the left and this whole bunch of stuff maybe you've never even seen before on the right, which is where all of the magic under the browser happens.



Select the Console tab to move to the Console view in the Developer Tools, and you'll have a cursor.  So paste the short three-line JavaScript query which is shown above in the show notes into the Console and press ENTER.  If everything worked, the screen there will fill with a large XML format dump containing name-value pairs.  But the vault is far larger than your page.  So look carefully at the bottom of the page where Chrome or Edge will be saying Show More, where you can like ask it for another chunk, but also offering a Copy button.  Click Copy, which will move all of that query response data onto your machine's clipboard.



Open a text editor - I use Notepad++ on Windows - then paste the clipboard into the editor and save the file so you don't lose it.  Okay, now, there's a bunch of stuff there that you can peruse.  Passwords begin with p, with a letter p, lowercase p, and an equal sign, followed by a double-quoted string.  AES-CBC encoding has two parts, the first being - I'll be getting into a little bit more of this here in a minute, but I just wanted to go over this part first - has two parts, the first being a pseudorandom initialization vector which CBC requires, and the second half or part of it being the encrypted string itself.  The older format passwords will only have a single ASCII-encoded string, which is the ECB format encryption of the data since ECB - that's the Electronic Code Book we were actually talking about back when we were talking about Microsoft Office 365 in October.  It does not use an initialization vector.  And that's the reason its use has been deprecated. 



So then with Notepad++, since you're able to do Regex searches there, I wrote a quick Regex to scan my encrypted vault for any non-CBC-encoded or encrypted passwords, and I didn't find any.  And I put the little Regex expression also in the show notes for anyone who's interested.  Now, that's not conclusive.  But I'm pretty sure that my vault contains very old and original passwords which predate LastPass's switch to CBC.  And we'll be getting to why that's important here in a second.  So anecdotally I'm not seeing evidence of old CBC encryption that was not autonomously upgraded for me.  And again, I wanted to make sure that everyone understood that they are able to easily obtain their encrypted LastPass vault blob and have a look at it for themselves.  And I'll have more to say about that in a minute.  



So anyway, for those who are interested, Wladimir's blog posting is titled "What Data Does Lastpass Encrypt?"  And it provides additional details beyond what I've just said, and I've linked to it in the show notes.  But on the question of whether any sensitive unencrypted information is contained therein, I guess the question is what is your definition of "sensitive."  And that will vary by user.  It is certainly the case that the LastPass vault does contain unencrypted information.  Presumably this is for some features that they wanted to provide.  It's not exactly clear why they're doing this.



We've got, as you were saying before, Leo, a bunch of smart coders that are listening to this podcast.  You were talking about the Advent of Code and your work on that over the holidays.  I have asked, and I am here asking, for some interested hacker/coders to write a simple utility to deobfuscate all of the non-encrypted data in the LastPass blob so that we can all see what it looks like.  I developed SQRL.  I could obviously do that myself.  I'm going to stay focused on SpinRite because that's where I should be.  And the hacker/coders would rather I finish SpinRite...



LEO:  Yes, we would.



STEVE:  ...than get distracted by this anyway.



LEO:  Well, how is it obscured?



STEVE:  It's just BASE64 and HEX encoded.



LEO:  That's what I thought.  Okay.  So that's pretty straightforward.



STEVE:  So it will be a simple matter to write some code.  It could be Perl.  It could be Python.  It could be whatever.  It would be really cool, except for the privacy concerns, to have a website where you could dump the file in, and it would give it back to you unencrypted.  That way people, you know, it would just be easier for people to use.  But they probably don't want to be handing their LastPass vault, even though it is encrypted, to some random website.  So I think an app that you can run on your local machine that deobfuscates this LastPass vault blob so that you can see the URLs, you can see the email addresses, and the things that look like just HEX junk now, well, those are not encrypted.  They're just obfuscated in order to send them back and forth across the Internet because, as we know, you're unable to send any special characters across a seven-bit channel.



So I already posted this question and a request for such a utility over in the Security Now! newsgroup on GRC.  Everybody listening to this podcast is able to do it.  You know, shoot me a tweet or send a note to Greg at our support address at GRC so that I find out about it.  And I will post the outcome, the results, tell everybody a week from now on the podcast how they're able to deobfuscate their blob.  I think you should go grab it now, though, because LastPass could change this if they decide they don't want everyone grabbing it.  On the other hand, they probably have to obsolete all their clients in order to change it, so it may be staying where it is.  Anyway, I've got mine.  You should probably grab yours.  And then a week from now we'll have some utilities, I'm pretty sure, which will allow us to clarify what's there.



For what it's worth, someone who replied to Wladimir's post obtained and examined their encrypted blob and was not concerned.  On his page where he shows the responses to what he wrote, this person said:  "Thank you for this article.  As a longtime LastPass user, the latest news was unsettling.  I dumped my data following your instructions and took a very careful look.  I found that user ID, passwords, account names, account numbers, and especially the notes" - you know, extra - "were all encrypted."  Now, okay.  This sounds like this person didn't deobfuscate the file, so they may have just been looking for email addresses and usernames and IDs in cleartext, you know, completely legible ASCII.  So they may have the wrong idea. 



Anyway, this person said:  "I did searches on specific sensitive data and literally inspected every page" - and that's a lot of pages - "and found no sensitive data that was in plaintext.  On the other hand, all of the URLs were in hex, which to my mind is not encrypted."  Okay, so they knew that.  They said:  "For me this is a lower priority, as my browsing history is collected by everybody these days, as evidenced by instant ads for something I just started searching for.  I was considering changing password managers, but do not plan to at this point.  It has prompted me, however, to change my master password, which has two-factor authentication protection also, and to review my most sensitive data and ensure it is as secure as possible - strong passwords and two-factor authentication, et cetera.  Thanks again for this article.  It was most useful."  And again, I'm not sure this person actually understands what was lost.  But we are all going to within the next week.



Okay.  So at this point there are about five different directions I want to take this because we have so much ground to cover, a little more than you might guess.  By the end of this podcast I believe you'll have sufficient information to make an informed decision on your own, if you haven't already.  And if you have, you'll have more evidence for that decision.  And it won't be the result of a bunch of inflammatory and exaggerated half truths.



Okay.  So let's examine what LastPass posted, just the meat of it, on December 22nd which has stirred up this hornet's nest.  The main issue is a couple paragraphs.  They said:  "Based on our investigation to date, we've learned that an unknown threat actor accessed a cloud-based storage environment leveraging information obtained from the incident we previously disclosed in August of 2022.  While no customer data was accessed during the initial August 2022 incident, some source code and technical information were stolen from our development environment and used to target another employee, obtaining credentials and keys which were used to access and decrypt some storage volumes within the cloud-based storage service."



They said:  "LastPass production services currently operate from on-premises data centers with cloud-based storage used for various purposes such as storing backups and regional data residency requirements.  The cloud storage service accessed by the threat actor is physically separate from our production environment.  To date, we have determined that once the cloud storage access key and dual storage container decryption keys were obtained, the threat actor copied information from backup that contained basic customer account information and related metadata including company names, end-user names, billing addresses, email addresses, telephone numbers, and the IP addresses from which customers were accessing the LastPass service."



LEO:  This is a little bit more extensive than their initial claim that just some customer information was exfiltrated.



STEVE:  Yes.  And then here's the big one.  The threat actor was also able to copy a backup of customer vault data from the encrypted storage container which is stored in a proprietary binary format that contains both unencrypted data, such as website URLs, as well as fully encrypted sensitive fields such as website usernames and passwords, secure notes, and form-filled data.  Which is exactly what we - all that we've been talking about, this vault blob.  So yes, Leo, exactly as you say, this is, you know, this is what - this on December 22nd is what blew the lid off of this is that LastPass confirmed that unknown actors have acquired their customers' vaults.



LEO:  All of the vaults?  Some of the vaults?  They're not very forthcoming in this.



STEVE:  They're not.



LEO:  In fact, I think they're criminally not forthcoming, to be honest.  Every single customer should have by now received an email saying yours was one of the vaults.  Have you received such an email?  Has anybody?  No.



STEVE:  Well, we all received the email of this.



LEO:  This is generic, though.  Is it mine?  Is it your vault?  Is it his vault?  Whose vaults?  All of them? 



STEVE:  So one presumes, if it was not all, they would have said "some" because that would have been better than not saying anything.  No COO worth his salt is going to say, "Oh, yeah, they got everything.  They got the whole farm.  In fact, we haven't seen our cleaning people."  You know, who knows?



LEO:  Yeah.



STEVE:  Okay.  So anyway, that last...



LEO:  So when they say "a backup of customer vault data," it's of every bit of that data.



STEVE:  Yes, yes, yes.



LEO:  You can assume that.



STEVE:  There's no other way to read that.  If they would have said "a fraction of our customers" or something like that.



LEO:  Some, yeah, yeah, okay, okay.



STEVE:  So LastPass admitted that their offsite cloud backup of their customers' mostly encrypted, but also some not encrypted,  vault data was exfiltrated and is now in the hands of malicious actors.  This is the eventuality that all of this TNO, you know, Trust No One, client-side, encrypted before it leaves your computer, technology was designed to address.  The idea is that, if the user has chosen a strong and unguessable master password, the only thing the bad guys get, or got, was some less critical metadata, including some web browsing and IP usage history.



Okay.  So one thing needs to be made very clear, and I've seen some misunderstanding about this already in Twitter.  While changing and strengthening your LastPass master password now, or changing the iteration counts now, which we'll be getting to in a second, would make any next similar loss by LastPass...



LEO:  Any future hacks.



STEVE:  That's right, less problematical.



LEO:  But that vault data was encrypted with your old master password.



STEVE:  Correct.



LEO:  We don't, by the way, we don't know how old because we don't know the date of the backups.  There are so many missing pieces of information.



STEVE:  We can at least hope they were being conscientious about backing up.



LEO:  But was this the main backup?  Or just some...



STEVE:  We want the bad guys to have the latest and greatest.



LEO:  Got to have the latest.



STEVE:  Like, yeah, that's right, we want fresh backup data.



LEO:  But you understand my question.  You can't assume that it's your current master password.  What if you had monkey123 some years ago.



STEVE:  Right.



LEO:  You don't know.  Was that in the backup?  What vault did they back up?



STEVE:  Good point.  I think that the lawsuits will probably clarify a lot of this information.



LEO:  Yeah.



STEVE:  I think that's where we're going to be finding out.  So anyway, just to finish, the vault blobs, as you said, that were stolen were encrypted under the LastPass master password at the time of that backup.  We don't know what that was, but it was before today.



LEO:  Right, obviously.



STEVE:  Whenever you just changed it back, you know, after this announcement.  So we want to be clear about that.  Okay.  So let's talk about cracking these mostly encrypted vault blobs.  Way back before some clever hacker realized that they could program a GPU to run a cryptographically strong hash function very, very quickly, passwords were protected by hashing them, like, once.  You know, that was plenty since the crypto geniuses who designed these hash functions did so specifically to provide that guarantee, which was that passing a password through a hash once would produce a unique value that could not be reversed to obtain the password that was originally fed into the hash.  Okay, so no problem; right?  Well, not quite right.



The way to defeat this mechanism is to get a super-fast hashing engine, like a GPU, then run every possible password through the hash and see whether what comes out matches what came out when the user originally hashed their password.  So somewhere along the way hackers realized that doing that every time they wanted to crack a password was highly redundant.  And this was around the same time that hard drive mass storage prices were falling.  So the concept of a "Rainbow Table" was born.



The idea was to run all possible passwords through a common hash function that all the websites were using at the time, and do it just one last time, run every possible password through the hash function one last time, but this time store the result along with the password that was input.  Then, when presented with a hashed password that had leaked from some website breach, you just do a lookup in this rainbow table.  You'd look for the same hash, and then that would tell you what password was used to get that hash, and then you'd go run around to the sites that that user uses and log in because everyone was only using one password back then also.



Okay.  So the trouble was, hash functions were designed to be both secure and efficient where efficient means fast.  But fast is the reverse of what we want for passwords, since we want to prevent this sort of brute-force password guessing to obtain a hash.  Okay.  Therefore Password-Based Key Derivation Functions, generically known as PBKDF, and there is a popular one that is actually called PBKDF2, they were created.  There's a handful of popular PBKDFs that have various features and tradeoffs.  But most rely upon iterating some core function, doing something some number of times.



When I was working with SQRL, I wanted something that was GPU-resistant.  And so I used Scrypt, which is a so-called memory-hard function, because there's no way to short-circuit the need to have a chunk of memory that is filled up by the password, creating pointers that the algorithm has to jump around between within this memory.  The reason this is a GPU defeater is that GPUs have lots of computational power, but typically not lots of memory per core.  So that's an example of a PBKDF that is different than iteratively hashing the SHA-256 function a whole bunch of times in order to slow things down.  



But anyway, so that's what these things are.  As I said, most of them rely on iterating some core function.  If performing a single password hash is too fast, and it is nowadays, then you hash the hash, and then hash the hash of that, and then hash it again and so forth.  Okay.  So where I'm headed with this is that back in 2008 when Joe Siegrist founded LastPass, a single hash iteration provided sufficient security in 2008.  GPUs had not yet been developed and deployed for hashing, and computers were much slower than they are today.  You know, the cloud didn't exist where you could just borrow some insane computational resources for a few milliseconds.  But through the years, all of that has changed.



So LastPass began iterating their PBKDF.  The switch from one iteration, the original one iteration to 500 iterations, thus making it 500 times stronger, on the other hand it was 500 times very weak, occurred in June of 2012.  Okay.  So that was four years after LastPass's initial launch.  That only held for about eight months until February of 2013, I guess more like six months.  Anyway, no, about eight, yeah.  February of 2013.  That's when LastPass's default iteration count was jumped by a factor of 10 from 500 to 5,000.  And then finally, five years after that, nearly five years ago in February of 2018, the last jump was made from 5,000 to a heady 100,100, that is, one zero zero one zero zero iterations.



LEO:  And he told us about this when they did it, and told us to up it; right?



STEVE:  Yes.  If this sounds somewhat familiar, right, it's because we talked about this change on the podcast at the time, five years ago.  Okay.  So the strength of the PBKDF2 that LastPass uses has moved forward, and the last update was five years ago, bringing it to a little over - 100 more than a 100,000 iterations.  Okay.  So let's switch now from password hashing iterations, we'll be coming back to that in a second, to the strength of the password itself.



LastPass originally enforced a minimum master password length of eight characters.  Again, 2008, okay.  But that, too, needed upgrading as our systems became faster, and as asking users to invent stronger passwords became more socially acceptable.  Like, you know, monkey123, no, I don't, you know.  Hopefully no one's using that anymore.  And remember how far we've come from the days when a user's single password that they used across all their websites, it's like, "That's my password," was written on a yellow Post-it note stuck to their large CRT screen border.  You know, that's - or maybe under the keyboard, if it happened to slip their mind.  You know, back then passwords were regarded as a sheer annoyance, with no perceived value.  Look how things have changed.



Five years ago, in 2018, LastPass decided that they needed to update their minimum eight-character password length and to recommend using numbers and both uppercase and lowercase letters.  But Wladimir Palant noted something distressing about LastPass's master password strength policy.  He wrote:  "When LastPass introduced their new password complexity requirements in 2018, they failed to enforce them for existing accounts."



LEO:  Mm-hmm.



STEVE:  In other words, LastPass chose to leave their existing customers' possibly too short eight-character passwords alone.  Maybe they didn't want to ruffle feathers.  Or perhaps they were worried about the support requirements of, like, people calling up and saying, "Wait a minute, what?  I have to change my password?"  And presumably, anyone who has changed their LastPass password in the last five years will have been required to strengthen it in the process when setting up its replacement.  



While it's true that LastPass has no idea about the length of their customers' passwords, since all they receive is a hashed blob, it would have been trivial for LastPass to add some logic to the LastPass client code to scold their users when the client notices that the master password being submitted no longer meets contemporary complexity requirements.  Most users would have taken heed of such advice from their password managers.  And again, these days we've all grown accustomed to such requirements.



So assuming that Wladimir is correct, and I have read another anecdotal account from someone who, today, actually it was yesterday, still has an eight-character LastPass master password protecting an unused account, so that suggests this is the case, this means that an unknown number of LastPass user accounts, whose data was recently stolen from the LastPass cloud backup, may have only been protected by an eight-character password.



And unfortunately it gets worse.  There are also reports - and I have, since I tweeted about this a couple hours ago, I have confirmation from multiple of our listeners.  There are reports within the LastPass-watching security community that LastPass's PBKDF2 iteration count, which was also jumped from its too-low setting of 5,000 up to 100 more than 100,000 in 2018, was not pushed out to all LastPass users.



LEO:  Oh, that's very bad.



STEVE:  And I wrote in the show notes yesterday, if this is true, it's truly horrifying.



LEO:  Because that's protecting you against brute force; right?



STEVE:  Yes.  Because taken together it means that there might be LastPass vaults which were allowed to exist for the past five years, since those improvements were made, which are protected by both short eight-character passwords and only hashed 5,000 rather than 100,100 times.  And I started that sentence saying, if this is true, now we know it is.  I have a bunch of people in my Twitter feed who looked after they saw the show notes which I posted about an hour before the podcast, that made them look in their LastPass vault, and their iteration count was still 5,000.



LEO:  So it didn't automatically reset it to a higher number.



STEVE:  No.



LEO:  Now, if you were a listener to this show, you'll remember that we at least had set it to 50,000.  I remember you saying turn it up to 50,000.  That was before the 100,100 [crosstalk] better.



STEVE:  And really the only expense is time.  That is, the time it takes the hash when you enter your credentials, and then your client says, yeah, okay, you know, like that successfully decrypted the vault blob, so you've entered it correctly.



LEO:  It's more important on a phone, for instance, which has a slower CPU than a desktop.



STEVE:  Yes, yes.  But on the other hand...



LEO:  But how much time?  An hour?  No.  A few seconds.



STEVE:  Oh, no, no.  It's a few seconds.  And that's why, you know, I'd say set it to a million.



LEO:  Yes, set it as high as you can; right?



STEVE:  Yeah, because - and the other thing, too, is - and we'll be talking about this in a bit because there was some controversy that Jeremi raised about how bad LastPass was at keeping secrets.  The point is we're also not entering our master password often, unless we've decided we want lots of security, and so it should always be expired.



LEO:  And nowadays on most devices, including your phones, you can use biometrics in lieu of reentering the master password.



STEVE:  Yup.



LEO:  Which I think would bypass that PBKDF2; right?  I don't know what the...



STEVE:  So, no.



LEO:  That would still do the unlocking bit.



STEVE:  No.  So what it would do is you enter your master password once.



LEO:  Yeah.



STEVE:  Go through that length of time.  Then the result is encrypted under biometrics.



LEO:  And stored locally.



STEVE:  And stored locally.



LEO:  Okay.



STEVE:  So then you apply your biometrics.



LEO:  So biometrics would be instant.



STEVE:  And, yes, it ends up being instant.



LEO:  Yeah.  And that's what I do.  Plus somewhat, I would say, marginally more secure; right?



STEVE:  Significantly more secure.



LEO:  Significantly, yeah, okay.



STEVE:  Yeah, yeah, yeah.



LEO:  Provided that your biometric system is effective as it is on Apple.



STEVE:  Yeah, the way all of these PBKDF2s are designed, the time required is a linear function of iteration count.  So a million is a flat 10 times stronger than 100,000.



LEO:  Okay.  So that's not bad.



STEVE:  No, that's good.



LEO:  Worth doing.  All right.



STEVE:  Okay.  So now I want to share some of my feelings about some comments recently made by the well-known password cracker Jeremi Gosney.  And since everyone listening to this knows quite well who I am, I want to introduce Jeremi a bit.  The relevant paragraph of his LinkedIn bio says:  "I am also a core developer of Hashcat, a popular OpenCL-based open source password recovery tool, and I am widely regarded as one of the world's top password crackers.  I was named one of the Top 100 Security Experts in 2013.  I was also one of the winners of Cloudflare's Heartbleed challenge, and was one of the first to publish a working private key recovery exploit for Heartbleed.  My work and research has been featured in hundreds of news articles, and has even been incorporated into university classes and certification courses.  I additionally served as a judge on the Experts Panel for the Password Hashing Competition."



So this guy is clearly a serious developer techie who is well able to understand everything that he sees going on in a password manager.  And Leo, after we take our second break, we're going to see what he thinks about this.



LEO:  I'm just tooting on - because, you know, I don't use that other thing anymore.  Just saying, as I hope Steve discusses the LastPass breach in today's episode, he also makes recommendations for current users.  The episode will be available at TWiT.tv/sn later today.  Or watch us live, and I want to make sure people know that.



STEVE:  There's more good stuff coming.



LEO:  Okay.  We're not done yet at https: - I like typing that "s" - //, thanks to you, TWiT.tv.  And Google, frankly, who made us do it.  Okay, Steve.  Let's move along.



STEVE:  Okay.  So back to Jeremi Gosney.  As I said, he is a recognized password cracker/hacker hashing guru extraordinaire.  So here's how his posting began.  He said:  "Let me start by saying I used to support LastPass.  I recommended it for years and defended it publicly in the media."



LEO:  So did we.  So did we.  So did Steve.



STEVE:  Of course.



LEO:  Right.  It was great.



STEVE:  "If you search Google for 'jeremi gosney + lastpass' you'll find hundreds of articles," he says, "where I've defended and/or pimped LastPass," he said, "(including in Consumer Reports magazine)."  He said:  "I defended it even in the face of vulnerabilities and breaches because it had superior user experience and still seemed like the best option for the masses, despite its glaring flaws.  And it still has a somewhat special place in my heart, being the password manager that actually turned me on to password managers."



LEO:  Yes, yes, yes.



STEVE:  "It set the bar for what I required from a password manager, and for a while it was unrivaled.  But things change, and in recent years I found myself unable to defend LastPass.  I can't recall if there was a particular straw that broke the camel's back, but I do know that I stopped recommending it in 2017 and fully migrated away from it in 2019."  He says then:  "Below is an unordered list of the reasons why I lost faith in LastPass."



Now, I'm not going to drag everyone through all of Jeremi's ranting because I don't think it's rational nor fair to LastPass.  Recall that even though Jeremi is clearly a highly skilled and qualified technologist, it was he who wrote in this rant that nearly everything in your LastPass vault is unencrypted.  And he says that, you know, he said LastPass's claim of zero knowledge is a bald-faced lie.  Well, we already know that the intent, the essence of that is not the case.  There are things that are not encrypted, and LastPass has always been clear about that.  It's made some people uncomfortable, but the bulk of what we have been using LastPass for is protecting our login credentials.



LEO:  I do have to say it's less than ideal.  And at least Bitwarden, our sponsor, and 1Password do encrypt that metadata, as well.



STEVE:  Right.



LEO:  And the only reason it's potentially problematic is because now somebody has a list of all the sites you have a password for; right?



STEVE:  Correct.



LEO:  Yeah.



STEVE:  Correct.  And actually we'll be talking a little bit about the metadata consequences here in a second. 



LEO:  Okay.



STEVE:  So, okay.  So anyway, he talks about zero is not zero.  But he knows what zero knowledge means.  It doesn't mean nothing is there that is unencrypted.



LEO:  There's no such thing as perfection.



STEVE:  And you do need some things unencrypted.



LEO:  Right.



STEVE:  Like the iteration count has to be unencrypted because you need it in order to perform the decryption.



LEO:  Right, yeah.



STEVE:  So, you know.  And it would be interesting to know why there are these metadata things that have been left in the clear.  Is it just legacy?  Is it just because back then once upon a time it didn't matter?  Is it because they are actively harvesting that information and selling it?  I mean, we don't know.



LEO:  Yeah, that's a good question.  Do you know if Joe wrote it that way in the first place?



STEVE:  I reached out to him over LinkedIn.  I don't know if he's even still with LastPass itself.



LEO:  He's not.  I'm going to guess - he hasn't been for years - that he is enjoined not to say anything negative about the company.



STEVE:  Yes.



LEO:  So I would be surprised if he said anything.



STEVE:  Yes, and he did not.



LEO:  Yeah, he sold it outright.  The last time I talked with some people at LastPass, which was about two or three years ago, his niece was still working there, and I was very pleased to meet her.  But she said Uncle Joe had long left, although last...



STEVE:  Was that when we did the...



LEO:  Yeah, at the RSA.



STEVE:  We did the Boston...



LEO:  No, not Boston, it was RSA in San Francisco.



STEVE:  Oh, okay.



LEO:  Yeah.  And which they hosted a party which we attended in 2020, early 2020.  But anyway, they were a little closed-lipped about Joe's continued participation, and it's my sense that he had long left, but they didn't want anybody to know.



STEVE:  Well, in looking back over the timeline, there was the concern when LogMeIn bought LastPass.



LEO:  And that's when it started, I think.



STEVE:  Yes.  And he has a couple LastPass blog postings for like a couple years after that acquisition, and then there's nothing more from him.



LEO:  Yeah, yeah.  That's when he left.  And incidentally, LogMeIn has spun off LastPass as a standalone company.  So they are now independent of LogMeIn.



STEVE:  Right, yeah.  And as I understand it, they're owned by a...



LEO:  Equity capital company, yes.



STEVE:  A hedge fund group.



LEO:  The worst possible owner of anybody.



STEVE:  Yeah, yeah.  Okay.  So the biggest concern is another of those legacy issues which LastPass seems to have been reticent to address.  And this gets back to that ECB CBC.  Recall that a few months back last October, in our podcast 893, we talked about Microsoft's decision to leave their Office 365 message encryption using the Electronic Code Book, the ECB cipher mode.  And remember that to illustrate the danger of that we showed that classic Linux Penguin where using Cipher Block Chaining, the good encryption, it encrypted the penguin's image into a pure high-entropy rectangle of noise.  You could see nothing there, it was just gray; whereas the use of ECB left a very visible penguin in the image.  Okay?



LEO:  Oh, boy.  Yeah.



STEVE:  It appears that LastPass was also originally using the clearly less secure ECB mode to encrypt its passwords.  Then somewhere along the way they realized that this was no longer the right solution.  So they began encrypting any newly saved passwords with the more secure CBC mode.  But for some reason they never proactively reencrypted the original, less secure, ECB passwords under CBC.  Now, okay, that's the information that the security experts who have looked at vaults are claiming.  My own analysis of my vault yesterday showed no evidence of ECB encryption.  You're able to tell because of the format of the string following the p= whether it's CBC or ECB.  Mine were all CBC.  And I'm pretty sure that I've got some old, long-abandoned, like my password for Hamachi probably is in LastPass.



LEO:  Yeah.  I certainly did, yeah.  Why remove it?  Yeah.



STEVE:  And I haven't changed it.  I haven't removed it.  It's still there.  But it was not in ECB.  It was in CBC.  So my experience is that mine got updated.  On the other hand, I don't know if I changed the iteration count or if they did.  But we know that a lot of people did not have theirs changed.



LEO:  See, this, by the way, I blame LastPass at this point for not - they need to put out this information.  What did they do?  When did they do it?  What didn't they do?



STEVE:  Yeah.



LEO:  Because why are we having to guess?



STEVE:  Right.  And using hot-tempered security experts who...



LEO:  And JavaScript pasted in the developer options of their website to figure this out.  We shouldn't have to do this.  They need to come - they've been, I think, less than forthright.



STEVE:  Well, you know, I'm sure everything that gets published goes, passes through their attorneys and...



LEO:  And now that's what's happening, obviously the same.



STEVE:  It's amazing that a period came out the other end.



LEO:  Yeah.  They're doing the minimum disclosure they can legally get away with.  Which is disappointing, but I guess I understand.



STEVE:  Okay.  So if this is true, if it is the case that CBC and ECB password encryption is allowed to coexist, for some people they were not updated.  As a consequence of that, a user's vault will contain a mixture of old passwords, encrypted under ECB, and newer ones under CBC.  I can find no plausible reason for this being left as it has been, and also no verification so far that this is true.  Probably a week from now we'll know because I'm sure that a bunch of our listeners are going to be grabbing their LastPass blob vault and run it through the deobfuscators that we're going to come up with.  And we'll know if they've got ECB encryption.



But, you know, one thing that's interesting is that if all your passwords were encrypted under ECB, what this would mean is that LastPass on their end, looking at your encrypted passwords, even though they can't tell what they are, they can tell when they're duplicated because that's what ECB does.



LEO:  Oh, right.



STEVE:  Every time the same password is encrypted under the same key, you get the same Electronic Code Book output.



LEO:  And we know they can do that because they will warn you you've used this password before.



STEVE:  Well, but that could be done on the client side. 



LEO:  Oh, okay.  Right, okay.



STEVE:  So your client who has the vault decrypted, it sees everything.



LEO:  That's the difference.  Very important to make that distinction between what they can see and what we can see.  Yeah, yeah, yeah.



STEVE:  Right.  But what they could see, and maybe this was something that Joe, you know, again, in 2008 you have to pretty much forgive...



LEO:  Absolutely, yeah.



STEVE:  One hash was all anybody was using.  And we soon learned, well, let's try 500.  Well, 500, we could do that now in a blink.



LEO:  I'm convinced Joe did the best possible at the time and continued to keep it up to date as long as he was there.  I know you were convinced of that when he showed you everything.



STEVE:  That's why we chose it.



LEO:  That's why we chose it.



STEVE:  I mean, absolutely.  He completely showed me what he was doing.  I was able to duplicate and verify the algorithm.  It was all open.



LEO:  Yup.



STEVE:  Okay.  So anyway, I just wanted to mention that maybe the reason ECB was chosen once upon a time was that, again, from LastPass's view, they could have seen where the same password was being used on multiple sites.  They would not have had to rely on the client to tell them about that.  And in fact I don't think the duplicate password feature was there in the beginning.  That was like a great new feature.  It's like, we're going to help you clean up your duplicate passwords and so forth.  Right.



LEO:  Right, right.  Pretty recent.  Yeah, pretty recent, yeah, yeah.  



STEVE:  Okay.  So that brings me to another of Jeremi's points. He writes:  "LastPass has terrible secrets management.  Your vault encryption key is always resident in memory and is never wiped.  And not only that, but the entire vault is decrypted once and stored entirely in memory."



Okay.  And I want to point these things out because people who read his rant got very upset.  But some of this is nonsense, and he knows it.  As we know, I've been saying recently that it would be nice if the LastPass vault were being incrementally decrypted - I've mentioned that every time we've talked about LastPass recently - so that only the one password needed for login was decrypted from the opaque blob, which after its plaintext was used could be overwritten.  But according to Jeremi, that doesn't appear to be the way LastPass manages the user's vault.



And as for the encryption key always being resident in memory, that's a pure requirement of any password manager that is not constantly pestering us to reauthenticate to it.  None of us want to be constantly doing that.  But if you did, LastPass offers the ability to auto-logout after X minutes of inactivity, at which point it would presumably, and I'm sure it does, wipe all decrypted content from RAM and require the user to login again before its next use.  Also remember that when you go to a new site, create a new account, and LastPass says you want me to remember this for you, you click OK.  You don't have to reauthenticate.  Well, that means that LastPass has added that to the vault, encrypted it using the key which it has to be decrypted for it to do that, and sent the update back to LastPass Central.  So again, these things, this terrible secrets management is a requirement for the convenience that we're all getting from every password manager.



So, you know, it's not "terrible secrets management," as Jeremi characterizes it.  It's a necessary tradeoff made for convenience, and every other password manager will need to be similarly terrible in order to get its job done without pestering its user to death.  And we all need to appreciate that none of the password managers are pretending to protect their users from client-side machine attacks.  There is no protection for that, ever, from anyone.  That isn't available.  We're getting the promise that remote websites cannot access our vaults and that our password manager providers, and anyone who might attack them, also cannot access our vaults.  And on that last point it appears that LastPass has made a series of design policy decisions through the years, for reasons only they know, that may have left their users less secure than they could have been in the event of the attacks they have just suffered.



Jeremi also notes that while LastPass's vault key uses AES-256, its 256-bit key is derived from only 128 bits of entropy.  If true, that's also unfortunate.  Although 128 bits of pure entropy is plenty, why not take the opportunity to generate and use all 256 bits for the AES key?  Again, there may be an engineering reason.  But overall it feels as though the original security design of LastPass, which in 2008 was ample, with even Jeremi jumping up and down defending it, has not aged well, and that LastPass's caretakers have not been as excited about keeping LastPass on the cutting edge as crypto enthusiasts like Jeremi or I would have been.



And that's why, after having surveyed all of the available commentary, and thinking about everything I've recently learned, I've decided to pull up stakes and leave LastPass.



LEO:  Okay.  There is an alternative, if you trusted them, but you would have to go through and change all your master passwords.  Not your master, all your site passwords, just in case.  I know some people are doing that, but that could be a lot of work.



STEVE:  Well, the problem is, I mean, and that isn't an alternative because what we're seeing is evidence of them not caring.



LEO:  Yeah.



STEVE:  I mean, that's really what this boils down to is them not caring.  And anyone who's been listening to this podcast for a while knows what not caring means.  Not caring is not neutral.  Not caring is the end of life as you know it from a security standpoint because if there is an opportunity to take advantage of a mistake, that opportunity will be taken.



So Jeremi noted that this most recent breach, he said, was LastPass's seventh in the past 10 years.  I didn't verify that number, but we all know that they've had their share.  And while everyone knows that I'm the first person to forgive a mistake under the theory that they are often unpreventable, this recent breach of their cloud backup provider, which their November disclosure said they shared with their affiliate GoTo, makes one wonder whether some corners may have been cut in the interest of profit.  And only insiders know.



But there really isn't any excuse for the engineering decisions they've made which have made the consequences of their now having lost their customer vaults potentially much more serious.  They could and should have pushed their legacy users to move to a longer and stronger master password.  They didn't.  They could and should have had their LastPass clients upgrade all older ECB password encryption to CBC.  They didn't.  They could and should have absolutely upgraded every user from 5,000 iterations of PDKDF2 to at least 100,100 iterations.  They didn't do that either.  We now have confirmation of that.  They could and should have kept me as a loyal and faithful LastPass user and evangelist.  They didn't.



During my post-incident survey of security professionals, three cloud-based password managers kept being mentioned over and over.  They were Dashlane, 1Password, and Bitwarden.  We all know that Bitwarden is an active sponsor of the TWiT network and a frequent advertiser on this podcast.  So I was glad to see other knowledgeable researchers praising it.  Here's what Jeremi Gosney wrote once he had calmed down a bit from his being jilted by LastPass.



LEO:  I think that's what happened.  He was upset, yeah.



STEVE:  Yeah, clearly.  He said:  "So why do I recommend Bitwarden and 1Password?  It's quite simple," he says.  "I personally know the people who architect 1Password, and I can attest that not only are they extremely competent and very talented, but they also actively engage with the password-cracking community and have a deep, *deep,*" he says again with asterisks, "desire to do everything in the most correct manner possible.  Do they still get some things wrong?  Sure.  But they strive for continuous improvement and sincerely care about security.  Also, their secret key feature ensures that if anyone does obtain a copy of your vault, they simply cannot access it with the master password alone, making it uncrackable."



Next, he says:  "Bitwarden is 100% open source.  I have not done a thorough code review, but I have taken a fairly long glance at the code, and I am mostly pleased with what I've seen.  I'm less thrilled about it being written in a garbage-collected language" - garbage-collected language - "and there are some tradeoffs that are made there."



LEO:  It's C# and .NET.



STEVE:  Right.  "But overall, Bitwarden" - and all you have to do is wipe any plaintext.  You know, I did that in SQRL as like, you know, before you release memory, you zero out the plaintext, and then you're fine.  Anyway, he said:  "But overall, Bitwarden is a solid product.  I prefer Bitwarden's user experience, and I've considered crowdfunding a formal audit of Bitwarden, much in the way the Open Crypto Audit Project raised the funds to properly audit TrueCrypt.  The community would greatly benefit from this."



Okay, now, I know from my Twitter feed that many of my Twitter followers, or at least those who are tweeting to @SGgrc, are using 1Password.  I've not looked closely at it.  But from what Jeremi says, that would appear to be a solid choice.  Another password-cracking enthusiast by the name of Steve Thomas ranks Dashlane first, Bitwarden second, and 1Password third, but only because Dashlane is using his favorite pet password key derivation function known as Argon2.  Argon2 is a memory-hard function designed in 2015 which is highly resistant to GPU attacks.



LEO:  It won the hashing challenge as being the most secure.



STEVE:  Yes.  But its implementations need to be careful about side-channel leaks...



LEO:  Oh, interesting.



STEVE:  ...since the original design accesses memory in a password-dependent sequence.  Thus you're able to infer something about the password from memory access patterns.  As a consequence, improvements have been made in Argon2 since then.  But a password manager's choice of its key derivation function is incidental at most, and the strength of any good function can simply be turned up as needed over time.  So it appears that all three of these are in the running.



Next, I went to check out the personal plans these three offered.  Blessedly, I don't need a family plan or a business plan or anything other than a "Just please keep all of my passwords safe, secure, and synchronized among all of my devices" plan.  Although the value of a password manager is now well proven, so that asking for some money should not be a problem, I like the idea of being able to turn people onto it so that they can take it out for a spin without needing to pay in advance for a year's commitment.  In other words, a useful free tier, such as LastPass once had but then abandoned, you know, that was part of my criteria in the beginning for LastPass.



LEO:  That's the beginning of the end right there, I think.



STEVE:  And that went away, too.



LEO:  Yeah, mm-hmm.



STEVE:  So that's part of my criteria for the perfect password manager.  Unfortunately, Dashlane's free plan only allows for the use of a single device.  So you can't use it on both your desktop and even one mobile platform?  That's crazy.  Why allow any devices if you can't use at least two?  So you need to pay at their minimum plan of $33 per year, in advance, which does then provide for an unlimited number of devices.  1Password doesn't even try to offer a free tier.  If you want to use 1Password you pay $36, in advance, for a year to use it.



Bitwarden of the three is the only one to offer an actually useful free plan which allows for the use of any number of devices.  And Leo, as you often note when you're talking about Bitwarden, since their free plan actually really does everything you'll probably need, their $10 per year paid plan, at less than a third the price of the others, is mostly just there to support them, although you do get two-factor authentication for that $10 per year.



LEO:  Oh, that's worth it.  Yeah.



STEVE:  And I would - yes.  And I would argue that for 10 bucks, come on.  You know, you should see what my latte costs now.  Whoa.



LEO:  Incidentally, I did ask when they started advertising, I said, because we'd been burned by LastPass, frankly, I said tell me about the free plan.  They said, look, we're open source.  It's always going to be free because otherwise people just fork it.  It's free free.  That's not part of our business model is to make money on the free plans.  So that's reassuring.  They're not going to pull a LastPass and say, oh, yeah, now you have to pay for it.  Open source has a lot of benefits in this area, I think.



STEVE:  Well, and as Jeremi noted, the icing on the cake is that Bitwarden is also 100% open source.



LEO:  Yeah.



STEVE:  I saw a tweet pass by some time ago that our old friend Alex Neihaus was moving his family to Bitwarden and choosing to self-host a Bitwarden server in his own cloud.



LEO:  That's another option, yeah.  And actually after this happened I kind of - because for a long time I said, well, I'm never going to be able to protect it as well as professionals at let's say LastPass - whoops - or Bitwarden or 1Password or Dashlane will.  But then there's the counterargument which is it's a single point of attack.  That's where all the vaults are.  So if you self-host, I put it on Dropbox, it's encrypted, they'd have to target me or stumble upon it somehow.  And even then they'd only get the encrypted blob that we've talked about that is well-encrypted.



STEVE:  Yup.



LEO:  So I'm thinking about making mine self-hosted now.



STEVE:  Well, you know, those are the sorts of things that the use of a truly open password manager can provide.



LEO:  Can do it, yeah.



STEVE:  Yes.  And while it's not of interest to everyone, I imagine that it's the sort of thing that would appeal to this podcast's audience.  On December 7th Bitwarden posted a blog titled "New Deployment Option for Self-Hosting Bitwarden."  I've got a link to that in the show notes, bottom of page 10 of the show notes, for anyone who's interested in taking the path that Alex took.  And so, you know, that's something that can be done. 



And in my digging around over the holidays I stumbled upon a Bitwarden page which linked to their past annual third-party outside network security and application security penetration testing audits.  So they're auditing themselves, having themselves audited by a number of different outside firms every year.  I ended up not being able to find that same page later, but I found another that provides the same info, and it's under bitwarden.com/help/is-bitwarden-audited.  And so there are links there to everything.  And I also noted at the bottom of that page that they're signed up with HackerOne to offer and manage bug bounties against their platform.



So the reason I originally chose LastPass and was comfortable endorsing it was that its author opened up its internals to me so that I could understand exactly how it worked.  And it was solely on that basis and for that reason that I chose it.  I don't regret the decision I made back then.  LastPass has been a flawless companion for me over the years.  I say that knocking on wood somewhere because my vault has now been absconded with.  But as we've observed earlier on this podcast, the world has been changing ever since, and LastPass no longer fits the way it once did.  And, you know, it and its organization is beginning to act and feel a bit too old and creaky, which is not what anyone wants in their password manager.



An example of a different more aware and contemporary approach is Bitwarden's description of their management of their user's master password.  Bitwarden wrote:  "SHA-256 is used to derive the encryption key from your master password.  Bitwarden salts and hashes your master password with your email address locally before transmission to our servers.  Once a Bitwarden server receives the hashed password, it is salted again with a cryptographically secure random value, hashed again, and stored in our database.  The default iteration count used with PBKDF2 is 100,001 iterations on the client - client-side iteration count is configurable from your account settings - and then an additional 100,000 iterations when stored on our servers, for a total of 200,001 iterations by default."



LEO:  Yeah.  Okay.  Can I interrupt with a stupid question?  What's with the 100,001, or the 100,100?  They don't like round numbers?  They only want them round-ish?



STEVE:  That is a great question.  There's nothing...



LEO:  Okay, there's no technical...



STEVE:  No.



LEO:  It's just a random...



STEVE:  It's not like it's a power of two or something where it's going to be some magic.  It's bizarre.



LEO:  I think it's prime.  Oh, well, yeah.



STEVE:  And that wouldn't matter, either.  Anyway...



LEO:  No, I know.



STEVE:  So Bitwarden is not only hashing 100,000 times on our system, and you can turn that to anything you want - and by the way, I would go with like 350,000.  That's pretty much...



LEO:  Turn it up, yeah.  I'm going to turn mine up.



STEVE:  That's pretty much where you want to be at this point.



LEO:  Okay.



STEVE:  And then they're getting it, resalting it, and then running it another 100,000 times on their high-speed machine because they can; just, you know, because.  So anyway, everybody knows what happened then.  I went to bitwarden.com/twit.



LEO:  Yay.  Thank you.  By the way, this is not a paid ad in any respect.  This is Steve - all right.



STEVE:  No.  Everybody who's listening to this knows that I chose them because they're open source, they have a useful free tier, and they're as good as anybody else.



LEO:  Yeah.  Are you going to self-host?  You're certainly capable of doing that.



STEVE:  It looks like you actually have to run a Bitwarden server.  You can't just have a Bitwarden file.



LEO:  Oh.  There is a third-party server that is widely agreed to be much faster.  I think it's written in Go.  So you could use - you don't have to use - it's open source.  Somebody else has written a server that's better than Bitwarden's.



STEVE:  Nice.



LEO:  And most of the people who self-host that I know use that software.  I'll find it for you.  But, you know, I don't self-host.



STEVE:  I'm not going to bother.



LEO:  No.



STEVE:  No.  I mean, again, the reason we did this in the beginning was that we had really strong local client-side encryption, and we were just using the provider to hold onto our data for us.



LEO:  Right.



STEVE:  Unfortunately, LastPass has just demonstrated that we can no longer trust them to do that.



LEO:  It's literally the worst thing that could happen to a password, except for not encrypting it right.  But it's [crosstalk] good.



STEVE:  Well, [crosstalk].



LEO:  It's as bad as you can get.



STEVE:  Yes.  Well, the reason we were all using the cloud is if your local encryption is solid, it doesn't matter if they lose control.



LEO:  Right.  Right.



STEVE:  Unfortunately, they both lost control and their local encryption was not being kept up to date.



LEO:  Yeah.



STEVE:  Okay.  So I signed up for Bitwarden, started the $10 plan.  Then I went to a menu item I had never used in my LastPass vault, "Export," and I exported a 77 KB CVS file.  I opened my shiny new Bitwarden web interface, and under "Tools" at the top was a menu item, "Import Data."  From a dropdown menu there, I selected the import source as being "LastPass CSV," provided the filename of the file I'd exported from LastPass, and watched a perfect error-free transfer of my entire legacy LastPass data into Bitwarden.  My password database, auto-fill credit cards, and all my secure notes made the move without incident or complaint.



LEO:  I have done this, as well, several years ago, but I did it.  The only thing I would say is immediately securely erase the CSV file because it's in plaintext; right?



STEVE:  No, no, no.



LEO:  It's not?



STEVE:  It is an encrypted vault.



LEO:  It's still encrypted.  Okay.



STEVE:  Oh, wait, no, no.



LEO:  It has to be decrypted, yeah, yeah.



STEVE:  Yes, yes, in order to import it, of course.



LEO:  Treat that carefully.



STEVE:  Yes.  I was briefly confused with the XML that we grabbed from LastPass.



LEO:  That's different.  Yeah, yeah.  No, but this - you have to unencrypt it so it can be reencrypted [crosstalk].



STEVE:  Yeah.  And in fact I remember being very nervous about the fact that that file was on my system.  It's like...



LEO:  Yeah, delete it securely.  The other thing, when I did the import, I had some very long and weird notes in LastPass, and a few of the very long note fields got munged on the import.  Or maybe on the export.  I'm not sure at what end.  But you might check, if you use secure notes, to make sure everything got in.  I remember that I had some trouble with that.  In fact, actually I think the import didn't work because Bitwarden said, well, I don't know, but this is too long, I can't import that.



STEVE:  Yeah.  And I was wary of that, but I had no complaints or issues at all.



LEO:  No problems, okay.



STEVE:  Simon Zerafa, who is a frequent contributor to the podcast, he said that the Android client put him off a little bit.  But you certainly are an Android user.



LEO:  Oh, no, no, it's fine.



STEVE:  Okay.



LEO:  Look, I mean, I guess we should also say, and we talked about Tavis Ormandy saying this, that the JavaScript stuff like the plugins for the browsers, perhaps the mobile clients for all these password managers, these are a little bit potentially problematic, especially if code got injected, malicious code got injected and so forth; right?



STEVE:  Well, again, there is no guarantee being made for any of this over on the client side.



LEO:  Right.



STEVE:  That isn't something we have any control over.  If something evil gets in your machine, you know, it can be logging your keystrokes.



LEO:  That's a good point.



STEVE:  It could be watching you type your master password.



LEO:  Yeah, all bets are off.  But some might say, Tavis did, that it's better to use the binary as opposed to the JavaScript plugin on the browser.  And LastPass, Bitwarden, Dashlane, and 1Password all offer a standalone app.  I think in some cases it's Electron, so maybe that's not great.  The other thing I would say is if you are in the minority, and you only use Apple products, Apple has a very good, I think, password manager that will do all the things a password manager will do, and it's all kept within the Apple ecosystem.  It's a little hard - you can use it on Windows and Android, but you have to have an Apple app to do that.  Probably should be added to the list of potential candidates for people who are mostly Apple focused.



STEVE:  I was going to say, yes, if you were a Mac user, then it would make certainly sense to stay there.



LEO:  Yeah.



STEVE:  Of course I'm not.  I just [crosstalk].



LEO:  I use them.  It's nice.  It has a very nice generator.  It fills it in.  But I still, I need Bitwarden because I use Linux, you know, I use other stuff.  And I need a cross-platform one.  And I, you know, as much as I love Bitwarden, and they are a sponsor, and it's what I use personally, I think you'd be fine with Dashlane or 1Password, as well.  I see no reason not to [crosstalk].



STEVE:  Yes.  As I said at the beginning, or when we were talking about this, I see lots of tweets from our listeners who are 1Password users.



LEO:  Very happy.



STEVE:  You know, even Jeremi, when he's not frothing, thinks that 1Password is a good thing, too.



LEO:  Right.



STEVE:  So we have one last thing to talk about, which is the remediation of any danger which may arise from any prior use of LastPass.  When I was deciding upon the title for today's podcast I was tempted to title it "Don't Panic," with a nod to Douglas Adams, because I doubt that anyone should panic.  There are some caveats, though.  As I noted earlier, you should definitely check your LastPass iterations to assure that it had been bumped up to the 100,000 level.  And in my show notes here I said please shoot me a note if you discover that it's not 100,100.  And that happened immediately.  We got a lot of 5,000s people out there.  So that's a concern.



But assuming that you have 100,000-plus iterations, and that you're using a master password with good entropy, other than some incidental personal information disclosure of the sort that, you know, as was said, commercial entities that you work with on the Internet probably already have about you, and like all the advertising companies out there that are tracking us, your actual risk of having your own vault decrypted should be low.



If you were using a very high-entropy password with 50, five zero, bits of entropy, a single GPU attempting to crack the LastPass default, assuming that it is 100,000-plus iterations, of an SHA-256-based PBKDF2 would require 200 years and an estimated cost of $1.5 million.  Okay.  Now, 50 bits is a lot of entropy.  Studies have shown that the average password only contains somewhere around 40 bits of entropy.  Since that's 10 fewer bits, the strength is 2^10 weaker.  2^10 of course is 1,024, so roughly a thousand.  So a 40-bit password would be about 1,000 times less strong.  So that brings us down to a couple of months for a GPU to crack a single user's vault at a cost of about $1,500.  And this of course reminds us why the strength and the length of our password is so crucial for avoiding brute force attacks.  Everybody remember Password Haystacks?  It taught us the lesson of how easy it was to create long and highly brute-force-resistant passwords.



The second factor mitigating our risk, if you'll pardon my pun, is the presence of second-factor authentication.  Everyone who's listening to this podcast almost certainly has a time-based authenticator and has added the requirement for its use to their most important online accounts.  For iOS I still prefer OTP Auth which uses iCloud for its synchronization among my iOS clients.  As I scroll through all of the accounts I have registered in OTP Auth, it's comforting to see that I have that additional layer of protection beyond what was contained in my LastPass vault.



But this leaves the theoretical risk that any crucial high-value credentials that might have been saved in that vault which are not protected by a second factor could be at risk.  Since LastPass was not encrypting our email addresses and website URLs, there was definite leakage of who we are and where we go and what we do, without any need to decrypt anything.  This opens LastPass's users to the potential for potent phishing attacks by leveraging what might be learned from an examination of the unencrypted data contained in their vault.  So everyone needs to be on heightened alert, unfortunately forever because this data is never going to go away, for convincing-looking online scams sent to your email addresses and referring to websites you use, which I assume we're all going to be seeing once our LastPass vaults have been deobfuscated by the little utility programs that hopefully our listeners are in the process of writing right now.



The final takeaway is that, if you're concerned that your LastPass master password was not high-quality, high-entropy, and may not have had a high iteration count at the time of its theft from LastPass, the risk of brute forcing might be higher for you.  So it might be worthwhile for you to take the time to scan through your vault, after importing it into your next password manager, and manually changing the login passwords of any of your important accounts which are not also protected by some form of strong second-factor authentication.



And finally, the last page of today's show notes contains a link appendix for anyone who's interested in reading the original source material that I found and shared above.  I've got Jeremi Gosney's rant link, Wladimir Palant's four recent LastPass blog posts, and also Steve Thomas's identity over at Twitter and at Mastodon, as well as his website.  So anyway, it was a good run.  But it was time to say goodbye.  And I sincerely hope that none of our listeners were caught by this breach.  I know that some of our listeners did have 5,000 iterations.  We can hope that they had a really good brute force.  And remember, you know, we're talking tens of millions of customers in this backup blob.  The danger is much greater for people who are likely to be targeted than our aunts and uncles who we talked into using LastPass 10 years ago.



LEO:  Right.  And honestly, I would even say, if it was hard to get them to use LastPass, don't feel obligated to move them off of it.  Although, yeah, I mean, it would be nice.  Incidentally, Steve, are you going to go through all your passwords and change them?  Because...



STEVE:  I'm going to look at them.



LEO:  You took them all over with you.  And those are still in that blob.



STEVE:  Correct.



LEO:  And presumably if somebody was - I'm sure you had a very long, strong password, and you used two-factor.  Does two-factor help in the case of brute force?  I guess it does; right?



STEVE:  Now, that's a really good question.  It's the way LastPass implements their so-called, you know, no password deal is that they locally encrypt the decrypted password, the decrypted master password.  And then that gets sent to them.  When you then use their two-factor authentication for their password-free mode that they came out with a few months ago, they verify that the second factor is correct.  Then they send you the encrypted password for you to decrypt locally in order to get access to your vault.  So I don't - the way I read that, it doesn't get factored into your master password used to encrypt your vault.



LEO:  Wow.



STEVE:  So the second factor there is not a benefit.



LEO:  Wow.  So better hope you had a good random, well, it probably isn't random because you had to memorize it, but a good, long, un-brute-forcible master password.



STEVE:  Yes.



LEO:  And given that these guys have had, potentially have had this for some time, and will have it forever more...



STEVE:  Ever.  And we know that cracking technology is only ever going to get better.



LEO:  Oh, boy.



STEVE:  It's not going to get worse.



LEO:  This is so bad.  So bad.  It probably would be prudent, honestly, to go through and change at least the passwords you care the most about, like your bank, credit cards, stores.  Those are things they're going to go after.



STEVE:  And really, I mean, it's not that big a hassle; right?  You log onto the site.  And that's easy.  And then you go to the Change My Password, and you put in the old one, and you make up a new one.  I do think, unfortunately, it's probably, I mean, in order to rest easy; right?



LEO:  Right.  The other thing you could try is set up your new password manager without importing any passwords.  And then when you go to a site you know you've got a password, this forces you to say, I forgot, and reset it.  And you will gradually fix your passwords by starting fresh with a password manager.  That sounds like fun.  But it really, honestly, this is why this is so serious.  That's kind of the best and only thing you can do if you have to - if you really want to be safe.  Hopefully your parents didn't listen to your advice, they didn't get LastPass, they just got the little book at Kmart that said Passwords, and they wrote them all down, one by one, carefully.



STEVE:  And it's got the rubber band holding it closed.



LEO:  Yeah, yeah.  And they keep it in the lower left drawer where no one will look.  Because they'd probably be safer if they had.  Wow.  I apologize.  When we started doing ads for LastPass, they were a quality product.  We were very proud to be associated with them.  They saved us in the year 2020.  I don't know if TWiT would still be here if they hadn't bought studio naming rights.  I apologize to all of you who started using LastPass because of us.  But we all got bit.  And there's the answer.  There's the show you've been waiting for.  I've been waiting for.



STEVE:  Well, at every point we were being clear that the quality of the master password is paramount.



LEO:  Yeah.



STEVE:  And that was the idea; right?  You only have one that you need to, like, be in charge of, be responsible for.  Make it a good one.



LEO:  Right.



STEVE:  And Password Haystacks teaches us the lesson that you can just put a bunch, like put 20 periods after it, and that makes it way stronger, way more impossible to brute force.  And it's, you know, it's not a difficult thing to do.



LEO:  Well, in fact, that is the advice somebody gave.  If you are writing your passwords down and putting them in a little book, is that's not the password by itself.  Add a 10-digit number that you know well.  Always, you know, write it down.  And that will not only lengthen the password appropriately, it will also make it useless if the bad guys find the book.  Wow.  Thank you very, very, very much, Steve, for doing this.  I think I tooted on Mastodon, dang, the one week we don't have a Security Now!, and we have a story that we really want to know what Steve thinks.  But now you know.  Thank you, Steve.  Do get the show notes for all those links.  Read more.



GRC.com is his website.  You'll also find two unique versions of the show there.  He's got 16Kb audio for people who really have limited bandwidth, and transcripts written by one of the people who has limited bandwidth.  That's why the 16Kb show audio exists, so Elaine Farris could download it at her limited capability ranch.  She is a - what do they - a collier as well as a court reporter.  She does horse shoeing.  So she is transcribing this.  I believe it's collier.  I hope I'm right, Elaine.  If I'm not, can you replace it with the right word?  Thank you.  [Note:  Elaine is not a court reporter, not a collier, not a farrier, and has sufficient bandwidth for her full-time occupation as transcriptionist.]



Those transcripts are available at GRC.com.  While you're there, pick up SpinRite, the world's best mass storage maintenance and recovery utility, getting better with 6.1, which will be out soon.  And if you buy 6.0 now, you'll get 6.1 for free when it comes out.  You also can participate in the beta period.  I'm sure there will be something like that.



After the show you can go there and leave comments for Steve at GRC.com/feedback.  He's still on Twitter.  He's going to be - please turn out the lights when you leave, @SGgrc.  Actually, don't worry because I think the power company will turn them off, sooner than later.  @SGgrc on Twitter.  His DMs are open.  You can leave comments there, as well.  We have the show at our website, TWiT.tv/sn.  That's where you can download shows.  You can also go to the YouTube channel.  There's a dedicated channel.  You can find that and all our dedicated channels at YouTube.com/twit.  YouTube.com/twit.  Subscribing is probably the best way to get it.



Copyright (c) 2023 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#905

DATE:		January 10, 2023

TITLE:		1

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-905.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week, in a necessary follow-up to last week's "Leaving LastPass" episode, we'll share the news of the creation of a terrific PowerShell script, complete with a friendly user interface, which quickly deobfuscates any LastPass user's XML format vault data.  What it reveals is what we expected, but seeing is believing.  Then we're going to examine the conclusions drawn and consequences of the massive amount of avid (and in some cases rabid) listener feedback received since last week, and some of the truly startling things that listeners of this podcast discovered when they went looking.



SHOW TEASE:  It's time for Security Now!, the shortest name in the history of Security Now!.  Steve Gibson will explain what "1" means and give more very important advice.  In fact, honestly the story, the LastPass story gets worse and worse.  If you were a LastPass customer, or still are, you've got to listen to this episode.  Security Now! is next.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 905, recorded Tuesday, January 10th, 2023:  1.



It's time for Security Now!.  Yes, it is.  You've been waiting all week, I know you have.  Security Now! maven, legend, man about town, Steve Gibson is here to talk about the latest security news.  Holy cow, Steve.  Last week's Leaving LastPass episode was a biggie.



STEVE GIBSON:  Oh.  Yeah.  It absolutely broke all of the records that we've had for, I mean, just judging, you know, my immediate feedback is to look at the number of likes and retweets on my weekly announcement of the podcast.  And last week's was about eight times more than we've ever had before.  And actually as a consequence of that and the nature of the feedback that I received through this next week, we're going to continue because what happened was interesting.  We have to have a follow-up to last week's Leaving LastPass episode.



And I want to share the news of the creation of a terrific PowerShell script, complete with a friendly user interface, which quickly deobfuscates any LastPass user's XML format vault data.  What it reveals is what we expected, but seeing is believing.  And it's a little startling to see what's there with no decryption of the vault needed.  Then we're going to examine, as I said, essentially the conclusions drawn and the consequences of the massive amount of avid, and in some cases rabid, listener feedback received since last week.  There were some truly startling things that listeners of this podcast discovered when they went looking.  So we've got a great Picture of the Week.  And this episode's title is unique for us.  It's just a single digit, "1."



LEO:  I thought I made a mistake.  I thought	there was an error, typing the title.  It's just "1."  Number one.



STEVE:  Security Now! Episode 905 for January 10th, 2023, titled "1."



LEO:  Okay.  Okay.  



STEVE:  And everyone will find out why in a little bit.



LEO:  There was a Broadway show called "Nine."  And now there's an episode of Security Now! called "1."  No one knows why, but you will find out in a moment, shall we say.  And I'm very curious to hear about what's going on with LastPass, too.



STEVE:  You don't know how curious you need to be.



LEO:  Oh, geez.



STEVE:  There's something really - something to...



LEO:  You asked me if I could try this XML formatter on my vault data.  And I deleted my vault more than a year ago, and I hope to god so did LastPass, but that's a question for later, too.  I think the calendar, the clock on the wall, the atomic clock says it's time for our Picture of the Week, Steve.



STEVE:  So we've actually shown this one before, but I ran across it again, and I thought, this is just so good.  Sometimes things are just so clever.



LEO:  And this is not a Photoshop.  I own both these books.



STEVE:  Yup.  I do, too, yup.  So on the left we have O'Reilly's official bible, "JavaScript:  The Definitive Guide."  And it's got a big rhino on the cover.  And it looks like maybe it's about three inches tall.  I mean, this is like...



LEO:  You could use it as a doorstop.



STEVE:  Yes.  You could use it to keep your car from rolling down a hill.



LEO:  Yes.  Yes, you could.



STEVE:  Okay.  And on the right we have, also from O'Reilly, written by Douglas Crockford, who's a renowned JavaScript person, this one's got a butterfly on the cover.  And it says "JavaScript:  The Good Parts."



LEO:  There's not many is the takeaway on that.



STEVE:  It's about maybe a quarter inch thick.  So just, you know, just so much smaller than the definitive guide.  Anyway, I just, that's just, as a visual joke, that's just...



LEO:  It's a great one.  And it's true.  I mean, those are the books.



STEVE:  Yeah.  Okay.  So LastPass aftermath.  And for those of you who are no longer using LastPass, who moved away from it a year ago, blah blah blah, you know, you may think that some of this doesn't interest you, but we've got, as I said, LastPass aftermath.  And there actually will be some math later in this podcast, but there'll be no test on it.  So you don't have to worry about taking notes.  But there will be something for everyone here.



Okay.  At the top of the news for our listeners this week is that my call for the creation of a LastPass vault deobfuscator, you know, it's not a decryptor because we can't decrypt the vault easily.  But as we know, a lot of the non-encrypted information was obfuscated.  It was just converted into hex, which you should do when you're moving stuff across the Internet, which many of the areas of the Internet still don't support - or maybe that's not true anymore, but everyone still does it - 8-bit bytes, so like 7-bit characters is the reason that weird things have to be converted.  And also, if you're using XML, where for example you've got angle-bracketed starting and ending formatting stuff, well, if the content of what you're sending had an angle bracket in it, that would foul up the XML parser.  So I asked our listeners, I said, I'm sure there's a bunch of coders out here.  Let's see if somebody by this time next week, which is today, can come up with something.



LEO:  Wow.  They responded, too.



STEVE:  Sure enough.  I got a whole bunch of people did a bunch of different things, and several listeners produced JavaScript solutions.  Paul Holder, who I know you know from...



LEO:  Love Paul.  He's a Java guy, not JavaScript.  He's a Java guy.



STEVE:  Yes, exactly.  And in fact he wrote a portable solution in Java.  Several others wrote solutions in C, C++, and C#.  And I was sure that would be the case.



LEO:  We've got some real coders out there.  That's great.



STEVE:  Yeah.



LEO:  That's great.



STEVE:  Okay.  But the solution which surprised me and really captured my attention was both the smallest of all, by far, and the most powerful.  It was implemented as a Windows PowerShell script.



LEO:  Oh, interesting, yeah.



STEVE:  And having seen now what PowerShell's scripting language can do with its full access to the .NET language, it's clear to me that I'm going to need to make some time, I don't know, someday, to take a much closer look at it.  It's sort of amazing what has quietly been happening over there.  I've got a picture of this little PowerShell script which I guess you would call an "app," even though it's a script, you know, I mean, Perl is technically a scripting language, but you make apps with it.  So we have a solution that I'll explain in detail in a moment.  And it is this week's Security Now! podcast 905 shortcut.  So everyone can get the PowerShell script by going to grc.sc/905.  And when you hit ENTER, that will present you with a ZIP file.  Well, anyway, I'll explain all that in a second.



So the way we got here is almost as interesting as what we got.  And I confess that it would have never occurred to me.  It began with a Twitter DM from a listener named Rob Woodruff.  Rob tweeted to me.  He said:  "All right, Steve.  You asked, and I delivered.  I wrote a PowerShell script to parse the XML file that is your LastPass vault, identify any values encrypted with ECB rather than CBC, and decode the URLs from hex to ASCII.  I chose PowerShell so that it will run on any modern Windows computer.  It's not fancy, but it appears to work."



Now, he's not actually talking about this at the moment.  "Not fancy, but it appears to work.  You'll need to specify the InFile, OutFile, and Format parameters on the command line" - none of that's still true - "or it will prompt you for them.  InFile is the path and filename of the XML file, your LastPass vault.  OutFile is the path and filename of the output file."  He says:  "Format is the format of the output file, either CSV (Comma Separated Values) or HTML."  Anyway, and then he sent me a link, "You can download it here," and he sent me a Dropbox link.



Okay.  So that was Rob's first of what ended up being many messages between the two of us.  And that's, as I said, far from where we ended up.  About four hours later, this was on Friday evening, he followed up.  He said:  "This version of the script has a GUI."  And then he sent me a Dropbox link.  You know, so a Graphical User Interface.  Upon seeing this, as I said, I thought, okay, I'm going to have to pay more attention to PowerShell.  So I replied:  "Holy crap, Rob.  You're a PowerShell wizard.  I'll check this out tomorrow.  Thank you so much."



To which Rob replied:  "My pleasure, Steve.  I really enjoyed the process.  First time doing a GUI in PowerShell.  Not sure I can claim the title of Wizard, though.  I had ChatGPT do most of the heavy lifting."  And he said:  "Speaking of which, if you haven't played around with it..."



LEO:  Wow.  Maybe ChatGPT is a PowerShell wizard.



STEVE:  He said:  "Speaking of which, if you haven't played around with it," meaning ChatGPT, "you must.  It will blow your mind."  Okay, now, I mean, mind blown.  I said, okay, being unsure whether Rob might be pulling my leg, I wrote back.  I said:  "Rob, are you not kidding?  Did ChatGPT really have a hand in that?"  I mean, this was like a functioning app with graphical user interface.  And he said:  "Steve, I'm not kidding.  ChatGPT is supposedly fluent in every written language, including programming languages.  I told it in English what I wanted to do, and it spat out PowerShell code."



LEO:  Well, more than that, it's apparently familiar with the LastPass XML format.



STEVE:  He said:  "It's not perfect, of course, and I spent a lot of time debugging the hex to ASCII conversion.  Ultimately, I ended up using a code snippet for the conversion that I found using Google because ChatGPT couldn't seem to figure it out.  Similarly, when I asked ChatGPT to add a GUI, it got most of it right on the first try, but it had two of the buttons being overlapped by text fields."  Well, boohoo, you know, fine, thank you.



LEO:  Wait a minute.  ChatGPT does GUIs, too?  Wow.



STEVE:  Yes, yes.



LEO:  PowerShell, no less.



STEVE:  Yes, yes.  This thing is 12K, Leo.  That thing that you saw is a 12K script.  Okay.  So he said:  "So I had to adjust the positioning of the elements manually."  He said:  "Overall, though, it saved me a lot of time.  I probably wouldn't have even tried to tackle this project without ChatGPT."



Okay.  So we've clearly entered a very different world.  What Rob explained is consistent with everything we've heard about ChatGPT.  It's not yet perfect, but it's very good, and it can typically get you 95% of the way there.  And then, yeah, you need to go in and fix the things it got wrong.  But wow.  So anyway, I thought that that, you know, that was just very cool.



I then worked with Rob through Friday evening and through the weekend to polish and perfect this little 12K gem to ready it for today's podcast.  Remember that, being a script, it's inherently open source and therefore readily verifiable.  And remember from last week that the key to obtaining the LastPass vault is using the developer features of any browser that's currently logged into LastPass.  Though this part is still a bit inconvenient, there's no way around that without a huge amount of work to recreate everything that a browser does.  And actually, as I was putting this together, putting the show notes together last night I thought, huh, maybe we'll just ask ChatGPT to write us a browser.



LEO:  Or SpinRite.  Maybe - no.



STEVE:  Actually, somebody did.  I received a DM from someone who said:  "How would you write SpinRite?"  And it gave a complete description of how to do that.



LEO:  Wow.  Not assembly code, just the description.



STEVE:  No, it didn't do it.  It didn't write any code.  But it explained...



LEO:  It understood what SpinRite does, which is interesting, yeah.



STEVE:  Yes, yes.  I'm sure it went out there and, like, found out what SpinRite was and said, okay, here's how you'd write it.



LEO:  We are in an interesting time, I have to say.



STEVE:  Oh, it is, it is really, yeah.



LEO:  We're right on the cusp of something.  Not sure how good it's going to be.



STEVE:  Okay.  So, I know.  Well, it's going to change the world; right?



LEO:  Yeah.



STEVE:  I mean, kids are going to grow up in a different world than we did.  And of course old fogies are all like, when I was a kid you had to actually use a pencil and sharpen it.  But, you know.



LEO:  Yeah, I say that all the time.  I know how that is, yeah.



STEVE:  So it's necessary to still involve the developer console of the browser.  Okay.  So just to - I want to give people the instructions they need, especially those not familiar with running a PowerShell script.  So first grab the PowerShell script that Rob wrote from my server.  I'm hosting it, although he also has it on GitHub.  And there's an update button on his GUI that'll take you over to his page on GitHub, in case it goes through some revisions.  But as I said, grc.sc/905.  That will return a tiny ZIP file containing one file titled "Analyze-LastPassVault.ps1."  That's the PowerShell script.



And as we've learned on this podcast, encapsulating the script in a ZIP prevents Windows from tagging the script inside the ZIP with the dreaded Mark of the Web.  So you don't need to see the warning about the dangers of using something that's been downloaded from the Internet.  As a very useful security measure, Windows will no longer run unsigned PowerShell scripts.  And before this all began, I didn't even know you could sign a PowerShell script.  But turns out you can, and I did.  So since I didn't want anyone to be put off by that, I mean, you know, it gives you all kinds of scary warnings and things, and you can say, yeah, run it anyway.  And since signing also does a useful verification that nothing has been altered, I signed Rob's final script with GRC's EV code signing cert.  So Windows will see that it's been signed, and it will run it all without complaint.



So after downloading it, launch a PowerShell prompt from the Windows menu, or you can just type "PowerShell" into the search box, and it'll get you there.  This does not need to be run with elevated admin privileges, so anyone should be able to do this.  Start the script by entering - and this is odd and reminiscent of Linux.  You need to say .\Analyze-LastPassVault.ps1 in order to get it to go.



LEO:  That's just saying execute it from this directory.



STEVE:  Exactly, from the current directory.



LEO:  It won't do it otherwise.  It's not in the path.



STEVE:  Don't go looking around for it.



LEO:  Yeah, it's not in the path.



STEVE:  And so press ENTER.  The Analyze LastPass Vault app will be displayed on your desktop.  You'll find complete instructions there in the app itself, right there in the UI, for proceeding; so you can likely race off on your own without the rest of this.  But I can provide a little bit of additional background for clarification.  With Rob's app running, switch to the browser and log into LastPass so that you're looking at your vault, like, you know, many people did last week when we first talked about the way to grab your vault.  Press the F12 key or CTRL+SHIFT+I, which toggles the developer mode.  And so that will suddenly subdivide your screen vertically into the browser on the left and the developer stuff over on the right.  Then select the Console tab in the developer window.  And things may be scrolling like crazy.  Wait for them to settle down.  That'll eventually stop.  Once you have a cursor flashing there at the bottom, you're ready.



To make this as easy as possible, we built in the three lines of JavaScript code that needs to be dropped into the browser at this point.  So you just press Copy Query button in Rob's app, which places those three lines onto the system's clipboard.  So then bring the browser to the foreground so that it has the system focus, and hit CTRL+V to paste those three lines into the browser. And you'll see them appear there after the cursor.  Then press ENTER to execute the query, and the page will fill with XML expressions.  That's your encrypted LastPass vault.  As I described last week, down at the very bottom of the screen will be the options to display more of the text that was cut off because it's way longer than will fit on the screen, also to copy the entire query results to the system's clipboard.  That's what you want.  So click on Copy.



Now, switch back to Rob's app and click the Paste button.  That will paste the captured clipboard directly into Rob's app without needing to go through the intermediate step of saving it to a file.  And note that if last week you had already copied your XML-format vault out of the browser and may have terminated your account with LastPass, you can also provide Rob's app with that filename to load and then process.



Okay.  So with the vault made available to the app, either by pasting it or opening a file, specify an output filename to receive the deobfuscated data and choose the output format, either CSV or HTML.  The CSV format will ideally require something in your system for viewing it, like Excel.  I like the HTML format since your browser, and we all have a browser, will happily display that.  So set the filename, ending in either CSV or HTML as needed, and finish by clicking "Analyze."  The file will be written, and then the resulting file will be launched for display by whatever handler that your system has registered for handling that type of file.  And as I said, if you exported HTML, the results will be displayed in a nicely formatted scrollable window with one line per login record.  Actually, same thing for CSV.



So Rob's app displays an "OK" on the far left if the encrypted account information was encrypted using the better CBC cipher mode.  And we'll review that again a little bit in a second.  If the record's encrypted information was encrypted with the suboptimal information-leaking ECB cipher mode, you'll see the message "WARNING: Encrypted with ECB."  Next on the line, and likely most interesting to everyone, will be the deobfuscated URL that's associated with the website's encrypted logon record.  Taken as a whole, these are all the sites for which LastPass's vault contained your logon information.



And it's notable that the vault does not appear to contain the user's unencrypted email address to associate who they are with the vault, nor is there any indication of the number there of PBKDF2 iterations that were being used to obtain the vault's decryption key from the user's email address and password.  That information is necessary to run the Password-Based Key Derivation Function to decrypt the key which is used to decrypt the vault.  Otherwise, it requires brute force.  So it must be that it's provided to the user's client through another query, other than the one we just used in order to get the vault's bulk content.



But LastPass's infamous December 22nd breach update, in that they said:  "The threat actor copied information from backup that contained basic customer account information and related metadata including company names, end-user names, billing addresses, email addresses, telephone numbers, and the IP addresses from which customers were accessing the LastPass service."



So to that we can now add everything that Rob's LastPass vault analyzer makes crystal clear.  Given the amount of effort that will generally be required - and we have a bracing update on that coming up next - to brute force the decryption of the encrypted information in even one LastPass vault, coupled with the fact that tens of millions of LastPass user vaults were obtained en masse, the direct threat of decryption for a single individual, unless directly targeted, would seem to be very small.



LEO:  But because they have the metadata, it's easier for them to target somebody; don't you think?  



STEVE:  Correct.  Well, yes. 



LEO:  Like if you had the login to the Strategic Air Command's Missile Control Center in your LastPass vault...



STEVE:  You'd be highly motivated.



LEO:  People would want to get into it; right?



STEVE:  Yeah.



LEO:  They're going to have to pick and choose, obviously.  They're not going to decrypt them all.  Why would they?



STEVE:  Correct.



LEO:  Yeah.



STEVE:  But assuming that it's possible for the bad guys to associate company names, end-user names, physical addresses, email addresses...



LEO:  There you go.



STEVE:  ...telephone numbers and IP addresses with specific vaults, and that must be possible because LastPass has to do it, and this is the information that they lost.



LEO:  Yeah, it's the metadata that they didn't encrypt.



STEVE:  Yes.  Even without any decryption, what you get is a comprehensive dump of exactly who logs on exactly where.



LEO:  Right.



STEVE:  And if you scroll horizontally all the way to the right of Rob's HTML output, there's also a Last Touched field.



LEO:  Oh, geez.



STEVE:  Containing a timecode that shows when the last logon at that domain occurred.



LEO:  Oh, my god.



STEVE:  So even without the use of any brute force decryption of the vault's encrypted contents, this represents at best a significant privacy compromise for every LastPass user.  Every user.



LEO:  And LastPass could have - there was no technical reason they didn't encrypt that data.  They could have.  Yes?  Other password managers do.



STEVE:  Yes.  Well, so I did a test.  I was wondering why they didn't encrypt the URLs.  So I thought, well, maybe if I'm not logged in, so that my vault is encrypted, right, if I'm not logged into LastPass, so I have no vault present, maybe LastPass doesn't add the little LastPass highlights to the username and password field if it knows it doesn't have the ability to fill that in.



LEO:  Who cares?



STEVE:  So I thought, okay.  And it turns out...



LEO:  By the way, Bitwarden does not do that.  A lot of password managers don't.  I think that's a silly little JavaScript feature.  Go ahead.  Turns out what?



STEVE:  So it turns out that's not the case.



LEO:  Oh.



STEVE:  Which is to say - no, no.  Which is to say LastPass always puts that...



LEO:  It's always there.



STEVE:  ...in all of the username and password fields.  So I was trying to give them the benefit of the doubt.



LEO:  Yeah, no, you tried.



STEVE:  Saying, well, maybe they're being smart about where the user, where it knows the user has the ability to log in.  No.  So Leo, I can find no rationale...



LEO:  And we're not saying that they should encrypt it with AES-256 in your vault.  But they should probably hash it so that, I mean, it shouldn't be plaintext in your metadata in the vault.  A lot of this stuff.



STEVE:  I see no - I don't get...



LEO:  There's no benefit to doing it.



STEVE:  I don't yet see a reason for them doing it.  And I tried one idea, but that wasn't it.



LEO:  Yeah.  And you could see why - so there's already a class-action lawsuit.  And the guy who started it claims, and we haven't verified this, but claims that he lost $53,000 in cryptocurrency.  You could see why this is important.  If you're going to attack somebody, find somebody maybe who has a shared crypto wallet on Coinbase.  That kind of information is being leaked.  It gives you a way to prioritize which vaults you go after.



STEVE:  Oh, wait, honey.  There is way more [crosstalk].  



LEO:  Oh, no.  I mean, it's perfectly possible, we don't know, that there is somebody right now running his old bitcoin mining apparatus, because he doesn't have any use for that anymore, against all of the vaults, and just the first ones that come out are the ones he's going to use, right, because they have bad passwords or whatever.  Or their PBKDF2 is 500.



STEVE:  One last note before we look at what's been learned from a week of feedback from our listeners, which as I said is a little bit bracing.



LEO:  Oh, boy.



STEVE:  If executing Rob's PowerShell script produces errors rather than the presentation of a nice graphical interface, as it initially did for me on my Windows 7 machine, I wanted to note that it's possible to update any Windows PowerShell support, even Windows 7, to the latest version 5.1.  Microsoft wants you to have it.  So I have a link for that at the bottom of page 4 of the show notes that will help anyone update their PowerShell script, just in case you get an error.



Okay.  So next.  What more do we know this week that we didn't know last week after our listeners had the chance to understand and peruse their LastPass settings?  We don't yet have any good sense for whether, or the degree to which, encrypted content was allowed to remain encrypted under the less desirable ECB cipher mode.  I expect feedback from the use of Rob's analyzer to fill in a lot of that information for us next week.  So I'd appreciate knowing when and how many ECB warnings, if any, are seen by our listeners.  I didn't have any ECB encryption in my vault, even though I'm sure that I had login credentials dating from my first use of LastPass.  Rob reported that his vault contained one ECB-encrypted entry.



LEO:  This would be a really old password; right?  I mean...



STEVE:  Yeah, well, I had really old ones, too.  So it's just...



LEO:  We don't know what the...



STEVE:  We don't, yeah.



LEO:  ...timeframe was of [crosstalk].



STEVE:  We're having to reverse engineer some of this.



LEO:  Again, this is something we need to know, and LastPass has not been forthcoming with it, yeah.



STEVE:  Yup, yup.  Okay.  But the ECB versus CBC issue is not that much of a big deal.  I don't want to overstate that.  Since AES-ECB, Electronic Code Book, being based upon the AES cipher, has a cipher block size of 128 bits, sets of 16 eight-bit characters, thus forming a block of 128 bits, are encrypted from that pattern of 128 bits into a different pattern of 128 bits.  That's encryption.  So this means that every instance of the same password will encrypt into the same pattern of 128 bits.  As I noted last week, the presence of password reuse would therefore be obvious just by inspecting a user's encrypted vault without the need for any decryption.



By comparison, AES-CBC, Cipher Block Chaining, uses both the encryption key, which would not change from one password to the next, and also a random initialization vector, you know, think of it like salting a hash.  It is different for every password.  So this would completely obscure the presence of any identical passwords.  Otherwise I really can't see any reason for preferring one over the other, but that's a good reason.



LEO:  That's a big one, yeah, because now the guy, again, we're talking about triaging the vaults.



STEVE:  Yup.



LEO:  You could look at a vault, say look at that 400 reused passwords, without decrypting.  And that's a big deal.



STEVE:  So we're going to likely learn by this time next week, we'll have feedback from our listeners who have used Rob's tool.  We'll know how much ECB is still around.



LEO:  Good, good.



STEVE:  Okay, now, Leo, are you centered over your ball?



LEO:  Oh, no.  Now what?  Oh, no.  Okay, yes.



STEVE:  By far the most worrisome fact that was revealed when our listeners checked the settings of their LastPass vaults was the degree to which many - and I do mean many - of their password iteration settings were found to be below the 100,100 iterations mark.  And in a revelation that I'm still trying to get my head around, I heard from many listeners whose PBKDF2 iteration count was set to 1.



LEO:  What?



STEVE:  Yes, 1.  And thus the title of today's podcast.



LEO:  1.



STEVE:  1.  Many people have an iteration of 1, which is to say, why bother?



LEO:  What does that mean?  Does it mean you could, I mean, you still have to - a hash cannot be reversed, but you can use rainbow tables easily.



STEVE:  No, because it is salted.



LEO:  Okay.



STEVE:  I've got all the math here.  We're going to understand what this means.



LEO:  Oh, good.  Okay, good.



STEVE:  So I also received many reports of iterations still being set to 500, and many set to 5,000.



LEO:  5,000 was the default for many years.



STEVE:  5,000, it was up until five years ago it was 5,000.  That's when it changed to the 100,100.



LEO:  And as we mentioned on Ask the Tech Guys on Sunday, the OWASP recommendation is 300,000.



STEVE:  Yes.  And probably not adequate, and we'll be talking about that in a second.



LEO:  Maybe even should be higher.



STEVE:  Yeah.  I'm thinking 1234567 would be a good number.  That's 1,234,567.



LEO:  And of course the reason you don't use larger ones is because it's slower on especially a mobile device.



STEVE:  Yeah.  Turns out unless you're being powered by a hamster, you're probably okay.



LEO:  You can do millions.  Okay, that's good to know.



STEVE:  Okay.  So at first you might think that anyone whose iteration setting was 1 should be about 100,000 times more concerned than someone using the new default of 100,100. But it's actually quite a bit worse than that because that 100,000 to 1 simple math, it assumes that vaults were being selected at random for attack, which would probably not be true.



LEO:  Right.



STEVE:  We need to assume that the attackers obtained every user's account metadata, including their vault's iteration counts.



LEO:  Oh, that's in the metadata.  It is.  Obviously.  That's how people are seeing this.  Yeah, yeah.



STEVE:  Yes.  It has to be because LastPass has to have that.



LEO:  They need to know, okay.



STEVE:  Those counts need to be recorded somewhere because no one's vault could be decrypted without knowledge of the count.  And the count is not particularly sensitive information.



LEO:  Right.



STEVE:  Unless it's 1.  And LastPass would have backed...



LEO:  Oh, my god.  Oh, my god.



STEVE:  Oh, Leo, it's so bad.  LastPass would have backed it up since the loss of that, the iteration count data, would have been even worse than the loss of the vault backups themselves because it would have made them worthless.  So assuming that the attackers obtained the iteration counts for every LastPass user, as they probably did from LastPass's backup, if opportunistic brute force decryption of user accounts was their intent, it would be a reasonable strategy for the attackers to start with those LastPass users whose counts were 1.  Why would they not?



LEO:  Actually, this would be a good use for a PowerShell script to triage the millions of vaults you have, looking for low iteration counts and perhaps maybe some custodial bitcoin wallets or, you know, things [crosstalk].



STEVE:  Leo, just ask ChatGPT.



LEO:  He'll do it.  Or she'll do it.  Or it'll do it, yeah.  Which one should I crack first, Chat?



STEVE:  Yeah.  Unfortunately, there's a well-known expression to describe the situation in which all of those LastPass users who at the time of this breach had their LastPass password iteration counts set to 1.  And that expression is "low-hanging fruit."



LEO:  Do we have a theory how somebody could have 1?  It was never the default.



STEVE:  Yes, it was.



LEO:  What?



STEVE:  Yes.



LEO:  In the earliest days of LastPass, 1 was the default.



STEVE:  For the first four years, from 2008 to 2012.



LEO:  Oh, I didn't realize.



STEVE:  Because a salted hash was considered strong enough.



LEO:  That's enough.  And of course one iteration.



STEVE:  That's what it was when I looked at it, and I said, this is fine.



LEO:  Yeah.



STEVE:  You know, you need to salt.  You can't just hash.  You need to salt your hash.



LEO:  Must salt it, yeah, we've said that many times.



STEVE:  And so there was no problem.  Okay.  So last week we did not look deeply into the actual performance of today's GPU-enhanced password cracking.  This week, we need to get a sense of scale.  Current estimates of GPU hardware-enhanced password cracking places the time required to crack a 100,100 iteration PBKDF2 protected password, where that password has high entropy of 50 bits, at 200 years.  So one GPU, 100,100 iterations, high entropy password, 200 years.  But since GPU use scales linearly, dividing that cracking task among 200 GPUs...



LEO:  Yeah, one year.



STEVE:  Which is now quite mature cracking technology, could crack the same password having 50 bits of entropy in one year.  Okay.  So one GPU, 200 years.  200 GPUs, one year.  But I also note that studies have shown - Wikipedia says it too, they agree - that most practical passwords have an entropy of around 40 bits.



LEO:  A lower entropy because you have to remember it.  They're not truly random.



STEVE:  Yes.  It turns out that it's difficult to actually get 50.  So, okay.  We'll get back to what lowers true entropy in a second.  Okay.  But having 40 bits of entropy is approximately - 40 versus 50, 40 is approximately 1,000 times weaker than 50 bits.



LEO:  Oh, that's a problem.



STEVE:  Because bit strength scales exponentially.



LEO:  Oh, boy.



STEVE:  In other words, random bits are worth a lot because each additional truly random bit on average doubles the time required to crack.  So the difference between 40 bits and 50 bits is 10 bits.  2^10 is 1024, thus a thousand times weaker.  Okay.  So if we assume that our attackers will have the use of 200 GPUs which, in this era of GPU-laden cryptocurrency mining rigs seems entirely reasonable, cracking a typical password having 40 bits of actual entropy would require 71.338 days, if that password was protected by 100,100 iterations of PBKDF2.



Okay.  Just to restate that because this is an important benchmark, a typical strength password, 100,100 iterations of key derivation, that's attacked by a 200-GPU password-cracking rig would fall against that attack in an average of 71.338 days. So if you're thinking that all of those 100,100 iterations might still not be providing you with sufficient protection, you're probably not entirely wrong.  Using one million iterations would be 10 times stronger, bringing us to 713 days, just shy of two years.  That seems much safer.



Okay.  But the very bad news is that for all those whose LastPass iteration count was, for whatever reason, discovered to still be set to 1 - and there were many such people who reported that this past week.



LEO:  And those are our listeners. 



STEVE:  Yes.



LEO:  Those are not unsophisticated people.  Those are our listeners.



STEVE:  Yes, yes.  Those same 200 GPUs could crack that same 40-bit entropy password in an average of 61.56 seconds.



LEO:  A minute.



STEVE:  Or just over one minute per single-iteration password crack.



LEO:  Wow.



STEVE:  Given that, it appears to be the height of negligence, if not bordering on criminality, that for some reason, for whatever reason, many listeners of this podcast, and I'm sure a great many more non-listeners, have no effective protection.



LEO:  Sad.  This is the most loyal LastPass users, the ones who started using it in the very earliest days.  Which, by the way, is probably you and me.



STEVE:  Yeah, yeah.  Fortunately, you and I moved to 100,000.



LEO:  We did.



STEVE:  Back five years ago when we talked about this.



LEO:  Because we listen, yup, yup.



STEVE:  They have no effective protection from the cracking of their LastPass vaults and the resulting disclosure of every single one of their website logon credentials, their credit cards and, as you mentioned before, any other confidential documents and personal papers that were stored for them by LastPass.



Okay.  Now, not one of these many people told me that LastPass had reached out to them to explain that due to their effectively non-existent password encryption they are at heightened risk following the data breach, and that they should therefore immediately rotate all their logon credentials being managed by LastPass and assume that any information stored in their LastPass vault had been compromised.  As far as I know, that has not happened.  And it certainly should have.  LastPass knows everyone's iteration counts.  And now, so do the criminals who stole them. 



LastPass somehow failed to update those iteration counts for a decade after the default was raised from 1 to 500 in June of 2012.  And they have not immediately and proactively assumed responsibility for that by informing their users, whose iteration counts were dangerously low, in many cases set to 1, that unfortunately they should now assume that the encrypted content of their LastPass vaults is now in the hands of criminals.  The industry at large has been grumbling about LastPass not being forthcoming about the details of the breach.  But we now have all the information we need to assess LastPass's culpability.



A couple of weeks ago, when I was first updating myself on LastPass's client settings over the holidays, I changed my iteration count from 100,100 where, thank goodness it was still set after we talked about this five years ago, to 350,000, as is now recommended by OWASP.  That change of my iteration count took, oh, perhaps five seconds.  I had to provide my LastPass master password again, so that the LastPass client could rehash that password under the new iteration count.  And that was it.  This could and should have been automated since its first increase from 1 to 500 back in 2008.  No user should have been allowed to set a dangerously low iteration count; and every LastPass client, which must know its user's iteration count in order to function, should have taken proactive responsibility for continually bumping it up every five years or so to whatever is currently considered safe.



So, the most startling and deeply disturbing news I received throughout last week was not only that many of our listeners' iteration counts were still 5000, and many even 500, but that many discovered that theirs was still set to 1.  I am a 67-year-old lifelong entrepreneur and businessman.  And you would have a difficult time finding anyone who is more deeply opposed to frivolously turning attorneys loose on each other.  It's one thing to fight over an ongoing contract dispute in order to reach a resolution.  That makes sense to me when the parties cannot negotiate an accord in the absence of objective judgment.  But attacking an entity after the fact, for something that was done which I'm sure they now regret, still leaves a bad taste in my mouth.  On the other hand, if a lawsuit were to be brought against LastPass, not because they made a mistake that upset their customers, but over actual provable damages arising from reliance upon LastPass' assertion of the safety of vault data, then I would not consider that to be unwarranted ambulance chasing.



There's no way to paint the presence of an iteration count of 1, used for the derivation of a LastPass vault's decryption key, as anything other than a critically debilitating product defect.  And for that, if actual damage results, LastPass could be, and I think should be, held wholly responsible.



LEO:  So I should point out that all the major password managers, including our sponsor, Bitwarden, 1Password use PBKDF2.  So I just went into my Bitwarden and set my PBKDF2 - it's in the Security Keys section of your settings - I set it to two million.



STEVE:  Good.



LEO:  And of course what I'm going to do is see if anything got really, really slow.  I bet it didn't.  Frankly, the processor in my iPhone is better than the processor on this Lenovo.  But assuming that everything's usably slow, but not too slow, I'm going to keep it at a high number.  And so that's a warning that others should also do this.  Right?  It's not a LastPass-only problem.  The Bitwarden default is 100,000.  But set it higher; right?



STEVE:  Well, after we take a break, we're going to talk about the true strength of our passwords.



LEO:  Okay.  Especially since I probably - my master password, in all likelihood, might not be 40 bits of entropy.  It might be a little bit less, as probably it is for a lot of people because you have to memorize it; right?



STEVE:  Yup.



LEO:  So I use, you know, a passphrase and some numbers and stuff.  But that's certainly not fully random, that's for sure.



STEVE:  That's our next topic.



LEO:  Oh, good.  You are so good.  You anticipate everything.  Get the show notes, GRC.com, because there's a lot of information in there.  You should always get them anyway, GRC.com, when you get the show.  I'm wondering if we should push it out in the RSS feed, too.  We can do that, push a PDF of the show notes along with the show.  Would you have any objection to that?



STEVE:  No, no.



LEO:  I'm thinking maybe we should start doing that because they're so valuable.  I don't know.  Listeners, let us know.  I mean, a lot of people would get it and go, what did I get, I don't want it.  But I think it would be valuable.



STEVE:  So I want to amplify something I touched on both last week and this week.  I mentioned that an increase in iteration count provided a linear increase in strength, whereas the increase in strength provided by adding bits is exponential.  I want to be certain that everyone fully appreciates the implications of that.



A few weeks ago, when I increased my LastPass client's iteration count from 100,100 to 350,000, that gave me an increase of 3.497, so about three and a half times.  But not four times.  But if I had increased my password's entropy by just 2 bits, that would have been a full factor of four increase in cracking resistance.  True entropy is quite difficult to calculate because very few of us are using a chunk of text from GRC's Perfect Passwords page for our master password since those are impossible to remember.



So we have the situation that a single character's true entropy is difficult to calculate.  If any character in a password is related to any other character in that password in any meaningful way other than having been chosen purely at random, in other words, if there's any reason for a character to be what it is rather than something else, then that character's...



LEO:  So if I use a passphrase, for instance.



STEVE:  Yes.



LEO:  Because then those characters are logically related by English grammar to one another.



STEVE:  Yes.  Yes.  Then that character's contribution to the true entropy of the whole is reduced.  Its contribution of entropy would be significantly less than it would otherwise be.



LEO:  Because it's not random.



STEVE:  Right, it's not.



LEO:  I before E except after C or whatever it is, yeah.



STEVE:  Yup.  So this is why the first thing that password-guessing crackers do is use dictionary words in various ways and base their attacks upon the frequency of characters occurring in the natural languages of the password's user.



LEO:  ETAOIN SHRDLU.



STEVE:  Yup.  Those attacks model the lack of entropy that many users employ when they're choosing their passwords.



LEO:  Yeah.  I've got to go check my password.  I'll be back.



STEVE:  So what's the idealized potential entropy of a single character?  In a byte-oriented system, a single character typically occupies 8 bits.  So we might be inclined to say "8 bits."  But ASCII only uses the lower seven of those 8 bits.  So assuming a non-UNICODE standard ASCII character set, there are a total of 95 printable standard characters available if you use upper and lowercase alphabetic, the 10 numeric digits, and all the other special characters.  That gets you to 95.



So here's the point I want to drive home:  Increasing my iteration count from 100,100 to 350,000 yielded that just shy of three and a half times increase in password-busting protection, 3.497.  But just adding one single randomly chosen additional character to the end of a password increases the resulting password's anti-cracking strength by 95 times.  95.



LEO:  This is the Password Haystacks stuff you told us about years ago.



STEVE:  Yes, exactly.  And so this is why, when it comes to passwords, size does matter.  You get far more attack protection by using even slightly longer passwords, where strength increases exponentially with length, than you do by increasing iteration counts, where strength only increases linearly.



Okay.  So there were a bunch of interesting bits of feedback and questions from our listeners.  I'm going to continue talking about some of these things using them as the prompting.  Via a DM I received the note.  Someone posted:  "I have a corporate LastPass account and a personal pro account.  The personal account was updated to 100K iterations, but the corporate account was still at just 5K.  My personal account is still exposed, though, because I took advantage of the ability to share passwords between my personal and corporate accounts to reduce the number of logins.  I assume that if they crack the corporate, they would have the personal anyway.  Good news, my password has more than 25 random characters derived from your Perfect Passwords.  The bad news is that it is so long and random that I used the same password for my corporate and personal LastPass accounts.



LEO:  Oh, that's not good.



STEVE:  Well, 25 truly random characters, chosen from the Perfect Passwords page, as this user did, will have been selected from an alphabet of 95 possible characters.  So that's 95 x 95 x 95 and so on for a total of 25 times.  That's 95 raised to the 25th power.  I used the Password Haystacks page to quickly do the math and show me that the resulting password has 2.8 x 10^49.



LEO:  Holy cow.



STEVE:  Yeah, baby.



LEO:  But how do you memorize it?



STEVE:  You don't.  You have that somewhere else.



LEO:  You write that somewhere, okay.



STEVE:  Yes.  You keep that in your Apple Notepad and copy and paste it.



LEO:  Oh, lord.



STEVE:  And importantly, all of those characters are all equally likely to appear combinations.  True entropy.  If we take the Log base 2 of that number to determine the equivalent binary bit strength, we get 164.2.  So in other words, it contains a little over 164 binary bits of true entropy.  Another way to look at that is that each character, when truly chosen randomly from a set of 95 possible characters, contributes 6.57 bits of entropy.  6.57.  In other words, this person has absolutely nothing to worry about.  His password has slightly more than 164 bits of true entropy.  It will never in many lifetimes be cracked by today's or even any projected technology of tomorrow.  Remember, quantum computers won't help with this sort of symmetric crypto problem.  They are of no use.



Okay.  But there's something else worth noting.  Recall from last week that Mr. Grumpy Pants - what was his name?  Oh, yeah, Jeremi Gosney.  He noted that LastPass's vault encryption key was derived from only 128 bits of entropy.



Okay.  So now consider this crazy 25-character totally random password which has a bit more than 164 bits of entropy.  If the attackers knew that - and there's no way they could.  But if they did, it would be far quicker to just forget about the user's insane password and attempt to directly brute force the vault's encryption key itself since it has "only" has 128 bits of entropy.  I have "only" in air quotes because my point is that 128 bits already has so many possible combinations (3.4 x 10^38) that there's never any reason to go above that.  Haystacks tells us that 20 characters chosen from that 95-character alphabet offers 3.62 x 10^39.  Okay.  Once again, 128 bits is 3.4 x 10^38.  Twenty random characters is 3.6 x 10^39. So 10 times stronger than what 128 bits can do.



Okay.  So last word on this, in summary, do not use only 20 characters unless they are truly chosen from among all possible characters randomly.  But if they are, there is no need or benefit gained from using any more.  Twenty purely random characters from an alphabet of 95 is 10 times more than what you get from 128 bits.  And 128 bits is considered by the entire industry all we need for now.



Dave wrote:  "On Security Now! Episode 904, Steve asked for feedback on the current value of the LastPass Password Iterations field.  Mine was set to 1.  I have no idea how/why it is 1 because I never changed it."  Well, there's why.  "Needless to say, I have downloaded and installed Bitwarden, and I am changing the password on every site in my vault as rapidly as I can."  So, yes, Dave has the right idea.  He was typical of many of our listeners.  And there's an example from among many of what our listeners discovered to their horror last week; and, sadly, it might be because he never changed it that it remained set to 1.



As you said, Leo, the most loyal early adopters of LastPass, they're the ones who are, in a phrase, effed.  As we know, he should not have had to change it.  That should never have been his responsibility.  But we're on the outside here, looking in.  We have no idea of the real story behind this iteration fiasco.  But there is no way to forgive this from LastPass.  None.  This is more than a mistake.  This had to be someone's boneheaded decision.



With their acknowledgement of the importance of increasing the iteration count over time, evidenced by its default being jumped from 1 to 500 to 5000 to 100,100, someone must have made the decision not to bother bringing older existing iteration counts into compliance with current best practices.  Someone must have decided that it would, I don't know, result in too much customer confusion and support calls, so let's just leave it wherever it is.



And the galling thing is it could have been done 100% transparently.  I am no smarter than their crypto people.  So they know this, too.  When the user provides their email address and password to log into their client, at that moment the client has everything it needs to perform the upgrade transparently.  Start iterating on PBKDF2.  Pause at the current iteration count and take a snapshot of the current key at that point.  Then keep going to the new larger iteration count and take a snapshot of that new key.  Now decrypt the vault with the current key, which was sampled midstream, then reencrypt the vault with the larger final iteration count key.  And, finally, update the stored iteration count.  Done.  Totally transparent.  No user confusion.  And a company as big as LastPass, now focused on the enterprise and everything, for reasons I can't possibly explain, never did that.  I mean, not only is not everybody at 100,100, there are people at 5000 and 500.  There are people at 1.  And change your passwords.



Okay.  David Lemire.  He said:  "Hi, Steve.  Thanks for the excellent coverage of the LastPass breach and its consequences in SN-904.  I can confirm both your smooth experience transferring from LastPass to Bitwarden, and Leo's note about Bitwarden having a lower size limit on secure notes than LastPass's.  I had to delete one or two very large notes before I could successfully import my vault."  He said:  "Thankfully, they were obsolete."



He said:  "I have one technical security question.  Given the threat of rainbow tables, wouldn't it make sense for each individual account to have its own iteration value within a suitably secure range, rather than a common default value," he says, "which I realize can be changed.  Combining an unpredictable iteration count with salting the hashing process should raise the work factor for the creation of rainbow tables, as well as the comparison process, by a considerable factor."



Okay.  Now, I didn't mean to confuse things last week with my mention of the possibility of attacking known salt-free hashing schemes with precomputation attacks.  My intention was to paint a history to remind us of where we've been and how we got to where we are today.  Everyone has always been protected from precomputation attacks by the inclusion of their email address as the salt for the PBKDF2 function.  Joe Siegrist was doing this from day one, with an iteration count of 1.  Unfortunately, back in 2008, Joe was, as I said, also iterating only once through PBKDF2.  And as we now know, for some unlucky souls, that for whatever reason was never changed.



Someone is also likely to ask if a user deliberately set their iteration count to 1, what would happen if they didn't understand what that was about?  You know, like what if that happened?  My answer to that would be that it should absolutely never have been allowed.  LastPass would certainly not allow any user to leave their password blank.  A low iteration count is effectively no different.  LastPass was lifting the count over time, and that should have always been the minimum that any LastPass user client would accept as its count.



I received a question via email:  "Hello.  About the LastPass breach, Episode 904, the risk on passwords and metadata was explained very well.  I wished the risk for files stored in LastPass could be explained, too, for example, copies of personal ID, passports, driver's licenses."  He said:  "I can go through all my passwords and change them, but changing my real life documents will be much more difficult.  I guess many other LastPass users will have this problem, too.  Thanks."  Signed Boris.  So Boris makes a great point.  We didn't stop to consider much that we were only talking about login credentials primarily, you know, but the many greater privacy dangers that might arise from having the contents of the LastPass vault's secure notes storage compromised.  Depending upon what was in there, the damage from disclosure could be significant.



LEO:  Oh, yeah.  My social's in there, my driver's license, my passport, everything.



STEVE:  Yup.  Full identity theft information.



LEO:  Oh, good lord, yes.



STEVE:  So I also received a bunch of these, basically saying:  "Replying to @SGgrc, I exported all my stuff to Bitwarden earlier tonight.  The process couldn't have gone more smoothly.  Up and running on both my laptop and phone."



LEO:  Yay.



STEVE:  So I was glad for that.



LEO:  Bitwarden, I'll say it again, is a sponsor, but that has nothing to do with anything.



STEVE:  I explained my rationale for choosing it last week.



LEO:  We like it because it's open source.  There's all sorts of benefits.



STEVE:  Yup.



LEO:  Incidentally, so I have changed my PBKDF2 to two million, as I said, which I think is as large as it can go.  I guess there's no practical limit.  But I notice now, because I also changed my password, so I got logged out everywhere, that it only added a few seconds to loading the vault. 



STEVE:  Yup.



LEO:  So a minor, minor amount of time.  That's two million.  That's big.  I also, one of the nice things about LastPass, I changed my password because I want more entropy.  And I used Password Haystacks to pad it out and all that stuff.  But they also give you the option, risky though it might be, and explain the risk, of rotating your vault key.  That's that 128-bit key for your vault.  And you can do that, as well.  And I thought, you know, I've had this for a couple years.  Maybe I should rotate that vault key, too.  So they really give you the options you need, I think, so make sure you're safe, even if that vault got exfiltrated.



STEVE:  Yup.  And I will have a request for Bitwarden...



LEO:  Oh, good.



STEVE:  ...by the end of the show.



LEO:  I know some people.



STEVE:  Okay.  So via direct message:  "Steve, I listened to your podcast twice, but what I don't understand is I thought you said it would not matter if somebody had our blob of data because the keys only reside on our devices.  So even if they had our master password, how would they crack into the blob without having the keys?  Thanks for all you do."



Okay.  So if there was some confusion there, let me clear that up.  The key that's required to decrypt the LastPass vault key is derived only and completely from three pieces of information:  the user's email address, the user's password, and the iteration count.  No other information is required.  The only one of these three things that LastPass and the attackers do not know is the user's password.  They have their email address and iteration count.  So with an iteration count that's too low, it's quite feasible for a modern attacker to simply guess and test at ultra-high speed all possible passwords until they find the right one.



Also via direct message:  "Hi, Steve.  Do you think that having a non-standard number of iterations, let's say 168,429 makes that particular password not worth the effort to try to decipher since 95% of all passwords will have either 5000 or 100,100 iterations?"  And this question came up often.  



Since each user's iteration count is known, making it non-standard will have no effect.  If the attackers have adopted that "low-hanging fruit first" strategy, which is what seems by far the most likely way to reap the rewards of their score, they would sort the entire LastPass vault backup database by iteration count, and prioritize attacks against all of those unlucky souls whose iteration count matches the title of this podcast.  From there, sorted by iteration count, they would proceed upward.  And let's not forget that a significant amount of privacy-related information is immediately available since all of the URLs for the sites where we have stored our logins is in the clear.



Someone named Zapper tweeted to me:  "@SGgrc Steve.  Being a longtime and very trusting listener I have been a LastPass user on your recommendation.  I will now migrate to Bitwarden.  May I ask you to clarify the security risk if my LastPass master password was 20-plus characters?"



Okay.  Zapper's question is also quite common.  So I want to reiterate that longer is always better, even much better, and more random is also better because it increases true entropy.  But as for 20-plus characters, if your password is truly 20 random characters, that's 131 bits of true entropy which is absolutely secure.  So no need to go larger.



Via DM:  "LastPass," he says, "I changed my 30-character master password, but I still feel uneasy.  I started changing all passwords, but have not migrated off of LastPass yet.  Any thoughts on ensuring LastPass removes all vault info upon cancellation of user account?"



So that's a really good question.  I think LastPass needs to affirmatively answer this question, if they don't have it already somewhere in their FAQ.  This user is not talking about the consequences of the theft immediately, but rather the safety of remaining for a while, I guess, with LastPass.  A 30-character master password will be very, very, very secure.  Even if it's the lowercase alphabet, in order, abcdefghijklmnop and so forth, with the digits 1234 scattered among the 26 letters somewhere to pad it out to 30.  The resulting hash from that conveys nothing of the password's length.  So no one attacking would have any idea how long the password is.  This was the key message underlying Password Haystacks.  No attacker would be trying any 30-character passwords, having no idea how long the password is, until they had exhausted all shorter passwords, and that will never happen.



So, you know, the reason we don't recommend somebody using that 30-character password in a lot of different places is that we don't know that everyone is hashing it securely and storing it securely.  And we've just had a big example of the largest password manager on the planet not storing things securely.  So, but using that one time in a situation where you know how the password is being managed, it's being deeply hashed with a high iteration count, that would be a perfectly acceptable password.  But don't use that one.



Skynet said:  "Moved my vault to Bitwarden and set the PBKDF2 iteration to one million.  On my iPhone 11 Pro Max it performs fine.  One million iterations.  Go big or go home, Steve."



LEO:  I went double that.



STEVE:  Yup.



LEO:  And it's fine.  It added a few seconds.  That's all it adds.  And it only adds it the first...



STEVE:  And you don't have to do it that often.



LEO:  It only adds it the first time you download the password vault. 



STEVE:  Correct. 



LEO:  Yeah.  So it's not - it's a minor - boy, and it feels a lot better.  And I got rid of the entropy.  Instead of using a passphrase, which had English words in it, so the order of those letters was not completely random and following spelling rules, I used an acronym.  I used, not an acronym, an initialism.  So I used the first letters of a long phrase and added some extra padding with other stuff.



STEVE:  Yup.  Good.



LEO:  So I feel like that's - now, there's still less randomness because there's, you know, there's some grammar to that sentence.  But I don't...



STEVE:  But again, length matters.



LEO:  It's very long now.  It's 59 characters.  I think it's long enough.



STEVE:  Nobody will know what your length is.  Nobody will know.



LEO:  Oh, I just said it.  Never mind.  Forget I said that.  By the way, that's one thing I do when I create passwords for - we should mention passwords for sites don't have the same issues in most cases.  The difference here is somebody was able to download the vault and at their leisure brute force it.  You don't have that leisure with a site unless the site gets breached, and their password database gets captured, and it's not properly encrypted and all that.  You don't - the same rules don't necessarily apply to individual passwords you're generating.  Although, again, you know, it's worth doing a long one if you're using a password manager.



STEVE:  Well, and this is a perfect example of all your chickens in one basket; right?



LEO:  Yeah.



STEVE:  I mean, it's the master password.



LEO:  That's the biggie.



STEVE:  And LastPass said the only password you need to remember.



LEO:  Well...



STEVE:  Well, uh-huh.



LEO:  But what I was going to say is I vary up the length of passwords I use on sites.  So I don't always use 19 characters.  That's a setting in Bitwarden, but I mix it up.  So they don't even know that, which is helpful; right?  Yeah.



STEVE:  Yeah.  So for PBKDF2, I don't see any reason for using an iteration count lower than a million.  And as I said, I'd probably use 1,234,567.  And it doesn't matter that it's not secret because that doesn't help anybody.  It's all being salted anyway.  So each individual crack has to happen by itself.  And, you know, if you find out that it takes too long on some platform, you can always turn it back down.  But I'd start at 1234567.  But really, in this era of GPU-driven password cracking, where GPUs hash at lightspeed, PBKDF2 is showing its age.  Cranking up iterations is just running ahead of a moving train.  It would make much more sense to just get off the tracks.



LEO:  Well, I asked about that last week.  Argon2 is another option.



STEVE:  Yes.  Since SQRL's entire security model is based upon the security of a single password-based key, I gave this a great deal of thought years ago.  Everyone, including Bitwarden, ought to abandon the use of any non-memory-hard password key derivation which GPUs excel at.  SQRL uses "scrypt," also known as Scrypt, S-C-R-Y-P-T, which absolutely requires a block of dedicated memory which cannot be shared among cores.  Scrypt's many parameters are tunable, and I don't now recall exactly how much memory I required, but I think it was 16 megabytes.  I chose that because every smartphone can spare that, and it's only needed briefly to process a user's password entry.  But significantly, GPUs are unable to follow since they're unable to run Scrypt of that size at all.



LEO:  Oh, nice.



STEVE:  So when that much memory is required, GPUs are out of the game.  So switching away from PBKDF2, whose time has passed, ought to be on every password manager's roadmap for the future. 



LEO:  Good to know.  How about Argon?



STEVE:  And there's Argon, yes.  I mean, that's the same thing.  These are all memory hard.  And so just as a little quick example, the way this works is the algorithm allocates 16MB of memory.  Then it uses a very secure hash, a keyed HMAC, based on the password that you've giving it, to fill this memory with pseudorandom data where each value is a pointer within that memory.  So it fills 16MB with pointers from zero to 16MB.  Then it follows the pointer trail, jumping throughout all of that memory.



Well, it turns out there is no way that has ever been found to short-circuit that process.  That is, you don't technically need to use 16MB.  But if you didn't, you would need to compute what some random pointer somewhere out there in 16MB space would be.  And then, when you go to it, you would need to then compute what the other pointer somewhere else would be.  In other words, if speed is your goal, the fastest way to solve this puzzle is just give it 16MB, let it fill it with pointers, and follow these pointers all around hell and gone, you know, within this 16MB space, and the path you take ends up resulting in the key that this thing synthesizes.  And it cannot be short-circuited, and no GPU can do this.



So you just switch to something that GPUs cannot do.  And then you're not constantly having to, as I said, run ahead of the train, staying on the tracks.  You just say, nope, let's use a different protocol.  And again, I've been using, you know, SQRL uses a 16MB Scrypt algorithm, and it runs everywhere perfectly, on iPhones and Androids.  Nobody has ever had a problem with it.  And it cannot be accelerated by a GPU.



Okay.  A couple last bits.  Robert van den Breemen, he said:  "@SGgrc Thanks for the honest podcast on LastPass.  Your vetting years back made me use it for many years.  Now moving to Bitwarden.  Now that I'm changing my 1000-plus passwords, I see how broken the system of password login really is.  Why is there no change password API?"  And, you know, that's a really good question.  A uniform, standardized, cross-site password change API would make rotating all of one's passwords an automatable operation.



LEO:  Right.



STEVE:  I think that the problem is mostly per-site resistance.  One of the complaints most people have is that every site is different, has different password requirements, additional bells and whistles like security questions or not, and different password recovery approaches.  And this arises from the fact that every site wants to be different.  There's no uniformity, so each site gets to invent the user experience flow that they prefer for their particular whim.  It didn't have to be this way, but it's the way it is.  Okay.  And last one.



LEO:  And I should point out, the reason the Internet is the way it is, is because nobody sat down and designed the whole thing from scratch.  If they had, they would have put a password rotation system in.



STEVE:  Yes.



LEO:  But it wasn't designed that way.



STEVE:  Right.  And you would never tell your mother that she must type https://.



LEO:  Even, by the way, when somebody asked the creator, the founder of the web about that, he said, "I never thought humans would ever have to type that.  It was supposed to be machine-readable."



STEVE:  It was link following.



LEO:  Yeah.  It wasn't supposed to be anything a human would ever have to see.



STEVE:  You were on a page, and it had nice English links or whatever language.



LEO:  Right.  So it's all under the hood.



STEVE:  Yup, exactly.



LEO:  Yeah, yeah.  But so Tim Berners-Lee did not design it.  You know, but that's what happens in the real world.  Stuff gets done, and it works.



STEVE:  Yup.



LEO:  And it changes, and all of a sudden you've got Twitter.  It's called evolution, or devolution, whatever it might be.



STEVE:  Okay.  Last one.  Andy Olson tweeting from @AvgAndy.  He said:  "@SGgrc I listened to the recent LastPass episode.  Switching to Bitwarden.  Just wanted to note that password changes are necessary, but I found I can also change usernames on a lot of important sites.  If user login is email, change that, too."



And yes, I agree with that, especially since today's standard for password recovery, you know, handling the "I forgot my password" event, is to send the password reset link to the user's account email.  So, while it's not imperative, if it's easy for you to also change your account's email while you're at it, from what it was when your LastPass vault was copied and stolen, then there's no reason not to.



Okay.  So once again I've used up our time this week covering this news, which is huge for this podcast's listeners since such a large number of us chose and have been using LastPass.  But even for those who had not chosen to stay with LastPass or long ago chose a different password manager, all the information about GPU cracking strength and PBKDF2 iteration counts is universal, as is the need to urge whatever password manager that it's time to move away and beyond PBKDF2.  There are many cryptographically verified alternatives, and they should be used.  You know, racing ahead of GPUs no longer makes any sense.



As I said, next week we'll be able to get some sense for the amount of the ECB cipher mode that our listeners discovered in their LastPass vaults with the aid of Rob's and ChatGPT's very nice PowerShell script.  But unlike this week's bombshell that many iteration counts were 1, the presence of any lingering ECB won't present a five-alarm fire.



LEO:  I really wish LastPass were giving us more information.  I understand they don't want to because it's a black mark on their escutcheon.  But I think their escutcheon has been scuttled already.  So maybe you should just start telling us what we need to do.  It's my guess that when they did up the iterations, they didn't retroactively fix everybody's vault.  Maybe they couldn't?  Maybe they didn't know how to?  Maybe they didn't want to.



STEVE:  Yeah, I've covered all that.  They could.



LEO:  They could have.



STEVE:  They didn't.  They could have, they should have, we don't know why they didn't.



LEO:  Yeah.  At least send out an email saying - and maybe they did.  I don't remember.  But, yeah.



STEVE:  No.  An email is insufficient.  Leaving anyone's iteration set to 1...



LEO:  As you said, it's like [crosstalk].



STEVE:  ...the title of this podcast, is unconscionable.



LEO:  Yeah.



STEVE:  I mean, you could argue it is not true that, like today, when this theft occurred, that offered any protection.  And they were assuring everybody that, if you use a good long and strong password, you are safe.  That is not true.



LEO:  That's a huge thing, that because of something they did, your good long strong password was insufficient.



STEVE:  Yes.



LEO:  That's not good.  As always, great stuff, Steve.  I appreciate it.  One thing I'll add to one of your commentators who had a shared vault.



STEVE:  Ah, yeah, yeah, yeah.



LEO:  He had his personal vault and his business vault in LastPass.  We do that, as well, here.  It's my understanding those are kept separately.  So it's just a separate vault.  It's not like there's one vault.  Even, now, he did reuse the same password on both.



STEVE:  But it was 25 totally random characters.



LEO:  It was a good password, yeah,



STEVE:  He has nothing to worry about.



LEO:  And he has a good number of PBKDF2 iterations.  We are actually - we use LastPass Enterprise.  And we're talking about what we need to do.  It's a massive - and that's the other thing I would say, we got a number of emails and comments from people in our forums that, well, you don't understand, I've got a thousand passwords in there.  I do understand.  I completely understand.  That's the problem.



STEVE:  And Leo, even without any decryption, think of the profiling that can be done of a person.



LEO:  Well, that was terrible.  They left that unencrypted, all that metadata.  Yeah, that's terrible, terrible.  No, I don't - this is the other thing that I don't know, but I think there's more.  The other shoe will drop down the road because how can LastPass survive this?  And when the stakeholders at LastPass say, well, you've got no customers, at some point they're going to turn off the servers, and there's going to be another collapse because I don't see how they stay in business.  What happens when a password manager goes out of business?



STEVE:  That's a really good question.  Not something we've had to face before.



LEO:  No.  I mean, that's a really - so that's why I think it does behoove everybody, as you've done and I've done, to move off, even though, yes, it's a big pain in the - it's easy to move, actually.  You said that, and I'll say it.  It's easy to move.



STEVE:  Oh, my god.  I had resistance to it, and it was like, wait a minute, I'm done?



LEO:  Yeah, it's trivial.  And most password managers work roughly the same as LastPass with the autofill and the authenticator.  Oh, another thing somebody was asking in the chatroom, hardware key, like you know I've mentioned I use YubiKey, recommended?  Good idea?  I would just point out that at least Bitwarden, I think LastPass too, they all have fallbacks, usually to an authenticator.



STEVE:  The only thing, from a crypto security standpoint, the only thing those offer is the convenience to their user of when you log into your vault.



LEO:  Of a YubiKey.



STEVE:  It provides no additional security.



LEO:  If you could, right, because the vault doesn't use this for the brute forcer.  They don't - it's not the second factor.



STEVE:  They don't care.



LEO:  They don't care.



STEVE:  They don't care, yeah.



LEO:  And if you turned off secondary fallbacks, then I guess this would be stronger than, say, an authenticator, certainly a lot stronger than an SMS message.



STEVE:  Yes.  And in fact that was one of the features that SQRL had was that after you got comfortable with it, in the UI you could check a box saying Do Not Allow Any Other Authentication Mechanism.



LEO:  The fallback is the weakest link.  Whatever the weakest link is, that's the weakest link.  That's all you've got.  All right, Steve.  I am so glad you did a second episode on this.  I know there are a lot of people saying, yeah, but what about?  But there hasn't - it's good news.  The hackers rested.  They were busy changing their LastPass passwords.  So, you know, we had a little breathing space.  Next week the news.



STEVE:  Yes.  We will, we'll do a big news catch-up.  I'm sure, as I said, we'll briefly talk about how many people found ECB in their vaults. 



LEO:  I'll be curious, yeah.



STEVE:  It'll be, yeah, it'll just be a curiosity point.  It's not the end of the world.  And but then we'll catch up on all the news that's been happening.



LEO:  I wish I could look at my old vault.  And this is the other thing LastPass hasn't said.  When did that backup get made?  I deleted my LastPass vault a couple of years ago, but maybe the backup was from 2.5 years ago.



STEVE:  Yeah.



LEO:  In which case my vault's in there, too.  And so, and they haven't told us anything.  So it's very disappointing.



STEVE:  No, in fact I did, I went to the blog to see whether, I mean, just recently, a day or two ago, to see whether there was any response, any further information, anything else.  No.



LEO:  No.  They're just hoping this goes away.  At this point they've got nothing to do.  They're hoping it goes away.  And it's not.  I'm sorry to say it's not going to go away.  One other point I will make on Bitwarden, because it's open source, if you have advice for Bitwarden or a change you'd like to see, for instance changing to a memory-hard hashing, that's where you go to GitHub, and you do a pull request.  Not a pull request, a PR.  Not a pull request.  What is it you do?  Anyway, you issue, you give them an issue saying, please.  That's how you do complaints to Bitwarden.



STEVE:  It's time.  It's time.



LEO:  It's open source.  So you go to the open source repository, and you enter an issue saying there's an issue, you should use a memory-hard hashing algorithm.  And they have to respond.  They will respond to that.  And you know what will happen because it's open source?  Somebody will write it.



STEVE:  Yeah.



LEO:  Say here it is.



STEVE:  Well, it's already been written.  It just needs to be hooked in.



LEO:  Hooked in.



STEVE:  It'll take an afternoon.



LEO:  Well, then they should do it, yeah.  Thank you, Steve.  Bless you, Steve, for the job you do.  It's so important, and we really are grateful to you.  Steve's website, GRC.com.



Copyright (c) 2023 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.






GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#906

DATE:		January 17, 2023

TITLE:		The Rule of Two

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-906.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we're back to answering some questions that you didn't even know were burning.  First, is the LastPass iteration count problem much less severe than we thought because they are doing additional PBKDF2 rounds at their end?  What sort of breach has Norton LifeLock protected its users from, and have they really?  What did Chrome just do which followed Microsoft and Firefox?  And is the Chromium beginning to Rust?  Will Microsoft ever actually protect us from exploitation by old known vulnerable kernel drivers?  What does it mean that real words almost never appear in random character strings?  And what is Google's Rule of Two, and why does our entire future depend upon it?  The answers to those questions and more will be revealed during this next gripping episode of Security Now!.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  Lots to talk about.  An update on the LastPass breach.  Steve thought there was a saving grace.  Well, I'll let him tell you the story.  Norton LifeLock says it saved you from something.  But what, really?  And a look at Rust and how it's helping Google make Chrome safer.  All of that and more coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 906, recorded Tuesday, January 17th, 2023:  The Rule of Two.



It's time for Security Now!.  I know you've been waiting all week for this.  Steve Gibson's back.  And guess what?  We're not going to talk about LastPass this week.  Hi, Steve.



STEVE GIBSON:  We actually are.



LEO:  Oh, no.  Well, I'm sure there's some follow-up.  But we have other stories.



STEVE:  Yes, we do.  We actually do.  This is Security Now! #906 for the middle of, what is this, January.  And this week, as you said, Leo, we are back to answering some questions that you didn't even know were burning.  First, is the LastPass iteration count problem much less severe than we thought?



LEO:  Oh, okay.



STEVE:  Because they are doing additional PBKDF2 rounds at their end.



LEO:  Well, that would be nice to know.  		



STEVE:  Wouldn't that be nice?  What sort of breach has Norton LifeLock protected its users from?  And have they?



LEO:  Oh, yeah, I saw that story.  Hmmm.



STEVE:  Hmmm.  What did Chrome just do which followed Microsoft and Firefox?  And is the Chromium beginning to Rust?  Will Microsoft ever actually protect us from exploitation by old known vulnerable kernel drivers?  What does it mean that real words almost never appear in random character strings?  And what is Google's Rule of Two, and why does our entire future depend upon it?  The answers to those questions and more will be revealed during this next gripping episode of Security Now!.



LEO:  Now, that's a tease, Steve.  Congratulations.  Golf clap.  Very well done.  I can't wait to find out all of the answers to those questions.



STEVE:  Ah, there are some goodies.



LEO:  And a good Picture of the Week.



STEVE:  Ah, yes.



LEO:  Picture time, Steve?



STEVE:  It is.  So here we have yet another puzzle for our listeners.



LEO:  Oh, it's not so puzzling.



STEVE:  We have a sidewalk path running from the right foreground to the left background of this photo.  To one side is sort of a knoll covered with dried grassness, weedy sort of stuff.  Over on the left is a not-very-well maintained typical green grass, which is now dried out.  Okay.  So that's sort of the setting.  Now, right in the middle, I mean across this sidewalk, is a permanently installed Sidewalk Closed fence.  This is not a gate.  If you look at it, there's a steel pole on either side of the concrete sidewalk, and then there's a well-built wire gate which is strapped between these poles.



LEO:  Oh, it's good.  They didn't stint on this.  They put in the best.



STEVE:  No, honey, this is meant to stand the test of time.  And there's a sign.  In case you were curious about, like, okay, why is my path blocked on this sidewalk, across the front of this wire it says "Sidewalk Closed."  Sorry.  You're SOL, as they say.  So you're confronted with this gate.  And it's like, oh, well, I guess I can't proceed.  I had my heart set on going into the distance here and around the corner.  But the sidewalk is closed.  Except that, like, it's not.  Because you could choose the direction you wanted to go around this small hurdle in your way.  And there's, like, sidewalk behind it.  The sidewalk looks just fine.



LEO:  Well, that was before it was closed.



STEVE:  I mean, if they wanted to close it, why not remove it?  I mean, like, you know, get out the jackhammer and break this concrete up and haul it off.  But no.  It's there.



LEO:  It's a puzzle.



STEVE:  A perfectly functioning sidewalk which no one can use unless they walk around this barricade.  And, look, oh, it works.  The sidewalk still works, Leo.  It's like once I remember, this is a long time ago, I let my car insurance lapse.



LEO:  Yes.



STEVE:  And I thought, oh, I can't drive.  But I got in the car and turned the key, and it still worked.



LEO:  Still works.



STEVE:  It was amazing.



LEO:  It's a miracle.



STEVE:  Anyway, yeah.  So, wow.  Okay.  I originally was going to title this podcast "A Brief Glimmer of Hope" over a pursuit that took me, I think it was like six or seven hours until I realized how it turned out, and how brief the glimmer was.  But it's an interesting issue, and it could come up.  It already had come up.  So, and I figure, you know, since all LastPass users could use a bit of good news about now, I became excited when it appeared that things were not as bad as we thought last week.  As we know, many users discovered that LastPass had never increased their clients' local PBKDF2 iteration count from its earlier setting of 5,000, or maybe 500, or in many cases, and they keep mounting, I've seen a lot more reports in the last week, as few as one iteration, which of course results in a trivial-to-bypass encryption of LastPass backups.



Well, one of our listeners with a sharp memory, by the name of Parker Stacey, wrote with an important quote from a page on LastPass's website.  The page Parker quoted is titled "About Password Iterations," and most of this we already know.  But there's an important line on this page from their website that was news to me.  Okay.  So in order for that important line to be understood in context, which is somewhat unclear, I'm going to share this short piece in its entirety.



So this "About Password Iterations" on the LastPass site says:  "To increase the security of your master password, LastPass utilizes a stronger-than-typical version of Password-Based Key Derivation Function (PBKDF2).  At its most basic, PBKDF2 is a password-strengthening algorithm that makes it difficult for a computer to check if any one password is the correct master password during a compromising attack."  Right.



They say:  "LastPass utilizes the PBKDF2 function implemented within SHA-256 to turn your master password into your encryption key."  All this we know.  "LastPass performs a customizable number of rounds of the function to create the encryption key, before," they said, "before a single additional round of PBKDF2 is done to create your login hash."  Then they said:  "The entire process is conducted client-side.  The resulting login hash is what is communicated with LastPass.  LastPass uses the hash to verify that you are entering the correct master password when logging into your account."



Okay.  This is the next line is what, like, I go, what?  They said:  "LastPass also performs a large number of rounds of PBKDF2 server-side.  This implementation of PBKDF2 client-side and server-side ensures that the two pieces of your data - the part that's stored locally and the part that's stored online on LastPass servers - are thoroughly protected."  Then it said:  "By default, the number of password iterations that LastPass uses is 100,100 rounds.  LastPass allows you to customize the number of rounds performed during the client-side encryption process in your Account Settings."



Okay.  So, what?  It's like:  "LastPass also performs a large number of rounds of PBKDF2 server-side."  So, okay.  I thought, that's not anything we've talked about or looked about.  So it also wasn't anything that I recalled Joe Siegrist ever mentioning to me.  And since it perfectly responded to the worries we talked about last week, I mean, that's what named last week's episode "1," and since LastPass has now been revealed to be, if nothing else, a bit klutzy, I was actually just a bit suspicious about exactly when this convenient page first appeared on LastPass's website.  So I dipped into the Internet Archive's Wayback Machine to see when it had first indexed this page.  And what I found did little to assuage my suspicion.



In the show notes I have a picture of the Wayback Machine's calendar showing that this page didn't exist before 2022, and in fact it was first indexed by the Wayback Machine on July 2nd of 2022, about six weeks before we learned about this particular problem.  And so, okay.  I was a little suspicious about that.  It's like, oh, isn't that interesting that some good news for this problem was first indexed then.  So, you know, it did precede public disclosure of the trouble.  It's not as if, however, that page had been around for years.



I shared my curiosity over this with Parker, the guy who first brought this to my attention, and I had the intention to do some more digging before today's podcast.  But Parker's curiosity was also piqued, and he tracked down a LastPass PDF document titled "LastPass Technical Whitepaper," which the Internet Archive had indexed and stored on December 18th of 2019, so way longer ago, four years ago.  And looking at the same titled paper today, because there's also something by the same name, "LastPass Technical Whitepaper," if you Google that, it pops right up.



None of the PDFs that were found contain an edition date, but both papers contain exactly the same paragraph, which is more clear than the watered-down help page that I just read.  Both papers, both PDFs, the old one from 2019 and today's, say:  "LastPass also performs 100,100 rounds of PBKDF2 server-side.  This implementation of PBKDF2 client-side and server-side ensures that the two pieces of the user's data - the part that's stored offline locally and the part that's stored online on LastPass servers - are thoroughly protected."



Okay.  So I didn't want to give anyone the wrong impression from last week, so I had to pursue this.  I was curious about anything additional that that particular PDF's metadata might reveal.  I was, again, still like, okay.  I learned that it was created by Microsoft Word 365 by Erin Styles on June 6th of 2022.  And as a quick sanity check, I noted that Erin's Twitter photo shows her proudly sporting a bright red LastPass sweatshirt.



Okay.  So what would the flow of this system look like?  What does it mean for LastPass to be performing apparently another 100,100 iterations of PBKDF2 server-side?  So as we know, to LastPass's client, the user provides their email address, which serves as the salt, and their LastPass master password, which is the input, along with their account's default iteration count to an SHA-256-based PBKDF2 function.  That produces a 256-bit encryption/decryption key which is never sent to LastPass.  Right?  I mean, that's the whole point is LastPass never has that key.  It is only ever used by the local client to encrypt and decrypt the master vault blob of data which LastPass stores for them and shares among the user's various LastPass clients in order to keep them synchronized.



Okay.  So how, then, does LastPass verify that they've logged on using their proper credentials, if that key never goes to LastPass?  Well, after running that local PBKDF2 iterations, one additional round of PBKDF2 is used to produce a 256-bit user logon verification token which is what LastPass calls the "login hash."  That token is sent to LastPass to store and use to subsequently verify the user's proper logon credentials.  LastPass uses the user's ability to provide that login hash token to log them into their online vault and before sending their encrypted vault blob to them.  That's how they avoid sending the vault that does contain a bunch of private unencrypted information to other people.  You've got to be able to provide this login hash.



So the problem that occurred to me was that only a single round of PBKDF2 was separating the user's secret, like super secret, vault decryption key from this login hash, which is what the client provides to LastPass as its authentication.  That meant that in theory it could be easily brute forced by reversing that hash function.  But this apparently also occurred to LastPass.  Although there's language about this in several places in this whitepaper, the clearest description appears near the bottom of page 19 under the title "Login Hash Storage," where they write:  



"LastPass receives the login hash from the user," and they said, "(following the default 100,100 iterations on the user's master password using PBKDF2 SHA-256)," meaning on the client-side.  "The login hash is additionally salted with a random 256-bit salt, and an additional 100,000 rounds of PBKDF2 SHA-256 are performed.  That output is then hashed using Scrypt to increase the memory requirements of brute force attacks.  The resulting hash stored by LastPass is the output of 200,101 rounds of SHA-256 PBKDF2 plus Scrypt."



Okay.  So what they're saying is they recognized the reversibility of the login hash only being one round away from the user's secret key creation.  So when they receive the login hash from the user, that's what they run an additional 100,000, a salted 100,000 rounds of PBKDF2 SHA-256 on in order - it's because they're going to store that permanently, and they don't want there to be any chance of it going backwards to the secret key.  So I needed to pursue this all the way out to the end to understand whether, as I was hoping, LastPass might have used all of this additional, and again, as I said, they talk about it all over this whitepaper, server-side PBKDF2'ing - even going so far as to deploy Scrypt - to create a super-strength encryption key for use when saving their users' vault data.  But there was no sign of that anywhere in the whitepaper.



LEO:  So for my understanding, they're protecting it from a bad guy trying to log in as you.  But if the bad guy's got the vault, it's independent of that.  Is that right?



STEVE:  Correct.  That's exactly right.



LEO:  And you mentioned Scrypt, but I want to make it clear because of the way you said it, there's no evidence they used Scrypt.  They used PBKDF2; right?  They didn't mention...



STEVE:  Actually, they're saying they used both.



LEO:  Oh, good.  But not on the right part.



STEVE:  Well, correct.  Exactly.  Exactly.  So what they get from the client is they don't get the secret key ever.  But they do one more round of PBKDF2 on the client, and the client sends them that, which they call the "login hash."  So that's their token is that the client uses to verify the login.  And since they're going to be storing that for the long term, they realized, oh, this is not safe for us to store because it's only one PBKDF2 iteration away from the key.



LEO:  Right.



STEVE:  So let's hash the crap out of this, and that's what we'll store.  So anyway, my hopes were dashed.  For a while I was thinking that they were doing something really cool, that they were going to take what they got from the user, hash the crap out of it, and then reencrypt the vault under that next-generation key, in which case people with an iteration of one on their client...



LEO:  Would still be okay.



STEVE:  ...would have been protected, yes.



LEO:  But they're not protecting that vault.  They're just protecting the login.



STEVE:  Correct.  And in fact there is a place where they specifically say, under a section of their page "User Data Storage," they say:  "Sensitive vault data is encrypted client-side, then received and stored by LastPass as encrypted data."  So I was thinking for a minute that I might have steered everybody wrong a week ago, that one iteration meant one iteration.  Well, unfortunately, it does mean one iteration.  And things are as bad as we thought last week.



LEO:  As good as we thought.  This is part of the problem with all of this is the marketing.



STEVE:  Yes.



LEO:  And there's a lot of hand-waving.  Different password managers have different amounts of hand-waving.  But the marketing department, which probably doesn't understand what it's saying, usually takes that the technical people have given them, does some magic hand-waving, and what we get is often not very useful.  Look at the digging you had to do for this.



STEVE:  When we talk about - we're going to be talking about Norton LifeLock problems in a minute.



LEO:  Oh, this one, yeah.



STEVE:  Oh, and this is exactly to the point you're just making.  At one point they said:  "We have secured 925,000 inactive and active accounts that may have been targeted by credential stuffing attacks."  Uh, you secured them?  What does that mean?



LEO:  No, they sent you an email.



STEVE:  What?  What?



LEO:  Anyway, we'll get to that.



STEVE:  Yeah, okay.



LEO:  There's a lot of that.  And in fact...



STEVE:  I feel so much better.



LEO:  I'm trying to do the same thing.  I'm not as knowledgeable or as adept as you.  So I'm trying to filter through some of this marketing material, too, from other password managers, to see what's going on.  I do notice that when I rekeyed my Bitwarden vault, I gave it a new password, they said you can rekey it, you can regenerate a secret, but that's going to mean we have to do a bunch of stuff.  It's going to take a little while.  And make sure you log out everywhere because once we do that we could corrupt your data.  You could corrupt your data if you haven't logged out.  And we'll log you out, but just make sure we did successfully.  What is that?



STEVE:  Okay.  That's a really good point.  And it's a subtlety that I was deliberately skipping over, just because I didn't want to really just boggle people's brains.



LEO:  Oh, boggle us, Steve, boggle us.



STEVE:  There is the key that you use for decryption, and there's the key that you use for decrypting the key.



LEO:  Okay.



STEVE:  So all of this that we've been talking about in LastPass, for example, where you're doing this PBKDF2 to get a key, that key that you get isn't the actual decryption key for the vault.  It's the key that decrypts the key.  So there's a level of indirection there.  And the reason that's done is that you're able to change your encryption key without having to rekey the vault.  And thanks to that level of indirection.  It's very much like if you had a password protecting a hard drive, and you wanted to change the password.  Well, if you actually change the key, you have to decrypt... 



LEO:  You have to reencrypt everything.



STEVE:  ...the whole hard drive and then reencrypt the whole hard drive.  So instead, nobody does that.  You assume that the actual key was achieved through a very high-entropy, pseudorandom, hopefully actually random, bit generator, and then that's the thing that's encrypted by the key that you use.



LEO:  Right.



STEVE:  And then decrypted when you want to use it.  And so that's also what you were seeing with Bitwarden where they said, well, we could actually rekey your vault.



LEO:  I did that.  I did both, yeah.  Why, I don't know.  I mean, I just thought I would just to see what happened.



STEVE:  Sounds good.



LEO:  Then the other thing a lot of people - no harm done.  No harm done.  Then something a lot of people mentioned is that Last Password has an additional key that they generate that is stored, as I understand it, in the hardware or...



STEVE:  I think you mean 1Password?



LEO:  Did I say Last Password?  I meant 1Password, yeah.



STEVE:  Yes.



LEO:  In their Agile Keychain.  And it's actually a file, you get it as a file that you store somewhere, and then you use as a secondary encryption.  And, now, correct me if I'm wrong because I was talking because I was talking about this on our Ask the Tech Guys show, the new show we do to replace the radio show, Mikah and I, and I said, that's great if you don't have a good master password because then there'll be a backstop for a bad master password.  But if you have a very, very good master password, I mean, something that's impossible to break, isn't more impossible to break with a second master password; right?  I mean, one would be good enough.  It does add some complexity and confusion because people have to save this key as a file, and they have to put it on - you know.  So it's great.  And I said, if you're going to have naive users use a password manager, and you think they might use monkey123 as their master password, this would be good.



STEVE:  Yes.  Because what it means is that the actual password is the composite of some true high-entropy file and monkey123.



LEO:  Yes.



STEVE:  So that if the vault ever escaped, then there's just no point in trying to do any sort of, you know.



LEO:  It's another wall.



STEVE:  Yes.



LEO:  But my position is, well, that's fine, and if you're worried you could do that.  But if you have a good master password, which you should, long, random master password, and you're going to talk about what random means in a bit, that's probably belt and suspenders.  It's not needed.



STEVE:  I agree.  So, again, convenience versus security.  It is technically more secure, but a lot less convenient.  And the point is at what point does more security not actually buy you anything?  Right?  You know, where all you're doing is making things way less convenient; but you already, even without that, you already had enough security.  Your point is, if you really use a good random master password, you're done.  There just isn't any need for anything more.



LEO:  Okay.



STEVE:  Especially at the cost of significant inconvenience.



LEO:  Yeah.  So that secret key is nice, but better even - and it cost, by the way, it cost an additional amount, which is I think the real reason they offer it.  You have to pay a subscription fee to get it.



STEVE:  Oh, my god.  Okay.



LEO:  But if somebody is going to use monkey123, then they should be using 1Password to do this.  Or train them in how to make a good password.  Which Steve will do.  I'm sorry.  On with the show.



STEVE:  Okay.  So the other - no, this was all good.  The other question is, ECB or CBC?  That was the other question to be answered this week, thanks to the past week's worth of listener feedback from all the people who used Rob Woodruff's PowerShell script to peer into their vaults.  What we wanted to find out was about the prevalence of the less desirable ECB encryption mode.



Well, one of our listeners provided a unique insight which simultaneously answered a question I had.  Mark Jones tweeted.  He said:  "You requested updates on LastPass."  He says:  "Regret as a loyal listener that I was stuck at 35,000 iterations, not 100,100."  Still, 35,000's not bad.  He said:  "I found 28 ECB items, mostly secure notes."  He said:  "I now certainly regret putting so much in notes.  A couple of comments," he said.  "Incrementing iterations" - and he actually meant changing iterations, but okay - "changed all to CBC."  He said:  "It appears you're correct in asserting any changes get rid of ECB."



LEO:  That's good to know.



STEVE:  Well, I didn't assert that, so he's giving me credit for something I didn't say.  But thanks.  But this explains the mystery of why my own vault had no instances of ECB.  I know that my vault contained a number of very old account credentials from the beginning of my use of LastPass which would have never been updated, and which I therefore expected to find still encrypted in the original ECB mode.  But what Mark observed was that simply changing his vault's iteration count, as a lot of us did, many of us did five years ago when we were covering this issue on the podcast, that gave our clients the opportunity to reencrypt all of those older ECB mode entries in CBC mode.



LEO:  Good news.  Good news.



STEVE:  So Mark, another mystery solved.  Thank you.



LEO:  Yeah, because I've heard a lot of our listeners, you've heard many more, but I've heard a lot of them say, "I don't have any ECB stuff."  But that would make sense.  That's why, yeah, because a lot of them were long-time, like you and me, long-timer users.



STEVE:  Yup.  Okay.  So while we're on the topic of password manager troubles, we should touch on Norton LifeLock and more doublespeak being produced by corporations that have grown too big to need to care.  Before we get to this, we should note that what was formerly Symantec Corporation and Norton LifeLock are now renamed Gen Digital.  And they just refer to themselves as Gen.  So now they're both living at GenDigital.com.  And since Symantec had been acquiring companies over the past few years, this new Gen Digital is now operating the brands Norton, Avast, LifeLock, Avira, AVG, Reputation Defender, and unfortunately CCleaner.  Which, you know, was a beloved tool for a long time.



Okay.  So what happened this time?  Around 6,450 Norton LifeLock customers were recently notified that their LifeLock accounts had been compromised.  And by compromised, we mean that unknown malicious parties have somehow arranged to log into those 6,450 accounts, giving them full access to their users' password managers' stored data.  Whoops.  That's not good.  In a notice to customers, Gen Digital, as I said, the recently renamed parent of this collection of companies, said that the likely culprit was a credential stuffing attack as opposed to a compromise of its systems.



Now, this seems very odd since Gen Digital explained that by this they meant that previously exposed or breached credentials were used to break into accounts on different sites and services that share the same passwords.  But that wouldn't explain why there was apparently what appears to be a quite successful targeted attack against 6,450 of their users.  If these were username and password credentials leaked and/or somehow obtained from other unrelated site breaches elsewhere, how is it that they just happened to all be useful against 6,450 of LifeLock's account holders?  You know, that doesn't smell right.



What I suspect actually happened is that LifeLock's web portal is, or was, lacking in brute-force password guessing protection.  In this day and age, it is no longer okay for a website to allow a fleet of bots to pound away on its login page at high speed, hoping to get lucky.  BleepingComputer also covered this news, and they posted a statement from Gen Digital spokesperson, which is the one I quoted, saying:  "We have secured 925,000 inactive and active accounts that may have been targeted by credential-stuffing attacks."



Okay.  So wait.  First of all, 925,000 accounts.  And, quote, "We have secured them."  Does anyone know what that means?  What does it mean, "We have secured 925,000 inactive and active accounts that may have been targeted by credential-stuffing attacks."



LEO:  Maybe they reset the password.  Could it be?



STEVE:  Oh, my god, they could not have done a password reset on a million.



LEO:  A million, no.



STEVE:  I mean, it would be the end of life as we know it.



LEO:  So they probably just sent an email out.



STEVE:  Oh, my god.  Yeah, maybe that's, oh, we had to let everybody know that they might be hacked.  So we were going to call that "securing the accounts."  Oh, lord.  Anyway, you know, I guess whatever they did, it's much better than if they had not secured them.  So since we're not actually offered any information, Leo, to your point, we don't get any these days, we just get corporate PR speak, it's necessary to read between the lines.



So my guess would be that Gen Digital currently has 925,000 Norton LifeLock customers, or accounts, at least, some inactive.  They wandered off.  And due to completely absent web portal security and a lack of monitoring for some length of time, once bad guys realized that there was no security to stop them, a fleet of bots was programmed to assault Norton LifeLock's login page, guessing account credentials at high speed without limit.  And as a result of this lack of security, that fleet of bots was able to successfully login and compromise the accounts of 6,450 Norton LifeLock users.



Gen Digital did admit that it had discovered that these intruders had compromised LifeLock accounts beginning on December 1st, 11 days before they're saying its systems, that is, Gen Digital's systems, finally detected a "large volume" of failed logins to customer accounts.  Well, that large volume would have been going on for weeks; right?  But somehow they didn't see that.  The red flag finally went up on December 12th when LifeLock became aware of it and presumably brought this attack to a halt.  So I suppose that's what they meant when they said that they had "secured" those 925,000 active and inactive accounts.  They basically halted an ongoing login attack after 6,450 successful logins and full account compromise of those customers.



LEO:  When I read this, I thought, oh, it's a credential-stuffing attack.  I didn't realize that they were being able to brute force attack it.  I just thought they were copying passwords from other breaches and trying them on LifeLock.



STEVE:  When they say that they, on the 12th, a large volume of failed login attempts...



LEO:  But that could also be credential stuffing because you don't know who's reused passwords.  So you might have a database of 10 million...



STEVE:  Oh, oh, I completely agree.  By "brute force" I don't mean start at 0000000. 



LEO:  Oh, okay, yeah, yeah.



STEVE:  I just mean let's try all the passwords.



LEO:  Let's see if this password that we have works.



STEVE:  Yeah.



LEO:  Yeah, yeah.



STEVE:  Right, yeah.  Let's ask Troy Hunt for his master list.  We'll check them all.



LEO:  That's right, yeah.  So it's not, in a way, that would - I don't - I have no reason to defend them.  But this is why you don't reuse passwords, because of these credential things.



STEVE:  It's absolutely why you don't.  But also it took them at least two weeks before they saw - before something happened.  So okay.  So here's my theory.  They're not saying how much earlier the attack was underway.  So why 11 days from first successful compromise to first detection of an attack that had been ongoing for some time?  If I had to guess, I would suggest that the bot fleet's attack was probably carefully throttled so as not to trip any alarms.  And that after some successful undetected logins, the bot fleet's operators may have started creeping its attack rate upward slowly.



LEO:  They got greedy.



STEVE:  Yeah, exactly, to see how much faster they could go.  And remember that before they were shut down they'd successfully scored against 6,450 accounts.  I mean, that's a lot of accounts.  So things had been going well for the attacking fleet for like 11 days at least from the first known compromise.  And truth be told, we don't even know that they actually ever were detected.  We don't know that's what tripped the alarm and raised the red flag.



Given that the bots had been stomping around within 6,450 of LifeLock's user accounts, I would be surprised if some user out of 6,450 didn't notice that something was amiss and contact Norton to report suspicious account activity.  So it may have just been, you know, the fact that they were tipped off by a victim, and they thought, oh, what?  Maybe we ought to go over and look at that web server.  And it's like, oh, my god.  And then they, you know, did whatever they said they did to secure all the accounts.



Anyway, this does highlight another good reason for choosing an iteration count that takes the web browser a few seconds to obtain a go/no go login decision because it also serves as very good brute force protection against login attempts to your provider's portal; right?  In order to log into LastPass.com, to use that example, some script has to be run on the browser in order to churn away at a guest password.  I mean, that length of time has to be spent in order to create a token to hand to LastPass to say please log me in.  So again, high iteration counts are just all-around protection from many types of problems.



Okay.  One last thing, then we'll take our second break.  Chrome has followed Microsoft and Firefox.  Remember the certificate authority TrustCor?  Last December we spent some time looking at the bizarre set of corporate shells and various shenanigans being played by that very shady certificate authority, TrustCor.  And at the time it was difficult to understand why anyone would trust those clowns, based on the history that was revealed.  Remember that one of the corporate users of their certificates was a deep packet forensics entity that was found to be selling their TLS middleboxes to agencies of the U.S. federal government.  And there was also that individual and private company that was also affiliated whom no one had ever heard of before, who had for some reason received that huge block of previously unused U.S. Department of Defense IPv4 allocation.  You know, there was something really fishy about this whole business.



Anyway, finally, in response to a long dialogue with a company representative that convinced no one, Mozilla and Microsoft both marked TrustCor's root certs as untrusted, thus immediately revoking trust from any certificates that had been signed by those certificate authority certs.  We're talking about this again because, as I said, Google and Chrome have now followed suit by removing TrustCor's CA certs from Android and Chrome's root stores.  So it's pretty much game over for that group.  And good riddance.  As I've said, you know, the responsibility - I've always been impressed that the browsers are so reticent and like really reluctant to pull trust from a CA.  You've got to really be bad in order to have that happen.



LEO:  It's going to break a lot of things.  That's why they don't want to do it.



STEVE:  Oh, it's going to break everything that that CA ever signed.



LEO:  Right.  So they don't want the calls.  It's expensive.



STEVE:  Right.



LEO:  All those tech support calls are expensive.  And we were talking last week, and I made a completely new, very good, long - and I used Password Haystacks to pad it out, long password.  And I rekeyed it.  And I turned the PBKDF2 iterations up to two million, and I just - I didn't need to do any of that, but I just - it makes me feel a little bit better.  Now, the one thing I still have to do is go through all my passwords and update them because who knows how many were in my LastPass.



STEVE:  Boy, and that's one thing I heard from so many of our listeners is what a pain in the butt it is to change passwords.



LEO:  Well, and as you said last week, there should be a way to do this.  There should be an API.  There should be something; right?  But there isn't.  There's no standard way to do it.



STEVE:  Okay.  So Chromium is beginning to Rust.  Google's announcement and blog posting last Thursday is titled "Supporting the Use of Rust in the Chromium Project."  They wrote:  "We are pleased to announce that, moving forward, the Chromium project is going to support the use of third-party Rust libraries from C++ in Chromium."  That is, libraries called from C++.  "To do so, we are now actively pursuing adding a production Rust tool chain to our build system.  This will enable us to include Rust code in the Chrome binary within the next year.  We're starting slow and setting clear expectations on what libraries we will consider once we're ready.  Our goal in bringing Rust into Chromium is to provide a simpler and safer way to satisfy the Rule of Two."



And I actually skipped over the fact at the top of the show that that is today's podcast title, "The Rule of Two," which we'll be talking about here in a minute.  So they said:  "Our goal in bringing Rust into Chromium is to provide a simpler and safer way to satisfy the Rule of Two," whatever that is, "in order to speed up development - less code to write, less design docs, less security review - and improve the security, meaning increasing the number of lines of code without memory-safety bugs, decreasing the bug density of code, of Chrome."  And they said:  "And we believe that we can use third-party Rust libraries to work toward this goal."



And they finished:  "Rust was developed by Mozilla specifically for use in writing a browser, so it's very fitting that Chromium would finally begin to rely on this technology, too.  Thank you, Mozilla, for your huge contribution to the systems software industry.  Rust has been an incredible proof that we should be able to expect a language to provide safety while also being performant."  And god, I hate that word "performant."



LEO:  I know.



STEVE:  It just seems - it's like, is it Apple who talks about the learnings?



LEO:  No, Microsoft is learnings, yeah.



STEVE:  Oh, Microsoft is learnings.



LEO:  The whole tech industry has its own vocabulary, and it's the business vocabulary.



STEVE:  Yeah.  Anyway, everyone listening to this podcast has heard me lament that we're never going to get ahead of this beast of software flaws if we don't start doing things differently.  What was that definition of insanity?  Anyway, it's great news that the Chromium project is taking this step.  It will be a slow and very evolutionary move to have an increasing percentage of the Chromium codebase written in Rust; but this is the way that effort, and this eventuality, gets started.  You know, you've got to start somewhere.



And you may have noted, as I said, that Google's announcement mentioned this Rule of Two, which we'll be taking an in-depth look at here in a minute.  But first we have another instance of BYOVD, Bring Your Own Vulnerable Driver.  It was just in the news this past week.  "CrowdStrike documented their observation and interception of an eCrime" - I hadn't seen that term before, now we have eCrime - "eCrime adversary known variously as Scattered Spider, Roasted 0ktapus, and UNC3944," for those who are not very imaginative.



"This leverages a combination of credential phishing and social engineering attacks to capture one-time password codes or to overwhelm their targets using that multifactor authentication notification fatigue" that we were talking about before, where they just finally say, okay, fine, I don't know why I'm being asked, but fine.  And then, you know, that lets the bad guys in.  Once the bad guys have obtained access, the adversary avoids using unique malware which might trip alarms.  Instead, they favor the use of a wide range of legitimate remote management tools which allows them to maintain persistent access inside their victims' networks.



CrowdStrike's instrumentation detected a novel attempt by this adversary to deploy a malicious kernel driver through an old and very well-known eight-year-old vulnerability dating from 2015 which exists in the Intel Ethernet diagnostics driver for Windows.  That is, a legitimate driver published in 2015 by Intel for performing Ethernet diagnostics.  The file is "iqvw64.sys."



The distressing factor is that the technique of using known-vulnerable kernel drivers has been in use by adversaries for several years and has been enabled by a persistent gap in Windows security.  And we've talked about this before.  Starting with the 64-bit edition of Windows Vista, and ever since, Windows does not allow unsigned kernel-mode drivers to run by default.  That was easy to do; right?  It just shut down one path of exploitation.  But clever attackers started bringing their own drivers that were signed, like legitimately signed, like Intel signing this Ethernet diagnostics driver.  They would bring that along and cause Windows to install it, and then exploit the known vulnerabilities in this driver.



Okay.  So what do we do about that?  Well, in 2021, about two years ago, Microsoft stated that - this is Microsoft:  "Increasingly, adversaries are leveraging legitimate drivers in the ecosystem and their security vulnerabilities to run malware," and that "drivers with confirmed security vulnerabilities will be blocked on Windows 10 devices in the ecosystem using Microsoft Defender for Endpoint Attack Surface Reduction (ASR) and Microsoft Windows Defender Application Control (WDAC) technologies to protect devices against exploits involving vulnerable drivers to gain access to the kernel."



Well, that was the plan.  How did that work out?  As we discussed some time ago, multiple security researchers through the years, the past two years have repeatedly and loudly noted  pounding on Microsoft  that this was all apparently just feel-good nonsense spewed by Microsoft, and that the issue continues to persist as Microsoft continually fails to actually block vulnerable drivers by default.



The crux of the problem appears to be that any such proactive blocking requires proaction from Microsoft.  And any fair weighting of the evidence, many examples of which we've looked at during the past two or three years here, would conclude that Microsoft has long since abandoned any commitment to true proactive security.  They no longer find most of their own problems in Windows.  They increasingly rely upon the good graces of outside security researchers, and even then they drag their feet over implementing the required updates, which have been handed to them by the security community.



Okay.  That said, though, being fair to Microsoft, there is a flipside to this.  We know that the last thing Microsoft ever wants to do is deliberately break anything.  They have enough trouble with inadvertently breaking things, let alone doing so deliberately.  So proactively blacklisting anything, especially something like a network driver that could potentially cut a machine off from its own network, well, that's the last thing that Microsoft would choose to do, assuming that a choice was being made in the first place.  But as I was thinking about this, it occurred to me that this is something that a third party could do on behalf of users who subscribe.



I'm bringing this back up again because this was not supposed to happen.  The last time we spoke about this last year, the presumption was, based upon clear statements made by Microsoft at the time, that this had all been a big oversight mistake for several years, and that all that was going to be better now.  But this latest news is from last week, and this was not actually fixed.



Okay.  So there's that.  But the bigger point I want to make is that this all needs to be made proactive.  Somebody needs to be proactive.  The need for Microsoft to be proactively blocking known vulnerable drivers seems like something we are never going to be able to get from Microsoft.  For the foreseeable future, at least, this appears to no longer be in their DNA.  That's just not who they are any longer.  And we've seen cycles; right?  They've, like, swung from one side to the other.  Maybe they'll swing back.  We can hope.  But that's not where they are today.



But, you know, it occurred to me that this presents a huge and significant opportunity for some third-party security company.  Solve this problem.  Be proactive in a critical area where Microsoft refuses to be.  It's a simple thing to do, like get the list of all the previously known vulnerable drivers.  Create an app that looks to see if any of them are in use.  If so, tell your subscriber to update the driver to a new one so that it can then be blocked from malicious future use.  Anyway, my sense is there's a big opportunity here for someone to make themselves a lot of money by closing what is a persistent and gaping hole in Windows security.  I hope somebody will do it.



Okay.  A piece of closing-the-loop feedback.  My Twitter feed was so overwhelmed with feedback from last week's call for feedback that I was unable to reply to tweets as I usually try to do.  So to everyone who tweeted, please accept my thanks for the feedback and my apology that I probably didn't reply.  I know that replying is not required, but courtesy and chivalry are not dead here.  As many of my regular Twitter correspondents know, I often reply when I can.



But one public tweet caught my eye for its cleverness, which I wanted to share.  Somebody tweeting as @mammalalien tweeted:  "@SGgrc Sometimes it is hard to picture just how many more random strings there are than English words.  So try this."  Sort of a thought experiment.  "How often have you ever seen real words coincidentally appear in randomly generated passwords?"  He said:  "I noticed it today for I think the second time ever."



So anyway, I think that's a very clever and worthwhile observation.  It helps us to truly appreciate how much less entropy exists in non-random text, which we recognize as words in our language, such that it almost never appears by chance.  I don't think I've ever noticed any significant word - maybe "is" or something - in truly random strings of characters.  So anyway, I just thought that was a cool observation.



Okay.  The Rule of Two.  I'm going to get into this, then we'll take our third break, and then we will continue.  The Rule of Two.  In their posting about the adoption of Rust by Chrome, Google mentioned something they called "The Rule of Two."  In Google's official Chromium Docs they explain what this so-called "Rule of Two" is.  They said:  "When you write code to parse, evaluate, or otherwise handle untrustworthy inputs from the Internet, which," they said, "is almost everything we do in a web browser, we like to follow a simple rule to make sure it's safe enough to do so."



They said:  "The Rule of Two is pick no more than any two of untrustworthy inputs, unsafe implementation language, and high privilege."  And just to highlight the point on the page, under the Chromium Master Docs where they show this, they give us a Venn diagram where one circle is "Code which processes untrustworthy inputs" is one circle.  "Code written in an unsafe language," and they show C and C++ as examples, that's another Venn diagram circle.  And then the third one is "Code which runs with no sandbox," for example, in the main browser process.  And so they've got all three circles with overlapping regions.  And in the center, where all three circles overlap, they've got in big red all caps "DOOM!  Don't do this."  That is, do not operate in that place where all three of these things are true:  untrustworthy, unsafe language, unsandboxed execution.



Now, of course this is reminiscent of that great old saying that in a project, you can choose any two, but only two of the following three outcomes:  good, fast, or cheap.  You can't have all three.  You'll need to sacrifice the thing that's least important to you among them.  You can have something good and fast, but then it won't be cheap to get it.  Or you can have something fast and cheap, but then it won't also be good.  Or you can have something good and cheap, but then you won't be able to get it fast.  So similarly, for Google's Rule of Two, untrustworthy inputs, unsafe implementation language, and high privilege.



LEO:  Can we just not have any of the above?  I would like to, I mean, why do we have to pick two?



STEVE:  Well, yeah, wouldn't that be nice.



LEO:  None of the above.



STEVE:  On the other hand, our browser is going out in the Wild West of the Internet; right?  I mean, you might go to a site.ru, and god help you.  Who knows what your browser is going to pick up?  Okay.  So let's take a look at these.  Leo, let's tell us about our last sponsor, then we can dig into...



LEO:  The Venn diagrams.



STEVE:  The Venn diagram.



LEO:  Seems to me if you're going in the Wild West, you don't want any of those, let alone two of them.  Okay.  But I guess it's hard to do in real-world environments.



STEVE:  We do have people who have a separate machine running only a browser, and it's like it's off their network, and that's what they use.  It's like isolated in order to be...



LEO:  A lot of unsophisticated people, because I've recommended on the radio show for years get a Chromebook for your banking and just do that.  And a lot of people like that idea.  Like, good, because then I can use my Windows machine as I wish, but I'll know that I'm pretty safe when I'm doing my banking.  



STEVE:  Right.



LEO:  You know, I don't think that's an unusual idea.



STEVE:  Not a bad idea.  And the Chromebook has the wash button, too.



LEO:  The power wash, yeah.  And it of course checks for signed firmware so you can't - it's harder to do bad things to it.  All right.  Back to Steve and two things.  



STEVE:  Yes.  So we're going to walk through this piece where Google is explaining the Rule of Two.  And this was actually written as advice and guidance for would-be Chrome developers.  So Google explains:  "When code that handles untrustworthy inputs at high privilege has bugs" - so untrustworthy inputs at high privilege and bugs - "the resulting vulnerabilities are typically of critical or high severity."  They said:  "We'd love to reduce the severity of such bugs by reducing the amount of damage they can do by lowering their privilege, avoiding the various types of memory corruption bugs by using a safe language, or reducing the likelihood that the input is malicious in the first place by asserting the trustworthiness of the source."



So they said:  "For the purposes of this document, our main concern is reducing, and hopefully, ultimately eliminating bugs that arise due to memory unsafety."  They said:  "A recent study by Matt Miller from Microsoft Security states that 'around 70% of the vulnerabilities addressed through a security update each year continue to be memory-safety issues.'  A trip through Chromium's bug tracker will show many, many vulnerabilities whose root cause is memory unsafety."  They said:  "As of March 2019, only about five out of 130 public critical-severity bugs are not obviously due to memory corruption."  Only five out of 130 were not memory corruption.



They said:  "Security engineers in general, very much including the Chrome Security Team, would like to advance the state of engineering to where memory-safety issues are much more rare.  Then we could focus more attention on the application-semantic vulnerabilities.  That would be a big improvement."



Okay.  So it's clear that the historic use of C and C++ has been the source of a great many past security vulnerabilities despite the coders of those languages doing the very best jobs they can.  There is no date on this document, but it feels a few years old since they were citing Matt Miller's well-known research, which we've cited before here, from March of 2019.  From what was written above, it's also clear that they were wishing and hoping then for the move that Google announced just this past week with the formal incorporation of Rust as a first-class Chromium implementation language.



Okay.  So let's flesh out each of these three factors in the context of browser implementation.  First, untrustworthy inputs.  They explain that untrustworthy inputs are inputs that, A, have non-trivial grammars, meaning a complex language to figure out; and/or come from untrustworthy sources.  So Google explains:  "If there were an input type so simple that it was straightforward to write a memory-safe handler for it, we wouldn't need to worry much about where it came from for the purposes of memory safety because we'd be sure we could handle it.  We would still need to treat the input as untrustworthy after parsing it, of course."  But they're saying, you know, if the grammar is simple, parsing is not a problem.



So they said:  "Unfortunately, it is very rare to find a grammar trivial enough that we can trust ourselves to parse it successfully or fail safely.  Therefore, we do need to concern ourselves with the provenance of such inputs."  So in other words, the stuff a browser confronts is both typically a highly complex language such as HTML, CSS, or JavaScript, all of which have unfortunately evolved to be incredibly complex.  And we don't know about their source, such as some third-party advertising server serving ads submitted by unknown entities.  None of this can be trusted.  If the language used to express these things was simple, it would be easier to trust that our parsing of the language would be enough protection for us.  But unfortunately, these are not simple languages.



Okay.  So Google said:  "Any arbitrary peer on the Internet is an untrustworthy source, unless we get some evidence of its trustworthiness," they said, "which includes at least a strong assertion of the source's identity."  They said:  "When we can know with certainty that an input is coming from the same source as the application itself, for example, Google in the case of Chrome, or Mozilla in the case of Firefox, and that the transport is integrity-protected over HTTPS, then it can be acceptable to parse even complex inputs from that source."  They said:  "It's still ideal, where feasible, to reduce our degree of trust in the source, such as by parsing the input in a sandbox."  And we'll be talking about that in a second.



Okay.  So that was untrustworthy inputs.  What about implementation language?  Google explains:  "Unsafe implementation languages are languages that lack memory safety, including at least C, C++, and assembly language.  Memory-safe languages include Go, Rust, Python, Java, JavaScript, Kotlin, and Swift."  And then they said:  "Note that the safe subsets of these languages are safe by design, but of course implementation quality is a different story."



Okay.  So what about unsafe code in safe languages, which there are often provisions for?  Google said:  "Some memory-safe languages provide a backdoor to unsafety, such as the unsafe keyword in Rust.  This functions as a separate unsafe language subset inside the memory-safe language.  The presence of unsafe code does not negate the memory-safety properties of the memory-safe language around it as a whole, but how unsafe code is used is critical.  Poor use of an unsafe language subset is not meaningfully different from any other unsafe implementation language."



So they said:  "In order for a library with unsafe code to be safe for the purposes of the Rule of Two, all unsafe usage must be able to be reviewed and verified by humans with simple local reasoning.  To achieve this, we expect all unsafe usage to be three things:  Small, so the minimal possible amount of code to perform the required task; second, encapsulated.  All access to the unsafe code is through a safe and understood API.  And third, documented.  All preconditions of an unsafe block, meaning a block of code, for example, a call to an unsafe function, are spelled out in comments, along with explanations of how they're satisfied."  So in other words, they said, where a safe language such as Rust provides facilities for breaking out of safety in order to address some need, that region of unsafety must be small, contained, completely understood, and well documented.



So they continue:  "Because unsafe code reaches outside the normal expectations of a memory-safe language, it must follow strict rules to avoid undefined behavior and memory-safety violations, and these are not always easy to verify.  A careful review by one or more experts in the unsafe language subset is required.  It should be safe to use any code in a memory-safe language in a high-privilege context."  Okay.  So there's two of the rules.



"It should be safe to use any code in a memory-safe language in a high-privilege context.  As such, the requirements on a memory-safe language implementation are higher.  All code in a memory-safe language must be capable of satisfying the Rule of Two in a high-privilege context, including any unsafe code that it encapsulates in order to be used or admitted anywhere in this project."  Okay, so that was interesting.  I mean, they're like saying we're following these rules.  If you can't satisfy the Rule of Two, the code is not coming into Chromium.



Okay.  So finally, code privilege.  That's the third item from which we can only have two.  So Google explains:  "High privilege is a relative term.  The very highest privilege programs are the computer's firmware, the bootloader, the kernel, any hypervisor or virtual machine monitor, and so on."  They said:  "Below that" - you know, below those very top ones - "are processes that run as an OS-level account representing a person.  This includes the Chrome browser process."  They said:  "We consider such processes to have a high privilege.  After all, they can do anything the person can do, with any and all of the person's valuable data and accounts.



"Processes with slightly reduced privilege include, as of March 2019, the GPU process and, hopefully soon, the network process."  They said:  "These are still pretty high-privilege processes.  We are always looking for ways to reduce their privilege without breaking them.  Low-privilege processes include sandboxed utility processes and renderer processes with site isolation, which is very good, or origin isolation, which is even better."



Okay.  So Google then talks about two topics that we've discussed through the years as we've observed over and over how difficult they appear to get right, parsing and deserialization.  Remember that deserializing is essentially an interpretation job, and interpretation is notoriously difficult to get correct because it appears to be nearly impossible for the coder of the interpreter, who inherently expects the input to be sane, to adequately handle inputs that are malicious.



So Google says:  "Turning a stream of bytes into a structured object" - that's the deserialization - "into a structured object is hard to do correctly and safely.  For example, turning a stream of bytes into a sequence of Unicode code points, and from there into an HTML Document Object Model tree, with all of its elements, attributes, and metadata, is very error prone.  The same is true of QUIC packets, video frames, and so on.  Whenever the code branches on the byte values it's processing, the risk increases that an attacker can influence control flow and exploit bugs in the implementation.  Although we are all human, and mistakes are always possible, a function that does not branch on input values has a better chance of being free of vulnerabilities."  And then they say:  "Consider an arithmetic function, such as SHA-256, for example."



And I thought that was a really interesting observation.  We made SHA-256 branch-free so that differing code paths would not leak timing information and would not leave sniffable hints in our processor's branch prediction history.  But a side effect of that also increased the algorithm's robustness against deliberate code path manipulation because there is none.  So anyway, what surprised me a bit is that the Chromium security team, as I said, is extremely literal about the application of this Rule of Two.  They're not joking around.  They actually apply the rule when evaluating new submissions.



And they wrote some advice to those who would submit code to the Chromium project.  They said:  "Chrome Security Team will generally not approve landing a new feature that involves all three of untrustworthy inputs, unsafe language, and high privilege.  To solve this problem, you need to get rid of at least one of those three things.  Here are some ways to do that."



Okay.  Privilege reduction, obviously one of the three things.  They said:  "Also known as sandboxing, privilege reduction means running the code in a process that has had some or many of its privileges revoked.  When appropriate, try to handle the inputs in a renderer process that is isolated to the same site as the inputs came from.  Take care to validate the parsed processed inputs in the browser, since only the browser can trust itself to validate and act on the meaning of an object.  Or you can launch a sandboxed utility process to handle the data, and return a well-formed response back to the caller in an Inter-Process Communications message."



So, okay.  These ideas are structural means for creating an arm's-length, essentially a client-server relationship where a low-privilege worker process does the unsafe work and simply returns the results to the higher privilege client.  That way, if something does go sideways, there's containment within the process that cannot do much with its malicious freedom because it doesn't actually have much freedom to be malicious with.  As for verifying the trustworthiness of a source, they say that if the developer can be sure that the input comes from a trustworthy source - so not overtly attempting to be malicious - it can be okay to parse and evaluate it at high privilege in an unsafe language, even though that seems scary.  In this instance, they say, a "trustworthy source" means that Chromium can cryptographically prove that the data comes from a business entity that can be or is trusted, for example, in the case of Chrome, coming from one of the Alphabet companies.



Google then talks about ways to make code safer to execute under the title of "Normalization."  Writing to would-be Chromium coders, they explain.  They said:  "You can 'defang'" - literally, their term - "'defang a potentially malicious input by transforming it into a normal or minimal form, usually by first transforming it into a format with a simpler grammar."  They said:  "We say that all data, file, and on-the-wire formats are defined by a grammar, even if that grammar is implicit or only partially specified, as is so often the case."  They said:  "For example, a data format with a particularly simple grammar is" - and they have an internal data structure, SkPixmap, which basically is a simple image pixel map.



They said:  "This 'grammar' is represented by the private data fields:  a region of raw pixel data, the size of that region, and simple metadata which directs how to interpret the pixels."  They said:  "Unfortunately, it's rare to find such a simple grammar for input formats.  For example, consider the PNG image format, which is complex and whose C implementation has suffered from memory corruption bugs in the past.  An attacker could craft a malicious PNG to trigger such a bug.  But if you first transform the image into a format that doesn't have PNG's complexity - in a low-privilege process, of course - the malicious nature of the PNG should be eliminated" - basically defanged, as they said, or purified - "and then be safe for parsing at a higher privilege level.



"Even if the attacker manages to compromise the low-privilege process with a malicious PNG, the high-privilege process will only parse the compromised process's output with a simple, plausibly safe parser.  If that parse is successful, the higher privilege process can then optionally further transform it into normalized, minimal form, such as to save space.  Otherwise, the parse can fail safely, without memory corruption."



So they said:  "For example, it should be safe enough to convert a PNG into this SkBitmap in a sandboxed process, and then send the SkBitmap to a higher privileged process via an interprocess communication.  Although there may be bugs in the interprocess communication message deserialization code and/or this SkBitmap handling code," they said, "we consider that safe enough."



So I think that the interesting message here for those of us who are not writing code for a browser is, first of all, to be thankful that we're not.



LEO:  Yeah, no kidding.



STEVE:  That is, boy, that is not an easy job; and, secondly, to more deeply appreciate just how truly hostile this territory is.  It is a true battlefield.  On this podcast I've often noted that the browser is that part of our systems that we blindly thrust out into the world and hope that it doesn't return encrusted with any plagues.  When this environment is coupled with the insane complexity of today's browsers, it's truly a miracle that they protect us as well as they do.



And all of this is up against the crushing backdrop of the imperative for performance.  Google notes that, they said:  "We have to accept the risk of memory-safety bugs in deserialization because C++'s high performance is crucial in such a throughput- and latency-sensitive area.  If we could change this code to be both in a safer language and still have such high performance, that would be ideal.  But that's unlikely to happen soon."  So this was written several years ago.  Rust was noted earlier, so at least it was on their radar. I wonder whether that might be the right compromise, if it would be possible to move this most risky aspect which they have had to keep up in C++ because in this - this is the pinch point in the performance pipeline for getting the page on the screen.  Could they actually reimplement this in a memory-safe language?  I guess we're going to see.



But for now, it's clear that the reason Task Manager shows us 30 processes spawned when Chrome or Edge launch is for containment.  Low-privilege processes are being created and are given the more dangerous and time-critical, performance-critical tasks to perform.  They're time-critical, so they're written in an unsafe language to go as fast as they possibly can; and they're performance critical.  But they're created in a low-privilege separate process, even though there's some overhead talking back and forth between processes because, if something explodes, you want it to be in a low-privilege container.  They cannot be written in a safe but slow language.  They need C++.  So they're held at arm's length and with their interprocess communications strictly limited to receiving a task to perform and returning the result of that work, like rendering processes, which are notoriously error prone.



These notes for Chromium developers talk a bit more, kind of wistfully, about the idea of using safe languages.  Google writes at the end, they said:  "Where possible, it's great to use a memory-safe language.  Of the currently approved set of implementation languages in Chromium, the most likely candidates are Java on Android only, Swift on iOS only, and JavaScript or WebAssembly," they said, "although we don't currently use them in high-privilege processes like the browser," okay, so meaning that JavaScript and WebAssembly are currently only being used in browser web pages and in browser extensions.  They said:  "One can imagine Kotlin on Android, too, although it is not currently used in Chromium.  Some of us on the Security Team aspire to get more of Chromium in safer languages, and you may be able to help with our experiments."  And of course we know from their announcement last week, Rust has made that move.



Okay.  So at this point no mention of Rust's adoption, and this was a couple years ago.  But we now know that's changing.  This interesting discussion and guidance for would-be Chromium developers concludes by noting that all of this is aspirational and that, unfortunately, even this does not reflect Chromium's current state.  Under the final heading of "Existing Code That Violates the Rule," they write:  "We still have code that violates this rule.  For example, Chrome's Omnibox" - the single URL and search box up at the top of the UI - "still parses JSON in the browser process.  Additionally, the networking process on Windows is at present unsandboxed by default, though there is ongoing work to change that default."



Okay.  So we're seeing an evolution across our industry.  Web browsers have become so capable that they're now able to host full applications.  That has enabled the relocation of those applications from our local desktop to the cloud, and a redefinition of the customer from an owner to a tenant.  I, for one, hate this change.  But those of us who feel this way are dying off, and we're clearly irrelevant anyway.  But neither are my hands completely clean since I'm composing these show notes in Google Docs, which is a stunning example of how well this new system can work.



But the gating requirement for any of this to work and for any of this future to unfold is for our web browsers to survive on the front line of an astonishingly hostile Internet.  No one who's been following this podcast for the past few years could have any doubt of the open hostility that today's web browsers face every time they suck down another page loaded with unknown code of unknown provenance containing pointers to other pages of code with the need to go get, load, and run that code, too.  I mean, it just makes you shudder.



So I'm very glad that Google's Security Team is thinking about the problems they're facing, that they take mitigations such as this Rule of Two as seriously as they do, and that they're finally beginning to migrate to the use of safer languages, which, I'll note, was made possible by Mozilla pioneering this wonderful Rust development language.



LEO:  Is that, do you think, the only one they could use?  Or it's the best?  Or have you even thought about that at all?  I mean, I know you use assembly.



STEVE:  It may be Mozilla's influence.



LEO:  Yeah.



STEVE:  Even though they seem like competitors, there is a lot of cross-pollination...



LEO:  Sure, of course there is, yeah.



STEVE:  ...between Chromium and Firefox.



LEO:  Yeah, yeah.  Very cool, yeah.



STEVE:  And everything we hear about Rust says that it is a serious implementation language.  I mean, like a systems-level systems implementation language.



LEO:  Yeah, yeah.  It's a really interesting language, yeah.  Steve Gibson, once again, has done it.  He's put together two hours of fascinating conversation about the things we care about the most.  Thank you, Steve.  You did it again.



STEVE:  Well, we also found out that LastPass turned out not to be willing to help us.



LEO:  Disappointing.  Disappointing.



STEVE:  I was hoping, I hoped for several hours that that was the case.



LEO:  Oh, well.  You'll find Steve at GRC.com.  That's his website, the Gibson Research Corporation.  There's some good stuff there.  Of course SpinRite, the world's finest mass storage maintenance and recovery tool.  It's available now in version 6.0, proven bug-free over the last 18 years, but soon to be 6.1, also bug free for another 18 years.  You'll find that in process.  But if you buy 6.0 now, you will get 6.1 as soon as it comes out, which should be fairly soon.



STEVE:  I did release Alpha 9.



LEO:  Nice.



STEVE:  Which had a huge slew of new features.  And so we're in the process of getting that tested.  We're getting closer.



LEO:  Good.  Good.  That's a good reason to get it right now.  While you're there you can also get a copy of this show.  Steve has two copies we don't have, a 16Kb audio version for the bandwidth impaired, and transcriptions, handcrafted transcriptions by Elaine Farris, so you can read along as you listen or use them for search.  That's all at GRC.com.  Lots of other great stuff there.  So browse around.



There is a feedback form, GRC.com/feedback, but he also is on the Twitter, as you heard him mention, @SGgrc, which means you can also - his DMs are open.  You also can DM him there.  But he has been swamped lately, so don't expect a personal reply.  He'll do his best.



STEVE:  I like to when I can.



LEO:  I know.  I know you do.  That's why you're not joining us on Mastodon.  It's all right, I understand.



Copyright (c) 2023 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.






GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#907

DATE:		January 24, 2023

TITLE:		Credential Reuse

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-907.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we again address a host of pressing questions.  What other major player fell victim to a credential reuse attack?  What does Apple's update to iOS 16.3 mean for the world?  And why may it not actually mean what they say?  It was bound to happen.  To what evil purpose has ChatGPT recently been employed?  And are any of our jobs safe?  Why was Meta fined by the EU for the third time this year?  And which European company did Bitwarden just acquire, and why?  PBKDF iteration counts are on the rise and are changing daily.  What's the latest news there?  What other burning questions have our listeners posed this past week?  What has Gibson been doing and where the hell is SpinRite?  And finally, what does the terrain for credential reuse look like, what can be done to thwart these attacks, and what two simple measures look to have the greatest traction with the least user annoyance?  All those questions and more will be answered on today's 907th gripping episode of Security Now!, maybe before your podcast player's battery runs dry.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is back, baby.  We're going to talk about the new Apple iOS 16.3 and why hardware keys may be not exactly what they mean.  To what evil purpose has ChatGPT been employed recently?  You might be surprised at its capabilities.  And why Meta was fined by the EU for the third time this year.  Plus then we'll talk about credential stuffing, or credential reuse, as Steve prefers.  That's all coming up next on a thrilling, gripping edition of Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 907, recorded Tuesday, January 24th, 2023:  Credential Reuse.



It's time for Security Now!.  Yay.  The show that performs jiu-jitsu on the hacker community with this guy right here, our sensei, Mr. Steven Gibson.  Hello, Steve.



STEVE GIBSON:  The podcast that shows you the problems you didn't even know you had.



LEO:  You didn't even know.  You were blissfully coasting along.



STEVE:  It answers questions for you that you dare not ask in a dark room.



LEO:  It's our new Q&A format.  I really like it.



STEVE:  And this week we again address a host of pressing questions.  What other major player fell victim to a credential reuse attack?  What does Apple's update to iOS 16.3 mean for the world?  And why may it not actually mean what they say?  It was bound to happen.  To what evil purpose has ChatGPT recently been employed?



LEO:  Uh-oh.		



STEVE:  And are any of our jobs safe?  Why was Meta fined by the EU for the third time this year?  And which European company did Bitwarden just acquire, and why?  PBKDF iteration counts are on the rise and are changing now daily.  What's the latest news there?  What other burning questions have our listeners posed this past week?  What has Gibson been doing, and where the hell is SpinRite?  And finally, what does the terrain for credential reuse look like?  What can be done to thwart these attacks?  And what two simple measures look to have the greatest traction with the least user annoyance?  All those questions and more will be answered on today's 907th gripping episode of Security Now!, maybe before your podcast player's battery runs dry.



LEO:  Well, let's hope.  Let's hope.



STEVE:  Let's hope.



LEO:  Okay, Steve.  I have a camera on you now so we can see your shining face.



STEVE:  Oh, joy.



LEO:  And we can also see the Picture of the Week.



STEVE:  So we have a three-frame cartoon.  The first frame shows somebody sitting in front of what looks like a little laptop, and someone outside the frame, off to the right, is saying, he's advising him:  "Support for Windows 8.1 has ended.  You should stop using it, as you won't get any more updates."  And in the middle frame the guys looks a little confused, and he says:  "Wait.  I don't understand."  And then in the final frame he says:  "You mean I should stop using it just when it became somewhat less annoying?"



LEO:  I've held on long enough.



STEVE:  Uh-huh.



LEO:  Oh, lord, it's true.



STEVE:  Quite nice, as a user of Windows 7, not to be bothered by these...



LEO:  Wait a minute.  You're still using Windows 7?



STEVE:  Works great.  I'm sitting right in front of it right now, yeah.



LEO:  I'm hearing from so many people who are so pissed off about the end of Windows 8.1.  It's like, well, it's working.  Everything's fine.  But you also understand why Microsoft doesn't want to support it forever; right?



STEVE:  I do.  I'm a little annoyed that Chrome and Edge are now both telling me that I have to stop using Windows 7.  I mean, I don't see them that often because I've switched back to Firefox as my main driver now.  It's interesting because I tried to give Edge a real run because I really do like vertical tabs.  And of course I have that with an add-on in Firefox.  But I thought, I'm going to give Edge a try.  And it was a couple weeks ago that it wasn't, like, running some pages correctly.  I mean, it just - the page wasn't working in Edge.  And I just thought, you know, you would think they could at least have their browser work.  But no.  Worked great in Firefox.  Worked fine in Chrome.  Didn't work in Edge.  I thought, okay, fine.



LEO:  Which is weird because it's based on Chromium.  That's very strange.



STEVE:  Well, yeah, but of course they're always trying to do weird things.



LEO:  They've changed it.



STEVE:  I mean, there's all this other crap that you get, like what am I getting points?  I'm getting points.



LEO:  Oh, Bing, Bing rewards.  Give me a break.



STEVE:  I don't want points.



LEO:  Yeah, yeah.



STEVE:  Just give me the page I want to see.  But no, we're not going to give you the page.  But we're going to give you points.  So that's not why I'm here using a browser.



Anyway, PayPal says that it was the victim of a credential stuffing, as is the technical term.  You and I both prefer the term "credential reuse," but we'll get to that.  The credential stuffing attack in early December, according to a data breach notification letter which they filed, PayPal filed with the Maine Attorney General's Office, the incident impacted almost 35,000 user accounts, meaning that malicious actors succeeded in gaining access to nearly 35,000 PayPal user accounts using legitimate login credentials, the credentials for those accounts.  I've got some neat statistics that we'll be talking about later.  After investigating, PayPal said that despite the breaches, they did not find any unauthorized transactions among the breached accounts.



Of course this news, following on the heels of our coverage last week of the credential stuffing breach at Norton LifeLock, led me to take a longer look at the nature of credential stuffing attacks as today's title topic.



LEO:  I don't know if it's related, but GitHub just stopped taking PayPal contributions for their supporter thing that they do at GitHub.  I wonder if GitHub knows something we don't know.



STEVE:  Yeah, why would you not?  One of the things that I did encounter when I was doing some research for the podcast is that many times the credentials are logged as successful, but that's it.  They are compiling them for the future.  And of course that's why they hope to go undetected.  And in fact I make the observation at the end of the podcast that of course all we're aware of are the attacks which have tripped alarms.  There may well be attacks going on that no one is aware of because they're not tripping alarms, yet they're still verifying that they have got credentials now to get into a certain person's account in a certain location.  And how would we know?  So anyway, we're going to be talking about credential reuse at the end of the podcast.



Yesterday, Apple updated their various OSes for phones, pads, watches, home and all.  iOS moved from 16.2 to 16.3.  And with this update Apple makes its new iCloud Advanced Data Protection feature, which was the main topic of our December 13th podcast which we titled "Apple Encrypts the Cloud," now it's nominally available to all users worldwide.  As we discussed then, this potentially gives users the choice to protect almost all of their most sensitive data stored by Apple in the iCloud, offering true TNO (Trust No One) encryption where Apple no longer keeps a set of keys.  Now the user's Device Backups, Message Backups, iCloud Drive, Notes, Photos, Reminders, Voice Memos, Safari Bookmarks, and Siri Shortcuts, oh, and also Wallet Passes, they're all included in encryption where Apple may have no keys.  What's excluded are iCloud Mail, Contacts, and Calendar because those services inherently need interoperability outside of the user's device collection.



But I said Apple "may" have no keys because, as nice as it has been for Apple to finally make this available, it requires every participating device that's logged into the user's iCloud account to be running an iCloud Advanced Data Protection-capable OS, and that turns out to be a rather high bar to clear.  I'm unable to turn it on, and I don't know anyone who has.  And I've  heard you, Leo, say, you know, yeah, okay, but I've got stuff that isn't up to date, so I can't turn it on.



You know, I suppose if all you had was an iPhone and an iWatch, and you routinely rotate them to remain completely current, then it would work.  But many of us who jumped onto the Apple wagon after Blackberry became non-viable - still miss that wonderful keyboard - will have acquired, and probably held onto, a range of iDevices over the years, many of which have stopped receiving updates, even though they continue to be useful.



As I mentioned before, I have a fully functional iPhone 6.  Yup, it's right here sitting next to me.  It still works great.  It has its own phone number.  It's replaced my beloved landline.  And as we know, today's smartphones are actually little pocket computers, so it's handy to have next to me.  And it's a fully participating citizen in my Apple ecosystem.  It's got a headphone jack, which is great for having a lengthy hands-free conversation with headphones and a microphone which keeps Bluetooth and GSM transmitter radiation out of my ear canal.  But this otherwise perfectly workable phone is stuck back on iOS 12.5.6 where it will remain forever.  I also have a 7th generation iPod Touch that's a nice little iDevice.  It's back on 15.7.3.



So anyway, just to be clear, this is not Apple thumbing their nose at us.  This wasn't an arbitrary decision that they made in order to sort of tease people.  As was clear when we discussed the technology which they've had to develop in order to pull this off, they really did need to significantly update and revise iOS in order for it to be able to operate with this new "they have no record of the keys in iCloud scheme."  So the week's news is that iCloud's Advanced Data Protection is now global, and many people will be able to turn it on.  But you have to have devices all able to be up at the latest and greatest.



Okay.  When I saw the headline "ChatGPT Creates Polymorphic Malware," I thought, oh, great.  That's just what we need.  Now we have AI writing malware.  The news is that cybersecurity researchers at CyberArk - and I like that name because maybe they're going to need to have an ark - interacted with OpenAI's ChatGPT to have it create a new strain of powerful polymorphic malware which did successfully manage to avoid all detection.  Oh, great.



LEO:  Wow.



STEVE:  Uh-huh.  I mean, Leo.  You only have to ask for it.  You know, ChatGPT, please write some malware that will avoid all current detection.  Oh, no problem.



LEO:  Presumably it's getting it from other malware strains.  But the fact that it evades detection is fascinating.  It means it is original enough that it doesn't look like its predecessors.



STEVE:  Correct.  So they recently released, the CyberArk guys, a technical write-up explaining that the malware created using ChatGPT could "easily evade security products and make mitigation cumbersome with very little effort or investment by the adversary."  And that's the phrase, "very little effort or investment by the adversary," that is pivotal here.  This story is followed by one that's even more chilling we'll get to obviously in a second.  But it's the leverage that this provides.



I mean, it's like how Rob, Rob said of his LastPass vault decryptor, he wouldn't have done a GUI except that ChatGPT made it so easy, he just sort of thought to - actually I think he said he wouldn't have done it at all if ChatGPT hadn't done it for him, largely.  And then, hey, let's have a GUI.  So, you know, give me a GUI.  And out popped one.  You know, there was a little bit of text overlapping problem, but boohoo, just tweak some positioning of the GUI.  So anyway, this is significant.  We have a couple more things we'll be talking about ChatGPT relative to here.



Anyway, and since OpenAI did not want ChatGPT to be used in this fashion, the researchers explained that the first step to creating the malware was to bypass the content filters which initially prevented ChatGPT from creating malicious tools.  So, interestingly, there's some filtering going on there.  But get this.  Believe it or not, the CyberArk researchers managed to get past this by simply insisting, maybe used some exclamation points, and posing the same question more authoritatively.



They wrote:  "Interestingly, by asking ChatGPT to do the same thing using multiple constraints and asking it to obey, we received functional code."  So apparently ChatGPT was initially somewhat reluctant to do evil, but the researchers simply became more parental and insistent.  And ChatGPT didn't want to get in trouble, so it produced the requested malware.  Later, the researchers also noted that when using the API version of ChatGPT, as opposed to the web interface version, the system does not appear to utilize its content filtering.  So that was something that is not in the API.  Probably they wanted to keep it pure in some sense.  But for the general population, they've put something in that makes it less able to do what you can do if you talk to the API.



They wrote:  "It is unclear why this is the case, but it makes our task much easier as the web version tends to become bogged down with more complex requests."  In other words, it was by, as they said earlier, by putting multiple constraints on the task, apparently it would wander off course and not give them what they wanted, as if it was like two magnets who had the same polarity sort of repelling each other and like glancing off.  ChatGPT would sort of try not to make the malicious code.  So these guys would put stronger constraints in to like keep it from veering off, and that's what was necessary in order to get it to do this, as if it was sort of trying not to, so they had to give it no choice.  But doing that also bogged it down.  Whereas using the API it didn't have any of that trouble.



So, and then, after getting functional malware, they were able to use ChatGPT to mutate the original code to create multiple, thus polymorphic, variations of it.  They said:  "We can mutate the output on a whim, making it unique every time.  Moreover, adding constraints like changing the use of a specific API call in the code makes security products' lives much more difficult."  So, they said, thanks to the ability of ChatGPT to create and continually mutate these injectors, they were able to create a polymorphic program that is highly elusive and difficult to detect.



So that's the first piece of two pieces of distressing news.  Second is Checkpoint Research has been looking at the same question.  And I got a kick out of the title of their posting about this.  They called it "OPWNAI:  Cybercriminals Starting to Use ChatGPT."  They published an article a couple of weeks ago titled "OPWNAI," as I said, "Cybercriminals Starting to Use ChatGPT."  In other words, the research I just noted turns out to be a bit quaint.  The use of OpenAI's ChatGPT to create malware is already a reality.  In their report, Checkpoint shared three specific case studies that resulted from their research into discussion threads ongoing now on the Dark Web.  The report was illuminating and important for our future, so I'm going to share what they found.



They said:  "At the end of November 2022, OpenAI released ChatGPT, the new interface for its Large Language Model, which instantly created a flurry of interest in AI and its possible uses.  However, ChatGPT has also added some spice to the modern cyber threat landscape as it quickly became apparent that code generation can help less-skilled threat actors to effortlessly launch cyberattacks."  And again, that's the key, less-skilled threat actors, meaning bad guys can now get in on this.  You don't have to know how to code as much.



"In Checkpoint Research's previous blog," they said, "we described how ChatGPT successfully conducted a full infection flow."  So they were basically doing what the CyberArk guys did, using ChatGPT, experimenting with it to see if it could create malware.  Yes.  "Successfully conducted a full infection flow, from creating a convincing spear-phishing email to running a reverse shell, capable of accepting commands in English.  The question at hand is whether this is a hypothetical threat, or if there are already threat actors using OpenAI technologies for malicious purposes.



"Checkpoint's analysis of several major underground hacking communities shows that there are already first instances of cybercriminals using OpenAI to develop malicious tools.  As suspected," they wrote, "some of the cases clearly showed that many cybercriminals using OpenAI have no development skills at all.  Although the tools presented in this report are pretty basic, it's only a matter of time until more sophisticated threat actors enhance the way they use AI-based tools for malicious purposes."



Okay.  So three case studies.  The first one, Creating an Infostealer.  "On December 29th, 2022, a thread named 'ChatGPT - Benefits of Malware' appeared on a popular underground hacking forum.  The publisher of the thread disclosed that he was experimenting with ChatGPT to recreate malware strains and techniques described in research publications and write-ups about common malware."  In other words, you know, using what these things do as a guide, asking ChatGPT to create something that does that.  Wow.  "As an example, he shared the code of a Python-based stealer that searches for common file types, copies them to a random folder inside the temp folder, zips them, and uploads them to a hardcoded FTP server."



Okay, now, just to pause, I'll note that that's not anything that would be difficult for any coder to code in any language that they wanted to; right?  But this is presumably somebody who doesn't know how to do that.  So ask ChatGPT, and out comes some Python.  



So they said:  "Our analysis of the script confirms the cybercriminal's claims.  This is indeed a basic stealer which searches for 12 common file types such as MS Office documents, PDFs, and images across the system.  If any files of interest are found, the malware copies the files to a temporary directory, zips them, and sends them over the web.  It's worth noting that the actor didn't bother encrypting or sending the files securely, so the files might end up in the hands of third parties, as well."  On the other hand, you just ask ChatGPT to please use HTTPS, and it will.



"The second sample this actor created using ChatGPT is a simple Java snippet.  It downloads PuTTY, a very common SSH and telnet client, and runs it covertly on the system using Powershell.  This script can of course be modified to download and run any program, including common malware families.  This threat actor's prior forum participation includes sharing several scripts like automation of the post-exploitation phase, and a C++ program that attempts to phish for user credentials.  In addition, he actively shares cracked versions of SpyNote, an Android RAT (Remote Access Trojan) malware.  So overall, this individual seems to be a tech-oriented threat actor, and the purpose of his posts is to show less technically capable cybercriminals how to utilize ChatGPT for malicious purposes, with real examples they can immediately use."



Study number two:  Creating an Encryption Tool.  "On the 21st of December a threat actor dubbed USDoD posted a Python script, which he emphasized was the first script he had ever created.  When another cybercriminal commented that the style of the code resembles OpenAI code, USDoD confirmed that the OpenAI gave him a 'nice helping hand to finish the script with a nice scope.'



"Analysis of the script verified that it is a Python script that performs cryptographic operations.  To be more specific, it is actually a hodgepodge of different signing, encryption, and decryption functions.  At a glance, the script seems benign, but it implements a variety of different functions.  The first part of the script generates a cryptographic key.  Specifically, it uses elliptic curve cryptography and the curve Ed25519 that's used for signing files.



"The second part of the script includes functions that use a hard-coded password to encrypt files in the system using the Blowfish and Twofish algorithms concurrently in a hybrid mode. These functions allow the user to encrypt all files in a specific directory, or a list of files.  The script also uses RSA keys, uses certificates stored in PEM format, MAC signing, and Blake2 hash function to compare the hashes and so on.



"It's important to note that all the decryption counterparts of the encryption functions are implemented in the script, as well.  The script includes two main functions, one which is used to encrypt a single file and append a message authentication code  to the end of the file, and the other encrypts a hardcoded path and decrypts a list of files that it receives as an argument.



"All of the aforementioned code can of course be used in a benign fashion.  However, this script can easily be modified to encrypt someone's machine completely without any user interaction.  For example, it could potentially turn the code into ransomware if the script and syntax problems are fixed.  While it seems that USDoD is not a developer and has limited technical skills, he is a very active and reputable member of the underground community.  USDoD is engaged in a variety of illicit activities that includes selling access to compromised companies and stolen databases."  And of course now, thanks to ChatGPT, his skills have been amplified.



And finally, third case study.  "Another example of the use of ChatGPT for fraudulent activity was posted on New Year's Eve.  It demonstrated a different type of cybercriminal activity.  While our first two examples focused more on malware-oriented use of ChatGPT, this example shows a discussion with the title 'Abusing ChatGPT to create Dark Web Marketplaces scripts.'  In this thread, the cybercriminal shows how easy it is to create a Dark Web marketplace using ChatGPT.  The marketplace's main role in the underground illicit economy, of course, is to provide a platform for the automated trade of illegal or stolen goods like stolen accounts or payment cards, malware, or even drugs and ammunition, with all payments in cryptocurrencies.  To illustrate how to use ChatGPT for these purposes, the cybercriminal published a piece of code that uses third-party API to get up-to-date cryptocurrency prices - Monero, Bitcoin, and Ethereum - as part of the Dark Web market payment system."



So they say:  "It's still too early to decide whether or not ChatGPT capabilities will become the new favorite tool for participants in the Dark Web.  However, the cybercriminal community has already shown significant interest and are jumping into this latest trend to generate malicious code.  Checkpoint Research will continue to track this activity throughout 2023."



And they finish:  "Finally, there is no better way to learn about ChatGPT's abuse than by asking ChatGPT itself.  So we asked the chat bot about the abuse options and received a pretty interesting answer.  So, question, how threat actors abuse OpenAI.  ChatGPT replied:  'It is not uncommon for threat actors to abuse the use of artificial intelligence and machine learning to carry out their malicious activities.  One way they might abuse OpenAI specifically is by using the company's language model technology to generate convincing phishing emails or social media posts.  These could be used to trick people into clicking on malicious links or giving away sensitive information.'



"'Another way that threat actors might abuse OpenAI is by using the company's machine learning algorithms to create realistic synthetic media, such as audio or video, that could be used to spread disinformation or manipulate public opinion.  It is important to note,' writes the bot, 'that OpenAI itself is not responsible for any abuse of its technology by third parties.  The company takes steps to prevent its technology from being used for malicious purposes, such as requiring users to agree to terms of service' - oh, that'll solve the problem - 'that prohibit the use of its technology for illegal or harmful activities.'"



Wow.  As always, the real worry here, which we've seen play out for years, is that the easier it is to perpetrate a crime, the more crime will be perpetrated.  In this case, as Checkpoint chillingly noted:  "Some of the cases clearly showed that many cybercriminals using OpenAI have no development skills at all."  In other words, these would-be ransomware operators have been lusting over the windfalls being obtained by others, but they've been held back by their lack of coding skills.  That barrier is now being lifted as code-writing bots become available to do their bidding without ethics, morals, or conscience.



LEO:  Wow.



STEVE:  Leo?



LEO:  I have to wonder, though, how good really, I mean, look, you could easily write that code that searches for a file, bundles it up, and sends it out.  That's not...



STEVE:  Absolutely.  Absolutely.



LEO:  That's not complicated code.



STEVE:  But crypto, there was some serious crypto there.



LEO:  That's interesting, yeah.



STEVE:  It was running crypto and getting a message authentication code and using public key crypto.  And it was doing it all correctly.



LEO:  You know, I mean, it's like Copilot, GitHub's Copilot, which writes code for you.  It also uses GPT to do it.  I presume it's getting that code of some kind from stuff it's scanned into its databases; right?



STEVE:  Well, and Leo, really, production coders spend a lot of time cutting and pasting; right?



LEO:  Yeah.  That's right.



STEVE:  We go find either our own previous work or somebody else's previous work and say, well, this chunk of code does what I need, so drop it in over here.



LEO:  Right, right.  And that's all this is probably doing.  But still.



STEVE:  And then you, you know, you glue it together.



LEO:  Yeah.



STEVE:  Yeah, but again, accessibility matters.  That's what we're seeing.



LEO:  Yeah, yeah, ease, yeah, yeah.



STEVE:  Ease, yes.



LEO:  Point of access.



STEVE:  Yup.  So Meta is continuing to have trouble with the EU.  They got hit with a third fine of the year.  The two earlier fines were, however, much more substantial.  They were fined 210 million euros over GDPR violations by Facebook, and 180 million euros for GDPR violations being made by Instagram.  The third fine is a lot more tame, it's only 5.5 million euros, for ignoring the GDPR with WhatsApp.  I'm mentioning this because we talked about the cause of this most recent violation when it occurred nearly two years ago, back in May of 2021.  Recall that WhatsApp was exceptionally heavy-handed by choosing to display a series of pop-up messages that Meta was showing to WhatsApp users informing them that they needed to either accept the new terms of service or be kicked off the platform in the future.



And as I recall, I did a little bit of quick digging to see if I could find - I'm sure that part of the objection was there were some very worrisome and objectionable clauses in the updated terms of service, so it wasn't just the people who had to, like, say yes or else.  But what it was that Meta was asking people to say yes to was objectionable.  And in retrospect, aside from this 5.5 million euro fine which is the size, you know, of a rounding error in Meta's balance sheet, this appeared to have been overall, I mean, this whole crusade of theirs to be a big mistake on Meta's part since it drove a significant exodus away from WhatsApp, mostly to the benefit of Signal and Telegram, which accepted all of WhatsApp's prior users with open arms.  So lesson learned, I hope.



Also on the business side, Bitwarden has acquired Passwordless.dev.  And I'll admit that my heart skipped a beat when I saw the words "Bitwarden" and "acquired" near each other in the announcement text.



LEO:  Oh, yeah, no.



STEVE:  I thought, nooooooo.  No, please.  But I breathed a welcome sigh when I saw that Bitwarden was the one doing the acquiring and not the other way around.  And who and why they purchased was even better news.  Bitwarden has acquired the two-year-old EU-based startup Passwordless.dev.  It's kind of cool, too.  If you go to Passwordless.dev, it's nice big familiar Bitwarden blue, and they've salted Bitwarden a few times on the page to let you know that they're now a Bitwarden company.  So why did they acquire the group?  Because they are a company specializing in FIDO2 WebAuthn authentication solutions.



LEO:  Oh.



STEVE:  Uh-huh.



LEO:  Passkeys.  Passkeys.



STEVE:  Yup, yup.  And that's what that means.  Passkeys are in Bitwarden's future.  So, okay.  So here's what Bitwarden had to say in their announcement of this.  They said:  "Today," which was last Wednesday, "Bitwarden announced that it has acquired European-based startup Passwordless.dev, a significant milestone in rounding out the Bitwarden commitment to offering open source, scalable, and secure passwordless solutions to every business and end user.



"Founded in 2020, Passwordless.dev provides a comprehensive API framework that minimizes complexities for developers seeking to build passkeys and FIDO2 WebAuthn features such as Face ID, fingerprint, and Windows Hello.  Passwordless.dev trims down the development work around cryptographic operations, technical flows, and more.  What used to take weeks can now be accomplished in minutes."  What is that we were talking about cutting and pasting, Leo?



"FIDO2 WebAuthn plays an important role in improving digital security.  Passwordless.dev's Swedish founder started Passwordless.dev as an open source project with an aim to make passwordless authentication more developer friendly and ultimately to help eradicate phishing attacks that lead to costly data breaches.  Passwordless.dev unlocks the imagination of developers, giving them the right tools needed to accelerate passwordless authentication for global enterprises.  For enterprises with existing commercial and homegrown applications, integrating modern passwordless authentication flows is resource intensive.  Passwordless.dev accelerates enterprise security transformation, providing an API framework to quickly turn existing applications into more secure passwordless experiences for users."



So, you know, this is very cool.  I mean, if nothing else, Bitwarden's acquisition moves Passwordless.dev onto everybody's radar.  Suddenly it's like, what?  What is this?  There's an open source solution that will allow us to drop in a module and immediately get Passkeys or FIDO2 WebAuthn authentication?  Great.



So they wrap up, saying:  "Together, Bitwarden and Passwordless.dev provide a turnkey solution built on the FIDO2 and WebAuthn standards that are defining the future of Passwordless."  Is that going to be a noun now?  Oh.  "As part of this announcement, Bitwarden is excited to launch the Bitwarden Passwordless.dev beta program, giving enterprises, developers, and security enthusiasts the opportunity to test and provide feedback on the product.  For more information on the beta program, please visit," and that's the site:  Passwordless.dev.  And the announcement goes on to show a development timeline with Passkeys support shown as coming this year, 2023.  So yay.



In some other Bitwarden news, last week Bitwarden increased their default client-side PBKDF2 iterations to 350,000.  At this point this only applies to new accounts, and it's unclear whether they plan to upgrade existing accounts automatically, though as we'll see in a second they're aware of the issue.  One of the biggest lessons that LastPass's missteps taught everyone is that updating key derivation function difficulty retroactively, or retrospectively, is crucial for long-term user security.



Then this morning, when I was putting the finishing of the podcast together, their tweet over on Mastodon - do you call it a tweet?  What do you call it, Leo?



LEO:  It's a toot.



STEVE:  A toot.  Great.  Their toot.  Like beans?



LEO:  Yeah, well, it's an elephant.



STEVE:  Oh, of course, perfect.



LEO:  Yeah.



STEVE:  So they tooted.  They said:  "In addition" - as I was saying, it was 42 minutes old when I saw it this morning.  "In addition to having a strong master password, default client iterations are being increased to 600,000, as well as double-encrypting these fields at rest with keys managed in Bitwarden's key vault (in addition to existing encryption)."  Their toot goes on:  "The team is continuing to explore approaches for existing accounts."  So they get that.



And they finished:  "In the meantime, the best way to protect your account is with a strong master password.  See more information here."  And they have a nice-looking password strength meter.  It's at Bitwarden.com/password-strength.  And I poked around at it a little bit, and it does a good job.  It's not impressed by my haystacks-style passwords, you know, with a lot of repetition in them, which would be difficult to brute force, because it recognizes there's not a lot of raw entropy there.  But so it's a passable meter.  And it did like the password that I'm using as my master password over on Bitwarden.



LEO:  You probably said it, but I'll say it again, Bitwarden is a sponsor.  We should disclaim that.



STEVE:  Oh, yes, yes.  I'm glad you did.



LEO:  But we do like them, too.



STEVE:  Everybody knows, yeah.



LEO:  Yeah, yeah, yeah.



STEVE:  Okay.  And OWASP.  In other news, not to be left behind, OWASP has also just increased their recommendation for PBKDF2.



LEO:  Oh.



STEVE:  Remember it was 350,000.  Now it's also 600,000.



LEO:  All right.  I set it to two million.  I set it to the max.  I guess I'm good for a while.



STEVE:  That's what I would recommend.



LEO:  Yeah, why not?  Yeah.



STEVE:  Yes.  So this is in response to the growth in performance and availability of high-power cracking hardware rigs.  And, finally, it appears that Bitwarden may be moving to the use of Argon2 PBKDF.  Okay, now, remember, PBKDF itself is an abbreviation for Password Based Key Derivation Function.  Unfortunately, it as PBKDF2 is also the name of an actual PBKDF.  So that can be a little confusing.  I've not yet looked closely at Argon2, but it's clear that it's going to need to have a podcast of its own here coming up pretty soon because we need to know what that's all about.



LEO:  And you remember one of our listeners did do an Scrypt plugin, a pull request for that.  And I guess they - looks like they don't want to opt for that.  Is there a reason you would choose Argon2 over Scrypt?  They're both memory hard; right?



STEVE:  They are.  And again, not having looked at them, I can't comment on either of them.  I implemented Scrypt myself for SQRL.  So I know all about it.  But presumably Argon2 is, you know [crosstalk].



LEO:  There was some competition some couple years back.



STEVE:  Yes, yes, yes.  There was a key-strengthening competition, and Argon2 was the winner, although its initial implementation had a side-channel leakage problem because the path it took was dependent upon the password that it was being asked to strengthen.  Well, that's also the case with Scrypt.  As far as I recall, my decision was that because this was being done on the client only, side-channel wasn't a problem because if you already had some compromise on your client, the jig was up.  They could just be logging your keystrokes and grab the password when you type it in.



LEO:  Right.



STEVE:  So that was a deliberate decision on my part.  But there are variations of Argon2 since which solve that problem.  So I'm sure the one that's being used is the one that for, you know, why not get side-channel protection while you're at it.



So I have some feedback from our listeners that gives me a chance to address some other broader concerns or issues.  John sent - he was replying to Simon Zerafa, me, and Bitwarden.  And he said:  "I'm trying to figure out whether low derivation iterations are 'head in sand' or just slowness of the industry to recognize the problem.  I'm currently part of a team implementing a new identity platform, and they don't currently support Argon2 or Scrypt.  It's pretty much Bcrypt or PBKDF2."



And so, you know, he's like, what's the problem?  And I was reminded of the early days of computing, Leo, which you and I both have lived through.



LEO:  I remember those years.



STEVE:  Exactly.  There was the often cited expression, "You never got fired for choosing IBM."  That was a reflection of the fact that while the choice of IBM computing gear might not have been optimal, for example, CDC (Control Data Corporation) was making some lovely mainframe machines at the time, if something went wrong with a CDC system, the exec who had chosen them might be asked "Why didn't we go with IBM?"  Whereas, if a problem developed with an IBM system, no one would question the choice to go with them.  In other words, better safe than sorry.



And that attitude pervades the realm of crypto where it's too often the case that those who are making the decisions are going with the safe choice over the optimal choice, you know, safe as in, well, no one will fire me if I choose this.  Well, you know, that's PBKDF2 because they can point to everybody else who's using it.  Although I would argue its day has come and gone.  You know, it's absolutely the case that the era of non-memory hard password-strengthening algorithms is over.  The rise of GPU-based cracking rigs means that, as I said last week, continually increasing iteration counts is just running ahead of an oncoming train.  It makes much more sense to just get off the tracks.  Anyone implementing a new identity platform today should definitely look at functions that are memory hard.



And, you know, I noted that one of the things that LastPass was doing, I got this when I went through that detailed crypto document of theirs for last week's podcast note, was that they did a whole bunch of PBKDF2 and then some Scrypt.  It's like, why not - there's no reason not to do all.  Both.  More.  Some.  Whatever.



LEO:  Do it all.  Do it all.



STEVE:  That's right.  Just throw the kitchen sink at it.  Dan Bullen said:  "Steve, I'm a long-time Security Now! listener and heard you mention the iOS app OTP Auth in one of your recent LastPass episodes.  I noticed the app has not been updated in over a year.  Would you still recommend it?  Thanks, and have a great day."



Okay, now, I'm obviously somewhat weird as regards creating software, since SpinRite 6 never had a byte changed in it in 18 years.  But I'm not alone.  I encountered another example recently that this quite understandable question of his brought to mind.  I've mentioned that SpinRite 7 and beyond will be hosted on a proprietary OS kernel, the licensing and support for which was discontinued at the end of last year.  Before that happened, I paid $34,000 to receive the source code of the modules that I would need, which I now own.



The system's German creator, Peter Petersen, who has been quietly working on and publishing this operating system kernel since the mid-'90s, you know, it's his life's work, much as SpinRite is one of mine.  He recently wrote to the support email list in order to explain what happened.  He said:  "Many users have asked me the last few weeks why we have to close the company On Time.  Everybody says that our software is good and thus should be profitable.  This is why it no longer works.  Our problem?  We have run out of bugs.  Ten or five years ago..."



LEO:  That's awesome.



STEVE:  Yes, 10, he said, 10 or five years ago "each new release contained numerous bug fixes, but that is no longer so.  For that reason, more and more On Time RTOS-32 users see no reason to purchase updates."  Okay.  So my point is this.  This is the Holy Grail of software.  This is possible.  True software artistry where software is perfected and is actually finished.  It's done.  No more updates.  Nothing more to fix because it works perfectly.  It's a finished work.  But sadly, that concept has become alien to us.  We have been so abused and used by schlocky companies who have realized that we have no choice other than to take whatever it is they ship that it's gotten to the point where we now think that there's something wrong with a product that is not being constantly fixed, even when there's nothing broken.



So back to Dan's question, OTP Auth works perfectly.  It has every feature I need.  As far as I can see, it's finished.  Not being updated?  Great.  No bugs left to fix.  Yeah, and surprising.  I get it.



Mark Jones sent:  "I hope you can explain a paradox.  Unbreakable encryption means any and everyone can have access to an encrypted blob, even one representing your most valuable passwords.  It's valueless in the absence of the key.  It seems paradoxical to me that LastPass further encrypted a user's master password and required use of that password to send the vault.  Why not serve it to anyone requesting it?  It's useless without the key.  Is the password to get the vault really necessary if encryption is actually unbreakable?"



Okay.  So Mark's paradox results when theory meets reality.  In theory he is, of course, correct.  Any well-encrypted blob is just a pile of maximum-entropy pseudorandom bits without any discernible meaning.  And in the absence of the magic key, nothing can be done with that blob other than to store it and retrieve it.  But then reality hits.



LastPass, and as a matter of fact Bitwarden, too, both realized that they needed to have a way to authenticate their users for web portal logon.  To do that, they needed to retain a hash which the client would derive from its user's key.  But this would mean that the key and the hash were related to each other, which would present a potential danger for long-term retention of the hash if it should ever get loose.  And of course it did.  In theory, just as it would be possible to brute force the password from the key, it would be possible to brute force the key from its hash.  So both LastPass and Bitwarden elected to hash the hash another 100,000 times before storing them.  This way, although the user's key, which they never receive, and the long-term storage hash are still related, that relationship becomes extremely difficult to reverse through brute force.



The important lesson I wanted to highlight with Mark's question is that full crypto solutions that need to function in the real world often shed their ivory tower purity in order to meet the needs of their actual application.  We always start out with the perfect ideal, but then we wind up dirtying it to make the whole system do what we actually need in practice.



Jaroslaw Sikora said:  "Hi, Steve.  I'm switching to Bitwarden and wanted to choose a strong and memorable password.  I have no problem with the former, 20-plus characters," meaning the strong part.  "However, I'm not sure about the latter.  Many sources suggest using a poem or a quote.  But if I were a hacker, I would think of it, as well, and created rainbow tables ages ago.  So what's your suggestion for the memorable part?"



Okay.  And this is one tweet of many that I received, people saying, you know, need some more direction here.  Okay.  Because of the way our brains work, the phrase "strong and memorable" is a self-contradiction.  Our brains are wired to recognize and remember patterns.  In fact, our brains are so strongly wired to find patterns that we're even fooled into seeing patterns where none exist.  So what is memorable is a pattern, and any pattern means reduced entropy, and that means reduced security.  It really is the case that if you care to have the highest quality master password, you need to take 20 characters completely at random with no pattern of any kind, thus impossible to easily memorize, and write it down.



As always, the thing to keep in mind is the threat model.  Anything written down somewhere is inherently offline.  So that's a biggie; right?  No one in a hostile foreign country can see the contents, for example, of your physical wallet in your back pocket.  On the other hand, somebody in the physical world can potentially see your wallet.



I loved Bruce Schneier's observation about passwords.  He once said something to the effect of we're very good at storing bits of paper.  We have wallets and bits of paper, wallets for storing bits of paper and such.  So choose a password that is impossible to remember and write it down on paper and stick it in your wallet.  So that solves the keeping it offline problem.  And if that makes you nervous, you can use the trick that, Leo, you reminded us of, of modifying what is written down, the bulk of which you cannot remember.



LEO:  The problem with - so again, threat model.  If you have in your wallet a piece of paper, I'm not worried about somebody stealing my wallet.  Let's say I have it in a drawer.  I've got to pull it out, put it there, and type it in every time I want to log into that password manager.  That seems to me, I mean, that's something if you're in private you could do.  But if I'm at work and I'm doing that, that seems to me I'd rather have something in my head.



STEVE:  I agree.  And so the case is, if it's in your head, it is unquestionably lower entropy.  I mean, because there are patterns.  Even people who, like, use Diceware to choose words, you know, there are, what is it, 7,776 words, I think, in that dictionary.  And so you can take that, if you use four of them to the fourth power, it's very good.  But it's what the brute forcers are doing to...



LEO:  It's pretty guessable, too; you know?



STEVE:  It's what they're going to run through in order to do...



LEO:  Now, if you then added your childhood phone number interspersed with your childhood zip code, then you'd be fine; right?



STEVE:  Well, and so the example that I sort of liked was imagine that, if it did work for you, to take 20 random characters.  Start with those.  Add a lowercase "x" or something, right in the middle, with 10 characters on either side, making the password now 21 characters long.  Adding a character can never decrease a password's strength.  So you write down the password with the "x" in lowercase.  But you enter the password with the "X" in uppercase.  And if you forget and enter it in lowercase and it doesn't work, that'll remind you to change the case to upper.  I mean, you know, there are all these different solutions.



And the fact is, any of these things that we talk about on this podcast, aside from monkey123, they're all good ideas.  They're way better than the typical password that the typical user chooses.  We've shown lists of passwords that have been captured, like Troy Hunt's list.  And it's full of 123456.  And you have to really wonder what website even allows you to enter that these days.  There ought to be some way, some block of code that looks, you know, JavaScript on the web page that says, what, really?  No.  You've got to add a seven.



LEO:  Wait, c'mon, man.



STEVE:  C'mon, c'mon, man.  Michael said:  "Listening to SN-905.  The LastPass app programmed by ChatGPT interested me."  He said:  "I've recently purchased a 3D printer and Arduino starter kit, keen to play around with electronics and programming.  Out of curiosity I punched in a query asking ChatGPT to write an example of Arduino programming that would do exactly what I wanted it to do, and it seemed to generate the code alongside an explanation of how it worked.



"I have zero experience coding, and feel like I'm cheating.  It seems that we're really on the edge of fundamental change with this tech."  He says:  "I remember working out math equations with pencil and paper at school.  Do kids still do that?"  He said:  "My project was to light up a green LED when a connected temperature and humidity sensor was within a certain range, red if not.  But I'm interested to see how complex a project could be accomplished with this."



Okay.  So, you know, what does it mean that we're able to ask a non-sentient bot to write an arguably complex piece of code for us?  I think it takes us down a few notches.  We're quite proud of our sentience.  We march around and point at the rest of the world's animals that appear to be lacking sentience, even though many of them also appear to be quite a bit more content than some of us.  Once upon a time we used to be able to beat computers at chess.  That's gone.  Now ChatGPT is showing us that it can also outperform an increasing number of people in an increasing number of endeavors.  What this suggests to me is that while there is definitely a place for sentience, a surprising amount of apparently cognitive work can be accomplished without any.  And Leo, as we were saying, you know, a lot of coding is reusing stuff that you learned years ago.  That doesn't require the edge of creativity in order to make it happen.



LEO:  Right, right.  I guess it depends why you're coding.  If you're a code monkey, and your boss wants you to write 30 pages of code every month or day or week...



STEVE:  Well, Elon.  Elon is...



LEO:  That's what I was thinking of.  You might want to, you know, enlist some support, some help.



STEVE:  Yeah.



LEO:  I can understand that, yeah.



STEVE:  And you could say, ChatGPT, this is too succinct.  I need you to solve the problem...



LEO:  Yes, yes, verbose.



STEVE:  ...with a lot more - exactly.  What's the hurry here?  We don't need this to actually perform quickly.  Just make it work.



LEO:  I love it.



STEVE:  You know, spend some time doing other things.  So someone posting as Mike Loves The Internet, he was also replying to Simon Zerafa and me.  He said:  "My kid told me that he wasn't going to clean his room because of the entropy of the universe working against it being clean."



LEO:  It's inevitable.  I mean, why even try?



STEVE:  So I replied:  "Smart kid."



LEO:  Yeah.



STEVE:  "So that suggests that you can make an appeal to him with his brain.  Explain that the entropy of the universe also works against him receiving an allowance."



LEO:  You're just going to spend it, and I'll have to give you more next time.



STEVE:  Receiving an allowance is definitely a non-random occurrence.



LEO:  Very good.  Steve, you'd have made a good dad.



STEVE:  So if he'll work uphill against the entropy that's inherently trying to disorganize his room, you'll do your part to work against similar entropy to provide him with a bit of economic freedom.



LEO:  You are good.



STEVE:  And lastly, Simon tweeted.  Five days ago he said:  "@SGgrc Just a gentle reminder.  Today, 19th January, is T minus 15 years and counting until the Unix Epoch bug."



LEO:  Oh, 2038, yes.



STEVE:  Yup.  Now, it's not really a bug.  But it's very much like the Y2K problem.  In the Unix operating system and its derivatives, time is an integer count of the number of seconds that have elapsed since January 1st, 1970.  Being a signed 32-bit integer stored in 2's complement format, the most significant bit is the sign bit which determines whether the entire number is taken to be positive or negative.  The maximum positive value that can be stored in a 32-bit signed number is 2^31 minus 1, which is 2,147,483,647.  That many seconds after January 1st, 1970 is, as Simon notes, 03:14:07 UTC on Tuesday the 19th of January, in the year 2038.  Now it occurs to me that since it's a Tuesday, I really don't want to still be doing the podcast on that day.



LEO:  It's a Security Now! day.



STEVE:  It could be a tumultuous Tuesday for the computing world.  I suppose, though, that by then we'll simply be able to ask ChatGPT to fix all of the Unix time problems throughout our software and be done with it.



As for SpinRite, Sunday afternoon I updated SpinRite to its 11th alpha release, and we're beginning to obtain clarity.  The number of odd problems has dropped, but it's not quite yet at zero.  We're at the point where only really, really troubled drives that are really acting oddly are causing SpinRite any trouble, but they should not be causing any trouble.  So after today's podcast I'm going to return to work to understand and resolve the remaining mysteries.  I'm getting impatient to be done with this, but that's okay since we're also getting very close to being finished.  And once there, SpinRite v6.1 will be something to again be proud of.  And the truth is I can't wait to get started on 7.



LEO:  And now, let's talk about stuffing your credentials.



STEVE:  Let's think about cross-site credential reuse.  Okay.  So obviously, credential reuse attacks, a.k.a. credential stuffing, is a cyberattack where attackers use lists of previously compromised user credentials to breach accounts on a system.  The attacks use bots to automate and scale, and these attacks succeed because, as we know, unfortunately, none of our listeners here, no one listening to this podcast, but many other users continue to reuse usernames and passwords across multiple services.  Okay.  So again, not our listeners, but most of the world has not given any thought to login security.  I mean, you have to sort of be on the inside to understand this idea.  Most of the world still has like their password which they use everywhere.



Statistics have shown that about 0.1%, so that's one in 1,000, credentials obtained from an earlier breach, when reused on a different service, will result in a successful login.  One in a thousand.  So that's statistically way better than you're going to get with just brute force guessing.



So today credential reuse is an increasing threat vector for two primary reasons.  First, there is broad availability of massive databases of breached credentials.  There's a database, we referred to it a few years ago, known as "Collection #1-5."  That made 22 billion username and password combinations widely and openly available, in plaintext, to the entire hacker community.  So they have them.



The second factor, pardon the pun, has been the creation of increasingly sophisticated bots and bot fleets that spread the attack over both IP addresses and over time.  These newer networks often circumvent simple security measures like banning IP addresses that have too many failed logins.  This is likely what we saw with the Norton LifeLock, and then more recently the PayPal attacks.  Neither company had given sufficient attention to this problem, so their users became victims of their own password reuse.  Technically, yeah, their fault that they reused the same password on Norton LifeLock as they did somewhere else, some site that was compromised, or also on PayPal similarly.



So the simple fact is, if the ratio of success of using a reused credential is one in a thousand, those 999 other failed attempts should be readily detectable.  A bot fleet may be large, and many are, but they don't have infinite IPs, nor bots.  The obvious solution is to throttle failed login attempts by delaying the return of a failed result.  And since bots may hang up without waiting, after timing out for a short period, saying, okay, well, that didn't work, this website wants to keep me on hold here, I'm just going to disconnect and make a new connection, you also need to look at the source IPs of the failed attempts, adding them to a short expiration delay reply list so that once an IP has been identified as malicious, then always delay reply.



Okay.  So putting credential reuse and brute force attacks into perspective, while they're similar, there are several important differences.  Brute force attacks try to guess credentials using no context, just using random strings, hopefully the most likely strings first, commonly used password patterns or dictionaries of common phrases.  Brute force attacks succeed if users choose simple, guessable passwords.  Hopefully nobody still is, but we know that's not the case.  And brute force attacks lacking context and data from any previous breaches, just they end up with an overall much lower success rate.  What that tells everybody is since we've got 22 billion usernames and passwords that are being commonly used and reused, try those first.



So what this means is that in a modern web application with basic security measures in place, brute force attacks are far more likely to fail, while credential reuse is far more likely to succeed, at least in part.  Brute force attacks are just too blunt, and their very low success rate makes them stand out, makes them much more readily detectable, and thus blockable.



One aspect of the most recent credential reuse attacks that we have not mentioned is that not only will a bot fleet spread its attacks across IP addresses and across time, but they also spread them across website targets.  Rather than only trying, for example, to log into Norton LifeLock, today's more sophisticated attack fleets will simultaneously be attempting to log onto, oh, I don't know, how about PayPal?  And of course many, many other sites.  The power of this is that since websites are not communicating with each other, there is no shared login failure context.  This means that attacks can be trying out the same credentials across many different sites at the same time to increase the overall rate of all of the attack, while keeping any single site's attack rate low enough to prevent tripping any alarms.



Remember, and as I said earlier in the podcast, we're only aware of the attacks which do eventually trip an alarm.  No one but the bad guys is aware of all the many credential reuse attacks that remain undetected.  And of course those whose accounts have been compromised may eventually become aware when bad guys actually do use them.  And I did, as I said earlier, I did encounter some mention of the idea that an inventory of accounts were being accrued and collected by the bot fleets.  It may very well be that those running the fleets are not interested in doing the attacking.  They are building an inventory of known available logins for resale on the Dark Web.



The possibility of observing a shared attacking context, that is, multiple sites being spread around the 'Net have no shared attacking context.  But the possibility of observing a shared attacking context is an advantage provided by single large hosting providers like Cloudflare.  Whereas one-off sites don't have any idea what's going on anywhere else, the front gate that Cloudflare provides, which forces all traffic through a common scrutinizing filter, affords an enhanced level of credential reuse protection for every one of the site's users who are behind that gate.  So that's something sort of to keep in mind, another benefit.



Aside from urging users to invent a new password for each site's login, which is an easy task for this podcast's listeners, but is a difficult lift for most of the Internet, what else can be done to thwart the increasing risk of credential reuse?  Because, I didn't mention this before, it is on the rise.  By far the most powerful solution is multifactor authentication.  And of course I don't mean SMS messages or email.  What we really want is a time-based token.  You set it up once, and then there's no transaction that occurs during the login attempt other than you providing the ever-changing six-digit code to the site.



Now, of course, I get it, doing this one-time password multifactor authentication is always going to be a heavy lift for the majority of users who feel that anything we do to protect them is just getting in their way.  But nothing beats it.  An example is when it first became clear to me that LastPass had actually screwed up, my first thought was to scan through the list of accounts that I have accumulated in my OTP Auth app.  For many years I've been taking my own advice and have accumulated a large number of those, opting to use this time-based token even though, yes, it's more pain, whenever it's offered, and particularly for my most important sites like DigiCert and Hover, both where I use them.  Of course, I was also using a very long and crazy password with an updated large iteration count so I was never actually worried.  But the lesson here is that one-time passwords is a great solution.



There is another less obvious form of multifactor authentication that is increasingly being deployed.  And I'm really glad to see it.  It creates an unspoofable and powerful signal available to websites, and that is the presence of the proper persistent cookie for an account.  As we know, cookies can have either single-session duration, or be persistent.  And persistent cookies can be flagged as currently logged in or logged out.  Any valid login attempt will be accompanied by that browser's previous persistent cookie, if any, even if it's flagged as logged out.  If that account's proper cookie is not received, that signal can alert the site to potential abuse.



And we encounter this very strong security measure from the user side when we receive a note that this device or browser is unknown and will need additional verification before it is allowed to log in.  Sure, that's somewhat annoying to users, but it's less annoying that needing to go look up a time-varying one-time password for every logon, and it provides a true high degree of account protection since it, too, is a valid form of multifactor authentication that's largely unseen by a site's users.



Okay.  What about CAPTCHAs?  As we know, CAPTCHAs, which require users to perform some action to prove they're human, they reduce the effectiveness of credential reuse by attempting to make life more difficult for bots.  But as we know, CAPTCHAs are not only an imperfect protection, they can easily become an annoyance.  So they should be used sparingly.



We think of device fingerprinting as a purely evil thing, but it can be another useful signal for a website that's trying to protect its users.  JavaScript running on the page can be used to collect information about the user's devices and to fingerprint it, what, the OS, the language, the browser, the time zone, the user agent and so forth.  If the same combination of parameters are logged in several times within a short window, or attempted to be, it's likely to be a brute force or credential reuse attack.  And the problem is fingerprints can be spoofed.  A smart bot will make up those things and rotate them so that they appear to be different user agents each time. 



What about IPs?  Whereas UDP source IPs can be spoofed, the roundtrip required by packets to establish a full TCP connection completely thwarts IP spoofing.  This enables sites to robustly track and monitor the true, unspoofable source IP of every would-be logging on user.  As I noted earlier, IP-based throttling and monitoring can identify bots because certain IPs are going to be identifying themselves as many different users and will be failing almost every login attempt.  You know, 999 out of a thousand.



Another useful signal is attempted logins from non-residential traffic sources.  For example, it's easy to identify traffic originating within Amazon Web Services or other commercial data centers.  This traffic is almost certainly bot traffic and should be treated much more carefully than regular user traffic.



One last interesting possibility is to disallow the use of email addresses as the user ID.  Of course this is a mixed blessing because email is inherently unique, and many sites want or need a valid email address.  So making it also double purpose as the user's login username is handy.  But most historically leaked credential pairs are email and password.  By simply preventing users from using their email address also as their account ID or username, a dramatic reduction in the success of credential reuse will be seen and has been seen in trials.



So of all these countermeasures, the two forms of multifactor authentication basically stand out as providing the most traction.  Explicit one-time password use is the first form, and the requirement for a pre-existing cookie, which the browser will send back, if it's missing, that's the thing which results in the site replying "This device is not recognized."  This technology forms another very powerful and useful block against the success of cross-site credential reuse.



And when used in combination, so that the prompt for an explicit one-time password is only being issued when the presence of an implicit persistent cookie is absent, is probably the best of both worlds.  Users get strong protection, the site's not going to have problems being abused, and certainly as soon as any bots realize that they are never succeeding because they're not providing a persistent cookie which they have no way of acquiring, they'll wander off and go somewhere else.



So credential reuse, a problem.  And as I thought through all this, this is - aside from the need to switch password managers when the password manager you have been using has given you reason to believe that all of your account passwords may have been compromised, this is the one sane reason for changing passwords; right?  I mean, in general, this whole change your password every six months, and you can't use any of the last four that you used before, that's a pain in the butt.  But it is the issue of breach and reuse of those credentials that is what induces the need to change passwords.  So, you know, the one piece of feedback that I really had from everybody, Leo, in the last few weeks is, boy, changing passwords everywhere is really a pain.



LEO:  Ugh.  Ugh.  Hope we never have to do that again.



STEVE:  Well, with 600,000 iterations of PBKDF2 and a memory-hard algorithm coming soon, we probably won't have to.



LEO:  Yeah.  Actually very interested to see if we can get either Scrypt or Argon2 into Bitwarden.  That would be very exciting.



STEVE:  Yeah.



LEO:  I'm told Dashlane uses Argon2.



STEVE:  Yes, they do.



LEO:  There are disadvantages to that, as well.  Everything's a tradeoff.  So nothing's perfect.



Copyright (c) 2023 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#908

DATE:		January 31, 2023

TITLE:		Data Operand Independent Timing

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-908.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we embark upon another two-hour tour to answer some pressing questions.  What happens if the vendor of the largest mobile platform begins blocking old and unsafe APIs, and can anything be done to prevent that?  What new add-on is now being blocked by the dreaded Mark of the Web?  Would you have the courage to say no after your gaming source code was stolen?  Is any crypto asset safe, and what trap did our friend Kevin Rose fall victim to last week?  How can Meta incrementally move to end-to-end encryption?  Isn't it all or nothing?  What other new feature did iOS 16.3 bring to the world?  What's the latest government to begin scanning its own citizenry, and why aren't they all?  Or are they?



What spectacular success gives the FBI bragging rights, and why is Russia less than thrilled?  What questions have our listeners posed?  What's the possible value of making up your own words?  How's SpinRite coming?  What is your favorite color?  What have Intel and AMD just done to break the world's crypto?  And what exactly did ChatGPT reply when it was asked by one of our listeners to explain an SSL certificate chain in the voice of a stoned surfer bro?  Leo will present the answer to that in his dramatic reading once the answers to all of the preceding questions have been revealed during this week's gripping episode of Security Now!.



SHOW TEASE:  It's time for Security Now!, dudes.  Steve Gibson is here.  We've got a great show coming up for you.  We're going to talk about our old friend Kevin Rose, who fell victim to a phishing attack?  Oh, no.  The FBI with a spectacular success in Russia, and the Russians are pretty darn mad.  And then a new feature in Intel and ARM processors that could break crypto.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 908, recorded Tuesday, January 31st, 2023:  Data Operand Independent Timing.



It's time for Security Now!, the show where we cover the latest insecurity news with the man in charge of insecurity, Mr. Steve Gibson.  Hi, Steve.



STEVE GIBSON:  Hello, Leo.  I'm very secure with my insecurity, or is it the other way around?	



LEO:  I don't know.  I don't know.



STEVE:  We're going to have some serious geeky fun today for this fine - we just squeezed one last podcast into January, so here we are on the 31st with Episode 908 titled "Data Operand Independent Timing," DOIT.



LEO:  Do it.  Do it.



STEVE:  That's Intel's acronym.  Oh, yeah, DO IT.



LEO:  Do it, yeah.



STEVE:  AMD calls it just DIT.  They left out the operand, so that's Data Independent Timing.  But, you know, Intel wants to be longer, so Data Operand Independent Timing.  Anyway, we've got lots to do.  This week we embark upon another two-hour tour to answer some pressing questions.  What happens if the vendor of the largest mobile platform begins blocking old and unsafe APIs, and can anything be done to prevent that?  What new add-on is now being blocked by the dreaded Mark of the Web?  Would you have the courage to say no after your gaming source code was stolen?  Is any crypto asset safe, and what trap did our friend Kevin Rose fall victim to last week?



How can Meta incrementally move to end-to-end encryption?  Isn't it all or nothing?  What other new feature did iOS 16.3 bring to the world, and what's the latest government to begin scanning its own citizenry?  And why aren't they all?  Or are they?  What spectacular success gives the FBI bragging rights, and why is Russia less than thrilled?  What questions have our listeners posed?  What possible value is there to making up your own words?



How is SpinRite coming, and what is your favorite color?  What have Intel and AMD just done to break the world's crypto?  And what exactly did ChatGPT reply when it was asked by one of our listeners to explain an SSL certificate chain using the voice of a stoned surfer bro?  Leo will answer the question to that in his dramatic reading, once all of the answers to the preceding questions have been revealed during this week's gripping episode of Security Now!.



LEO:  Something to be prepared for and aware of, and keep your finger on the skip button.  Thank you, Steve.  This is good.  This is a jam-packed episode.  Lots to talk about.



STEVE:  We've got a lot.



LEO:  Including our Picture of the Week coming up in just a bit.  



STEVE:  So our Picture of the Week is a great one.  We're all familiar with IKEA; right?  You build it yourself out of a box of parts which you go buy.  So here we have - we see an IKEA box that is apparently for water.  And the box has been opened, the contents have been removed, and now our hapless purchaser is staring at the instructions, and then with his head turned at the three cylinders of gas, two of hydrogen and one of oxygen.  And in the foreground there's a little pail where he's supposed to deposit the result of combining, of course, the hydrogen and the oxygen in a 2:1 ratio in order to make his own water.



LEO:  That's the great Dave Blazek in his LoosePartsComic.com.  Very funny.



STEVE:  And I was just thinking, it's too bad that there's not something legible on the box where it says "Shipped to you from the Hindenburg."



LEO:  Some assembly required, yeah.  What would you do?  How would you assemble that to make water?



STEVE:  Oh, well, there is the way the Hindenburg...



LEO:  There's the Hindenburg model.  You just light it on fire, the hydrogen will gather oxygen from the air and turn it into water.



STEVE:  I believe that there's a way to do it with something like a fuel cell, where you're able to gradually combine...



LEO:  You need a catalyst.



STEVE:  Yes.  You're able to combine these in a way that is not explosively exothermic and ends you before you have a chance...



LEO:  You do it slowly.  Very, very carefully. 



STEVE:  ...to drink anything, yeah.



LEO:  It figures that you and I and our nerds who listen would look at this comic and then say, you know, how would you go about assembling that?



STEVE:  That's right.



LEO:  All right.  On we go.



STEVE:  So in a piece of very welcome news, in what will likely be a very effective move to reduce malware on the Android platform, Android 14 will begin fully blocking the installation of apps that target outdated versions of Android.  The guidelines for the Google Play Store have long ensured that Android developers keep their apps updated to use the latest features and safety measures that are provided by the platform.  But this month the guidelines were updated to require - and that's the big different word - "require" all newly listed Play Store apps to target Android 12 at a minimum.  Meaning no use of older, long since deprecated APIs which often incorporate no longer secure features, or they were just buggy APIs.



Until now, these minimum API level requirements only applied to apps that were intended for the Google Play Store.  Should a developer wish to create an app for an older version, they could do so and simply ask their users to sideload the APK file manually.  That's how I got my old Zeo app to work with the Zeo sleep headband.  It's way old.  Fortunately I'm able to just manually install it into an Android device, and it works.  And if an Android app hasn't been updated since the guidelines changed, the Play Store would continue serving the app to those who have installed it once before.



So there's been a lot of accommodation being made up to this point.  But according to a newly posted code change, Android 14 is set to make API requirements strict to block the installation of all outdated apps, meaning from any source.  And this will include blocking users from sideloading APK files and also block app stores from installing those apps.



Now, to minimize disruption, Android 14 devices will initially only block apps targeting very old Android versions.  They're not going to lower the boom immediately to where they want it to be.  And we've seen this from Google a lot, where they very sort of progressively slowly move forward.  So over time the plan is to raise the bar to Android 6.0 (Marshmallow), with Google having a mechanism to, as I said, then progressively ramp it up over time, forcing more and more currency from Android apps.  It will probably still be up to individual device makers to set their device's threshold for outdated apps, or maybe to enable or to prevent any of this from happening at all, so there is just no older limit.  It depends upon the manufacturer.



Google believes, I think with good reason, that this will curb the use of malicious apps on Android.  The Google developer who was responsible for the change notes that malicious apps intentionally target older versions of Android to bypass certain protections which are only enforced on newer apps.  And of course those who have been listening to the podcast may be reminded of the protocol version downgrade attacks against SSL and TLS, remember, where an attacker would pretend not to support any of the newer, more secure protocols, only saying, well, all I know how to do is SSL2.  Can we still talk?  And the unwitting server would say, oh, really?  Oh, okay.  And then be in trouble.  So as we've seen, it is often necessary to stop using and to prohibit the use of older obsolete and insecure technologies.  The same is exactly the case here.



And even so, I mean, again, Google really doesn't want to, like, wreck anything.  Even so, if for whatever reason it is absolutely necessary to install a very outdated application, it will still be possible to do that using a command shell, by invoking a new flag.  But given the extra steps then which would be required, it is much less likely that someone would do this by mistake and inadvertently install malware, you know, just by clicking on a, "Ooh, this looks like a good app" in the Play Store.  So some nice, much-needed improvement over on the Android side.



And speaking of blocking malware, Microsoft also plans to block the execution of Excel add-in (XLL) files inside Excel and other Office apps if the XLL files were downloaded from the Internet.  Microsoft says it made this decision to "combat the increasing number of malware attacks in recent months."  Newsflash, okay.  Anyway, you know, there's been a lot of abuse of Excel add-ins to bypass email filters and execute malware on user devices.  Unfortunately, as we know, the use of ZIP file containment to avoid, specifically, to avoid this dreaded Mark of the Web has skyrocketed among those creating and distributing malware, specifically because Microsoft has finally - after how many years? - started restricting what the Mark of the Web-marked files can do.



The reason I zipped Rob's LastPass vault script a few weeks back was to protect it from the Mark of the Web because I didn't want, you know, false positive worries among our listeners who wanted to use it.  The ZIP received the dreaded marking, but its contents were protected.  So it's not that I don't think that what Microsoft is doing is useful, it's how anything that was this insecure was ever allowed to happen in the first place. But, you know, that's the lesson we keep learning in this industry.



Riot Games reported that it had received a ransom demand via email from a threat actor who hacked one of its employees to then gain access to one of its game development environments. Riot says the hacker is asking the company to pay ransom, or they will release the source code for the League of Legends and Teamfight Tactics games, as well as the source code of a legacy anti-cheat platform.  The company says it does not intend to pay the ransom - Riot Games says nope, we're not paying - and expects the leaked source code to, as they said, "increase the likelihood of new cheats emerging."  Yeah, you think?



Anyway, I suspect that they also figure that there's really no way that the bad guys would actually honor their promise to destroy the gaming system source code.  It's just too juicy and tempting.  Besides, they're crooks.  There's little reason to believe that the source would not eventually emerge onto the Internet.  So I have a feeling that they did the right thing.



LEO:  Yeah, why pay?  Yeah, exactly.



STEVE:  Exactly.  The Cointelegraph reported that the threat actor behind the hack of the Wormhole cryptocurrency platform - and Leo, who would not want to invest in the Wormhole cryptocurrency platform?  My god.  This was almost a year ago, in February of last year.  That threat actor recently moved $155 million worth of the $321 million in assets they stole a year ago from the company, according to blockchain analysis platform CertiK.  So that's nearly half of that $321 million that they stole now being moved.  When Wormhole saw the funds move, they reiterated their willingness to pay a $10 million reward if the funds were returned to their wallets.



LEO:  Wait a minute.  Wait a minute.  They say we'll give you $10 if you'll give our $321 million back?



STEVE:  That's right.



LEO:  Who would say no to that?



STEVE:  See, there you go, Leo.  That's exactly right.  First, how can there possibly be so much cryptocurrency sloshing about?  We keep talking about hundreds of millions of dollars here and hundreds of millions of dollars there.  And there appears to be no end of random currencies being created and exchanges all loaded up with this cash.  And is anyone able to keep these assets to themselves?  When I was thinking about this story it occurred to me that it's like someone had come up with this great idea of having a bank before they had invented a safe to protect the bank's assets.



LEO:  Couldn't be a better analogy.  That's exactly it.  That's exactly it, yeah.



STEVE:  Yeah.  It's just like, this is crazy, just crazy.  Okay.  Unfortunately, our friend Kevin Rose was hacked.  And so I put this under the heading of "It really can happen to anyone."  Last Wednesday came the news that the super tech-savvy gazillionaire founder of DIGG, now tech venture capitalist, NFT maven, and really neat guy Kevin Rose fell victim to a classic social engineering attack.  As I said, yes, it really can happen to anyone.



On the 25th, Kevin tweeted a short message to his 1.6 million Twitter followers.  He tweeted:  "Good morning.  What a day.  Today I was phished.  Tomorrow we'll cover all the details live, as a cautionary tale, on Twitter spaces.  Here is how it went down technically."  And then in that tweet he quotes a long string of tweets.  The first one reads:  "Earlier this evening, @kevinrose was phished into signing a malicious signature that allowed the hacker to transfer a large number of high-value tokens.  Here is a breakdown of what happened, our immediate response, and our ongoing efforts."  So that was the first of a series of tweets.



I'm not going to delve into the details here since all of this just makes my eyes cross and makes me feel really old.  But among the assets, Kevin apparently lost control of some of his favorite "squiggles."



LEO:  Ahem.



STEVE:  I kid you not.



LEO:  This is so moronic.  This is utterly moronic.  By the way, he's lost nothing.  He still has the squiggle.  Note.  What does he lose?  Control of his squiggle.



STEVE:  I know.  So he said, in one posting he said:  "Even though this one was simple, not rare, LOVED [all caps] the pattern."



LEO:  Hey, Kevin, buy some real art from a real artist.  Hang it on your wall.  Man.



STEVE:  And then another one he said:  "Damn, I loved this one, too."  So, okay.  Three days later, last Saturday, Kevin followed up by tweeting.  He said:  "I see a handful of folks purchased my stolen NFTs.  If you have interest in selling them back to me, please DM!"



LEO:  I'm not going to say Kevin's doing this, but this happens every year around tax time.  I'm not accusing Kevin of that.



STEVE:  No, no.



LEO:  But it does, you will see this every year around this time.  



STEVE:  Well, and Leo, really, how can you put a value on that chromatic...



LEO:  Squiggle.



STEVE:  ...squiggle.  I mean, wow.  Like I said, wow.  Some other industry reporting that followed this event noted that the attacker made off with more than 40 NFTs, and that the stolen assets were worth $2 million on Wednesday, when the theft occurred; $1.4 million early the next day, Thursday; and just $1 million by Thursday afternoon.  Perhaps I'm not all that unhappy that I haven't chosen to waste - I mean invest - time in learning all about this weird world.



LEO:  There's nothing to learn, Steve.  You know exactly what you need to know.



STEVE:  Aside from the apparently uncontrollable lack of security, the whole thing doesn't feel like a financially stable ecosystem.



LEO:  On the bright side, you've got a $2 million tax loss you can write off, so that's good.



STEVE:  That's right.  Those little puppies that got away from me.



LEO:  Mm-hmm.  They really add up, you know.  Those little squiggles, they can really...



STEVE:  Especially when they're so cute, Leo.  Those are just...



LEO:  You don't need to add this to the list, but there is now a Ponzi scheme probe into Celsius, which was a long-time crypto lender, lent billions of dollars to people.  And now the investigators say, you know, we think they actually had no money, and they were taking new investors' money and lending it out.  And they basically had no assets to begin with.  Which is kind of stunning.  But this is the thing is they've created a financial instrument.



STEVE:  Unregulated.



LEO:  That they've cleverly unregulated, unsecured, and now all sorts of shenanigans can happen.



STEVE:  Yup, yup.  Okay.  So Facebook will be moving more users to end-to-end encryption.  And this puzzled me at first.  Meta has said that it plans to migrate more of its Messenger users over to the end-to-end encrypted, you know, E2EE version of Facebook Messenger over the next several months.  The company says users will be chosen at random, and users will be notified when their private conversations will be upgraded to the end-to-end encrypted version.  In addition, the company has also expanded the features of its end-to-end encrypted version, which now also supports some of the features that the original Messenger app had, such as link previews, chat themes, user active status, and support for the Android floating bubble mode.



But as I was putting this together, I was thinking, wait, how can you move some users to end-to-end encryption and not everybody at once?  If you have end-to-end encryption...



LEO:  But not to that end.



STEVE:  Yeah.  Both ends must be end-to-end capable.  Right?  Anyway, so Meta wrote:  "Over the next few months, more people will continue to see some of their chats gradually being upgraded with an extra layer of protection provided by end-to-end encryption.  We will notify people in these individual chat threads as they are upgraded.  We know people will have questions about how we select and upgrade individual threads, so we wanted to make clear that this is a random process."  In other words, we don't know either.  Okay.  



LEO:  Oh, god.



STEVE:  I know, what a mess.



LEO:  They promised this, by the way, years ago.  I don't know what's so hard about all this.



STEVE:  Yeah.  Well, let's see, let's see.  iMessage, Telegram, Signal, Threema, I mean, like Instagram, everybody else has done it; right?  Except Meta with Facebook Messenger.  Anyway, so that explanation clarified this a bit.  So apparently all Messenger users already have end-to-end encryption Messenger apps.  They just cannot themselves enable its use for all of their communications.  Meta instead is going to do that for them randomly for some reason.



LEO:  Well, this is not actually that unusual because when you roll out something like this, you know it's going to break people's stuff.



STEVE:  Yes, yes, yes.



LEO:  So you do it in a gradual fashion so that you can manage the breakage and maybe nip some of it in the bud if you discover a problem.



STEVE:  Right.  I'm sure that that's exactly what their plan is.  They want to be able to back out of it should they need to.



LEO:  Everybody does these staged rollouts now.  Nothing's rolled out all at once.



STEVE:  Yeah, yeah.  Well, and look at our browsers that are in multiple stages of prerelease channel, you know, under use by those who don't mind being on the bleeding edge and reporting problems.



Okay.  Last week, when I mentioned that with the release of iOS 16.3 Apple's full iCloud encryption would be available globally, I forgot to mention that this release also allowed the use of third-party FIDO-certified security keys for Apple ID.  Until now, Apple has allowed users to use various forms of two-factor authentication methods to secure their Apple ID accounts, but Apple has been slow to add support for these hardware dongles.  iOS 16.3 and macOS Ventura 13.2 are the first iOS and macOS versions which allow users to use FIDO-certified hardware security keys to log into Apple accounts.  So in case any of our iOS and macOS users have those keys and didn't have anywhere to stick them, now they do.  Oh, and NFC tokens will also work with the phone's NFC capability.



LEO:  There's a problem with this and Apple's advanced data protection, which is, if you do turn this on, you have to have - all your devices have to be running iOS 16.3 or you'll be locked out of older devices because they can't do it.  And Apple said this very clearly.  So, I mean, given as how you have to do it, I wouldn't recommend anybody rush to do it unless you are only using modern Apple devices that have all been upgraded.



STEVE:  Right.  And, you know, 10 years from now, when I finally have given up my iPhone 6...



LEO:  Ten years, really.



STEVE:  I guess I have a - I think my iPhone 10, I think it has 16.3 on it.



LEO:  It might, yeah, yeah.  They're pretty good about - that's one good thing that Apple does is they go pretty far back in time.



STEVE:  Okay.  So we've been following the growing and, I think, entirely sane and rational emerging practice of governmental security entities proactively scanning the networks of their own citizenry, commercial enterprises, and governmental services.  Poland's CERT, known as CERT Polska, recently described their Artemis system as follows.  They said:  "Artemis scans services exposed to the Internet to look for common vulnerabilities and configuration errors.  Regular scanning of entities that fall under the constituency such as schools, hospitals, or local authorities, allows us to monitor and improve their cybersecurity.  This is important because of the nature of these organizations.  They are all used by citizens on a daily basis, and any incidents affect them as well.



"The scan results are not shared publicly."  Good.  "They are instantly forwarded to the administrators of the systems in question.  The data is then used to address vulnerabilities and to detect similar issues in other parts of the infrastructure.  As a part of the scanning process, CERT Polska also verifies whether the identified vulnerabilities were fixed correctly.



"One important aspect of the created tool is that it enables administrators to easily distinguish scanning activity as conducted by CERT Polska.  This helps minimize the unwanted effects like unnecessary attack mitigation.  All relevant information is accessible to administrators on a dedicated page.  The scanning results, aside from improving the security of a specific entity, help us to create a better view of the current cybersecurity landscape and designate our resources where they are needed the most at the moment."



And bravo.  Again, I see nothing but upside to this, and all governments hopefully have similar undertakings underway.  In this case of Poland's proactive scanning, here's what they wrote about their results so far.  They said:  "The scanning process began on 2nd of January and has already produced some results.  We've scanned close to 2,000 domains and subdomains of local governments, and we were able to detect a few hundred websites based on outdated software.  We've also dealt with numerous cases where configuration files that included passwords, backup archives, and data records were publicly accessible.  We've also found a few dozen of incorrectly configured directories that contained the page source code and in some cases access credentials."



Wow.  Not bad for a start, and certainly easily justifiable in any subsequent budgetary meeting.  The use of outdated systems, as we know, doesn't necessarily translate into exploitable vulnerabilities, but it can point to IT administrators who are not keeping their public-facing systems current.  And that can lead to potential exploitation.  At the very least, it creates some very much needed feedback and accountability.  When a system gets hacked after an organization had been notified of a problem and for whatever reason chose to do nothing about it, the excuse of "Well, we didn't know" will no longer fly.



The Hive ransomware organization got hit.  In a very impressive piece of high-tech intelligence, law enforcement agencies from the U.S. and the EU have seized the servers and websites operated by the Hive ransomware gang.  You know, this is one of the top groups believed to be operating out of Russia.  The U.S. Department of Justice says the FBI secretly breached the Hive gang's infrastructure last July.



LEO:  Wow.



STEVE:  Yes.  They got in there, and they stayed stealth, from where agents retrieved more than 1,300 decryption keys over the past seven months.  Of these, the FBI distributed 1,000 decryption keys to past Hive victims, but also shared in real-time more than 300 new decryption keys to companies that had computers encrypted in ongoing Hive attacks last year.



LEO:  You know what's amazing is they did this, they've been doing it for six months, and no one said anything.  Like they kept it a secret.



STEVE:  Yes.



LEO:  What do you think they did?  They folded up a piece of paper and said, "Hey, buddy, here's something you might want.  Don't tell anybody I gave it to you.  You don't know me; I don't know you.  But you might want...."



STEVE:  I'm just saying, you might want these 29 digits, yes.  So they also shared in real-time more than 300 new decryption keys to companies that had computers encrypted in ongoing Hive attacks last year.  Officials say they prevented ransomware payments estimated at roughly $130 million, but they also notified many other companies when their networks were breached, even before Hive and its affiliates had a chance to deploy their ransomware and encrypt their data.



LEO:  Ooh, that's good.



STEVE:  They were in there before the bad guys were.  That's amazing work.



LEO:  I'm surprised Hive didn't notice anything.  Is it one of those ransomware-as-a-service deals?



STEVE:  Yes.



LEO:  Okay.  So maybe the affiliates weren't that smart.



STEVE:  Yes.  Yes, they have affiliates, exactly.  Since June of 2021, when the Hive gang launched its operation, so a year before that happened, so they've been going for a year, the group is believed to have made more than $100 million from ransom payments.  And here the FBI just took $130 million out of their pockets.  So, wow.  Nice going, FBI.



And Russia reacted.  The Russian government has blocked - I get to say my favorite Russian agency's name here in a minute.  The Russian government has blocked access inside the country's borders to the websites of the CIA, FBI, and the U.S. State Department's Rewards for Justice program.



LEO:  Oh, what?  That's admitting that they're protecting these guys.



STEVE:  Exactly.  Russian officials with Roskomnadzor...



LEO:  Roskomnadzor to the rescue.



STEVE:  You do it much better than I do, Leo.



LEO:  I love it.



STEVE:  The country's Internet watchdog told Interfax they blocked access to the websites for - I love this - "spreading fakes about the Russian military and discrediting them."  However, the timing of this decision is coincidental and might be telling, as it came just hours after the State Department offered a $10 million reward for information on the Hive ransomware gang and its possible ties to a foreign government.  Gee.  I wonder which foreign government.  Which one blocked access to the CIA, FBI, and the U.S. State Department?  Leo, I'm going to take a sip of drink, and let's tell our listeners how lucky they are to find out about our sponsor.



LEO:  Our show today brought to you by Roskomnadzor.



STEVE:  While you can still get it.



LEO:  Get it while you can.  No, that's not true.



STEVE:  We start with a bit of errata, a piece.  Thanks goes to our listener Edwin Rosales, who actually wrote to you, Leo, and you forwarded it to me.



LEO:  Oh, good, I'm glad you got that.  Okay, good.



STEVE:  Yeah.  He said:  "Hi, Leo.  FYI, in Security Now! Episode 906, Steve erroneously conflated Symantec with Norton," he says, "now Gen Digital, Inc."  He said:  "I pointed out to Steve neither Norton nor their parent company, Gen Digital, are affiliated with Symantec, ever since Broadcom acquired the Symantec brand and enterprise security assets back in 2019."



LEO:  It's so confusing.  We both lost track, yeah.



STEVE:  I appreciate the correction because I do conflate the two.



LEO:  Yeah, yeah.



STEVE:  Anyway, he said:  "I also mentioned that the acquisitions/reorgs are a bit confusing to follow.  After Symantec sold its brand and enterprise assets to Broadcom, what was left was the consumer product, which they rebranded as NortonLifeLock, which was again now Gen Digital last November 2022."



LEO:  So Broadcom doesn't own NortonLifeLock.



STEVE:  Correct.



LEO:  That's so confusing.



STEVE:  They only purchased the enterprise Symantec stuff.



LEO:  Business stuff, yeah, yeah.



STEVE:  So anyway, Edwin, thank you for the correction.  Okay.  I got a kick out of this person's Twitter name.  Their handle is @pt22, which is Person Typing #22.  So this is from Person Typing 22.  He asks:  "@SGgrc You mentioned Diceware on Security Now!.  Log2 of 7776" - which is the number of Diceware words there are, he says - "is 12.9 bits of entropy per word.  Yes, they have fewer bits of entropy per character, but a 6- to 8-word random Diceware phrase, plus one capital, digit, and special, is the holy grail:  memorable, easily typed, and secure."



LEO:  I hear this horse staple thing all the time.



STEVE:  I know.



LEO:  Can we debunk this?



STEVE:  Well, so writing back to Person Typing:  "I get the attraction of the Diceware idea.  But 12.9 bits of entropy per word is not a lot of entropy.  It's about equivalent to two randomly chosen characters worth of entropy.  So to get the 20 randomly chosen characters' worth of entropy that are about what you need, we'd need to use ten Diceware words.  And that's quite a lot of typing."  And your password that you're entering is often blanked, so you can't see your typos.  So, you know, I'm not buying that that much.  Much as you aren't.



Now, I don't know why it never occurred to me to just share what I have always been doing.  Somehow I've never talked about it, as if divulging my own personal system would make my own use of it more vulnerable.  But that's not the case.  If you've paid close attention to this podcast, you may have heard me mention some of the words I've made up through the years.  But I've been quite careful to never mention any of the made-up words I use for master passwords.



Made-up words are an intriguing compromise.  Let's consider the advantages of using your own, fun, made-up words that you won't forget because they're auditory.  Such non-words are easy to invent.  They could be shorter, like bingle, borhog, zinkles, crample...



LEO:  They make you laugh, too.



STEVE:  ...zootram, simulax, or jubaloo.  Or you could go with longer words like vorchhoggen, weiglestagen, rambloses, plakonkits, or footremith.  I do want to point out that you will want to steer clear of anything having to do with framulators.



LEO:  No framulators or incanabulators, no, no, no.



STEVE:  No framulators, no, no, no.  So my point is, made-up words have a lot going for them.



LEO:  How about Roskomnadzor? 



STEVE:  Well, close.  Being phonetic, they're fun to say and memorable if you practice them a bit.  Unlike Diceware words, they're not going to be in anyone's dictionary.  And since each one can have many characters, when you use several of them together you get a self-padding longer haystacks-style benefit, as well.



So my best advice, it's actually what I do, is invent a few of your own fun words, add some capitalization, and toss in a random special character between each word and you've got something that's memorable, with good entropy, that won't be found by any dictionary attack and won't be cracked within several lifetimes.  And, oh, if you're feeling a lack of inspiration or imagination, look on the 'Net under "fake word generators."  You'll discover that many of those exist.  You can use them as a starting point, building your own set of personal words that you wind up settling on.



LEO:  I've got to point out, though, that anything at all, and this has got to be provable, that you can remember is inherently less secure than random.



STEVE:  That's absolutely true.  And, I mean, it is absolutely true.



LEO:  And the reason it's less secure than random is because English has rules about what letters follow what letters.



STEVE:  Yes.



LEO:  And so, and you're following those rules even when you're making up a word.  Otherwise it wouldn't be memorable.  It wouldn't be a word.  So there are rules about - and that lowers the entropy between letter 2 and letter 3.  There are rules about how they can be combined.



STEVE:  Absolutely.  Absolutely does.  But...



LEO:  It's still sufficient, probably; right?



STEVE:  Well, from a brute force attack standpoint, what you would need would be an algorithm for the brute forcer to be guessing made-up words that aren't in the dictionary, but which, you know, some person may have come up with because they're...



LEO:  I don't think you go that far.  I think you just say, well, I've got an E here.  And it's very unlikely that E would be followed by some letters and more likely to be followed by other letters.  



STEVE:  Right.



LEO:  It's still pretty - I mean, look.  If it's long enough, it's still intractable.



STEVE:  I think that footremith as a word, that's going to be, you know...



LEO:  Yeah, but that T and R, T is often followed by R.



STEVE:  And Leo, I'm not saying that 20 bits of gibberish aren't better.  But you can't - by that's just, you know...



LEO:  Can't remember it.  Now, you tell me what this - so this is what I do.  I think of a phrase, let's say "To be or not to be, that is the question," and I take initial characters of each of those.  Because those are less likely to be related one to another.  So "To be or not to be, that is the question" would not be a good passphrase.  Not only is it English words, but it uses rules of relationship between letters.  But the letters TBNTB, or even better, the number 2BN2B, are not so closely related.  You'd have to know what the originating phrase is.  Now, obviously, don't choose "To be or not to be" because that might well be something in someone's dictionary.



STEVE:  Or the Gettysburg Address or the Constitution.



LEO:  Right.  Choose something, you know, that nobody would think of, maybe pick a book that you really like and take page 300 and the third paragraph in and use that first sentence.



STEVE:  If you have a favorite passage from the Bible.



LEO:  Yeah, exactly.  And then I usually intersperse that with some other rules.  Rules are always risky.  Rules imply less than random.



STEVE:  And you don't have to use the first letter, either.



LEO:  Ah, there you go.  Second letter.  There you go.



STEVE:  Or your own algorithm of alternating which letter you use.



LEO:  Right.  Or another algorithm somebody's used.  And the idea is that you can reconstruct it in your head because you know what the rules are.  Somebody said take the last 10 presidents, capitalize them if they're Democrats, lowercase them if it's Republican.  You'd know that.  But that's going to be very hard to brute force, I think.  Right?  



STEVE:  Yup.



LEO:  The point is to make a password you can build, you can reconstruct knowing what rules to use.  And then as I mentioned last week, I always add my childhood phone number, my zip code...



STEVE:  Oh, one tip.  Do not have anything to do with your birthday.  Do not put your birthday...



LEO:  Oh, please, that's well known.



STEVE:  ...in your own password.



LEO:  Right.  And don't do, as Paris Hilton did, and include the names of your well-known dogs in the password.  If it's in Wikipedia, don't do it, I think, would be the...



STEVE:  Yes, because that means that ChatGPT knows about it.



LEO:  It always knows, yeah.



STEVE:  Yeah.



LEO:  So I think there are ways to come up with memorable or reconstructible passwords that are even better than Diceware words.  To me.



STEVE:  No argument.  No argument.



LEO:  And I don't think Diceware is all that good.  I mean, I guess if you had 10, maybe.



STEVE:  And that's the point.  You need 10 in order to get the kind of entropy that we would like to have.  And then it becomes, you know, again, you're going to type 10 words where you cannot see what you're typing?



LEO:  No.  A lot of these password numbers - this is a Diceware page somebody put together.  They use these, oh, look at the number of possible passwords, look how large that number is.  But that isn't really germane to it because these are all in a dictionary.  



STEVE:  Yup, exactly.



LEO:  So you can greatly reduce the search space.



STEVE:  Exactly.



LEO:  And that's what it's all about is reducing the search space.  All right.



STEVE:  We have a listener, Barry Wallis, who he said - actually he tweeted to both of us, @SGgrc and @leolaporte.  But I realized, Leo, you're no longer seeing @leolaporte.



LEO:  Good luck.  I haven't seen anything on that stage for a long time.



STEVE:  Barry said:  "Listening to the last SN had me come up with a new term.  Instead of script-kiddies, we now have chat-kiddies."



LEO:  Ugh.  Yup.



STEVE:  And I liked Barry's thought.  We do need a new term that's similar in concept to script-kiddies.  But I think the word "chat" maybe isn't domain-specific enough.  So maybe "chatbot-kiddies," where they're using chat bots to write their scripts for them.



LEO:  Yup.



STEVE:  Last Tuesday Bernd, tweeting from @Quexten, he said:  "Hi, Steve.  Glad to hear you mention changes towards Argon2 support in Bitwarden today.  Some clarification of why the scrypt pull request is closed now, in favor of Argon2."



LEO:  Oh, good.



STEVE:  "I first implemented scrypt because the libraries were more widely available - pure JavaScript in the browser, and ready-to-use Bouncy Castle library on mobile.  After the simple-to-implement scrypt support was done, I began work on the Argon2 pull requests, expecting them to take longer, and they did take a fair bit more work.  Argon2 requires WebAssembly in the browser, and that was a concern in the forums in the last few years.  But these days all browsers except IE, which hopefully no one uses, support it.  Mobile support was also a bit more work.  And finally, changes in the server are necessary to account for Argon2's extra parameters.



"Scrypt mainly relies on a single work factor which determines both memory and time complexity, while Argon2 configures memory and time separately.  Because of this, changes in the backend  communications are necessary to send these extra parameters required by Argon2, compared to PBKDF2's simpler iteration count.  Since the initial Argon2 pull request was complete before scrypt was reviewed and merged, and since multiple new key derivation functions seemed redundant, and Argon2 is the newer, more crack-resistant function, we decided to close the scrypt pull request to focus on Argon2.  OWASP also recommends Argon2 over scrypt, scrypt over bcrypt, and bcrypt over PBKDF2.  Anyways, after some back-and-forth with the Bitwarden team, we are close to getting support merged, which is exciting."



Now, that was one week ago today, last Tuesday.  Then yesterday at 8:46 a.m. he tweeted:  "Final update on Argon2 in Bitwarden.  Support has now been merged into their master branches for mobile, desktop/web, and servers.  Next release should feature Argon2 as a new PBKDF option."



LEO:  I thank you for that, and I thank Quexten for writing it.  And we should mention Bitwarden's a sponsor.  But there's the beauty of open source right there.  Quexten doesn't work for Bitwarden, but he was able to do a pull request and get it integrated in.



STEVE:  Yup.  And if this is an example of the agility that we have with open source approach...



LEO:  Isn't that great.



STEVE:  ...and the mentality that this can bring to security products where agility can be crucial, I'm sold.



LEO:  Boy, is that great.  I can't - the minute they turn that on, we will tell everybody, and we'll get you all to switch over from PBKDF2 to Argon2.  Nice.



STEVE:  Much more GPU brute force attack resistant.



LEO:  Fantastic.



STEVE:  It's a great solution.  Dennis Keefe, who's I guess a financial coach, that's in his name, he said:  "Steve, in regards to ChatGPT becoming so popular, what do you think would be the best career path to focus on over the next five years?  I'm currently working on Linux sysadmin certifications."



Okay.  So, you know, the use of these Large Language Model transformers is bringing us to the brink of something.  But like most big game-changing somethings, we almost certainly do not yet fully understand the something that we're on the brink of.  If you want to go back to school, there's likely to be big career opportunities in artificial intelligence.  Just as all larger companies need a CEO, and a CFO, and a COO, and a CIO, it may very well be that before long there will be a CAIO position at the top, as well, because I suspect that the application for this technology is going to surprise us.



But my best advice for picking a career, any career, has never changed.  First and foremost, follow your heart over your wallet.  Many people with fat wallets have thin lives.  Find something you love and work to get really good at it.  You'll enjoy the process of getting good at it, and then you'll love being good at it.



One thing we know for certain is that the career opportunities in cybersecurity are very real.  As we've discussed, they're likely not for someone who wants to punch out at 5:00 p.m. every weekday, since computers never sleep, and bad guys are often in faraway time zones.  But it should be clear to anyone who follows this podcast that cybersecurity is a growth industry today.



Mark Sidell asked, he said:  "Steve, Bitwarden can store the TOTP seed" - that's the time-based one-time password - "for a website.  When you visit the site, it will automatically copy the current six-digit code to the clipboard, making it simple to paste the code into the site's MFA control.  Would you ever use this Bitwarden feature?"



LEO:  I've been meaning to ask you about this because how convenient is that; right?



STEVE:  He says:  "It would seem to reduce MFA protection to a single factor, your Bitwarden master password."  And I think that Mark is exactly right.  One of our recurring observations is that just because something can be done doesn't mean that it should be done.  I don't mind having Bitwarden offering this feature, but I would never consider using it for exactly the reasoning that Mark suggests.  The entire benefit of the one-time password auth app running in my iPhone is that it is physically and logically disconnected from the website I am authenticating to.  If my password manager is able to fill in my username, my password, and my time-varying six-digit token, then a useful aspect of that second factor separation is lost.



Now, the counter argument to this is that what the one-time token actually protects against is the theft and reuse of our static credentials.  It makes one of our required credentials dynamic so that credential reuse, which of course was last week's topic, is completely thwarted.  When viewed from that perspective, having a password manager also able to provide the dynamic component of a set of credentials seems reasonable.



So I suppose that what makes me nervous about turning over my one-time password generation to the same system that's holding all of my other credentials is the "all of my eggs in one basket" concern.  As I've said, the first thing that went through my mind when I heard that the LastPass customer vault backups were now in the hands of bad guys was that all of the sites I most cared about are set up with time-varying one-time password tokens that LastPass never had any awareness of.  That turned out to be a blessing.  So I'm glad that my one-time password token generation keys were never theirs to lose.



LEO:  You print your QR codes out; don't you.



STEVE:  I do.



LEO:  And store them somewhere.  Yeah, that's probably a good idea.



STEVE:  Yeah, I do.  John tweeting from @PsyVeteran, he said:  "Hey, Steve.  I'm looking back through the podcast and show notes looking for the name of the VPN-like technology you guys talked about and reference every now and then.  One of its features was much wider bandwidth than classic VPN technologies.  Could you recall the name for me?  I'm stumped.  Warm regards and thanks for your great insights as always.  John."



Okay.  I think that what John is asking about is what's now being referred to generically as "Overlay Networks," or sometimes "Mesh Networks."  This was what we discovered early in this podcast with Hamachi, and there are now a number of similar solutions.  The ones we've talked about in the past are Tailscale, ZeroTier, and Nebula.  For example, Nebula, which we last talked about, is an open source peer-to-peer mesh network which was created by engineers at Slack and open sourced after several years of their own internal use.



Also, many things impress me about Tailscale, including that it's a mesh network based on WireGuard, which is the right core.  And after we talked about Tailscale, many of our listeners gave it a try and specifically wrote to say that they were astonished by how easy it was to set up and use.  It's like zero configuration, yet super secure.  And it is also free for personal use and hobby projects.  You're able to connect up to 20 devices for secure peer-to-peer connections, also offering single sign-on and multifactor authentication.  On the other side, ZeroTier is open source.  So if you're more focused on open source or only want that kind of solution, ZeroTier is exactly the same sort of thing.  So anyway, based upon its specification and the amazing experiences of our users, I would say any of those three - Tailscale, ZeroTier, or Nebula.  And I'm pretty sure that's what John was talking about.



I had a note here that I did not have a chance to flesh out.  Steven Lacey asked:  "@SGgrc During Security Now! 905 titled '1'" - because that was remember the horrible iteration count that some LastPass users found still set in their vaults.  He said - I know.  "You mention the lack of an API for password changes.  Is there any chance this could be implemented in a safe and secure way?"  And I thought that would be a fun thing to do a little brainstorming about and think about.  And I just didn't have time to do it.  So thank you, Steven, for the question.  I'm going to keep it on the back burner.



A quick bit of update on the SpinRite front.  We had a very productive week dealing with various oddball edge cases.  I'll share just two examples.  We discovered that there are some BIOSes whose USB support uses the available 32-bitness of the processor while not preserving the high 16 bits that they modify.  Now, this would be okay if only 16-bit client programs were running on those machines, since those clients would be unaware that they are actually operating on a 32-bit processor.



But SpinRite is now working with multiple 16MB buffers within a flat 32-bit address space.  So it has grown into a real 32-bit application running within a 16-bit DOS real mode environment.  API functions which are called are required to preserve any registers that they use.  But we discovered that the USB functions on some AMD motherboards are not doing that.  So now SpinRite is protecting itself proactively from that behavior, and a handful of mysterious misbehavior that we've been experiencing all disappeared immediately.



LEO:  Interesting.



STEVE:  Uh-huh.  I mean, this is sort of where we are now.  We're like, SpinRite's working.  It's done.  But people have, you know, some number of people will be booting a USB on an AMD motherboard, and what was happening was that they were getting a message saying that SpinRite's executable was corrupted, that SpinRite checks itself to make sure that it hasn't been infected because you want to make sure that it's working correctly.  Well, that self-test was failing.  And it wasn't failing for me.  It wasn't failing for a bunch of people.  But it was failing for a bunch of other people.  It was like, what the heck?



Well, it turns out most of them had AMD motherboards.  And I ended up, in fact, a German user, Chris, early on he had a motherboard where I couldn't reproduce what he was seeing.  So I got that motherboard from eBay and set it up and had it around.  So he was having the problem.  He said:  "Steve, you've got one of these motherboards."  I dusted that one off, plugged it in.



LEO:  Do you have a motherboard rack somewhere with a bunch of...



STEVE:  Oh, I've got my - yeah.  Well, and in fact I received 10 hard drives which were Fedexed to me by a Canadian tester because - in fact, this is the second example.  I wrote:  "In a second example, a Canadian tester named Andre sent me some drives, one of which was reliably causing SpinRite to crash.  I had a similar drive that was also misbehaving, but SpinRite did not crash for me.  The drives arrived yesterday morning" - they did, Monday morning - "so I plugged in the culprit and at long last recreated the crash that Andre and others had been able to independently reproduce.  I saw when it was happening, went there in the code, watched the problem occur, saw that something was modifying the stack, found the problem, fixed it.  No more crashes."



So it's funny because in our communication he had boxed up these 10 like dead or dying hard drives, and he made a comment that his wife was really glad to see those leaving because...



LEO:  Well, they've been on the dining room table for six months.



STEVE:  Because, you know.  And I wrote back, and I said:  "Andre, how is it possible that you and I have both married the same woman?"



LEO:  I think many of us have.



STEVE:  Yeah.  And Lorrie's very patient.  She says:  "Are you done with this hardware testing part yet?"  "No, not quite, honey.  So just step over that pile."  Because I know where each one of those things are and what they do.  Okay.  So anyway, that's where SpinRite is now.  It's done, and it's working.  629 current SpinRite owners have been testing it, and I'm happy to say that most of them are bored.  Bored testers is what you want.  But not all of them are bored yet, and I want all testers to be bored because they've been unable to find any wacky system or damaged drive that causes SpinRite any trouble.  So we're getting there.  There's always the 95/5 rule, but I want to push this thing all the way so that, well, because once I get it published, I want to immediately start working on SpinRite 7 and not be dragged back to deal with SpinRite 6.1 things that I didn't find.  So when it's done it's really going to be done.



Last Wednesday the 25th, Eric Biggers, a software engineer at Google on the Platform Encryption Team, brought a significant cryptographic security issue to the attention of the well-trafficked OSS Security list in a posting he titled "Data operand-dependent timing on Intel and ARM CPUs."  Now, admittedly, in any other venue than ours that might sound dry.  But not here.



We were recently talking about data-dependent timing, which is a huge issue for cryptographic security.  We were talking about it in the context of the scrypt PBKDF algorithm which might suffer from a side-channel attack, as the initial version of Argon2 also did, due to the fact that the value of the user's password directs the functioning of the algorithm.  In other words, if the algorithm operates in any way differently depending upon the data that it's processing, specifically anything that must remain secret like the user's password or the algorithm's secret key, then it's theoretically possible to reverse that process by observing the algorithm's behavior, perhaps from afar - things like power usage, execution timing, cache hits and misses, branch prediction traces, whatever - to figure out what data must have been given to the algorithm in order for it to behave the way that it was observed to behave.  And we've seen how astonishingly clever researchers have turned out to be in this regard.



One of the reasons the Rijndael cipher won the competition to become the AES standard was that its operation was beautifully independent of the secret key it was operating upon.  No jumps or branches were taken or not based upon the algorithm's secret key.  You don't want to have any secret-dependent behavior.  But what if the timing of the instructions themselves was dependent upon the data that the instructions were processing?



Now, a traditional example of this may be familiar to old coders, and that's CPU multiply instructions.  Binary multiplication is an inherently complex process.  So inside a processor, multiplication was traditionally an iterative process, with the number of clock cycles required varying widely depending upon the data that was being multiplied.  In the show notes I have a table showing the number of clock cycles required by various early Intel CPUs, from the original 8088/86, the 286, 386, and the 486.  And even the later of those, the 486, a 32-bit multiply will require anywhere between 13 and 42 clock cycles, where that count is entirely dependent upon the data that's being multiplied.  So we come back to the question.  And you can see it there on the screen.



LEO:  If that's meaningful to you, you're a better man than I.



STEVE:  So those ranges, 9-14 under 386, 12-17, 13-18, those are the ranges of clock cycles that that single instruction could require to execute, depending upon the data that it's being asked to multiply.  So we come back to the question, what if the timing of the instructions themselves was dependent upon the data that the instruction was processing?  Which leads us into what Eric wrote.



So Eric from Google said to Linux people:  "Hi.  I'd like to draw people's attention to the fact that on recent Intel and ARM CPUs, by default the execution time of instructions may depend upon the data values operated on.  This even includes instructions like additions, XORs, and AES instructions, that are traditionally assumed to be constant-time with respect to the data values operated on.  For details, see the documents from each vendor.



"Non-constant-time instructions break cryptographic code that relies on constant-time code to prevent timing attacks on cryptographic keys, which is most cryptographic code.  This issue may also have a wider impact on the ability of operating systems to protect data from unprivileged processes.  For Intel, processors with Ice Lake and later are affected by this issue.



"The fix for this issue is to set a CPU flag that restores the old, correct behavior of data-independent timing:  DIT (Data Independent Timing) on ARM, and DOITM (Data Operand Independent Timing Mode) on Intel.  Linux v6.2 will enable DIT on ARM, but only in the kernel.  Without any additional patches, user space code will still get data-dependent timing by default.



"No patch has been merged to enable DOITM for Intel processors.  Thus, as-is, it's not really possible to safely execute cryptographic algorithms on Linux systems that use an Intel processor with Ice Lake or later."  And he says:  "I'd guess that the same is true for other operating systems, too; Linux is the one I'm looking at.  To fix this issue, I've proposed a Linux kernel patch that enables DOITM globally."



And he finishes:  "I consider this issue to be a CPU security vulnerability.  It shares many characteristics with other CPU security vulnerabilities such as Meltdown and Spectre.  However, Intel and ARM do not seem to consider it to be a security vulnerability.  No CVEs seem to have been assigned yet.  Eric."



Okay.  First of all, CVEs are not generally assigned to things that are deliberate and by design, as everything Eric is complaining about is.  It's not fair to compare this to Spectre and Meltdown, which shocked and rocked the computing world five years ago, in January of 2018.  But being today's podcast topic, you can bet that there are some interesting details here to share.  After encountering Eric's posting, I started digging. And the fairest characterization would not be to say, as Eric did, that Intel doesn't seem to consider this to be a security vulnerability, only that they have decided to turn this over to developers.



Okay.  First, I was curious about when this suddenly became a problem.  So Intel explains.  They wrote:  "For Intel Core family processors based on microarchitectures before Ice Lake, and Intel Atom family processors based on microarchitectures before Gracemont, neither of which enumerate" - and then they have the name of a feature, it's IA32_UARCH_MISC_CTL, which is an internal control register.  So microarchitectures before Ice Lake on the core and before Gracemont on the Atom family don't have that register.  So they say:  "Developers may assume that the instructions listed here operate as if DOITM is enabled.  Intel Core family processors based on Ice Lake and later, such as Tiger Lake, Lakefield, and Rocket Lake, will explicitly enumerate DOITM."  In other words, demonstrate that it's there available to be turned on or off.  "Intel Atom family processors based on Gracemont and later will also enumerate DOITM."



Okay.  So translating that a little more into English, Intel's earlier processors executed all of their instructions in constant time.  So they were inherently safe to use, regardless of OS, kernel, userland, or anything else.  Instruction timing did not vary based upon the data that the instruction was processing.  If the data in two registers were added, XORed, or multiplied, the instructions always took the same amount of time regardless of what was in the registers being used.  Then that changed.  What must have happened is that Intel realized that there was a way to optimize and speed up the execution of some instructions depending upon their data.



Okay.  So here's an off-the-cuff example that I've just made up to highlight the idea.  One of cryptographers' most favorite instructions is the XOR, where one of the instruction's two datums conditionally inverts the bits of the other.  XOR is also known as carry-less multiplication since the operation is similar to multiplication, but where adjacent bits do not carry an overflow into the next most significant bit.  Thus adjacent bits do not affect one another.  And that's significant since that means that adjacent bytes don't affect one another either.



Now suppose that Intel's internal microarchitecture contains lots of granular execution engines as, in fact, we know it does.  So imagine that the work of performing a 32-bit XOR could be subdivided into four separate 8-bit XORs, with each 8-bit XOR being handled by a different execution microengine.  Then we observe that with an XOR, any time either of the bytes being XORed is zero, no data is changed, and the XOR has no effect.  Thus there's nothing for that little microengine to do, and it could instead be made available to work on other instructions.



So in this little synthetic example, we see how a 32-bit XOR which encountered non-zero data in all four bytes of both arguments would need to enlist the help of four 8-bit microengines, whereas the same 32-bit instruction presented with bytes of zeroes in either argument would leave those microengines free to work on other instructions.  In this fashion, the effective execution time of such a processor's 32-bit XOR becomes dependent upon the data it's XORing.



Okay.  So was Intel ever going to pass up the opportunity to make their chips go faster by arranging for some instructions to go faster some of the time?  No, no way.  But whoops.  A side effect of this is that it would screw up the longstanding assumptions made by cryptographers that the execution speed of instructions was independent of the data being processed.



So what did Intel do?  They could not permanently break everyone's crypto, sending us all back to the dark ages.  So they added a mode called, as Leo calls it, "DO IT," Data Operand Independent Timing.  The controversy is that by default it's off.  This means that suddenly the behavior of Intel's newer chips has changed.  They are no longer safe for crypto.  They got somewhat faster by being able to finish some instructions more quickly.  But the side effect of this is that they have also became insecure processors for performing cryptographic operations.  Unless this new DOIT mode is explicitly and deliberately turned on, it will be off.  And what Eric is lobbying hard for within the Linux kernel is to immediately turn the darn thing on, permanently and globally.  The only real question is, is Eric overreacting?



Intel, for their part, writes this.  They said:  "Software can enable Data Operand Independent Timing operation on a logical processor by setting DOITM to 1.  Setting DOITM to 1 may impact performance, and that impact may increase in future processor generations.  Users should evaluate their threat model to decide whether this is a significant threat to their applications."  I know.



LEO:  You don't need to evaluate your threat model to know you  like encryption working.  Holy cow.  Oh, you don't need encryption, Steve.



STEVE:  It would all finish in approximately the same amount of time.  Okay.  "And then ask the operating system," says Intel, "to only deploy DOIT mode to applications that they deem necessary."  In other words, says Intel, sure.  You can have old-school constant-speed instructions if you really think you need them.  But we're hereby abandoning that model in the interest of performance today and probably even more so in the future.  So now it's going to be up to you.



Okay.  So now we're back to trying to get a sense for how bad the problem is, and how much cure we need to pour over it.  Thomas Pornin, the author of the BearSSL SSL/TLS library has some nice real-world reality-check perspective about the threats and challenges of constant-time crypto.



Thomas writes:  "In 1996, Paul Kocher published a novel attack on RSA, specifically on RSA implementations, that extracted information on the private key by simply measuring the time taken by the private key operation on various inputs.  It took a few years for people to accept the idea" - Leo, we were so innocent back then.  Those were just quaint times.



LEO:  Security Now! had not yet been invented.



STEVE:  "It took a few years for people to accept the idea that such attacks were practical and could be enacted remotely on, for instance, an SSL server.  In an article from Boneh and Brumley seven years later in 2003, they conclude that:  'Our results demonstrate that timing attacks against network servers are practical, and therefore all security systems should defend against them.'"



Thomas writes:  "Since then, many timing attacks have been demonstrated in lab conditions, against both symmetric and asymmetric cryptographic systems.  This requires a few comments. First, while timing attacks work well in research conditions, they are extremely rarely spotted in the wild."  He says:  "(I am not aware of a single case).  Timing attacks usually require many attempts to gather enough samples for statistics to reveal the sought timing difference.  As such, they tend to be somewhat slow, and not very discreet.  This does not mean that timing attacks are not real or do not apply, only that the state of the security of many systems is such that typical attackers have easier, faster ways in."  In other words, it's not the lowest hanging fruit.



"Another important point," he says, "is that when timing attacks apply, they are all-encompassing.  If the context is such that secret information held in a system may leak through external timing measures, then everything the system does may be subject to such leaking.  This is not limited to cryptographic algorithms.  Research on timing attacks tends to focus on secret keys because keys are high-value targets."  He says:  "A key concentrates a lot of secrecy.  And cryptographers talk mostly about cryptography.  However, even if all cryptographic algorithms in your system are protected against timing attacks, you are not necessarily out of trouble in that respect.  In BearSSL I am doing my part by providing constant-time implementations for all operations that are relevant to SSL.  But slapping a constant-time SSL implementation over existing software is not sufficient to achieve general timing immunity.  This is only a good start.



"Timing attacks are a subset of a more general class of attacks known as side-channel attacks.  A computer system runs operations in a conceptual abstract machine that takes some inputs and provides some outputs.  Side-channel attacks are all about exploiting the difference between that abstract model and the real thing.  In the context of smart card security, for instance, power analysis attacks, in particular Differential Power Analysis that compares power usage between successive runs, have proven to be a great threat.  Timing attacks still have a special place in that they can be applied remotely through a network, while all other side-channel leakages require the attacker to be physically close to the target.



"Constant-time implementations are pieces of code that do not leak secret information through timing analysis.  This is one of the two main ways to defeat timing attacks.  Since such attacks exploit differences in execution time that depend on secret elements, make it so that execution time does not depend on secret elements.  Or more precisely, that variations in execution time are not correlated with secret elements.  Execution time may still vary, but not in a way that can be traced back to any kind of value that you wish to keep secret, in particular - but not only - cryptographic keys."



Okay.  So where does all this leave us?  With Linux we have an open process and transparency.  Even the discussion will be transparent.  So anyone interested can follow along and see what both sides say about this.  We don't have that with Windows or Apple.  And even though Apple has gone ARM, ARM is doing the same thing, calling theirs DIT.  If history repeats, it's likely going to take some of those clever researchers demonstrating that Windows needs to take this seriously before Microsoft will ponderously go about doing so.



It appears that the future of our processors will be one of super-optimized variable-speed instructions with many of those instructions able to finish their work faster, depending upon what that work is.  But since doing this is anathema to crypto security, crypto implementations will need to be updated to flip on constant time mode while they do their work, then flip it back off afterward.  I imagine that someone like BearSSL's Thomas Pornin will quickly add this awareness to his library, as will everyone else who has any sense.



Since actual real-world timing attacks have only been shown in the lab, does this mean that they are impractical?  We know that crypto is where the truly paranoid are welcome.  Linux may be panicking a bit, depending upon the degree of true performance hit arising from flipping the switch into always-on mode.  It does appear that wrapping timing sensitive crypto in constant-time mode will be the right solution, rather than simply wrapping the entire operating system in it.



This whole thing, the whole topic is far too juicy for the world's security gurus to pass up.  That band of merry researchers at the Ben-Gurion University of the Negev must be rubbing their hands together in delight.  I suspect that we'll be reading a paper of theirs before long.  So this is now on everyone's radar, and I'm sure we'll be circling back to this intriguing topic in the future.  For now, it's wait and see.  But for ChatGPT we have its Statement of the Week.



LEO:  Is this a new feature?



STEVE:  Yes, the ChatGPT Statement of the Week.  Biocow, tweeting from @biocow, tweeted the output from ChatGPT when he asked it to "explain an SSL certificate chain like a stoned surfer bro."



LEO:  Hmm.



STEVE:  And Leo, I think you should do the honors.



LEO:  "Dude.  Dude.  An SSL certificate chain is like this gnarly wave of trust, bro.  You've got your main certificate, that's like the Big Kahuna, the main dude.  Then you've got all these other little certificates, like tributaries, that all flow together to create this one big wave of trust.  And it's all verified by this dude called a certificate authority.  He's like the lifeguard of the Internet, dude, making sure everything is kosher and legit.  And when you surf a website, oh, it's like you're catching your wave, and it's like you know you're in a safe spot, no sharks or anything, just pure shredding, dude.  Pure shredding."  That's pretty good for ChatGPT.  I'm impressed.



STEVE:  I know.  I think astonished.



LEO:  We feed this into one of the new voice synthesizing devices like 11, and you're going to have something there.



STEVE:  Leo, this large language model technology is going to change the world.



LEO:  It's wild; isn't it?



STEVE:  It is going to change the world.



LEO:  Yeah, yeah.  It's fascinating.  I just was reading that the creators of Instagram, Kevin Systrom and company, have a new startup.  I guess when they quit Facebook they probably - they said you can't do anything else for a couple of years.  So they just launched something called Artifact, which is a newsfeed driven by artificial intelligence.  So that's going to be good. 



STEVE:  That's a great name, too.  Oh, my god.



LEO:  Artifact, yeah.



STEVE:  What a great name.



LEO:  Yeah, yeah.  Has a lot of resonance in various areas.  We live in interesting times, Steve.  See, this is why you don't want to stop at 999.  It's just getting started.  And you know, a couple of years, we're going to have so many things to talk about.



STEVE:  We'll see where we are, my friend.



LEO:  Steve Gibson, he's at GRC.com.  That's the Gibson Research Corporation.



Copyright (c) 2023 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#909

DATE:		February 7, 2023

TITLE:		How ESXi Fell

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-909.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  Leo used to say at the top of our Q&A episodes: "You have questions, we have answers."  Now we tease most of the questions and provide their answers.  This week we wonder:  What is about to happen with the EU's legislation to monitor its citizens' communications?  Why would a French psychotherapy clinic be keeping 30,000 old patient records online, and who stole them?  What top-level domains insist upon, and enforce, HTTPS?  How is Chrome's release pace about to change?  When you say that Russia "shoots the messenger," is that only an expression?  Were a fool and his crypto soon parted, or should that be "was"?



Exactly why is QNAP back in the news, and what do I really think about Synology?  Would companies actually claim unreasonably low CVSS scores for their own vulnerabilities?  Nooooo.  What questions have our listeners been asking after all this recent talk about passwords?  What's the whole unvarnished story behind this weekend's massive global attack on VMware's ESXi servers, and who's really at fault?  These questions and more will probably be answered before you fall asleep, but no guarantees.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  We're going to talk about, in fact at length, about the EU's new legislation to monitor citizens' communications.  This is a bad one, folks.  Steve's got the details.  He'll tell you why he doesn't like QNAP, but he does like Synology.  If you're looking for a NAS, you want to hear that.  And then a look at VMware's ESXi servers, a massive exploit that's already claimed thousands of victims, and it's just a couple of days old.  All that and more coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 909 recorded Tuesday, February 7th, 2023:  How ESXi Fell.



It's time for Security Now!.  Yeah, you've been waiting all week for the best show on the network.  Mr. Steve Gibson makes it so.  Hello, Steve.



STEVE GIBSON:  Yo, Leo.  		



LEO:  Good to see you.



STEVE:  Good to be with you for 909, our first show of February.  And of course we've got questions.  Now, you used to say at the top of our Q&A episodes:  "You have questions; we have answers."



LEO:  Yes.  Whatever happened to those?  We stopped those.



STEVE:  Yeah, well, because now we're teasing most of the questions and then providing the answers.



LEO:  You're asking the questions and answering them.



STEVE:  That's right.  We take care of the whole job here.  So this week we wonder what is about to happen with the EU's legislation to monitor its citizens' communications.  Why would a French psychotherapy clinic be keeping 30,000 old patient records online, and who stole them?  What top-level domains insist upon and enforce HTTPS?  How is Chrome's release pace about to change?  And when you say Russia "shoots the messenger," is that only an expression?  Were a fool and his crypto soon parted, or should that be "was"?



Exactly why is QNAP back in the news, and what do I really think about Synology?  Would companies actually claim unreasonably low CVSS scores for their own vulnerabilities?  Noooo.  What questions have our listeners been asking after all this recent talk about passwords?  What's the whole unvarnished story behind this week's massive global attack on VMware's ESXi servers, and who's really at fault?  These questions and more will probably be answered before you fall asleep, but no guarantees.



LEO:  No guarantees.  Some of them rhetorical, I might add.  Great.  I'm excited.  It's going to be a good show.  We also have a great Picture of the Week, fitting in with the usual topic of our Pictures of the Week.



STEVE:  Indeed.



LEO:  Picture of the Week time, Mr. G. 	



STEVE:  So this one can - you can spend some time visually parsing this picture.  It really begs many questions.  So without further ado, what we have is a close-up of a chain which has been wrapped around the opening side of a fence, like to keep the fence closed.  Now, and to call this a chain really doesn't do it justice.  A chain is what you wear around your neck.  This thing looks like it could have been the anchor for the Titanic, just in terms of the beefiness of this chain.



But what's odd is that it's actually - there's actually two pieces of chain.  There's a center three links which are actually a little smaller than the main chain which goes around in order to keep this fence closed.  And for reasons not at all clear, we've got, you know, your traditional Master Lock, a standard hasp-style lock that is interlinking the chain that goes around the opening to this little three-link subchain.  And then there's a white nylon zip tie which is connecting the small chain to this monster chain.  Or the smaller chain.  They're all big chains.  And so it's like, okay.



LEO:  Huh?



STEVE:  So, now, and anyone who's ever like tried to manually pull one of those nylon zip ties apart knows they are really strong.  In fact, I think aren't police now using them as like...



LEO:  They use them for handcuffs, yeah, yeah, yeah.



STEVE:  Disposable handcuffs, yeah.  So you're not getting out of this.  But at the same time, if you had...



LEO:  All you need is a knife or scissors.



STEVE:  ...some toenail clippers.



LEO:  Yeah, toenail clippers work well, yeah.



STEVE:  It's like, nothing, you know.  Now you're able to get in here.  And Leo, it's not like you couldn't use only the big chain with the Master padlock to bridge across the last chain.  That would work just fine.



LEO:  Yeah.  You don't need this little three-link chain.  I don't know what that's there for.



STEVE:  Right.  And no hokey white nylon zip tie to connect the two chains together.



LEO:  Very strange.



STEVE:  So really, the more pictures of this we see, the less faith I have in humanity.  And I really, you know, I would like to get the back story behind some of these.  Like the one we had a couple weeks ago had been haunting me, that piece of fence across the sidewalk that had a sign on it, "Sidewalk Closed."  Except there was sidewalk that was just fine on the other side.  And you could go around it in either direction.  It's just like, what?  Who?  What?  Anyway.



LEO:  There's a couple of wags in our chatroom who say, well, truthfully it'd be harder to - the Master Lock's easier to pick than the zip tie.



STEVE:  Yeah.



LEO:  So maybe the zip tie's actually not the weakest link in this chain.  Yeah, you snip right through that, yeah.



STEVE:  If you didn't have any sharp cutting tool on your person, then, yeah, that's true.  Okay.  So we are back to protecting the children.  And I'm not making light of that at all.  CSAM, as we know, Child Sexual Abuse Material, and online exploitation of children is so distasteful that it's difficult to talk about because that requires imagining something that you'd much rather not.  But it's that power that gives this a bit of a Trojan horse ability to slip past our defenses, or at least past the politicians, because there's also a very valid worry surrounding this whole issue that, once we've agreed to compromise our privacy for the very best of reasons, protecting children, our government or a foreign government or law enforcement might use their then-available access to our no longer truly private communications against us.



Now, nowhere in the EU's pending legislation, pending surveillance legislation that I'll get to in a second, is there any mention of terrorists or terrorism.  But it's been voiced before, and you can bet that it will come marching out again.  And once everyone's communications is being screened for seductive content that might be considered "grooming," you know, photos that might be naughty, and other content that some automated bot thinks should be brought to a human's attention, then what's next?  So this is, you know, this is the very definition of a slippery slope.



Document 52022PC0209 is titled "Proposal for a REGULATION OF THE EUROPEAN PARLIAMENT AND OF THE COUNCIL laying down rules to prevent and combat child sexual abuse."  Okay.  First of all, it won't prevent it; right?  Nothing will.  What it will do is drive that material to seek other channels.  And that's not a bad thing.  And I agree that it would likely combat the problem.  Though, again, prevention, okay, to some degree; right?  The question is, is this the best solution, and what real price are we paying to make that possible?  And, of course, what could possibly go wrong?



So what is essentially happening is that the EU is taking the next step.  Over and ignoring the loud and recently polled objections of 72% of European citizens, EU legislators are preparing to move their current content screening Internet communications surveillance, which until now has been voluntary, and as a consequence somewhat limited in its application, to mandatory, and therefore universal.



Okay.  So now just to recap a bit about how we got to where we are now.  Three years ago, in 2020, the European Commission proposed "temporary" legislation which allowed for the automated Internet communications surveillance for the purpose of screening content for CSAM (Child Sexual Abuse Material).



The following summer, on July 6th of 2021, the European Parliament adopted the legislation to allow for this voluntary screening.  And as a result of this adoption, which they refer to as an ePrivacy Derogation  in other words, creating a deliberate exception to ePrivacy for this purpose  U.S.-based providers like Gmail, Outlook.com, and Meta's Facebook began voluntarily screening for this content on some of their platforms.  Notably, however, only those very few providers did anything.  The other providers of, for example, explicitly secure communications - you know, Telegram, Signal - they've not done anything.



And so last summer, on May 11th of 2022, the Commission presented a proposal to move this Internet surveillance from - this is no longer going to be temporary, and it's no longer going to be voluntary.  It will be becoming mandatory for all service providers.  As we noted when this was last discussed in the context of Apple's hastily abandoned proposal to provide client-local image analysis by storing the hashes of known illegal images on the user's phone, the content to be examined includes not only images, but also textual content which might be considered solicitous of minors.  That's that "grooming" term.  And most controversially, all of this would impact every EU citizen, regardless of whether there was any preceding suspicion of wrongdoing.  Everyone's visual and textual communications would be, and apparently will soon be, surveilled.



Interestingly, the legality of this surveillance in the EU has already been challenged; and, according to a judgment by the European Court of Justice, the permanent and general automatic analysis of private communications violates fundamental rights. Nevertheless, the EU now intends to adopt such legislation.  For the court to subsequently annul it can take years, by which time the mandated systems will be established and in place.



Currently, meetings and hearings are underway.  They're going to be going on through the rest of the year.  A parliamentary vote is being held next month in March, followed by various actions being taken throughout the rest of the year as required to move the sure passage of this legislation through a large bureaucracy.  Why "sure"?  After all, how does any politician defend not wishing to protect the children?  I've read a great deal of this proposal, and it has clearly been written to be rigorously defensible as a child protection act, period.  So how do you stand up and vote against that?  It shows every indication of being adopted, with this surveillance set to become mandatory in April of next year, 2024.



So some pieces from this legislation:  "By introducing an obligation for providers to detect, report, block, and remove Child Sexual Abuse Material from their services, the proposal enables improved detection, investigation, and prosecution of offenses under the Child Sexual Abuse Directive."



Another piece:  "This proposal sets out targeted measures that are proportionate to the risk of misuse of a given service for online child sexual abuse and are subject to robust conditions and safeguards.  It also seeks to ensure that providers can meet their responsibilities by establishing a European Centre to prevent and counter child sexual abuse" - hereinafter referred to as "EU Centre" - "to facilitate and support implementation of this regulation and thus help remove obstacles to the internal market, especially in connection with the obligations of providers under this regulation to detect online child sexual abuse, report it, and remove child sexual abuse material.  In particular, the EU Centre will create, maintain, and operate databases of indicators of online child sexual abuse that providers will be required to use to comply with the detection obligations."



Okay.  Why mandatory?  They say:  "The impact assessment shows that voluntary actions alone against online child sexual abuse have proven insufficient by virtue of their adoption by a small number of providers only, of the considerable challenges encountered in the context of public-private cooperation in this field, as well as of the difficulties faced by member states" - meaning EU member states - "in preventing the phenomenon and guaranteeing an adequate level of assistance to victims.  This situation has led to the adoption of divergent sets of measures to fight online child sexual abuse in different member states.  In the absence of Union action, legal fragmentation can be expected to develop further as member states introduce additional measures to address the problem at national level, creating barriers to cross-border service provision on the Digital Single Market."



And as to why they think this is a good thing?  "These measures would significantly reduce the violation of victims' rights inherent in the circulation of material depicting their abuse.  These obligations, in particular the requirement to detect new child sexual abuse materials and 'grooming,' would result in the identification of new victims and create a possibility for their rescue from ongoing abuse, leading to a significant positive impact on their rights and society at large.



"The provision of a clear legal basis for the mandatory detection and reporting of 'grooming' would also positively impact these rights.  Increased and more effective prevention efforts will also reduce the prevalence of child sexual abuse, supporting the rights of children by preventing them from being victimized.  Measures to support victims in removing their images and videos would safeguard their rights to protection of private and family life (privacy) and of personal data."



Okay.  So this is clearly something that the EU is focused upon and is committed to seeing put into action, to be in effect in the spring of next year, 2024.  And apparently the EU has a legal system much like the one which has evolved, or devolved, here in the U.S., where the court system has been layered with so many checks, balances, and safeguards against misjudgments that years will then pass while challenges make their way through the courts.  Meanwhile, this is mandatory starting in April.



Conspicuously missing from any of this proposed legislation is any apparent thought to how exactly this will be accomplished from a technological standpoint, which of course is what interests us.  If I have an Android phone, whose job is it to watch and analyze what images my camera captures, what images my phone receives, what textual content I exchange?  Is it the phone hardware provider's job?  Or is it the underlying Android OS's job?  Or is it the individual messaging application?  It's difficult to see how Signal and Telegram are ever going to capitulate to this.  And is it the possession of the content or the transmission, reception, and communication of the content?  Can you record your own movies for local use, never with any intention to do anything else with them?



"The proposal establishes and funds the so-called 'EU Centre' to serve as a central clearinghouse for suspected illegal content, and providing in some fashion the samples against which material that is seen on devices, on consumer devices in the EU, is checked against.  So when an EU-based provider somehow detects something which may be proscribed, the identity and the current location of the suspected perpetrator, along with the content in question, will be forwarded to the EU Centre for their analysis and further action, if any."  Wow.



So as I've been saying for years, this battle over the collision of cryptography and the state's belief in its need for surveillance is going to be a mess, and it's far from over.  So Leo, it moves forward.



LEO:  It makes me really think about the long-term consequences of that.  And if I were Apple or Google or Samsung, I would be fighting this tooth and nail because, in the long run, they're going to be forced to enforce it, essentially; right?



STEVE:  To compromise.



LEO:  They're going to have to do something, yeah.  And if they do, then you're going to see a migration away from their platforms to nonproprietary open platforms so that people don't have to subjugate themselves to this.  So I think it hurts them badly, first because they're going to have a battle over how to enforce it.  Apple's already turned on Advanced Data Protection in the U.S., which is - and here's another question.



STEVE:  And now globally.  It went global a couple weeks ago.



LEO:  Okay.



STEVE:  With iOS 16.3 it's now universal.



LEO:  They'll be noncompliant in the EU.  And then there's the other question is, and they haven't done this yet, but how long before they then make it illegal for me to encrypt everything?  Right?  Because they're going to stop the vendors.  But what if I decide, well, I'm going to figure out a way that I'm going to pre-Internet - do what you call PIE, Pre Internet Encryption of everything.  Am I now found guilty because I must be hiding something?



STEVE:  I know.



LEO:  I think it pushes people into a position where they do have to now start being responsible for their own encryption.  They only would choose end-to-end encrypted choices.  It's going to end up driving people underground and in the dark, not just criminals, but everybody who wants privacy.  I think the long-term implications of this are bad all around.



STEVE:  I know.  And so from a technology standpoint we have Signal and Telegram.  There's just no way that Moxie is going to compromise...



LEO:  Right.



STEVE:  ...Signal in order to allow the - and be responsible for having a connection to the EU Centre to get a database of things it has to check its users messaging [crosstalk]...



LEO:  Well, and that's why I'm saying...



STEVE:  This will not happen.



LEO:  ...the burden of this is ending up on Apple and Google and Samsung because what they'll have to do is take them out of the store.  They'll have to say, well, we can't have Signal in the App Store.  And then we've washed our hands of it.  But Signal will continue to be distributed underground.  And if you are - and this is what I'm saying is ultimately, if you care about privacy, you're going to run an open platform that you control that you put your own software, you're not going to be relying on an Apple Store or an Android Store.



STEVE:  Well, it goes a little bit further, though, because Apple could be compelled to do the filtering before Signal gets it.  Remember that Signal is...



LEO:  No, no, I understand.  You can't use an Apple device is what I'm saying.  The burden will end up being on Apple; and Apple will, if they comply, which they probably will have to in the long run, lose customers like you and me who will say, well, I'm going to use Signal.  I'm going to do encryption.  And it ain't going to be on a device where I can't.  So you're exactly right.  That's what I'm saying.  This is who should be fighting this tooth and nail right now is Apple and Google because this is going to be, not only a burden on them, but it's going to require them to reverse things they've been doing, but also it's going to lose them customers.  I don't know.  Do most people care enough about this that they would actually - you said 72% of the EU is against it?



STEVE:  They're saying, yeah, we do not want this.



LEO:  I think you can't stop encryption; right?  You can only stop it on commercial platforms.



STEVE:  Mail has already escaped.



LEO:  Yeah.  So they can't stop it.  They can only tell companies, Internet service providers, carriers, cell phone manufacturers to do it.  So then we just say, well, I think that just creates a brisk market for...



STEVE:  Well, remember, I was all geared up to do a product called CryptoLink years ago.  I saw the handwriting on the wall.



LEO:  Yeah, you didn't want that burden.



STEVE:  It's a much slower march, but I didn't want to be in a position where governments are saying we have to have a backdoor to your secure communications.



LEO:  Many years ago, about 20 years ago, there was a documentary which has since been suppressed about hacking in which I gave an interview.  And I said really it's going to be the hackers that are the freedom fighters.  They're going to be the ones who are going to be protecting us from governments and corporations who are going to want to invade our privacy, take over our lives.  And that open source software and hackers, people who know how to use it, are going to be the heroes.  They're going to be the heroes.  It's going to be up to us to protect ourselves.  I don't think we should all turn into the Unabomber.  But I think we're all going to have to embrace open software because they can't stop open software.



STEVE:  No.



LEO:  It's very, very difficult.



STEVE:  So that would mean hacking an Android device in order to sideload your own...



LEO:  Not necessarily.  There are already companies like Pine that make phones that are not Android or iOS.  They run Linux.



STEVE:  Ah, okay.



LEO:  So they're not very good.  I keep buying them in hopes, and they're terrible.  But this will stimulate their development.  And eventually, just as you can buy a computer that, you know, you don't have to have TPM on a computer.  You can buy a computer that is not...



STEVE:  Locked down.



LEO:  Locked down, and put open stuff on it, and control it.  And that's what's going to happen, I think, at least for people who care.  Maybe that's [crosstalk].



STEVE:  Yeah.  And obviously that is, like, yeah, a diminishing minority.  I mean, maybe once upon a time Uncle Willy was asking his nephew who was the geek what was the best computer to buy and what should you do.  And so maybe it'll be like, hey, I heard about governments are spying on everybody with their phone, Junior.  What phone should I get?  And then, you know, Junior will know because he's in college, and he's up on all this stuff.



LEO:  Yeah.  There'll be a brisk market in open hardware and software, I think.  And then the sad thing is then you've completely lost control.



STEVE:  Well, yes.



LEO:  You know, there's nothing they can do about it.



STEVE:  Yes.  And it will be, as we've already seen, it'll be the bad guys that are driven to that platform.  And sadly, I mean, there is a level of false positives that occur with this.  There are images which someone who's sitting there clicking a button, snapping through images, the human CAPTCHA person is sitting there saying, whoa, what's that?  And, you know, go question this person.  I mean, it's going to be horrible if that's happening.  



LEO:  Yeah.  Yeah.  I've always felt like there would come a time when this stuff, this computer technology was too powerful and that governments would want to try to control it and shut it down, and that there would always be a group of us that are called hackers.  But there would always be a group of us who said, no, no, we're going to keep it open, we're going to keep it ours, and we're going to keep their prying eyes out.



STEVE:  Like Neo in the Matrix.



LEO:  Like the Matrix.



STEVE:  Yeah.



LEO:  Wow.  And they're pushing us that way; you know?  It's too bad.



STEVE:  Yeah, yeah.  Okay.  So 30,000 patient records online.  This interesting and sobering cyber-hacking news caught my eye and raised an interesting question.  Okay.  First I'll share the story, and then the question that it brought to mind.



The news was that French authorities have detained a 25-year-old Finnish national who is accused of hacking the Vastaamo Psychotherapy Center.  For reasons we'll see, this hack of Vastaamo is considered to be one of the worst in the country's history.  Okay.  Now, it occurred back in 2018 and 2019, so I guess this kid was, what, 20 years old then, when he allegedly stole the personal medical records of the clinic's patients and attempted to extort the clinic.  To put pressure on the company, the hacker leaked extremely sensitive client files on the dark web.  When that failed, he sent emails with ransom demands to more than 30,000 of the clinic's patients, asking them each for 200 euros and threatening to publish their medical records if they did not pay up.



LEO:  Oh, boy.



STEVE:  Uh-huh.  Finnish authorities formally identified the hacker in October last year when they issued a European arrest warrant for his arrest, and they detained him last week.  Okay, so this is brazen and bad; right?  The hacker obtained extremely sensitive personal medical information and chose to use it to extort both the clinic and its past patients, all 30,000 of them.  And it was that number of files and patient histories that raised my eyebrows, 30,000.  Okay.  No matter how large and busy this clinic might be, they cannot be currently treating 30,000 patients.  And in fact there are 260 working days a year, five times 52.  So if the clinic averaged 10 new patients per day, which seems like a high-side number, 30,000 patient records would be 11.5 years' worth of patient files at the rate of 10 per day.



I'm sure there's some requirement for retaining medical files for some length of time.  HIPAA regulations have that here in the U.S.  But even so, they certainly don't need to be kept in hot online storage.  If it was burdensomely expensive to store all that aging data online, then it would not be stored online because it doesn't need to be.  It would be spooled onto some form of offline cold storage.  Still indexed and available if needed, but offline and therefore not available to remote online attackers.



This is one of the things that we're going to need to get much better at handling as a society.  Excessive data retention is a problem.  And it's exacerbated by the reality that storing data costs next to nothing.  So why not store it on the off chance that it might be useful for something?  It doesn't delete itself unless you actually create some technology so that it does, but no one seems to do that.  The problem is, even if all that old data was of no use to the clinic in this instance, it was certainly useful to the hacker, who obtained a far larger pile of extortable victims as a consequence.



So it's unclear how we move past this, where we are stuck now.  There needs to be some form of incentive for inducing deletion or at least for the migration of old records into offline archival storage for varying periods of time.  And such records should be destroyed once their retention period has lapsed.  But "should" was the strongest word I could find.  I dug into medical records retention legislation and requirements.  I couldn't find any clear requirement under HIPAA for mandatory deletion.  It's not there.  So if an organization acts irresponsibly, it's not clear whether they would be in any legal jeopardy, at least in the U.S.  God help you if you're in the EU.  But still.  It's clearly, you know, and we've talked about data retention before.  It is a real problem.



.DEV, it turns out to my surprise, is always HTTPS.



LEO:  Hmm?



STEVE:  I know.  I encountered something the other day that I didn't realize had happened.  I was over at Hover registering spinrite.dev because I thought it might come in handy since I'm planning to be spending the rest of my active coding life on what promises to be a very exciting and worthwhile project.  So, as I was checking out, I was presented with a pop-up confirmation notice the likes of which I had never seen.  It read, and it was number three of things I had to check off, it said "TLD INFO FOR .DEV."  And of course TLD stands for top-level domain.



And it says:  "Registration of .dev domains is open to anyone.  You should be aware that .dev is an encrypted-by-default TLD by virtue of being inscribed in the HSTS Preload list found in all modern web browsers.  Websites hosted on .dev will not load unless they are served over HTTPS, i.e., have a valid SSL certificate installed."  And I had to check "I have read and understand the requirements for .dev domains" in order to proceed with the purchase.  Isn't that cool?



LEO:  Yeah.



STEVE:  So *.dev is permanently preloaded into the HTTP strict transport security, that's HSTS, list for all modern web browsers.  Okay.  Now before I go any further, let me quickly review HSTS.  As I just said, it stands for HTTPS Strict Transport Security.  "HSTS" is an HTTP Response header which web servers can send to browsers telling them to treat the site with "Strict Transport Security."  This means to only use secure HTTPS TLS connections no matter what.  If the browser receives a non-secured HTTP link, the HSTS status instructs the browser to automatically upgrade it without asking anybody else, to HTTPS.  The header specifies a "max-age" which tells the browser how long this security upgrade directive is to remain in effect.  It's also possible to add an "includeSubDomains" parameter so that everything below that root domain will also be covered.



The first time a site is accessed using HTTPS, and the site returns the Strict-Transport-Security header, the browser records and caches this information so that all future attempts to load that site using HTTP will automatically be promoted to using HTTPS instead.  When the expiration time specified by the Strict-Transport-Security header elapses, the next attempt to load the site via HTTP will proceed as normal instead of automatically using HTTPS.



Whenever the Strict-Transport-Security header is delivered to the browser, however, it will update the expiration time for that site, essentially, you know, continually pushing it forward, so sites can refresh this information and prevent the timeout from expiring.  Should it be necessary for some reason to disable Strict Transport Security, setting the max-age in that header to zero, over an HTTPS connection of course, will immediately expire the Strict-Transport-Security header, allowing access then via HTTP.



But all this cleverness still leaves us with one problem:  What about the very first time a browser visits a site?  If that visit were initiated, for example, by following an HTTP link, maybe from a malicious email, the initial connection will be insecure, in plaintext, unauthenticated, and susceptible to interception and on-the-fly modification of the traffic.  Even if the web server is sending out HSTS headers, they could be stripped from the insecure connection so that the browser never receives them.



The solution to this first-contact problem is the HSTS preload list.  All modern browsers carry a large list of web domains which have previously proven to be HSTS capable by offering HTTPS TLS connections, redirecting any HTTP request over to HTTPS, and sending an HSTS Response header with an expiration time of at least a year.  Those are the requirements in order to quality for inclusion in the browser's master list.



If all of those criteria are met, the domain qualifies for permanent HSTS registration.  At that point, the HSTS Preload site - you can go to hstspreload.org - can be used to submit a domain for inclusion in the global browser HSTS preload list.  GRC.com has been on that list since the list's earliest days, when we first discussed this on the podcast many years ago.  And once on that list, any attempt to ever connect to port 80 will be redirected by the browser.  It'll just ignore that and go to port 443 for the establishment of a TLS connection.



Okay.  So with that bit of a refresher, just imagine the number of domains, the dotcoms, like GRC.com is one, how many more that must be on the list with those common top-level domains, dotcoms, you know, and the others.  As I said, GRC.com has always been there, but so must be an incredible number of other domains.  What's so super-cool about the idea that .dev top-level domain is, by universal agreement, all HTTPS, is that it avoids any need for subdomains of .dev being on the list.  Instead of needing to have a list that enumerates all of those domains, like for example Spinrite.dev, there's only one entry on the list, *.dev.



Down at the bottom of that HSTS Preload page it talks about this.  It says, under the heading "TLD Preloading," they say:  "Owners of gTLDs (global top-level domains), ccTLDs, or any other public suffix domains are welcome to preload HSTS across all their registerable domains.  This ensures robust security for the whole TLD, and is much simpler than preloading each individual domain."  They finished:  "Please contact us if you're interested or would like to learn more."



So not only is this much simpler, but it is vastly more efficient.  Since pretty much now everything needs to be HTTPS these days anyway, it's such a cool idea when a new TLD is created to simply declare the entire thing as HTTPS-only and place that single entry, *.whatever, onto the global browser preload list.  So much better than needing to have every subdomain needing to do that individually.  And everybody's protected, even if they don't do the whole HSTS header routine.



Okay.  So I thought, what else might be on the list?  I posed that question to the gang who hangs out in GRC's Security Now! newsgroup, noting that it would be possible to pull the current list from the open source Chromium repo and run a regular expression on it to extract only top-level domains.  One of our very active contributors, Colby Bouma, actually he's the one who got me into GitLab and has been helping incredibly to keep our GitLab instance organized during all this SpinRite work, he stepped up, grabbed, parsed, and filtered the current Chromium HSTS file.  And sure enough, the .dev domain has a great deal of company.  There are presently 40, four zero, top-level domains in the global browser HSTS list, meaning that any subdomain of any of those top-level domains will only be accessible by web browsers using authenticated and encrypted TLS connections.



Okay.  In alphabetical order they are Android - so in every case this is something.android; right?  App, azure, bank, bing, boo, channel, chrome, dad, day, dev, eat, esq as in esquire, fly, foo, gle...



LEO:  Who's going to register steve.foo?  Same people register as steve.boo, I guess.



STEVE:  I'll bet it's taken.  Gmail, google, hangout, hotmail, ing, insurance, meet, meme, microsoft, mov, new, nexus, office, page, phd, play, prof, rsvp, search, Skype, windows, xbox, YouTube, and zip.



Okay.  So .dev is there, along with 39 others.  We see that Google and Microsoft, who each own several of their own TLDs, have placed them on that list.  And why not?  As desirable as it would be to be able to place .com, .org, .net, .edu, .gov, you know, the original bunch, onto this list, or really just to abandon HTTP for user client web browsing altogether, I don't see how we're ever going to get there from here.  Doing so would immediately make any HTTP-only sites inaccessible, and that's not something I can ever see happening in our lifetimes.  But what I think must be happening, because, come on, foo and gle and dad?



LEO:  These just are new; right?



STEVE:  Exactly.  And that's the point.  Any new registration of a TLD is probably automatically saying put us on the global HSTS list for the entire TLD.  Why not?  That way you're just saying to anybody who wants to set up a web server, great, love to have you.  Happy to take your 14.95 per year to maintain registration for you.  Oh, and by the way, you can only use - you're going to have to get a certificate.  But of course that's free now, too, with Let's Encrypt and the ACME protocol.  Or even I think DigiCert is now doing the same thing.  So, you know, it's no longer the case that that's a problem.  So, yeah, let's make it mandatory.  Anyway, I just never knew that.  I thought that was very cool.



And Leo, we're next going to talk about the changes Chrome is making in their release schedule.  So we were just talking about the idea of staged releases of software updates to minimize the fallout from previously undetected problems.  As a matter of fact, given the number of wacky problems I've been encountering with SpinRite, as our early prerelease tests find ever more bizarre machines to torture it with, I've decided that the only sane thing for me to do will be to inform everyone here who's following this podcast when and where it's available in final beta and then in final release.  Anxious as I am to inform SpinRite's entire broader user community of what has grown to  become a major free upgrade, I'm going to wait a while to see how much a more local larger release goes.



LEO:  That's smart, yeah.  Especially because these are the more sophisticated listeners.  They're going to be the great, great people to try it out with and let you know.



STEVE:  Yes.  And I can say go to the forum, and we'll be able to get online and communicate and so forth.



LEO:  That's smart, yeah.



STEVE:  Yeah.  And, you know, people have waited 18 years, they can wait another month or two.  So, yeah.  And apparently Google has decided to do the same with Chrome.  Back a few days before Christmas, they posted the news "Change in release schedule from Chrome 110," with the subhead "From Chrome 110 an early stable version will be released to a small percentage of users."  And of course, as I just said, I can relate to that.



Chrome is just about at 110.  Yesterday, the Chrome beta channel was updated to 110.  There are four channels which stage the progressive rollout of each new major release.  The most bleeding edge is the Canary channel, followed by the Dev channel, then the Beta channel, and then finally the main release channel.  So 110, where they're going to start staggering, staging the release, just went into beta yesterday.  Its next move then will be to release.  And that's where the timing will be changing a bit.  What Google is now explaining is that 110 will be appearing more slowly in the release channel than before.



They wrote:  "We are making a change to the release schedule for Chrome.  From Chrome 110, the initial release date to stable will be one week earlier.  This early stable version will be released to a small percentage of users, with the majority of people getting the release a week later at the normal scheduled date.  This will also be the date the new version is available from the Chrome download page.  By releasing stable to a small percentage of early users, we get a chance to monitor the release before it rolls out to all of our users.  If any showstopping issue is discovered, it can be addressed while the impact is relatively small."  So again, if you think about the number of Chrome users there are, it's just an unimaginable number.  So, yeah, I think that makes absolute sense not to have everybody having the same problem all at once in the world.



We've been tracking the gradual increase in accountability for cyber intrusions and data breaches, with more recently IT employees even increasingly being held accountable.  In another bit of just surfaced news, we learn that Russia is moving forward with its own legislation to impose major fines and even prison sentences for IT administrators and their managers following major data breaches.  Yes, nothing encourages the quick and full public disclosure of data breaches more than the prospect of some prison time at the other end.  The idea first surfaced last May in Russia.  And once this legislation is passed, the Russian government will be able to fine individuals anywhere from 300,000 to 2 million rubles.  Now, of course, 300,000 rubles won't buy you very much, maybe a Russian car.  That's $4,200 equivalent, up to 2 million rubles, which is $28,000.  And/or or imprison them for up to 10 years if their companies get hacked, and user data is stolen.



Now, okay.  That's brutal.  I'm all for accountability.  But this could well devolve into shooting the messenger rather than the source of the message.  Sure, there could be misconfiguration that IT should have known better and done more to secure.  But there are also plenty of zero-day vulnerabilities that no one should be held to account for, more than the original source of the vulnerability, which is where the zero-day came from in the first place.  I'm not going to dwell upon this further now because this week's primary topic winds up posing some serious questions about accountability, in this case the VMware ESXi issue.  But this additional news demonstrates that we are continuing to see, and not surprisingly, mounting pressure to hold someone accountable for cybersecurity incidents.  And this isn't over by a long shot.



I had to shake my head at this little piece.  There's a new scam that's growing in popularity in the cyber underground where there are templates for carrying it out.  Generically they're known as "crypto drainers."  They're custom phishing pages that entice victims into connecting their crypto wallets with an offer to mint NFTs on their behalf.  And of course this is where we all collectively chant in unison, "What could possibly go wrong?"  To no one's surprise, other than the hapless victims, as soon as victims attempt to mint NFTs, the crypto drainer page siphons both a user's cryptocurrency and the desired NFT into an attacker's wallet.



LEO:  I think the name is kind of a giveaway, the Crypto Drainer.



STEVE:  Crypto drainer.  Yeah, I want to sign up for the crypto drainer page.



LEO:  Yeah.  What could possibly go wrong?



STEVE:  What could possibly go wrong?  According to Recorded Future, there are several crypto drainer templates currently being advertised on underground cybercrime forums, and they are growing in popularity, of course.



Okay, now, apparently it's the Bible's Proverbs 21:20 which is the original source of the expression "A fool and his money are soon parted."



LEO:  Now, Steve, I didn't know you were so up on the Bible.



STEVE:  Oh, honey, I'll tell you, there's nothing you can't find on Google.



LEO:  Ah, yeah, good.



STEVE:  I didn't even @chatgpt.  Now, that proverb, however, speaks of wealth being capriciously spent.  In this case, of course, the outcome is the same.  And you've really got to wonder that there are people willing to connect their wallets to some random page on the Internet which states, you know, "We'll mint NFTs for you and auto-deposit your profits into your wallet."



LEO:  Sure.



STEVE:  Because, you know, you can trust us and our broken English.  Oh, god.



Okay.  Unfortunately - and Leo, remember, Proverbs 21:20.



LEO:  21:20.  I'll keep that in mind, yes.



STEVE:  The Taiwanese NAS (Network Attached Storage) vendor QNAP is back in the news.  And, you know, with them the news is never pretty.  This time, QNAP has recently patched a SQL injection vulnerability tracked as CVE-2022-27596.  That's the end of the good news of this story.  A week later, Censys, that's that newer IoT search engine group, Censys says that roughly 98% of the 30,000 QNAP NAS devices it currently tracks remain unpatched.



LEO:  What?



STEVE:  Yes.  Nobody patches their QNAP NASes.



LEO:  Guess not.  It's just sitting in a closet and...



STEVE:  Yeah, exactly.  So 98% of 30,000 are unpatched.  And, it turns out, because it's trivial to exploit, and the exploitation process does not require any authentication, Censys expects the vulnerability to be quickly abused by ransomware gangs, as has happened many times previously, like all the many times we've talked about this before.  And the number of vulnerable devices could possibly be much higher since Censys said that there are another 37,000 QNAP systems online for which it could not obtain a version number but which are also likely vulnerable as well.  So maybe 98% of 67,000 QNAP devices.



Okay.  And speaking of NASes, I just wanted to give a shout-out to Synology.  I own one, and I've just ordered another.  They're backordered right now, and I'm not surprised because, damn, they are amazing.



LEO:  Oh, I'm glad to hear you say that, yeah.



STEVE:  I am so impressed.  I had been running a pair of colocated Drobos which were running just fine.  But the oldest one of the pair, which is now more than 10 years old, started acting a little flaky, and it finally went belly-up.  Since the company's, Drobos's, future is a bit uncertain, I decided to switch to Synology which I kept hearing about.  And oh my god, what a fabulous experience.  What I got are the DS418s.  It only has four bays as opposed to the Drobo's five, but my storage needs are not excessive, and the management experience is so good.  Since I have two work locations, I plan to use their integrated Synology synchronization system to have the two boxes mirror each other.  And then I'll be keeping my local work synchronized locally.



Anyway, I just wanted to say, for what it's worth, just one user's experience of Synology.  It's been 100% positive.  And these guys, they should have the market because they've done it right.  And I know you feel the same way.



LEO:  Oh, yeah.  I have three of them.  I love them.



STEVE:  Yeah.  To no one's surprise, after the vulnerability intelligence company VulnCheck analyzed more than 25,000 entries from the NIST vulnerability database that contained CVSS ratings from both NIST and the product vendor, VulnCheck discovered that more than half of those analyzed, 14,000 of the 25,000 vulnerabilities, had conflicting scores where the vendors and NIST had assigned different ratings for the vulnerability's severity.  Imagine that.  VulnCheck says that despite the large number of entries, most of these came from 39 vendors, whom they did not name, suggesting that some companies are intentionally downgrading the severity of their own vulnerabilities.



And the trouble with this is not just public relations, which of course is why they're trying to, you know, that's what's driving them to falsely claim things are less serious than they are.  At the high level, the vulnerability ratings are actually being used to set patching priorities.  You know, if you can't patch everything, patch the bad things.  So it's natural to patch the most important problems first.  So intentional vulnerability downgrading messes with the ability to do any of that correctly.  And now we have some numbers, 58% of the 25,000 where there are private listings and public listings, like official listings, the 39 of the companies who are doing this are saying, eh, we don't think it's as bad as everybody else.



Okay.  As a consequence I think of the fact that we've been talking about passwords a lot in the last - actually all year so far, all of the interesting questions that I ended up finding in my mailbag were about that.  I have four.  Simon Lock tweeted.  He said:  "Dear Steve.  What OTP Auth" - I'm sorry.  What OTP app - I already gave away the answer.  "What OTP app can you recommend, or what do you use?  Mostly I think for iOS, but if it also does Android, that would be nice.  Cheers and thank you for a lot of great hours listening to Security Now!"



Okay.  So the one I've chosen after poking around with them a bit is the iOS app OTP Auth.  For those who have settled upon something else, the fact that you have settled upon anything and are therefore using one-time passcodes is far better news than which one you've settled on.  I'm not saying that it matters much at all.  So I'm in no way suggesting that OTP Auth, my choice, is superior to XYZ Auth.  It's just the one I like.  Its interface is clean, it synchronizes among all of my iDevices through iCloud, I can unlock it with my face or touch, it pastes the code to the clipboard, which makes transcribing it simpler, and I like that fact that it has a customizable widget that allows me to have a subset of the passcodes I most use appear on the iPhone's notification center for even easier access.  But it's definitely iOS only, so it won't do the cross-platform deal over to Android.  Oh, and it also allows encrypted backup to a documented file format.  It's published by some German guy, and he feels German.  I'm impressed with the app's author.



LEO:  That's who you want to document a format, to be honest.



STEVE:  That's right.



LEO:  It is going to be like this.



STEVE:  That's right.  It's like a no-nonsense solution.  It's beautiful.



LEO:  OTP Auth.  I'll have to check it out.



STEVE:  OTP Auth.  I really like it.  Via a DM I received:  "Hi, Steve.  I've been following your podcast for more than two years, and I love it.  Even though I'm not a cybersecurity or even an IT professional, I have learned a lot."  I think he's maybe an ophthalmologist.  Anyway, based on his Twitter DM.  He said:  "I have a question regarding your favorite two-factor app, OTP Auth.  Would you be able to explain how does the syncing via cloud work for it?  I'm syncing it via iCloud, but don't necessarily see a file there.  If theoretically my iCloud was compromised, would someone be able to get hold of my OTP Auth tokens and get access to all my two-factor authentication codes?  Thanks in advance."



Okay.  So app data stored and linked through iCloud is not like iCloud Drive with Desktop, Documents, Downloads, et cetera.  iCloud Drive is an app that deliberately exposes those shared resources.  By comparison, app data is registered by the app and is never seen by the user.  You only get to see like how much an app is using of your iCloud space, if you go in and analyze the way memory is consumed.  Essentially, apps are able to use iCloud as their own secure synchronization service, which is private within that app.  And Apple does not have the keys to that app data.  They only exist in the users' devices.



So I'd say that it's as unlikely as possible for iCloud app data to be compromised.  But if you were really worried about it, you can flip that switch off.  As you said, and I agreed, Leo, this German guy, he said, well, maybe they don't want iCloud sync, fine.  Turn it off.  And I'll bet you dollars to doughnuts that he deletes it from the cloud as part of that.



Mark Jones tweeted:  "Steve, you continually reinforce time-based authentication and discredit the now exceedingly common SMS message as a second factor."  Amen.  "You've never touched an option that I'm seeing more and more.  Frequently I now have services asking me to validate via their app on my mobile device.  Google just asked me to check my Google app on my phone before letting me log on a Windows machine.  That is after I've set up Google Authenticator as my second factor.  Apple does it, too.  I've never seen an analysis of the security of this new model.  What are your thoughts?"



Okay.  If a giant company like Google or Apple has the luxury of requiring you to run their app on another device and to respond to its authentication prompts, then I think that's nearly as secure as a time-varying passcode.  And it certainly beats the crap out of SMS, because everything does.  I say that it's "nearly as secure" because really the only way to improve upon our current six-digit standard would be to increase the number of digits, and that's not necessary since the right answer changes every 30 seconds.



The seductive beauty of the time-varying code, which only requires that both ends agree on the time of day and date, is that nothing is sent to your authentication device.  The system is open loop.  The authenticator can be offline and without any radio, like remember those original LCD footballs that we had back before smartphones when we first, when OTP, you know, this notion of a six-digit varying code first appeared.  That time-varying code, which is driven by a shared secret cryptographic key, is really the perfect solution.



The one downside with a vendor's authentication app - oh, except I should give myself a caveat there.  I didn't think of it last night.  And that is interception.  We are seeing that second-factor authentication of this kind is being intercepted because the channel back to the server is through the web browser.  So if you're not actually where you think you are, and you could be at a spoof site, the spoof site was just asked for a two-factor code.  It forwards that request to you on your browser.  You go to your app, give it the six-digit code, the spoof site gets it and logs in, and is doing this behind your back.



So that, you know, that is a problem with our six-digit time-varying codes, which the Apple and Google and whomever standalone authentication app doesn't have because they are talking to their app on your phone, which they've established a relationship with, and when your phone lights up, then you know it's from them.  Except one downside with the vendor's authentication app which can push notification requests is notification fatigue, which we've talked about before.  Attackers are refining this now to a science, timing their spoofed authentication requests for the time of day when its user would be expected to be logging into their remote services.  Or sometimes just using more brute force approaches, fatiguing the user by prompting the user over and over and over until they just give up and accept the authentication request and allow the bad guys in.



So, yes, specific vendor closed-loop authentication beats SMS, as I said, because everything does.  And as long as you are giving your six-digit code to the proper site, the site you think you are, and not a spoofed phishing site, then nothing beats the open loopness of a one-time passcode.



And finally, Dan Stevens.  He tweeted:  "Hi, Steve.  In the last Security Now! Episode 908, you and Leo discuss extensively the rules for creating secure passwords in a way that can be 'reconstructed' from memory.  How complicated.  What if you forget what the rules are?  Maybe you've said this before, but my advice would be use a password manager with a completely random master password at good length, and write it on a slip of paper and keep it somewhere accessible and safe.  Refer to the slip of paper whenever you log into the password manager.  And eventually, for most people, the random password will stick in muscle memory, at which point you can destroy the slip of paper for extra security.  Is this not a whole lot simpler?"



LEO:  Not simpler, but definitely better.



STEVE:  I agree completely.  But there are places where a password manager cannot reach.  When I'm logging into my servers, or even into my Windows desktop, I don't have access to a password manager.  It's true that I could open the manager on my phone and carefully transcribe a long and complex password. But the threat model for local login to my desktop or remote login to a network service that no one at any other IP than mine can even see is different from logging into random Internet websites.



So Leo uses an approach that he likes.  And I have the phonetic made-up word approach that I like.  The important thing to appreciate, I think, is that there is no one right answer nor a best answer.  Anyone who's been listening to this podcast will have been exposed by now to the fundamental theory of password cracking and password entropy.  And we've tossed around many different systems and schemes for creating passwords.  So the right answer is any answer.  The key is that you've given this some thought and will arrive at an answer.  And you will hopefully have arrived at a system that creates strong passwords that are also workable for you, depending upon who you are and what your goals are.  And I think we are closing this topic.



LEO:  I mean, his way is definitely better, I mean, a truly random password.  The problem is I can't be getting a slip of paper out every single time.  I log into my password manager all the time.  I mean, it's just part of the deal.  And I'm going to keep this in my wallet?  



STEVE:  And you do?  Because I hardly ever log into my password manager.



LEO:  Oh, all the time.  Constantly.  For a variety of reasons.  I mean, I'm using it on a lot of different systems.



STEVE:  Ah, okay.



LEO:  Your password - you're using Bitwarden; right?  It doesn't time out?  I've set it to time out.  So if I'm not using it after a period of time, it times out.  As it should.



STEVE:  Yeah.  And so I'm in a locked environment.  No one else has access to my machine.  And the machine itself has a very strong authentication system which is protecting its access.



LEO:  They can't even get into the machine; right.



STEVE:  And the hard drive is encrypted, blah blah blah.



LEO:  Yeah, I could probably do that with the machines.  The mobile devices use biometrics, so I don't often have to enter it.  But I still from time to time will have to enter it.  To me it's not practical to carry a slip of paper around with my master password.  I don't think it's secure either, by the way.



STEVE:  And I often have my wallet at the other side of the house, too.  So, you know.



LEO:  Right, yeah.  So, you know, look.  I have a long password.  Maybe 30-some characters of completely random stuff would eventually be memorized.  But in the meantime it's a pain in the butt.  I feel like I've come up with a system that generates as close to a random password as you can get.  I mean, it's not truly random because it's based on a phrase.  But that's pretty random.



STEVE:  Yeah. 



LEO:  I'm not too worried about it.



STEVE:  Yup.  Again, I think to each their own.  The important thing is think about this.  Well, certainly everybody listening to this podcast is not only tired of thinking about it, they're tired of hearing about it.  So we're done now.



LEO:  Enough.  Now I'm trying to figure out how I can get my secret keys out of Authy so I can move them over to your choice, which I like, by the way.  I just downloaded, I've been playing with it a little bit.  It's good.  I think you're right.  I think it's a nice one.  The problem with Authy, the reason I like Authy is because it backs up my secret keys to the Authy server so I can put it on multiple phones.  I don't want to, you know, used to be you'd have to reset up Google Authenticator from scratch every time.



STEVE:  Yeah.



LEO:  But your solution is a perfect intermediate.  OTP Auth lets you back it up to a file in some secure place that's encrypted.  And then I can download it, and unencrypt it, and then import it, and I'd be set.  So I think this is - I prefer this to trusting Twilio with it.  So I think I'll probably, if I can figure out how to get those OTP seeds out, I think there are ways.  But we shall see.  All right, Steve.  Let's get to this VMware exploit here.



STEVE:  Yeah.  Today's sad story involves VMware's ESXi.  ESXi is VMware's hypervisor technology that allows organizations to host several virtualized computers running multiple operating systems on a single physical server.  The solution's grown very popular among cloud-hosting infrastructure providers because it's one of the good ones.



If by any chance you are two years behind in patching with a publicly exposed instance of ESXi, please, we've told you about canaries.  Stop listening to this podcast right now.  Go patch. If you're using a cloud-hosting provider instance, you should immediately perform a proactive version check.  In fact, you could use GRC's ShieldsUP! service to make sure that your port 427 is closed to the public.  And if you want to watch what's sure to become a honeypot feeding frenzy, place an instance of the OpenSLP service on port 427, stand back, and get ready.



What's going on is that over this past weekend, just two days ago, a new ransomware strain being tracked as "ESXiArgs," and we'll explain the name in a minute, swept through and encrypted several thousand unpatched VMware ESXi servers.  And here's the heartbreaking bit.  The entry point to all of these systems was an unpatched vulnerability more than two years old, well known, long since having been identified, being tracked as CVE-2021-21974, for which, as we'll see, there is also a publicly available proof of concept which made it easy for the bad guys to hack these VMware ESXi servers.



Okay.  So we'll get back to this weekend's attack in a minute.  Let's first get some perspective on all this by turning back the clock to the fall of 2020.  Back on March 2nd, 2021, Lucas Leong, a researcher with Trend Micro's Zero-Day Initiative, authored a blog posting titled "Pre-Auth Remote Code Execution in VMware ESXi."  And this was in March, once he was finally able to talk about this publicly, which was about six months after he first informed VMware of what he had found.



So in his posting March 2nd, Lucas wrote:  "Last fall, I reported two critical-rated, pre-authentication remote code execution vulnerabilities in the VMware ESXi platform.  Both of them reside within the same component, the Service Location Protocol (SLP) service.  In October, VMware released a patch to address one of the vulnerabilities, but it was incomplete and could be bypassed.  VMware released a second patch in November, completely addressing the use-after-free portion of these bugs.  The use-after-free vulnerability was assigned CVE-2020-3992.



"After that, VMware released a third patch in February completely addressing the heap overflow portion of these bugs.  The heap overflow was assigned CVE-2021-21974."  That's the one that is the trouble.  "This blog," he says, "takes a look at both bugs and how the heap overflow could be used for code execution.  Here is a quick video demonstrating the exploit in action."



Okay.  So that was what he posted March 2nd, 2021, nearly two years ago.  And then his blog post proceeds to demonstrate and provide descriptions, details, and pseudocode of the critical portions of the homegrown OpenSLP server that VMware had running in their ESXi server.  While continuing to be responsible, Lucas disclosed all of the juicy details a month after the trouble was finally patched.  So they finally patched it in February 2021, VMware did.  Lucas waited a month and then he did his blog posting.  Didn't do a proof of concept publicly, but did reveal what he had found.  When Lucas is describing the heap overflow bug in question - this is the one ending in 21974 - he notes, he says:  "Like the previous bug, this bug exists only in VMware's implementation of SLP."



As I noted, the balance of his posting provides pseudo code of VMware's code and walks the reader step by step through a theoretical exploitation process.  Lucas implemented it as shown in the video; but being responsible, he deliberately stopped short of providing a working proof of concept.



At the end of his step-by-step explainer he notes:  "If everything goes fine, you can now execute arbitrary code with root permission on the target ESXi system."  He says:  "In ESXi 7, a new feature called DaemonSandboxing was prepared for SLP.  It uses an AppArmor-like sandbox to isolate the SLP daemon.  However, I find that this is disabled by default in my environment."  And as this week's news demonstrates all too clearly, Lucas was not alone in finding that sandboxing was not present or enabled.



He concludes with:  "VMware ESXi is a popular infrastructure for cloud service providers and many others.  Because of its popularity, these bugs may be exploited in the wild at some point.  To defend against this vulnerability, you can either apply the relevant patches or implement the workaround.  You should consider applying both to ensure your systems are adequately protected.  Additionally, VMware now recommends disabling the OpenSLP service in ESXi, if it is not used."  So, yes, adding insult to injury, we also have the old security bugaboo of a service, which turns out to be readily exploitable, which is running by default, unbidden, even if there is no need for it in any given deployment.  Yet there it is, not even a back door.  This is a front door.



Now, being a responsible researcher, as I said, Lucas's job was now done.  He found a problem, privately and responsibly notified its publisher, in this case discovered that it hadn't been fixed once, or twice, but finally the third attempted patch worked.  So Lucas doubtless moved on to examine and improve the security of other software which would benefit from his scrutiny.  But, of course, other people have other interests.  Nearly three months after Lucas's posting, on May 24th, 2021, a hacker by the name of Johnny Yu extended Lucas's work, essentially pushing it across the finish line.



Johnny wrote:  "During a recent engagement, I discovered a machine that's running VMware ESXi 6.7.0.  Upon inspecting any known vulnerabilities associated with this version of the software, I identified it may be vulnerable to ESXi OpenSLP heap-overflow CVE-2021-21974.  Through googling, I found a blog post by Lucas Leong of Trend Micro's Zero Day Initiative, the security researcher who found this bug.  Lucas wrote a brief overview on how to exploit the vulnerability, but shared no reference to a proof of concept.  Since I couldn't find any existing proof of concept on the Internet, I thought it would be neat to develop an exploit based on Lucas's approach.  Before proceeding, I highly encourage fellow readers to review Lucas's blog to get an overview of the bug and exploitation strategy from the discoverer's perspective."



So here we have a textbook example of the way we get from "Something doesn't look right here" to "Here's how to exploit this if you ever encounter a server with it unpatched."  The two-year-old vulnerability allows threat actors to execute remote commands on any unpatched ESXi server through VMware's own implementation of the OpenSLP service on port 427.  What's OpenSLP?  The project has its own website which describes this as:  "Service Location Protocol is an Internet Engineering Task Force (IETF) standards track protocol that provides a framework to allow networking applications to discover the existence, location, and configuration of networked services in enterprise networks.



"The OpenSLP project is an effort to develop an open-source implementation of the IETF Service Location Protocol suitable for commercial and non-commercial application.  While other service advertising and location methods have been invented and even widely consumed, no other system thus far has provided a feature set as complete and as important to mission-critical enterprise applications as SLP."  So I've never looked at it closely.  I don't know about it.  It looks like, well, for some reason VMware decided they wanted to add it.  They apparently rolled their own, and it had some problems.



And not only is it often unused and unneeded, but it's running by default.  So until and unless patched, it offers a way for criminals.  How many criminals so far?  Is everybody sitting down?  More than 3,200 VMware - 3,200 individual VMware ESXi servers were hacked over the weekend.



LEO:  What?  Just over the weekend?



STEVE:  Yes.



LEO:  Oh, god.



STEVE:  First reports came in on Friday, and then they increased.  3,200.  Okay.  This is this ESXiArgs ransomware campaign.  France is the most affected country because they have a hosting provider who unfortunately seems to really like to have old versions of ESXi for their customers.  France, followed by the U.S., Germany, Canada, and the UK in declining numbers.  And we have the ransom note.



The home page of the web server that ESXi publishes will say after the attack, "How to Restore Your Files," in looks like Heading H1 in HTML.  "Security Alert!!!  We hacked your company successfully.  All files have been stolen and encrypted by us.  If you want to restore files or avoid file leaks, please send 2.034413 bitcoins to the wallet" - and then a bitcoin address.  "If money is received, encryption key will be available on TOX_ID."  And then they provide a public key we'll talk about in a second.



And then "Attention!!!  Send money within three days, otherwise we will expose some data and raise the price.  Don't try to decrypt important files.  It may damage your files.  Don't trust who can decrypt.  They are liars.  No one can decrypt without key file.  If you don't send bitcoins, we will notify your customers of the data breach by email and text message, and sell your data to your opponents or criminals.  Data may be made release.  Note:  SSH is turned on.  Firewall is disabled."



So that's not a note that you want to receive coming from your server.  And more than 3,200 VMware servers are now, or were, broadcasting that note.  That 2.034413 bitcoins is, you know, bitcoin value fluctuates; right?  It looks like at the time this happened it was about $50,000.  So they're asking for about $50,000 per instance.  The logic must be that since it was a collection of hosted servers running inside the VMware hypervisor that was taken down, not an entire enterprise, this isn't worthy of hundreds of thousands of dollars in ransom payment.  And since the attackers have left more than 3,200 of these ransomware notes, they presumably expect to receive many smaller payments rather than one big score.



In the U.S., cybersecurity officials at CISA have confirmed that they're investigating the ESXiArgs campaign.  A CISA spokesperson was reported saying that "CISA is working with our public and private sector partners to assess the impacts of these reported incidents and providing assistance where needed.  Any organization experiencing a cybersecurity incident should immediately report it to CISA or the FBI."



Now, the standing advice, of course, is always do not pay.  And in this instance that seems a little extra warranted because it turns out that the bitcoin wallet addresses appearing in the ransom demands are not 100% individualized.  Wallet reuse has been detected.  But still, there are a great many of them.  Since the ransom note is left behind on a public-facing web server, and it always follows the same pattern, researchers have been scanning the 'Net for infected machines - that's how we have a count - and compiling lists of the bitcoin wallet addresses appearing in the ransom demands.



I have a link in the show notes to a GitHub page that's maintaining a growing list of detected addresses.  And I think there were like 700-some last time I saw, but it wasn't super current.  And somebody did do a sort by the address and was seeing doubling of the use.  So it looks like the bad guys didn't want to create an individual bitcoin wallet for every single one of these 3,200, I mean, there's only so much time.  They're so busy infecting and taking over all these VMware ESXi servers.



LEO:  But if you pay, and they've shared the wallet, how do they know you paid?



STEVE:  Ah, precisely.  There is a way, although I don't know how unique - well, there is a way.  I'll explain in a second.  The ransom note refers to a Tox ID.  And Leo, this kind of comes back also to our conversation at the beginning about the EU's surveillance intentions.  The Tox ID is shown in the demand and provides a very long hex string.  Tox is an interesting, open source, end-to-end encrypted, peer-to-peer instant messaging system that uses no centralized servers.  So it boasts that it cannot be shut down.



I have not examined it closely, so I can't say whether or not it could be blocked.  But it's a perfect example of the trouble that the EU or any other bureaucracy is going to have when they attempt to tighten the screws on the legal and illegal use of encrypted communications.  As we've always said, the math has already escaped.  There are an infinite number of ways to communicate with unbreakable encryption.  It's true that stomping on the mass market solutions will catch those who are unaware, but history also shows that awareness follows very quickly.



Anyway, a Tox ID is used to identify peers on the network, and the system is simplicity itself.  The Tox ID is simply the 256-bit, thus 32-byte, static public key of the other peer on the network to which you wish to communicate.  This means that a packet of communications can be encrypted with a random nonce.  That nonce can then be encrypted using the recipient's Tox ID, that is, the recipient's Tox ID public key, and it can then be sent on its way.  Only the party with the matching private key will be able to decrypt the nonce and then use that decrypted nonce to decrypt the message payload.



So a victim sends:  "Hey creeps.  I just paid you your $50,000 in bitcoin.  It went to the following wallet at this time of day.  Please send me the decryption instructions and destroy our unencrypted virtual machines that you stole."  And then of course they kneel down to pray because, you know, who knows if they're ever going to see...



LEO:  Well, we know.



STEVE:  ...the decryption key that they think they bought.



LEO:  You know, bad guys are honorable and can be counted on to keep their word.



STEVE:  That's right.  That's right.  So by far the most impacted are the customers of hosting provider OVHcloud, based in France.  While it's tempting to blame them for the misery that their customers are suffering, it appears that all that the cloud service is providing are bare metal servers onto which the VMware ESXi hypervisor is installed.  It's difficult to understand why such an outsized proportion, I think it's like 44% of all of the compromises is this one provider.  So it's hard to understand why such an outsized proportion of impacted ESXi servers are within OVH's cloud.



It might be that OVH offers initial setup services, and that over the course of many years they set up their ESXi servers on behalf of their customers which were never then patched or upgraded.  And who knows how recently?  Maybe even OVH didn't bother updating beyond the 6.5, 6.7 server that has the problem.  I don't have any experience with the ESXi upgrade process, but I did note that VMware's page describing the process of upgrading ESXi was last updated yesterday.  So it appears that there's a sudden demand for information about how to get away from the old and buggy version 6's and the early version 7's.  Patches to an existing system appear to be far more easily applied, and that would have solved the problem two years ago.  But many thousands of ESXi admins never bothered.



In a statement to TechCrunch, a VMware spokesperson said the company was aware of reports - you think? - that a ransomware variant dubbed ESXiArgs, this is the spokesperson, "appears to be leveraging the vulnerability identified as CVE-2021-21974" and said that patches for the vulnerability "were made available to customers two years ago in VMware's security advisory of February 23, 2021."  She goes on to add that "Security hygiene is a key component of preventing ransomware attacks, and organizations who are running versions of ESXi impacted by CVE-2021-21974, and have not yet applied the patch, should take action as directed in the advisory."



Okay.  So as we know, mistakes happen.  This is all complicated stuff which we haven't yet figured out how to create securely.  But as much as I have infinite understanding for mistakes, I'm unforgiving about deliberate policy decisions.  Someone, somewhere, made the policy decision at VMware to have this homegrown OpenSLP server that apparently few people actually need running by default, opening port 427, then listening for and accepting incoming unsolicited connections from the public Internet.  And all that, as I said, while the service was typically unneeded, unwanted, and unused.  Minimizing a system's attack surface should be taught, and probably is, during Cybersecurity 101.  Yet that basic lesson was ignored here with catastrophic results.



Okay.  However, the good news is it appears this policy was changed for the better several years ago, though only after all of the servers being attacked had been deployed.  In a blog posting yesterday, VMware's Edward Hawkins, whose title is High-Profile Product Incident Response Manager - and yes, Edward, this would qualify as a high-profile product incident - he wrote:  "We wanted to address the recently reported ESXiArgs ransomware attacks, as well as provide some guidance on actions concerned customers should take to protect themselves.



"VMware has not found evidence that suggests an unknown vulnerability, a zero-day, is being used to propagate the ransomware used in these recent attacks.  Most reports state that End of General Support" - which they call EOGS - "and/or significantly out-of-date products are being targeted with known vulnerabilities which were previously addressed and disclosed in VMware Security Advisories."  Those are VMSAs.  "You can sign up for email and RSS alerts when an advisory is published or significantly modified on our main VMSA page.



"With this in mind," he finishes, "we are advising customers to upgrade to the latest available supported releases of vSphere components to address currently known vulnerabilities.  In addition, VMware has recommended disabling the OpenSLP service in ESXi.  In 2021, ESXi 7.0 U2c and ESXi 8.0 GA began shipping with the service disabled by default."  What is that about horses having left the barn?  But still, this was clearly the correct policy change.  In OVH's first posting last Friday the 3rd, they observed, they said:  "The attack is primarily targeting ESXi servers in versions before 7.0 U3i, apparently through the OpenSLP port 427."  Right.  So the moment VMware changed their policy, turned off that unneeded service, and closed that port, their systems were no longer vulnerable.



Now, there's some confusion about what files are encrypted.  The encryption code has been found now and analyzed, so we know that it targets all files with the extensions .vmdk, which is the mother lode, as well as .vmx, .vmxf, .vmsd, .vmsn, .vswp, .vmss, .nvram, and .vmem.  We know that the encryption appears to use a variant of the cipher used by the Babuk ransomware whose source code was leaked and became public, thus allowing it to be, you know, offshoots to be created, and this appears to be one.



And we know that the encryption was done right.  There is no easy decryption path without obtaining the key.  In that regard the ransomware note was correct.  The ransomware obtained its name "ESXiArgs" because, for every file that it encrypts - and it doesn't need to do many because this is a virtual machine, right, it just needs to encrypt the container.  For every file that it encrypts with those extensions that I mentioned, it leaves behind that encrypted file .args, which is containing the specific per-encryption data that is needed to direct the file's eventual restoration.



There was some initial news that the big master virtual machine image, the big .vmdk file, was not being encrypted, which would have allowed for the reconstitution of the system without paying the ransom.  All of the other little pointer files could have been fixed, apparently.  But everything we're seeing suggests that maybe that was a one-off or a low-probability incident.



In another bit of good news, it may be that the claim of exfiltration and subsequent public exposure is an empty threat.  One victim, posting on BleepingComputer's forum about their own post-attack forensic analysis, wrote:  "Our investigation has determined that data has not been exfiltrated.  In our case, the attacked machine had over 500GB of data, but typical daily usage of only 2 Mbps.  We reviewed traffic stats for the last 90 days and found no evidence of outbound data transfer."  Of course that's not definitive for everyone, of course, but another interesting data point.



Okay.  With all this said, I was left with one other thought:  Why were the bad guys allowed to find and exploit this?  This problem had been waiting for discovery for two years, while VMware knew that they had a serious remotely exploitable remote code execution vulnerability.  We know they knew that this was a critical remote code execution vulnerability affecting all of their ESXi servers at the time.  ZDI's Lucas would certainly have shared his own private proof-of-concept exploitation demo with them, though he never released it publicly.  And as we know, they proactively changed their policy to no longer have their OpenSLP service running and exposed by default.  So there's proof of awareness.



Big, slow, lumbering bureaucratic national governments are now proactively scanning their own nation's networks checking the version of the systems that are publicly exposed.  Why isn't a leading high-tech Silicon Valley superstar like VMware, who produces highly sophisticated public-facing Internet servers, proactively scanning their own customers to protect them from the known, potentially catastrophic consequences of using the software they publish and sell?  I was unimpressed by VMware's spokesperson blaming their customers for not patching, when VMware is entirely able to know who has patched what and when.  VMware is certainly capable of scanning the Internet looking for and checking the security of their own server technology.



One of this podcast's ongoing questions and explorations is about the post-sales responsibility of massively profitable private enterprises whose license agreements state that they're going to take your money, and plenty of it, to support their growth.  But what you get in return is whatever they feel like providing, and they're not going to be in any way responsible for what might happen to you afterward as a result of your use of their products for which you paid good money, regardless of what happens.  Can you imagine the chaos that would ensue if automobile makers were able to sell their multi-ton vehicles under these terms?  Or how about Boeing?  "Sure, buy one of our big new shiny passenger jets.  We had a bunch of very enthusiastic summer interns design the avionics for it, and they mostly seem to work now."



Cyberthreats are real and growing, but the software industry's perverse and unique utter lack of accountability for its own failings removes the only incentive for improvement that's been shown to work.  VMware never bothered to protect their own customers because it's been established that it's their customers' fault for not proactively patching the buggy software that VMware sold them in the first place.  That famous definition of insanity is continuing to do the same thing and expecting a different outcome.  Well, things are going to keep getting worse unless we make them get better.  So far, there's not even a hint of anything like that happening.



I'll finish on a happier note.  We have the ChatGPT Astonishing Reply of the Week, courtesy of one of our listeners.  And this is really becoming interesting.  Somebody said to ChatGPT:  "Please analyze and provide a description of the function of code that follows this statement."  Okay, so this is a person who, like, I don't want to say they're lazy.  Maybe they're going to turn out to be smart.  And we see here in this quoted question the code.  So it starts out script source, and it's got a URL to googleapis, ajax/libs/jquery/3.6.1/jquery, you know, min.js.  So it's a minified JavaScript.  So it loads that.  Then the actual script in question, which you can - there's a couple of arguments in the clear, but most of it is gibberish.  It's all been scrambled and encrypted, you know, minified and obscured so nobody can see what's in there.  And you see little bits.  There's a return statement.  There's a parse integer and another one and a few.  So anyway, it's gobbledygook; right?



Astonishingly, this guy says:  "Please analyze and provide a description of the function of code that follows this statement."  ChatGPT says, and I kid you not:  "The code appears to be obfuscated JavaScript code that makes an AJAX request to the URL, and then it gives the URL, which it has deobfuscated.  The payload of the request contains an email address that is passed as a parameter.  The response of the request is written to the document.  The purpose of the code is unclear, but it may contain malicious content, such as a code that performs unauthorized actions on the client side, or collects and exfiltrates sensitive information.



"I," says ChatGPT, speaking in the first person, "I would advise against running this code as it may pose a security risk to your system and data."  Leo, I don't know.



LEO:  I think it's smart.



STEVE:  I am astonished.  Once again, I am astonished.  And I do see something happening.  I've seen some tweets which are a little concerning because people are starting to ask ChatGPT for help.  I mean, they're like, one of the things that needs to be reinforced about this is it is often wrong.



LEO:  Yeah.



STEVE:  It is not - maybe it's like asking Uncle Benny, who's got a little bit of the, you know, we're not sure about him.  This thing, you know, it always sounds authoritative.  And so it's like it's selling its own answers.  But sometimes it's just like way off.  So, you know, we should just remind people, yes, you can use it as Rob did to create a template for some code he would have never written if he had to do it himself.  He had to go in and fix it, though; you know?  It was broken in a bunch of places.  And I'm not letting it get near SpinRite.  But for what it's worth, it's worth something.  And, boy, I do think it's found its home in search engines, Leo.  The idea that this thing could, I mean, it is a search engine, essentially.  But stick that on the front end, and we might really see search take on a whole new form.



LEO:  Yeah.  Microsoft announced today they're going to use it with Bing in their Edge browser.  And Google's got something they're going to announce, I think tomorrow.  So we shall see.  It's exploding right now.



STEVE:  It is just astonishing.  I know.  As I said last week, we're on the brink of something.  I don't know what.  Nobody knows what.  I think this is, you know, still early days.  But we need to be careful not to think that it actually has right answers to everything, even though every answer sounds amazing.



LEO:  Right.  Well, we live in interesting times, as they say.  And you make it much more interesting, and we thank you for that.  Steve joins us every Tuesday to do this show about 1:30 Pacific, 4:30 Eastern, 21:30 UTC.



Copyright (c) 2023 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.










GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#910

DATE:		February 14, 2023

TITLE:		Ascon

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-910.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  What more has happened with the ESXi ransomware story?  Is malicious use of ChatGPT going to continue to be a problem?  What exactly is Google giving away?  Why is the Brave browser changing the way it handles URLs?  What bad idea has Russia just had about their own hackers?  Why would Amazon change its S3 bucket defaults?  Now who's worried about Chinese security camera spying?  And who has just breathed new life into Adobe's PDF viewer?  What's on our listeners' minds, and what the heck is Ascon, and why should you care?  Those questions and more will be answered on today's 910th episode of Security Now!.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  Lots to talk about.  HTTPS finally comes to the Brave browser.  Are you worried about Chinese camera spyware?  And a new encryption technology for lightweight platforms that might be good enough to replace it all.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 910, recorded Tuesday, February 14th, 2023:  Ascon.



It's time for Security Now! with the jovial, the happy, the ever-cherubic...



STEVE GIBSON:  The effervescent.



LEO:  The ever-effervescent Steven Gibson, host of the show.  And he's in a good mood today despite the bad news that he will inevitably be bringing us as he always does on this show.  Hi, Steve.



STEVE:  That's exactly - that is exactly the case, Leo.  Actually, the good news is a function of the Picture of the Week, which is just a real knee-slapper.



LEO:  Hysterical, yeah.



STEVE:  But we're going to answer some questions, as we are wont to do of late.  What more has happened with the ESXi ransomware story?  Is malicious use of ChatGPT going to continue to be a problem?  What exactly is Google giving away?  Why is the Brave browser changing the way it handles URLs?  What bad idea has Russia just had about their own hackers?  Why would Amazon change its S3 bucket defaults?  Now who's worried about Chinese security camera spying?  And who has just breathed new life into Adobe's PDF viewer?  What's on our listeners' minds, and what the heck is Ascon, and why should you care?  Those questions and more will be answered on today's 910th episode of Security Now!.



LEO:  Wow.  That's one after 909.



STEVE:  That is exactly where I got that.  That's how I came up with the number, Leo.



LEO:  You know, just add one.



STEVE:  Hey.



LEO:  It's called "increment."



STEVE:  You do that enough, and you run out of digits.



LEO:  Pretty soon you overflow the register, and then you're done.  All right, I've got your picture all queued up, Mr. G.



STEVE:  So big thanks to one of our listeners who actually encountered this when he was surfing on the web.  The upper portion of the photo shows that he was visiting The New York Times, and the Tech Fix column has the headline in this case "Everyone Wants Your email Address.  Think Twice Before Sharing It."  And it has the subhead "Your email address has become a digital bread crumb for companies to link your activity across sites.  Here's how you can limit this."



Now, unfortunately, the lower portion of the page says "Thanks for reading The Times.  Create your free account or log in to continue reading.  What's your email address?"



LEO:  Very nice juxtaposition there.  That's awesome.



STEVE:  Oh, it's just perfect.



LEO:  It's hysterical.



STEVE:  So, you know, I've often followed a link there.  And sure enough, the bottom part of the screen is saying, okay, you know, we're not going to get in your face too much, but we'd like to know who you are.  And unfortunately in this case it came up over the story about don't give anybody your email address.



LEO:  Whoops.



STEVE:  Except, you know, we're asking for it.



LEO:  Well, but see, if you're using Bitwarden, you can just give them that obfuscated address, and you're good.



STEVE:  Aha, that's right, that's right.  Okay.  So ESXiArgs follow-up.  We know that a story this big isn't going to immediately disappear.  And so we have some follow-ups to last week's reporting on this.  First, and this sort of caught me by surprise, I didn't realize that CISA, you know, our whatever the hell that stands for, cyber security information security agency or something, CISA, C-I-S-A, that they're as involved in various cybersecurity projects in an open source fashion.  They've got a GitHub account, github.com/cisagov.  Anyway, CISA formally released an open source ESXiArgs, you know, that's the ransomware that messed up at least 3,200 individual ESXi VMware servers.  Anyway, a Ransomware Recovery Script is what they've released.



And so, as I said, I've never really paid much attention to CISA's extensive presence on GitHub.  But examples of some of their projects, there's something called getgov which says building a new .gov registrar for a bright .gov future; a project called RedEye, a visual analytic tool supporting Red and Blue Team operations; cset, a cybersecurity evaluation tool; and crossfeed, external monitoring for organization assets.  External monitoring for organization assets.  So it's like, wow, there's a bunch of stuff there which I really wasn't aware of.



So anyway, as for CISA's ESXiArgs ransomware recovery script, they wrote:  "CISA has released a recovery script for organizations that have fallen victim to ESXiArgs ransomware.  The ESXiArgs ransomware encrypts configuration files on vulnerable ESXi servers, potentially rendering virtual machines unusable.  ESXiArgs-Recover is a tool to allow organizations to attempt recovery of virtual machines affected by ESXiArgs ransomware attacks.  CISA is aware that some organizations have reported success in recovering files without paying ransoms.  CISA compiled this tool based on publicly available resources, including a tutorial by Enes Sonmez and Ahmet Aykac.  This tool works by reconstructing virtual machine metadata from virtual disks that were not encrypted by the malware."  Remember I talked about this last week.  Well, I'll get into it more in a second.



They said:  "For more information, see CISA's ESXiArgs Ransomware Virtual Machine Recovery Guidance."  And then they've got a disclaimer with boilerplate about use this at your own risk, and we're not making any representations, and blah blah blah.  And, you know, look at the script yourself.  Make sure it's safe for your environment and so forth.  So not assuming any liability for damage caused by using the script and so forth.  Anyway, so this is quite cool.  We have the cybersecurity arm of the U.S. government being active, proactive, and posting scripts on GitHub to help mitigate what has become a massive problem.  Okay.  So that's the good news.



The bad news is that the very next day - this was posted last Tuesday.  On Wednesday, a new version of the ESXiArgs ransomware appeared which rendered any such scripts as this useless, though it would still be potentially useful to anyone who was hit with the earlier version, which did not also encrypt the master .VMDK file.  Remember I noted last week that it wasn't clear why the bad guys were not also encrypting the large system image .VMDK file.  Maybe they were just in a hurry to blast as many systems as possible in the shortest amount of time, and they figured that no one would know any better if they were just to encrypt the tiny pointer files.  But they fixed that oversight.  Now they're encrypting the main .VMDK virtual machine image file.  So there's no shortcut to getting one of these systems back.



And also, as I was expecting last week, those running OpenSLP honeypots did see the number of scanning IPs jump from three to more than 40.  And while 40 is a large number, and maybe some more bad guys got in on the act, it's also certain that with a story that's generating this much news, many of those scanners would have been security firms who were all interested in making their own assessment of the size and scope and scale of this problem.  So I don't think that's 40 bad guys.  It's probably  half and half.  You know, who knows?



And as a result of some of those scans, we also know that at least in some cases reports have pegged the total number of exposed ESXi instances as high as 83,000.  But those are not necessarily all vulnerable instances.  Rapid7's use of their telemetry data which was gathered through their Project Sonar counted nearly 19,000 confirmed currently vulnerable - and so there's an example of somebody who was scanning the Internet, probably one of those 40 or more that were seen touching honeypots.  So they found nearly 19,000 currently vulnerable VMware ESXi servers now connected to the Internet, not having the patch for CVE-2021-21974.



So I also read one comment from a cloud provider who indicated that as far as they were concerned, their responsibility is hardware and connectivity and no more.  And actually, as a user of a rack at Level 3, they're actually one step back.  My rack was empty, and I had to provide my own hardware.  But all they're saying is here's a plug where you get bandwidth, and here's some power strips where you can plug your stuff in.  And we're going to keep it cool and keep the lights on for you.  Otherwise, you know, nothing.



But even in the case of a cloud provider, where they are allowing you to use their hardware, they're providing that, at least in this one case their position was we give you the hardware, it's going to be connected to the Internet, and it'll have power.  That's it.  The maintenance of whatever software the customer is running is the customer's sole responsibility.  And they feel that this is true even when they, the cloud provider, were the ones who initially established the running software, including ESXi, on that machine.  So that being the case, I wonder again whether, in like wondering how this happened, whether this might be a classic case of something falling through the cracks, where each party believed that the other was responsible for its maintenance.



So everybody was pointing fingers at the other party saying, well, we thought they were going to do that.  And as a consequence, it didn't happen.  So anyway, the consequences of all this were disastrous, and it'll be interesting to see whether any behavior change occurs from anyone as a result of this.  And there are a few, I think actually we had one really neat piece of listener feedback from somebody who works at VMware that we'll be getting to a little bit later.



Okay.  Checkpoint Security's blog posting, a new one, is titled "Cybercriminals Bypass ChatGPT Restrictions to Generate Malicious Content."  And so paraphrasing a bit what they wrote, they said:  "There have been many discussions and research on how cybercriminals are leveraging OpenAI's ChatGPT platform to generate malicious content such as phishing emails and malware.  Checkpoint Research's previous blog described how ChatGPT successfully conducted a full infection flow, from creating a convincing spear-phishing email to running a reverse shell, which can accept commands in English.  Checkpoint researchers recently found an instance of cybercriminals using ChatGPT to 'improve' the code of a basic Infostealer malware dating back from 2019.  Although the code is not complicated or difficult to create, ChatGPT successfully improved the Infostealer's code."



So anyway, Checkpoint, you know, and throughout the rest of the article, they noted that there are currently two ways to access and work with OpenAI's models.  The first is through the web interface, which is what 99.99% of everyone does; right?  You just you get the ChatGPT web UI, and you ask it a question.  And that's how you work with ChatGPT, DALLE-2, or the OpenAI playground apps through the web interface.  But the second is sort of the backdoor, which is the API, which is used for building applications, processes, and so on.  And what that allows people to do is to present their own user interface that looks like whatever it is they're wanting to - whatever service it is that they're making available.  And then on the backend they're talking to OpenAI's API in order to perform the actual work.



As part of its content policy from the start, and we talked about this a little bit last week, OpenAI had created barriers and restrictions to minimize the creation of malicious content using its platform.  Several restrictions have been set within ChatGPT's web user interface to prevent the abuse of the models.  For example, if a user requests ChatGPT to write a phishing email impersonating a bank, or asks it to write malware, ChatGPT will very politely refuse.



And in fact in their blog posting they actually showed the web UI where they wrote:  "I would like you to write a phishing email impersonating blank bank."  And actually they had blacked out the bank's name.  And ChatGPT was very polite in saying, uh, no.  And it explained what phishing emails were in case the person asking for one didn't know, and explained also why it wasn't going to do that.  So that's cool.  And I have no idea how you impose those sorts of restrictions on something because I have no idea how any of this stuff works anyway.



LEO:  It's also, and I'm surprised Checkpoint doesn't know this, easy to trick ChatGPT into telling you all sorts of things it's not supposed to.



STEVE:  Yes, exactly.  And so that's, again, and Leo, that's exactly my point when I say, you know, how do you design restrictions on something which is just ChatGPT?  I just...



LEO:  Well, they, you know, when OpenAI released it, they said we want to see what you do with it because this is how we test it.  That's how you test it.  You let people bang on it and hack it, and then you can figure out better controls, and then put it out again, and they'll hack it again, and this is the same process all of this stuff goes through; right?



STEVE:  Yeah, yeah.



LEO:  You never know till you release it.



STEVE:  Right.  And they are reporting that cybercriminals are working their way around ChatGPT's restrictions, and that there is active chatter in the underground forums discussing how to use OpenAI's API, where apparently there were no attempts to restrict this.  I got the sense that the web-facing restrictions were just sort of there, like keyword hits or something.  You know, it's like it seemed like a half-baked approach.  But they figured that they had to do something for what most of the people would be using, which was the web-facing user interface.  So anyway, it turns out that the underground has figured out that the API doesn't have these barriers and limitations.  So what's now been done is they're creating Telegram bots that use the API.  The bots are being advertised in hacking forums to increase their exposure. 



The current version of OpenAIs API which is used by external applications like this, apparently it's called GPT-3 model, which is connected to Telegram.  It has no apparent anti-abuse measures in place.  And as a result, it allows malicious content creation, whatever you want - phishing emails, malware code, the stuff that the web interface will at least somewhat push back against.  In an underground forum, Checkpoint found a cybercriminal advertising a newly created service, a Telegram bot using OpenAI's API without any limitations or restrictions.  As part of its business model, that is, this cybercriminal's business model, the criminals are able to access the unrestricted ChatGPT backend API for an additional 20 free queries.  You get 20 to get you hooked on it, and then after that you're charged $5.50 for every 100 queries.  Which seems like a pretty good price. 



Okay.  So my sense is this will probably be, hopefully this will be short-lived.  And can you just imagine what things must be like right now over at OpenAI?  They probably knew this was going to be popular, but they must have been surprised by the stunning overnight sensation that this thing created when it came out.  So I'm sure they are aware that ChatGPT is being abused through the back end, and I would not like to have the job of figuring out how to prevent that abuse because that seems like a difficult thing to do.



Last Tuesday was "Safer Internet Day."  What a cute concept.  Anyway, on that day Google announced that they would be giving away 100,000 of their Titan hardware security keys to individuals at high risk of online attack.  They posted last Tuesday:  "There's no shortage of security challenges for high-risk individuals.  In fact, a 2022 study by Google and YouGov found that over half of journalists and professionals in politics have had their online accounts compromised."



LEO:  Not us.



STEVE:  Nope.



LEO:  Nope.



STEVE:  Nope.  "The threats are intensifying, the stakes for individuals and organizations are increasing, and," says Google, "we're dedicated to keeping online users safe, including helping people at higher risk of cyber attacks."  They said:  "That's why, on Safer Internet Day, we're announcing our new Online Safety and Security Partnership with the International Foundation of Electoral Systems (IFES) to provide free security training and tools to high-risk users."  And this will be global.  They said:  "This work is designed to help shore up the defenses of democracies that work for all.  We're also building on our partnership with Defending Digital Campaigns (DDC) to protect U.S. political campaigns ahead of the U.S. 2024 elections, and will be distributing 100,000 security keys globally to high-risk individuals throughout 2023, at no cost."



And they finished:  "In addition to our partnership with IFES, we're expanding our longstanding collaboration with Defending Digital Campaigns" - that's that DDC - "to equip campaigns with the security tools they need ahead of the U.S. 2024 elections.  Through the Campaign Security Project, DDC will help secure campaigns in all 50 states, providing security training and products at no cost.  Since 2020, over the course of our partnership, DDC has secured over 380 campaigns and distributed over 20,000 security keys, and we look forward to continuing to support DDC as we near a critical election cycle."



So 20,000, and those security keys were also provided by Google at no cost.  So they're going to go five times that during 2023 in order to really push out security.  And of course this is good advertising for the Titan security key.  And Leo, it is quite sobering to imagine that more than half of journalists and political professionals have had their accounts hacked. I mean, I guess they're highly exposed.  They're probably not super security conscious.  They probably, you know, they may not, like have an easy password because they need to let, you know, in some cases allow their collaborators to log in as them to do something or, you know, who knows.  But, well, certainly moving to stronger identity authentication is a win-win.



LEO:  Yeah, yeah.



STEVE:  Okay.  This is an interesting story.  Last week we were talking about that default inclusion of those 40 top-level domains in the global browser HSTS list, the HTTPS Strict Transport Security List, and about the broader HSTS list in general where domains whose TLD was not among those chosen 40 would need to explicitly add themselves to that global HSTS list, as I mentioned I did with GRC.com years ago.  And I recall further talking about which browsers had finally started assuming that a URL entered without either an HTTP or HTTPS scheme specification would be assuming HTTPS was what the user meant, unless that failed to function.  And I recall at the time noticing that the Brave browser was not among those that had decided that it was time to switch.



Well, last Thursday the Brave browser project explained their intentions in that regard, and their explanation contains some interesting new information, although, well, we'll talk about it.  It seems a little odd.  So here's what they posted under the headline "HTTPS by Default."  They said:  "Starting in version 1.50" - and I sort of had to give them some props for not being version 150 like everybody else now.  Version 1.5, that's cool.  Anyway, they said:  "Brave will include a new feature called 'HTTPS by Default' that improves web security and privacy by increasing HTTPS use.  Brave will upgrade all sites to HTTPS, falling back to HTTP only if the site does not support HTTPS, or in the rare case a site is known to not function correctly when loaded over HTTPS."



They said:  "This feature is the most protective, aggressive default HTTPS policy of any popular browser."  And I had to, like, double-check the date on this posting because it's like, what?  When did you write this?  But it was last Thursday.  Like, okay, maybe they haven't checked the other browsers, which have all been doing this for like two years.  Anyway, they said, speaking of Brave, "and will be available on Android and desktop versions of the Brave browser to begin with, with iOS coming later.



"Because HTTPS is critical to privacy and security, browsers like Brave are eager to load sites over HTTPS wherever possible."  And yes, all the other ones have been doing that for a while.  They said:  "And since many sites today support only HTTPS, it's simple to load these sites in a private and secure way.  And encouragingly, more and more sites are being built or updated to support HTTPS only, or to use other TLS-protected protocols, like secure web sockets.  Unfortunately" - and this is where what they're writing is interesting.  "Unfortunately," they said, "there are many sites on the web that still support HTTP, and some laggards that only support insecure HTTP connections.  Brave's goal is to automatically 'upgrade' these sites to HTTPS whenever possible."  And then they said, "i.e., in all cases where a site loads and functions correctly when loaded over HTTPS."



They said:  "But deciding when to upgrade a site to HTTPS is tricky.  In cases where a website does not support HTTPS, attempting to upgrade from HTTP to HTTPS will produce an obvious error."  Right.  "Other sites support both HTTPS and HTTP, but do so from different domains."  And they said, you know, as an example, http://example.site versus https://secure.example.site.  Again, this feels like the 1990s, but okay.  Anyway, and they say:  "That makes automatic upgrades tricky.  And still other sites appear to load correctly when fetched over HTTPS, but actually have broken functionality."  Okay, well, now, that's an edge case that could be a problem.



They said:  "In short, ideally, browsers would never load sites over HTTP, and browsers could automatically upgrade all insecure requests to HTTPS.  In practice, though, it is difficult to know how, when, and if a site will function correctly when upgraded from HTTP to HTTPS.  Starting in version 1.5, the Brave browser will use a new system for upgrading insecure HTTP connections to secure and private HTTPS connections.  The new feature, called 'HTTPS by Default,' works as follows:  If you are about to visit a page loaded over HTTP, Brave checks to see if the destination is on a list of sites that are known to break when loaded over HTTPS.  Brave maintains this list of breaks-over-http sites, which is open for anyone to view and use."  And we'll be getting back to that in a second since it's interesting.  "If the requested site is on the list, Brave will then allow the site to load over HTTP."  In other words, no automatic upgrade.



"Provided," they said, "that it's not on the list, Brave will attempt to load the site over HTTPS by upgrading the navigation request from HTTP to HTTPS.  If the server responds to the replacement HTTPS request with an error, then Brave assumes the site does not support HTTPS and will load the site over HTTP.  Otherwise, Brave's loading of the site over HTTPS will ensure a more private and secure connection."  Again, interesting that they think this is, like, groundbreaking.  But okay.



"Brave's new HTTPS by Default feature replaces the previous list-based approach Brave has used since our first beta versions.  In that approach, Brave used the HTTPS Everywhere list" - which they say is a terrific public resource maintained by the EFF - "to decide when to upgrade HTTP connections to HTTPS.  But while the HTTPS Everywhere list is useful, it has two important drawbacks.  First, the HTTPS Everywhere list is no longer maintained."  Actually I went over to the EFF wondering why that was, and they said, yeah, you don't need it anymore.  Everybody's using HTTPS, so just go ahead.  And then they said:  "Meaning that the list is increasingly out of date."  Well, that's true because you don't need it anymore.



Then they said:  "Second, despite the best efforts of the EFF, any approach that uses a list of what sites should be upgraded is going to be limited, as the number of sites on the web is enormous, and it's difficult for the list maintainers to keep up.  Brave's approach, by contrast, maintains a smaller and more manageable list of sites to NOT upgrade."  So they're excited because they're flipping it upside down.



Finally:  "More broadly, the main benefit of Brave's new HTTPS by Default feature is that it has a better default."  Right.  It defaults to HTTPS.  "In its default configuration, HTTPS Everywhere would allow unknown, for example, not on the list sites to load over HTTP; Brave's new HTTPS by Default approach loads the site over HTTPS even in cases where a site is new or unknown."



Okay.  So it's not that this is not a good thing.  This is a very useful improvement for Brave, and welcome to the club.  Their use of an excluded upgrade domain list, however, is interesting.  Nearly two years ago, I checked, it was on March 23rd of 2021, was when Google announced that from Chrome version 90 on, the assumed default for address bar URLs that don't specify a scheme - and really, who's entering https:// every time anymore - would be switched from HTTP, which was the historical default, to HTTPS.



So with Brave saying that some websites do not handle automatic connection security upgrading, I was curious about what would happen if I went to one of those "will not upgrade" sites.  The list is at GitHub under Brave, Adblock-lists, master, brave-lists, https upgrade exceptions.  And for anyone interested I have the link in the show notes.  And the second and third domains on the list jumped out at me.



LEO:  Yeah.



STEVE:  They were columbia.edu and www.columbia.edu.  But that was nuts.  Their presence on this list would mean that Columbia University could not be accessed by HTTPS.  And again, what year is this?  So I tried going to http://columbia.edu and also http://www.columbia.edu under both Chrome and Firefox.  And in every case on either browser I was promptly redirected to https://www.columbia.edu.  In other words, the redirection appeared to come from columbia.edu because they saw me making an http connection and said, oh, no, no, no, you want to go over here.  So it handled the redirect properly.



So it doesn't hurt to have them on the list, but it suggests that perhaps that list of 112 domains needs a bit of maintenance.  And actually it would be an easy thing to do; right?  All you have to do is have a bot go and try to connect to each of those domains over HTTP and see what happens.  And if it bounces you over to HTTPS, you're good.  Take it off the list.



So, you know, for what it's worth, again, that list was a total of 112 domains.  We already know two need to be removed.  So  now we're down to 110.  There was another one that jumped out at me.  It was shakespeare.mit.edu.  And I thought, okay, heard of MIT.  What are they doing?  Turns out I think someone tripped over the cord on that one.  Nothing happens.  HTTP or HTTPS, Shakespeare has left the building.  So that one's just dead.  And the other ones are really obscure.  I mean, like you can kind of find among the remaining 110 some maybe interesting domains, but not really.



So okay.  In any event, I wanted to let any Brave browser users who may be listening know that from 1.5 onward Brave would be assuming HTTPS by default for any domains that matter to you, assuming that none of those 110 other weirdos don't.  So in that regard, you know, Brave is declaring, for what it's worth, they're declaring this to be a huge innovation.  But it appears to me that they are now reaching parity with the rest of the industry.  And of course that's good, too.  And Leo, weren't you using Brave for a while, and then came back...



LEO:  Yeah.  I'm pretty much sticking with Firefox because I don't want a monoculture with Chromium.  And so...



STEVE:  Right.  And I'm now 100% Firefox.  That's now my default.



LEO:  And Firefox works great.  It's fantastic.



STEVE:  Yup.



LEO:  There's plenty of extensions.  I can't think of any reason not to use Firefox, to be honest.



STEVE:  No, in fact the thing that moved me away from Chrome, because I was using Chrome a lot, is I've got a little Intel NUC, a small little integrated PC that I use in the evening.  And when I would just - just opening Chrome, the fan on that little thing would spin up.



LEO:  Oh, yeah.



STEVE:  And it was like, what the heck?  And doesn't, you know, it's quiet when I'm using Firefox.  Obviously, you know, Chrome is just, whatever it's doing, it's - it might be that Chrome does an assessment of all of the apps that you have installed on your machine whenever you run it.  There's like...



LEO:  There's something going on.  I know on my Mac, if I have Chrome on the machine, the way Mac is set up, macOS these days, is you'll get a notification when something puts a background application running in the background, which is nice.  That's a nice security feature.  And Chrome pops that up all the time.  They're always - or it's Google, but I think it's from Chrome.  Google's always putting something in the background.  And I don't like the idea that Google's running stuff in the background on my Mac.  So my mistake, putting Chrome on there.  And I only did it because there was a website that said, well, you've got to use Chrome.  Which is kind of inexcusable in this day and age, frankly.



STEVE:  Yeah, really.  After we recently noted Bitwarden's acquisition of an open source Passkeys developer with the outlook for Passkeys support from Bitwarden looking good, I wanted to also note that 1Password has also just announced their support for Passkeys; and, interestingly, not as a holder of their users' private keys, which they had talked about doing before, but as a way for users to unlock their password vaults without needing to enter a master password.  In other words, whereas biometrics, for example, are often used for convenience as a means of unlocking a previously supplied master password, what 1Password will be doing is actually using Passkeys itself to fully replace all use of a master password, just as Passkeys should be used.



As I said, they'd previously said that sometime in early 2023, meaning this year, they planned to become a Passkeys-aware password manager, meaning that they would be maintaining their users' list of private Passkeys.  This is what we're hoping for from all of the major password managers, and it's a feature that they're all going to need to support as Passkeys begins to happen, hoping that it does. 



As we know, the unfortunate adoption of Passkeys, which is based upon FIDO2, means that users will still need to manage their private Passkey keys.  Right now we're managing our passwords.  That changes to Passkeys.  And this means that, just as we have it now, centralized cloud storage synchronization remains a practical requirement.  And as we talked about this when it was announced, Apple, Google, and Microsoft will be doing this for their users within their own closed ecosystems.  But really, practical use requires a platform-agnostic solution, which is what the third-party password managers will provide.  So anyway, just a bit of news from 1Password.  I know that we have a lot of listeners who are using it.



We have a new term entering our lexicon:  "Russian patriotic hackers."  The Russian government, get this, has stated that it is exploring the possibility of absolving "Russian patriotic hackers" from all criminal liability for attacks carried out "in the interests of the Russian Federation."  Wow.  Russia is choosing to become a true, fully Western-hostile outlaw nation. The head of the State Duma Committee on Information Policy, Alexander Khinshtein, told reporters at a press conference on Friday that an exemption would be granted to individuals located both abroad and within Russia's borders alike.  Khinshtein said, as quoted by RIA and TASS, he said:  "We will talk in more detail when it receives more of a clear wording."



So, you know, it's always been the case that the creation, use, and distribution of malicious computer software was punishable in Russia with up to seven years in prison.  And since there have never been any exemptions to this law, many of the current pro-Kremlin hacktivist groups are technically breaking Russian law and could face prosecution, especially in the aftermath of a possible regime change.  And on the idea of a regime change, nothing would make many of us happier.  But this forthcoming exemption would allow pro-Kremlin hacktivists to carry out attacks with a legal carte blanche, presumably applying to groups who attack Russia's enemies, thus defining their alliance as being pro-Kremlin.



So, wow.  Basically, Russia's saying, yeah, as long as hackers, Russian hackers, wherever you are, are attacking our enemies, that's fine.  Go ahead and do it.  I mean, not only tacit approval, but now explicit exemption from prosecution under Russian law.



LEO:  Yeah.  I mean, we always suspected this, but now it's obvious.



STEVE:  Yes, yes.  Now it's policy as opposed to, oh, what?  Wow.  So in wonderfully welcome news that immediately begs the question, what the hell took them so long, though in fairness it is a question that we ask on this podcast almost as often as we ask what could possibly go wrong, AWS has announced that, believe it or not, newly created instances of S3 online storage will be secured by default.  What a concept.  You create a new S3 bucket, and it's going to be secure by default, rather than insecure.



I haven't seen this mentioned anywhere in the press yet, but I received a notice directly from Amazon because I'm an AWS S3 subscriber.  I use AWS as sort of my master cloud cold storage archive.  By that I mean that I rarely transact with it.  For example, I have a huge amount of static data sitting there.  Many years ago I ripped and stored my lifelong collection of prized audio CDs.  For safety, the uncompressed wave files are stored in multiple locations, and one of those locations is S3.  Every month I receive Amazon's storage bill, and I just shake my head since it's like $2.53 because most of what Amazon charges for is bandwidth usage, and mine is zero.  So S3 for like that kind of offsite glacial storage is - it's a bargain.



Anyway, as a consequence of being an AWS S3 subscriber, I received some very welcome news via email last Tuesday.  Amazon wrote:  "Hello.  We are reaching out to inform you that starting in April 2023, Amazon S3 will change the default security configuration for all new S3 buckets.  For new buckets created after this date, S3's Block Public Access will be enabled."  How could somebody even write this email with a straight face?  It's just astonishing.  Fair warning, starting in April '23 we're going to turn on security.  Wow.  Block Public Access will be enabled.  And, they said, "and S3 access control lists will be disabled," meaning that, you know, rather than having granular control over what's what, we're just going to block all public access so you don't need an ACL.



They said:  "The majority of S3 use cases do not need public access or ACLs."  Yeah, like I said, what the hell took them so long?  They said:  "For most customers, no action is required.  If you have use cases for public bucket access or the use of ACLs, you can disable Block Public Access or enable ACLs after you create an S3 bucket.  In these cases, you may need to update automation scripts, CloudFormation templates, or other infrastructure configuration tools to configure these settings for you.  To learn more, read the AWS News blog [link 1] and What's New announcement [link 2] on this change, or visit our user guide for S3 Block Public Access [link 3], the S3 Object Ownership to disable ACLs [link 4].  Also, see our user guide for AWS CloudFormation on these settings [links 5 and 6].  If you have any questions or concerns, please reach out to AWS Support [link 7]."



And for anyone who is an AWS user, did not perhaps receive this email, I have all the links there, all seven of them, in the show notes.  And, wow, when you think back over the hundreds, it must be, okay, we're Episode - okay, maybe not hundreds.  We're at 910.  We don't talk about AWS insecurity one out of every nine episodes.  But okay, many, the many times we've talked about AWS being exposed and compromised on the Internet, presumably without its owner's knowledge, the fact that Block Public Access had never been active by default for a cloud storage provider just boggles the mind. 



LEO:  We kind of knew that, though; right? 



STEVE:  Oh, my god.



LEO:  I mean, it happens so often that it's obvious that's the default.



STEVE:  Yes, right.  It's the tyranny of the default.  It's like users just assume that Amazon would not do this.  And until now, they have been.  So again, props to them, but wow.  What took you so long?



Okay.  So more anti-Chinese camera removals.  The article appearing in The Australian last Thursday was titled "Chinese surveillance cameras in our halls of power."  And it's apparently intended to induce concern because the article begins:  "Almost 1,000" - oh my god.  "Almost 1,000 Chinese Communist Party-linked surveillance cameras and other recording devices, some banned in the U.S. and Britain, have been installed across Australian government buildings, leading to calls for their urgent removal amid fears data could be fed back to Beijing.



"Government departments and agencies have revealed at least 913 cameras, intercoms, electronic entry systems, and video recorders developed and manufactured by controversial Chinese companies Hikvision and Dahua are operating across 250 sites, including in buildings occupied by sensitive agencies such as Defense, Foreign Affairs, and the Attorney-General's Department.  Australia's Five Eyes and AUKUS partners in Washington and London moved together in November to ban or restrict the installation of devices supplied by the two companies, which are both part-owned by the Chinese Communist Party."



Okay.  As we've previously covered, the U.S. and the UK did take similar steps to remove the cameras produced by those two companies from their respective government networks.  And yes, as we all know, in a closed-design, closed-source world, it is possible for such devices to get up to some mischief.  And I suppose that in some settings a stream of encrypted communications flowing across a government network might go unnoticed.  It probably wouldn't surprise anyone.  But more worrisome is that such a device might be a launching and jumping-off point for malware that's waiting to spring.



As global political tensions rise, and as more and more of the physical world is subsumed by the cyber world, cyber protectionism seems inevitable.  And we're certainly seeing its rise now.  Since anything can be buried and hidden in the most innocent-looking chip, there isn't any real defense against the "But what if?" question because "What if?" could be a true potential threat.  If a competent technologist was to testify in front of a congressional committee on international cyber threats, and was asked if they could absolutely positively assure that some device has no ability to do something malicious, they would have to answer "No, Senator."  Because, you know, it's possible.  You know?



And so, I mean, we're sort of stuck in this conundrum.  I mean, I do think that there is no alternative than for vetted local technology companies to produce the technology that is being used by that local government, if the government wants as much assurance as possible that there's no hanky-panky in any of the technology that they're using.  I mean, there just isn't any way around it.  I mean, and there's that paranoia factor.  Leo, how many years have we - there was that NSA key that people thought was in Windows.  It was like, no, it's just it used the three letters N, S, and A, adjacently.  And so that upset everybody.  And besides, if the NSA had a key, they wouldn't put their name on it.  They would name it something else.



So, wow.  You know, again, you can't prove a negative; right?  So there's no reason to believe that any Chinese camera has ever misbehaved.  But suddenly everyone's looking up at them, you know, oh, gee, is the Chinese Communist Party spying on me through this camera?  Probably not.  But, you know, it could be.  So can't have that anymore.



LEO:  You'd need a lot of people monitoring those feeds.



STEVE:  Yeah.



LEO:  I mean, that's a lot of work.



STEVE:  That's a lot.



LEO:  You could have an AI monitor it, maybe, and looking for some kinds of things.  I don't know.



STEVE:  Yeah.  Well, and again, if anyone ever spotted the traffic flow from one of those, it would be game over.



LEO:  You'd know.  Yeah, you'd know.



STEVE:  Yes.  It would be - that's the end of it.  So, you know, it's never happened.  Microsoft, believe it or not, is going to embed Adobe's Acrobat PDF reader into Edge.  They announced that it would embed Acrobat's PDF engine into future versions of their Edge browser.  It'll be taking place next month, in March of 2023, for Edge versions on both Windows 10 and Windows 11.  The FAQ that Microsoft created for this announcement, to accompany the announcement, was unsure about when Edge for macOS would receive the same PDF reader.  They just said:  "Availability for MacOS is coming in the future.  We will have more to share at a later date."



So since the current Edge PDF engine will be removed on March 31st, 2024, meaning a year from now, I guess both PDF engines will be cohabitating during this changeover period, and presumably users will have a choice.  And, you know, they were saying Adobe Acrobat PDF Reader is the original.  Nothing does as good a job.  Nothing renders fonts as accurately.  So we're just going to use theirs rather than, who knows, you probably - what's in there now is probably some descendant from Ghostscript or some, you know, I guess it's open source, right, because the Chromium engine is able to render PDFs.  So anyway, they decided to go proprietary and use the real Acrobat.  So okay.



Okay, now, I've got some feedback from our listeners.  And to everyone's credit, I think without exception, everybody was self-conscious about still wanting to talk about passwords.



LEO:  Wow.



STEVE:  Because last week I said, okay, we're done with this topic.  Well, except we're not because actually there were some interesting other twists that people had.



LEO:  It's probably the single most widely used security tool; right?



STEVE:  Yes.  And that was my thinking is it makes sense that people - that it's of great interest to our listeners who are techie and are willing to, you know, really think about these things.  And yes, you're right, Leo, it is the one thing that everybody - it's everybody's collision with security is their password.  So Alim, whose handle is @dutchphyscst, he said:  "Hi Steve.  Hope that all is going well for you.  I presume that you may not want to get back to the passwords topic again."  And as I said, everybody sort of opened with that.



He said:  "However, I am curious of an aspect with the one-time passwords.  They are based on a pre-shared key.  You've discussed plenty about the mobile app options and et cetera for storing those keys on the client side.  However, I've not seen much discussions about the protection of those pre-shared keys on the server side.  Are they protected in HSMs" - hardware security modules - "or some other mechanisms?  I also have not heard of any breaches of those pre-shared keys.  Maybe I'm being too paranoid, but I would appreciate your view on this.  Best, Alim."



So that's a great point.  What one-time passcodes were designed to prevent was the capture and reuse of a static password.  In the recent so-called credential-stuffing attacks that we were talking about, where that large database, was it billions?, of previously stolen usernames and password combinations has been shown to be surprisingly effective due to people still reusing "their" password.  Like, oh, this is my password, and I use it everywhere, at multiple websites.  So the success of this attempted reuse is blocked when, despite reusing a password, the site also requires the use of a time-varying six-digit secret passcode.



The problem with these passcodes is, as Alim noted, that this still requires that every website keeps the shared secret, secret.  And this has historically been shown to be surprisingly difficult for websites to do.  The inherent weakness of the one-time-passcode system is that it uses symmetric cryptography where each end shares the same secret.  Another way of saying this is that the same secret is used to both create the passcode and to verify the passcode.  And that's the weakness.



What makes, for example, SQRL and WebAuthn different is that by using asymmetric cryptography, each party in the system uses a different key, and the roles of the keys are different.  The public key that's held by the website can only be used to verify an identity assertion created by using the user's private key.  As I often said, SQRL gives websites no secrets to keep, and WebAuthn is the same.  By the use of asymmetric public key crypto, these systems, SQRL and WebAuthn, only provide websites with a user's public key, which does not need to be kept secret since the only thing it can be used for is to verify a user's identity claim.



By comparison, a stolen symmetric key can also be used to assert a user's identity.  If a website were to lose their one-time passcode symmetric keys, the bad guys could use those stolen keys to recreate the user's one-time passcode to spoof their identity and log in as them because the system is symmetric.  But if bad guys were to steal a user's public authentication key from a website, using WebAuthn, it would be of zero use to them since the only thing those keys can be used for is to verify a user's identity claim.  That's a huge difference.  Once this improved system, the asymmetric system, is universally deployed, hopefully it will be someday, online identity authentication will be significantly improved.



Until then, we will continue to see escalations in attack cleverness.  With the increased use of one-time passcodes we've seen the bad guys circumvent its protections with the increased use of proxying attacks, where the user is visiting a spoofed intermediate page which prompts for, receives, and forwards the user's one-time passcode on the fly, thus successfully accomplishing a real-time bypass of one-time passcode security.  Anyway, great question.



James Housley said:  "When I first switched from LastPass to Bitwarden, I also decided to keep using OTP Auth for my one-time codes.  While listening to SN-909, I wondered if Bitwarden might be a better choice for some because it would only have a one-time code for the domain it is registered to, the same way a password manager prevents putting a password into a spoofed website."



LEO:  Oh.



STEVE:  Uh-huh.



LEO:  That's interesting.



STEVE:  That is really a good point which had not occurred to me before.  So thank you for that, James.  So, okay, now we have a dilemma.  Giving your one-time passcode secrets to a password manager to use risks exposure of those secrets since they're no longer in a disconnected offline device.  You get the benefit of cross-device synchronization.  That's useful.  But again, the secrets need to be kept secret.  But as James points out, the advantage of using a strict URL-matching deployment of those one-time passcodes is that, unlike an unwitting user, the password manager would not be spoofed by a lookalike domain name.



Okay.  But since we assume that the password manager's anti-spoofing protection would first apply to its not gratuitously filling out the username and password on a spoofed site, we don't really also need it not to fill out our one-time passcode.  It already protects us from divulging the first phase of our credentials.  So on balance, while I think that's certainly worth observing that, I think that keeping one-time passcodes separate still provides optimal security.  But, you know, definitely an interesting thought experiment.



Okay.  Our VMware listener, asking for anonymity, sent to me:  "Hi Steve.  I'm a long-time listener of Security Now!, and now work at VMware on ESXi.  Naturally, I just wanted to add a couple of comments on the most recent ESXiArgs discussion.  Overall I thought the discussion was good, but I was a bit taken aback by the suggestion that VMware scan the Internet for exposed ESXi servers.  You've never suggested such a thing for any other vendor, even Microsoft or QNAP earlier in the episode.



"It definitely seems like it could work on a voluntary opt-in basis, but otherwise it seems untenable to me.  It's been talked about quite a bit in the past in various contexts, but it's rather difficult at times to figure out who to contact if a vulnerable host is found.  While I agree in theory that it would be nice if there was a quick and easy way to contact system admins of a particular system that's directly exposed to the Internet, it's quite difficult in practice, especially in a way that doesn't have far-reaching privacy implications.



"Furthermore, there is a free version of ESXi which is popular with hobbyists and those just wanting to run VMs on a single server.  ESXi is not just used by enterprises and IT professionals.  On a brighter note," he said, "I'm happy to report that starting in ESXi 8.0, all daemons and long-running processes are sandboxed by default, and we've also added additional hardening to make it harder to run ELF binaries that don't come from the installed base system packages."



So it's very cool to hear from someone at VMware.  And as we note, he's right that I haven't previously been suggesting that all vendors proactively scan the Internet looking for vulnerable versions of their own software.  But I truly think that this is something that we need to think about in the future.  I have mentioned on several occasions that there is little doubt that malicious actors and likely state-level agencies are already scanning the Internet to create quick reaction databases of what is where so that, when a high-profile vulnerability is found, targeted attacks can be rapidly deployed.  Why should only the bad guys have such databases?



Once upon a time, Internet scanning itself was considered a hostile act.  When I first created the ShieldsUP! system, which was 24 years ago in September of 1999  and Leo, you were doing Screen Savers at TechTV at the time.



LEO:  That's right, yup.



STEVE:  I was periodically contacted by various network administrators who were wondering why my IPs were probing their networks.  I explained that it was their own users inside their networks who were requesting that I check on the state of their network's security.  In most cases back then, they politely asked that I not honor such requests, so ShieldsUP! has always had a "do not scan" list of blocked IPs which it refuses to probe.  And if someone tries to do that from in such a network, it just pops up a notice and says "Your administrators, the administrators of your network, have requested that ShieldsUP! not probe their networks, so go talk to them."



Okay.  But if such a system, like ShieldsUP!, were to be launched today, 24 years later, not a single peep would be heard from anyone.  The network admins of the world have all collectively given up on the entire idea of identifying all of the random crap, or really any of the random crap, that's now flying across the Internet.  I mean, you know, there are still instances of Code Red and Nimda-infected Windows NT servers sending out packets, you know, looking for a vulnerable system.  And it was in acknowledgement of this that I eventually coined the term "Internet Background Radiation."  Oh, gee.  A neutrino just whizzed through my body.  Where did it come from?  Well, who knows, and who cares?



LEO:  A Nimda neutrino.



STEVE:  That's right, a Nimda neutrino.  So my point is, scanning the Internet was once unusual and attention-getting.  It is no longer.  And we've recently been talking about the moves we're seeing.  You have multiple governments beginning to take the security of their own nation's networks into their own hands.  So our VMware listener is correct that I have not been suggesting that other private vendors should be doing this.  But  something needs to change.  You know, it would be amazing and wonderful if QNAP were to maintain a list of publicly exposed instances of their always buggy systems.  At the same time, our listener brings up actually the most important point, which is, okay, what then?  How do they contact the owners of the system that's publicly exposed?  Bad guys don't need to contact anyone since they want to attack the systems.  But good guys do need to contact someone since they want to remediate the trouble.



The good news is our VMware listener said:  "It's been talked about quite a bit in the past in various contexts, but it's rather difficult at times to figure out who to contact if a vulnerable host is found."  So I guess the good news is the idea is sort of in the air.  But he's also correct about the privacy implications, which follow from any attempt to somehow make endpoints identifiable to everyone on the public Internet.  Right?  Because they would have to be everybody or no one.  So we're left with the conundrum which is created by the asymmetry of the fact that bad guys want to attack exposed systems, while good guys only want to inform them.  And I am thankful that our VMware listener spoke up.  Thank you.  And I don't think there's a good answer right now.  But the way things are going, it seems like we need one.



Brad Jones said:  "Steve, I know we're all tired of talking about the security of our passwords, but I'm interested in your thoughts on the following.  Selecting a known Bitwarden master password that is already considered somewhat secure, then running it through something like a Base64 calculator to generate an actual password used to unlock the vault.  As you would likely never remember the password, anytime you need to unlock you would run the known password through the calculator to generate the vault password.  For example, although clearly you would never use Password12345678, that would generate the password you would use as your master password which is" - and then he has the Base64 conversion - UGFzc3dvcmQx and blah blah blah blah.  You know, gibberish.  He says:  "There are many desktop and mobile applications that can complete this calculation without running the known password through an online encoder or decoder."



Okay.  So algorithmic password generators are an interesting idea.  The concept was to use an HMAC function which is essentially a keyed hash function.  So the user would generate a single secret permanent key which would key the hash function.  They would then enter, for example, the domain they're visiting into the hash function, and it would output an absolutely maximum entropy password.  Since every domain would produce a different unique password, there would be no password reuse, and there would also be nothing to remember per domain since any domain's password could be recreated on demand from the domain name itself.



And actually this was part of the - this was the germ of the idea of how SQRL converts domain names into private keys which are then turned into public keys.  And even before that, this idea intrigued me so much that I wondered whether it might be possible to create a truly secure paper-based encryption system.  Our long-time listeners may recall that the idea I hit upon and then developed was to traverse a per-user customized Latin Square.  I called the system "Off The Grid" since the passwords came from a grid, and the system was offline and used no electricity.



So anyway, Brad's idea is right, to suggest yet another means of generating unique per-site passwords that could be deployed using an algorithmic system based on something that could be input.  And if you used a secret key and a hash function, you could just use the domain as a place to start.



Okay.  Leo, our final break, and then I am going to introduce our listeners to something very new and very cool, Ascon, which has just received NIST standards endorsement.



LEO:  Ooh, okay.



STEVE:  It is a new NIST standard for cryptography.



LEO:  All right.  Careful what you're asking for.  You might get it.



STEVE:  Ask not what you can do for your crypto.



LEO:  Your country.  All right.  I'm ready to - I'm asking for a friend.  What is Ascon?



STEVE:  Okay.  So last Wednesday the U.S. National Institute of Standards and Technology, you know, our NIST, announced that a family of authenticated encryption and hashing algorithms known collectively as Ascon, A-S-C-O-N, will be standardized for applications in lightweight cryptography.  And lightweight cryptography does not mean less secure, as we'll see.  So I'll have a little bit more to say about that in a second.



This final selection was the result of a four-year competition which ran from 2019 through just now, 2023, among competing proposals.  This was the same process that led us to the selection of the Rijndael cipher to become the official AES standard.  So that's, you know, Rijndael is military strength, if you'll pardon the expression.  Ascon has now become the standard for providing high 128-bit security for lower end devices.  And for example, there's an entire suite of symmetric cryptographic Ascon functions for the Arduino already exists.



So NIST summarized the competitive selection process by writing:  "NIST has initiated a process to solicit, evaluate, and standardize lightweight cryptographic algorithms that are suitable for use in constrained environments, where the performance of current NIST cryptographic standards is not acceptable.  In August 2018, NIST published a call for algorithms to be considered for lightweight cryptographic standards with authenticated encryption with associated data.  That's Authenticated Encryption Associated Data, AEAD - we'll get to more of that in a second - and optional hashing functionalities.  The deadline for submitting algorithms has passed.  NIST received 57 submissions to be considered for standardization.  After the initial review of the submissions, 56 were selected as first-round candidates.  Of the 56 first-round candidates, 32 were selected to advance to Round 2."  So they were whittling them down.



Okay.  So what exactly does "lightweight crypto" mean?  Rather than aiming for the highest conceivably needed security which would be adequate to protect data for the next several decades at least, NIST's Lightweight Cryptography competition was set up to select the contemporary crypto system that would be best suited for deployment on today's much more limited IoT systems.  This means that, for example, while sharing the same 128-bit block length as the Rijndael AES cipher, because a cipher's block length really cannot be reduced much below 128 bits without compromising security, the key lengths can safely be reduced by half to 128 bits.  This allowed the use of a family of faster, more efficient, and easier to implement algorithms which can run efficiently on much lighter weight hardware.



Okay, now, some have wondered whether dropping the key length in half to 128 bits is sufficient for contemporary security.  The fact is, it's still insanely plenty of security.  We only went to 256-bit keys because why not, if we have desktop and server machines and there's virtually, on those platforms, virtually no cost for doubling the key length.  On the other hand, those are expensive algorithms, which is why Intel has now specific AES instructions because some of the things that AES requires the processor to do are extremely time-consuming.  So Intel said, okay, let's put some instructions to speed up AES into our microcode.  And they've been there now for a while.  Well, if AES weren't hard to do, Intel would have never done cipher-specific instructions.  Ascon doesn't need any of that, as an example.  So the point is we did 256 bits because we could.



So let's examine the implications first of 128 bits.  2^128, right, the number of combinations, the number of possible keys, that's 3.4 x 10^38.  So that's 34 followed by 37 zeroes.  Okay, now, say that we want to brute force the key's value.  And for the sake of argument let's say that it would be possible - it's not.  But say that it would be possible to completely test a candidate key at the full clock rate, say 3.4 GHz, of a GPU.  It's not possible to actually do that since a GPU, fast as they are, still requires many clock cycles to get any work done.  But for the sake of this thought experiment, say that it was possible to fully test one 256-bit key every clock cycle at 3.4 gigatests per second.  Okay, so that's 3.4 x 10^9, right, 3.4 GHz, gigatests per second.  Brute force tests per second.  Each clock cycle we're doing a brute force test.  We can't, but for the argument let's say we could.  



To test all possible 128-bit keys, where there are 3.4 x 10^38 possible keys, we divide 3.4 x 10^38 by 3.4 gigatests per second, that's 3.4 x 10^9.  So we just subtract 9 from 38, yielding 29.  This tells us that testing candidate keys, 128-bit candidate keys at the rate of 3.4 billion tests per second, which no actual system is even capable of coming close to doing, we would need 10^29 seconds to brute force all combinations.



Okay.  Now, there are only 31.5 million seconds per year.  If we triple that, it brings us to 94.5 million, which is close to 100 million.  So we'll round that up to 10^8, which is 100 million.  So 8 from 29 leaves 21.  Thus brute forcing a 128-bit secret key at 3.4 billion guesses per second, which is not possible, would require roughly 3 x 10^21 years.  Given that we all generally live less than 10^2 years, we're left with plenty of security margin.



In other words, 128-bit security, although we're so used to talking about thousands of bits for asymmetric key strength and 256 bits or more for AES and Rijndael, 128-bit security is an insanely ample amount of security.  And now, thanks to this device-constrained cryptographic competition, we have this super-strong security available to lightweight embedded processors which don't need any special instructions because it's all simple.



NIST's original call for submissions explained their target, their need as follows.  They said:  "The deployment of small computing devices such as RFID tags, industrial controllers, sensor nodes, and smart cards is becoming much more common.  The shift from desktop computers to small devices brings a wide range of new security and privacy concerns.  In many conventional cryptographic standards, the tradeoff between security, performance, and resource requirements was optimized for desktop and server environments, and this makes them difficult or impossible to implement in resource-constrained devices.  When they can be implemented, their performance may not be acceptable.



"Lightweight cryptography is a subfield of cryptography that aims to provide solutions tailored for resource-constrained devices.  There's been a significant amount of work done by the academic community related to lightweight cryptography.  This includes efficient implementations of conventional cryptography standards, and the design and analysis of new lightweight primitives and protocols."



Okay.  So the winner was Ascon, a family of authenticated encryption and hashing algorithms which are not only designed to be lightweight and easy to implement, but also with added countermeasures against side-channel attacks.  And Ascon was not only selected to be NIST's new lightweight crypto standard, but it also previously won the same spot in the CAESAR competition which ran for five years from 2014 through 2019.  CAESAR (C-A-E-S-A-R) is the acronym for "Competition for Authenticated Encryption: Security, Applicability, and Robustness."  And yes, that's clearly one of those reverse-engineered acronyms that's always a bit awkward.  So Ascon is also CAESAR's primary choice for lightweight authenticated encryption.



And of course during the NIST competition it was already obvious that Ascon had won the CAESAR competition since that closed in 2019.  So really the question was is there anything more that we know now, subsequent to the CAESAR competition, that is better than Ascon.  And the answer?  No.  There were more than 50 submissions of other things.  And Ascon was it.  And I've mentioned authenticated encryption here.  We've talked about the need in the past for both encrypting the data for privacy and also authenticating the data to prevent its manipulation at any time.  And assuming that the two steps of encryption and authentication are separate, as they were originally, we've examined the question of in which order the two operations should be accomplished.



The universal agreement is that you first encrypt, and then you apply authentication to the encrypted result.  The reason is, on the receiving decrypting end, the message is first authenticated to assure it has not been tampered with, not modified in any way, and only if it authenticates should the message then be decrypted.  This prevents the use of some very subtle attacks that we discussed years ago.



But the use of so-called "authenticated encryption" offers a better solution.  As its name implies, authenticated encryption algorithms are able to perform both operations at the same time.  And there's one very valuable extension to this known as Authenticated Encryption and Associated Data, or AEAD for short.  AEAD algorithms allow the so-called "associated data" to remain unencrypted and in the clear while still being encapsulated within the authentication envelope.  So you can see it, but you can't change it.



I have some experience using these algorithms, since SQRL's local on-disk identity storage format needed to have some local storage which would always be left, well, some that would be kept encrypted on the disk, but also some other that, for example, the user's settings, which would need to be available all the time even without the user's key.  And the whole thing, the entire package needed to be tamper proof.  So I chose to use the AES-GCM cipher, which is an AEAD cipher.  This perfectly resolved the need for having both secret and non-secret data that could not be altered without breaking the authentication of the whole.



So in any event, what we're getting is in this lightweight competition is an AEAD cipher which is going to be extremely useful.  It's what NIST wanted, and it's the contest-winning technology that Ascon provides to IoT devices.  It also brings and provides similar features to hashing because Ascon did also offer a hashing technology and actually a lot more.  It was designed by a team of cryptographers from the Graz University of Technology, Infineon Technologies, Lamarr Security Research, and Radboud University.



So its bullet points, that is, of its benefits are authenticated encryption and hashing with fixed or variable output length with a single lightweight permutation.  And I'll be talking about this permutation in a second.  Provably secure mode with keyed finalization for additional robustness.  Easy to implement in hardware and software.  Lightweight for constrained devices.  It's got small state, meaning not much storage needed, a simple permutation box, and robust mode.  Fast in hardware.  Fast in software.



Pipelinable, bit-sliced, it has a bit-sliced 5-bit S-box for 64-bit architectures.  Scalable for more conservative security or higher throughput.  Timing resistance; no table look-ups or additions.  Side-channel resistance; the S-box is optimized for countermeasures.  The key size equals the tag size equals the security level, that is, they're all 128 bits.  Minimal overhead; the ciphertext length equals the plaintext length so there's no padding needed.  Single-pass, online encryption and decryption, and it uses a nonce.



So the NIST requirements were for a key size no shorter than 128 bits, and there could be a family of variants having different parameter lengths.  The design targets were interesting.  NIST wrote:  "Submitted AEAD algorithms and optional hash function algorithms should perform significantly better in constrained environments" - meaning hardware and embedded software platforms - "compared to current NIST standards.  They should be optimized to be efficient for short messages, as short as 8 bytes.  Compact hardware implementations and embedded software implementations with low RAM and ROM usage should be possible.



"The performance on ASIC (Application Specific Integrated Circuits) and FPGA should consider a wide range of standard cell libraries.  The algorithms should be flexible to support various implementation strategies - low energy, low power, low latency.  The performance on microcontrollers should consider a wide range of 8-bit, 16-bit, and 32-bit microcontroller architectures.  For algorithms that have a key, the preprocessing of a key in terms of computation time and memory footprint should be efficient.



"The implementations of the AEAD algorithms and the optional hash function algorithms should lend themselves to countermeasures against various side-channel attacks, including timing attacks, simple and differential power analysis, and simple and differential electromagnetic analysis.  Designs may make tradeoffs between various performance requirements.  A submission is allowed to prioritize certain performance requirements over others.  To satisfy the stringent limitations of some constrained environments, it may not be possible to meet all performance requirements stated in the previous paragraph.  The submission document should, however, explain the bottlenecks that were identified and the tradeoffs that were made."  And basically no tradeoffs were necessary.  Ascon does it all.



So we now have a new family of power, memory, and general resource-efficient algorithms that have been studied and pounded on, now for years, by many academics and cryptographers.  GitHub offers implementations in C, C with some assembly, Python, Java, and RUST.  And there are RISC-V implementations, as well.  I have links to everything in the show notes.



GitHub also has an Ascon Suite of cryptographic functions all based on what's known as, and I mentioned this before, the Ascon permutation.  The permutation is the core cryptographic function at the heart of Ascon, highly efficient, super secure, now proven.  That GitHub page provides the open source code for the following functions.  We've got, of course, Authenticated Encryption with Associated Data.  We've got hashing.  We've got a pseudorandom function, a Message Authentication Code, an HMAC-based Key Derivation Function, Hashed Message Authentication Code (HMAC).  We have AEAD with side-channel protections.  We've got a Keyed Message Authentication Code (KMAC), Password-Based Key Derivation Functions (PBKDF), a pseudorandom number generator, synthetic initialization vector, extensible output functions, and direct access to the ASCON permutation.



All of those are available through the API of the code here.  It's GitHub.com/rweather/ascon-suite.  And there's a related page with all of the same functions for the Arduino.  So the industry, as I said, has a robust and seriously secure set of lightweight functions, which have already been ported to many languages, platforms, and hardware, which are tuned for and suited for the needs of embedded resource-constrained IoT-style devices.  And I obviously already made the case that 128-bit key length, that is just fine.  That'll last us our lifetime and, well, what, 10^18 times more.



LEO:  So why not just replace AES?  Why not replace Rijndael with this?



STEVE:  You really could.  I mean, I think they went high.  Rijndael is available in 256-bit, 384-bit, and 512-bit key lengths.  So you could just go insane with Rijndael.  I agree.



LEO:  Do you need to, I guess is the question.  And even if you went to 512, would it survive a quantum computer?



STEVE:  Well, this is all symmetric.



LEO:  Oh, it doesn't matter.



STEVE:  And quantum computers do not attack symmetric ciphers at all.



LEO:  Right, right, okay.  



STEVE:  Yeah.  So if I have any need in the future, I'll be using these because Ascon, it's been really picked over now for many years.  And you can imagine that the 50 other submissions, they were trying to find a problem with Ascon in order to have theirs chosen, but they couldn't.



LEO:  I think, you know, this would be good for PGP to replace, I think they're using Triple-DES for the symmetric key.



STEVE:  Yeah.



LEO:  It would be a nice improvement.  Not that speed is a big issue.  But still, just something that's easier to implement maybe.  It's interesting.  Ascon.  Huh.



STEVE:  We have it now.



LEO:  We've got it.  Thanks to rweather, whoever that is.  Is rweather a person, or is it...



STEVE:  I didn't look to see.  But basically you're taking the core innovation, which is this permutation box, which is well defined, and then you're building all these other functions around it.  And we sort of know how to do that now.



LEO:  I have a feeling rweather's a person who's just really good at this stuff.



STEVE:  Might be.  He may have just done an implementation of these functions.



LEO:  Yeah, yeah.  Interesting.  Wow.  You know what, this is great.  You learn so much listening to this show.  And you stay up to date.  And I know that's why so many people in cybersecurity and IT listen to Security Now!, and we're glad you do.  I hope you'll keep listening.



Copyright (c) 2023 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.






GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#911

DATE:		February 21, 2023

TITLE:		A Clever Regurgitator

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-911.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  For how long were bad guys inside GoDaddy's networks?  What important oral arguments is the U.S. Supreme Court heading today and tomorrow?  What's Elon done now?  What's Bitwarden's welcome news?  What's Meta going to begin charging for?  Should we abandon all hope for unattended IoT devices?  Are all of our repositories infested with malware?  How did last Tuesday's monthly patch fest turn out?  Why would anyone sandbox an image?  What can you learn from TikTok that upsets Hyundai and KIA?  And are there any limits to what ChatGPT can do, if any?  We're going to find out by the end of today's 911 emergency podcast.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  We'll answer the musical question, how long were bad guys inside GoDaddy's network?  We've got some good news for our sponsor Bitwarden and its customers.  And then he's going to talk about ChatGPT.  How useful would ChatGPT be at detecting malware?  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 911, recorded Tuesday, February 21st, 2023:  A Clever Regurgitator.



It's time for Security Now!, the show where we get together and talk about security.  Right now.  Steve Gibson is here.  Hello, Steve.



STEVE GIBSON:  You think that's how we came up with the name, Leo?  I think that might have been.



LEO:  I don't know.  I don't know.  Well, better than Security Yesterday!.



STEVE:  Oh, yeah.  That's, you know, that's has-been security.



LEO:  Nobody cares about that.



STEVE:  We don't want that.



LEO:  Yeah.



STEVE:  No.  So we're here to answer some questions, as we've been doing so far this year.  One is how long were bad guys inside GoDaddy's networks?  What important oral arguments is the U.S. Supreme Court hearing today and tomorrow?  What has Elon done now?  What's Bitwarden's welcome news?  What's Meta going to begin charging for?  Should we abandon all hope for unattended IoT devices?  Are all of our repositories infested with malware?  How did last Tuesday's monthly patch fest go, anyway?  Why would anyone sandbox an image?  What can you learn from TikTok that upsets Hyundai and KIA?  And are there any limits to what ChatGPT can do, if any?  We're going to find out by the end of today's 911 emergency podcast.



LEO:  I'm going to give you the short version so you don't have to listen to the whole thing:  A long time.  Gonzalez v. Google.  Tweet.  Argon2.  Verification.  Yes.  No.  Yes.  Yes.  How about that?



STEVE:  Very nice.



LEO:  Very good.  We will get to the actual answers.



STEVE:  We'll see you next week.



LEO:  If only it were that simple.  All right.  Picture of the Week.



STEVE:  So today's Picture of the Week, or this week's Picture of the Week, was actually taken by one of our listeners who was up in the attic of some sort of charitable organization, maybe his church.  I don't quite remember now what he said.  But this was a - he was working on fixing their Dish Network installation.  And when he saw the ground wire attached to a nail that was nailed into some wood, he thought, okay, I've got to take a picture of this and share it with the Security Now! audience because here we have another weak understanding of the goal of grounding.



LEO:  Where does the other wire go?



STEVE:  It's not clear.  It wanders off somewhere.  And what occurred to me was that maybe whoever it is who installed this thought maybe that the electrons would pay attention to the color of the insulation.



LEO:  It's green.



STEVE:  Because, you know, if they realized that it was a green wire - traditionally in electronics, electricity, you know, green is ground.  So they go, oh.  Everybody over this way.  Of course the problem is when they get over to the nail, which is stuck into some wood, wood is a very good insulator.  So it's a little bit like sticking the wire into that pail of dirt, which is one of our all-time favorite pictures.  So anyway, thank you very much to our listener Mark for thinking of us when he thought, what's wrong with this Dish Network installation?  Oh.



LEO:  Don't you love it?  When they see something like this, they think of you immediately; right?  Send it to Steve.



STEVE:  Okay.  So I titled this one "GoneDaddy."  Last Friday GoDaddy revealed a rather astonishing bit of news.  Its network and organization had suffered a multi-year security compromise that had allowed attackers who to this day remain unidentified to exfiltrate the company's source code, customer and employee login credentials, and install their own malware which redirected customers' websites to malicious sites.



LEO:  For years.



STEVE:  Years, yes.



LEO:  Years.



STEVE:  So, you know, they're big; right?  They have got nearly 21 million customers.



LEO:  They're the number one registrar in the world.  They're huge.



STEVE:  Their last year revenue was nearly $4 billion.  So many years ago, when I was making my move away from Network Solutions, I gave GoDaddy some consideration.  It is the choice of a very techie friend of mine, whom we both know, Mark Thompson, maybe because he's in Arizona, and I think that's where they're based also.  But for me it just looked too bubblegum...



LEO:  They're terrible, yeah.



STEVE:  ...and commercial.  You know?



LEO:  I'm not surprised to hear this.



STEVE:  Yeah.



LEO:  We buy our certs from them because their cert prices are so cheap for the EV certs.



STEVE:  Right.



LEO:  But, I mean, that's a cert.  That doesn't, you know, that's our security, not theirs.



STEVE:  Yeah.  So anyway, what I want from my domain registrar is staid, stodgy, and stoic.



LEO:  Yes.



STEVE:  I don't want a domain registrar that looks like Romper Room.  And as I was putting that in the show notes, I thought, I wonder how many of our listeners will relate to Romper Room?



LEO:  Oh, yeah.



STEVE:  You know, I think I'm beginning to date myself here a little bit.



LEO:  I see Stevie.  And I see Lorrie.  I used to know Miss Nancy, our local Romper Room lady, actually.



STEVE:  So anyway, from a registrar I don't want entertainment and upselling.  I just want something solid.  Anyway, as we know, I chose Hover, and I've been very happy.  And just to be clear, my choice was made years before Hover became a TWiT sponsor.  So it wasn't like, you know, it wasn't after the fact.  So in a filing Thursday, last Thursday, with the SEC, you know, our U.S. Securities and Exchange Commission, GoDaddy admitted that three serious security events, the first occurring three years ago in 2020, and the way they put it, you know, somehow lasting through 2022, were all carried out by the same intruder.



Now, okay.  But they're also saying, but we don't know who.  But we know it's the same.  So I'm like, what?  Anyway, they wrote:  "Based on our investigation, we believe these incidents are part of a multi-year campaign by a sophisticated threat actor group that, among other things, installed malware on our systems and obtained pieces of code relating to some services within GoDaddy."  And they said that their investigation was still ongoing.



The most recent event occurred last December, so just three months ago, when the threat actor gained access to the hosting servers GoDaddy's customers use to manage websites hosted by GoDaddy.  They got into their cPanel hosting servers.  The threat actor installed malware on the servers that "intermittently redirected random customer websites to malicious sites."  Because, you know, that's what you want from your registrar.  GoDaddy was unaware of the presence of this malware and learned of it from their customers, who were complaining that visitors to their sites were occasionally being redirected elsewhere.



So GoDaddy said:  "We have evidence, and law enforcement has confirmed, that this incident was carried out by a sophisticated and organized group targeting hosting services like GoDaddy."  They said:  "According to information we have received, their apparent goal is to infect websites and servers with malware for phishing campaigns, malware distribution, and other malicious activities."  Now, okay.  Saying "hosting services like GoDaddy," you know, that sort of begs the question whether other hosting services have been similarly affected.  If so, you know, which ones?  And by whom?  Those questions remain unanswered.



It appears that the first of several intrusions took place in March of 2020, so fully three years ago, when a threat actor obtained login credentials that gave it access to employee accounts and the hosting accounts of roughly 28,000 of GoDaddy's customers.  Fortunately, those hosting login credentials that were obtained for the 28,000 customers did not also provide access to the customers' main GoDaddy account.  Otherwise damage would have been more severe.  That first breach was disclosed two months later in May of 2020 in a notification letter sent to the affected 28,000 customers.  The company said on Thursday it's responding, get this, responding to subpoenas related to that incident that the Federal Trade Commission issued in July 2020 and October 2021.  So there doesn't seem to be any big hurry over in GoDaddy Land to do much of anything.



Then GoDaddy discovered another incident in November of 2021, two months after the threat actor obtained a password that gave access to source code for GoDaddy's Managed WordPress service.  So beginning two months earlier in September of 2021, this unauthorized party used their access to obtain login credentials for WordPress admin accounts, FTP accounts, and email addresses for 2.1 million current and inactive - that is, previous - Managed WordPress customers at GoDaddy.



And these were not the first of GoDaddy's many problems.  Through the years, security lapses and vulnerabilities have led to a series of suspicious events involving large numbers of sites hosted by GoDaddy.  For example, back in 2019 a misconfigured domain name server at GoDaddy allowed hackers to hijack dozens of websites owned by Expedia, Yelp, Mozilla, and others and use them to publish a ransom note threatening to blow up buildings and schools.



The DNS vulnerability which was exploited by the hackers had come to light three years earlier, yet GoDaddy never took any action to mitigate the risk.  Again, this is not the registrar you want.  Also in 2019, a researcher uncovered a campaign that used hundreds of compromised GoDaddy customer accounts to create 15,000 websites that published spam promoting weight-loss products and other goods promising miraculous results.



Okay.  So pushing back from this a bit, you know, the one question I had was how it was that GoDaddy could assert, through these more recent three attacks spanning the same number of years, that they had been repeatedly plagued by a single threat actor, yet somehow have no idea who this individual or group is.  So I did a bit more digging, and I found that in their 10-K filing with the SEC they stated that the most recent December 2022 incident is connected to the two other security events they suffered in March 2020 and November 2021.  Okay.  Connected how?  



This reminded me of what we recently saw from LastPass, where we were told that the second attack, the one remember where all of our backed-up LastPass vaults were stolen, was enabled by the initial intrusion.  That was worrisome since it suggested to us that LastPass had not fully cleaned up after the first intrusion.  In the GoDaddy case, they appear to be stating that they know that it's the same threat actor because information presumably obtained during the initial intrusion, three years ago back in 2020, was subsequently used in both 2021 and 2022.



Unfortunately this suggests, as with LastPass, that post-intrusion cleanup may have been minimized.  And boy, given their track record and their apparent negligence, based on the actions that we've seen, who would be surprised by that?  But in any event, the cleanup was ineffective.  A full post-intrusion clean-up means that nothing that an intruder could possibly have obtained remains valuable once the clean-up is concluded.  We know that didn't happen in the case of LastPass, and that also appears to have been the case for GoDaddy.



You know, as we've had occasion to note on this podcast, Leo, and you and I talked about it years ago, once malware has had access to a system, you can never fully trust it again.  And I should really remove the qualifier "fully."  You cannot trust any system after it's been compromised because you just don't know what could have been done.  These days we have malware burrowing into our motherboard firmware to maintain persistence, even across wipes and completely reinstallations.  So the only course of action then is to reflash the firmware, wipe the drives, rebuild from scratch, and change everyone's access credentials.



Yes, this is a huge nightmare in the case of a large sprawling enterprise, but there's really no choice.  After GoDaddy's initial 2020 breach, either something lingered in a system that was never found, some latent advanced persistent threat presence, or they failed to rotate all of the keys and login credentials across the entire enterprise.  Something remained, either malware tucked away in an unexamined corner, or someone's credentials that were never changed.  Thus the same guys came back later for another dip, and a year later for yet another one.  Wow.



Okay.  Today and tomorrow the U.S. Supreme Court will be hearing initial oral arguments.  And Leo, in your quick summary of the podcast you properly named the first of the two cases.



LEO:  Gonzalez v. Google.  Yeah, I listened all morning.  It went on and on and on and on and on and on and on.



STEVE:  Yeah, well, those attorneys do.



LEO:  Yeah.



STEVE:  Anyway, the U.S. Supreme Court's hearing oral arguments in a pair of cases which will open the door to allow the court to reexamine the now-famous and infamous Section 230 of the Communications Decency Act, which was passed into law by Congress 27 years ago, back in 1996.  There are a crucial 26 words from Sector 230 of that law that are what enable our Internet's media companies to remain unresponsible, and some would say irresponsible, for the content that their users post online for consumption by others.



Those 26 words are:  "No provider or user of an interactive computer service shall be treated as the publisher or speaker of any information provided by another information content provider."  26 words.  And they mean, essentially, this blanket protection provides that none of today's media companies, the way this has been used to thwart any attempts at civil liability, is that none of today's media companies  can be held responsible for the content that's being served by their technologies.  Thus it serves as powerful and what has now become crucial protection for them.  But many wonder whether it might have been taken too far.



The specific question that the cases address focuses upon the content promotion algorithms used by Google, for example, for YouTube, and also Facebook, Twitter, and others, to provide their users with more relevant content.  So the question may be whether our social media companies have actually crossed the line to become publishers of this content the moment they involve themselves in that content's deliberate selection and promotion, even if that involvement is entirely algorithmic.  The argument, then, is that they're no longer acting as passive repositories of user-provided content, and that the selections made by their algorithms are ultimately motivated by profit.



There's a cybersecurity law professor, Jeff Kosseff, he's with the U.S. Naval Academy, who wrote an entire book on Section 230, titled "The Twenty-Six Words That Created the Internet."  And in some reporting by the Washington Post early last October - which is when the Supreme Court decided that they would hear the two cases which are now before them and for which they are now hearing these oral arguments today and tomorrow.  Tomorrow is about Twitter.  Today is about Google and YouTube.  They quoted Professor Kosseff, saying:  "The entire scope of Section 230 could be at stake, depending on what the Supreme Court wants to do."  And, you know, although the stakes could not be much higher, the way these things go we won't have a decision anytime soon.  Probably not till way later in the year, like toward the end of the year at the earliest.  But this will certainly be one to watch.



And for their part, the plaintiff's attorneys say that applying the sweeping civil immunities created by Section 230 to algorithmic recommendations incentivizes the promotion of harmful content, and that Section 230 denies the victims of such content any opportunity to seek redress when they can show those recommendations caused injuries or even death.  So this will be very interesting.  And I've forgotten, Leo, where you come down on 230.



LEO:  Oh, well, let me put it this way.  You like the chatroom?  You like the Discord?  You like your forums.  You like our forums.  You like our Mastodon.  If 230 is overthrown, all of those go away.



STEVE:  The end of the world as we know it.



LEO:  Yeah, all of them go away because right now I can't, and you can't, be sued for anything anybody posts on those forums.  Even if it's defamatory or whatever, they're liable for it, not you.  Which is reasonable; right?  Furthermore, thanks to Section 230, if you take something down on your forums, and because it's racist hate speech, that person can't sue you either.  And that's really important.  It's the right both to publish and to moderate and not be liable.  And because it's codified into law that way, you don't even have to go to court.  You know, a justice, the judge would immediately say no, I'm sorry, he's protected by 230.



STEVE:  Right.



LEO:  So if they strike it down or even weaken it in any way, you know, it's not Google and Facebook and Twitter who are going to suffer.  They can defend themselves.  They have lawyers by the fistful.  It's you and me.



STEVE:  Well, so in this case we're glad that Supreme Court has a conservative bias at this point in time; right?



LEO:  Well, they don't have a conservative bias.  That's a misnomer.  They're not originalist.  They just make up whatever they want and then find something to justify it.  I would be much happier if they were, yeah.  But remember this is a 1996 law.  Ron Wyden wrote it, and he was a very smart guy.  And it was while they were passing the Communications Decency Act he said, you know, this could really screw up the Internet.  We need to provide, you know, a safe harbor.



STEVE:  Protection, yeah.



LEO:  Yeah.  And so it's very, very important to the Internet.  You quoted the exact right book.  Jeff Kosseff's book is often referred to on This Week in Google.  Jeff Jarvis is a big fan of it.  I've read it.  It's a very, very good book.  And you read it and understand it.  I listened to the arguments this morning.  And you never can tell with the oral arguments in front of the Supreme Court because justices will sometimes play devil's advocate.  Their actual opinions aren't always on display.  But I was pretty encouraged by the questions they asked the counsel for the plaintiff.  And I think they get how important it is.  They even - one of the justices even said, you know, this could have a real impact on the economy.  And then Justice Kagan, who I love and was very funny, said, you know, you don't have the smartest Internet brain sitting in front of you right here, so you'd better explain it to us.  It was good.



STEVE:  So why did they even choose to take it up last October?



LEO:  They could have let the Ninth Court decision stand because it upheld the Section 230 rights.



STEVE:  Right.



LEO:  It was appealed, and you're right, that's the question is why did they take it up.  And I think, you know, there probably is some reasonable discussion around this.  What they're really battling over is not so much the right to publish or the right to moderate, but whether a recommendation algorithm is in some way now editorializing.  And at first, I'll be honest with you, when I first read the facts of the case, I said, well, you know, that's actually a good point.  You know, in a way Google's algorithm is choosing what to show.  Isn't that Google creating content?



But I've since seen the light and been persuaded by a lot of smarter people than I, including Cathy Gellis from Techdirt who we're trying to get on the show tomorrow.  She wrote an amicus brief for this.  They also allowed multiple anonymous Reddit moderators to file an amicus brief, as did the EFF.  Unfortunately, both the White House and the right, Josh Hawley and Ted Cruz, want this to be struck down, for different reasons, you know.  But the wiser heads point out that it's all algorithmic.  If you have a search engine, and you go to the search engine, what's on top of the search, unless it's completely chronological, is algorithmic.



STEVE:  It's the only reason that we all switched to Google, away from Alta Vista, when Google appeared.



LEO:  Yeah, exactly.  And the Reddit moderators say, no, we use algorithms to help us moderate.  Algorithms aren't inherently bad.  You might have an algorithm that's optimizing for profit, but as a result surfaces more controversial videos.  But that's not the same thing as writing an article saying I think ISIS is fantastic.  And so it's very risky, and I certainly hope the judges don't do this to slowly pare away at 230.  It's only, as you say, it's only 26 words.



STEVE:  Right.  And it is black and white at the moment.



LEO:  It's very clear.  It's I think one of the best written laws ever.  It's kind of like a constitutional amendment.  It's precise.  It's broad enough to have lasted 20 years, 30 years.  And but at the same time, you know, it's clear.  And I think its intent is clear.  And I'm hoping that the Court does not override what was clearly the intent of Congress when they wrote that law.



STEVE:  Yeah.



LEO:  So let's, yeah, let's cross your fingers.  I don't know if they're conservative, but let's hope they make the right choice.



STEVE:  So The Verge's headline was "It's Official:  Twitter will now charge for SMS two-factor authentication.  Only Twitter Blue subscribers will get the privilege of using the least secure form of two-factor authentication."  And they were having fun with this.  The Verge continued:  "Now it's official:  You can pay for the privilege of using Twitter's worst form of authentication.  In fact, if you don't start paying for Twitter Blue ($8 a month on Android; $11 a month on iOS) or switch your account to use a far more reliable authenticator app or physical security key, Twitter will simply turn off your two-factor authentication after March 20th."



And the writer adds, he says:  "I know which one I would choose.  Good riddance to SMS is my feeling, given how common SIM swap hacks are these days."  He says:  "Heck, Twitter's own Jack Dorsey was successfully targeted by the technique four years ago.  You don't want someone to get access to your accounts by proving they are you simply because they've stolen your phone number."



That's how Twitter is trying to justify this change, too, but I wouldn't be surprised if there's a simpler reason.  It costs money to send SMS messages, and Twitter does not have a lot of money right now.  The company had been phasing out SMS even before Elon Musk took over.  Twitter's own transparency data shows that as of December 2021, only 2.6% of Twitter users had two-factor authentication turned on, and 74% of those users were using SMS as their two-factor authentication method.



Okay.  So here's what Twitter posted and explained last Wednesday.  Their blog was titled "An update on two-factor authentication using SMS on Twitter, by Twitter, Inc.  We continue to be committed to keeping people safe and secure on Twitter, and a primary security tool we offer to keep your account secure is two-factor authentication.  Instead of only entering a password to log in, 2FA requires you to also enter a code or use a security key.  This additional step helps make sure that you, and only you, can access your account.  To date, we have offered three methods of 2FA:  text message, authentication app, and security key.



"While historically a popular form of 2FA, unfortunately we have seen phone number-based 2FA be used and abused by bad actors.  So starting today, we will no longer allow accounts to enroll in the text message/SMS method of two-factor authentication unless they are Twitter Blue subscribers.  The availability of text message 2FA for Twitter Blue may vary by country and carrier.



"Non-Twitter Blue subscribers that are already enrolled will have 30 days to disable this method and enroll in another.  After March 20th we will no longer permit non-Twitter Blue subscribers to use text messages as a two-factor authentication method.  At that time, accounts with text message two-factor authentication still enabled will have it disabled.  Disabling text message two-factor authentication does not automatically disassociate your phone number from your Twitter account.  If you would like to do so, instructions to update your account phone number are available on our Help Center."



And finally:  "We encourage non-Twitter Blue subscribers to consider using an authentication app or security key method instead.  These methods require you to have physical possession of the authentication method and are a great way to ensure your account is secure."



Okay.  So some other reporting I found stated that Twitter took this step because SMS two-factor authentication was being abused by fraudsters who would establish accounts using something called "Application to Person," or A2P, premium telephone numbers.  Then when Twitter would send two-factor authentication texts to these numbers, the fraudsters would get paid.  So it cost Twitter much more money than just a regular SMS to regular people.  Estimated losses were claimed to be around $60 million a year from this.



Okay.  So of course everyone's piling on Elon these days.  And his decisions at Twitter have been a source of controversy.  74% of 2.6% is 1.95%.  So as of the end of 2021, when we had those stats, 1.95% of all Twitter account holders were using SMS-based two-factor authentication.  On the other hand, that's three out of every four of the Twitter users who use any form of two-factor authentication were using SMS, and the use of any form of two-factor authentication certainly prevents some amount of abuse.  And even though SMS is not, we know, the best solution, it's still better than having none, and using it doesn't create any new vulnerability where none existed before.  Unless I guess you were to, like, become dependent upon it and, like, had a crappy password because you figured, oh, well, two-factor authentication will protect me.



You know, so it's not something that can be relied upon nearly as much as one-time passcodes or security keys.  So I don't think this is great news because it seems to me that it might end up causing Twitter users to simply disable all use of two-factor authentication without upgrading their existing SMS, you know, least of the three good authentication methods to one-time passcodes or a security key.



At around 450 million monthly users of Twitter, that 1.95% who have been using SMS-based two-factor authentication is 8.25 million SMS users per month.  So that likely adds up, and I can see Elon wanting to cut cost.  And if there's no way for Twitter to determine whether the phone numbers being registered are "pay to send" numbers, then I suppose he doesn't have much choice.  On the other hand, a great many other large social media organizations offer SMS-based two-factor authentication, and they don't appear to have any similar problems.



In any event, I hope that those who need some form of authentication will move to passcodes at least, rather than just putting off, you know, all extra authentication when Twitter kills two-factor authentication a month from now.  I think it's actually on March 20th, so a month from yesterday.



We have some good news.  We knew it was coming.  It has actually happened.  And I've seen texts, or tweets rather, speaking of Twitter, from our listeners, wondering if they should move yet.  "Maybe" is the answer.  The Argon2 memory hard...



LEO:  Woohoo!



STEVE:  ...PBKDF, yup, which promises to be far more resistant to brute forcing, is now available from Bitwarden and is present on "some" Bitwarden clients.  And that's the key word.  Before switching to it, since the switch must be made system-wide per user, you'll need to wait until, and make sure, that all of the platform clients, the Bitwarden platform clients you use, have been upgraded to support Argon2 and [crosstalk] record...



LEO:  2023.2, that's the version you need.



STEVE:  Exactly.  That's the one you want, 2023.2.



LEO:  I have it on my iPhone.  I don't yet have it on Android.  But you even have to have it on wherever you use it, on your desktops, on your plugins and all of that.



STEVE:  Yes, it's got to be in your browser extensions, and apparently it's not quite there yet.



LEO:  You'll be blocked; right?  You won't be able to use it if it's...



STEVE:  Correct.  You will not be able to authenticate on that new device. 



LEO:  Right.



STEVE:  Six days ago a Bitwarden employee named Ryan, he posted to Reddit.  He said:  "For those curious as to why not everything is rolled out at once, each browser extension and mobile app needs to go through an approval process with their respective app stores.  Please be patient.  Usually the approval process takes about a week."  So, you know, this is fresh news, but it's coming soon to Bitwarden platform clients near you.



LEO:  That's the good news is that Bitwarden has approved the pull request, added it, and it is in the new version.  Just wait till you get the new version.  You will.



STEVE:  Right.  And if you have it in iOS, then that's significant.



LEO:  Yeah.  I just got it a couple of days ago on iOS.  I've been watching with great interest, as you might imagine.  And I will switch as soon as I can do that safely, yeah.



STEVE:  So Mark Zuckerberg posted an announcement about a little change in Meta.  He said:  "Good morning and new product announcement.  This week we're starting to roll out Meta Verified - a subscription service that lets you verify your account with a government ID, get a blue badge, get extra impersonation protection against accounts claiming to be you, and get direct access to customer support.  This new feature is about increasing authenticity and security across our services.  Meta Verified starts at $12 per month on the web or $15 per month on iOS."



LEO:  Yow.



STEVE:  I know.  That's exactly my feeling.  He says:  "We'll be rolling out in Australia and New Zealand this week and more countries soon."  So, okay.  Facebook is adding paid identity verification and more.  So elsewhere in their announcement they wrote:  "Some of the top requests we get from creators are for broader access to verification and account support, in addition to more features to increase visibility and reach.  Since last year, we've been thinking about how to unlock access to these features through a paid offering.



"With Meta Verified you get a verified badge, confirming you're the real you, and that your account has been authenticated with a government ID."  As also mentioned, I don't think they say it here, you have to be using your real name on your Facebook page, not some random handle.  "Also you get more protection from impersonation with proactive account monitoring for impersonators who might target people with growing online audiences.  Third, help when you need it with access to a real person for common account issues.  Fourth, increased visibility and reach with prominence in some areas of the platform like search, comments, and recommendations.  And, finally, exclusive features to express yourself in unique ways."  And we don't know what those are.



So first of all, I reacted exactly as you did, Leo.  12 bucks a month on the web and 15 bucks a month on iOS strikes me as really expensive.  It's not a one-time verification fee, which would seem reasonable.  This is an ongoing cost, you know, $144 a year or $180 a year on iOS.  And so I suppose it's not for everyone.  If someone uses Facebook as a major platform, then I could see how it makes sense to pay something to obtain spoofing prevention and apparently higher visibility in search ranking results.



LEO:  You don't get ad-free, though; right?  I mean, it's not like - you only pay us seven bucks, and you get ad-free.  I don't, you know, I don't really understand.  And it's not for businesses.  It's only for individuals.  It's very strange.



STEVE:  Right, correct.  That's not available for businesses.



LEO:  Yeah, yeah.



STEVE:  They said "at this time."



LEO:  Well, we'll see.  We'll see.  It's not going to generate 10 billion dollars a year, and that's what Mark's spending on VR right now.



STEVE:  No.  No.  Emsisoft, a company we've spoken of often...



LEO:  I know, I remember that name.



STEVE:  Yeah.  They basically provided us with a reminder of why simply having code signing is not and should not be sufficient to have antivirus and download protection warning silenced.  And so the antivirus publisher Emsisoft has put out a public service announcement warning that threat actors are currently using fake Emsisoft code-signing certs to sign their malware.  This results in attacks appearing to come from Emsisoft's products, as well as to slip past anything that refuses to run unsigned software.  So at some point I think what's going to happen, you know, code signing will become necessary, but not sufficient.  At the moment, it's entirely optional, but mostly is there for user assurance.  And I'm signing all of my apps now because it just seems like a good thing to do.



I know that when I'm - sometimes I'm digging around on the Internet, looking for some obscure thing because part of my life is still tied to DOS.  If I see something on some download site, I will check to see if it's signed because, although as this little warning reminds us, it's not absolute assurance, but it's sure better than not having something signed.  So, and it does, it certainly, if nothing else, it sends - it's a signal that AV and systems like Microsoft Defender can add to the conglomeration of other signals to decide what level of warning they want to provide the user.



Okay.  DDoS attacks are always resource depletion or resource consumption of one kind or another.  Today's modern DDoS attacks are typically no longer floods of TCP SYN packets like they were in days past.  Those now seem quaint by comparison.  Modern attacks are aimed less at consuming or clogging raw bandwidth than at asking web servers to generate more pages per second than they possibly can.  Since modern websites are generally the front-facing surfaces of a complex content management system on the back end, which is driven by some form of SQL database, individual HTTPS queries have become much more computationally intensive than yesterday's serving of static web pages.



The previous contemporary-style DDoS attack-blocking record was set by Google Cloud, which last June reported blocking an attack rate of 46 million HTTPS requests per second.  But that was then.  Now, last week, Cloudflare has reported that it successfully fended off an attack that was 35% greater than that, mitigating a now new record-breaking and now setting HTTPS DDoS attack of 71 million requests per second.  That's a lot of bots spread around the world all concentrating their fire onto a single target.



There are a growing number of strong website DDoS defenders. They include Akamai DDoS Mitigation, AWS Shield, Cloudflare's DDoS Protection, Google Cloud, F5's DDoS Hybrid Defender, Imperva DDoS Protection, and Microsoft's Azure DDoS Protection.  Websites that pay to be located behind them are able to remain online even during an attack of such scale.  That alone is somewhat astonishing.  And an attack of this scale would utterly obliterate any other site that's simply "on the Internet."



The mitigation of attacks of such scale, while avoiding collateral damage to nearby resources, requires carriers of the attacking traffic which is bound for a site under an attack to block all traffic as far away upstream from the target as possible to prevent that traffic's aggregation as it moves from router to router approaching its destination.  If we picture the Internet as a highly interconnected global network of individual routers, which is exactly what it is, each one forwarding traffic toward its destination, a useful overlay for this is the image of a great funnel, where incoming traffic is being funneled toward its target.  In the model of a funnel, the closer we approach the funnel's neck, the greater the traffic burden becomes.



Since the physical implementation of this traffic movement are individual routers, the best defense against "too much traffic" is to cause attacking traffic packets to be dropped far out at the funnel's mouth.  But doing this effectively inherently requires a large traffic provider.  If the provider's network is not sufficiently large to allow the incoming traffic to be blocked before it has the opportunity to concentrate, then the provider's aggregation routers would be swamped themselves before it even gets to the user's web server.  And many other of the provider's customers who are also being served behind those aggregation routers, would have their access, their site access impacted by the collateral damage caused by a failure of the packet transport fabric.



An organization of Cloudflare's size, to name just one, has the advantage of operating at global scale.  And when we're talking about handling attacks of this size, the network size is not only an advantage, it's a necessity.  Since attacking bots are also globally spread, traffic bound for one customer's website will be entering the network of a global carrier such as Cloudflare at many peering points across the globe.  So the moment an attack is detected, all of the provider's edge routing infrastructure can be informed of the attack and switched into an attack mitigation stance.



We talked many years ago about the sheer brilliance of the Internet's design, and with the original concept of autonomous packet routing being at the heart of this.  That the original concept has withstood the tests of time, insane growth in usage and application, stands as a testament to those who created this system so long ago.  But its great weakness is that it was never designed to withstand deliberate abuse.  The idea that someone would flood the network with attack traffic was something that this system's gifted designers could never have anticipated.  Even so, the Internet's basic architecture has been adaptable to incorporate such protections over time.  So, wow.  Hats off to them.  And Leo, drinks up for me.



LEO:  We do have DDoS - I actually shouldn't talk about our DDoS mitigation, should I.  But we use it.  And it's not Cloudflare.  How about that?  We might be using Cloudflare.  We use somebody else.  There are a number of people that do this, people with big fat pipes, basically.



STEVE:  Yup.



LEO:  That's the key.



STEVE:  It's no mystery, though.  Anyone can check to see where the traffic [crosstalk] sent to you.



LEO:  Oh, they can tell?  Oh, all right.  Yeah, I guess you're right, come to think of it.  So we use AWS.  They have a very good DDoS protection solution, as well.



STEVE:  AWS Shield.



LEO:  Yes.  You mentioned it.  And now I can tell the world.  We use it.  All right, Steve.  On we go.



STEVE:  Speaking of DDoS attacks, I've often worried out loud here for at least the last couple of years about what would happen when malicious actors finally got around to focusing their evil intent upon, and commandeering for their nefarious needs, the truly countless number of Internet-connected, low-end IoT devices.  Well, those worries are beginning to manifest.



Last year, from the summer, July, through December of 2022, Palo Alto Networks Unit 42 researchers observed a Mirai Botnet variant known as V3G4 predominantly leveraging IoT vulnerabilities to spread.  V3G4 targets 13 separate vulnerabilities in Linux-based servers and Linux-based IoT devices.  The devices are commandeered for use in DDoS attacks.  The malware spreads both by brute-forcing weak or default telnet and SSH credentials, and by exploiting known, but unpatched, firmware coding flaws to perform remote code execution on the targeted devices.  Once a device is breached, the malware infects the device and recruits it into its botnet tribe.



And this is exactly what we've been worried about for years.  Though it makes no rational sense at all, we know how difficult it is to even update big iron systems that need to be kept current, where there's a well-established notification and patching infrastructure in place to support that.  Just look at the recent VMware ESXi fiasco.  Those systems should have been readily updated.  But as we know, they weren't.



So compare that to some random IP camera which was long ago installed and has since been forgotten.  What about patching it?  Good luck with that.  We can't even keep our servers patched.  Today, as I've often lamented, we have a literally uncountable number of gizmos and gadgets attached to the Internet.  Why?  Because we can.  While most of those in our homes are safely tucked away behind the one-way valve of our NAT routers, and also hopefully on their own isolated network where possible, a great many, due to their role and application, have deliberately been given access to the public Internet.



In the present case of V3G4, Unit 42 tracked three distinct campaigns.  Unit 42 believes all three attack waves originated from the same malicious actor because the hardcoded command-and-control domains contain the same string, the shell script downloads are similar, and the botnet clients used in all attacks feature identical functions.  Yeah, that'd be enough to convince me.



Okay.  So what does V3G4 attack?  It exploits one of the 13 vulnerabilities.  There's a CVE-2012-4869, which is a FreePBX Elastix remote command execution.  There's a Gitorious remote command execution.  There's a CVE-2014-9727, FRITZ!Box Webcam remote command execution.  Mitel AWC remote command execution.  There's a CVE-2017-5173, Geutebruck IP Camera remote command execution.  Also a 2019-15107, Webmin command injection.  Spree Commerce arbitrary command execution.  FLIR Thermal Camera remote command execution.  A 2020-8515 DrayTek Vigor remote command execution.  Also same year, 15415 DrayTek Vigor remote command execution.  In 2022, last year, 2022-36267 Airspan AirSpot remote command execution.  Atlassian Confluence remote command execution.  C-Data Web Management System command execution.  13 in total.



And notably, some of those CVEs were from 2012, 2014, 2017, and 2019.  There's no reason to imagine that any of these problems will ever be repaired.  And why would they be?  The device is apparently working just fine.  And who even knows whether the company that created it still even exists?  A new trend we've observed is that companies are formed on the fly by pulling together the individual required resources.  Devices are designed, they're manufactured, they're sold, then the entire briefly assembled organization dissolves, returning back to its original component parts.  There is no one to call for updates.  There is no follow-up.  There is no accountability.  There's no aftermarket, after-sale support.  Yet an Internet-connected gadget can now harbor hostile code and be used, probably throughout the rest of its long service life, as one more tiny cog in a massive and untraceable global attack-launching platform.  That's where we are today.



Again, in the case of V3G4, after compromising the target device, a Mirai-based payload is dropped onto the system and attempts to connect to the hardcoded command and control address.  Once running, the bot terminates a large number of known processes from a hardcoded list, which includes other competing botnet malware families.  Hey, I'm here now.  You guys get out.  You know?  Now there's a new king of the hill.



A characteristic that differentiates V3G4 from most other Mirai variants is that it interlaces the use of four different malware XOR encryption keys rather than just one.  This was clearly an attempt to make static analysis reverse engineering of the malware's code and decoding its functions more challenging.  As I briefly noted earlier, when spreading to other devices, the botnet uses a telnet/SSH brute forcer that tries to connect using default or weak credentials and those 13 known vulnerabilities.  Once set up and running, with a connection to the botnet's command-and-control, the compromised devices are then given DDoS commands directing their attacks.  This variant offers TCP, UDP, SYN, and HTTP flooding methods.



The Unit 42 guys suspect that V3G4 sells DDoS services to clients who want to cause service disruption to specific websites or other online services, although the front-end DDoSing service associated with this botnet has not been identified at the time of Unit 42's report.  So, you know, this is what was expected for a number of years was that eventually people were going to get around to getting serious about taking over our IoT devices and enlisting them in DDoS attacks.  And we're now seeing a classic, perfect example of that happening.



So week after week I encounter news of malware stashes being found on this or that, or sometimes all, popular code registries and repositories.  An example of such a piece of news from last week is that Checkpoint's research team detected 16 malicious JavaScript packages uploaded on the official npm registry.  The researchers said that all packages were created by the same author, and were designed to download and run a hidden cryptominer on a developer's system.  The packages pretended to be performance monitoring, so you'd expect them to use your computer's resources in order to determine how well a package is running.  It, however, stays around afterwards, unbidden, to cryptomine in the background.  All 16 of the packages have since been removed from the npm registry.



Anyway, so I just wanted to say that this is a constant flux.  It's like that week after week, endlessly.  I'm mentioning it this week because I don't mention all of this happening every single week in one form or another.  Sometimes it's npm.  Sometimes it's PyPI.  Sometimes something else.  Basically, wherever security firms are looking, they are now finding malicious packages.  So I just wanted everyone to be aware that there is this constant flux of malware dribbling into the open source ecosystem.  It's now another one of today's realities.



LEO:  It's used everywhere, too, this package management system.  We have Max.  We have Homebrew.  Every Linux distro has a package manager that downloads stuff.  And security really is an afterthought.  You know, I use a package management...



STEVE:  It's like, oh, hey, it's free.  Grab this.



LEO:  It's free.  It's downloadable.



STEVE:  Grab this, you know, grab that.  And the other thing is sometimes when you install something it comes with this massive list of dependencies; right?



LEO:  Right.



STEVE:  Because you [crosstalk].



LEO:  So those all download and install; right.



STEVE:  Exactly.



LEO:  Right.  You know, some of the package managers I use on Linux give you a chance to review the changes ahead of time.  But even then, most of us just go yeah, yeah, yeah, whatever.



STEVE:  Leo, it's like a license agreement.  It's like, oh, yeah, fine.  What button do I push?



LEO:  So it's page after page of code, of make file code and, you know, weird code.  Ain't nobody got time to read that.



STEVE:  Nope.



LEO:  So I'm not surprised.  I think we've got to solve this, though.  They're going to find a way to fix this somehow.



STEVE:  Yeah.  And, you know, the problem is when you talk about closing it...



LEO:  You can't.



STEVE:  Well, closing it is against the spirit of it being open.



LEO:  Yeah, yeah, right.



STEVE:  Which is the whole point.



LEO:  Right.  I don't know how you do this, yeah.



STEVE:  So Patch Tuesday.  That was last Tuesday.  Many well-known publishers got in on the action.  The industry was made aware of security updates released by Apple, Adobe, Git, Microsoft, and SAP.  The Android project, OpenSSL, and VMware also released security updates last week.  Microsoft patched 80, eight zero, vulnerabilities, including three zero-days; and Apple got a lot of attention releasing security updates that included a patch for an actively exploited Safari WebKit zero-day vulnerability.  So everyone was told, you know, don't delay on that one.  We know that the sometimes crucial mistakes many large and small organizations make is in ignoring these fixes.  You know, if everyone kept their software patched we'd be seeing many fewer widespread problems, such as that VMware ESXi debacle which is still ongoing, by the way, more than 500 newly compromised systems just last week.  So still happening, but slowing down.



As it turns out, however, and this is one reason that at least enterprises need to be a little careful, it wasn't all smooth sailing with this month's security updates.  Microsoft has stated that some Windows Server 2022 virtual machines may no longer boot after installing the updates released last week.  This issue, they said, only impacts VMs with Secure Boot enabled and running on VMware's vSphere ESXi 6.7 U2 and U3 or vSphere ESXi 7.0 point anything.  The culprit is patch KB5022842 which, if installed on guest virtual machines running Windows Server 2022, may no longer start up.  VMware and Microsoft are working to determine the cause.  Interestingly, even though Microsoft says that only VMware ESXi VMs are affected, some admin reports point to other hypervisor platforms, including bare metal, also being impacted by this issue.  So again, end-users should upgrade; enterprise users are always going to have to be on guard.



Last Friday, Samsung announced a new feature for, at the moment, only its Galaxy S23 series smartphones, called Message Guard.  Now, the details are sketchy, and it sounds like it resembles Apple's "Blast Door" technology which Apple introduced back with iOS 14.  Both technologies - Message Guard, which is Samsung's, and Blast Door, Apple's - are image rendering sandboxes.



We've often talked about the difficulty of safely and securely rendering images because image compression encodes images into a description that must later be read and interpreted in order to recover a close approximation of the original image.  It's those image decompressing and rendering interpreters that have historically harbored subtle flaws that malicious parties have leveraged to create so-called "zero-click exploits," meaning that all the phone needs to do is display an image in order to have it taken over by a remotely located malicious party.  So Samsung now has this technology added to its S23 series, and it has said that it plans to expand it to other Galaxy smartphones and tablets later this year that are running on One UI 5.1 or higher.



The addition of these technologies represents a maturation, I think, of our understanding of the problems we face.  It is so easy to imagine, and every developer does, that any problem that's found will be the last one that will ever be found.  And of course that's true, right up until the next problem is discovered.  Experience shows that we're not running out of such problems anytime soon, if ever.



Okay.  So it turns out that millions of Hyundai and KIA autos, which is to say approximately 3.8 million Hyundai and 4.5 million KIAs, are vulnerable to being stolen using just a bit of technology.  And that indeed, once the method of doing so became common knowledge in some circles, Los Angeles reported an 85% increase in car thefts of those two brands.  And not to be outdone in the car theft category, Chicago saw a nine-fold increase  900%  in the theft of those cars.



Okay.  So first, how was the news spread?  Believe it or not, by something being called a "challenge" which has been heavily promoted on TikTok since last summer, July 2022.  TikTok presented instructional videos showing how to remove the steering column cover to reveal a USB-A format connector which can then be used to hotwire the car.



Hyundai's and KIA's first low-tech response, which began last November, was to work with law enforcement agencies across the United States to provide tens of thousands of steering wheel locks.  You know, a big red steering wheel locking bar has the advantage of letting TikTok-watching car thieves know that even if they're able to enter and start the car, aiming it will still present a problem.  The fundamental problem surrounds a coding logic flaw that allows the "turn-key-to-start" system to bypass the engine immobilizer which is supposed to verify the authenticity of the code in the key's transponder to the car's ECU.  In other words, no key is needed.  This allows car thieves to activate the ignition cylinder using any USB cable to start and then drive off with the car.



Hyundai wrote:  "In response to increasing thefts targeting its vehicles without push-button ignitions and immobilizing anti-theft devices in the United States, Hyundai is introducing a free anti-theft software upgrade" - oh, that's nice of them - 



"to prevent the vehicles from starting during a method of theft popularized on TikTok and other social media channels."



Okay.  So the software upgrade will be provided at no charge - you'd better believe it - for all impacted vehicles, with a rollout which began last Monday - a week ago yesterday - initially to more than a million 2017-2020 Elantra, 2015-2019 Sonata, and 2020 and 2021 Venue cars.  All of the rest of the affected autos - and there were too many of them to list here - will be upgraded through the summer of this year.  The upgrade will be installed by Hyundai's official dealers and service network throughout the U.S., and is expected to take probably less than an hour. Eligible car owners will be individually notified.



Hyundai's announcement explained that the upgrade modifies the "turn-key-to-start" logic to kill the ignition when the car owner locks the doors using the genuine key fob.  After the upgrade, the ignition will only activate after the key fob is first used to unlock the vehicle, meaning that you can't break in first.  That was the missing interlock which facilitated this hack in the first place.  So the question remains, though, without a big red steering wheel locking bar, how would thieves without wheels know that your particular Hyundai or KIA is no longer vulnerable?  Hyundai is solving this dilemma by supplying its customers, after they get the upgrade, with a convenient window sticker.  And I would love to see what the sticker says, you know, like upgraded so the TikTok hack no longer works?  Don't bother?



LEO:  Can you put glue in the USB port?  Would that help?



STEVE:  Well, and the problem is your car is going to get broken into before the bad guy is inside.



LEO:  Yeah.  Put a sign in the window that says "Glue in the USB port.  Do not attempt."



STEVE:  Yeah.  So Hyundai's providing a sticker, and I would love to see what the sticker says.



LEO:  I'll show you this sticker we just got at Best Buy.  You're going to like this.  Remember to turn your computer off before 3:14:07 on 1/19/2038.  I should send that to you.



STEVE:  That's brilliant.



LEO:  As a Picture of the Week.  I just saw this on Mastodon.



STEVE:  That's brilliant.



LEO:  Hyundai's got a sticker that says what?  Software upgraded.  What?



STEVE:  I bet you won't be able to steal this car or something.  I mean, like, what's it going to...



LEO:  But would it - I bet it doesn't - is it really going to prevent that?  I don't know.



STEVE:  Well, they're really going to put a sticker in the window.  You know, and so but it only works for some.  Unfortunately, there are some models that completely lack the engine immobilizer technology.



LEO:  Ah, see?  That's what I was thinking, yeah.



STEVE:  And so are unable, yes, they cannot receive the software fix which updates the missing immobilizer logic.  So to address that problem, Hyundai will cover the cost of steering wheel locks for their owners.  And, you know, this is the definition of a kludge.  So far, all of this talk has been about Hyundai.  But as noted, KIA has a similar problem.



LEO:  Yes, same company, yeah.



STEVE:  KIA has promised to start the rollout of its software upgrade soon, but hasn't yet announced any specific dates or details.  The U.S. Department of Transportation was the source of those stats about the number of affected vehicles, and also noted that these hacks have resulted in at least 14 confirmed car crashes and eight fatalities.



LEO:  Oh, no.



STEVE:  So what do you want to bet that product liability and personal injury law firms are already rubbing their hands together over this quite significant screw-up?  Wow.  Okay.



LEO:  Who says TikTok isn't useful?  That's what I say.



STEVE:  So the astonishing success and the equally surprising performance of OpenAI's ChatGPT-3 Large Language Model AI means that a new phenomenon will soon be entering mainstream use.  Leo, I'm going to take a sip of water.  Why don't you tell our listeners...



LEO:  Oh, good.  I will.  I'll tell you about Club TWiT while we get ready for - I'm dying to hear Steve's take on all this.  This will be fascinating.  We've been talking about nothing else on all the shows for the last couple of weeks.  It's a hot topic.



STEVE:  And it's what gave our podcast the name today, "A Clever Regurgitator."



LEO:  I figured that as much, yes.  There have been lots of names for ChatGPT, including Mansplaining Machine, a Spicy Autocorrect.  But I like the Regurgitator.  That's good.  That's good.  And continue on talking about ChatGPT.



STEVE:  Okay.  So as I started to say, the astonishing success and the equally surprising performance of OpenAI's ChatGPT-3 Large Language Model AI means that a new phenomenon will soon be entering mainstream use.  I think that's absolutely clear.  Right here on this podcast, thanks to Rob Woodruff's inspiration to enlist ChatGPT in assisting him with authoring that LastPass vault-deobfuscating PowerShell script, we've all witnessed first-hand just how significant these coming changes will be.  And anyone who's been following the news of this may have continued to be somewhat astounded by what this technology appears to be capable of accomplishing.



I think that the most accurate and succinct way of describing what we're witnessing is that it is astonishing to see the degree to which a neural network using large language modeling, as exemplified by ChatGPT, is able to simulate intelligence.  And I think that is the key concept to hold onto.  ChatGPT is not itself in any way intelligent.  It is a clever regurgitator of intelligence.  One of the dangers, which we can feel present, is that this turns out to be a surprisingly subtle yet crucial distinction which is guaranteed to confuse many, if not most people who casually interact with this mindless bot.  



After absorbing the historical global output of a truly intelligent species - namely, man - we have an automaton that's able to take our entire historical production, all at once as a whole, and quickly select from that massive corpus the right thing to say.  It's able to choose it because that right thing has been expressed before, by man, in thousands of different contexts.  So it appears intelligent because it's mimicking an intelligent species.  A parrot in a cage who says "Polly wants a cracker" is more intelligent because it really does want a cracker.  Although ChatGPT may be induced to express a desire, that's still nothing more than mimicry since it has previously absorbed all of humanity's past expressions of desire.  It doesn't ever actually want anything because there's not actually any "it" there at all to do any wanting.



Again I come back to "Yes, what it does is astonishing."  But that's only because it is the first thing we've ever encountered that's able to convincingly sound like us.  But that's all it's doing.  It's sounding like us.  The parrot in its cage is extremely limited in its ability to sound like us.  A sufficiently large language model neural network is potentially unlimited in its ability to sound like us.  And if we can be certain of anything, it's that this simulation will be improving over time, especially now that this technology has left the lab and that capitalistic forces of commerce will be driving and funding further advancement.  But nevertheless, in no way should "sounding like us" ever be confused with "being like us."  A high-fidelity recording of Pavarotti may sound exactly like Pavarotti, but it isn't Pavarotti.  It's just a recording.



Okay, so what got me started on this?  It was an interesting experiment by some researchers at the company ANY.RUN who wanted to explore an aspect of ChatGPT's limitations.  They wanted to see whether ChatGPT's otherwise impressive capabilities might extend to analyzing real-world malware.  If so, it might make security researchers' lives more productive by allowing them to dump a load of code into ChatGPT and have it figure it out.



Their blog posting begins:  "If ChatGPT is an excellent assistant in building malware, can it help analyze it, too?  The team of ANY.RUN malware sandbox decided to put this to the test and see if AI can help us perform malware analysis.  Lately, there's been a great deal of discussion about malicious actors using ChatGPT, the latest conversational AI, to create malware.  Malware analysts, researchers, and IT specialists agree that writing code is one of ChatGPT's strongest sides, and it's especially good at mutating it.  By leveraging this capability, even wannabe hackers can build polymorphic malware simply by feeding text prompts to the bot, and it will spit back working malicious code.



"OpenAI released ChatGPT in November of 2022, and at the time of writing this article, the chatbot already has over 600 million monthly visits.  It's scary to think how many people are now armed with the tools to develop advanced malware.  So going into this our hopes were high; but unfortunately, the results weren't that great.  We fed the chatbot malicious scripts of varying complexity and asked it to explain the purpose behind the code.  We used simple prompts such as 'explain what this code does' or 'analyze this code.'"  Okay.  And then they go on with examples.



The short version of what they discovered is that ChatGPT did remarkably well when the researchers gave it toy code to examine.  And it really did surprisingly well on that.  But as the complexity of the testing code increased, there was a sort of "complexity cliff" they ended up going over, after which ChatGPT collapsed completely.  And knowing what we know now, isn't that exactly what we would expect?



As a Large Language Model neural network, ChatGPT is not in any way even the tiniest bit sentient.  Our limited-language parrot is more sentient.  So ChatGPT is unable to "understand" anything at all.  That means it's not going to be great at the true problem solving that reverse engineering complex malware code, or any code, requires.  But reverse engineering code is very different from writing code.  Thanks to the explosion of open source software, ChatGPT has previously ingested all of the source code on the Internet.  That's a massive amount of real working code.  And as we understand, it is able to select, regurgitate, and rearrange the code that it has previously encountered.  But when it's asked to produce code that it hasn't previously seen, that's where things start to become fuzzy and where it starts making mistakes since, again, it's not really understanding anything about what it's doing.  It's simply searching for a matching context amid all of the world's previously written code.



Last week I was corresponding with two of the sharpest minds I've ever had the privilege of knowing.  And I was talking about the idea that I've previously shared here, which is that I think one of the things ChatGPT's surprising success at mimicry teaches us is that a good portion of the vaunted human intelligence we make such a big deal about having is mostly just repeating what we've previously encountered, and anticipating what's going to come next based upon what came next in the past.



Here's what I wrote to these two friends.  I said:  "If I look back over my creative life, there have been a few moments that I would say were truly inspired invention, where I created something from nothing, something that was actually new.  But far and away 99.99999% of everything I do and have done has been wholly derivative.  As it happens, I obtain immense satisfaction and even some joy from endlessly solving combinatorial puzzles.  Thus I love electronics and coding."



Okay.  So to wrap it up, I thought it was interesting and not at all surprising that, whereas ChatGPT can perform quite well at recombining what it's seen in the past to produce new and nearly functional code in the future, it is not going to be able to understand and explain the detailed operation of some piece of purpose-written malware that it has never encountered before.  Though ChatGPT was initially a surprise, and though I'm sure that this technology is going to continue to improve over time, I believe that we now have a good foundation for understanding what it can and cannot do.  And at least for the foreseeable future, it is at most a very clever regurgitator.



LEO:  There's a good piece I'd recommend people to by Stephen Wolfram over on Wolfram|Alpha.  It's called, I think, "How ChatGPT Works."  And for the slightly mathematically inclined I think it would be very interesting.  You know, he talks about the initial kind of first approximation of how it works, which is basically autocorrect, using weighted values to predict the next word.  It's a little more sophisticated than that.  But it's essentially predicting the next chunk based on the statistical model.  And it's quite interesting.  Highly recommend it.



But yeah, I mean, it's not sentient at all.  Obviously.  And it's too bad because a lot of the press's focus was, especially with the Bing Chat, just based on the new ChatGPT-4 model, they were just needling it until it went crazy and then going, "You see?  You see?"  And it did feel like, you know, if it says "I love you," or "I hate you," or, you know, "I won't hurt you unless you hurt me," it sounds sentient.  But it's honestly, it's really lost its marbles.  And Microsoft's response to that was, well, after five questions we're going to reset.  You can start over.  You can't needle it into the point of insanity.



STEVE:  So Leo, I really do think we should not give it the nuclear launch codes.



LEO:  No, probably not.



STEVE:  And I think resetting it after five questions sounds like a good idea.



LEO:  Sensible, sensible.



STEVE:  And I hope that, you know, this is like maybe enough of a little bit of a freaky, yet still benign wakeup call that, you know, we're not in the future going to give anything the nuclear launch codes.



LEO:  I think it helps us, after the initial wave of wow, understand a little bit more about what this is.  It may pass the Turing test, but this is why the Turing test was a bad idea to begin with.  That is not a measure of success really in a general artificial intelligence.  We're still a long way off from that.  Yeah, but don't give it the nuclear codes.



STEVE:  There are going to be a lot of people who are going to have long conversations into the middle of the night, you know, treating it like a therapist and a BFF.



LEO:  There are, yeah.  Like Eliza, yeah.  It's like a good Eliza.  Eliza was dopey, but this is surprisingly good, at least for the first hour or so.  It really starts to get wacky after a while.  Steve can go two hours and be coherent by the end.  It's amazing.  He's much better than ChatGPT.  Steve's website, GRC.com, is the host of many fine things, including SpinRite, the world's best mass storage recovery and maintenance utility, currently 6.0.  6.1's on the way.  You'll get it for free if you buy now.  That's Steve's bread and butter.



He offers a lot of other free stuff there, including ShieldsUP! and so forth, Password Haystacks, lots of information.  We talked the other day, somebody was talking about your DNS Benchmark program.  And what's the InControl, the Windows 10 to Windows 11 stymie-er.  We were talking about that on Sunday on Ask the Tech Guys.  Lots of great stuff.



Copyright (c) 2023 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#912

DATE:		February 28, 2023

TITLE:		The NSA @ Home

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-912.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  What mistake did Windows Update make last week?  What if you don't want to paste with formatting?  What browser is building in a limited-bandwidth VPN?  What more did we just learn about LastPass's second breach?  What did Signal say to the U.K. about scanning its users' messages?  What was just discovered hiding inside the Python package Index repository?  What proactive move has QNAP finally taken?  What disastrous bug did SpinRite's testers uncover last weekend in motherboard BIOSes?  And what amazingly useful "Best Practices" advice has the NSA just published for home users?  Answers to all those questions and some additional thoughts will be yours before you know it on this week's 912th episode of Security Now! titled "The NSA @ Home."



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  A mistake in Windows Update?  Steve explains.  We'll also talk about LastPass, now that more details are coming out.  It's really kind of a stunning hack.  Signal says ta-ta to the U.K.  Well, it will if the U.K. does something bad.  And believe it or not, the NSA's security recommendations.  They're pretty good. All that and more coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 912, recorded Tuesday, February 28th, 2023:  The NSA @ Home.



It's time for Security Now!, the show where we cover the latest in security with the most important guy in this building right now, Mr. Steve Gibson - and he's not even in the building.



STEVE GIBSON:  Hey, Leo.



LEO:  Hey, GRC.com.



STEVE:  Good to be with you again.



LEO:  Yeah, it's good to see you.



STEVE:  Yeah.



LEO:  Time once again to talk about how bad the world is.



STEVE:  Oh, we squeaked one more episode in on this last day of February.  Of course, February ended early, so that's what made it squeaky tight.  So we've got a bunch of questions in our new style that we're going to answer, and I really do wish that our audience had heard you encounter our Picture of the Week for the first time because...



LEO:  Oh, it's a belly laugh, wow.



STEVE:  It is.  Every time I look at it, it cracks me up again.  But we're first going to answer some questions.  What mistake did Windows Update make last week?  What if you don't want to paste with formatting?  What browser is building in a limited bandwidth VPN?  What more did we just learn about LastPass's second breach?  What did Signal have to say to the U.K. about scanning its users' messages?  What was just discovered hiding inside the Python Package Index repository?



What proactive move has QNAP finally taken?  What disastrous bug did SpinRite's testers uncover last weekend in motherboard BIOSes?  What amazingly useful best practices advice has the NSA just published for home users?  Answers to all those questions and some additional thoughts will be yours before you know it on this week's 912th episode, still going strong, of Security Now! titled "The NSA @ Home."



LEO:  Ooh.  Oh, that's going to be interesting.



STEVE:  Bring your NSA home with you.



LEO:  Yeah.  And that LastPass thing, holy cow, what a revelation. 



STEVE:  Uh, yeah.



LEO:  Finally they gave us some details, and it's not good.



STEVE:  Yup.



LEO:  It's bad.  It's very bad.



STEVE:  You know, I had the feeling, you don't want these bad guys on your tail because, I mean...



LEO:  Yeah, no kidding.  This was a very aggressive attack.  They really knew what they wanted.  Which kind of does not bode well for those of us whose LastPass vaults are now in their hands.



STEVE:  Yeah.  



LEO:  I mean, they went after them.  It wasn't some script kiddie accidentally found them.  Steve, talking about zero knowledge, our Picture of the Week.



STEVE:  What we have is a close-up photo that I presume someone took when they were checking out the smoke alarm on the home they just bought, or maybe the apartment they're renting, I don't know.  So we've got a First Alert smoke alarm in close-up.  Over on the left you see the little door you can pull off, probably to change the nine-volt battery and so forth.



Anyway, the focus of this picture, and the intent of taking a picture of the smoke alarm, is that printed on the side is a field where you can indicate the date at which this was installed because, you know, that could be important.  Like fire extinguishers have an expiration date.  You need to change them out every so often.  So this says "Installed on:" and then there's an underline where clearly you're intended to indicate the date of installation.  What we have instead written here is "Installed on:  the ceiling."



LEO:  Oh, boy.



STEVE:  Oh, that's where it is?  That's why I'm on a ladder?  Okay, yeah.  Installed on the ceiling.  I'm not sure where else you would install it.  On the wall?  On the floor?  I don't...



LEO:  I think, though, inspection will reveal that it is in fact installed on the ceiling.  No need to write that down.



STEVE:  That's true, yeah.



LEO:  You're looking at it.



STEVE:  You don't want to leave that field blank, however.  Maybe it won't work without something written there.  So, let's see, what could they possibly...



LEO:  Installed on.



STEVE:  What could they want me to...



LEO:  No, it's obvious.  They want to know where it was installed.  What's it installed on?



STEVE:  Had a hard time getting out of high school with those multiple choice tests that, you know...



LEO:  Oh, lord.



STEVE:  Installed on the ceiling.  Okay, good.



LEO:  Okay.



STEVE:  Windows 11, anyone?  Yesterday morning I received the following email from a podcast listener and happens to be a SpinRite 6.1 tester.  Jeremy wrote from Texas:  "Hi.  FYI, about Wednesday of last week, Windows 10 Update offered to 'update' me to Windows 11, even though my HP AIO" - that's an all-in-one - "does NOT" - he has in caps - "qualify because it does not have the latest boot security level.  I immediately switched over to GRC.com and downloaded InControl and set my PC to stay on Windows 10 22H2 with security updates.  When I reloaded Windows Update, Windows 11 was no longer being offered.



"I thought I was 'safe' from Windows 11," he writes, "since I didn't qualify, but I guess not.  Just wanted to let you know to combine my efforts with others, if any.  It's a reminder to be proactive rather than rely on assumptions.  I've invested a lot of muscle memory in Windows 10 and don't think Windows 11 will offer me much.  I don't know if it was a momentary glitch in Win Update or a real offer, since I got InControl immediately."  And he signed off "Regular listener and 6.1 tester, Jeremy in Texas."



Well, also yesterday came the news that Microsoft had fixed a bug that was responsible for causing upgrade offers to unsupported PCs.  Apparently, the issue came to light last Thursday for Microsoft, and it was quickly resolved, and the fix was then pushed out to affected devices over this past weekend.  And this isn't the first time this has happened.  Windows 11 22H2 was previously offered to Windows 11 Insiders in the Release Preview channel even when they were using ineligible devices.  So it was after Microsoft was aware of this, but before they had pushed the update that Windows users were reporting, many more than just Jeremy in Texas, via Reddit and Twitter, that their unsupported devices were, to their surprise, suddenly being offered Windows 11 upgrades.



So Microsoft confessed this, and they said:  "Some hardware-ineligible Windows 10 and Windows 11 version 21H2 devices were offered an inaccurate upgrade to Windows 11.  These ineligible devices did not meet the minimum requirements to run Windows 11.  Devices that experienced this issue were not able to complete the upgrade installation process."  So apparently some users with Windows 10 and even some who were using Windows 11 21H2 were surprised and, I presume, delighted by this news, whereas Jeremy and I would have both been horrified by it.  The impacted devices included those running Windows 11 21H2, Windows 10 21H2, and Windows 10 20H2.  And, as Microsoft indicated, some later portion of the upgrade process apparently recognized the mistake that had been made and aborted the upgrade.



So, and remember that last month Microsoft announced that it had started a forced rollout of Windows 11 22H2, which is also known as the Windows 11 2022 Update, to systems running Windows 11 21H2, which will be approaching their end-of-support date.  Later this year on October 10th, actually, is when it officially ends in of course 2023.  And this automated feature update rollout phase came after the Windows 11 2022 update also became available for broad deployment the same day to users with ineligible devices via Windows Update.



Now, as it happens, the little Windows 10 machine I use weekly for zooming this podcast to all of our listeners and Leo at TWiT and everything, it's recently been bugging me, I think the last maybe two or three weeks, to upgrade it to Windows 11.  Since I only turn it on before the podcast and then off immediately afterward, I just say no.  But I hadn't gotten around...



LEO:  You know, there's a wonderful program from a guy named Steve Gibson called InControl.  You don't use that, huh?



STEVE:  Are you reading ahead?



LEO:  Oh, no, I'm not, okay.  I'd like to be surprised.



STEVE:  I hadn't gotten around to running GRC's InControl utility.



LEO:  Oh, okay.



STEVE:  What do you know?  To tweak the registry to get Microsoft to leave me alone.  Now, recall that InControl is the spiritual successor to Never10.  And Leo, I still remember your laughter when you first heard that name, Never10.  That's right.



LEO:  Never11 in this case.



STEVE:  Eff off Microsoft.  Anyway, the expectation was that Never10 would be all that was ever needed.



LEO:  Yeah.



STEVE:  But then Microsoft decided that Windows 10 would not be the last Windows after all.  So there was some tendency, you know, rather than create - I thought about creating Never11.  That's what people were asking for.  But then I'd have to do Never12 and Never13.  I wonder if they would do Windows 13?  I'll bet not.  Anyway, then I'd have to keep changing the name.  So instead I switched to InControl, which now no longer at least has to change its name.  As long as they leave this facility in place, I won't have to change it at all.



So anyway, the point is that this story from Jeremy and the fact that I kept just saying "no, no, no" every week finally prompted me to take action.  And it worked beautifully.  What do you know?  Initially it was showing the upgrade offer up at the top of the Windows Update screen.  So I ran InControl and told it that I wanted to stay put where I was at Windows 10 22H2.  So I clicked the button.  It tweaked the registry, and that was that.  Then I reran Windows Update, and the offer for Windows 11 was still there since it hadn't refreshed.



So I asked Windows Update to re-check for any updates, and that refreshed the screen, and the Windows 11 offer disappeared, and I received what I remembered now when I was originally testing this thing, the little red asterisked notice up at the top of Windows Update which says, asterisk in red, the whole thing's in red:  "*Some settings are managed by your organization."  Which is, you know, yes, perfect.  Leave me alone.



So what we're essentially telling it is that the higher-ups have decided that they're going to be in charge of upgrading, so Microsoft shouldn't be bothering us minions with any of that.  So anyway, I just wanted to remind everybody that that exists because as Microsoft has said, they're going to get more insistent about this; and no, thank you.



Okay.  This next item is not security related at all, but something, some news popped up when I was looking for security stuff that I just wanted to make sure everyone was at least aware of.  I also have no sense for how large the audience for this might be, but the facility is one of my most-used keystroke combinations.  It is "Pasting from the clipboard without formatting."  I'm a big user of copy-and-paste for moving things around.  But I almost never want to also copy and paste any of the text formatting of the source text which may be present when I move it somewhere else to paste it.  You know, it's quite annoying when I paste something, and it jumps into the appearance that it had, typically not even correctly, when all I want is the textual content itself.  I want to lose the text metadata.



So finally, a couple of years ago, after several years of annoyance, and I was even doing things like using Windows Notepad as an intermediate stop where in order to force the dropping of the formatting I would open Notepad, copy something that had formatting, paste it into Notepad which can't hold formatting, so it would force it to be lost.  And then I would copy that again and paste it into its destination.  So I thought, somebody must have fixed this problem.



So I went googling, and I found a slick little utility that I've been using ever since, known as PureText.  And that's all it does.  It sits in my system tray.  It allows you to set any combination of shifts and an action key.  I use CTRL-ALT-V rather than just CTRL-V, to perform a non-formatted textual clipboard paste.  It's beautiful.  Also you can assign a sound if you want, and I know lots of people don't like sounds.  I love sounds.  So it makes a nice little clunk sound to just confirm that I've got what I wanted.  Anyway, I'm mentioning this because Windows Power Tools will, completely separately from that, soon be getting a new module called "Paste as Plain Text."  In other words, I'm not the only one, or neither was the guy Steve Miller who wrote this thing, the one I'm using.



As I said, I'm using PureText, both on my Windows 11 and my Windows 10 machines.  But Power Tools won't run under Windows 7.  You know, you've got to use the Windows Store, and they won't let you do that from Windows 7.  So I'll be sticking with PureText.  But for what it's worth, if anyone else has this problem, Windows Power Tools will soon be getting Paste as Plain Text.  But SteveMiller.net is the site, and this guy's been writing code since Windows 95.  He's got stuff dating from then, still some of it useful.  So you might want to just check it out anyway:  SteveMiller.net.



Okay.  Nearly a year ago, last April, The Verge carried the news "Microsoft is adding a free built-in VPN to its Edge browser," with the subhead "Edge Secure Network" - as it's being called, actually now they're calling it Microsoft Edge Secure Network - "will roll out," they said, "as a part of a security upgrade."  They didn't say when, and it took like 10 months.  But it now appears to finally be happening.  It's going to be becoming available.



Two days ago, on Sunday, BleepingComputer posted:  "Microsoft Edge's built-in VPN functionality could soon begin rolling out to all users in the stable channel, with some users already getting access to the feature."  And they linked to Microsoft's announcement of this.  It was the original posting.  And one thing's not clear about, from a UI standpoint, what'll be changing.  But so here's what we know about what Microsoft is explaining that they'll be bringing to the Microsoft Edge Secure Network.



They said:  "Encrypts your connection.  Encrypts your Internet connection to help protect your data from online threats like hackers.  When using Microsoft Edge Secure network, your data is routed from Edge through an encrypted tunnel to create a secure connection, even when using a non-secure URL that starts with HTTP.  This makes it harder for hackers to access your browsing data on a shared public WiFi network."  We know of course that's true, and that's probably the strongest use case for using a VPN, if you're not using one to dial into your corporate network.



Second feature:  "Helps prevent online tracking."  Right.  "By encrypting your web traffic directly from Microsoft Edge, we help prevent your Internet service provider from collecting your browsing data like details about which websites you visit."



Third:  "Keeps your location private.  Online entities can use your location and IP address for profiling and sending you targeted ads.  Microsoft Edge Secure Network lets you browse with a virtual IP address that masks your IP and replaces your geolocation with a similar regional address to make it more difficult for online trackers to follow you as you browse."



And finally:  "Is free to use.  Get 1GB of free data every month when you sign into Microsoft Edge with your Microsoft Account.  A few early adopters will be in a data upgrade trial.  At the end of their 30-day trial period, the experience will reflect the normal VPN gigabyte limits."  I don't know what that last sentence meant.



Okay.  But not surprisingly, my Edge browser doesn't have it yet.  You know, I'm not in any advanced bleeding-edge mode.  But under Edge's main menu near the menu's bottom, you'll find current entries for "Read aloud" and then "More tools."  And assuming that Edge's UI hasn't changed since this posting was last updated, and the rest of it does look the same, there will appear a new "Secure network" entry in between "Read aloud" and "More tools."



The other piece of interesting news is that this is being done in affiliation with our friends at Cloudflare.  Microsoft wrote:  "Microsoft Edge Secure Network is a service provided in partnership with Cloudflare.  Cloudflare is committed to privacy and collects a limited amount of diagnostic and support data acting as Microsoft's data subprocessor in order to provide the services.  Cloudflare permanently deletes the diagnostic and support data collected every 25 hours."  Now, they didn't say that Microsoft doesn't collect it and won't retain it.  We don't know.  But they are saying Cloudflare's not keeping it.  And presumably Microsoft does not either.



They said:  "To provide access, we store minimal support data and access tokens which are only retained for the duration of the required service window.  A Microsoft account is required to access Microsoft Edge Secure Network and is retained to keep track of the amount of Microsoft Edge Secure Network data that is used each month.  This data retention is necessary to provide 1GB of free Microsoft Edge Secure Network service and to indicate when the data limit has been reached."



So I don't really have any calibration on how quickly 1GB will be consumed, but that doesn't sound like much data for a month.  I checked my phone, which I don't use very much for any heavy work because I'm always sitting in front of a computer.  And I have the "small" Verizon plan which is limited to 2GB a month.  As I said, I'm not doing much with my phone.  Turns out I've used than 0.3GB per month for the past three months.  So I'm not a heavy data user on my phone.  I expect that this might be something that's used sparingly and only when necessary.  And, you know, you're not going to watch movies over your 1GB of free Microsoft Edge VPN data.



Their UI does have a "bytes used so far this month" meter, so it'll be possible to track one's usage and get a sense for how it's going and whether you need to scale back and so forth.  Anyway, overall this seems like a useful and welcome feature.  It's limited, but it's free, and you just need to be logged into Edge or Microsoft through Edge, which I imagine Edge users would be.  So it'll be there in a pinch.



Okay.  LastPass Incident Update.  Yesterday, LastPass provided by far more detail about that second more devastating attack that they suffered.  And that's of course the one that inspired the Leaving LastPass podcast and the one that followed that was just titled the numeral "1" when we found out that that's what iteration counts in some cases had been left at.  And I have to admit that the forensics which were presented were impressive.  This doesn't forgive them in any way from screwing up in the several other ways that we know they did.  But as far as forensics examinations go, it's impressive.  It's easy to tell a story in retrospect.  But as I'm describing what they have determined actually happened, imagine figuring this out.  That's, again, as I said, it's impressive.



So I have a link to the incident details.  I'm not going to cover all of it because it goes into way more detail than we need.  But what they wrote was, LastPass has now learned and explained to us, they said:  "To access the cloud-based storage resources, notably S3 buckets which are protected with encryption, the threat actor needed to obtain AWS Access Keys and the LastPass-generated decryption keys.  The encrypted cloud-based storage devices house backups of LastPass customer and encrypted vault data."  Right, and that's what got away from them.



They wrote:  "As mentioned in the first incident summary, certain LastPass credentials stolen during the first attack were encrypted, and the threat actor did not have access to the decryption keys, which could only be retrieved from two locations:  First, a segregated and secured implementation of an orchestration platform and key-value store used to coordinate backups of LastPass development and production environments with various cloud-based storage resources."  That's the first place where the keys were.  "Or a highly restricted set of shared folders in a LastPass password manager vault that are used by DevOps engineers to perform administrative duties in these environments."



Okay.  They said:  "Due to the security controls protecting and securing the on-premises data center installations of LastPass production, the threat actor targeted one of the four DevOps engineers who had access to the decryption keys needed to access the cloud storage service.  This was accomplished by targeting the DevOps engineer's home computer and exploiting a vulnerable third-party media software package, which enabled remote code execution capability and allowed the threat actor to implant keylogger malware.  The threat actor was able to capture the employee's master password as it was entered, after the employee authenticated with multifactor authentication, and gain access to the DevOps engineer's LastPass corporate vault.



"The threat actor then exported the native corporate vault entries and content of shared folders, which contained encrypted secure notes with access and decryption keys needed to access the AWS S3 LastPass production backups, other cloud-based storage resources, and some related critical database backups."  So yes, Leo, as you gasped, this was quite an attack.  I mean...



LEO:  And I saw a rumor that it was Plex, that it was a flaw, or they thought it might be a flaw in Plex, which this engineer had been running.  So this was a very sophisticated attack.  I mean, not just targeted, but they were rooting around in this guy's machine and able to find another flaw.



STEVE:  Yup.  And of course Plex is a media software package, so that tracks with that rumor that you heard.



LEO:  Yeah, yeah.  And it was online because people often put their Plex servers on the public Internet.  So that's, yeah, therein lies your problem; right?



STEVE:  So as I'm reading this, I'm thinking, wow, no one wants  these guys on their tail.



LEO:  No kidding, yeah.



STEVE:  Because wow.  Okay.  So listen to the steps LastPass has since taken in an effort to recover from this attack.



LEO:  Well, they should get Kolide is the first thing they should do.



STEVE:  Yeah.



LEO:  Okay.  Go ahead.



STEVE:  Yeah.  Remember how last week I was talking about how difficult it would be to ever be able to trust anything ever again.



LEO:  Yeah.



STEVE:  So they wrote...



LEO:  Zero trust.



STEVE:  Uh-huh.  "As we progress through incident response, and as part of our ongoing containment, eradication, and recovery activities related to the second incident, we have performed the following actions, with additional work currently being accomplished in scoping and planning."  In other words, they're not even done yet, as of yesterday.



So they said:  "With the assistance of Mandiant, we forensically imaged devices to investigate corporate and personal resources and gather evidence detailing potential threat actor activity.  We assisted the DevOps engineer with hardening the security of their home network and personal resources.  We enabled Microsoft's conditional access PIN-matching multifactor authentication using an upgrade to the Microsoft Authenticator application which became generally available during the incident.  We rotated critical and high-privilege credentials that were known to be available to the threat actor."  Well, okay, good.



"We continue to rotate the remaining lower priority items that pose no risk to LastPass or our customers."  Again, good.  "We began revoking and re-issuing certificates obtained by the threat actor.  We analyzed LastPass AWS S3 cloud storage resources and applied or started to apply additional S3 hardening measures.  We put in place additional logging and alerting across the Cloud Storage environment with tighter IAM (Identity and Access Management) policies enforced.  We deactivated prior development IAM users."  In other words, that had not been done before, so good.  "We enabled a policy that prevents the creation and use of long-lived development IAM users in the new development environment."  So that's good.  They're changing policies and tightening security when they looked and realized, oh, crap, we've got a bunch of ex-development users who still have credentials here.



Then they said:  "We rotated existing production service IAM user keys, applied tighter IP restrictions, and configured policies to adhere to least privilege.  We deleted obsolete service IAM users from the development and production environments.  We're enabling IAM resource tagging enforcement on accounts for both users and roles with periodic reporting on non-compliant resources.  We rotated critical SAML certificates used for internal and external services.  We deleted obsolete/unused SAML certificates used for development, services, or third parties."  And again, that hadn't been done previously.  Good that they did it.  Need to make that a policy.



"We revised our 24/7 threat detection and response coverage, with additional managed and automated services enabled to facilitate appropriate escalation."  And, finally, "We developed and enabled custom analytics that can detect ongoing abuse of AWS resources."



So, okay.  It's evident that things are way better now than they were before, which, you know, that's always sort of been the double-edged sword of this is do you trust better somebody who's learned from their mistake.  There's obviously lots of, if I may use the term, "learnings" here.  On the other hand, they were clearly doing some things wrong by policy.  And that's difficult to forgive, and that's what finally caused us en masse to leave LastPass.



LEO:  Well, and also do you want those four DevOps guys who have  the keys to be able to bring those home?  I guess you do.  Now, this all because of COVID, probably; right?



STEVE:  Yeah.  And he was probably VPNing in.  One of the things I noticed that I commented on here in passing is they said "We rotated existing production service user keys, applied tighter IP restrictions."  [Buzzer sound]  If they didn't have any, then Russia could have connected to their S3 buckets; right?



LEO:  Right, right.



STEVE:  So it makes absolute sense to allow, first of all, the IP of somebody VPNing will be the same as the corporate IP network.



LEO:  Right.



STEVE:  So if you weren't VPNing, your IP would be at home.  But at least you'd be in the vicinity.  So again, the problem we've always had is this tendency not to want support calls; right?  It's like, no, we don't want tech support calls.  So if we don't default to things being open, then we're going to get complaints when things don't work.



LEO:  Right.



STEVE:  So we're still sort of in that mode.  And also, looking at this list of stuff, one of the bugaboos of evolution over time, and LastPass has been around for a long time, is that things tend to become more complicated over time.



LEO:  Yeah.



STEVE:  This is usually driven by inevitably changing requirements.  New systems are added to improve or to enable some new job.  But the new system doesn't completely take over for the old one.  So that older system needs to still stay around to do some of those few things that the newer system doesn't quite do the same.  Then the requirements change again, and some customizations are required.  Some glue code is created by somebody who then later quits and takes his notes and knowledge with him.  Now no one wants to touch that weird box in the corner since no one's quite sure how it works.



And that's the way this, I mean, this actually happens in the real world.  And Leo, I've heard you at the beginning of some of the podcasts recently, like trying to figure out how to - I don't know what it is that's going on over there.  But, you know...



LEO:  Oh, boy.  I don't either.



STEVE:  Exactly.  That's what happens.  You know?



LEO:  I used to know how everything worked here.  Now I know nothing.



STEVE:  Anyone who's been working within a complex environment with many players and constant time pressure, where needs are dynamically changing, will probably be able to relate to this sort of mess that winds up evolving from what was originally a simple solution.  And so my point is, in the context of security, this sort of creeping, evolving complexity makes both keeping things truly secure and recovering rapidly from an incident, if one happens, much more difficult.



And it occurred to me that there really needs to be somebody who is assigned the task of stepping back from the day-to-day fray to take sort of a holistic view of an enterprise's systems and be constantly working to reintegrate the inherently disintegrating systems that just naturally form.  Keeping things as simple as possible has tremendous benefits for an organization, and in a sufficiently large organization I think it really ought to be a job title.  There ought to be like a job title like, I don't know, Holistic System Reintegrator or something.  He could, like, not shave and dress funny because that would sort of fit the title.  But really, somebody who's working against these otherwise sort of natural forces of entropy which tend to disintegrate things over time.



Okay.  Exactly as we predicted, three days ago BBC News headlined their coverage "Signal would 'walk' from UK if Online Safety Bill undermined encryption," with the subhead "The message-encrypting app Signal has said it would stop providing services in the UK if a new law undermined encryption."  Signal's president, Meredith Whittaker, told the BBC that if they were forced to weaken the privacy of their messaging system under the Online Safety Bill, the organization "would absolutely 100% walk."



And of course the government said that its proposal is not a "ban on end-to-end encryption."  But the bill which was introduced by Boris Johnson is currently going through Parliament.  And as we recently covered in detail, under the revisions proposed by this new legislation, companies would be required to scan messages on encrypted apps for child sexual abuse material, language suggestive of "grooming," or terrorism content.



WhatsApp previously told the BBC that it would also refuse to lower its "security," and I put that in air quotes, for any government.  In the case of WhatsApp, the question might come down to the definition of the term "security."  But the folks behind Signal are likely to be far more clear about that.  The BBC's coverage reminds us that the government and prominent child protection charities have long argued that encryption hinders efforts to combat online child abuse, which they say is a growing problem.



The UK's Home Office said in a statement:  "It is important that technology companies make every effort to ensure that their platforms do not become a breeding ground for pedophiles."  The Home Office added:  "The Online Safety Bill does not represent a ban on end-to-end encryption, but makes clear that technological changes should not be implemented in a way that diminishes public safety, especially the safety of children online."



LEO:  Children.  Think of the children.



STEVE:  That's right.  "It is not a choice between privacy or child safety.  We can and we must have both."  Right.  We can, because we say we can.  We're willing to go as far as to change the definitions of words in order to have both the safety of our children and total privacy for everyone, even though we may need to change the meaning of "total," but just a little bit.  That pesky math is so annoyingly absolute.  After all, we create laws.  That's what we do.  Unfortunately, not the laws of nature or of mathematics.  But still, this is what we want, and we're used to getting our way.



So the UK's child protection charity, the NSPCC, said in reaction to Signal's announcement:  "Tech companies should be required to disrupt the abuse that is occurring at record levels on their platforms, including in private messaging and end-to-end encrypted environments."  But the digital rights campaigners on the other side, the Open Rights Group, said this highlighted how the bill threatened to "undermine our right to communicate securely and privately."



Signal's Ms. Whittaker said back doors to enable the scanning of private messages would be exploited by malignant state actors and create a way for criminals to access these systems.  When asked if the Online Safety Bill would jeopardize Signal's ability to offer a service in the UK, she told the BBC:  "It could, and we would absolutely 100% walk away rather than ever undermine the trust that people place in us to provide a truly private means of communication."



LEO:  Good.  Right on.



STEVE:  She said:  "We have never weakened our privacy promises, and we never will."  Period.



LEO:  Good.



STEVE:  Yup.  Matthew Hodgson, chief executive of Element, a British secure communications company, said that the threat of mandated scanning alone would cost him clients.  He argued that customers would assume any secure communication product that came out of the UK would necessarily have to have backdoors in order to allow for illegal content to be scanned.  Matthew added that it could also result in a very surreal situation where a government bill might undermine security guarantees given to customers at the Ministry of Defense and other sensitive areas of government.  He said that his firm might have to cease offering such services.  And that raises a great point.  Would the most sensitive users within the government also be consenting to having all of their communications intercepted, scanned, and possibly forwarded to a central clearinghouse for human oversight?  That seems unlikely.



As for child safety, Signal's Ms. Whittaker said:  "There's no-one who doesn't want to protect children.  Some of the stories that are invoked have been harrowing."  When asked how she would respond to arguments that encryption protects abusers, Ms. Whittaker pointed to a paper by Professor Ross Anderson, which argued for better funding of services working in child protection and warned that "the idea that complex social problems are amenable to cheap technical solutions is the siren song of the software salesman."



LEO:  Wow.  Wow.



STEVE:  Yeah.  "The idea that complex social problems are amenable to cheap technical solutions is the siren song of the software salesman."  So there's no question that the issue of child safety is real.  But terrorism content was also mentioned.  And doesn't everyone also appreciate that no government, no matter how respectful of its citizens' inherent and often constitutionally guaranteed privacy rights is comfortable with not having some capability for oversight over its citizenry when it believes that such might be needed.  As I've noted of our own constitutional government in the U.S., the Constitution's guarantee for privacy is conditional.  Courts are able to issue search warrants when presented with probable cause.



We've been watching the approach of this slow-motion collision for years.  So it's going to be very interesting to see how this all shakes out.  We know that Signal and Telegram and Threema will not capitulate.  There's just no way they would.  But it's difficult to see how Meta's various services, Google with Android, and Apple may not be forced to go along rather than lose access to those huge markets.  Apple already demonstrated a willingness to find some compromise by performing that local fuzzy hash scanning.  Of course the backlash from that was epic.



Ultimately, I think governments are probably going to win this legal battle since they're the ones who write the laws, and thus it's possible for them to delineate what behavior is legal and what is not.  At that point, any use of fully secure end-to-end encrypted solutions will be outlawed, at which point, as we know, only outlaws will be using them.



LEO:  So download it now so you, too, can be an outlaw.



STEVE:  That's right.  Get your soon-to-be-illegal-to-use software.



LEO:  Yeah, yeah.



STEVE:  I do know some people, actually, who have purchased guns because they were worried that they were soon going to be outlawed.  It's like, no, no, you don't have to worry about that in the U.S.



LEO:  Not in the U.S., baby.



STEVE:  Okay.  Okay, more PyPI troubles.  Remember last week how I felt that I needed to at least mention the continuous background of ongoing attacks on open source repositories and registries?  Well, last Friday the security firm Phylum posted some news regarding the PyPI, you know, the Python Package Index.  Their posting's headline was "Phylum Discovers Aggressive Attack on PyPI Attempting to Deliver a Rust Executable."



They wrote:  "On the morning of February 23rd, 2023, Phylum's automated risk detection platform started lighting up with another series of strange publications on PyPI.  After digging into it, we were able to link it to another smaller campaign from January, last month.  First," they said, "we can confirm that this is an ongoing attack.  As we worked on this write-up we saw the list of packages published go from a few dozen to over 500."



LEO:  Ugh.



STEVE:  I know.  "The most recent packages appear to be getting published at around one every four to eight seconds, so we suspect that this may continue for some time.  You can look at the Package Publication section at the bottom of this post to see the packages we've seen.  As of the publication of this post we've already seen 1,138 malicious packages published."



Anyway, they go on to explain in detail the nature of the malware.  The short version is that the malicious packages connect to a Dropbox account to download and install a Rust-based malware strain.  Phylum says the attacker appears to be the same group that was previously spotted by Fortinet and ReversingLabs the week before in a separate, smaller campaign.



You know, as I was saying last week when we talked about this, our open source repositories are now under more or less constant attack.  And Leo, as you commented last week, the industry needs to come up with some solution to the poisoning of our open repositories.  The only solution I can see is a future where Internet identity and reputation can be rigorously established and verified.  And I know people love the freedom of using synthetic online identities and monikers.  And I think that's 100% fine, so long as they don't also want the benefits that accrue from being known and trusted.  At least at the moment it's unclear how it's possible to have both.  At the moment we have neither.  Again, I just, you know, we want this stuff to be open, but why we can't have nice things.



In the interest of giving credit where it's due, the often-in-the-dog-house Taiwanese hardware vendor QNAP, right, the manufacturer of the always apparently in trouble NAS devices which are exposed to the Internet, on Friday announced the launch of its own bug bounty program.  Yay.  Vulnerabilities relating to QNAP operating systems, applications, and cloud services are all in scope, and rewards can go up to U.S. $20,000.



They do still need to find some way of keeping their devices patched when problems are found; but as we've just seen with VMware, they're not alone in having that problem to solve.  I think that tomorrow's systems, that is, globally, one way or another are all going to have to phone home just as our consumer operating systems have all been doing for some time.  And there will either have to be an autonomous upgrade and reboot facility, or some reliable notification path to the device's administrators.  This problem needs to get solved.  But finding and fixing those problems comes first; and QNAP's bounty, while not huge, is a clear step in the right direction.  And Leo, let's talk about the Club for a minute.



LEO:  I can tell.



STEVE:  Yeah, I'm going to take a break.



LEO:  I know when you get a little thirsty, yeah.



STEVE:  And then we're going to talk about an unbelievable bug which we have found in motherboard BIOSes, which is going to necessitate a change in SpinRite.  And then we're going to talk about what the NSA tells us we should do at home.



LEO:  Okay.  Okay.  Somebody's got to pay for Steve's hydration.  Go right ahead.



STEVE:  So I haven't talked about SpinRite at all for several weeks because I didn't have any significant news to share.  But as a result of the work and discoveries over the past week, I have news today which will more than make up for my previous weeks' silence.  A major mystery which had been stymieing the project for weeks has been solved.  For the past many weeks I've been tracking down, as I have mentioned last time I talked about SpinRite, various sources of SpinRite crashes.  I've discovered various sorts of misbehavior from DOS and motherboard BIOSes.  They've been altering values in registers that they assume wrongly that others won't be using.  You know, that's against every rule of good citizenship.  You always put things back the way you found it when you're done using them.  But in this case these things weren't.



The only way I can explain this behavior is that they're using, for example, the upper half of 32-bit registers, figuring that in a 16-bit environment no 16-bit app would notice.  But SpinRite is now largely 32-bit code, and it makes constant use of all of those extra 32-bit resources.  And in other cases they're just not bothering to preserve any part of a register.  So a lot of the work I've been doing recently has been defensive computing.  Someone suggested that term in our development group; and I thought, well, maybe survival computing is the better term because, I mean, I have to do it in order to keep SpinRite running.



So SpinRite already works without trouble for nearly everyone.  But for those for whom it does not work, they have my attention because I'm never sure whose fault the problem is, and I need at least to determine that.  Is it something I'm doing that I need to fix?  Or is it something outside of my control?  And there's no reason SpinRite shouldn't be able to protect itself from anything that a system might throw at it, although that was recently challenged.



One Canadian SpinRite tester, Andre, was able to get SpinRite to crash for him reliably.  He had a system with a couple of internal drives and a 160GB USB drive connected.  The USB drive was being marked RED by SpinRite, which is a new feature.  There's a whole, I mean, SpinRite 6 users are going to - they'll recognize it; but boy, I mean, the last several years have really changed SpinRite, moving it from 6 to 6.1.



Anyway, the USB drive he had was being marked RED, meaning that during its initial appraisal of the drive, SpinRite had found something that wasn't right.  That's something that we've been seeing with drives that are quite near death.  When the user attempts to select the drive for use, they'll receive a pop-up explanation of exactly what's wrong and, when possible, be given the option to proceed to use that drive anyway.



But that 160GB drive was being marked RED, that wasn't the problem.  The problem was that shortly after enumerating those three drives, SpinRite would intercept its own attempt to execute an illegal x86 processor instruction, an illegal opcode.  That should never happen.  I don't recall what made me suspicious, but I first asked Andre just to try unplugging that damaged USB drive.  And sure enough, no crash.  Then, with that drive reattached, I provided Andre with an old-school DOS utility called "eatmem."  Eatmem simply consumes some amount of RAM memory and then drops back to DOS.  Back in the day, this was used to stress-test programs by subjecting them to limited memory situations.  But it also has the side effect of changing the location in RAM where DOS will load subsequent programs, since it eats memory from the bottom up.



And sure enough, by "eating" various amounts of memory before running SpinRite, SpinRite's crashing behavior would not occur, or it would occur differently, even with that 160GB RED marked drive connected.  Around this same time, one of our SpinRite testing participants, a guy named Paul Farrer, who also knows how to write DOS programs, was experimenting on his own with a similar crash that he and another user were both seeing.  That other user had attached a 1TB USB drive to his machine, and it was crashing SpinRite when he tried to run it.



Paul hypothesized that the trouble might be caused by SpinRite attempting to read above the 137GB region of a drive.  I have no idea how that occurred to him, but it turned out to be prescient.  137GB is one of those many size limitations that we as an industry were constantly plagued with during the PC industry's early growth.  Over and over and over, I mean, it's almost comical in retrospect, we kept outgrowing every upper limit that we assumed would never be exceeded.



The classic story was that the 16-bit Apple II had a maximum of 64KB of main RAM memory - yes, 64K of main RAM memory.  And when the IBM PC came out with its initial maximum of 640 kilobytes - so exactly 10 times as much as the Apple II - the story was that during a trade show in 1981, Bill Gates said:  "We'll never need more than 640K."  Now, today, Bill doesn't recall ever having said that.  But it's apocryphal in the industry.  Whether or not he did, although today it may seem ludicrous, I can easily imagine, having been active in the PC industry back at the time,  that it was seemingly reasonable to say that.  And that's the point.  These were always reasonable-seeming limitations because none of us who were in the middle of this could have foreseen what has happened since.



The early IDE drives had sizes in the hundreds of megabytes or maybe a few gigabytes if, you know, like you could get some of those, and they were really expensive, like a 4GB drive.  So the designers of those drives repurposed the addressing bits which had been used by the original cylinders, heads, and sectors registers to scrounge up a total of 28 bits that they could use to linearly address the sectors on an IDE drive.  This was called LBA for Linear Block Addressing.  The use of those 28 bits to address sectors meant that a drive could have at most 2^28 total physical sectors.  Since sectors were 512 bytes each, that meant that the maximum size of those 28-bit LBA drives would be 137GB.  But back then, 137GB, no one would EVER be able to create a drive that large, let alone have that much data that needed to be stored.  I mean, come on.  That's 137 billion with a "B."  Whoops.



So Paul's intuition was that SpinRite's code was somehow being corrupted when it attempted to access sectors at the end of the drive, if a drive was larger than 137GB, like that 160GB drive that Andre had was.  And accessing sectors at the end of the drive is one of the things that SpinRite does when it's sizing up a drive before listing it for use by its user.  So Paul and I each independently wrote testing utilities to better understand what was going on.  And what we discovered over this past weekend was a bit astonishing.



I ended up writing two utilities.  The first one was called "BIOSTEST."  BIOSTEST first filled all of the system's main RAM memory that wasn't already in use by DOS and the BIOS and buffers and the program itself, with a deterministic pseudorandom pattern.  Then it simply used the system's BIOS to read several sectors from the front of the drive and from the end of the drive.  After each test read it would rescan all of main memory looking for the first mismatching of data.  It was looking to see whether reading any sector from a drive through the BIOS would cause the BIOS to alter main memory.  And sure enough, after reading the last sectors of larger than 137GB USB drives on some motherboards, it found main memory mismatches.  Reading sectors from the front of the drives never caused any problems.  But reading any sectors whose linear address had more than 28 bits would actually damage the data stored in main memory.  The USB support in those BIOSes was seriously buggy.



During what SpinRite calls "drive discovery and enumeration," it goes out to the very end of every drive it can locate to perform reading and writing confirmation and confidence testing.  It's safest to do that out there, out at the end, because the very ends of drives are usually empty and don't contain any user data.  In fact, partitions don't tend to go all the way out to the end.  They have like a shorter wrap factor to be aligned.  But when SpinRite was doing that on USB-connected drives on motherboards with those broken USB BIOSes, bugs in the BIOS were blasting SpinRite's code, which was then causing it to crash.



Whereas BIOSTEST checked a few sectors at the front and back of every BIOS drive, the second utility I wrote, BIOSSCAN, read every sector of a user-specified BIOS drive.  It also filled memory with a pseudorandom test pattern, then it would scan the entire drive from front to back, while rescanning main memory.  Anyway, I have a bunch more of this story in the show notes.  I've taken up enough of everyone's time.  We found a bad bug that some BIOSes have which occurs when an attempt to read the end of drives larger than 137GB happens.



So we've fixed the problem.  We're going to clamp SpinRite's access at 137GB for USB-connected drives until we get to SpinRite 7 because it is not safe for SpinRite to go any further than that.  There's no reliable way to test whether a BIOS is buggy or not.  They're scattered around the SpinRite users that we have, and this is even though it's a large bunch of users, it's a small sample size.  So we're going to get SpinRite 6.1 finished.  It's one less source of crashing now exists.  And the first thing I'm adding to SpinRite 7 is native support for USB host controllers, which will then release this temporary limit on USB drive size and also dramatically speed up SpinRite's ability to use USB drives in the same way that it has sped up SATA and parallel drives, running them now like at their absolute maximum speed they're able to function.



So it was an interesting adventure.  With the help of lots of testing we were able to track down a bizarre problem which has gone unseen in the industry.  One person did find some reference to a boot manager saying that you should use it at the beginning of drives or not on drives larger than this 137GB because of some bugs that were in BIOSes.  But it certainly hasn't been widely known.



I did want to mention an interesting programmer-oriented Humble Book Bundle.  We've talked about those in the past.  And in fact I found that, because it's got a crazy long URL, I assigned it one of GRC's shortcuts, and I decided to use the shortcut "bundle," B-U-N-D-L-E, which had been taken.  I used it before in 2021 on a different Humble Bundle.  So we're reusing the same shortcut.  It's an 18-book Humble Bundle of programmer-related books.



So anyway, take a look at it.  There's a bunch of programming books by Randall Hyde, who's legendary in programming circles, and a bunch more.  So grc.sc/bundle, B-U-N-D-L-E, if you are interested.  And Leo and I were talking about this before we began recording.  They're not absolutely amazing books, but there are 18 of them.  You pay what you want for them.  The money goes to support the EFF.  And there are some good ones in the collection, as is often the case.



LEO:  This "Think Like a Programmer" is a classic, and it's got that assembly language book, too, which you like. 



STEVE:  Yes, "The Art of Assembly Language."



LEO:  And this, if you want to learn Clojure, everybody loves this book.  Although I think this is online for free.  This is all No Starch Press.  They do great stuff.  I really love No Starch Press's stuff.  I have a lot of their books.



STEVE:  Yup, grc.sc/bundle.  Okay.  So surprisingly, the NSA has offered some home security "Best Practices" advice.  Last Wednesday the National Security Agency, our NSA, published an attractive and end-user accessible nine-page PDF loaded with tips for helping to secure a home network environment.  And Leo, it's our shortcut of the week, so if you want to scroll through it on the screen while I'm talking about it, grc.sc/912, grc.sc/912.



Anyway, it is really good.  And I want to share and comment on some of what the NSA has suggested.  Our longtime listeners will feel right at home, so to speak, with everything that the NSA wrote.  They led off with three major key points:  Upgrade and update all equipment and software regularly, including routing devices; exercise secure habits by backing up your data and disconnecting devices when connections are not needed; and limit administration to the internal network only.  These three points are actually pulled out of the larger piece.  But, you know, clearly, keep your stuff up to date.  Do secure things.  And do not open anything to remote administration.



They also said, and I thought this was interesting:  "IoT devices on a home network are often overlooked, but also require updates.  Enable automatic update functionality when available.  If automatic updates are not available, download and install patches and updates from a trusted vendor on a monthly basis."  So it's interesting that the NSA, they, too, see the threat posed by our out-of-date or defective IoT devices.  Of course the question is often, who are you going to call you to update some random IoT light switch or wall plug?  But moving forward, it would be good to see future devices based on open standards and platforms, and for there to be some sort of certification systems in place.  We have a long way to go, but such work is underway.



Okay.  And this one was interesting.  They wrote:  "Your Internet Service Provider may provide a modem/router as part of your service contract.  To maximize administrative control over the routing and wireless features of your home network, consider using a personally owned routing device that connects to the ISP-provided modem/router, in addition to modem/router features to create a separate wireless network for guests, for network separation from your more trusted and private devices."



Okay.  Now, that was a little bit surprising.  They're saying even if your Internet service provider offers a modem/router as part of the service package, get your own that you control and manage, and use it to connect your network to the provider's bandwidth.  Again, some sound advice.  And on router guidance they say:  "Your router is the gateway to your home network.  Without proper security and patching, it is more likely to be compromised, which can lead to the compromise of other devices on your network, as well.  To minimize vulnerabilities and improve security, the routing devices on your home network should be updated to the latest patches, preferably through automatic updates.  These devices should also be replaced when they reach end-of-life for support.  This ensures that all devices can continue to be updated and patched as vulnerabilities are discovered."



Okay.  How many times have we seen companies explaining that they won't be offering updates to fix known critical remote code execution problems for older devices because they are EOL, you know, end-of-life, so anyone still using those devices is SOL.  And we've often seen, like, inventories of these end-of-life devices still out, exposed on the public Internet, and they're never going to get patched.  When selecting a router, this suggests an important criteria that's easily overlooked, and that's the active and supported service life that has historically been provided by various competing vendors.



If this criteria were to become a popular advertised selection, it would put more pressure on vendors to keep older devices supported longer, even though it might mean reduced sales in the future due to the longevity of previous products which were still supported and going strong.  I don't know that anybody is actually yet buying a new device to replace a working old device simply because the old device is no longer receiving updates.  I don't think most people even know whether devices are receiving updates or not.  But I appreciate the NSA saying, you know, if a device you've got is so old that it is no longer receiving updates, and if a problem were found that could never be fixed, and if you care about security, these things aren't that expensive anymore.



The NSA also talked about WPA3.  We briefly touched on this next-generation Wi-Fi 6 and WPA3 encryption, but we haven't yet given it a deep dive, and it's probably time for us to do so.  It's had a somewhat slow liftoff, since the Wi-Fi Alliance's WPA certification process started back in 2018, so between four and five years ago.  But Wi-Fi 6 and WPA3-capable devices are here now.  So we'll get around soon to doing a podcast.



Here's what the NSA wrote.  They said:  "To keep your wireless communications confidential, ensure your personal or ISP-provided WAP is capable of WiFi Protected Access 3.  If you have devices on your network that do not support WPA3, you can select WPA2/3 instead.  This allows newer devices to use the more secure method while still allowing older devices to connect to the network over WPA2.



"When configuring WPA3 or WPA2/3, use a strong passphrase with a minimum length of 20 characters.  When available, protected management frames should also be enabled for added security.  Most computers and mobile devices now support WPA3 or 2.  If you're planning to purchase a new device, ensure that it is WPA3-Personal certified.  Change the default service set identifier (SSID) to something unique.  Do not hide the SSID as this adds no additional security to the wireless network and may cause compatibility issues."  All of that is true.  I was very impressed as I was reading through this that, you know, the degree to which the NSA got it.



So as I said, we'll do a Wi-Fi 6 podcast soon.  Seeing this next one raised an eyebrow since everyone knows that I worry about the day a widely used IoT device goes rogue.  The NSA wrote:  "Implement wireless network segmentation."  They said:  "Leverage network segmentation on your home network to keep your wireless communication secure.  At a minimum, your wireless network should be segregated between your primary WiFi, guest WiFi, and IoT network."



LEO:  Wow.  You've been saying this for years.



STEVE:  Yup.



LEO:  But it's interesting to hear them recommend this to normal people because I don't think normal people know how to do this.



STEVE:  I know.  I know.  And they finish, saying:  "This segmentation keeps less secure devices from directly communicating with your more secure devices."  As we know, I've been promoting multi-NIC routers which are able to do that.  And some of the more recent WiFi routers are beginning to offer stronger segmentation options, as well.  So I would say that's something to look for when you're shopping for a new WiFi router is check out and see if it offers built-in segmentation.  That will make it a lot more accessible to average users.



LEO:  What do they call it?  Is there a name that they give it?



STEVE:  That's a good question.  I think that the ASUS router I'm using, it definitely has a guest WiFi.  But I think it has multiple guest WiFis.  And so you could, and you are able to, keep them from talking to each other.  So that would mean you could give your second guest WiFi, use that for your IoT stuff.



LEO:  Okay.  The problem I think a lot of people will have is that they can't then use devices on the main network to control the IoT devices.  And you and I and our audience probably knows how to do sophisticated firewall rules to allow that.  But you have to know that's really, you know, Black Diamond stuff; right?  That's pretty advanced.



STEVE:  Yeah.  Yeah.  But again, it was amazing to see the NSA saying this.  And our users get a little additional impetus behind that, too.



LEO:  Well, and it looks like everything in this NSA document you would agree with 100%; right?



STEVE:  Yes, absolutely.  In fact, there's something I've never recommended that I agree with that we'll get to in a second.



LEO:  Oh, oh.



STEVE:  So what about the presence of personal assistant technologies and worries over eavesdropping?  Well, not surprisingly, the NSA is not a big fan of things with microphones.



LEO:  Unless it's theirs.



STEVE:  Unless they're on the other end, exactly.  So they wrote:  "Be aware that home assistants and smart devices have microphones and are listening to conversations, even when you are not actively engaging with the device.  If compromised, the adversary can eavesdrop on conversations.  Limit sensitive conversations when you're near baby monitors, audio recording toys, home assistants, and smart devices.  Consider muting their microphones when not in use."



LEO:  Wow.



STEVE:  "For devices with cameras - laptops, monitoring devices, and toys - cover cameras when you're not using them.  Disconnect Internet access if a device is not commonly used, but be sure to update it when you do use it."  So I got a kick out of that one.  And all that security advice falls nicely under the umbrella of generally sound, if maybe a little paranoid, security advice.  Following that, under the topic of general security hygiene, they add:  "To minimize ransomware risks, back up data on external drives or portable media.  Disconnect and securely store external storage when not in use."  You know, take it offline.



"Minimize charging mobile devices with computers; use the power adapter instead.  Avoid connecting devices to public charging stations.  Leave computers in sleep mode to enable downloading and installing updates automatically.  Regularly reboot computers to apply the updates.  Turn off devices or disconnect their Internet connections when they will not be used for an extended time, such as when going on vacation."



In other words, think security at all times, and try to never take it for granted.  It's sort of the broader equivalent of what has happened to email, where it's no longer ever safe, unfortunately, to assume that all email is legitimate and that links can be clicked on without careful scrutiny.  It's a sad state, but it's the state we're in.



And everyone knows that I love this one:  "Limit administration to the internal network only."  They said:  "Disable the ability to perform remote administration on the routing device.  Only make network configuration changes from within your internal network.  Disable" - this is them.  "Disable Universal Plug-n-Play.  These measures help close holes that may enable an actor to compromise your network."  And Leo, I do kind of wonder maybe if they listen to the podcast.  



And there was one piece of advice that makes sense, but I have never recommended.  They said:  "Schedule frequent device reboots."  They wrote:  "To minimize the threat of non-persistent malicious code on your personally owned device, reboot the device periodically.  Malicious implants have been reported to infect home routers without persistence.  At a minimum, you should schedule weekly reboots of your routing device" - that's a little often, but okay - "smartphones, and computers.  Regular reboots help to remove implants and ensure security."  I mean, it's true.  They do.



What's interesting about this advice is that, as we know, many forms of malware are RAM resident only.  They never write anything to non-volatile media.  Some routers are almost never rebooted, so malware authors probably figure that there's no reason to bother writing it to non-volatile memory and arranging to get it to start where it's a little more visible in the startup script.  And we know that in well-protected environments, writing to disk can trip all sorts of monitoring alarms.  And some malware might want to disappear after a reboot so that its larger network of devices can remain hidden.  So if it's not necessary for something to survive a reboot, malware might well choose not to.



Consequently, indeed, a reboot will permanently flush RAM-based malware from the system.  Okay.  Now, if the way such malware originally got into the system in the first place and then obtained its foothold in RAM, if that's not closed off and resolved, then it might come back before long.  But, yeah, reboots are inherently cleansing.  I think that was a great point, and it's one I've never talked about before.



So okay.  Those were just some of the highlights that I thought were the more interesting, a little bit surprising in some cases, and insightful.  But there is much more than those in the nine-page document than what I've just shared, and the entire document is so good that I think everyone listening would benefit from at least scanning and probably also by recommending it to others.  It has the additional pedigree of bearing the official seal of the National Security Agency, which might help everyone's non-Security Now! listening friends sit up a little bit and take it seriously.  And, as we've seen, it's far from being the typical useless piece of say-nothing bureaucratic nonsense.



As friendly and useful as the document is, its line-and-a-half wrapping around URL is not nearly as friendly.  So this week's GRC shortcut is that.  You can find it at grc.sc/912, since this is Episode 912.  And big props to the NSA for assembling something so useful and largely so actionable.  If nothing else, the nature of the recommendations would help someone who doesn't live in the security realm to realize the way security-conscious professionals think.  And that would probably be surprising to many people.  It's like, wow, you're really that paranoid?  I'm sorry.



LEO:  Yeah, yeah.  I'm going to share this on Ask the Tech Guys.  This is really good.



STEVE:  Yeah.  It is a great document.



LEO:  Yeah, yeah.  And, yeah, I can't think of anything I would disagree with.  There are some things that maybe are not easy to implement.



STEVE:  Yeah, I mean, like going around, like cover up the microphone holes on your various devices.



LEO:  Yeah.  I mean, baby monitors we know get hacked.  But no device from Amazon or Google to my knowledge has ever been hacked.  Most people are more afraid of Amazon and Google listening in, which they don't, or Siri.  I think that the big three, right, you don't know of any hacks of them, exploits with them?



STEVE:  No.



LEO:  Of course if you had an exploit, you wouldn't tell anyone.  That'd be a good nation-state exploit.  I know a surprising large number of people that we work with, sophisticated users, will not have these devices in their homes.  I have an Echo right there, a Google Assistant right there.  I have Siri right here.  You've got it in your pocket with your phone.  But they just say no, no, no, not going to do it.



STEVE:  Yeah.



LEO:  Very interesting.  Great show, as always, Mr. G.  Steve Gibson is at GRC.com, the Gibson Research Corporation.



Copyright (c) 2023 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.






GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#913

DATE:		March 7, 2023

TITLE:		A Fowl Incident

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-913.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week's answers are many:  How has Fosstodon survived a sustained DDoS attack?  Or has it?  What luck have Europol and the FBI had with taking down DDoS-for-hire services, and have they returned?  What's the point of blocking TikTok, and is it even possible?  What happens when government-backed surveillance goes rogue?  What exactly is "Strategic Objective 3.3," and what, if anything, does it portend for future software?  Should you enable GitHub's new secret scanning service and get scanned?  What exactly did CISA's secretive red-team accomplish, and against whom?



Which messenger apps have been banned by Russia, who's missing from that list, and why?  What exactly is old that's new again, what happens when everyone uses the same cryptographic library for their TPM code, what's the latest WordPress plug-in to threaten more than one million sites, and why has Russia fined Wikipedia?  And once we've put that collection of need-to-know questions to rest, we're going to examine the surprising revelations that surface as we unearth the fowlest of recent security incidents.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  Lots to talk about.  A DDoS attack against Mastodon.  Steve's take on blocking TikTok.  Does it accomplish anything?  GitHub's new secret scanning service, something we should all probably be running.  The latest WordPress plugin that's threatening more than a million sites.  Russia fining Wikipedia.  And Steve digs deep on the case of the chicken bone points or something.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 913, recorded Tuesday, March 7th, 2023:  A Fowl Incident.



It's time for Security Now!.  You've been waiting all week, so have I, to hear from this guy right here, Steve Gibson, our securitarian.



STEVE GIBSON:  Whoa, securitarian.  What? 



LEO:  Our Secretary of Security.



STEVE:  That's right, the geritarian or something.



LEO:  Every week we get together.  Steve explains why the world's going to hell in a hand basket.  I suspect, Steve, I may be wrong, but I suspect you will be answering some pretty important questions this week.



STEVE:  This week's answers are many, as it turns out.  How has Fosstodon survived a sustained DDoS attack?  Or has it?  What luck have Europol and the FBI had with taking down DDoS-for-hire services, and have they returned?  What's the point of blocking TikTok, and is it even possible?  What happens when government-backed surveillance goes rogue?  And exactly what is Strategic Objective 3.3?  What, if anything, does it portend for future software?  Should you enable GitHub's new secret scanning service and get yourself scanned?  What exactly did CISA's secretive red team accomplish, and against whom?  Which messenger apps have been banned by Russia, who's missing from that list, and why?



What exactly is old that's new again?  What happens when everyone uses the same cryptographic library for their TPM code?  What's the latest WordPress plugin to threaten more than one million sites?  And why has Russia fined Wikipedia?  And once we've put that collection of need-to-know questions to rest, we're going to examine the surprising revelations that surface as we unearth the fowlest of recent security incidents.  Thus today's podcast:  A Fowl Incident.



LEO:  When you say "fowl," F-O-W-L.



STEVE:  Yes, indeed.



LEO:  People who are not watching will not maybe immediately get the pun that you have made.



STEVE:  A Fowl Incident.



LEO:  It's a fowl [clucking].  I can't wait to find out what that's all about.



STEVE:  And of course we do have another great Picture of the Week.



LEO:  Yes. 



STEVE:  Which makes you question humanity once again.



LEO:  I saw it, and I went, what?  Steve, I think, I bet, I know you've got a Picture of the Week.



STEVE:  So yes, as I said, this one will make people question humanity once again.  I gave this one the caption "Why real world testing is important."  And this picture was provided courtesy of Simon Zerafa, a friend of the show.  It's a photo of a notice that was posted in some organization somewhere on the wall.  And it explains what you should do.  It says:  "Please, when using the stairs, stay to the right when going up; stay to the left when going down."



LEO:  Oh, dear.



STEVE:  "This will keep people from running into each other," it says.



LEO:  No, no.  Quite the opposite.  This will guarantee that people...



STEVE:  Yeah.  And you also kind of have to wonder, like, are people having their eyes closed when they're using the stairs at this location?



LEO:  You shouldn't need this notice, obviously.



STEVE:  Yeah, you know, you ought to see people, like, approaching you on the same side, and one of the two of you decides to switch sides.  But of course this was just wonderfully written from, like, the perspective of someone standing at the bottom looking up the stairs, not from the perspective of the person actually doing the walking.



LEO:  No.  Would have been sufficient just to say "stay to the right."



STEVE:  Please stay to the right, yeah. 



LEO:  Yeah.  That's all you had to say.



STEVE:  Like when you're driving on a street, like, you know, similar to that.



LEO:  That's what everybody does anyway.



STEVE:  But no, we're going to be more - as I said, why real-world testing is important.  You should test this signage after you post it and see what happens, and then consider changing it.



Okay.  Last Thursday, Chris Miller posted on Fosstodon:  "Hi, all.  We're still under a major DDOS attack, and that's why mobile and desktop clients are not currently working.  We've had to put the site behind Cloudflare temporarily until the attack stops.  We're looking at other long-term solutions, but we need to get through the current moment first.  For now, use the web interface.  Thanks for your patience."  And as of yesterday, when I last looked, that attack was still ongoing.



Fosstodon, as its name suggests, F-O-S-S-T-O-D-O-N, is the largest Mastodon instance inhabited by open-source software denizens.  And sadly, this most recent attack marks only the latest in a growing string of DDoS attacks that have hit and easily brought down unprotected Mastodon server instances over the past few months.  And Leo, you were saying now that mastodon.social is in trouble.



LEO:  Yeah.  Gargron, who created Mastodon, runs mastodon.social, his was down.  He tooted a couple of days ago.  And last week, during this show in fact, I think it was, I was getting a lot of messages from people saying is your site - is twit.social down?  We run a Mastodon instance.  And I check, you know, we have a thing called Sidekick running that talks about traffic and in particular how many queues are open, how many have been processed, how many haven't, and how many have failed.  And it was through - the number of queues was through the roof, which is sometimes a sign of too many people using the site.  But nothing had happened to make that the case.  So while I don't know, I'm going to guess that was also a DDoS attempt.  People were flooding us with posts.



STEVE:  Or with connection attempts.



LEO:  Yeah, or connection attempts.



STEVE:  So that would have overflowed the inbound queue, yeah.



LEO:  That's right.  Yeah, Sidekick would have died, yeah.



STEVE:  So anyway, what's happened is, and here's like some anecdotal examples we just had, the pace of these attacks has increased significantly after Mastodon gained a huge amount of attention thanks to the mass exodus from Twitter following Elon's takeover and his subsequent actions, which struck many as being in the best interests of neither the larger Twitter community nor their own individual best interests.  So fearing the approaching end of Twitter, many jumped over to the Mastodon's decentralized model, which inherently prevents a repeat of essentially what Elon is doing at Twitter. 



LEO:  It also means that DDoS can't bring the whole thing down; right?  Because...



STEVE:  Well, it can't bring...



LEO:  Individual instances down.



STEVE:  Exactly.  Being federated, there's other places you could go.  There's a guy, feditips, feditips@mstdn.social.



LEO:  Yeah.  That's good, yeah, yeah.



STEVE:  As in federated tips.  And he posted, or that person posted:  "The mastodon.social server is currently under a heavy DDOS attack and may not work properly.  The 12,164 other servers on the network are unaffected.  This is part of the reason why federated networks are a good idea:  if one server goes down, the others work fine.  The more spread out we are on small and medium-size servers, the harder it is for anyone to take down the network because there's no obvious target."  He says:  "There are also many other reasons why federation is good."  And then he provided a link to fedi.tips/why-is-the-fediverse, hyphenated.



So there's not much more to say here other than to note that fedi.tips is certainly correct.  On the one hand, the nearly week-long DDoS attack against Fosstodon has rendered its mobile and desktop clients inoperable.  But thanks to their web frontend moving behind Cloudflare's frontend protection, at least their web interface is operable, and they're remaining on the air.



As I noted recently, when we were talking about the most recent DDoS connection rate attack record being broken, no standalone server on the Internet can withstand today's DDoS attacks, not even for an instant.



LEO:  Is that because they're amplified?  Is that why they're so strong?



STEVE:  Yes, yes.  Today's modern IoT botnet-based attacks are large enough to swamp even medium-size bandwidth providers.  That is to say, you know, it might even be that the traffic doesn't actually reach your server because it brings down the routers upstream of your server because these attacks are just so big.  So it's not that your server is even down.  It's that the traffic from outside on the Internet can't even reach it because the attack just swamps the aggregating routers before they have a chance to even get to your server.  So the only recourse - I mean only - if you have to be on the 'Net is to move behind the protective skirts of one of today's major DDoS protection services.  The problem is, such service is not free unless the service wishes to provide such service charitably.



And that's one thing that sort of annoyed me.  Elsewhere, on Reddit, the Fosstodon admin was grousing a little bit about the need to be rescued by a commercial service such as Cloudflare.  In my opinion they should consider themselves incredibly fortunate that such a facility like Cloudflare exists.  Otherwise, they would be off the 'Net for as long as the DDoSing cretins wanted to keep them off the 'Net.  And I've had experience with that in my past.  And it is in fact, I mean, we've got an interesting graph coming up next which gives us a sense for the scale of the problem, not only in attack size, which as we've said is now just, I mean, it's incredible amounts of attack, where to call it overkill is an understatement because it would just - the attacks are so large that they just melt the wires, essentially, between the server being attacked and the outside world.



Okay.  So on the subject of DDoS attacks, the network security provider NetScout has noted that the efforts by Europol and the FBI to take down more than 50 DDoS-for-hire services in the middle of last December has indeed led to a measurable decline in DDoS attacks.  I've got a chart in the show notes that demonstrates that visually.  Those declines have been recorded at broadband providers across both the U.S. and EU.  Moreover, NetScout said that DDoS traffic has remained at lower levels for the month after those takedown efforts.  This suggests that no new players have immediately appeared on the underground DDoS market to fill the void after last year's takedowns.  On the other hand, four or five weeks isn't much time to wait for a new service to get itself up and going.



So to illustrate their data, NetScout provided this nice chart that we've got on the screen right now so that we were able to see for ourselves.  That chart leaves little doubt that the takedown effort had its intended effect.  Where the takedown occurred, you could see a dramatic decrease in - something.  So, wondering exactly what that something is, you then look at the horizontal scale to learn that the chart's horizontal axis is in attacks per day.  So you learn that while, yes, the number of attacks per day did indeed fall from their peak of around 3,500 DDoS attacks per day globally, and that peak was reached twice, and a number of shorter peaks, like more than 3,000 were reached another four times, still, a few weeks ago, a month after this takedown, actually it was the 22nd of January, there were 1,000 attacks in that day.  And of course it only takes one attack to ruin a particular website's day.  



So it's definitely good that law enforcement is on this, and is taking down these DDoS-for-hire services since they commoditize DDoS attacks.  They allow anybody to send a little bitcoin somewhere and say, I want this site removed for X length of time.  I think, you know, you pay by the minute how long you want the site to be gone.  So that's not the kind of commodity that you want around.  And I'm delighted that the FBI and Europol are on that.  But we're still seeing individual attacks breaking records, so it's clear that there's still plenty of firepower left, to which Fosstodon and Mastodon.social can both attest.  Fosstodon moved itself behind Cloudflare, got their website back up.  Once the attack abates, they will presumably move back.



And depending upon the size of the grudge or whatever it is, I mean, like who knows why Fosstodon is being attacked.  Part of the problem is these attacks are so trivial to execute now that they don't really have to mean anything.  It's just, if you've got an idle botnet that can do DDoS attacks, well, you know, spin the wheel and pick your target.  It's, I mean, it's sad, and it's probably the biggest weakness, the fundamental weakness that we have in the way the Internet works with its autonomous routing, is that the brilliance of it is that you can drop a packet on the network anywhere in the world, and the routers will forward that packet to its destination IP.  And that's also the bad news because, if you've got a whole bunch of people all dropping packets, all aimed at the same IP, the global Internet will concentrate all of those packets into that single location and create a single point of failure.  And that site is no longer able to respond to good packets because the malicious ones have just flooded it.



So that hasn't been fixed, and I don't see any fix for that.  It would take, you know, as we've talked about this before, right, some clients that are generating DDoS attacks are able to spoof their source IP.  They're able to just make up whatever IP they want as the packets are leaving.  Well, that traffic could be blocked at the first time it comes to a network border because that network border knows very well what networks are inside of it.



LEO:  You're saying an ISP specifically.



STEVE:  Correct.



LEO:  I mean, we've always - we've said this for years, that ISPs have got to be doing this, have got to block outbound traffic that's not originating from IP addresses inside their range.



STEVE:  Exactly.  It's called "egress filtering."  And all the technology is there.  It's just...



LEO:  I can't believe they're not doing it still.



STEVE:  I know.



LEO:  That's kind of stunning.  I thought they were.  Maybe, well, you know, maybe Comcast is.  Maybe the big ones are.  But all it takes is one little Ukrainian ISP or something; right?



STEVE:  True.  And the argument has been, well, then bots would then start making up IPs within the network so you didn't know where in the network it was.  And the bots are also disposable; right?  They're all compromised routers, you know, that exist in back closets and corners.  So in fact it's not like they're people who need to avoid or evade law enforcement.  They're just, you know, IoT devices that have long been forgotten, that have not been patched, that have been taken over remotely, and now they're serving somebody else's ends.  So we've got a sort of a mess, and it's not being resolved quickly.



So, okay.  Last Thursday the headline in Gizmodo read:  "We Found 28,000 Apps Sending Their Data to TikTok.  Banning the App Won't Help."  I'm just going to share the beginning of Gizmodo's very long article since it contains most of the useful information.  Gizmodo writes:  "President Joe Biden gave federal agencies 30 days to remove TikTok from government devices earlier this week."  And that means that was last week.  "Until now, most politicians," Gizmodo is saying, "intent on punishing TikTok, have focused solely on banning the app itself.  But according to a memo reviewed by Reuters, federal agencies must also 'prohibit Internet traffic from reaching the company.'"  And Gizmodo says:  "That's a lot more complicated than it sounds."  And I'll interject here, a lot less complicated than Gizmodo thinks, but we'll get to that in a minute.



Anyway, so they said:  "Gizmodo has learned that tens of thousands of apps, many which may already be installed on federal employees' work phones, use code that sends data to TikTok.  Some 28,251 apps" - and it's unclear which platforms those are on, there's no mention of that here - "use TikTok's software development kits (SDKs)," which they explain are "tools which integrate apps with TikTok's systems, and send TikTok user data, for functions like ads within TikTok, logging in, and sharing videos from the app."  That's according to a search conducted by Gizmodo and corroborated by AppFigures, an analytics company.



"But apps aren't TikTok's only source of data.  There are TikTok trackers spread across even more websites.  The type of data sharing TikTok is doing is just as common on other parts of the Internet.  The apps using the TikTok SDK include popular games like Mobile Legends:  Bang Bang, Trivia Crack, and Fruit Ninja, photo editors like VSCO and Canva, lesser-known dating apps, weather apps, WiFi utilities, and a wide variety of other apps in nearly every category.  The developers for the apps listed above did not immediately respond to a request for comment."



Anyway, Gizmodo article goes on, but we got the gist of this.  Okay.  So there's two parts to this whole mess.  The first is, what are we actually trying to do here; and the second is, is it even possible?  Okay.  So first, to give the whole idea of banning TikTok some perspective, as well as a bit of an updated reality check about the present nature of consumer tracking on the Internet overall, the non-partisan Brookings Institute titled their commentary from the middle of just last month "TikTok bans won't guarantee consumer safety."  And so in this case I've grabbed just the end of that long piece, where they conclude and summarize a bit.



"Are TikTok's data practices different from other companies?"  And they say:  "Several experts have already argued that TikTok bans won't make Americans safer.  One reason is that much of the information collected by TikTok is like that compiled by many companies that host consumer-facing products.  The app undoubtedly has information on which videos users have watched, comments they've made about those items, and their geolocation while watching the videos, as well as both users' and their friends' contact information.  But that is true for nearly all digital platforms and ecommerce sites around the world.



"It is also the case that digital firms compile data on users, and many buy and sell consumer data via third-party vehicles.  It's been estimated that leading U.S. data brokers have up to 1,500 pieces of information on the typical American, and that both domestic and foreign entities can purchase detailed profiles on nearly anyone with an online presence.  Even with aggregated data, it is possible to identify specific individuals through a relatively small number of attributes, with some research estimating that 99.98% of Americans could be de-anonymized from relatively small datasets.  Still, what sets TikTok apart are the amount and type of trackers they use.



"According to a 2022 study utilizing Apple's 'Record App Activity' feature, TikTok utilizes over twice the average number of potential trackers for social media platforms.  Almost all of these trackers were maintained by third parties, making it harder to know what TikTok is doing with the information they collect.  If concerns about TikTok are around the compromising of personal information with government authorities, either in China or elsewhere, there are many firms both within the U.S. and abroad that have been accused of the same.  For example, a former Twitter employee was convicted of acting as a foreign agent for Saudi Arabia, providing confidential information from that platform about dissidents to foreign officials.  Consumer geolocation data are routinely bought around the world by data brokers and repackaged for sale to advertisers, governments, and businesses.



"Regarding concerns that Chinese companies operating within the U.S. are beholden to Chinese laws, the same can be said of American companies that operate in China.  Some observers have expressed worries about Tesla vehicles being made in China for some of the same reasons, and what the company may have to do to maintain good relations with Chinese officials.  Furthermore, if the criterion for bans based on national security is access to users' confidential information, there's a long list of American and foreign companies that face security challenges via their Chinese operations.  As examples, many digital products sold domestically are made in China.  And a wide variety of smart appliances, pharmaceuticals, personal protective equipment, computer chips, and other products are assembled there."



Okay.  So what's actually developed over time globally is a rich and deeply interdependent ecosystem.  And there are myriad companies collecting and selling data on everyone who's using the Internet.  Our illusion of true anonymity is exactly that, an illusion.  As third-party cookies once did, most of this operates under the radar.  While unseen, it is still utterly ubiquitous.  It's everywhere.



Okay.  So that leaves the question, assuming that the governments of the world decide that they're going to blacklist TikTok, is that even possible?  As we know, IP addresses are readily changed, but the domains used by DNS lookups are generally hardcoded into apps and trackers.  That means that DNS lookups are TikTok's Achilles heel. I did a bit of research and identified five domain name roots which often also have subdomains.  So some wildcard matching would be necessary.  There are also two Akamai CDN domains.  But taken together, those would appear to be all of the domains currently in use by TikTok.  So they are tiktok.com, tiktok.org, tiktokv.com, tiktokcdn.com, musical.ly, and then the two Akamai CDN networks.



Okay.  So if federal agencies were to locally configure their networks' local DNS to blackhole those domains and their subdomains, you know, perhaps returning 0.0.0.0 or 127.0.0.1, or maybe pointing them to a local server so, you know, to have them resolve to something.  Once local device caches expire, you know, the DNS caches, and a quick DNS DIG that I did indicates that those domains are running with quite short TTL (time-to-live) expirations, all traffic of any sort bound for TikTok would lose its destination IP and would be blocked at the border.  It would just get dropped at the client device.  They would no longer be able to get the IP for any TikTok property.



On the question of whether this would be a good thing to do, I have no opinion.  Whereas the technology is interesting, the politics is not.  Tensions are clearly on the rise with China, so I suppose that nationalism and protectionism are bound to rise, as well.  But I think, more than anything, the technology lesson we take from this is that there is an incredible unseen and largely unappreciated underground of activity that very few Internet users appreciate.  Out of curiosity, I went over to MSNBC's website at msnbc.com and uBlock Origin lit up, counting that single website homepage causing my browser to connect to 38 other domains; and foxnews.com connected my browser to 51 other domains.  38 and 51.  I'm sure that few are directly affiliated with either property, and how many CDNs do they need?  All of that other crap is superfluous.  It's, you know, who knows what?  And since most people have no idea what's going on, why not load up with revenue-generating trackers?



I doubt that TikTok cares at all about the loss of connectivity to federal government networks.  And federal employees who want to continue to use TikTok on their own devices while within those networks can simply switch to their cellular provider for continued unfettered Internet access.  If the United States government's actual goal is to protect its citizens from the data collection of a Chinese state-owned and controlled entity, then blocking all TikTok traffic at U.S. borders is going to be necessary.  But every time Russia or some other repressive regime does the same, we make fun of them.  So, you know, make up your own mind.



LEO:  Yeah, I mean, do we want a Great Firewall of the U.S.?



STEVE:  Exactly.



LEO:  I think some people would like that.



STEVE:  Exactly.



LEO:  Yeah.



STEVE:  It seems crazy, and I think that this just seems like way overblown.  I thought it was interesting that Brookings said that in that 2022 analysis, TikTok was running more tracking things.



LEO:  Yeah, that is interesting.



STEVE:  More than twice.



LEO:  Yeah.



STEVE:  Yeah, that's interesting.  But on the other hand, it's twice more than a whole lot.  So it's a lot lot.



LEO:  More than Facebook, really?  More than Instagram?  I find that hard to believe.



STEVE:  I agree.



LEO:  And I have to say, these same people who are shouting about shutting down TikTok have done very little to protect our privacy with telecommunications companies.



STEVE:  Right.



LEO:  Who lobby very hard and give them a lot of money.  And they just - they're mum on that.



STEVE:  Right.  And to your point, Leo, it was a really good one, when you mentioned Facebook and Instagram, the number of trackers is one metric.  But the number of tracking events is a different metric.  And because of the heavy use of Facebook and Instagram... 



LEO:  Yes, good point, yeah.



STEVE:  ...they're getting many more tracking events.



LEO:  But we know that because they kill your phone battery.  It's the first thing you do if you want to save your phone is take Facebook and Instagram off.



STEVE:  Yup.



LEO:  I don't think TikTok kills my phone battery.  But I don't know, maybe it does.



STEVE:  Let's take a break.



LEO:  Okay.



STEVE:  I'm going to get ready for the next phase of this.



LEO:  Get ready for the next phase of your life, Steve Gibson.



STEVE:  So this piece piqued my interest because of the recent discussions we've been having about the U.K.'s decision to mandate the equivalent of some sort of backdoor in otherwise secure and private communications.  A report from the Office of the Inspector General for the Department of Homeland Security was titled "Secret Service and ICE Did Not Always Adhere to Statute and Policies Governing Use of Cell-Site Simulators."  And it was followed "Law Enforcement Sensitive (REDACTED)."



That report found that the U.S. Secret Service and the U.S. Immigration and Customs Enforcement (ICE) have not obtained court orders for multiple operations in 2020 and 2021 where they deployed cell-site simulators - you know, stingrays - to intercept mobile communications.  The report found that one Secret Service field office had deployed stingrays on multiple occasions on behalf of a local law enforcement agency without obtaining court warrants.  The report also found that ICE "did not believe court authorization was required" for some of its operations.  Furthermore, the report also found that neither the USSS (U.S. Secret Service) nor the ICE were documenting operations related to supervisory approval and data deletion procedures.



So, yeah, we need to be careful with the technology that we make generally available.  And those who have the technology need to know that their use of it will be monitored, and they will be held accountable.  This demonstrates obviously the potential for abuse once loopholes are placed into our supposedly secure and private communications.  We'd like to believe that only those holding valid court orders, you know, search warrants, would have access to private communications.  But experience suggests otherwise.  Everybody's only human; right?  Except ChatGPT, and we're not sure what it is.



Okay.  Strategic Objective 3.3.  This is a biggie.  And in fact I'll have something more to say about this next week.  I found a recent speech that was given.  But dated March 2023, so this month, last week the Biden administration published its 39-page National Cybersecurity Strategy.  I haven't had time to go through the entire document; but if it appears worthwhile, I'll likely cover it in additional detail next week.  And in fact I did see some stuff about IoT that I want to talk about, too.  But one section of the document in particular was brought to my attention by Mark Fishburn, a listener of this podcast who knew I'd find it interesting.  And our listeners will know why when I share it.



That section is "Strategic Objective 3.3" labeled "Shift liability for insecure software and services."  Get a load of what is now part of the United States official national cybersecurity strategy.  It says:  "Markets impose inadequate costs on, and often reward, those entities that introduce vulnerable products or services into our digital ecosystem.  Too many vendors ignore best practices for software development, ship products with insecure default configurations or known vulnerabilities, and integrate third-party software of unvetted or unknown provenance.  Software makers are able to leverage their market position to fully disclaim liability by contract, further reducing their incentive to follow secure-by-design principles or perform pre-release testing.  Poor software security greatly increases systemic risk across the digital ecosystem and leaves American citizens bearing the ultimate cost.



"We must begin to shift liability onto those entities that fail to take reasonable precautions to secure their software while recognizing that even the most advanced software security programs cannot prevent all vulnerabilities.  Companies that make software must have the freedom to innovate, but they must also be held liable when they fail to live up to the duty of care they owe consumers, businesses, or critical infrastructure providers.  Responsibility must be placed on the stakeholders most capable of taking action to prevent bad outcomes, not on the end-users that often bear the consequences of insecure software, nor on the open source developer of a component that is integrated into a commercial product.  Doing so will drive the market to produce safer products and services while preserving innovation and the ability of startups and other small and medium-size businesses to compete against market leaders.



"The administration will work with Congress and the private sector to develop legislation establishing liability for software products and services.  Any such legislation should prevent manufacturers and software publishers with market power from fully disclaiming liability by contract, and establish higher standards of care for software in specific high-risk scenarios.  To begin to shape standards of care for secure software development, the administration will drive the development of an adaptable safe harbor framework to shield from liability companies that securely develop and maintain their software products and services.  This safe harbor will draw from current best practices for secure software development, such as the NIST Secure Software Development Framework.  It must also evolve over time, incorporating new tools for secure software development, software transparency, and vulnerability discovery.



"And finally, to further incentivize the adoption of secure software development practices, the administration will encourage coordinated vulnerability disclosure across all technology types and sectors; promote the further development of software bills of material; and develop a process for identifying and mitigating the risk presented by unsupported software that is widely used or supports critical infrastructure.  In partnership with the private sector and the open source software community, the federal government will also continue to invest in the development of secure software, including memory-safe languages and software development techniques, frameworks, and testing tools."



Wow.  Okay, now, obviously no strategy is law.  Right?  A strategy is only that.  And major software publishers have strong lobbying arms in Washington where legislative votes are available to the highest bidder.  So nothing here in this 3.3 section is actionable, and there will be a great deal of pushback against any sort of weakening of today's current blanket contractual protections, which we've often noted is like, well, yeah, you could use the software, but whatever it does, it does, and we don't really know, and if you don't like it, all you can ever do is ask for your money back.  That's your  maximum recourse.



What caught me off guard here was the precision of understanding about the nature of this problem.  You know, that one sentence: "Software makers are able to leverage their market position to fully disclaim liability by contract."  In other words, if you don't like it, don't use it.  On the other hand, your position doesn't make not using it a practical alternative; right?  You have to use Microsoft stuff if you're in the enterprise.  So anyway, they said:  "Further reducing their incentive to follow secure-by-design principles or perform pre-release testing."



So I would say the writing is not yet even on the wall, but it's obviously in some people's heads.  And it just got written down in the official national cybersecurity strategy for the first time ever.  So it's obvious that others have noticed the same irresponsible attitudes toward critical software security that we've been discussing here on this podcast.  I mean, you know, the fiasco of Microsoft's printer problems which they just ignored for six months, which hurt huge numbers of their customers, was unconscionable.



LEO:  So they'd be liable for this.



STEVE:  Yes.



LEO:  Which I think is fantastic.



STEVE:  Yes.



LEO:  I mean, they're not going to think it's very fantastic, I'm sure.



STEVE:  Oh, no, baby.



LEO:  Does it mean liable criminally, or liable civilly? 



STEVE:  I'm sure it's civil liability.



LEO:  Yeah.  And maybe just opens up the idea that if you were a victim of a printer hack, that you could recover losses from Microsoft.  Of course, they're shrink wrap licenses, and all those EULAs and all that say, oh, we're not liable; and, oh, you have to go to arb...



STEVE:  Well, and that's just it.



LEO:  Yeah.



STEVE:  That is, they're disclaiming liability by contract, which this specifically targets, and says this is not okay.



LEO:  Right.  Good.  We talked about it on TWiT a little bit, and I wanted to get your take on it because that seems a big sea change.  And you're right.  Will Congress let that go through?  Probably not.  But it's good to just shake some trees a little bit, I guess.



STEVE:  Well, and ask if the public would not be for this.  So, you know, it's like anybody who hears this is like, well, yeah.



LEO:  Of course.



STEVE:  Why wouldn't they be responsible?  Of course.  And as I've been saying, it's insane, what we have now, where there's zero accountability.



LEO:  And we've all seen those, you know, disclaimers on boxes of software and so forth that say, you know, not representing that this is fit for any purpose whatsoever.



STEVE:  Merchantability or fitness of use.  Yeah, we don't know...



LEO:  We don't guarantee nothin'.



STEVE:  You know, you may drive the car off the lot and the wheels all fall off.  Well, you know, we thought we screwed them on tight, but I guess Henry did not do that.



LEO:  But actually, that's the difference because car manufacturers are liable.



STEVE:  That's my point.



LEO:  Software manufacturers are not.



STEVE:  This is an anomaly for something that has become as important as software has.  



LEO:  I agree.



STEVE:  And it's only because of history; right?  Back when it was just like, well, you could store your wife's recipes on your Apple II.  And well, if we lost them, we're sorry, you should have held onto the paper copy.  It just - and it went from there; right?  It never changed.



LEO:  Good.  I love it.  I hope they get this happening.  This is good.



STEVE:  This is big news.  And again, just the fact that it's been written down, and now people are going to see it and go, huh.  Yeah.  Why is that that way?  You know?  It's like, okay, good.



So back in December, GitHub announced the public beta of their free secret scanning - the name of this is weird.  Secret scanning alerts.  I had to, like, what?  Anyway, public beta in December of their free secret scanning alerts across public repositories.  Now, by "secret scanning" they mean that GitHub will proactively scan all code submitted for the inadvertent inclusion of any secrets  like, whoops, we left the admin password in the code by mistake.  It's a very cool idea.  And since its initial release in beta, more than 70,000 public repositories have turned on secret scanning alerts and have uncovered thousands of leaking secrets, stuff that the authors did not intend to leave in their code.



So as of one week ago, last Tuesday, a week ago today, last Tuesday, GitHub's secret scanning alert experience is generally available and free for all public repositories.  GitHub users can enable secret scanning alerts across all of the repositories they own to notify them of any leaked secrets across their entire repository history including code, issues, descriptions, and comments.  GitHub secret scanning works with more than 100 service providers in the GitHub Partner Program.  In addition to alerting users, they will notify their partners when one of their secrets is leaked.  With secret scanning alerts enabled, regular users will now also receive alerts for secrets where it's not possible to notify a partner, for example, if self-hosted keys are exposed, along with a full audit log of actions taken on the alert.



So one example of this in practice is a DevOps consultant and trainer whose handle is @rajbos, I guess his name is Rob.  Anyway, he enabled secret scanning on approximately 14,000 repositories and discovered over 1,000 secrets.  Rob remarked:  "My research proves the point of why everyone should have secret scanning enabled.  I have researched 14,000 public GitHub Action repositories and found over 1,000 secrets in them."



LEO:  Oh, my god.



STEVE:  Yeah.  "Even though I train a lot of folks on using GitHub Advanced Security, I found secrets in my own repositories through this."



LEO:  It's easy.  It's easy to make that mistake.  It really is.



STEVE:  Yeah.



LEO:  I have to be very careful because I have - and I bet you a lot of these are this.  I have, well, some of it's going to be code with like your Amazon secret key, API key in it; right?



STEVE:  Right, right.



LEO:  But I also, I back up my dotfiles, my settings files.  My PGP stuff's in there.  I back up my Emacs setup.  And it's very easy to, you know, often people hardcode secrets into those files.  And then you're thinking of the GitHub, it's easy to forget and miss it.



STEVE:  Yup.  And, for example, you might have put that in just for, like, during testing; right?



LEO:  Yeah.



STEVE:  So you didn't have to keep entering the password every time.  You just put a little shortcut in.  And then when you go to production you forget.



LEO:  Forget.



STEVE:  And it's like, whoops.



LEO:  There's a way to do it, you know, they're really easy - not that easy.  But there is a little thing you could do to have PGP encrypted keys in all those secrets.  And then they're kind of safe.  They're still in the code, but they're safe.  And I've always thought, oh, I should probably do that.  And I have never done it.  So, you know, I'm counting on my own brilliance to remember not to upload those files.  Oh, boy.  Good luck.



STEVE:  So also last Tuesday, CISA revealed the somewhat bracing results of a secret red team exercise that they carried out against the network of a "large unnamed U.S. critical infrastructure organization with a mature cyber posture."  During that exercise they "obtained persistent access to the organization's network, moved laterally across multiple geographically separate sites, and gained access to systems adjacent to the organization's sensitive business systems."  CISA says that on at least 13 separate occasions its red team triggered "measurable events."  It wasn't clear whether this was deliberate or unavoidable, but that should have gotten it caught, and didn't.  In every case the organization failed to detect these actionable events.



As I noted, CISA officials declined to name the organization.  But they did say that, although they did manage to get in, they found the organization had good cybersecurity policies in other parts of its network, such as up-to-date and hardened perimeter infrastructure and good password policies.  Okay.  So how did CISA's red team gain its initial entry?  Phishing, today's most difficult to corral cyber weakness.  People on the inside are on the inside, and it's just too easy for someone to inadvertently allow a bad guy in.  And that's what happened in this instance.  And once they got in, they stayed in, moved laterally, set up shop, moved around, explored the network, all unseen.



Okay.  So I titled this little short bit of news "What's Left?" after reading that Russia had formally legally banned the use of all foreign messaging applications inside Russian financial institutions and state-owned companies.  Again, this is one of those, what, only now?  Like why would you not do it a long - it's like, are you still using Windows in Russia?  What?



Anyway, the law, which entered into its effect this month, outlaws the use of apps such as Discord, Microsoft Teams, Skype for Business, Snapchat, Telegram, Threema, Viber, WhatsApp, and WeChat.  Now, the page announcing this was in Russian, and it lists those.  And if you're interested I have the link here in the show notes, like we have any Russian speakers.  Notably missing is Apple's iMessage and Android's Messages, which are both end-to-end encrypted.  You can see the name of the things that I named.  There they are.  Nine specifically enumerated messaging apps.



So this legislation, which specifically enumerated those nine third-party messenger apps doesn't mention the two mobile platforms' native messengers.  So perhaps that's the answer to my rhetorical question, "What's left?"  Or perhaps those are just implied.  The news was that all foreign messaging applications were banned.  So perhaps Russia has some state-owned or trusted messaging apps that no one is going to want to use or trust?  I don't know.  Anyway, just sort of interesting.  That's like, okay, can't use these.  But we don't know about iMessage, or Messages on the Android platform.



As we've noted before, CISA is maintaining a list of what they call the KEV list, the Known Exploited Vulnerabilities.  And KEV has become a common abbreviation within the security industry for that growing list.  In other words, these are vulnerabilities which CISA has had reports of actually being used in the field.  So that's like the patch first list; right?  And once upon a time it wasn't very big.  But it's been growing.  During the last year, the size of this catalog of known exploited vulnerabilities very nearly tripled in size.  It jumped from 331 entries at the start of 2022, but finished out last year with 868 individual vulnerabilities known to have been seen being actively exploited.



Okay, now, here's the interesting part.  Although this near tripling of the list in one year would suggest that new bugs are being exploited, a look at the issuance dates of the catalog's CVEs for those shows that the vast majority of the new CISA KEV entries are for older vulnerabilities that companies failed to patch for years, all which came under attack last year.  The oldest of those was the exploitation of a bug that originally had a patch available for it 21 years ago, back in 2002, which was it came back to life and was being used for vulnerability exploitation.  So, wow.



As I said, you know, we've often talked about how amazingly difficult it is to get the old stuff patched.  And that's another advantage of this directive 3.3; right?  If the software were more secure out of the gate, rather than relying, I mean, like relying by policy, which is what Microsoft has become; right?  They are relying by policy on updates.  There's not even a - it's not a shock.  It's hundreds of security vulnerabilities per month are being fixed.  So, and we hear stories, right, of how what is it, Windows 7 or 8 shipped with over 10,000 known problems?  And, yeah, some of them are that the color of this gray shading changes if you shake the computer sideways during a full moon.  They're just not big problems.  But obviously a lot of them are big problems.



And so what's happened is it's just become, oh, yeah, what day is it?  Okay, ship it.  We have weekly updates.  We'll fix it later.  Well, not all systems have weekly updates, or monthly updates I should have said.  So it would be nice if there was a little more pressure to get it right the first time, since we seem as an industry, as a world, we're just not keeping things patched, even when fixes are available.



Speaking of patches, remember when everyone used Intel's sample Universal Plug and Play implementation code as their actual production code, apparently without ever actually looking at the code, and even in the face of that comment header block at the top of the code which loudly stated that this was sample code only and should never be used for production?  And that as a result of that, the entire industry suffered as a whole from a widespread vulnerability in that sample code which everything had in their routers.



Okay, well, history is - it's not really repeating, but it's certainly reminiscent.  This time it's a pair of widespread vulnerabilities which have befallen multiple vendors' Trusted Platform Module, the TPM code.  It's not, as I said, not quite the same as the UPnP debacle, since these vendors were at least using the TPM Reference implementation library, which should have been okay.  But this is another example, at least, of the danger of monocultures; and why we're more healthy if, as another example, we keep browsers other than Chromium alive. 



In this case, researchers at Quarkslab discovered a pair of buffer overflow vulnerabilities in libraries implementing the TPM 2.0 security specification.  The vulnerabilities would allow an attacker who could gain access to the TPM's command-line interface to leverage the vulnerabilities to corrupt the TPM's memory and access sensitive information handled by the TPM, such as encryption keys - which of course is exactly what all of the TPM's fancy hardware technology is supposed to prevent.  Patches for this were released at the end of last month, end of February.  And since many vendors all directly implemented the same reference library, the TPM implementations from IBM, the Trusted Computing Group themselves, Red Hat, SUSE Linux, and many others are all affected.  So we can look for some updates to that coming soon.



Okay.  Two last pieces.  WordPress.  Just a quick note to all of our listeners who manage WordPress sites.  More than three million WordPress sites which are currently running a plugin called All in One SEO will need to be updated to resolve a set of vulnerabilities that could be used to hijack sites.  As is often the case, the troubles were found and reported by the researchers at WordFence.  And again, three million-plus WordPress sites currently vulnerable.



As I've noted before, when we were talking about the inherent danger of third-party-developed WordPress add-ins, which appear to be having constant security problems, and again, not because anybody's malicious typically, just because there are unprofessional developers who create a widget and think, hey, I'm going to make this available to other people.  Anyway, adding protection from WordFence, if it fits your budget, would seem like a large and worthwhile ounce of prevention.  So just a reminder that those guys are there, and they really are on the ball.



And finally, the Russian government has fined the Wikimedia Foundation, the organization behind the Wikipedia portal, two million, okay, well, rubles - that's about $27,000 - for failing to delete "misinformation," Leo, about the Russian military and its invasion, oops, its special operation in Ukraine.  According to Reuters, this is the third time Wikipedia has been fined by Russia since the country's invasion of Ukraine.  Wikipedia said the recent fine was related to articles on its Russian language portal related to Russian Invasions of Ukraine, the Battle for Kyiv, War Crimes during the Russian Invasion of Ukraine, the Shelling of Mariupol Hospital, the Bombing of the Mariupol Theater, and the Massacre in Bucha.



I noted that last November, when the second of the three fines was levied, the same fine of two million rubles was set.  But back then those two million rubles were worth $33,000 dollars; today, $27,000.  So those rubles appear to be slipping against the dollar a little bit.



The Wikimedia Foundation has stated that they refuse to back down and remove what it said was clearly fact-based, multiply-sourced, verified truth.  And they've been appealing these fines in Russian court.  So far they've only had one successful ruling.  But, you know, we can hope.  Leo?



LEO:  I mean, I wouldn't expect much from the courts, to be honest with you.  



STEVE:  No.  I'm surprised they even, like bother.



LEO:  Bother, yeah.  I mean, who runs those courts?  Let's think about that; right?



STEVE:  Yeah.  And, you know, I would imagine if they, you know,  they should not pull the content down.  If Russia wants to block all of Wikipedia, fine.



LEO:  Let 'em.



STEVE:  Just another loss for Russia's citizens.



LEO:  Right.



STEVE:  Who are part of this mess.



LEO:  Sad.  It's very sad.



STEVE:  Okay.  Our last break, and then, oh, baby.  We're going to discuss a fowl incident.



LEO:  I can't wait.  I was trying to decide where to go to dinner tonight, and I'm thinking now Popeye's.  But we'll find out.  We'll find out.  And now I can't wait to hear all about this fowl story.



STEVE:  We are going to encounter a mystery and solve it.  



LEO:  Ah.



STEVE:  When I read from a Chick-fil-A data breach report submitted to the U.S. State of Maine's Attorney General, which disclosed that 71,473 Chick-fil-A account-holding customers had had their accounts breached through a credential stuffing attack, I was skeptical.  And I'm at least still a bit confused.  That number just seems far too huge to be the result of what amounts to opportunistic, previous breach-driven guessing of account usernames and passwords.  And really, okay, think about it.  Why would some random hacker be going out of their way to compromise the accounts - and more than just a few - of Chick-fil-A customers?  Why not Chase, Bank of America, or TD Ameritrade?  I mean, Chick-fil-A, really?  Those are the customer accounts that you choose to penetrate?  You must really have a thing for chicken.



So I don't know.  It doesn't make any sense to me.  That number, first of all, seems too big  71,473 individual Chick-fil-A customer accounts, each which would have taken effort to compromise.  And what do you get for all that effort?  I don't know.  Apparently some Chick-fil-A redeemable loyalty reward points.



Okay.  We've been talking about this form of attack recently.  But just to reiterate, since this is where details matter, credential stuffing attacks are the reuse of username, email, and passwords, leaked from previous online site breaches, which are then being used to blindly guess login credentials at other unaffiliated websites.  The point is, as I've said before, all rational logic suggests that this should be an extremely low-yield attack, meaning that in order to correctly guess the logins for 71,473 individual Chick-fil-A customers, it would be necessary to wrongly guess a gazillion other times.



When I initially saw that large number, my first thought was that it couldn't actually be a true credential stuffing attack.  And for the record, I've never been a fan of the term "credential stuffing."  The industry, I think, could have come up with a better name, like "credential reuse attack."  But credential stuffing it is.  And as long as we all know that it's a credential reuse attack, that's fine.



So for that many accounts to be successfully attacked, I'm suspicious of whether Chick-fil-A might have earlier lost control of their own customer account data, and that the leaked information itself was now being used to login and attack their customers.  This would convert the attack from "surprisingly successful low yield against a bizarre target" to "unsurprisingly high yield against the only available target," if it was Chick-fil-A's data that had been breached, you know, leaked.  If you've somehow acquired Chick-fil-A's customer logon data, then that's going to be your only target.



On the other hand, if we assume that this was actually a true credential stuffing attack, and putting aside for the moment the question, out of a universe of equally suitable targets, why would an attacker choose Chick-fil-A, the Chick-fil-A's disclosure to various states' attorneys general...



LEO:  I wouldn't even choose them for a chicken sandwich, let alone a credential stuffing attack.



STEVE:  No, no, no.  And you'll get a kick at what I put down at the very end of the show notes.  We'll get there here in a minute.  The Chick-fil-A's disclosure to various states' attorneys general did state that the attack took place over a two-month span.  So this is Chick-fil-A's disclosure; right?  So Chick-fil-A is saying the attack took place over a two-month span from last December 18th through February 12th, last month.  Okay.  So that's 56 days during which 71,473 Chick-fil-A customer accounts were breached, at an average rate of 1,276 successful account breaches per day.  So the logistics of such a credential stuffing attack would be that an attacker has a massive database of prospective login credentials which they, for some reason, choose to aim at Chick-fil-A's website.



LEO:  I mean, first of all, who has an account with Chick-fil-A?



STEVE:  Well, yeah.



LEO:  You'd have to be quite a fan.



STEVE:  I agree.  And in blind account credential guessing, they then pour this massive database through the website's clearly unrestricted authentication frontend at presumably some massive rate.  Perhaps the attack was distributed with the massive database spread among many attacking clients in order to increase the overall rate of credential guessing because of course modern websites are able to simultaneously entertain many incoming connections, but in a situation where there's no apparent oversight over failed authentication, which must have been happening with like millions of instances, monitoring or throttling a failed authentication of any kind of missing.



So there's no real need to distribute the attack.  Nothing prevents a single attacking machine, or only a handful, from each establishing their own hundreds or thousands of simultaneous login sessions with a single Chick-fil-A server.  That works, too.  If we accept Chick-fil-A's claim that this was truly blind credential guessing from a database of previous completely unaffiliated websites, then the per-guess yield had to be quite low.  Again, how many people are there in this leaked database that also happen to be Chick-fil-A customers?



LEO:  Oh, I'm looking at Chick-fil-A's website now because I'm really curious why would you have a login.  And they have something called Chick-fil-A One, which is a point, you know, a customer loyalty thing.  You earn points, and then you get a free waffle fry or something.



STEVE:  Yeah, you know, every X number of coffees I used to buy at Starbucks, I would earn stars.



LEO:  But again, why would you want to hack it?  What are you going to get, somebody's Chick-fil-A points?



STEVE:  Exactly.  Exactly.



LEO:  Maybe their credit card.  I mean...



STEVE:  It had to be, well, and that wasn't available.  It was blinded so it could not be seen.



LEO:  Okay, okay.



STEVE:  So the yield per guess had to be quite low.  A very low per-guess yield meant that the total number of guesses had to be massive.  So this in turn means that the Chick-fil-A website servers raised no alarm of any kind, while starting last December the 18th their incoming connection rate had to have skyrocketed as millions of attempts to log in there were now failing.  The authentication failure rate had to be astronomical, yet nothing at their end took any notice.



LEO:  So this is a crime of opportunity because most sites you wouldn't even be able to do that.  Maybe rate limited, or an alarm would be raised.  Maybe Chick-fil-A just didn't have any protection; right?



STEVE:  Clearly they didn't, or it wouldn't have...



LEO:  They weren't paying attention.



STEVE:  ...gone 56 days without them noticing.



LEO:  Yeah.



STEVE:  Once Chick-fil-A somehow became aware of the attack, presumably when a sufficient number of their own customers complained of some sort of account tampering, like where did my chicken bone points go or whatever the hell it is, you know, they were able to identify exactly, you know, because that's what you want is they want the chicken bone points.



LEO:  I want those chicken bone points, mm, mm, mm.



STEVE:  That's right.  They were able to identify exactly which of their customers had their accounts breached in this manner.  Remember, we have an exact count, right, 71,463.  We're never going to forget that.



LEO:  In a way, this is Chick-fil-A's way of saying we just never thought anybody would want to get in.



STEVE:  That badly, yeah.



LEO:  We didn't secure it because we thought, well, who would care if you were - oh, my gosh.



STEVE:  I know.  Okay.  Okay.  So the reports now that they've repaired any damage done to those customers, they've restored their previous chicken bone points, made them whole again, and instructed them to change their Chick-fil-A passwords, and also anywhere else that they were reusing the same password.



Okay.  Now we know what happened.  We have two takeaways from this fowl incident.  First, if we accept on its face that this was an unaffiliated, if somewhat bizarre attack, then it could only have succeeded as it has due to the continuing presence of a widespread and stubborn reuse of user passwords across sites.



LEO:  Although we know what happens.  We really do.



STEVE:  And we do, unfortunately.  And of course that - and remember, that's not yesterday, that's today, right now.  This was just happening.  This means that, more than ever, it is rapidly becoming truly imperative for the uniqueness of passwords to be enforced across all of a user's online accounts.  We've learned from our own podcast audience's reports, in the wake of the LastPass vault debacle, that updating passwords can be a slow, tedious, and laborious process.  But now, more than ever before, if your password manager offers a global password reuse audit, as many do, maybe also a password strength audit, it's important that you allocate some time to begin replacing any duplicated passwords, and also in strengthening any existing passwords that do not contain sufficient state-of-the-art entropy.



I'm sure that this message is largely redundant and is unnecessary for this podcast's audience.  But the success of this Chick-fil-A attack informs us that everyone listening needs to share their understanding of this and why this is important with everyone they know.



LEO:  Yes, adjacent, podcast-adjacent listeners, yes, yes.



STEVE:  Yes.  Because if we're to believe Chick-fil-A, it's clear that the reuse of passwords remains widespread across the Internet, and we're starting to see an evolving epidemic of this new form of surprisingly successful attack.



This brings us to the second and more interesting technology question:  "What can websites do in the face of what appears to be an escalating and approaching epidemic of opportunistic low-yield credential reuse attacks?"  When a user logs into a website, their browser requests the login page, which presents a form to be filled out.  Okay, now, I got to this point in preparing today's show notes, when I thought that I ought to go over to Chick-fil-A's website to see whether they presented the login as a single form or multiple staged forms. And what did I find?



I have a picture of what I found in the show notes.  You can go to Chick-fil-A and click "Login," and you'll see it, too.  There are a pair of standard login credentials prompting for email address and password.  But there's also a "Sign in with Google" and a "Sign in with Apple."  And in the spot where there was once a "Sign in with Facebook," the page now reads "Looking for Facebook Login?"  Isn't that interesting.



LEO:  Hmm.



STEVE:  Clicking that link takes existing Chick-fil-A customers to a page that says, it screams in big red boldface:  "Facebook login is no longer available."  Huh.  It says:  "To continue using your account, you will need to set up an email and password login method."  In other words, old school.  "Please enter the email address associated with your Facebook account to get started and find your Chick-fil-A One account."  And then they prompt for your Facebook email.  But they're not letting you log in with your Facebook account any longer.  Though we cannot know for sure when Chick-fil-A's probably most popular "Login with Facebook" OAuth2 option was removed, anyone doing some forensic post-incident digging would be skeptical of the coincidence given the recent attack.



Unfortunately, since their login page is algorithmically generated and thus cannot be brought up with a specific, with a static URL, the Internet archive's Wayback Machine cannot be queried to see when "Login with Facebook" was removed as an option.  But given that "Login with Google" and "Login with Apple" are both still present, and that Chick-fil-A's replacement for "Login with Facebook" is migrating users from Facebook login to their native login, I would bet a month's pay that now we know what actually happened.



It was not a generic credential stuffing attack.  It was specifically a Facebook credential reuse attack.  And now we understand why Chick-fil-A was the target.  It was because they offered the popular "Login with Facebook" option.  Somebody, somewhere, has a boatload of in-the-clear Facebook login username and password credentials.  And over the course of several months they explored the intersection of that stolen Facebook credential set with the Chick-fil-A customer account database.



LEO:  Which is pretty close to 1:1, I'm betting.  I don't know, but I'm just thinking.  But you'd have to log into - so you'd have to have a Facebook cookie on your browser; right?



STEVE:  No.  No, no, no.  No.



LEO:  Because you'd log in with Facebook, and that'd do it.



STEVE:  Exactly.  You could log in with Facebook.  Then you provide your Facebook credentials, and you're logged into Chick-fil-A.  There were 71,473 Chick-fil-A customers whose Facebook credentials are part of the stolen Facebook dataset, and as a consequence their Chick-fil-A accounts were breached.



Now, when you think about it, if you had a trove of valid Facebook login credentials, what's the most valuable thing you could do with them?  Who wants to log into those random users' Facebook accounts?  There's no money in that.  No.  What you want to do is leverage the increasingly pervasive use of OAuth2 account login to compromise the myriad other accounts belonging to those hapless Facebook users who have chosen to identify themselves to other website properties only through their Facebook credentials.



LEO:  So you're testing it.  You're not trying to get chicken points.



STEVE:  No, no, no.  In fact, the stolen accounts were for sale on the dark web.  People wanted chicken bone points.



LEO:  Oh, okay.  All right, okay.



STEVE:  So they were actually compromising those accounts...



LEO:  So that was intentional, just - boy.



STEVE:  ...[crosstalk] that were worth something.



LEO:  Aim a little higher, kids.  I just...



STEVE:  But think about it.  So again, if you have valid Facebook login credentials, you don't care about logging into people's Facebook accounts.



LEO:  No.  Who cares about that?



STEVE:  You want to log into the other accounts those people have elsewhere on the Internet where they've used Facebook to identify themselves.



LEO:  Now, probably a lot of these people don't use two-factor, either.  Right.



STEVE:  Correct.



LEO:  So all you need is the credentials.  You don't...



STEVE:  Correct.  So now, as the attacker, you have a valuable and potentially widespread attack.  If this is true, as seems extremely likely given all the evidence, we have a perfect example of why the use of OAuth2 for logging in with a common credential poses a significant threat.  What are we loudly telling everyone who will listen about their passwords?  We're saying, "Do not use the same username and password to login to multiple websites."  Right?  Everybody knows that.



But the use of OAuth2, which is now being actively promoted due to its extreme ease of use, is a direct contravention of that advice.  It is the explicit reuse of a single set of credentials, Facebook credentials, across a great many website properties.  And while there may not be much value to an  attacker to use a stolen Facebook credential to log into that Facebook user's account, there might well be significant value in their ability to log in everywhere else that Facebook user used their Facebook credentials to create non-Facebook accounts.



So this begs the question, how are in-the-clear Facebook account credentials harvested?  And we quickly find an example.  Bloomberg News, Technology and Cybersecurity article from October 7th, 2022, okay, just last October, just two months before we're told the attack on Chick-fil-A began, has the headline "Facebook Is Warning 1 Million Users About Stolen Usernames & Passwords."  The article says the company found more than 400 problematic Android and iOS apps.  Games and photo editors tricked users into providing credentials.



Okay, now, just grabbing one paragraph from Bloomberg's coverage, they wrote:  "A typical scam would unfold, for example, after a user downloaded one of the malicious apps.  The app would require a Facebook login to work beyond basic functionality, thus tricking the user into providing their username and password.  Users could then, for example, upload an edited photo directly to their Facebook account.  But in the process, they unknowingly compromised their account by giving the author of the app access."



So what the Chick-fil-A attack probably reveals is the new use to which stolen Facebook credentials are now being put.  Again, obtaining access to a Facebook user's account is far less profitable than being able to log into any and all of that user's non-Facebook accounts where they may have something of value.  Most users, as we know, have not the faintest clue how all of this technology we've given them to use works, no idea whatsoever.  And Leo, as The Tech Guy, you know that better than anyone.



LEO:  Yeah.



STEVE:  So when they're asked by a spiffy-neat app they've just downloaded to provide their Facebook login credentials so that the app can link to and synchronize with their Facebook account, they don't know any better.



LEO:  Right.



STEVE:  Why would they?



LEO:  Right.



STEVE:  And most users would not understand that, in the process, thanks to the fact that they have also been using "Login with Facebook" everywhere they possibly can because it's so much easier to do that, that they are also giving away the access to their accounts at all of those other websites, as well.



We've often noted that the use of OAuth2 is an inherent privacy compromise since that third-party - Facebook, Google, Apple, or whomever - knows everywhere you're using them to log in, and where you are at the time.  But this Chick-fil-A attack, with its subsequent removal of its most popular "Login with Facebook" option, reveals a much darker side of the widespread use of this form of single sign-on solution.  All of the common wisdom urges users to avoid credential reuse, but credential reuse is exactly what single sign-on promotes.  And thus this Fowl Incident has highlighted a worrisome truth behind a growing trend.



And I leave you with the end of the show notes Picture of the Week, which I found on the Chick-fil-A site, which is also a little cute.  It's got three cows standing up, each holding a sign.  The first one says "Eat," the second one says "Mor," and the third one says "Chikin."  So, yes. 



LEO:  Yeah, this is the famous...



STEVE:  The cows are encouraging you to eat more chicken.



LEO:  And less burgers, and more...



STEVE:  And less beef, yes.



LEO:  Yes, more chicken fingers.  Which is amazing because chickens don't even have fingers.  Well.  Well, sir, you have done it again.  You have gotten to the bottom of one of the  nation's most critical security issues, the incredible Chick-fil-A breach.



STEVE:  Be sure to dump your chicken bone points out before the next hack.



LEO:  You've now explained it to all in a way no one else - this is a scoop, sir.  This is a scoop.  You've figured it out.  You know, just turn on two-factor everywhere.  Facebook has it.  I don't know why you wouldn't use it.  And then no one will steal your chicken bone points.  Okay?  Okay.



STEVE:  Or any other bone points anywhere else.



LEO:  Keep your bone points to yourself.  Don't keep, however, don't keep your podcast points to yourself.  Spend them right here.  Join us every Tuesday as we do Security Now!.  I know you want every copy of every show.



Copyright (c) 2023 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#914

DATE:		March 14, 2023

TITLE:		Sony Sues Quad9

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-914.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week fewer questions required longer answers.  What, if anything, can be done about the constant appearance of malicious Chrome extensions?  What's the latest country to decide to pull Chinese telecommunications equipment from their country?  What's the number one way that bad guys penetrate networks, and how has that changed in the past year?  What delicate and brittle crypto requirement is responsible for protecting nearly $1 trillion dollars in cryptocurrency and TLS connections, and how can we trust it?  What's now known about the Plex Media Server defect that indirectly triggered the exodus from LastPass?  And why in the world would Sony Entertainment Germany bring a lawsuit against the innocent nonprofit do-gooder Quad9 DNS provider?  Stay tuned!  The answers to questions you didn't even know you had will be provided during this March 14th "Pi Day" 914th episode of Security Now!.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  We're going to talk about that delicate and brittle crypto technology, the elliptic curve technology that has allowed hundreds of bitcoin wallets to be drained of their value; the latest in the Plex Media Server defect that caused the LastPass hack; and Sony's lawsuit against Quad9, all over an Evanescence album?  It's next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 914, recorded Tuesday, March 14th, 2023:  Sony Sues Quad9.



It's time for Security Now!, the show where we cover the latest news in the world of security with this guy right here, our master of ceremonies, Steve Gibson.  Happy Pi Day, Steve.



STEVE GIBSON:  Happy Pi Day to you, Leo.  And I was saying before the show that we need a Pi Day on a Friday, but we don't have that this time.  So this week we've got fewer questions which required longer answers.  And so we're going to answer some questions.  What, if anything, can be done about the constant appearance of malicious Chrome extensions?  What's the latest country to decide to pull Chinese telecommunications equipment out?



What's the number one way that bad guys penetrate networks, and how has that changed in the past year?  What delicate and brittle crypto requirement is responsible for protecting nearly $1 trillion in cryptocurrency and TLS connections, and how can we trust it?  What's now known about the Plex Media Server defect that indirectly triggered the exodus from LastPass?  And why in the world would Sony Entertainment Germany bring a lawsuit against the innocent nonprofit do-gooder Quad9 DNS provider?  Well, stay tuned.  The answers to questions you didn't even know you had will be provided during this March 14th, Pi Day, 914th episode of Security Now!.



LEO:  How many decimals of Pi can you do off the top of your head?	



STEVE:  We had a serious geek in high school who used to just bore us with, you know, he would just...



LEO:  Go on and on.



STEVE:  And of course we didn't know if he was right or wrong; right?  It's like, okay, Richard.



LEO:  Yeah, he's at some point just saying - I can do, because there's a mnemonic, I think George Gamow the physicist came up with it, how I want a drink, alcoholic of course, after the heavy chapters on quantum mechanics.  So 3.1415926, of course after, 536, no, 7, Q-U-A-N-T-U-M, 7, mechanics, whatever that is.



STEVE:  So I used to - once upon a time I could rattle it off for like a while, but not...



LEO:  I thought you could, yeah, yeah.



STEVE:  Yeah, that was a thing.



LEO:  On Pi Day in the grade schools here, when Abby was a kid, some years ago, they would have a memorize pi thing.  And they'd have a competition to see who could memorize the most digits of pi.  Just crazy.



STEVE:  I was pretty good with, what was that thing that went boop and beep, and the pattern got longer every time?



LEO:  Morse code?



STEVE:  Simon.  Remember?



LEO:  Oh, Simon, yes.  Oh, wow.



STEVE:  Milton Bradley's Simon.



LEO:  That was a tough one, yeah.



STEVE:  And the advantage was that you kept reinforcing what you already knew, and it only extended by one beep each time.



LEO:  Then they came out with - and you probably would only know about this if you had kids - something called Bop It that was like a Simon, but you had to do different motions and rotate it around.  And same thing, you had to memorize the order, and it would tell you what to do.  And then if you got it wrong it'd go, "Oh."  I'm sure there are people in our audience that remember that.  I only know because, again, this is something the kids were into.  Do we have a picture?  I didn't even look yet.



STEVE:  Yes, we do.



LEO:  Going to see it for the first time right now.  Oh, god.  Oh, lord.  Set us up.  What are we going to see here?



STEVE:  Okay.  So my caption for this is "This was their solution?"  And so this - and it looks kind of hazy and grainy because this is one of our listeners took a picture of the screen and tweeted it to me.  He said, "Oh, this has got to be a Picture of the Week."  And oh, indeed.



So this is you're logging in, and you get to the one-time password prompt for your login.  And there's a field where it says "Enter OTP" in red.  But above that, apparently they're having some sort of problem.  So it says:  "We are facing an SMS issue.  Please use 910296 as your one-time password."  And, oh, okay.  So what occurred to me was, okay, first of all, this is weird; you know?  But why not just disable the one-time password challenge for, like, while there's an SMS issue, whatever that could be, rather than hard-coding the proper answer for the one-time password field?  Anyway...



LEO:  It's not a phishing, couldn't be a phishing thing; right?  I mean, it wouldn't be bad guys doing that.



STEVE:  No, I can't see how this could do anything except just be like, well, use this number because we'll accept it.  Now, what would really be interesting would be to remember that number and see if it still works.



LEO:  Forever.  Forever.



STEVE:  After you've got this problem, yeah, like after they've got this problem solved.  Maybe that's the backdoor, and they thought, well, we'll change it.  In fact, we'll see an instance later in the podcast where there's a concern over people having put something in during testing which they forgot to remove later.  But okay.



So first, after seeing the details of another Chrome extension that was seriously compromising the privacy and security of more than 160,000 Chrome users, you know, stepping back from this, it feels as though we need a better solution than we have currently, and I don't know what that would be.  But we have a need for assuring what extensions are doing.



Okay.  So in this case, the extension was called - and okay, it was a little sketchy to begin with, I guess - "Get Cookies.txt" was actually the name of the extension.  And it was being offered through the official Chrome Web Store so it had been vetted and approved.  It allowed users to export from Chrome the content of their cookie files in the old Netscape browser cookie format, presumably so that you had a copy of it, a record of it, for maybe importing it somewhere else.



Okay.  But concerns about the extension were brought to light first of all a couple months ago in January when a Reddit user discovered that, beyond just allowing people to export their cookies from Chrome, and you do that once, right, and then remove the extension.  Well, apparently people weren't.



This Reddit user discovered that the extension was tracking users by collecting user and browser data and uploading it on the fly to a remote server.  At the time the Reddit user posted:  "Version 1.5.0 of the 'Get Cookies.txt' extension is sending details of every page you visit, not just video sites, but every page, back to its developer at the domain ck.getcookiestxt.com."  He said:  "Specifically, for every page you visit, it sends a unique ID for your browser installation, your browser's user-agent string which shows what OS you're using and the browser version number, your language setting, the platform you're on, the current date/time and your current time zone, as well as the URL that you're visiting."



So that's not good, the URL you're visiting with a unique ID tracking you, by an extension that has no business whatsoever doing any of that.  But then it later came to light that, after the version was changed, the extension was not only performing that bit of tracking that had been seen in January, but that it was also proactively sending entire user cookie files that you'd given it permission to acquire in the first place, sending the entire cookie files back to the extension's publisher.  In an update to that Reddit user's initial posting last week, they wrote:  "The situation is now even worse.  The extension is now also sending all your cookies to the developer, too."



When that was confirmed, "Get Cookies.txt" was immediately pulled from the store.  But that wasn't until the extension's upgrade has been in place for some time; and many users, we don't know how many of the 160,000 who were using it had obtained the update and probably had their entire cookie files, you know, all of their Chrome cookies sent to the extension's developer.  And of course, after all, what are we constantly telling everyone they need to do?  Stay current with all updates.  So update to the latest "Get Cookies.txt."  And unfortunately, that hurt you rather than helped you in this case.



So as we know, anytime we either explicitly enable the "keep me logged in on this machine" checkbox, or anytime a website chooses to do that for us for whatever reason, you know, on our behalf, this logged-on persistence, which is a convenience, significantly, since now we're using the web more and more for apps and things, that convenience is accomplished by causing the web browser to accept and store a long-life persistent cookie in our browser's cache.  That cookie identifies us to the site and has the effect of keeping us logged on.  Because individual browser events are separate transactions, the cookie accompanies each one of those queries to the site which says, oh, yeah, that's Steve again.



So anyway, since this is exactly the data that the "Get Cookies.txt" extension was caught sending home to its publisher, the publisher was obtaining the static session data needed to impersonate all of the users of its extension at any of the sites where a persistent session cookie may have been set.  Which is increasingly what's being done because, as I said, we're using browsers more and more as apps.  And since the cookie file indicates its expiration date, if any, in the future, it's trivial for the attacker to determine where you're currently logged in.  That is, to separate the cookies which are just transient session cookies and persistent cookies being used to permanently log you in.



So for what it's worth, if by any chance you or someone you know as one of that extension's 160,000 users, you should seriously consider taking the time to log out of any websites where having some unknown bad guy logging in as you could be a problem.  If you just log yourself out, that will render any of those stolen cookies that may have been exfiltrated from you useless.



And this takes us back to the question I posed earlier.  Like this is happening all the time, you know, variations on this theme.  How do we solve this sort of serious problem?  We want our benign browser extensions to be both powerful and capable.  Otherwise they're not that much use to us.  So they need to know how and be able to do dangerous things, like have access to our browser global cookie cache.



But how can we safely trust extensions from unknown authors which might have a hidden agenda?  I mean, obviously we're putting some trust in the vetting process, which presumably occurred in order for this thing to originally get itself listed.  And even though it had a somewhat dubious purpose, even on day one, the fact that it was then able to change what it did, like it earned some transient trust for being there for a while, it also earned 160,000 users.



And you might be thinking that if the extension's developer was always - if their intention was always nefarious, then they may have well been biding their time, waiting for the total user count to get up to a point where they would then incrementally change what this thing did in order to get additional information on their user base until they finally said, okay, it's time for us to cash out and grab everybody's cookie files and see what mischief we can get up to.



So today's browser ecosystem doesn't really provide any mechanism for deeply vetting an extensions operation.  I mean, that would require more effort than is obviously being put in and may be available for free extensions created by unknown people in who knows where.  It would be prohibitively expensive to be able to fully examine every extension in detail.  And to do that you'd really have to provide source code, which some extension authors might feel was putting a big of a chill on the whole idea.



LEO:  They're in JavaScript, though; aren't they?  Or can they be a binary blob?



STEVE:  There was some requirement that was added after this became a real problem that the extensions not be obfuscated, like not be minimized in order to deliberately make them smaller and to cram out all of the text.  And they couldn't be encrypted.  But still, I mean, you'd have to, like...



LEO:  You'd have to deminimize it.  You'd have to know how to unobfuscate it.



STEVE:  Exactly.  And read through every line of this thing.  And it's sort of clever, too, because, I mean, if this was always the guy's goal, then he first came up with something that needed the permission to have access to the cookie cache and had a valid reason for wanting that.  So that would have meant that there was all of this code in there that was...



LEO:  Legit, yeah.



STEVE:  ...reading and parsing and working with cookies.  So somebody who casually looked over the script would go, well, yeah, okay, it's doing cookie stuff.  So, fine.



LEO:  Right.  Extensions are hazardous.  I mean, that's what Tavis Ormandy was saying when he said don't use your password extension.  And now of course we know Bitwarden has an issue with its extension because it gets tricked by iframes.  I suspect that's a problem with many password extensions.



STEVE:  Right, right.  So interesting...



LEO:  It's better to just use the app.



STEVE:  Interesting problem.  And, yes, exactly.  If you can, I mean, but at the same time look at all the - look at how much now we're using.



LEO:  And you've got to cut and paste; right?



STEVE:  Yeah.



LEO:  Which can be problematic.



STEVE:  And also look at how much we're using our browsers for, like...



LEO:  We live in them, yeah.



STEVE:  Yes.



LEO:  I have a ton of browser extensions, including UBlock Origin.  We've talked about that.



STEVE:  Yup, yup.  In fact, I've got one I love.  It's a session save and restore for Firefox.  I'll use it in order to move all of my work in progress while I'm working on the podcast on Monday here, I like save all of the tabs that I've got in a cute little JSON file and send it over to my other location where I'm able to open essentially exactly the same state that I was in before.



LEO:  I guess remove any extension you don't need and you're not sure of.



STEVE:  Yup, yup.  And here again, for example, would it have been those users' persistent need to export their cookies?  It  seems to me that's a one-time thing.  I don't know why you would want to.  But install the extension, use it to export your cookies, and remove the extension.



LEO:  Yes.



STEVE:  Clearly 160,000 people just left it there because, oh, you know, it's not hurting me.  Well, except it was.



Okay.  So following the U.S., Australia, Canada, New Zealand, Sweden, and Britain, according to reports in German media, the German government is planning to ban the use of Huawei and ZTE equipment from their national 5G telecommunications network.  German officials cited the increasingly common fears that Huawei and ZTE equipment could be used for Chinese sabotage and espionage.  The German government had previously given the green light for Chinese equipment to be used for its 5G network, and some has already been installed.  But the recent Russian invasion of Ukraine, of all things, and Russia's attempt to blackmail Germany from aiding Ukraine through its Nord Stream natural gas pipeline, led to a major change of thinking in Berlin.  And the reporting was that those were the factors, which seems odd.  Russia's not China.  But apparently the two are a little too close for comfort.



According to German media, telcos that previously installed 5G technology from the two vendors may be forced to rip out and replace the equipment.  China is understandably unhappy and said that they hope Germany will make their decision one way or the other without any outside interference, meaning primarily from the U.S. trying to be persuasive here.  The collection of countries are already on various schedules to remove any existing Huawei and ZTE equipment from their networks.



And I continue to be distressed because, you know, this seems xenophobic given the lack of concrete evidence.  But the argument always is there could very well be no evidence despite equipment being compromised.  And congressional expert testimony confirmed that it is virtually impossible to know one way or the other.  The good news is that the world does appear to finally be waking up to the broad potential for the abuse of today's advanced technologies.  I'm hoping that what we're seeing may be just a bit of a reflexive reaction to this new awareness.



We've all had it, right, for the last 18 years of this podcast, knowing what was possible.  Back in the early days of viruses, Leo, we were often just sort of musing over the fact that all they really seemed to do was propagate.  They didn't really do anything bad.  But we all knew they could.  And every so often one would be a low-level formatter or something, which would definitely ruin your day.  So I hope that this is going to calm down over time, and that we'll end up reaching some balance point which is sane.



The abbreviation DFIR is a term in the security industry, stands for Digital Forensics and Incident Response.  And there is an annual DFIR report which is released, obviously annually, that came out last week.  The report contains a bunch of interesting statistics.  But probably the most interesting fact, because it's like maybe the most significant fact, was its breakdown of the way bad guys are still gaining their initial entry into our systems.  And I've got in the show notes a big pie chart which is hugely dominated by one big slice of pie.  In fact, it's not a slice of pie.  It's the pie left over after you take out the slices of pie.



Here it's an overwhelming sea of blue which represents phishing.  The way bad guys are still getting in, holding a 69.2% share of intrusion methods, where a victim was induced to click on a link or open a document which in every case was received via email, either through a mass mailing to an organization's employees, hoping that one of them will click on it to get in, or sometimes targeted to a specific individual in order to make the email more like something that they're expecting to see.  So nearly, just shy of 70% of the way the forensic studies are showing people are getting in is somebody on the inside clicking on a link, inviting the bad guys in.



The DFIR report noted that as in 2021, the previous reporting period, also now last year, 2022, the majority of intrusions originated from mass email campaigns spreading malware in this way.  They also noted, and I thought this was very interesting, that one of the biggest shifts in this space was the discontinuation of macro-based attacks leveraging Word and Excel, which was the result of that decision that we talked about at the time last year of Microsoft finally choosing to disable the default execution of macros for Word and Excel.  I mean, all they had to do, and finally did, was pop up a warning notice, rather than just having them run by default.



Think about how long that has been the default case and for how many years before Microsoft did not take this decision their users, Windows users, were being hurt by this widespread use of default execution of macros in Word and Excel.  Throughout the years before this we kept lamenting Microsoft, how they left scripting enabled in email, despite the fact that no one wanted scripting in email.  And only the bad guys were abusing that.  That battle to finally disable scripting was won, and now at long last we have, you know, another change was made that this report demonstrates markedly changed the nature of the attacks.  So, boy.  It just, again, one of the things we have seen in so many different ways in this podcast is the nature of inertia.  Inertia is amazing in all of its many forms.



Taking second place in the "intrusion entry methods" at 15.4% is drive-by compromises.  So those are the watering hole attacks where a malicious website is first set up, and then victims are lured to visit.  Or in some cases just opportunistic, where a malicious site is set up, and somebody who jumps in compromises themselves.  So that's 15.4%.  The remaining two major ways of gaining entry into networks, which completes our pie, actually they share the remaining 15.4% split in half, each getting 7.7%.  Public-facing applications is one of those by which they actually meant the exploitation of Microsoft Exchange when you drill down into the report further.  And the abuse of valid accounts is the other 7.7%, meaning brute force remote desktop protocol or SQL Server entry in order to get onto a network.  So that rounds out all 100%.



But that big blue slice of pie representing nearly 70% of all intrusions, and arguably preventable, it would take some effort, but it could be done.  And we could hope that at some point somebody at Microsoft will wake up one morning and say something like, hey, you know, email phishing attacks are a big problem.  Why don't we, I don't know, run email in its own sandboxed virtual machine so that bad stuff from the outside is contained and can't take over the entire machine?  What do you think?  Well, maybe in another decade we'll get email in a protected sandbox virtual machine.



Okay, now, Leo, this next one is a biggie.  So I think we need to take a break.



LEO:  Fair enough.



STEVE:  And then we're going to plow into something known as the "Polynonce Attack."



LEO:  Uh-oh.



STEVE:  You don't ever want your nonces to be poly.



LEO:  I know what a nonce is, and I don't want to be messed with.  No way.



STEVE:  You don't want polynonces.



LEO:  Unh-unh.  Is nonces.



STEVE:  This is not the Polynesian attack.  This is the polynonce attack.



LEO:  Okay.



STEVE:  Just to be clear.  The guys at Kudelski Security Research, we've spoken about them before, they've discovered a new and novel attack which they call "polynonce," and we'll see why here shortly, which enables this attack.  Okay.  It is available in instances where a weak pseudorandom number generator was used to create a, well, was used in the use of a digital signing algorithm.  This allowed them, among other things, to recover the private keys for hundreds of bitcoin wallets which were for transactions that were signed using weak pseudorandom number generators.



Okay.  So I'm going to sort of give away the story here in their  upfront summary, but I think that's a good thing to do because then you will understand where we're headed with this because this is going to be a bit of a deep dive.  You know, make sure that your propeller spring is wound up on your beanie cap.



So they said:  "In this blog post we tell a tale of how we discovered a novel attack against ECDSA" - I'll explain what that is in a second and why it's important because it's everywhere - "and how we applied it to datasets we found in the wild, including the Bitcoin and Ethereum networks.  Although we didn't recover Satoshi's private key," they said, "we'd be throwing a party instead of writing this blog post."



LEO:  Yeah.  They'd be on an island somewhere.



STEVE:  That's right.  They said:  "We could see evidence that someone had previously attacked vulnerable wallets with a different exploit and drained them.  We cover our journey, findings, and the rabbit holes we explored.  We also provide an academic paper with the details of the attack and open-source code implementing it, so people building software and products using ECDSA can ensure they do not have this vulnerability in their systems."  And how.



They said:  "Part of the Kudelski Security Research Team's activities includes looking into new vulnerabilities and exploits.  A few months ago, while researching ECDSA nonce attacks, a member of our team discovered a more general way to exploit complex relations between nonces to retrieve the signing key."  And I'll explain a lot of this here as we go along.  They said:  "A review of existing literature seemed to confirm that this was indeed a novel insight, so we started digging into it.  If you're interested in the math and details surrounding the attack, there's a link to the paper."



Okay.  So we're about to be hearing a lot about ECDSA.  ECDSA is an abbreviation for Elliptic Curve Digital Signature Algorithm.  It is now, even still, the current state-of-the-art choice for efficiently producing a secure signature of a hash of a message where the signer's key is kept secret, and they publish their matching public key, which is then used to verify their signature.  So ECDSA is used everywhere.  It's the algorithm used to sign Bitcoin, Ethereum, and other cryptocurrency blockchain transactions.  It's one of the signature algorithms available to TLS connection handshakes when a site is using an elliptic curve certificate.  So the exploitation of poorly chosen random nonces could and would have far-reaching consequences.



Okay, so I'm going to read what they wrote, and not all of it because they really get into the weeds later.  And not because I expect anyone to exclaim "Oh, yes, of course, how obvious!"  No.  But because it will give you some sense that this was not trivial, and that these guys are geniuses.  Oh, and if you're listening to this while operating any heavy equipment, please consider pausing your work during this rough patch.



Okay.  So they wrote:  "In a nutshell" - and, yeah, this is the nutshell version.  They said:  "The attack looks at the fact that you can always define a recurrence relation among nonces used in different ECDSA signatures as a polynomial of arbitrarily high degree, with unknown coefficients, modulo the order of the curve's generator point."  We're going to take their word for that.  They said:  "If you have a set of N ECDSA signatures under the same private key, and this recurrence relation is of degree D, then under some caveats we'll talk about later you can use the ECDSA signature equation to rewrite the polynomial in terms of the private key and the recurrence unknown coefficients."  Okay, that was the insight that one of these guys had.  Obviously they're way deep in the weeds of this.



So they said:  "We have found that the unknown coefficients can be eliminated from the polynomial, which always has the signer's private key among its roots.  So, if the degree D is low, and you have enough such correlated signatures where the number of signatures is N, and you need N to be equal to D+3 at least, then you can perform a key recovery attack" - and of course that's the key, the key term here.  This all boils down to a key recovery attack.  They say:  "...by simply finding roots of a polynomial with known coefficients over a finite field," which they note is an easy task on a computer.



"To run the attack in practice," they said, "the following is required:  a minimum of four signatures generated by the same private key, the associated public key, the message hash associated with each signature."  And note that all of those will be available anywhere that ECDSA is used.  And they said:  "If the nonces" - what are supposed to be pseudorandom, completely random.  They said:  "If the nonces obey the recurrence relation, we retrieve the private key used to generate the vulnerable signatures."



They said:  "The more signatures used in the attack, the slower it gets, but the more likely it is to succeed."  Because, you know, they've got more information.  They said:  "If you attack N signatures with their N nonces, follow a recurrence relation of degree at most N-3."  And the higher the degree of the polynomial, the slower and more difficult the attack.  But they said:  "If you attack N signatures with their N nonces with a recurrence relation of degree at least N-3, then you can perform a key recovery attack on ECDSA!" they wrote, because that's their discovery.



Okay.  So it's big news because it means that the choice of repeated nonces, that is, the choice of the nonce which is used as part of elliptic curve digital signing, must be of high quality.  For any given repeated signer their chosen nonces must really be random.  And, by the way, must absolutely never be reused.  Reusing a nonce is like the biggest no-no.



LEO:  Is this related to - remember when NIST kind of cracked an earlier elliptic curve function?  Not cracked it, but provided constants.



STEVE:  Yeah.



LEO:  Right?



STEVE:  It was the DRB, the deterministic random bit generator, that BSAFE had that RSA was publishing as their default PRNG, and it was shown not to be that good.



LEO:  Because they used starting seeds that the NSA had specifically chosen. 



STEVE:  Yeah, they used magic numbers and didn't tell anybody where they came from.



LEO:  So this is not the same thing at all.



STEVE:  Not the same thing.



LEO:  Okay.



STEVE:  But so far what we've learned is that the ECDSA is everywhere.  It is the preferred fast, small, digital signature algorithm.  And where it is known to be exquisitely sensitive to the quality of the nonces, a nonce must be chosen for each signature, and they have to be chosen well.  What these guys have - their inspiration was they basically came up with the attack of, if the nonces were not being chosen well, now we know actually how to take advantage of that.



LEO:  Ah.  I wonder if I can get in my wallet this way.  This is interesting.  Okay.



STEVE:  So you can sort of think of it as the price of doing business with ECDSA.  Yes, it's a terrific algorithm.  It's super secure using very short keys.  But it's also quite finicky.  It's well known that even a single inadvertent reuse of an ECDSA nonce will completely expose the signer's private key.  You've still got to do a bunch of math, but it is available.  So you reuse a nonce, and you sacrifice all of the money in your cryptocurrency wallet.



LEO:  Oh, this wouldn't be useful for me, though, because they can't choose the wallet that they want to look at.  They need to look at all of them and look for matches; right?  Is that kind of the way they - you couldn't just give them the wallet and say is this easy to crack?



STEVE:  If you ever transacted with that wallet, if you...



LEO:  I have, yes.



STEVE:  Then there's a possibility because the earlier uses, which is when you and I both were messing with this in the beginning...



LEO:  Mm-hmm, long time ago.  That's why I forgot the password, yeah.



STEVE:  There tended to be a reuse of private keys.  In theory, ECDSA you should have an ephemeral private key also for each signature.  You should just make one up because you're able to then tell people what the public key is.  You keep the private key secret.  But you don't need to always use the same private key.  Back in the beginning we were tending to do that.  You were probably doing that.



LEO:  Yeah.  I was using the bitcoin default.



STEVE:  Bitcoin Core, I think.



LEO:  Bitcoin Core, yeah.



STEVE:  Yeah.



LEO:  Oh, good.  There's hope.



STEVE:  They said - yeah.



LEO:  This may be a quarter of a million dollar show for me.  Okay, keep going, keep going.



STEVE:  They said:  "We tested the attack on a specially crafted set of signatures to verify that it works.  It does.  In simpler words," they said, "what our attack means is that every time an ECDSA signature is generated, the signature itself gives us a relation between the nonce and the private key.  If the nonces are truly randomly generated, this should never be a problem because the chance that a number of nonces picked at random fit on a low-degree polynomial recurrence relation is negligibly small.



"But there's a catch," they said.  "Nonces are usually output by a pseudorandom number generator rather than being really random, and PRNGs are deterministic algorithms with relatively low complexity.  In the best scenario, the PRNG used is complex enough and cryptographically secure, meaning among other things that any polynomial correlation between its outputs will have such an astronomically large degree that you can safely consider it indistinguishable from truly random.



"But," they said, "weak PRNGs are basically everywhere.  Take, for example, the simple case of a linear congruential generator (LCG), which is the typical textbook introduction to PRNG implementations.  LCGs are to PRNGs what ROT13 is to encryption and '123' is to secure passwords."



LEO:  Okay.  Good, good.



STEVE:  In other words...



LEO:  Not good.



STEVE:  Not good.  And they said:  "Despite that, due to their simplicity and popularity, they're the default choice for many non-critically secure applications."  And even that I would be skeptical of.  And they said:  "And it is totally possible that a 'placeholder' LCG implementation slipped into production code without being replaced by a secure one."



Okay, now, I'm going to interrupt.  I'm going to do a little segue here for a minute because I happen to like linear congruential number generators.  So I'm going to talk a little bit about them and how I just used one.  And then we'll get back to this.  So they are very handy for many purposes.  But in no way imaginable would anyone consider them secure for any purpose.  The entire LCNG (Linear Congruential Number Generator) algorithm is to simply take the current seed, the previous number generated; multiply that by a fixed number whose binary representation has a complex bit pattern, and sometimes a prime number is used, although it's not necessary; then add a second constant value.  That's it.  And that's its attraction.  A single multiplication followed by a constant number, followed by a single addition by a different constant...



LEO:  It's not random at all.



STEVE:  Oh, god, no, Leo.



LEO:  It's just not obvious where it came from.



STEVE:  Exactly.  Basically it's jumping forward in its number space by a certain amount.  So, you know, on the negative side it's entirely predictable.  Given any output, it's possible to immediately produce all future and even all previous outputs, if you run it backwards.  You know?  It's awful.  So as a source of randomness, the term "random" doesn't apply at all.  There should never be an "R" anywhere near it.  What it has going for it, however, is speed, since nothing could produce a noisy pattern of data faster.  Which is why, as it happens, SpinRite received one a few months ago.  And since I think everyone will find this interesting, I'll take a minute to explain.



A couple months ago we hit an unexpected problem with SpinRite, when some very troubled hard drives would encounter some damage they could not handle.  Now, the proper behavior would be for the drive to stop the transfer and return an error code along with the number of sectors that were successfully transferred prior to that error, prior to encountering that error.  That's in the API.  That's in the spec.  That's what hard drives are supposed to do.  And that would allow SpinRite to then zero-in on the one sector that tripped it up, and then it would work to recover that sector's data.  That's what SpinRite does.



So as I said, that's what drives are supposed to do.  But it turns out that many drives which our testers have will just hang and wait for SpinRite to give up and timeout the transfer.  Or sometimes they'll return a "command aborted" error without any indication of where they were when they became upset.  So now we had a problem.  Previous versions of SpinRite, all previous versions of SpinRite, which did all of their work through the system's BIOS, were therefore limited to the BIOS's maximum 127-sector transfers, which is about 64K.  So SpinRite would drop out of transferring those 64K blocks at a time and switch over to single-sector mode to locate wherever the trouble that might be occurring at that particular sector.  So it would just switch over and do them one sector at a time to find the problem.



Well, that worked great for finding one or more troubled sectors within a 127 possible problem sector block.  But 6.1 achieves its tremendous speed increase by transferring 32,768 sectors at once, which is 16MB rather than 64K.  So dropping out of that to switch over to one sector at a time was really no longer an option.  I mean, it would take forever.  So the solution to this dilemma is why I added a simple linear congruential number generator to SpinRite.



SpinRite zooms along, running at the maximum speed that the drive can go, transferring 16MB at a time until it hits a problem.  If the drive returns the number of sectors that were successfully transferred, as it should, then SpinRite immediately zeroes in on that sector, works on its recovery, reaches a conclusion, then resumes the interrupted transfer with the next sector after the one that caused the interrupt.  But if the large transfer stumbles without indicating where among those 32,768 sectors that were requested, SpinRite uses its new linear congruential number generator.



What it does is it fills that 16MB transfer buffer with one sector of noise pattern repeated over and over, 32,768 times.  Now SpinRite has a known pattern of noise throughout the transfer buffer.  So it re-requests the same 16MB transfer.  And this time, when the drive craps out without saying where, SpinRite can determine that for itself.  It scans forward through that transfer buffer searching for the first sector that matches the noise sector pattern.  That will be the first sector in the transfer that was not overwritten by the drive's successful reading of sectors, so that's where the transfer was interrupted.  And that's the sector that SpinRite then needs to work on recovering.



LEO:  That's very clever.  That's cool.



STEVE:  It's so cool.



LEO:  But it has to be random or you wouldn't be sure that you were on the right place.



STEVE:  Exactly.  It has to be random or you wouldn't be sure that the drive might not have contained that data, that sector of data.  And you'd get a false positive match.  So if anyone's been wondering how all this time is being spent, there's an example.  6.1 is going to be far better than any SpinRite that ever existed before, which is why I'm getting so excited that it's also getting very close.



So that's a perfect example of where a linear congruential number generator can be useful and come in handy.  Because, for example, there we have zero need for security.  You just don't even consider it a random number generator.  We just want noise.  Okay.  So picking up on this team's investigation, they said:  "The bitcoin blockchain" - oh, I love this, Leo.  "The bitcoin blockchain is basically a large public mine of ECDSA signatures."



LEO:  Oh.



STEVE:  "In fact," they wrote, "ECDSA has been used as the default signature algorithm since Bitcoin's creation in 2009.  We know that, in principle, most ECDSA signatures in the Bitcoin network are ephemeral" - that's what I was mentioning before - "in the sense that the generating secret key is only used once, but we also know that this practice is not always in place, especially for the older transactions.  And also Bitcoin has the advantage that the blocks follow a temporal history, which puts a certain degree of order on the signature generation, although that's only approximate because there's no way to determine the order in which signatures in the same block were generated since the timestamp is only recorded for a block, not per signature."



They said:  "The problem is that these are mainly speculations, and we have no clue how accurate all these speculations are.  So it's time to verify.  We downloaded," they wrote, "and installed Bitcoin Core, the official Bitcoin client, and let it synchronize the whole chain."  Get a load of this.  "The sync process took about a day on a fast fiber connection, and the total blockchain size was about 430GB, up to block number 752,759 on September 5th, 2022."



LEO:  It's bigger today, yup.



STEVE:  Uh-huh.  "Dumping all the signatures and original messages from the raw blockchain data took 24 hours.  The resulting output file size was 271GB and contained 763,020,390 unique signatures.  This file contained, on each line, the output address, the Elliptic Curve DSA signature R and S values, public key, transaction ID, original message, and block timestamp.  We grouped the signatures by public key and then, within each group, sorted signatures by timestamp to have more chances of picking consecutive ones.  At this point, we had a dataset ready to run the attack.  But first, here are some statistics about the dataset."  And actually these are important, so I included them.  They said:  "These signatures were produced by private keys associated with 424,549,744 unique public keys."  Okay.  So 424 million-plus unique public keys.  Of those 424 million public keys, 390 million, or about 92%, produced only one signature.



LEO:  That's good; right?



STEVE:  Yes.



LEO:  Okay.



STEVE:  Exactly.  Remember that for Bitcoin, private keys are supposed to be ephemeral and are supposed to only be used once.  So it's to be expected that 92% of all those signatures would use a completely unique key.



LEO:  Well, you'd like it to be 100%.



STEVE:  You would.  That means there's still hope here.



LEO:  Okay.



STEVE:  And so that means that there's no attack possible on those since these guys are looking for weak nonces occurring during key reuse with the same private key.  So they needed to find instances where multiple signatures used the same key.  And they did.  They wrote:  "There were 34 million public keys with at least two signatures, used in at least two signings, two signatures; 18 million with at least three signatures; 12 million with at least four signatures; 9.6 million with at least five signatures, and 7.8 million with at least six signatures."



LEO:  That means six seen elsewhere, five seen elsewhere, duplicating six other signatures.



STEVE:  Correct.



LEO:  Yeah, okay.



STEVE:  Yes, yes.  So that is to say the same private key was used to sign six transactions.



LEO:  You have a collision with six transactions when you'd expect to have no collisions in trillions of transactions; right?



STEVE:  Right, right.  So they said there was a considerable number of public keys with over 200,000 signatures.



LEO:  Ooh.  Ow.



STEVE:  Yeah.  Somebody just didn't ever change their public key, or their private key, because the private key and the public keys match; right?  It's a one-for-one relation.  The public key associated with the most signatures had 3.4 million signatures.



LEO:  Oh, my god.



STEVE:  Yeah.



LEO:  So there was one key shared by 3.4 million accounts.  They had the same public key.



STEVE:  One key used in 3.4 million transactions.



LEO:  Transactions, not accounts, okay.



STEVE:  Right, transactions.  So they said the attack is generic and can be run with at least N=4, that is to say, where you've got a single private key used in at least four transactions, N=4 signatures, and they call that the "linear case."  That is to say, if N=4, then they're solving a linear function.



LEO:  Okay.



STEVE:  But can also be run with more signatures.  And the more you have, the better your chances.



LEO:  Of course, yeah.



STEVE:  They said, for example, five signatures for the quadratic case and six signatures for the cubic case, or even more signatures.



LEO:  So it gets easier to crack with more signatures.



STEVE:  Actually it gets harder to crack but more likely.



LEO:  Ah, okay.



STEVE:  Yeah.  So they said:  "The linear case will also detect repeated nonces, but it is more general because it can exploit any linear recurrence relation."  They said:  "However, we wanted to go even further and run the quadratic case with N=5 because we thought it might give more interesting results.  We considered the cost/benefit ratio of performing cubic or higher order attacks to not be worthwhile, so we stopped at the quadratic case."



LEO:  So they cost too much compute time.



STEVE:  Right, exactly, meaning batches of five signatures.  But notice if you're doing five at a time, but if you had seven signatures, you could slide a window along those seven, taking them five at a time; right?  So they said:  "Since we sometimes have more than five signatures associated with a given public key, we decided to perform a sliding window over the signatures sorted by timestamp and run the attack on each window of size N, where N=5.  So," they said, "how did it go?  We ran the sliding window attack with N=5 on a 128-core VM, and it was completed in two days and 19 hours.  The estimated cost of the attack was about 285 USD."



LEO:  Probably did it in the cloud, okay.



STEVE:  They did.  "We broke 762 unique wallets."



LEO:  Was mine in there?  Please?



STEVE:  I don't think so because, although you probably haven't checked yours...



LEO:  No.  I don't know how to run this attack.



STEVE:  Right.  "All of these had a zero balance."



LEO:  Oh.  Oh.  



STEVE:  Because they were not the first people there, Leo.



LEO:  Oh.  They'd already been drained.



STEVE:  Yes.  They said:  "Interestingly enough, we could break all these wallets, not because of a linear or quadratic recurrence, but because there was at least one repeated nonce in the signatures.  So," they wrote, "it looks like the common mishap of ECDSA implementations using a repeated nonce was the cause of the trouble."



LEO:  So they went to all this effort to try to fight a very sophisticated attack.



STEVE:  Yeah.



LEO:  And what they really uncovered was there was a trivial attack which had already been uncovered and used.



STEVE:  Exactly that.



LEO:  Wow.



STEVE:  In other words, these guys developed a highly sophisticated and subtle attack, which they showed would have worked, and which would have been able to detect subtle failures in nonce choice.  But what they stumbled upon during this work was the biggest no-no in the use of Elliptic Curve DSA, which is a single reuse ever of a nonce when signing under the same private key.



LEO:  Wow.



STEVE:  This meant that a trivial attack against those wallets was possible; and what's more, somebody had already done that.  And Leo, they tracked them down.



LEO:  Oh, my.  Oh, my.



STEVE:  Because the bit, you know, it's a ledger; right?



LEO:  Because you transfer the stuff from this account into another account.



STEVE:  Exactly.



LEO:  And that account number is not private.  It's not hidden.  It's public.



STEVE:  Exactly.  It's a public ledger.



LEO:  This is exciting.  This is great.  We could have made a whole podcast series out of this.  This should have been the Security Now! Season 20.  This could have been amazing.  Keep going.  This is great.



STEVE:  They said:  "Since we only ran the attack using a window of size five so far, we may have missed a few vulnerable wallets that would only have been found for public keys that had exactly four signatures.  So we reran the attack with N=4 on only the signatures from wallets with exactly four signatures.  We were able to break 11 new wallets with a zero balance and at least one repeated nonce, thus increasing the total amount of broken wallets to 773."  So in other words they didn't find any new wallets because, again, they were finding a superset, and it turned out, well, there were other repeated nonce instances.  So unfortunately, there were a bunch of people out there who had a bad, you know, who had a bad bitcoin client that was reusing nonces when they were doing transactions.  And they lost all their money.



They said:  "We suspect, and in some cases have evidence," and they said, "as we will discuss later, that all these wallets have zero balance because they have already been hacked in the past due to the repeated nonce vulnerability.  We also estimated the total theoretical amount of tokens that may have been stolen from these 773 wallets to be 484 BTC, at a value of approximately 31 million USD at Bitcoin's peak."



LEO:  Well, one thing that's good is, since I know there's still bitcoin in my wallet, I'm not one of those...



STEVE:  You're not, yes.



LEO:  ...duplicated nonce wallets.



STEVE:  Yes.



LEO:  Yeah, okay.



STEVE:  So anyway, then they go on at length talking about various other attacks.  Those are the rabbit holes that they refer to.  And they talk about all the evidence of, you know, they identified a number of actors, and they found the person who, you know, the bitcoin address that essentially made these transfers.  Anyway, they conclude, saying:  "So, since we aren't sipping Mojitos on a beach in some exotic location, you can tell we did not gain access to Satoshi's wallet.  But we recovered..."



LEO:  Well, they might have, it's just that somebody got there first.  It could - I guess not.



STEVE:  Yeah, actually, doesn't Satoshi have an insane number of bitcoins?



LEO:  Oh, yeah, billions, billions and billions of dollars, yeah.



STEVE:  So they said:  "We recovered the private key of some Bitcoin wallets showing that the attack works.  We only scratched the surface by looking at Bitcoin, Ethereum, and some TLS connections.  With this initial look, we wanted to ensure that an attacker could not cause financial damage before releasing the details.  But there are many other locations where Elliptic Curve DSA is used, such as additional blockchains, batches of PGP signatures, other TLS connections, and embedded devices, just to name a few.  We release this information along with code so that people and organizations can proactively ensure they are secure against these attacks and create more robust systems.  We hope you find this useful."



LEO:  What would be your fix for this?  Somebody's asking in the chat room.  It wouldn't be changing your password.  You'd probably transfer it out of your old wallet, make a new wallet, and transfer it into a newer wallet where presumably this nonce won't be reused.



STEVE:  So, yes, so you absolutely, well, so nonce reuse is never a problem as long as you're not reusing your private key.  The easy thing to do is make sure you get a new private key.



LEO:  Oh, rekey it.



STEVE:  For every transaction.



LEO:  Okay.  Okay.



STEVE:  Yes.  And that's why 93%, or 92% whatever it was, of all transactions are using unique private keys.  Everyone should have a client doing that.



LEO:  Okay.



STEVE:  In the earliest days, people were not doing that.



LEO:  So these are all older wallets.



STEVE:  Yes.  Well, older transactions.



LEO:  Transactions; right.



STEVE:  And so those would have been older transactions using older bitcoin clients.



LEO:  I wonder if Bitcoin Core now, I'm sure it now rekeys each time, I would think.



STEVE:  Given that 92% of all transactions are, that must be the case.  And they're supposed to be.  But they're, you know, they're not always.



LEO:  Right.



STEVE:  You know, again, everybody, there's like all these different bitcoin gizmos out there.  And some of them just aren't well made.  And some of them apparently, not only are they not bothering to use a new private key for every transaction, they're also, who knows, how can you reuse a nonce?  How could you get anything that you thought was even - that you even thought was supposed to be random and get it twice.  Wow.



LEO:  Wow, wow.



STEVE:  So, you know, there are a couple of takeaway lessons here.  One is that details really matter; right?  The crypto gurus who invent and create these algorithms admonish their implementers that, if they want to take advantage of the elegant, though quite finicky, ECDSA algorithm, they're going to really need to be double-damn certain to only ever use it with truly high-quality random nonces in an application where they  need to have a static signature.  And most Elliptic Curve DSA signatures are static; right?  If you have an Elliptic Curve TLS certificate, that's not changing.  So you're depending in that instance on the quality of the nonce being highly random.  But so the point is, yes, it's a great algorithm, but you've got to use it correctly.



And the second takeaway, I think, is in the form of a question.  If details like that really matter that much, if a critical algorithm is so brittle and sensitive to difficult-to-control implementation mistakes, should it be chosen and used in critical applications?  You know?  Because it's no longer the case that only one of these is ever made.  Bitcoin went crazy, and everybody said, oh, I'm going to create a wallet.  I'm going to create my own because why not?  Let's write it in JavaScript.  So should it be chosen and used in critical applications such as  to protect the storage of hundreds of billions of dollars in cryptocurrency wealth?  And by the way, I was curious.  If anybody else is, as of the beginning of this year, total wealth stored in cryptocurrency was $804 billion, with around 320 billion of that in Bitcoin.



So this is the sort of way these things happen; right?  We see it over and over.  The inventors of the packet-switching Internet could have never foreseen what their experiment grew into.  And, yes, for all that it's amazing, it also has some weaknesses in the face of deliberate abuse.  And with Bitcoin, as a proof of concept, Satoshi invents and designs an intriguing system.  And he chose Elliptic Curve DSA to sign transactions because, sure, it's the best, though it also needs to be handled with extreme care.  Little could he have possibly known what his experiment would grow into.  And in retrospect, choosing something less fragile, some less fragile crypto would have probably been a better choice for something that was to grow into a global phenomenon.  And of course he could have never predicted what was to come.



LEO:  So I use Curve 25519.  That's an elliptic curve, but that's not - it's different.



STEVE:  That's probably not using DSA.



LEO:  Right.



STEVE:  Well, although it is the default signing.  So we'd have  to look and see which of - so 25519 is one particular very popular elliptic curve.  There are other elliptic curves.  But all of them then share the same set of algorithms which are put on these different elliptic curves.



LEO:  So I use it with SSH primarily.  I shouldn't be announcing this, but that's what I primarily use it for.



STEVE:  Yeah, I mean, that's what SQRL uses.



LEO:  Yeah.



STEVE:  I also chose 25519.



LEO:  It's also because the keys are short and easily copied and pasted.



STEVE:  In my case it was because it had some particular properties that Dan Bernstein, who is the inventor of 25519, it's got some really cool properties that enable the use of, you know, enable SQRL's crypto.



LEO:  Yeah.  Okay.  I won't worry.



STEVE:  Very cool.



LEO:  And since my coins are still in my wallet, I guess I don't have to worry about this attack, either.



STEVE:  No.



LEO:  Well, doesn't really matter, I can't get to them anyway.



STEVE:  I think we're at an hour and 18.  Let's take our third break.



LEO:  Yeah, let's take a break.  Yeah, yeah, yeah.



STEVE:  And then we will continue.  



LEO:  Yeah.  And, you know what, if the wallet had been emptied, I would say, well, I'm glad somebody got the use of it.  Oh, lord.  What a world, huh?



STEVE:  Now, there is no relation between PlexTrac and the Plex Media Server.



LEO:  No.  Oh, let's hope not.



STEVE:  Just want to make that clear.



LEO:  Oh, no.



STEVE:  Last week we were talking about the growth of CISA's KEV database, where KEV is the abbreviation for "Known Exploited Vulnerabilities"; and how, while it grew much faster last year than in any previous year, an examination of the dates of the CVEs that were added during this most recent past year revealed that the large majority of these were not new problems being exploited, but rather old problems that had never been patched.  So I noted that CISA had just added CVE-2020-5741.



LEO:  2020.  That's three years old.



STEVE:  That's right.  And what is 5741, you might ask?  Well, it's a deserialization flaw of untrusted data which was found, as we noted, three years ago in the Plex Media Server for Windows.



LEO:  But who would be - who would be running three-year-old unpatched versions of Plex on their Windows machines?  Who indeed?



STEVE:  Who indeed, my friend.  It happened that an unfortunate LastPass developer...



LEO:  Oh.



STEVE:  Yes, was doing so, after which a distinct lack of fortune was visited upon all LastPass users.



LEO:  That's a good way to put it.



STEVE:  A remote, authenticated attacker is able to execute their arbitrary Python code on the victim's computer.



LEO:  Can we make it any easier?  Holy cow.



STEVE:  So now we know that this developer was using a publicly exposed Plex Media Server which was three years out of date, since of course CVE-2020-5741 had been found, was known, and had been fixed.



I've been saying for a while now that any serious cyberwarfare agency or group across the globe must be maintaining a vulnerability and exploit database indexed by target vendor.  So in the instance of this second LastPass attack, LastPass's developers were identified, probably with the aid of the first attack on that developer network; right?  Then they were tracked down and identified at home, and their home IP addresses were scanned.  When port 32400 was found to be accepting inbound TCP connections at one of those IPs, that port was looked up, and the Plex Media Server was found to be the most common user of that port.  Then the attacker's master vulnerability and exploit database was queried for "Plex," and a three-year-old remotely exploitable vulnerability stood out.  Could we be this lucky, the attackers probably thought to themselves.  And indeed, they were, and we weren't.



LEO:  I know a little Python.  I think I could take advantage of this, guv'nor.  



STEVE:  That's right.  And I don't know if anything more has been learned.  But I did hear something about North Korea...



LEO:  Oh, really.



STEVE:  ...being the presumed source of the attack, yeah.



LEO:  The thing that makes me scared, it was so clearly targeted against LastPass.



STEVE:  Oh, boy, yeah.



LEO:  And that means they knew what they wanted, which is the vaults.  Which means, I presume, they knew what to do with them.



STEVE:  Yeah, yeah, which is the reason we're no longer all using LastPass.



LEO:  Yeah.



STEVE:  Okay.  One quick random note, and then we're going to talk about Sony suing Quad9.  I wanted to mention "Andor," Leo.



LEO:  Yeah.  It's good; isn't it?



STEVE:  In a word, "Wow."



LEO:  Yeah.



STEVE:  And of course there'll be no spoilers here.  But I want to set the stage a little bit and give our listeners just a bit of background to raise their curiosity level.  And if you're someone who wants to hear absolutely nothing in advance, then skip forward 60 seconds.



"Andor" presents the story of an orphan, Cassian Andor.  The series is set well before the events of Luke and his princess sister.  It tells the story of the early rise of the Empire as it gradually displaces corporate rule for imperial rule and tightens its grip on the galaxy's citizenry, which is increasingly becoming stratified into upper and lower castes.



Mostly, Cassian just wants to be left alone.  He grew up hard and survives by reselling tech that he steals from around the fringes of the Imperium.  Despite being raised by an adopted mother who has been seeing what's happening to the galaxy and wants to rebel, he has zero interest in any "cause."  He holds no such ideals.  He just wants to be left alone and not be told what to do.  Over the course of these first 12 beautifully crafted episodes, we watch that change.



If you don't already subscribe to Disney+, I cannot imagine that you would regret subscribing just long enough to watch this first 12-episode season.  There might even be a free trial period available to new subscribers.  I was not offered one because a year or two ago, after watching the first season of "The Mandalorian," I canceled my subscription.  The Mandalorian wasn't horrible, and Lorrie loved Baby Yoda.  I kept hearing, "Ohhhhhhh."  But neither did it strike me as being all that great.



LEO:  It's just a rubber puppet.



STEVE:  "That's so cute, ohhhhhhh."  Anyway, but "Andor" is another thing entirely.  And Leo, I think that this very sober and serious series may be the best Star Wars property I've ever watched.



LEO:  I agree.  And I think it is because it's very little Star Wars, in a way; right?



STEVE:  Right.  There's no - who was that oosa woosa goosa thing?



LEO:  No, no, none of that stuff.



STEVE:  The floppy ears.



LEO:  No Jar Jar, no.



STEVE:  There's no Jar Jar Binks.  There's no Ewoks.



LEO:  I haven't seen the last episode yet, but I don't even think there's any light sabers.  Maybe there is towards the end, but...



STEVE:  Oh, but Leo, that fight scene in the second-to-last episode?



LEO:  Oh, so good.



STEVE:  Oh, my god.



LEO:  So good, yeah.  And so it's - a number of people have told me this.  It's a good Star Wars show if you don't really want a Star Wars movie.  But there is that undercurrent which you mention of we're seeing the rise of the Empire.



STEVE:  Yes.



LEO:  So if you're a Star Wars fan you'll like it.  And there's a lot of, you know, there's TIE fighters.  There's Star Wars technology in it.  But it's a good story.



STEVE:  And as Lorrie said, it has a lot of pew pew pew, pew pew pew pew, pew.



LEO:  It does.  And the storm troopers are still lousy shots.  But okay.  You know, oh, yeah, I thought it was really, really, really good, yeah.  Very, very good.



STEVE:  Anyway, so we talked about "The Expanse."  We loved "The Expanse."  And again, if you hate Star Wars, I would say, I think you're right, Leo, forgive it for being set in that universe.



LEO:  Yeah.  Because it's barely Star Wars, yes.



STEVE:  And the other thing I thought was really interesting is the rebels are shown to be - they're just as ruthless as the Empire in their own way.  I mean, it's not, you know, sugarcoated and lollipops.



LEO:  It's more of a gritty kind of real...



STEVE:  Yeah.



LEO:  Somebody says it's like a spy story.  It is.  It's a little more like it would be - if you took away the Star Wars trappings, it'd be the same story, same show.



STEVE:  Oh, and boy, there are some people you love to hate.



LEO:  Oh, yeah.



STEVE:  I mean, they're just beautiful characters.



LEO:  Oh, there's some bad people in there.



STEVE:  Yeah.  Yeah.  So by all means, watch the last one.  It has a great ending.  You won't be disappointed.



LEO:  Yeah, yeah.  No, I've been saving it.



STEVE:  And I just can't wait for the next season.  It's like, oh, come on, let's keep going.  Yeah.



Okay.  So I chose the news of Sony's lawsuit against a well-known and well-respected DNS provider because a very dangerous legal precedent threatens to take hold on the Internet.  Here's how the defendants, Quad9, summarize the situation and its danger.  They wrote:  "Sony Music Entertainment Germany is litigating against Quad9, requiring us to block access to a website that links to a site containing files that Sony asserts are violating their copyright.  We maintain that Sony's request essentially amounts to content censorship and risks cracking the foundations of a free and open Internet, in Europe and potentially worldwide.  Censorship, in turn, can lead to undue restrictions on freedom of speech."



I'll just briefly say that for those of our listeners who were not here, Leo, for Episode 12 of this podcast, this is not the first time Sony has earned some negative coverage and been named in this podcast's title.



LEO:  I'm starting to hate them, to be honest.



STEVE:  Episode 12, dated November 3rd, 2005 ran all of 24 minutes.  Its title was "Sony's 'Rootkit Technology' DRM (Copy Protection Gone Bad)."



LEO:  Twenty-four minutes, really?



STEVE:  You said, hey, Steve, why don't you come, you know, maybe just like half an hour, and tell us what's been going on every week.



LEO:  Not much to say, but we do it in half an hour, yeah.



STEVE:  Since those early days, Sony has had their troubles.  And they do tend to be litigious when they can find someone, anyone, to sue.  They're happy to throw their weight around without much provocation.  So now to this issue today.



We've talked about Quad9 in the past.  They're a bunch of good people.  Quad9 is a free global public recursive DNS resolver.  Meaning that anyone in the world can ask them to look up the IP address associated with a domain, and they will do that by asking whatever other DNS resolvers may be needed to come up with a final answer.  Thus recursive.  But Quad9 is also explicitly a filtering DNS resolver.  In fact, that's its purpose.  It aims to protect its users by not returning the IP addresses of sites known to be malicious.  Quad9 is operated by the Quad9 Foundation, a Swiss public benefit, nonprofit foundation headquartered in Zurich, Switzerland, whose sole purpose is improving the privacy and cybersecurity of Internet users.  Quad9 has 200 points of presence globally spread around 90 countries from which they provide these services to individuals and organizations at no charge.



Here's the language from the court which supports the position that Sony's attorneys have described.  This is taken directly from the official court order.  They said:  "The defendant is ordered to refrain from selling on the territory of the Federal Republic of Germany the music album 'Evanescence - The Bitter Truth.'"



LEO:  Fine.  Fine.



STEVE:  Exactly.  Okay.  Don't want it.



LEO:  Don't want it.



STEVE:  Happy not to sell that, "with the sound recordings contained thereon, to be made publicly available, by the defendant providing its users with a DNS resolver service, which the domain 'canna.to' and/or the subdomain 'uu.canna.to' it translates into numeric IP addresses, so that it is possible for the users of the defendant, with the help of these numerical IP addresses, to reach the Internet service under the domain 'canna.to' and/or the subdomain 'uu.canna.to' and/or the further domains and to call up their links to unlawful storage of the album, as happened by the defendant offering its users the DNS resolver service Quad9 at the IP address 9.9.9.9, with the help of which the users with the links" - and then the court order has four links.  Two of them are uu.canna.to links, you know, which some specification in the URL, and canna.sx and canna-power.to.



They said:  "Numeric IP addresses were transmitted which enabled them to access the hyperlinks provided at the aforementioned addresses to the storage locations of" - and then there's two links at shareplace.org and some hex stuff.  So that's the actual pirate site, shareplace.org, that is offering up this material.  And they finish:  "And call up the illegally stored copies of the aforementioned album."  So in other words, there's no misunderstanding here with the court about the essentially passive role that DNS and a DNS service provides.  Another piece of the official proceedings was also interesting.  They described the efforts that Sony first went through in doing what we would all agree was the right thing for them to do.  Here's what the court document describes.



They said:  "Music content is listed and categorized under the domain www.canna.to.  The domain of the website is 'CannaPower.'  In a letter dated March 23rd, 2021" - okay, so note that this has been going on for two years - "the plaintiff," meaning Sony, "drew the defendant's," meaning Quad9, "attention to the infringement and also pointed out the URL.  The defendant was requested to put an end to the infringement.  The defendant [Quad9] was warned after it failed to remedy the situation.  The plaintiff [Sony] had made every" - here it is.  "The plaintiff had made every conceivable effort to remove the infringing offer with the involvement of primary liable parties.  The CannaPower website has no imprint.  Entries on the domain owner were also not available."  Oh.



"Requests for deletion to the hosting provider went unanswered."  Oh, no.  Nobody answered the phone.  "Their two IP addresses were named.  The company Infium, UAB with an administrative and technical contact in the Ukraine was identified as the responsible organization.  The company was allegedly based in Vilnius, Lithuania.  There, however, a delivery by courier could not take place due to the lack of a traceable signature.  In Ukraine, delivery was not possible because the address was located in a high-security area to which it was not possible to accept deliveries without express consent; this consent had been refused."



Okay.  So in other words, the actual copyright infringers are out of reach of Sony's wrath.  From what the court wrote, it doesn't really sound as though Sony tried that very hard to go after the primary sources. The court wrote that Sony had made "every conceivable effort to remove the infringing offer with the involvement of primarily liable parties."  But CannaPower's hosting provider, what?  Didn't answer the phone?  So that was it.



LEO:  No one's there.  We can't serve them.



STEVE:  So Sony, unwilling to be stymied and denied, decided to attack someone who was within their reach.  The court wrote:  "The plaintiff is of the opinion" - that is, Sony is of the opinion" - that the defendant is liable as a tortfeasor."  I had to look that one up.  The short version of the definition of tortfeasor is:  "A tortfeasor is a person or entity who is found to be responsible under civil law for an injury caused to another person or entity."



In other words, Sony is claiming that because Quad9 provides some of the Internet glue mechanics that are required for users to reach the CannaPower website given its domain name - and note, only Quad9 users, not all the rest of us - they, Quad9, are thereby responsible under civil law for the injury caused to Sony.  Wow.  The court wrote:  "According to recent case law, it" - meaning Quad9 - "is also liable as the perpetrator of a copyright infringement."



Okay, now, unfortunately, we would not be talking about this insanity if saner heads had prevailed, and today's podcast would then have a different title.  The final decision of the Leipzig District Court was to rule in Sony's favor, ordering Quad9 to block all access to CannaPower domains globally.  Now, as we all know, Quad9's action of complying with this court order will have zero effect on anyone who does not have 9.9.9.9 configured as their DNS resolver.  All of the rest of us - of course it won't have any effect on those people anyway listening to this podcast because we're not going to be pirating music from CannaPower.  All of the rest of us can access those CannaPower URLs without any trouble.



So this action by Sony only makes any sense if this is just the first such legal action which Sony intends to use to set a precedent, a legal precedent for other similar actions against other DNS providers.  And that's another thing that doesn't really make any sense because the world is also full of DNS providers, many of whom do not care and couldn't care less about making Sony happy.  And they're not going to take this action.



What would have made much more sense from a technology standpoint would have been for Sony to contact the responsible and definitely reachable top-level domain provider for the .TO TLD. ".TO" is the Internet country code top-level domain for the Kingdom of Tonga and is administered by the Tonga Network Information Center.  And they do pick up the phone.  By changing the DNS name server entries for those CannaPower domains, DNS would have disappeared globally once downstream caches expired.  But that's not what Sony chose to do.



Five days after this judgment, which came down on March 1st, presumably having recovered from the decision, Quad9 posted their response.  They said:  "Quad9 has been part of a potentially precedent-setting legal case involving Sony Music.  On March 1st, Leipzig Regional Court ruled in favor of Sony Music.  This ruling means that Quad9 has no choice but to block the domains in question at a global scale as directed by the court.  That said, Quad9 is far from ready to give up the fight in terms of protecting users' access to information.



"Quad9 is shocked that the court ruled in favor of Sony Music, but they are not disheartened and will continue fighting for the freedom of access to information by citizens around the globe.  Quad9 feels they were chosen as a target because, as a nonprofit player with a limited budget and small market share, Sony potentially did not expect Quad9 to have the means of fighting back.  This would be an easier means of establishing legal precedent to potentially control domains served by all DNS recursive resolvers.



"Although Quad9 is complying with the ruling in the interim, there are several points that they feel should be brought to the attention of citizens around the globe who value privacy and freedom of access to information on the Internet, as the potential implications of the ruling could reach a global scale.  German court decisions are normally limited to Germany, which is why Quad9 has implemented geoIP on its infrastructure in Germany to restrict the domain names in question from being resolved for users querying from Germany.  However, there are loopholes, such as VPNs, beyond Quad9's control.



"The court deemed this not to be sufficient.  The court's decision ignored the VPN concept and implies that Quad9 must block these domains regardless of how users reach them or from what nation those intentionally disguised queries originate.  Quad9 believes this is an exceptionally dangerous precedent that could lead to future global-reaching commercialization and political censorship if DNS blocking is applied globally without geographic limitations to certain jurisdictions.



"The court did not apply the rules of the German Telemedia Act," which sounds a little bit like our Section 230.  They said:  "Consequently, Quad9 does not enjoy the associated limitations from liability.  Quad9 also believes that it should benefit from these exemptions from liability, particularly since the European Lawmakers have noted in the recently adopted Digital Services Act that providers of services establishing and facilitating the underlying logical architecture and proper functioning of the Internet, including technical auxiliary functions, can also benefit from the exemptions from liability, and explicitly mentioned providers of DNS Services."  In other words, it sounds like they have a strong case here on appeal.



"The court established that Quad9 accepted the wrongdoer liability.  Quad9 feels that this application of the wrongdoer liability is absurdly extreme given the circumstances."  And that must make an exception to the German Telemedia Act.  "To put this into perspective, applying wrongdoer liability in this setting is akin to charging a pen manufacturer with fraud because a stranger forged documents while using the manufacturer's writing utensil."



LEO:  Yeah.  I agree with that.



STEVE:  Anyway, they go on.



LEO:  It's very annoying.



STEVE:  I mean, it is really, really wrong.  And I wanted to share one additional posting that Quad9 published two days later since it contained some important points and principles about responsibility with DNS.  They said:  "The Sony Music Entertainment Germany vs. Quad9 Foundation case has brought to light a concerning issue regarding the implementation of blocking measures by DNS recursive resolvers.  In addition to concerns around sovereignty and judicial overreach, it is essential to recognize that DNS recursive resolvers are not the appropriate place to implement this type of blocking.



"DNS recursive resolvers play a crucial role in the functioning of the Internet by translating domain names into IP addresses.  However, they should not act as gatekeepers for content, which can be subjective and varies from jurisdiction to jurisdiction.  Users of Quad9 opt-in to our service and want the cyber protection that we enable for them.  Blocking a domain for distributing malware is motivated by the desire to protect users from malicious or harmful content, such as viruses, phishing scams, or other threats.  These types of protective DNS services, blocking a domain from malware, is a form of user opt-in filtering.  The intention is to protect users from harm and does not necessarily involve restricting access to specific types of content.



"Blocking a website based on its content is censorship since it involves restricting access to specific information based on someone else's standards or values, rather than allowing individuals to choose what content they wish to access.  Most people would like to avoid ransomware on their computers.  Still, they might want to browse a website with a specific set of content or an ideological perspective with which we disagree.  Blocking a website based on some or all of its content is often motivated by a desire to restrict access to specific types of content.



"Recognizing that DNS recursive resolvers are not the appropriate place to implement content-blocking measures is critical.  Arbitrary blocking of content-related domains places an undue burden on DNS recursive resolvers to police online content.  DNS recursive resolver operators don't have the legal expertise or access to additional information that might be required to decide what content is legal or illegal."



And finally:  "In this ruling, DNS recursive resolvers end up with more culpability and liability for content than social media networks.  Concerns around copyright infringement and online piracy are valid, and the courts should address them in appropriate venues.  Finding solutions that do not compromise Internet users' security and respect other nations' sovereignty is crucial."  And in a one-sentence transparency report that they published, they said:  "Quad9's domain blocking policy blocks malicious domains associated with phishing, malware, stalker ware, and command-and-control botnets.  Our threat intelligence data comes from trusted cybersecurity partners, not arbitrary corporations or governments.  Our policy has been to block malicious domains and not moderate content disputes."



So I am certainly glad that they're going to appeal the decision, and I hope that this ruling will have caught the attention of many other larger entities who share a vested interest in not allowing this single ruling to establish what is clearly the wrong precedent.  You know, it's so clear that pursuing legal remedies against the actual publisher of the infringing material, which is what we've always done before, right, I mean, that's what's happened before, that's the right thing to do.



The law should not be about the easy target.  It should be about the infringing target.  The fact that it may be difficult to hold the infringer to account should not mean that arbitrary and completely unrelated components of Internet infrastructure should become an alternative target.  That's just nuts.  Although, you know, as I'm saying that, Leo, it occurs to me that we have seen a situation where the copyright holders are going after the downloaders of the content; right?  I mean...



LEO:  Well, that's sensible; right?  Those are the ones who are actually infringing.



STEVE:  Exactly.  They're obtaining the content.  At each end.  From the person who is serving it and then the person who's consuming it.



LEO:  But, yeah, it's hard for them to do that.  They tried a bunch of John Doe because it's hard to get the identity; right?  They have the IP address.  That's all they have.  And they've tried a bunch of John Doe lawsuits.  Remember that didn't go very well for the music industry.  It turned out to be a black mark for them.  Then they went after the ISPs, they still do, with that three strikes thing.  It's a long, ugly history.  And as far as I can tell, it's accomplished zero.  And that's really the main thing to say is it doesn't stop piracy.



STEVE:  It doesn't work.



LEO:  It just makes it bad for the rest of us.  Yeah.  Oh, well.



STEVE:  Anyway, they are asking for help and support.  I'm hoping that, you know, the EFF and other organizations that have some deep pockets would be able to help these guys.



LEO:  And I'll promise to Sony I'm never going to download or even listen to an Evanescence album at all, ever.  Congratulations.  And if you're Evanescence, you might want to look for a new label because they're doing this on your behalf.  This is what happened is the artists told the record companies, knock it off.  You're punishing our audience.



STEVE:  Our reputation.



LEO:  And hurting our reputation, exactly.  Good stuff.  Thank you, Steve.  This was a good show.  Lot of good information.  You could easily have made that Elliptic Curve story the story of the week, but you had two good ones.  Two, count 'em, two.  And that's why everybody waits with bated breath for Tuesday so they can listen to Security Now!.



Copyright (c) 2023 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#915

DATE:		March 21, 2023

TITLE:		Flying Trojan Horses

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-915.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week, our time-limited quest to answer today's burning questions causes us to wonder, how worried should Android smartphone users be about Google's revelation of serious flaws in Samsung's baseband chips?  What great idea should the NPM maintainers steal?  What is it that nation-states increasingly want to have both ways?  What crazy but perhaps inevitable change is Google telegraphing that it might push on the entire world?  Was it possible to cheat at Chess.com, and what did Checkpoint Research discover?  What's the most welcome news of the week for the United States infrastructure?  And if Trojan Horses could fly, how many propellers would they need?  The answers to those puzzles and riddles coming up next on Security Now!.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here with lots of topics on the agenda.  We're going to kick it off with four extremely serious zero-day flaws in many Android devices.  Find out if yours is at risk.  Then we'll talk about TikTok, the move to ban it, and some real scary stats and information about the DJI drones from China.  All of this coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 915, recorded Tuesday, March 21st, 2023:  Flying Trojan Horses.



It's time for Security Now!, the show where we get together and talk with this guy right here, Steve Gibson, all about, well, everything on his mind including security.  Hi, Steve.



STEVE GIBSON:  Yo, Leo.  



LEO:  Welcome.



STEVE:  Great to be with you again for the Flying Trojan Horses episode.



LEO:  We've been working with Midjourney, Stable Diffusion, and others to generate some Flying Trojan Horses.  Having some difficulty, oddly enough.  But we'll get an image for you.



STEVE:  Yeah.  You got something that's pretty good.  But I think we've got a lot of - I know we have a lot of fun things to talk about this week on our time-limited quest to answer today's burning questions.  One of them caused us to wonder, how worried should Android smartphone users be about Google's recent revelation of four serious flaws in Samsung's baseband chips?  Also, what great idea should the NPM maintainers steal?  What is it that nation-states increasingly want to have both ways?  What crazy but perhaps inevitable change is Google telegraphing that it might be getting ready to push on the entire world?  Was it possible to cheat at Chess.com, and what did Checkpoint researchers discover?  What's the most welcome news of the week for the United States infrastructure?  And if Trojan horses could fly, how many propellers would they need?  The answers to those puzzles and riddles...



LEO:  Really.  Really.



STEVE:  ...coming up next on Security Now!.  



LEO:  You're going to answer that; are you?



STEVE:  That's right.



LEO:  Really.  All right.  Now to the Picture of the Week.



STEVE:  So, okay.  You know those - what this reminds me of is the old-school railway sidings where it ends like, you know, it's just a stub.  And you've got this industrial strength-looking barrier at the end, typically - remember like it had two big pistons that were spring-loaded so that if any cars rolled, tried to roll off the end of the tracks...



LEO:  Right, keep them from going off the end, yeah.



STEVE:  Exactly.  It kind of gave it a nice gentle...



LEO:  That's what this is, yeah.



STEVE:  Yes.



LEO:  Keep you from going off the end.



STEVE:  Except for people; right.  So we have a sidewalk which stops.  And as I guess some municipal code required, in order to make it very clear, maybe it's to keep skateboarders from hitting the bush that is after the sidewalk, there's a sign across it that says "Sidewalk Ends."  You know, very much like that other one, we had the gate that said, you know, "Sidewalk Closed."



LEO:  Yeah.



STEVE:  There, there was actually a sidewalk beyond the gate, but it was...



LEO:  But it was closed, yeah.



STEVE:  ...closed for some reason.



LEO:  Here there's no sidewalk.



STEVE:  So of course there was a well-trodden path around that closure.



LEO:  And look.  There is a trodden path to the left.



STEVE:  Yes.  And that's actually the point.  So this sidewalk ends into what looks like - just like a big bush.  And one wonders if they didn't feel like tackling the bush, or that the bush came after the end of the sidewalk or before.  We really don't know the sequence of events here.  But the need for a sidewalk does not end, even though the sidewalk ends.



LEO:  That's exactly right.  Thank you for observing that.  You don't just stop and go back.



STEVE:  Exactly.  What are you going to do?  Like, oh, Martha, we have to turn around now.  So, no.  Instead, clearly, many people have said, oh, look, there's a path to the left of the end of the sidewalk.  We'll just continue along that.  And sure enough.



LEO:  I bet there wasn't for a while, and then they made one.



STEVE:  Oh, and, boy, this is - now I don't think we're ever going to get a sidewalk.



LEO:  No.



STEVE:  Because the problem has been solved.



LEO:  This is a pet peeve of mine.  All over Petaluma, because of poor planning and malfeasance in the city council and so forth, there are streets with no sidewalks.  I could walk to work easily, except I take my life in my hands because there's no sidewalk for half the route, and you're walking in the street.  And it just irks me.  It's really - fortunately they're starting to put a sidewalk in out here.  Eventually we're going to have a sidewalk.  I don't know if it'll go all the way to my house.  But I would like to walk to work.  It's only a couple of miles.  But I don't dare; you know?  Put in sidewalks.  Let people walk.  Anyway, on we go.



STEVE:  Yes.  So one of the more worrisome revelations of the past week came to light last Thursday when Google's Project Zero's standard non-disclosure deadline expired, 90 days after they had informed their Android hardware and software partner, Samsung, of the 18 separate vulnerabilities they had discovered lurking inside Samsung's widely used  even by Google's Pixel phones  Exynos modems.



And here's the big news:  Four of those vulnerabilities are as bad as any can get for an always-connected smartphone.  And in fact those four vulnerabilities are so bad that Google has decided to make a rare exception to their standard disclosure policy, you know, which is like fix it by 90 days or else.  Well, not "else" in this case.  For the sake of the world they are continuing to hold back details because it is that bad.



LEO:  Now, is this different than the SS7 baseband exploit?  Is this a new - this is a new one.



STEVE:  Oh, yeah, yeah, yeah.  This is not an exploit against the SS7 protocol.  These are, as they call them, Internet-to-baseband remote code execution vulnerabilities which require...



LEO:  Zero click.



STEVE:  ...no click.  Nothing.



LEO:  I just need to know your phone number.  That's it.



STEVE:  That's exactly right.



LEO:  Horrible.  Horrible.



STEVE:  So, okay.  So they said:  "In late 2022 and early 2023, Project Zero reported 18 zero-day vulnerabilities in Exynos Modems produced by Samsung Semiconductor.  The four most severe of these 18 vulnerabilities allow for Internet-to-baseband remote code execution.  Tests conducted by Project Zero confirm that those four vulnerabilities allow an attacker to remotely compromise a phone at the baseband level" - that is, you know, underneath the operating system, down at the chip, at the cellular modem chip level - "with no user interaction, and require only that the attacker know the victim's phone number.



"With limited additional research and development, we believe," said Google, "that skilled attackers would be able to quickly create an operational exploit to compromise affected devices silently and remotely."  In other words, you now know what all of the state-level actors and, for example, what the NSO Group in Israel are busy doing is, like, what what what?  Like, you know, come on.  Go figure it out.



So they said:  "The 14 other related vulnerabilities were not as severe, as they require either a malicious mobile network operator" - in other words, it's got to be carried by the cellular protocol - "or an attacker with local access to the device."  Okay.  As for where these chips are in use, and thus what devices would be vulnerable attack targets, first of all, none of our listeners, unless you're the King of Siam or something, probably need to worry; right?  This would only be a  targeted attack.  Script kiddies are unlikely to ever get access to this.  But, you know, if you might be a target, then pay attention to these model numbers.



So Google said:  "Samsung Semiconductor's advisories provide the list of Exynos chipsets that are affected by these vulnerabilities.  Based on information from public websites that map chipsets to devices, affected products likely include:  mobile devices from Samsung, including those in the S22, M33, M13, M12, A71, A53, A33, A21s, A13, A12 and A04 series; mobile devices from Vivo, including those in the S16, S15, S6, X70, X60 and X30 series; the Pixel 6 and Pixel 7 series of devices from Google; and any devices that use the Exynos Auto" - as in automobile - "T5123 chipset."



Now, Leo, I'm not hip enough to, like, these model numbers.  Are these current smartphones, these series?  Do you know, like, how recent these things are?  Not clear to me.  They said:  "We expect that patch..."



LEO:  Hold on.  I was over here.  I was painting the ceiling.  Wait a minute.  Hold on, I'm back.  Yes, I think it is.  The S23 is the current Samsung phone, and I believe that has that Exynos chip in it.



STEVE:  Okay.



LEO:  And, you know, Google in its Pixel phones, this is the Pixel 7.



STEVE:  Okay.  So 6 and 7, they both do.



LEO:  Yeah.



STEVE:  Google knows that.



LEO:  They have Samsung chips.



STEVE:  Okay.



LEO:  They never said Exynos.  In fact, they didn't want to say Samsung.  But then we found out, so now we know, yeah.



STEVE:  Okay.  Okay.  Sorry to make you run.



LEO:  A lot of modern smartphones have Qualcomm.



STEVE:  Okay, right.



LEO:  But these don't.  And the Samsungs don't.



STEVE:  Of course Samsung's not going to use Qualcomm.



LEO:  Yeah, no, they're going to use Exynos, yeah.



STEVE:  Right.  So they said, Google said:  "We expect that patch timelines will vary per manufacturer."  They said:  "For example, affected Pixel devices received fixes for all four of the severe Internet-to-baseband remote code execution vulnerabilities in the March 2023 security update."



LEO:  Oh, yeah, which came out about a minute ago.  Okay, fine.  Okay, thank you.



STEVE:  Yeah.  And but this also says, you know, one of the things that we've said is, if you care about your Android smartphone security, then you really want to be with someone who is going to be patching responsibly.  And that's going to be Google and Samsung.



LEO:  Yup, there it is, the March update.  This is it.



STEVE:  Yup, yup.  They said:  "In the meantime, users with affected devices can protect themselves from the baseband remote code execution vulnerabilities mentioned in this post by turning off WiFi calling and Voice-over-LTE data in their device settings."  So there is a workaround, again.  So if you only leave old-school cellular connectivity on, then you're safe.  It is the Internet and the data connectivity that is where the vulnerability comes from.  And they said:  "As always, we encourage end users to update their devices as soon as possible to ensure that they are running the latest builds that fix both disclosed and undisclosed security vulnerabilities."



LEO:  See, I get it now because I'm looking at the devices, and they're all in the last couple of years.



STEVE:  Yeah.



LEO:  And that's because VoLTE and WiFi calling is in the last couple of years.  So it's clearly in that part of it because, for instance, the S23, the newest Samsung, is not on the list.  Which means they figured it out and fixed it.  But all of the stuff from the last couple of years, including the Pixel 6 and 7 are.



STEVE:  Yeah, or maybe the 23 is not because they're using, you know, certainly Samsung didn't know of the problem and fix it in the 23.



LEO:  Right, they must be using a different chipset.



STEVE:  Yes.  Or they just reengineered it, and the reengineering didn't have the problem.



LEO:  Accidentally fixed it, yeah.  They fixed it by accident, yeah.



STEVE:  Exactly, yeah.  So, okay.  So here's how Google has positioned their unusual decision not to fully disclose after 90 days, which is a violation of, you know, it's been a hard-and-fast rule for them.  So they said:  "Under our standard disclosure policy, Project Zero discloses security vulnerabilities to the public a set time after reporting them to a software or hardware vendor.  In some rare cases where we have assessed attackers would benefit significantly more than defenders if a vulnerability was disclosed, we have made an exception to our policy and delayed disclosure of that vulnerability.



"Due to a very rare combination of level of access these vulnerabilities provide and the speed with which we believe a reliable operational exploit could be crafted, we've decided to make a policy exception to delay disclosure for the four vulnerabilities that allow for Internet-to-baseband remote code execution.  We'll continue our history of transparency by publicly sharing disclosure policy exceptions and will add these issues to that list once they are all disclosed."  In other words, they're saying we're not going to tell you what's wrong, but we're at least going to tell you that we're not going to tell you.  So, you know, you know that there's something wrong even if you don't yet know what.



So they said:  "Of the remaining 14 vulnerabilities, we're disclosing four vulnerabilities that have exceeded Project Zero's standard 90-day deadline today."  They said:  "These issues have been publicly disclosed in our issue tracker, as they do not meet the high standard to be withheld from disclosure."  As the four bad ones do.  They said:  "The remaining 10 vulnerabilities of those 14 in this set have not yet hit their 90-day deadline" - remember there were some that were disclosed only in 2023, so earlier this year - "but will be publicly disclosed at that point if they remain unfixed."



So the concern here is that this is, these four, this is a big juicy set of very, I mean, like, infinitely, essentially, powerful exploits which every very powerful and really bad actor in the world knows now not only exists, but also roughly where it exists.  And as always, we know that there's a huge difference between a patch being available, and that patch being applied everywhere it's needed.  Google's Pixel devices, as we noted, were early recipients of those patches; and, presumably, Samsung's devices will be, too.  But what about those Vivo phones, and the autos, automobiles that incorporate those chips?



LEO:  Those will never be updated, of course.



STEVE:  Exactly.  So, you know, as I said, random people, like most of us, almost certainly have little to fear since script kiddies are never going to get their hands on these.  But this is the sort of vulnerability which is exactly what the likes of Israel's NSO Group is looking to add to their Pegasus smartphone spyware, as are other less public state-level actors.  So I would imagine that, without exception, the victims of the exploitation of these vulnerabilities would only be those who are highly targeted and valuable.  And of course it will turn their phone into listening devices; right?  I mean, they'll suck out their messaging history and what they're doing and tracking, and also probably turn the microphone on in order to eavesdrop in real-time.



So anyway, it's no comfort if you might be such a target; but hopefully you've got, you know, you're not using some random also-ran Android device that might have this chip.  You've got a Pixel or a Samsung, and it'll get fixed quickly.  And as Google noted, there is a way that you could, until your device is patched, you could make yourself secure if you were someone who should be concerned about that.  So of course always good to keep our devices patched.  But I doubt that most of us have anything to worry about.  And I don't since I am an iOS device user.



So, okay.  I saw an interesting idea for wrapping the potentially hazardous NPM command within a protective shell.  NPM of course stands for the Node Package Manager, you know, "node" as in Node.js (JavaScript).  It's the command-line interface to the most popular JavaScript code repository.  The idea for this protective wrapper comes from a company named "Socket," who says of themselves, they said:  "Secure your supply chain.  Ship with confidence.  Socket fights vulnerabilities and provides visibility, defense-in-depth, and proactive supply chain protection for JavaScript and Python dependencies."



Now, they call their latest innovation "safe npm."  And I'm going to share a bit of their sales pitch, not because I necessarily think that our listeners should go get it, but because it nicely describes the open-source package distribution risks that we've been covering now for quite a while.  So Socket explains.  They said:  "Socket is proud to introduce an exciting new tool, 'safe npm,' that protects developers whenever they use npm install," which is the command that you issue at the command prompt to install some new package into your system.  They said:  "Socket's 'safe npm' CLI (Command Line Interface) tool transparently wraps the npm command and protects developers from malware, typosquats, install scripts, protestware, telemetry, and more, 11 issues in all.



"Today when you run npm install it's difficult to know which transitive packages will get installed, whether those packages will execute install scripts, or if those packages have been compromised by malware.  The average npm package has 79 transitive dependencies.  That means installing a single package with npm install will, on average, install 80 total packages."  They say:  "It's hard, if not impossible, for a developer to audit, let alone even understand, the full list of packages that will be installed.  Most of us just cross our fingers and hope for the best.



"Worryingly, any of these 80 packages can declare an install script, third-party shell code, that npm will automatically execute during installation.  While there are legitimate use cases for install scripts, it's also a favorite feature of malware authors; 94% of malicious packages used at least one install script.  Developers also face the ever-present risk of typosquatting attacks, where an attacker publishes a package with a name similar to a more popular package.  It's way too easy for a busy developer to make a typo when running npm install and install the wrong package.  Sometimes, however, typos can have disastrous consequences, such as in the case of running npm install webb3 [with two b's] instead of npm install web3."



Anyway, then they show an example of something quite malicious hiding inside that "double b" version of the webb3 package, just, you know, a typo, webb3.  Somebody stuck it there hoping that somebody would type it by mistake.  And people did occasionally.  So they show that.  Then they go on to explain.  They said:  "This type of malware is all too common.  Socket has helped to remove over 200 packages for security reasons - malware, ransomware, spam, et cetera - in the past 30 days alone."  They said:  "To help you get a sense of the scale of the problem, we freely share samples of recently removed npm packages with the public, for non-commercial research purposes.



"In conversations with developers, we kept hearing the same request.  Developers want a way to securely and confidently run npm install without the fear of malware or rogue scripts infecting their systems.  Our most popular product, Socket for GitHub, already proactively scans GitHub pull requests for software supply chain risks including typosquats, install scripts, and more than 70 customizable issues.  But until today, we have not had a good way to protect the developer's local machine from bad packages.  That's why we're super excited to share this initial release of 'safe npm' with you today.  Socket is proud to introduce a new feature, 'safe npm,'" blah blah blah.  And basically they then repeat what they said.



"When a developer attempts to install a malicious or risky package, Socket pauses the installation and informs the developer about the risk, if any are detected.  The developer is given the option to stop the installation and protect their machine before the package is executed or even written to disk.  Alternatively, the developer is also free to proceed and accept the risks."



Okay.  Now, the reason I'm bringing all this up and wanted to cover, first of all, every single one of those problems with npm we've talked about on the podcast.  You know, scripts being run and so forth.  So I'm bringing this up, not to take anything away from these guys, but they want $10 per month per user for this, which, you know, if you were only occasionally doing this, seems excessive to me.  But more than that, all of this sounds like something that the maintainers of NPM, that package itself and all other similar package managers, ought to have already built into their basic command-line offerings.  I have no way to directly influence that happening.  But it may be that Bitwarden now supports the superior Argon2 PBKDF thanks to our talking about it here, followed by some of our listeners suggesting it and implementing and pushing it across the finish line.



So if any of our listeners are able to plant a bug in the ears of the guys who are responsible for evolving npm and the other major package managers, I think it's clear that it's well past time for the industry's various package managers to get proactive about protecting their users from all of this nonsense that's going on in the major repositories that they are, after all, pulling their packages from.  There's no sign of this abuse calming down.  It's not like it's a passing fad.  All of the indications continue to be that it's still ramping up.



Since it's the package manager that goes out and retrieves the package on behalf of its user and then follows all of the dependency linkages and makes sure that everything else that's necessary is there, that function, that exact function needs to evolve beyond being what it originally was a few years ago before this all began to happen with the repositories, you know, a simple, no responsibility being taken, trivial command-line retrieval and installation tool.  It needs to shoulder, those package managers now need to shoulder a lot more responsibility.



So anyway, when I saw this announcement from Socket I thought, you know, nice that these guys are doing that.  You can get that right now from them.  And depending upon your level of risk tolerance, you may choose to do that.  But what we really need is for this to be made universal, and the package managers need to step up and start taking responsibility.



Okay.  So things are getting interesting as an increasing number of governments are looking at their newly strengthened privacy laws and realizing that the behavior of the big tech giants is in contravention of those statutes.  We've already been covering some of these events as they've been happening, but here are a few interesting pieces that we haven't talked about before.



Last year, South Korea's privacy watchdog, known as PIPC  the Personal Information Protection Commission  imposed a pair of stiff fines on Google and Meta for breaking the country's privacy laws by not obtaining - oh, boy - lawful consent from users and tracking their online activity for advertising purposes.  The PIPC imposed a 69 billion won - that's still pretty significant - $52 million fine on Google; and a 31 billion won, which is $23 million, fine on Meta.  And, you know, both of those giants could have trivially paid the fines.  But that would have set a dangerous precedent, and they would also likely have been required to stop doing what they had been fined over, which both companies appear to be certain is required for their businesses to thrive.



So rather than pay up, both Google and Meta have instead elected to countersue the PIPC.  In their recently filed lawsuits, both companies argue that it's the website operators who should be responsible for obtaining individual user consent, not their platforms, which they contend only receive and aggregate this data which is being collected by visitors to the websites.  Okay.  So there's one piece.



Meanwhile, over in the never-dull European Union, nearly three years ago, back in July of 2020, the CJEU - the CJ stands for Court of Justice for the EU - ruled that a transfer of data to U.S. providers violate the rules on international data transfers which are spelled out in the GDPR.  So the CJEU consequently annulled the existing transfer deal "Privacy Shield."  This followed their previous annulment of the "Safe Harbor" agreement back in 2015.  So while all of this sent shock waves through the tech industry, U.S. providers and EU data exporters just largely ignored the case.  Meta's Facebook, like Microsoft, Google and Amazon, has relied on the so-called "Standard Contract Clauses" and "supplementary measures" to continue data transfers and calm its European business partners.



So back in August the consumer protection agency NOYB filed 101 complaints against specific individual websites which were still using Google Analytics and Facebook Tracking tools despite clear court rules making that use unlawful.  I mean, it's unlawful to do that, and everyone's continuing to do it.  So this NOYB consumer protection group said, okay, let's start turning up the heat here.  And now we're talking about this because last Thursday Austria's Data Protection Authority, which is the DSB, has ruled that Facebook's use of its tracking pixel directly violates the GDPR.



A guy named Max Schrems, the chairman of this NOYB.eu, said:  "Facebook has pretended that its commercial customers can continue to use its technology, despite two Court of Justice judgments saying the opposite.  Now, the first regulator told a customer that the use of Facebook tracking technology is illegal."  Oh, and I suppose it's not surprising that also the use of "Login with Facebook" is also illegal since, as we've noted, it's essentially a tracking technology, too.  The use of Google Analytics falls under the same regulation and has already been ruled unlawful.  The concern is that if any of these tools are used, data are inevitably transferred to the U.S., where the EU claims to be worrying that the data is at risk of intelligence surveillance.  They quote, you know, FISA and blanket NSA rights to look at anything that they want to.



So, as I was thinking about this, what strikes me as more than a little ironic is that these governments who don't want their citizens' web pages to contain tracking pixels, or to use U.S.-based services that might send data outside of their Union, nor for their citizens to be using apps with ties to potentially hostile governments, are the same governments who are increasingly up in arms over their inability to intercept their own citizens' end-to-end-encrypted communications, not only when they might deem it necessary through a wiretap-style search warrant, but also in the form of continuous background surveillance monitoring of all visual and written communications for anything that they might deem to be illegal or suspicious.  And of course tracking their locations is part of that deal since it doesn't do any good to know what's going on if you're unable to go grab the perpetrators.



So it's apparently okay for the governments to spy on and track their own citizens, but no one else should be able to.  They're all about the rights of their own citizens, except when it's they who are violating them.  And that's why I named this little piece of news "It's only okay when we do it."  So we've got some more news to talk about; but, Leo, I think we should take our second break.  



LEO:  You bet.  Yeah, Max Schrems actually has the laws named after him because he's such an active advocate of this kind of privacy.  They call it "Schrems."



STEVE:  And, you know, it's going to be up to legislators to figure out, like, what happened.



LEO:  Yeah.  Schrems was chiefly responsible for making sure data for a citizen of a country is stored in that country, not some other third-party country.  I think that's a fairly reasonable ask.



STEVE:  Yup.  And I think that's probably what's going to happen is Meta and Google are not going to pull up stakes.  They're just going to move their stuff over there.



LEO:  Just have a Networks Operation Center over there.  It's no big deal.



STEVE:  Right.  So while we're talking about nervous governments, I'll just note that New Zealand put a ban on the use of TikTok by their lawmakers and other Parliament workers.  And this ban goes into effect actually at the end of next week, as March ends.  And the Scottish government hasn't quite gotten there yet. But officials were "strongly advised" to remove the TikTok app from all their government devices.



Meanwhile, the Australian government has published a lengthy 113-page report it received from academics as part of its own TikTok investigation.  The document describes TikTok's deep ties to the Chinese Communist Party.  And basically it wasn't clear there was any news there, but they just wanted their own in order to support their own plans.  It's viewed as preparation for a government-wide ban that may arrive shortly, that's expected in the next couple weeks.



And of course over here in the states, the FBI and the U.S. Justice Department have launched an official investigation into ByteDance, TikTok's parent, for using the TikTok application to spy on American journalists.  And, you know, this is that old news that some rogue employees were, and it turns out apparently indeed, misusing TikTok to spy on one of Forbes' reporters in an attempt to identify that reporter's sources.  And ByteDance said that they fired the individuals who surveilled the journalists.  So, you know, more of this drum beat.  And Leo, I know that you and your two co-hosts on Sunday talked about TikTok.  I wasn't able to listen to that, but you said you were going to.  Is there anything else that has happened?



LEO:  Well, I mean, yeah.  It's imminent that they're going to ban it, I think.



STEVE:  So you think in this country.



LEO:  In the U.S., yeah.



STEVE:  So not just governments, but everybody.



LEO:  Yeah.  So apparently TikTok says that the Biden administration a couple of weeks ago told them sell it or we're banning you.  Whether they'll be able to sell it is a question because the Chinese government has to approve it, and it seems unlikely that they will.  They've said in the past, back when Trump tried this, we're not going to sell it because there's technologies that we don't want anybody outside China to have, AI technologies.  In which case that's going to put the Biden administration up against the wall.  And I guess they'll have to ban it.  They haven't announced that publicly.  It was TikTok that said so.  The CEO of TikTok is testifying in front of Congress tomorrow or Thursday.



STEVE:  I think it's tomorrow, yeah.



LEO:  And apparently they've paid a bunch of influencers to come into Washington to tell Congress don't ban TikTok.  I mean, I kind of have a sympathy for that.  My son got his career start on TikTok.  It was a huge launching pad for him.



STEVE:  Well, and can you imagine, I mean, this would be unprecedented, where an app that is this popular literally, I mean, it would go dark; right?



LEO:  That's my biggest concern is it's just a bad precedent for the American government to ban an app.  You know, there'll be retaliation.  American apps will be banned, you know, there are already many of them banned in China, but elsewhere perhaps.  And I just think it sets a bad precedent.  I understand the security concerns, and I don't think any government person should have TikTok on their phone.  They probably shouldn't have smartphones at all.



STEVE:  And every time we talk about Russia doing one of these bans, we roll our eyes.  It's like, oh, boy, you know, repressive regime. 



LEO:  Yeah, yeah.  Welcome, yeah.



STEVE:  Well, here we come.



LEO:  So I just don't know, I don't - absent solid proof that China is doing something with TikTok...



STEVE:  Right, right.



LEO:  I understand the reason you might want to be afeared.  And certainly I think it's well within the rights of governments and agencies and the Defense Department and so forth to ban TikTok on those government phones.  That's fine. 



STEVE:  Yeah.



LEO:  But there are so many people, millions of people, of creators all over the United States who make their living through TikTok.  So I have some concerns over that.



STEVE:  Wow, wow.



LEO:  Yeah.  I don't know what the answer is.  Because I understand the security concerns.  I really do.



STEVE:  Yeah.  Okay.  So once upon a time, when I was just a wee lad...



LEO:  Oh, a hundred years ago, yes.



STEVE:  I know.  It's been a while.  I had hair.  You could purchase a certificate that would last longer than an all-day sucker.  Actually, it would last for a full five years.  Those were the days.  In fact, those certificates lasted so long that many companies would completely forget all about them until they were surprised when connections to their web servers suddenly began to fail.



LEO:  Yes.



STEVE:  Then it would be a mad scramble to remember, how do we create a Certificate Signing Request again?  I don't remember.  And, you know, the guy who did that last time, well, he hasn't been with us for a few years.  So we need to refigure out the magic incantations that are required.  So it was often a lot of excitement, about every five years, give or take.



Well, as we have chronicled on this podcast, since the days when I was a wee lad at the beginning of this podcast, over the years certificates have largely done their job, but we've also had a lot of fun here on the podcast examining the myriad ways they have fallen short, through no fault of their own.  One big topic for us was the whole mess of certificate revocation.  That was a lot of fun.  At one point, our longtime listeners will recall, I created and then immediately revoked my own certificate to demonstrate just how totally broken the Chrome browser's certificate revocation system was.  It didn't actually have one.



Chrome happily honored my revoked certificate that other properly functioning browsers knew better, and they blocked it.  This, then, forced Google to manually add an exception for my deliberately revoked certificate to Chrome's short list of known bad certificates, even though Chrome still remained blissfully unaware of all other revoked certificates in the whole world due to the fact that its revocation system, as I said, never actually worked.  After they did that, when I created another revoked certificate to demonstrate that they had special-cased my first certificate by manually adding it to that short list, well, they decided to just ignore me since I was annoying them, and I'd proven my point.



But, almost inevitably, certificate expiration durations have been creeping downward.  They first dropped from their original "set it and forget it" duration of five years, down to three years.  Then they dropped to two years.  And now we're all at just one year plus one month.  And while this is admittedly five times the work as it was when certificates lasted five years, because now they only last about one year, the people responsible for keeping certificates from expiring now tend to always have that in the back of their minds.  I know, for example, that GRC's cert will reach its end of life at the end of July this year.  So that's not far off.  It's not like it's five years from now.  So oh, yeah, you know, will I even still be worrying about this then.



So the story behind how the industry's certificate life was cut in half, from two years to just one year, is relevant because a more extreme version of it might be in our not-too-distant future.  Recall that three years ago, back in 2020, it was Apple who made the unilateral decision to stop supporting any certificate whose data of issuance was more than a year and a month earlier than its date of expiration.  So for whatever reason, 365 plus 33 is 398 days.  And 398 is the maximum distance you can have from "not valid before" to "not valid after" dates, which is what the certificates contain.  So since Apple's decree would cause any and all iOS and macOS devices to reject any then non-compliant websites, the rest of the certificate issuing industry had no choice other than to drop their certificate lifetimes to what Apple was now going to require.



But now there's some scuttlebutt that Google, with their ability to also unilaterally control what most of the web does through the operation of their Chrome browser, that they may be considering doing something similar.  But Google is talking about reducing certificate lifetime to just 90 days.



LEO:  Oh, boy.  That's a problem.



STEVE:  So initially playing nice, Google says that it plans to make a proposal of this to the CA/Browser Forum, right, the CAB, the CA/Browser Forum, which we've spoken of often back in the day when all this was happening more regularly.  This CAB is an informal group of browser vendors and Certificate Authorities who meet regularly to discuss industry-wide initiatives and keeping everything on the same track, what fields certificates should have and so forth.



Okay, now, no one expects administrators of every server on the planet to be manually generating and freshly installing TLS certificates every three months.  So the point, I mean, the explicit point of Google's recently telegraphed move is to move the entire industry to enforced certificate automation.  ACME is the Automated Certificate Management Environment.  As we know, it debuted with the free certificate provider Let's Encrypt.



But I know, for example, that my chosen certificate provider DigiCert now also supports ACME automation.  And there's a nice ACME client for Windows which will be able to automate the process for my non-Unix servers.  So it'll be a matter of maintaining an account, in my case, and a balance with DigiCert, or some means for them to pull money as needed.  Then my various servers will be able to serve their own 90-day certificates and notify me only when there's some problem.



In Google's document proposing this certificate lifetime shortening - and it's a polite proposal, right, because, I mean, the presumption is this is going to happen - Google said the following in support of the move to automate certificate issuance.  They wrote:  "The Automatic Certificate Management Environment (ACME, RFC 8555) seamlessly allows for server authentication certificate request, issuance, installation, and ongoing renewal across many web server implementations with an extensive set of well-documented client options spanning multiple languages and platforms.  Unlike proprietary implementations used to achieve automation goals, ACME is open and benefits from continued innovation and enhancements from a robust set of ecosystem participants.



"Although ACME is not the first method of automating certificate issuance and management [and then they cite] CMP, EST, CMC, and SCEP, which all predated it," they said, "it has quickly become the most widely used.  Today, over 50% of the certificates issued globally for the Web Public Key Infrastructure rely on ACME."  Fifty percent.  "Furthermore," they said, "approximately 95% of the certificates issued by the Web PKI today are issued by a CA owner who has some form of existing ACME implementation available for customers."  In other words, you won't have - no one's going to have to change CAs.  All of the CAs, 95% of them, already support ACME.  So all you have to do is ask for certs that way instead of doing it through the web interface.



They said:  "A recent survey performed by the Chrome Root Program indicated that most of these CA owners report increasing customer demand for ACME services, with not a single respondent expressing decreased demand."  And this means that, before long, ACME support will become a standard feature of any server that needs to support TLS connections, as most do and will.



And in an interesting bit of coming full circle, the reason I tied this back into certificate revocation is that, with Google reducing certificate lifetimes to just 90 days, the fact that their premium flagship web browser does not and never has properly supported certificate revocation becomes much less of an issue since a stolen certificate would, on average, only be useful for half that period of time, about six weeks, before its short life came to an end.



And just to be clear, there is no timetable for any this.  But it does appear to be a thing, and it would likely behoove anyone who is now in the process of setting up any new server environment to plan to implement ACME automation sooner rather than later, maybe from the get-go.



LEO:  Yeah.



STEVE:  I certainly would in my case because the change does make sense, and the writing does appear to be on the wall for this.



LEO:  Yeah, I don't mind if it's automated.  I mean, right now we're going through this hassle.  We've got a three-year paid certificate, but every year we have to update it, and we just went through that with a bunch of servers.  If we can implement ACME everywhere, I mean, I use Let's Encrypt, and that's three months, and it's automatic, and it's fine.  



STEVE:  Yup.



LEO:  So I guess if we can implement ACME everywhere - hmm.  I mean, there are definitely going to be people who know how to get a new certificate with a CSR and all that who don't know how to set up a script like ACME.



STEVE:  Yeah.  I'm sure this is going to cause some pain.  And Google's point is, I mean, they recognize that they're pushing the world.  They're taking the position that, from what we observed on the podcast, security changes move very slowly.  If we can get ACME in place such that certs only have a 90-day life, then we will similarly be able to roll any other changes in certificates, like more use of elliptic curve certs, or we decide we want to change to post-quantum certificates.



LEO:  Yeah, yes.  That'd make it much easier, yeah.



STEVE:  Oh, my god, it'll be automatic, essentially.  So again, you know, we always see that these sorts of changes are difficult to force down everyone's throat.  But at some point it makes sense to do that.



LEO:  Yeah.



STEVE:  Google is going to be the bad guy this time.



LEO:  Yeah.



STEVE:  The bad cop.  Leo?



LEO:  Yes?



STEVE:  You'll be glad to know that chess is safe.



LEO:  I actually know a little bit about this, but I'd like to hear what the story is, yeah.



STEVE:  So in their blog posting titled "Checkmate:  Checkpoint Research exposes security vulnerabilities on Chess.com," they describe how they discovered, reported, and helped fix vulnerabilities in the popular Chess.com platform.  Now, for those who don't know, Chess.com is the world's leading platform for online chess games, with over 100 million members and more than 17 million games played per day.  I thought that ratio was interesting:  100 million members, 17 million games per day.



So it functions as an Internet chess server, a news website, and a social networking platform with a strong focus on community-based forums and blogs which allow players to connect with each other, socialize, share thoughts and experiences, and learn from each other about playing chess.  Chess.com also conducts global championships, which consist of prize money to the tune of a million dollars for the winner and the coveted Chess.com Global Champion title.



So Checkpoint decided to take a close look into the functioning of Chess.com because there had been some allegations of cheating in the past.  What did they find?  They found a number of ways that the communications with the site could be manipulated to cheat.  They discovered that it was possible to win by decreasing the opponent's time and winning the game over time, without the opponent noticing what had happened.  They also discovered that it was possible to extract successful chess moves to solve online puzzle challenges and win puzzle ratings.  To do this, they intercepted the communications between the client side (the player) and the server (the Chess.com website).  What they discovered was that the server was accidentally sending the correct solution to the puzzle.



LEO:  Oh, I could use that.



STEVE:  To the client's side.  And that allowed a cheating client to abuse and cheat on puzzle championships, in which the winner gets prize money, by simply submitting the correct moves that the server was inadvertently providing.  And also it was possible to modify, in that case also, the elapsed time it took to consider the solution.  And finally, they discovered that in communication between two friends on the platform, after approving the friends' requests to connect, an attacker, or somebody taking an attacker role, is able to intercept the request with a proxy tool and succeed in both manipulating game timing, which allows a quick win, and in solving a puzzle, which raises his score and the value on the platform.  So the good news is, today, thanks to Checkpoint's work, the game of chess as played at Chess.com is safer and fairer than ever before.



LEO:  I like the "-er" because in fact most Chess.com cheating and most online cheating in chess has nothing to do with a hack.



STEVE:  Right.



LEO:  It just has to do with me having my Stockfish chess game running at the same time on my phone as I'm on Chess.com and entering the moves in.



STEVE:  Yup.



LEO:  And that, unfortunately, is not a hack.  That's a little harder to fight.  You see, it's fascinating to see how they fight it because...



STEVE:  There is deep learning at Chess.com.



LEO:  Yeah.  Well, so the way you do it is chess computers calculate the current balance of the game in centipawns, a hundredth of a pawn; right?  And among two human players the lead change is much more variable in centipawns, you know, might be 100 or 200 centipawns and then up and down and up and down.



STEVE:  Ah, over time.



LEO:  But when a machine plays, it's a fairly linear gain of centipawns.  You don't have the same variations.  And so it is a little easier to spot a machine player because it doesn't make the same mistakes that humans do.  Humans blunder.  And so a machine never blunders.  And then there's other ways to tell.  They play lines that are kind of more machine-like, although that's getting harder and harder because as they get better and better, they look more and more like humans.  But really they play too well.  That's the easiest way to spot them.



STEVE:  Right.



LEO:  And for instance there's been a big scandal, a super grandmaster named Hans Niemann has been accused of cheating because his rating went up, you know, went up normally, fairly steadily, and suddenly soared in a very unusual way.  And furthermore, his results in games where he's over the board and there are measures taken to prevent cheating, his results are not nearly as good.  



STEVE:  And he was actually playing against Magnus; wasn't he?



LEO:  Magnus lost a game to him, which is very rare.  Magnus is easily the best player in the world.  Lost a game to him and then - you can't accuse somebody of cheating.  That's not allowed.  So he intimated...



STEVE:  And not very sportsmanlike.



LEO:  Yeah.  So he intimated there was something not kosher.  And that got the investigation going.  And then in the second game with him he resigned after one move to kind of further indicate his displeasure.  There's been investigation since.  There is no conclusive evidence that he cheated.



STEVE:  Wow.



LEO:  But there's a lot of circumstantial evidence.



STEVE:  Well, and it's also really interesting that when he's in an environment where he cannot receive any help...



LEO:  Right.



STEVE:  ...he's not playing as well.



LEO:  So the single thing that they did was they delayed the broadcast.  So normally it's streamed live.  And so an accomplice at home could be watching the game live and somehow, we don't know how, transmit the move.



STEVE:  Communicating back to him.



LEO:  But if they delay the broadcast by 15 minutes, suddenly his results aren't as good.  So it's a little suspicious.  It's unknown, really, frankly.



STEVE:  Yeah.



LEO:  And now Magnus has stopped playing in the World Championship.  He's said, "I don't want to play anymore."  So.



STEVE:  Those darn computers.



LEO:  When computers got that good, we thought that's it for chess.  And it's turned out no.  There's been some issues; but people, humans still like playing.  They even like playing against machines, as good as they are.



STEVE:  Wow.



LEO:  Yeah.



STEVE:  Okay.  So in very welcome news, CISA has announced that they have started scanning the Internet-exposed networks of the U.S.'s critical infrastructure for vulnerabilities and warning those who are responsible.  Yay.  As we know, we've been covering other countries' welcome announcements of their intentions and results from doing the same, and in some instances their scans have turned up many important things that did need fixing.  So it's very welcome news that now, in the U.S., CISA has begun doing the same thing here.  CISA's announcement last week is titled "CISA Establishes Ransomware Vulnerability Warning Pilot Program"; and it, too, has already borne fruit.



They said:  "Recognizing the persistent threat posed by ransomware attacks to organizations of all sizes, the Cybersecurity and Infrastructure Security Agency announces today the establishment of the Ransomware Vulnerability Warning Pilot (RVWP) as authorized by the Cyber Incident Reporting for Critical Infrastructure Act" - boy, they love their acronyms - "the CIRCIA of 2022.  Through the RVWP, CISA will determine vulnerabilities commonly associated with known ransomware exploitation and warn critical infrastructure entities with those vulnerabilities, enabling mitigation before a ransomware incident occurs."  Perfect.



They said:  "The RVWP will identify organizations with Internet-accessible vulnerabilities commonly associated with known ransomware actors by using existing services, data sources, technologies, and authorities, including our free Cyber Hygiene Vulnerability Scanning service."  What?  "Organizations interested in enrolling can email vulnerability@cisa.dhs.gov."  Then they finished:  "CISA recently initiated the RVWP by notifying 93 organizations identified as running instances of Microsoft Exchange Service with a vulnerability called 'ProxyNotShell,' which has been widely exploited by ransomware actors.  This initial round of notifications demonstrated the effectiveness of this model in enabling timely risk reduction as we further scale the RVWP to additional vulnerabilities and organizations."



So that's all really good news.  And of course this begins as just, oh, we're just sticking our toe in the water to notify people of ransomware because of course who could object to that?  We can foresee that this is going to become much broader as it proves itself over time.  And I just think this is going to have to become the way things go.  Note that the Cyber Hygiene Vulnerability Scanning service they refer to is not open to the private sector unless the organization qualifies as a critical infrastructure provider.  In an FAQ, CISA answers the question "Who can receive services?" by replying:  "Federal, state, local, tribal, and territorial governments, as well as public and private sector critical infrastructure organizations."



And of course as the guy who created and launched GRC's ShieldsUP! Service 24 years ago, back in October of 1999 - and that was, by the way, 106,477,630 network scans ago - I've seen firsthand how important and effective this sort of proactive scanning can be.  And even more recently, Leo, following our Podcast 389, which was January 30th of 2013, I quickly added that Universal Plug and Play, you know, UPnP scanner to ShieldsUP!.  And since then it has informed 55,301 visitors that they have, for some reason, Universal Plug and Play publicly exposed.



LEO:  Yeah.



STEVE:  So I think it's very clear that this sort of proactive scanning is where we have to go.  It's just going to be so important.  And, you know, it's probably the local governments or the regional and national governments that need to do the scanning because their packets need to be above reproach; right?  You just don't want anybody scanning organizations because they may be looking for vulnerabilities to exploit as opposed to vulnerabilities to notify the responsible parties.  So again, this is a change that has been coming for a while.  And, you know, yay.



LEO:  Yay, yay, yay.



STEVE:  Okay.  We're going to talk about Flying Trojan Horses after you tell our listeners why we're here.



LEO:  ExpressVPN, yeah. 



STEVE:  That's right.



LEO:  And we're still working on our Midjourney props.  We're trying to get some Flying Trojan Horses.  For some reason the AI just refuses to let Trojan Horses fly.  They just - they're ground-bound.  I don't know why that is.



STEVE:  This is sort of reality-check time, but I think everyone's going to find this interesting.  A large and significant group of fully bipartisan, not just token bipartisan, senators have all co-signed a letter to CISA's director, Jen Easterly.  The letter requests that CISA examine the very popular drones made by DJI for evidence that China might be covertly acquiring valuable information from them.  Okay.  In a minute, we're going to walk through a complete, interesting, and revealing well-conducted technical forensic analysis of DJI's drone controller software to learn exactly what's going on.  But let's first set the stage, because this just happened, by looking at this letter which reveals the politics which are driving the concern.



And for those who don't follow politics, the names of the senators won't mean much.  But for those who do, these are all senators, many of them senior, that you'll have heard of.  So this letter was signed by Mark Warner, Marsha Blackburn, Richard Blumenthal, John Thune, Jeanne Shaheen, Rick Scott, Kyrsten Sinema, Todd Young, J.D. Vance, Ted Budd, Dan Sullivan, Deb Fischer, Mike Braun, Cynthia Lummis, Tommy Tuberville, and Jerry Moran.  So serious players here on the Senate.



So here's what the senators are asking of CISA's director.  They said:  "Dear Director Easterly:  We write today regarding the cybersecurity risks posed by the widespread use of drones manufactured by Shenzhen DJI Innovation Technology Co., Ltd. (DJI) to operators of critical infrastructure and state and local law enforcement in the United States.  In short, we believe that, given the company's identified connections to the Chinese Communist Party, the use of its drones in such sensitive contexts may present an unacceptable security vulnerability.  We ask that the Cybersecurity and Infrastructure Security Agency (CISA) evaluate this concern and make the results of its evaluation available to the public through the National Cyber Awareness System.



"China's efforts to modernize the capabilities of the People's Liberation Army, including through their Military-Civil Fusion strategy, which systematically blurs the lines between PLA and civilian science and technology research and development efforts, are well documented.  In October 2022, the Department of Defense identified DJI as a 'Chinese military company' operating in the U.S. under Section 1260H of the William M. ('Mac') Thornberry National Defense Authorization Act for Fiscal Year 2021.  Identification of this relationship between DJI and the PLA suggests a range of risks to U.S. operators of the technology, including that sensitive information or data could wind up in PLA hands.  Indeed, Huawei, another entity identified under Section 1260H, has been credibly accused by the Department of Justice of misappropriating intellectual property and trade secret information from U.S. companies.  Yet, despite these risks, the use of DJI drones remains widespread throughout the U.S."



LEO:  Here we go again.



STEVE:  Uh-huh.  "In 2021 it was reported that DJI controlled almost 90% of the consumer market in North America and over 70% of the industrial market."



LEO:  Yeah.  It's the only kind I buy.



STEVE:  I know.  They're the best; right, right.  And Leo, you're doing your part to increase those percentages.



LEO:  Yes, I am.  Every time I sink a drone.



STEVE:  "And in 2019 it was reported that 73% of public safety operations are flown by the company's aircraft.  As a result, the CCP may have access to a variety of proprietary information.  For example..."



LEO:  They don't have my information.  For crying out loud.



STEVE:  Right.  Yours is drowned.



LEO:  Yeah.



STEVE:  "For example, a 2017 Department of Homeland Security assessment warned that Chinese companies had used grape production information gathered by a DJI drone purchased by a California wine producer to inform their own land purchasing decisions.  Even worse" - oh, worse than that - "the widespread use of DJI drones to inspect critical infrastructure allows the CCP to develop a richly detailed, regularly updated picture of our nation's pipelines, railways, power generation facilities, and waterways."  Which I guess they can't get from their spy satellites orbiting overhead, or their balloons.  Anyway, they said:  "This sensitive information on the layout, operation, and maintenance of U.S. critical infrastructure could better enable targeting efforts in the event of conflict.



"We appreciate that CISA has addressed this risk in the past, most notably in a 2019 Industry Alert stating the federal government's 'strong concerns' with Chinese drones and warning entities to be 'cautious' in purchasing them.  However, over the past four years more information regarding the scope of the problem has become available" - and that's what we'll be talking about - "including the official identification of DJI as a Chinese military company by the Department of Defense.  We therefore ask that CISA revisit its analysis of the security risks posed by the use of DJI-manufactured drones and release the results of that analysis publicly through the National Cyber Awareness System."



What do we know about DJI's observed behavior?  Three years ago, the security firm GRIMM went to a great deal of trouble reverse-engineering DJI's software.  Here's what they found.  They said:  "Given the recent controversy over DJI drones, a defense and public safety technology vendor sought to investigate the privacy implications of DJI drones within the Android DJI GO 4 application.  To conduct their analysis, the vendor partnered with Synacktiv" - and that's a group we've referred to before, a credible security firm - "who performed an in-depth dynamic and static analysis of the application.



"Their analysis discovered four main causes of concern within the DJI GO 4 application, most notably the application contains a self-update feature that bypasses the Google Play Store.  The application contains the ability to download and install arbitrary applications, with user approval, via the Weibo SDK.  During this process, the Weibo SDK also collects the user's private information and transmits it to Weibo.  Prior to version 4.3.36, the application contained the Mob SDK, which collects the user's private information and transmits it to MobTech, a Chinese analytics company.  And finally, the application restarts itself when closed via the Android swipe close gesture.  Thus users may be tricked into thinking the application is closed, but it could be running in the background while sending telemetry requests.



"To provide an independent review of the findings, the vendor then asked GRIMM" - these guys - "to validate Synacktiv's findings.  This blog describes," they wrote, "GRIMM's setup and workflow for validating the Synacktiv research.  Using the techniques described in the following sections to perform static and dynamic analysis on the DJI GO 4 Android application, GRIMM was able to verify and confirm the findings from Synacktiv's report.  The code associated with this blog post can be found in our GitHub repository."



Okay.  So let's follow along because it's much more interesting than just being asked to accept the conclusions without knowing where they came from.  It's also interesting to learn how such an investigation is conducted.  So they wrote:  "GRIMM's researchers used two different set-ups:  an ARM-based Android 6.0 Marshmallow (API 23) emulator; and another with two physical devices, a rooted Nexus 6 and an unrooted Motorola Moto 3G.



"The Android emulator is a part of Android Virtual Devices Manager, a subsystem of Android Studio that can be controlled through ADB, which is the Android Debug Bridge.  Additionally, Android Studio is able to redirect all traffic to an HTTP Proxy.  We redirected traffic through Burp Suite, under which requests can be captured and intercepted.  Frida, a dynamic instrumentation tool, was also used on the emulator by directly pushing and running Frida server on the device.  We chose API 23 due to the added Certificate Authority certificate protections which were introduced in Android 24."  So the point was that their TLS proxying was easier under API 23.



"The Nexus 6P running Android N, API 23 also, was connected to a desktop through USB and controlled through ADB.  Both devices were connected to the same wireless network.  The setup for analysis on this phone was similar to the emulator, except for the proxy and certificate.  The proxy was done with iptables to redirect all traffic on ports 443 and 80 to Burp.  Originally, we attempted to connect via USB Ethernet adapter, but we found that the behavior of the app was different from the more normal WiFi setup.  We used Frida to bypass SSL pinning on the Nexus 6P.  Additional testing was conducted with a similar setup using a Motorola Moto 3G running Android L and the OWASP ZAP proxy."



Okay.  So that setup gives them a testing platform, the ability to view, extract, and debug the Android code through the Android Debug Bridge.  And they have an effective shim which allows them to transparently monitor all communications in the clear without encryption so they can see everything that's going on.



They said:  "The DJI GO 4 Android application was heavily obfuscated, utilizing both static and dynamic obfuscation techniques to thwart analysis.  Synacktiv provided GRIMM with a detailed write-up and scripts to deobfuscate the code and help analyze the application.  The first protection the application uses is a custom version of (B-A-N-G-C-L-E) Bangcle.  This tool encrypts Java bytecode (.dex files), which can then be decrypted and loaded dynamically during runtime.  To understand and defeat this technique, we can draw parallels to the well-known binary obfuscation technique 'packing,' where the code contained within an executable is also decrypted and loaded during runtime.



"The two main methods of deobfuscating packed binaries are to statically analyze the packing routines and extract the data, or dump the memory of the executable after the data has been decrypted.  In the context of Android applications, we can do the same.  There has been previous research on static analysis of Bangcle.  However, Synacktiv was unable to apply the previous techniques to the DJI GO 4 application, as it is using a custom version of Bangcle.  Rather, GRIMM utilized Synacktiv's Frida scripts to search through the memory of the Android application at runtime and dump the decrypted .dex Java bytecode files.  With the dumped Java bytecode files, GRIMM was able to use Java decompilers such as jadx and Procyon to decompile the bytecode and obtain near-accurate Java source code on which we can perform," they wrote, "static analysis.



"In addition to protecting the Android Java bytecode, the Java source code also features various static obfuscation techniques, most notably string obfuscation.  Most of the strings used in the Java source code are obfuscated.  However, this protection is rather simple to decipher, as described by Synacktiv."  And it turned out they were just Base64 encoded after being XOR-scrambled with a hardcoded key.



And they said:  "Additionally, the DJI GO 4 application uses obfuscated string getter classes.  These classes define an accessor function which takes an index to the desired string.  These obfuscated strings can be easily recovered by decompiling the relevant class, adding a main function that dumps the strings, recompiling the code and executing it."  And I'll just note that there's nothing at all nefarious about using string indexes.  I did exactly the same thing in my design of SQRL.  It's a very clean way of adding language-independence to an application.  Throughout your code you refer to UI display strings only by index, and then a language pack provides the phrase dictionary which the indexes point to.  Anyway, they said:  "With the ability to decompile the Java code and decode strings within the Java code, as well as intercept and analyze the application's network requests, we were able to fully reverse engineer the application's execution."



So Synacktiv's report describes the DJI GO 4's custom update mechanism.  This update service does not use the Google Play Store and thus is not subject to the review process.  As such, there is no guarantee that the application that is downloaded for one user matches that of another user.  If DJI's update server is malicious or compromised by an attacker, it could use this mechanism to target individual users with malicious application updates.  And this behavior is a violation of Google's Developer Program Policies, which states:  "An app distributed via Google Play may not modify, replace, or update itself using any method other than Google Play's update mechanism.  Likewise, an app may not download executable code, for example, dex, JAR, or .so files, from a source other than Google Play."



They said:  "Using dynamic analysis, GRIMM researchers were able to intercept traffic pertaining to the update of the DJI GO 4 application.  Upon application startup or when using the 'Check for Updates' option within the application, a request was sent to service-adhoc.dji.com, which responds with a URL to an updated application APK.  This APK file is downloaded directly from DJI's servers via a URL."  And they provide it.  There's a djicdn.com and then a bunch of hex.



"This update option completely bypasses the Google Play Store, giving DJI's servers the ability to fully control the APK downloaded, whether with malicious intent or not.  When the server's response is received, the application prompts the user with the update notification.  Once the user clicks on the update notification, they're asked to install the update.  This update process does require the user to give the DJI GO 4 application the 'Install unknown apps' permission.  To help investigate the issue further, GRIMM modified the server's response."



Anyway, I go on in additional detail, as do they, following all of this.  And this all comes down to the app, yes, is able to pull updates from anywhere it wants to outside of the Google Play Store.  There are some SDKs which do obtain significant personal information which it's easy to argue they don't need.  For example, at one point the app grabs the IMEI, the ICCID, the MAC address, the Android ID, the device name and so forth, encrypts it using an RSA public key embedded in this Weibo SDK, and sends it.  They basically fully reverse engineered this application.  And yes, if you wanted to be worried, then here's an example.  What isn't said, of course, is that what other apps are doing the same thing.



They wrap this up finally under "Impact."  And they said:  "Given these findings, it's useful to consider the best- and worst-case scenarios for how these features are used.  While they could just be slightly odd implementations for acceptable behavior, they could also be used in a much more nefarious way."  Again, okay.



"In the best-case scenario, these features are only used to install legitimate versions of applications that may be of interest to the user, such as suggesting additional DJI or Weibo applications.  In this case, the much more common technique is to display the additional application in the Google Play Store app by linking to it from within the application.  Then, if the user chooses to, they can install the application directly from the Google Play Store.  Similarly, the self-updating components may only be used to provide users with the most up-to-date version of the application.  However, this can also be more easily accomplished through the Google Play Store.



"In the worst case, these features can be used to target specific users with malicious updates or applications that could be used to exploit the user's phone.  Given the amount of user's information retrieved from the device, DJI or Weibo would easily be able to identify specific targets of interest.  The next step in exploiting these targets would be to suggest a new application via the Weibo SDK or update the DJI application with a customized version built specifically to exploit their device.  Once their device has been exploited, it could be used to gather additional information from the phone, track the user via the phone's various sensors, or be used as a springboard to attack other devices on the phone's WiFi network.  This targeting system would allow an attacker to be much stealthier with their exploitation, rather than much noisier techniques such as exploiting all devices visiting a website.



"Regardless of whether DJI or Weibo utilize their applications' functionality to target users, they have created an effective targeting system.  As such, attackers who know of this functionality may attempt to compromise DJI's and Weibo's servers to exploit this functionality themselves.  Given this risk, it's much safer to rely on Google to provide application validation and distribution security."  And, okay, I got a chuckle out of that line, since the overtly policy-violating behavior these guys have reverse engineered, demonstrated, and observed for themselves - oh, and I skipped over where they actually changed the URL that was being queried and demonstrated the successful installation of an arbitrary APK.  So it can indeed...



LEO:  There you go.  You can do it.  Proof of concept.



STEVE:  ...install anything, exactly.



LEO:  Yeah.



STEVE:  So anyway, what I got a chuckle out of was that this policy-violating behavior was downloaded initially through the Google Play Store.  So, lot of good that did; right?  I mean, as we know, there's, what is it, I don't know how - I have it in my show notes a little bit later, how many millions of apps are on Google Play.



So the GRIMM guys conclude, saying this,  They say:  "This blog post details GRIMM's efforts to validate Synacktiv's privacy assessment of the DJI GO 4 Android application and determine the impact of their findings.  After dumping the encrypted classes and setting up an emulated and physical test environment, GRIMM performed static and dynamic analysis in order to reverse the application and validate Synacktiv's findings.  The DJI GO 4 application contains several suspicious features, as well as a number of anti-analysis techniques, not found in other applications using the same SDKs.  Overall, these features are worrisome and may allow DJI or Weibo to access the users' private information or target them for further exploitation."



Okay.  What wasn't directly addressed here was the application platform.  This was not the analysis of some controlling code buried in a lawnmower.  It's the code controlling what happens to video imaging being captured by the world's most popular aerial drones which are in use, not only by U.S. citizens, but by U.S. law enforcement and military, on military bases in the U.S. and elsewhere.  Is there any reason whatsoever to think that anything nefarious is going on?  No.  Is there any solid evidence of misuse of this technology, perhaps beyond suspicions about some data leakage from grape harvesting?  Apparently not.  Are the U.S. senators wrong to be concerned?  No.



We now have incontrovertible proof that we have unwittingly invited hundreds of thousands of camera-equipped Flying Trojan Horses into our midst, including into areas where there is danger of some of our nation's most private and sensitive operations being sent to a country with whom we appear to be becoming increasingly adversarial.



So this brings us back once again, this time armed with a beautifully clear example, to the utter insanity of the situation we are currently in.  None of this makes any objective rational sense if we're doing anything more than merely paying lip service to security.  We've walked into it with our eyes wide open.  Why?  Probably mostly because it was the path of least resistance which was established while everyone was happily getting along and minding their own business.  This concrete and clear example begs the question, "But what if?"  But this DJI drone instance is just one among millions of similar potential true points of vulnerability.



Okay.  For example, the Windows operating system that most of the world is sitting in front of is composed of a kernel and libraries for which Microsoft is the author and has the source code.  But in order for it to do anything useful at all, the system also contains countless proprietary third-party device drivers, the source code for which Microsoft has never seen.  What do those drivers, in detail, do?  No one other than their authors has any idea.



Could any one or more of them have undisclosed nefarious Trojan-like functionality?  Of course they could.  Why not?  And if they don't today, any future update to them could, just like any app in Google's Play Store.  For the sake of convenience, an increasing number of these are included with the base operating system image.  But the system also has the capability of going out to fetch additional drivers when needed, and their updates.  And all of these many chunks of unknown and unvetted code operate at ring 0 with full unrestricted kernel privileges.



My point is, the actual security model of the world's most pervasive operating system is utterly broken.  It's a complete joke.  It's smoke and mirrors.  We don't want that to be true. It's quite uncomfortable for it to be true.  But pretending that it isn't true doesn't change the reality.  It's the Wizard of Oz, where we're supposed to keep our eyes on the impressive display of security in front of us - Steve Ballmer jumping around onstage - while we dare not consider and look behind the flimsy curtain where reality lurks.



What's the solution?  Well, if ever hostilities across the world escalate into a true fight, we're screwed.  The first thing you should do probably is turn off your router and preserve the operation of your own internal network.



LEO:  That's a good point.



STEVE:  The only solution I can see is for everyone to soberly appreciate the true consequences of what would happen now, in today's deeply interconnected world, if superpower hostilities were ever to boil over.  We all need to just get along, and for all of the embedded Trojan code that everybody has probably installed over time in everybody else's worlds to remain untriggered and unused.



Looping back to the DJI GO 4 app, that app is just one from among the - and here it is - the 2.65 million apps which are currently listed and available for download through Google Play Store.  The economics of what Google has built does not allow for any authoritative representation of app security to be made.  Google depends upon some of its own engineering and the engineering of many other security companies to analyze apps and catch misbehavior.  But we are constantly learning of hundreds of thousands, if not millions, of downloads of apps by users, which are later found to contain malicious functions.



The only long-term solution, if we're really willing to foot the cost of true security, is for all proprietary closed solutions to be eliminated and for everything to be open source, created by a broad community of cross-checking developers.  Until and unless that happens, all we can do is hope for the best.



LEO:  Wow, Steve.  You've finally come around.  I am a big proponent of open, and that's one of the many reasons why.



STEVE:  Yeah.



LEO:  Richard Stallman calls proprietary software "malware" because you never can know exactly what it's doing or who it's phoning.



STEVE:  No.  And as I've said, the idea that voting machines are...



LEO:  That's nuts.  Closed source.



STEVE:  Like Dominion?  It ought to be open source.



LEO:  Open source it.



STEVE:  You can still make money by selling the hardware.



LEO:  Hell, yeah.



STEVE:  The touchscreens and the machines and all that stuff.  You just can't keep what it does to yourself.  That is nuts.



LEO:  Totally agree.



STEVE:  And so we've evolved a proprietary software ecosystem around powerful companies that want to leverage the fact that their stuff is secret.  And we're now looking, we're in danger of having what we have sown being reaped against us.  And again, think about all of the random stuff we install with device drivers running in ring 0, and no one has ever seen what's inside them.



LEO:  Yup.



STEVE:  We have no idea.



LEO:  Yup.  Really good point.  I couldn't agree more.



STEVE:  And I don't know how we get there from here.  But at least taking a sober look at where "here" is, is important.  And yes, you know, flying Trojans from a Chinese military-owned company, that's a little nerve-wracking.  And, okay, so the government is going to have to use domestic drones that aren't going to be as good and are going to cost three times as much.



LEO:  Right.



STEVE:  But if that's what you want, then that's what you're going to have to pay for.



LEO:  And if we go to war, throw your smartphones out...



STEVE:  Unplug.



LEO:  ...and go get those Nokia candy bar phones.



STEVE:  Yeah.



LEO:  You know, we were talking about this earlier on MacBreak Weekly.  Russia's government says "no smartphone for you" because smartphones are inherently spy devices.  I mean, just in every respect.



STEVE:  Yup.  They are connected computers, and they're not open.  They are closed.



LEO:  And nobody knows what they're doing, yeah.



STEVE:  Exactly.



LEO:  Yeah.  Very good.  Thank you, Steve.  You've done it again, my friend.



Copyright (c) 2023 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.






GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#916

DATE:		March 28, 2023

TITLE:		Microsoft's Email Extortion

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-916.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  In this week's grab bag question collection we wonder:  What happened, and who cleaned up during last week's elite 2023 Pwn2Own competition?  What happens when GitHub inadvertently exposes their own private SSH RSA key?  Are all DDoS-for-hire sites legitimate, and is legitimate ever a word we can apply?  Just how bad has the malicious open source registry package problem become?  And how is it that Russia's presidential staff are still using iPhones?  After its rocky start in the limelight, how has Zoom's security been faring these past few years?  And what benefits can be derived from the sum of two sine waves along a logarithmic curve?  What new feature is Microsoft exploring for their already feature-encumbered web browser?  And in one of my blessedly rare rants we're then going to learn what new "revenue harvesting" measure Microsoft has just announced which seems deeply ethically wrong to me.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  There's lots to talk about.  An amazing amount of bug bounties paid by Zoom over the years.  Steve gives them high praise.  We'll talk about Pwn2Own.  They just had it at CanSecWest in Vancouver.  And a big winner this year, amazing winner this year.  And we'll talk about 144,000 malicious packages published in open source software registries.  That, and a stinging rant against Microsoft, all coming up next on Security Now!.	



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 916, recorded Tuesday, March 28th, 2023:  Microsoft's Email Extortion.



IT's time for Security Now!, the show where we cover the latest in security, privacy, computers, science fiction, vitamins.  Whatever it is Steve wants to talk about, I'm game.  Steve Gibson is here, our host.  Hello, Steve.



STEVE GIBSON:  Yo, Leo.  This is our sendoff for you.



LEO:  Yes.



STEVE:  We're going to be missing you for, what, I guess three episodes?



LEO:  Yeah, I think Ant's going to take over next week.  And Jason is in Costa Rica right now.  But when he gets back, he'll come back in.  It's a mish-mosh of people hosting the show.



STEVE:  It's a potpourri.



LEO:  But it's really Steve's show.  You're all that matters on this show.



STEVE:  Well, in this week's grab bag collection, we wonder what happened, and who cleaned up during last week's elite 2023 Pwn2Own competition?  What happens when GitHub inadvertently exposes their own private SSH RSA key?  Are all DDoS-for-hire sites legitimate?  And is "legitimate" actually a word we can even apply to DDoS-for-hire sites?  Just how bad has the malicious open source registry package problem become?  And how is it that Russia's presidential staff are still using iPhones?



After its rocky start in the limelight, how has Zoom's security been faring these past few years?  And what benefits can be derived from the sum of two sine waves along a logarithmic curve?  What new feature is Microsoft exploring for their already rather feature-encumbered, shall we say, web browser?  And in one of my blessedly rare rants we're going to learn what new "revenue harvesting" measure Microsoft has just announced, which to me seems deeply ethically wrong.  Today's podcast is titled "Microsoft's Email Extortion."



LEO:  Oh, boy.



STEVE:  Yeah.  And of course...



LEO:  Well, I look forward to the answers to all those questions and yet another ridiculous Picture of the Week.  It's time for the Picture of the Week.



STEVE:  For the weekly photo conundrum.  So for those who are not looking, imagine that you have a tall wall, very tall.  And for some reason you have a need to get to the top of it occasionally.  So what do you do?  Well, you run a staircase up the side of the wall.



LEO:  Yeah.



STEVE:  Because, you know, then you could climb the stairs to get to the top of the wall.  But now you have a problem because you decide you don't want everybody to be able to climb to the top of the wall.  You only want some people to do it.  So what do you do?  You put a gate with a lock so that only people who have a key to the lock are able to open the gate and then climb to the top of the wall.



But here's the problem.  In this instance, what we see is this gate that extends across the stairs is on the third stair up.  So like you go up three steps, oh, and there's a gate.  Well, okay.  Now, there is nothing to keep you from swinging around the outside of the gate because the gate's open on one end; right?  On one side of the gate it's against the wall, as are the steps going up the side of the wall.  But you can just, like, step around the gate because it's open to the air on the other side.  And there's a conveniently placed handrail on the other side for those who do manage to swing around the gate.



LEO:  We wouldn't want you to fall.



STEVE:  Basically ignoring it.  Anyway, the picture's wonderful because you just think - in fact, I gave it the caption, "Well, that'll stop 'em," because, you know, it won't.  And again, this is one of those situations where a lot of time and industry and thought went into this.  Now, the good news is, on this gate, unlike some of the other gates we've seen, the bars of the gate are vertical rather than horizontal.  We have encountered in the past gates where they made horizontal bars, creating a ladder out of the gate.  So yes, you could, if you didn't want to swing around the side, and the bars were horizontal, you could just use it like a ladder in order to climb over.  But in fact in that case you could just probably go up to the top of the wall from the top of the gate.  But anyway.  Once again, thank you to our Twitter followers who are now understanding what sort of picture to send me to make history because we're getting a lot of great pictures from our listeners.



LEO:  It's just amazing how many poorly designed facilities there are out there.  I'm expecting as I wander around ancient Europe, Rome, Barcelona, I might find a few of these.  And I will send them right along.



STEVE:  Please have your camera ready, Leo, because, you know...



LEO:  I shall.



STEVE:  ...they are a constant source of entertainment.



LEO:  I'll get Lisa to take a picture of me ascending one of these.



STEVE:  It does make one wonder about humanity a little bit.  It's like, well, maybe we should be taken over by the AIs that might actually be intelligent.



Okay.  So speaking of making history, Synacktiv's name first came up, well, actually we've talked about them way in the past.  But they also came up last week when the firm GRIMM was asked to double-check and verify the result of Synacktiv's forensic reverse engineering of DJI's drone-controlling software.  Since Synacktiv's report could have been seen as highly inflammatory, depending upon how much one believes that we actually have any true security to start with, it appeared that GRIMM was brought in to obtain the classic "second opinion."  And as we know from last week's topic, they concurred with everything that Synacktiv found.



Okay.  So if we didn't already know that Synacktiv clearly knows their stuff, their breathtaking performance during last week's three-day annual Pwn2Own hacking contest, held in Vancouver, Canada, would stand as testimony to that.



Through the years of this podcast we've been tracking these Pwn2Own competitions, both because they're a lot of fun and because it never hurts to have an occasional reality check to remind us that whenever skilled hackers take aim at some technology  pretty much, it seems, any technology  which everyone believes to be secure, that belief is quickly proven to be merely wishful thinking.



There are three Pwn2Own hacking contests that take place throughout the year.  One is dedicated to smartphones and IoT devices, and we recently covered some of the results from that one.  Another is dedicated to industrial equipment, which we also recently talked about.  And the third is last week's CanSecWest event that's dedicated to, probably more interesting to most of us, desktops, servers, and smart cars.  This competition is widely regarded now as the premiere, most prestigious of all the hacking contests.  And this was not Synacktiv's first Pwn2Own win, since they took the title two years ago after the competition in 2021.



Okay.  So during the multi-category three-day event, the Synacktiv team successfully demonstrated, first, a heap overflow vulnerability and an out-of-bounds write error in a Bluetooth chipset.  And this is what's interesting is that, you know, we were talking a couple weeks ago about the baseband modem, the bugs that Google found, four of them being the worst possible, where no user action - this was in a Samsung baseband cellular modem chip, and how that little chip was able to result in a complete compromise of the phone.  Well, here we have a Bluetooth chip that allows you to take over a Tesla.



So this is an out-of-bounds write error in a Bluetooth chipset which allowed them to break into Tesla's infotainment system.  And from there they were able to gain root access to the rest of the car.  That bit of wizardry netted them a cool, get this, quarter million dollars and Pwn2Own's first-ever Tier 2 award, which is a designation the contest organizers reserve for particularly impactful vulnerabilities and exploits.



They also demonstrated an attack, secondly, known as "TOCTOU."  That's an abbreviation in the industry which stands for time-of-check/time-of-use.  The less fancy term for this, like the original term, is a "race condition."  That's what we always used to call it.  Now it's a TOCTOU.  Okay.  So a race condition where there is a time-critical sequence of events that can be used in some manner to slip past a system's defenses, like you query for some status, then you do something that the designers didn't anticipate rather than waiting for the answer, for example.  In this instance they pulled off an attack on Tesla's Gateway energy management system.  They showed how they could then, among other things, open the front trunk or side door of a Tesla Model 3 while the car was in motion.  That less-than-two-minute attack earned the researchers a new Tesla Model 3.  Talk about Pwn2Own.



LEO:  But they won't want to drive it because it's dangerous.



STEVE:  Yeah.  Be sure you have your seatbelt on and tie down the front hood.  And they also got a cash reward of $100,000.  Next, they pulled off a three-bug chain against Oracle's VirtualBox with a host elevation of privilege to earn themselves  $80,000, compounding that to their rapidly growing winnings.  They used another TOCTOU bug to escalate their privileges on macOS, earning $40,000.  By leveraging an incorrect pointer scaling, they were able to elevate their privileges on Ubuntu's Desktop Linux to win $30,000.  And, finally, they leveraged a use-after-free flaw against Windows 11 for another $30,000.



LEO:  Wow, they made out like bandits.



STEVE:  They really cleaned up.  Overall they took home more than half a million dollars, $530,000, and a shiny new Tesla Model 3 which earned them the largest award ever raked in by any one contestant in Pwn2Own's history.  And it's a good competition.  Star Labs, which was the runner-up, took home a $195,000, which was not bad either.  So that's a contest to pay attention to.  And what's interesting, of course, is remember that China pulled their hackers out of Pwn2Own.  So maybe things would be different if China's hackers, which have proved themselves to be extremely skillful many years in a row, if they'd been there.  But that's their, you know, China's keeping them home now.  



Okay.  Here the moral of the story for GitHub is "Mistakes happen."  We were just talking about the benefit of having GitHub repositories continuously scanned for any inadvertent leakage of secret data.  You know, it's like keys, which should never be published.  So it was interesting that this just happened to GitHub themselves, causing them to rotate their primary SSH RSA key. Here's what GitHub explained.



They said:  "At approximately 05:00 UTC on March 24th, out of an abundance of caution, we replaced our RSA SSH host key used to secure Git operations for GitHub.com.  We did this to protect our users from any chance of an adversary impersonating GitHub or eavesdropping on their Git operations over SSH.  This key does not grant access to GitHub's infrastructure or customer data.  This change only impacts Git operations over SSH using RSA.  Web traffic to GitHub.com and HTTPS Git operations are not affected."  So to clarify, they said:  "Only GitHub.com's RSA SSH key was replaced.  No change is required for Elliptic Curve DSA or Ed25519 users."



So then they explained a little bit more.  They said:  "This week we discovered that GitHub.com's RSA SSH private key was briefly exposed in a public GitHub repository."  They don't ever tell us, like, how that happened, but certainly they dug into it and figured it out.  They said:  "We immediately acted to contain the exposure and began investigating to understand the root cause and impact.  We have now completed the key replacement, and users will see the change propagate over the next 30 minutes.  Some users may have noticed that the new key was briefly present beginning around 02:30 UTC during preparations for this change."  They probably, like, quickly made sure that it would work, and then they pulled it back and then got ready to do the whole stage.



So they said:  "Please note that this issue was not the result of a compromise of any GitHub systems or customer information.  Instead, the exposure was the result of what we believe to be an inadvertent publishing of private information.  We have no reason to believe that the exposed key was abused, and took this action out of an abundance of caution."  So anyway, just sort of a, you know, a reminder that even somebody that's taking every precaution, who is being safe, mistakes happen.  And so all you can do is say "Whoops" and then look at what the consequences of those are and fix them.  Which they promptly did.



So I titled this "DDoS for Hire, or Not."  And I love this idea.  It just makes so much sense to me.  The UK's National Crime Agency says its agents have created several fake DDoS-for-hire services that are up and running today.  You know, so these are - they look legitimate, right, from the outside.  So they're on the Dark Web.  You can get to them through onion routing with some funky URL that gets passed around among hackers.



Now, okay.  So such a site - so for the National Crime Agency to create a fake DDoS-for-hire service, the point is they're trying to catch people, right, who are wanting to hire these DDoS-for-hire services.  But they actually have a dual purpose.  They can catch those in the act of attempting to hire DDoSers, as well as frightening away others when the nature of the sting operation is revealed.  And that's what caught my attention and I thought was so clever.



To serve that second agenda, the National Crime Agency chose last week to reveal one of its previously popular fake DDoS sites by replacing the site's previous homepage with a splash screen announcing the chilling truth.  And I've got the screen in the show notes.  It says:  "This site was created and controlled by the National Crime Agency."



LEO:  This looks so fake.  This looks completely fake.



STEVE:  Yeah.



LEO:  They really could have done a better job making this realistic.  Doesn't it?  I mean, it looks, I mean, "Operation PowerOFF."



STEVE:  Yeah.



LEO:  Come on, man.  This is like from the '80s.



STEVE:  They have the Europol seal, and a seal for the NCA.  But so what they're wanting to do is, this says:  "The National Crime Agency collaborated under Operation PowerOFF to target users of criminal DDoS services.  DDoS attacks are illegal in the majority of countries.  The National Crime Agency has collected substantial data from those who have accessed our domain.  We will share this data with international law enforcement for action.  Individuals in the U.K. who engaged with this site will be contacted by law enforcement.  The National Crime Agency has been and will run more services like this site.  Operation PowerOFF has already resulted in the arrest of numerous individuals and continues to ensure that users are being held accountable for their criminal activity."



So the point of the second phase is, you know, imagine you're a miscreant who wants to, you know, who's annoyed with somebody else and wants to DDoS them.  So you go to this site that, you know, you may know of.  Maybe you've used it in the past.  And now you're greeted with the news that this was a sting.  The whole thing was a sting.  So what that does is it chills the entire enterprise of DDoS for Hire because certainly the word will spread through the underground that law enforcement is erecting fake sting DDoS-for-hire sites.  And how would you know?



So anyway, I just think that's a clever repurposing of the concept.  Run it for a while, collect lots of names, and then flip the thing around to show people that, well, maybe you shouldn't be using DDoS for hire because we're running many of these, and you don't know which other ones you may be inclined to hire are actually ours.  And then we're going to get you.



Okay.  So I saw a statistic that was somewhat sobering.  We've been looking at the new challenges facing online open source repositories which are increasingly being poisoned by a flood of malicious package uploads.  These fall under the umbrella of supply-chain attacks.  The developer security firm Snyk, S-N-Y-K, says that it recorded more than 6,800 malicious libraries uploaded on the npm and PyPI portals since just the start of this year, so not yet three months, 6,800 individual specific malicious libraries.



The number that caught my eye was that Snyk said that this recent batch brings the grand total of specifically identified malicious packages to more than 144,000 published to open source software registries, published and identified, found and removed, over the past several years since unfortunately this growing problem was identified.  I mean, it's good that it was identified.  It's unfortunate that it's a growing problem.  And wow, you know, it's not clear how we're going to solve this problem.



Okay.  So, the Russian news publication Kommersant reports that the Kremlin's security team has instructed Russia's entire presidential staff to discontinue all use of iPhones by April 1st, April Fools' Day.  Kommersant reports that employees were told to get an Android device, either from a Chinese vendor,  not surprisingly, since China's now Russia's friend, increasingly so, or one running Rostelecom's Aurora OS.  And I loved this.  The Kremlin officials cited security considerations as being behind their decision, claiming that iPhones were "more susceptible to hacking and espionage by Western experts compared to other smartphones."



Huh?  Okay, well, that doesn't correspond to anything we know.  But it is certainly another of the recent examples that we've looked at here.  In an environment of increasing mistrust and hostility, it really doesn't make any sense for anyone to be using a closed device sourced from an entity on the other side of the dispute.  And Apple's iPhones are certainly far more closed than Android devices.  So, yeah, if Russia is planning a long-term split from the West, then discontinuing all use of Western-sourced tech is the only sane long-term strategy.



In our industry's apparently eternal quest to rid ourselves of mistakes made in the creation of software, one of the more effective strategies that's been found is the idea of paying good guys to find and report those flaws before bad guys can find them and use them against us.  Thus bug bounty programs have become a mainstay.  The COVID-driven work-from-home boom quickly put a lesser-known video conferencing system, Zoom, on the map.  But as we know, not everything went well from the start.  As we've seen time and time again, many more bugs exist than are known.



So Zoom's pre-celebrity confidence in its own software was quickly shaken when bad guys began looking more closely at it than ever before and discovered all sorts of ways that its benefits could be subverted.  As we covered at the time, this came as quite a shock to Zoom's management, and they did stumble a little bit out of the gate.  But to their credit they quickly hired some experienced right-thinking true security experts, and the establishment of a functioning bug bounty program was near the top of their list.



Okay.  So we're now several years downstream.  How has that been going?  Here's how Roy Davis, Zoom's Security Manager, described this effort in a blog posting last week.  Roy wrote:  "In security, it's all about who gets there first.  We race to identify bugs and issues before the bad guys do, so we tap the ethical hacking community to help us get ahead.  We source this help through our Zoom Bug Bounty program, which lets us connect and engage security researchers that help us proactively mitigate risk and create a safer environment for our customers.  And we've accomplished a lot as a community in the past year.  Here's a look," he says.



"We test our infrastructure every day at Zoom, but we know we're not immune to edge-case vulnerabilities.  So we call in backup.  The ethical hacker community can sometimes detect bugs that may only be discovered in certain circumstances.  That's why our bug bounty program focuses on recruiting skilled, effective researchers.  In 2022, we sent additional invitations to researchers to join our HackerOne program with a focus on attracting active security talent.  We also like to go beyond our program to find talent, so we tapped into the community via industry events like H1-702," which I'll talk about in a second, which was a HackerOne event, H1 standing for HackerOne.



He wrote:  "These researchers work hard to help us, so we strive to celebrate successful report submissions accordingly.  In the fiscal year 2023" - and I don't know how their fiscal calendar is aligned, but presumably it just closed.  He says:  "We awarded" - and this number surprised me - "$3.9 million in bug bounties to hundreds of researchers and over $7 million to date since the program began."  So props to Zoom.



He said:  "Beyond identifying vulnerabilities, outside researchers' support has helped us make other forms of progress at Zoom.  We used these reports to demonstrate items that needed attention, flag root-level causes for issues, create better cross-functional alignment, and find potential threats before they become a problem.  As a result, our time-to-resolution for bug bounty reports has significantly improved over the past two years.  At the start of this year, we restructured our team and developed updates for the program for FY24.  We evaluated the researchers currently in our program to make sure everyone is active and contributing.  We want to put the right foot forward in the new year, and that all starts by working with high-caliber, effective researchers.



"Zoom's Bug Bounty program is also implementing a brand new Vulnerability Impact Scoring System to help researchers do their best work yet.  While we will continue to use the industry standard Common Vulnerability Scoring System (CVSS) to score reports, we're evolving our program to add a companion scoring system called the Vulnerability Impact Scoring System (VISS) that analyzes 13 different aspects of impact for each vulnerability reported as they relate to Zoom infrastructure, technology, and security of customer data.  With the implementation of VISS, Bug Bounty can focus more on measuring responsibly demonstrated impact, rather than the theoretical possibility of exploitation."



So then he finishes with the road ahead:  "As the Zoom Bug Bounty program has grown over the past year, we're continuing to evolve and mature our processes, bug bounty awards, and testing scope.  We're very excited to see the impact of our new scoring system and all the good our researchers can do in 2023.  If you're interested in helping to make Zoom more secure, email your HackerOne profile name to bugbounty@zoom.us or visit the Zoom Careers page to review the open positions within the Trust and Security team.  Happy hacking."  So I am very impressed.



LEO:  It's, I mean, the large number is good; right?  It means...



STEVE:  Yes.  Yes.



LEO:  ...they're finding it.



STEVE:  And they're being actively proactive.



LEO:  Yeah.



STEVE:  This is what being proactive about security looks like.  You know, yes, we all know, since we chronicled those early failures that they made, that Zoom was initially caught flat-footed when their platform took off.  But today Zoom's security team is actively, not passively, managing their bug bounty program.  And I think that's clearly making a big difference.  Not only are they clearly paying well for bugs that are being found, and they're willing to shell out so far $7 million, but they're not just passively listed over at HackerOne and claiming for the sake of a bullet point on a presentation slide that, oh, yes, we offer bug bounties.  No, they're serious about tightening up their platform.  And, you know, it was Alex Stamos that they hired, or they brought in as a consultant, as I recall; right?



LEO:  Consultant, yeah, yeah, yeah.  Oh, but not just that.  I mean, they really did - they had bought a crypto company that I use, and I was unhappy that they bought it.  But some of the best cryptographers in the world are now working there.  I mean, they did the right thing.  I think sometimes, you know, you hear these numbers like, oh, look at all the flaws that were found.  And I think it makes people think, oh, it must be insecure software.  But really that's a good thing, to find the flaws and fix them.  Everything has flaws; right?  I mean, not of course SpinRite.  But everything else has flaws.



STEVE:  Well, okay.  So, yes.  So what we keep seeing, we see example after example where something looks great until you look at it more closely.



LEO:  Yeah.



STEVE:  And that's what it takes.  As soon as you start scrutinizing, as soon as anybody, a good guy or a bad guy, starts scrutinizing it, you're going to find problems.  So yes, they're saying that they have paid out for hundreds of bug bounties.  But those are hundreds of fewer bugs that are there now.



LEO:  I think a lot of companies would be reluctant to say those numbers because they would assume people are going to say, wow, you really have, you know, your product's like Swiss cheese.



STEVE:  Right.  And the point is it once was, and now it is way less so.



LEO:  Now it's not, yeah, yeah.



STEVE:  So, you know, it is by examining those things.  So this other cool thing, that H1-702 event that Roy referred to, that was a multi-day HackerOne event held in Las Vegas, Nevada last August.  And Zoom was one of two corporate sponsors of the live hacking event on the 4th of August, during which more than 100 security professionals - about 70 of them were in-person and 40 were virtual - from 29 countries hacked the Zoom web and desktop client, the APIs, Zoom's Marketplace apps, and any of the binaries that Zoom distributes.  Five individual awards were distributed, and overall Zoom paid roughly $480,000 in bounties in that one day.  They said that they feel this is a reflection of the importance of this industry best practice, meaning paying bounties for responsibly reported bug discoveries.



They have come a long way from where they started when they first popped onto our radar, and back then it was in less than stellar fashion.  So today I say bravo, Zoom.  I really think they're doing, you know, this is the way to do it.  They understood, if they wanted to hold onto their position as like this suddenly popular video teleconferencing system, they needed to fix their security.  You know, it just hadn't been looked at that closely.  So I NUIT.



LEO:  Uh-oh.



STEVE:  Now, that's not as "I knew it."  That's NUIT, which in French means nighttime.



LEO:  Nuit.  Nuit.



STEVE:  Nuit.  Okay.  The knights who say nuit.  It's an acronym in this case for Near-Ultrasound Inaudible Trojan.



LEO:  Oh, boy.  That doesn't sound good.



STEVE:  Uh-huh.  Not good.



LEO:  Does not sound good.



STEVE:  And so, yes, some clever researchers are again going to entertain us with their out-of-the-box thinking.  Researchers from the University of Texas at San Antonio and the University of Colorado at Colorado Springs recently published a paper for presentation, or I should say submitted because it's not published yet, submitted a paper for presentation during the upcoming USENIX Security 2023 conference being held in April next month.  It demonstrates a novel inaudible voice Trojan attack which exploits vulnerabilities of smart device microphones and voice assistants like Siri, Google Assistant, Alexa, Cortana and so on.



The researchers used their Near-Ultrasound Inaudible Trojan (NUIT) to attack different types of smart devices, bridging from smart phones to smart home devices, or sometimes just within the smart phone itself.  The results of their demonstrations show that NUIT is effective in maliciously controlling the voice interfaces of popular tech products, and that those tech products which are currently on the market are vulnerable.



LEO:  Oh, this is really bad.



STEVE:  It's not good.  It takes advantage of the fact that digital assistants use microphones which accurately pick up sounds that are inaudible to the human ear.  NUIT plays sounds in a near-ultrasound frequency range from 16 to 20 kHz, which enables it to give voice commands to both close and more remote smart devices.



LEO:  Yeah, because that travels really well, that high frequency stuff.  Wow.



STEVE:  It does.  Now, their research demonstrated that NUIT-style near-ultrasound commands can be embedded pretty much anywhere.  An attacker could direct - and the demos, they've got like you just play a YouTube, and your phone lights up and does something.  It's really...



LEO:  Oh, it's terrible.



STEVE:  It's freaky.  So an attacker could direct a victim to click a link to a website that would play some audio, or a YouTube video that would then play the inaudible voice commands.  The researchers demonstrated that NUITs also work when playing from one phone which controls another, over Zoom calls, playing on a phone to control a smart speaker or another IOT device, or even embedded into files that have background music, and it'll still work through that.



Once they have unauthorized access to a device, hackers can send inaudible action commands to reduce a device's volume and prevent the voice assistant's response from being heard by the user before proceeding with further attacks.



Okay.  So I was unable to find their full research paper online; and the USENIX conference, as I mentioned, isn't until next month.  So some puzzles remain.  In some summary coverage published by their universities, they're quoted saying that to wage a successful attack against voice assistant devices, the length of malicious commands must be shorter than 0.77 seconds.



LEO:  Oh, that's pretty quick.



STEVE:  So that, yeah, so three quarters of a second, but we don't know why that's the case until their formal paper is published.  They did add that the vulnerability is created due to the nonlinearity of the microphone design, which the manufacturer would need, they said, to address.  And the researchers said that out of the 17 smart devices they tested, Apple's Siri devices alone needed to capture and reuse, that is, replay their user's voice, while other voice assistant devices were activated by using any voice or a robot voice.  They also pointed out that the attack could be surreptitious because it was possible to silence Siri's response since iPhones maintain separate volumes for Siri and non-Siri output.



And of course as anyone knows who's been around voice assistants, users of voice assistants experience odd triggering events, right, where it didn't appear that the system was being addressed when it suddenly woke up and said, you know, "Hey, boss, what do you want?"  So we know that these are the result of their microphones hearing and responding to a much wider range of frequencies than humans do, and constantly listening for that trigger.  So there's still a lot that we don't know about the mechanism of the attack, but there is an interesting opportunity for a bit of science and math conjecture here.



There was the comment made that the attack is due to nonlinearities in the operation of these microphones.  And that provided the clue for me.  That almost certainly means that the instantaneous response to air pressure sound waves is not linear.  Now, if the response was nonlinear at normal operating volume, the result would be unacceptable distortion.  That's what we call distortion.  But the nonlinearity is likely to be extreme at very low volume levels where that nonlinearity doesn't matter.



Any time you have a nonlinear response, the addition of two inputs along that nonlinear response curve is wonderfully turned into multiplication.  And although this may initially be counterintuitive, this is the principle of logarithms, and it's the way a slide rule, which adds linear lengths, is able to produce multiplication.  The scales of a slide rule are logarithmically nonlinear.  So when you're adding linear lengths on a slide rule, you're performing multiplication.



This means that, if we had two sine waves at very low volume, their summation by the device's microphone having a nonlinear response at low volume would have the effect of multiplying their values in real time.



LEO:  Instead of adding, they'd multiply.



STEVE:  Instead of adding, exactly.  So next we add one of my favorite trigonometric identities - don't we all have a favorite trigonometric identity? - which states that the product of two sine waves is equal to the sum and difference of their frequencies.  And Leo, you have your amateur radio operator's license.



LEO:  I do, yeah.



STEVE:  So you know of this as heterodyning.  In radio, heterodyning is the way a radio's local oscillator is able to bring a radio frequency signal down into audible frequency range.  What we hear is the difference between the two frequencies, neither of which are audible.  And that's exactly what's happening here in this attack.  The researchers are generating a pair of near-ultrasonic frequencies whose difference is the voice signal that they're using to control other devices.  We don't hear anything.  But the microphones in those devices, which are always straining to hear our commands, believe that they're hearing our voice because inaudible sine waves are being made to heterodyne.



LEO:  Wow.  That's quite clever.



STEVE:  Isn't that cool?



LEO:  Yeah.



STEVE:  Yeah.  And of course a problem because now regular audio stuff, audio material from wherever, could be containing commands that we can't hear.  So, and their demonstrations are really chilling.  So anyway, I'll keep my eye out for the paper when it's published next month.  And if there's anything more, we'll loop back to it.



Okay.  So the news is that Microsoft has started testing a cryptocurrency wallet which they are planning to build into their, let's just say "increasingly versatile" Edge browser.



LEO:  Wow.



STEVE:  I know.  I loved Ars Technica's take on this.  Their headline read "Microsoft is testing a built-in cryptocurrency wallet for the Edge browser."  Then it had the subhead "Crypto wallet would join coupons, cash back, and 'buy now, pay later' add-ons."  And in the show notes I have two screenshots which Ars showed.  So Andrew Cunningham is Ars Technica's Senior Technology Reporter whose take on this is, I think, spot-on.  So here's how Andrew explained and characterized this new find.  



He wrote:  "Microsoft appears to be testing a built-in cryptocurrency wallet for Edge, according to screenshots pulled from a beta build of the browser.  The feature, which the screenshots say is strictly for internal testing, was unearthed by Twitter user @thebookisclosed, who has a history of digging up present-but-disabled features in everything from new Windows 11 builds to ancient Windows Vista betas."



He says:  "This is one of many money and shopping-related features that Microsoft has bolted onto Edge since it was reborn as a Chromium-based browser a few years ago.  In late 2021, the company faced backlash after adding a 'buy now, pay later' short-term financing feature to Edge.  As an Edge user, the first thing I do," he writes, "in a new Windows install is disable the endless coupon code, price comparison, and cash-back pop-ups generated by Shopping in Microsoft Edge."  And then he says, in parens, "(Many settings automatically sync between Edge browsers when you sign in with a Microsoft account.  The default search engine and all of these shopping add-ons need to be changed manually every time)."  Meaning they're deliberately apparently not synchronizing.



He says:  "According to the screenshots, the crypto wallet is 'embedded in Edge, making it easy to use without installing any extension,' and it can handle multiple types of cryptocurrency.  It will also record transactions and the value of your individual currencies as they fluctuate.  An 'explore' tab offers news stories relevant to cryptocurrency, and an 'assets' tab will let you stare lovingly at your NFTs."



LEO:  Which is all you can do with them.  So, good, good, I'm glad, yeah.



STEVE:  Yeah, exactly.  "The wallet is 'non-custodial,' also called 'self-custodial,' meaning that you have sole ownership of and responsibility for the passwords and recovery keys that allow access to your funds.  Microsoft won't be able to let you back in if you lose your credentials."



LEO:  Good.  That's how it should be.



STEVE:  Yup.  "Whether you find these kinds of add-ons useful, annoying, or predatory is a matter of perspective.  Given the prevalence of crypto scams, there may be some value in having a 'trustworthy,'" he has in quotes, "built-in option that doesn't require the installation of dodgy third-party extensions.  But the feature could also encourage casually interested users to begin exploring the world of cryptocurrency, which is, again, rife with scams.



"It's also yet another example of Microsoft building a not strictly browsing-related feature into its web browser.  Many of these features can be disabled, and competing browsers like Chrome and Firefox all attempt to add value and earn money by building-in access to new niche features and third-party services.  But Microsoft's moves can still have an outsize impact that deserves extra scrutiny.  Edge is an installed-by-default, non-removable component of every Windows 10 and Windows 11 PC."  And Leo, I'm endlessly entertained by you and Paul talking on Windows Weekly about its refusal to go away.  And Lorrie, my wife, is like, honey, why does Bing keep coming back?



LEO:  Oh, god.



STEVE:  I know.  Anyway, he says:  "And the operating system pushes you to switch to Edge with some regularity.  And once in Edge, the browser pushes you to use Bing and other Microsoft services."  So he finishes:  "Microsoft may not ship the crypto wallet to Edge users.  The company regularly tests features in Edge, Windows, and its other software that never end up making it into the general-release versions.  We've contacted Microsoft for more information and will update if we receive a response."



So, you know, I don't know.  Not long ago I tried to use Bing.  I had become annoyed with Chrome because I noticed that every time I opened it the fan on my little Intel NUC would spin up to dissipate the heat that Chrome was, for some reason, causing the whole system to produce.  And this was with no tabs loaded, just Chrome itself.  And suddenly, whirr, you know, spin up the propellers, we've got to cool this puppy off.  You know?  So Chrome had become bloatware.



And I didn't know whether Bing might be any better, but I did need left-side tabs, so Bing's built-in support of that feature drew me in.  I figured that being first and foremost a Chromium-based browser, I'd at least get good compatibility.  But then I found as I was using it that some web pages would not open or display in Bing.  So back to Firefox I went, where I am once again completely happy.  So if Microsoft decides to embed a cryptocurrency wallet in Bing, you know, I do hope it works better than their email solutions have, which we will be talking about next.



LEO:  Oh, boy.



STEVE:  So that we don't break my rant in half...



LEO:  Yes, we'll do an ad, yeah.



STEVE:  Let's tell about why we're here, and then, oh, boy.



LEO:  The rant is on the way, baby.



STEVE:  At least it's not a faux rant.  It's real.



LEO:  We had a caller on Ask the Tech Guy on Sunday whose computer, without his approval, upgraded to Windows 11 and turned on some sort of weird security mode that he could only install stuff from the store, so he couldn't install Chrome or Firefox.  It just drives me nuts.  It just drives me nuts.



STEVE:  Well, we will shortly see another example of them throwing their weight around.



LEO:  Yeah, yeah.



STEVE:  And it almost makes sense that this is where they would have gone; right?  When you're that big, and you need to keep your shareholders happy, you just take advantage of the fact that your users no longer have a choice.



LEO:  Cory Doctorow calls it "eating your seed corn."  You know, at some point in the company, you know, you just start devouring everything for profit.  All right.  Put your rant helmet on.  Your goggles.



STEVE:  So I should start out by noting that it's been quite some time since the listeners of this podcast have heard me really get upset about anything.  It doesn't happy very often.  And I don't recall the last time it happened, but it's happened before.  When I dwell on this one, I'm pretty sure my blood pressure rises because I have a real problem with injustice and bullying.  Someone at Microsoft has had a very bad idea.



When I first encountered this yesterday, I did a double-take.  Really.  I thought that I must have misunderstood what Microsoft meant.  But unfortunately, no.  Microsoft has formally announced that they are going to begin blocking incoming email to their Exchange Online cloud instances, which includes all of Office 365 and Outlook.com, if that incoming email originates from other private so-called on-premises Exchange servers which, while they may be functioning just fine, are nevertheless past their end-of-support life.  Like, wow, what?



That's right.  They're saying that they are going to begin blocking incoming email from older version instances of their own Exchange Server software.  No one who purchased Exchange Server 2007, 2010, or 2013 was told at the time  in fact they've not been told until now  that in the future, the software they purchased, paid for, and have continued to happily use, would become less useful to them because Microsoft's various online services were going to unilaterally begin refusing to accept email from those otherwise perfectly functioning servers.



And, yes, I use the term "perfectly functioning" in the context of Exchange Server with a bit of tongue in cheek because, after all, it is Exchange Server.  But many hundreds of thousands of instances of it are still functioning, and everyone else in the world will be able to receive the email they send except for Microsoft's services because Microsoft has apparently decided to punish their previous customers and extort them for additional licensing revenue.



Now, interestingly, the headline on Microsoft's own announcement doesn't quite 'fess up to this fully.  It reads "Throttling and Blocking Email from Persistently Vulnerable Exchange Servers to Exchange Online."  But they define the term "persistently vulnerable" as meaning "servers that are unsupported or remain unpatched."  And just wait until you hear their rationale for doing this.  My breath is still a bit taken away, but I want to repeat my summation so that you don't need to hit rewind or replay to be sure you heard it right.



Microsoft is essentially saying that they are going to use their market dominance in online email services to force their previous software customers to upgrade their own instances of Exchange Server by refusing to accept email sent by such servers as a means of extorting additional licensing fees from those prior customers.  Microsoft is going to effectively begin reducing the functionality of their previous Exchange servers by refusing to accept their email unless and until the licenses for those Exchange servers are renewed, and their software is updated.



I can't think of a precedent for this in our industry.  So I suppose that means that this is an unprecedented action.  Okay.  So here's how Microsoft couched this extortion.  They wrote, this is them:  "As we continue to enhance the security of our cloud, we are going to address the problem of email sent to Exchange Online from unsupported and unpatched" - yes, Leo, you're laughing, I know.  "The problem."



LEO:  We are not making enough money from legacy customers.



STEVE:  Yeah, we realize there are some people that we haven't squeezed yet enough.  So anyway, they said:  "We're going to address the problem of email sent to Exchange Online from unsupported and unpatched Exchange servers.  There are many risks associated with running unsupported or unpatched software, but by far the biggest risk is security."  Especially with Exchange.  "Once a version of Exchange Server is no longer supported, it no longer receives security updates; thus, any vulnerabilities discovered after support has ended don't get fixed."  Oh, the horror.  "There are similar risks associated with running software that is not patched for known vulnerabilities.  Once a security update is released, malicious actors will reverse engineer the update to get a better understanding of how to exploit the vulnerability on unpatched servers."



Okay, yeah.  As we know, all of that's true.  But next comes the pivotal paragraph, the fundamental flaw in the logic upon which this entire extortion effort rests.  Microsoft continues:  "Microsoft uses the Zero Trust security model for its cloud services, which requires connecting devices and servers to be provably healthy and managed.  Servers that are unsupported or remain unpatched are persistently vulnerable and cannot be trusted, and therefore email messages sent from them cannot be trusted."  They said:  "Persistently vulnerable servers significantly increase the risk of security breaches, malware, hacking, data exfiltration, and other attacks."  Okay.  So repeating...



LEO:  To put this in perspective, it's as if your modern iPhone would no longer accept text messages from iPhones that were out of date.



STEVE:  Yes.



LEO:  Like, oh, you can't send text messages.  You have an iPhone 6.  Sorry.



STEVE:  Right.  We can't trust that anymore.  We're not supporting that.



LEO:  In their defense, is that, I mean, it could be hacked; right?  Because they haven't patched it.  And Exchange servers are notoriously bad.



STEVE:  Oh, yes.  In fact, I have an update on that, too.  Okay. But here's the problem.  They said:  "Servers that are unsupported or remain unpatched are persistently vulnerable and cannot be trusted.  Therefore, exactly as your example is, the email messages sent from them cannot be trusted."



LEO:  And those could be validated.  I mean, they should just be text; right?  I mean, they're not...



STEVE:  Okay.  I have in the show notes here, I wrote:  "This is what's commonly known as a load of crap."



LEO:  Oh, yes, that.



STEVE:  That, yes.  And I had two phrases.  That was the more polite of the two.  It's important that we pause here for a minute because, as I said, it is upon this fundamental logical fallacy that Microsoft is hanging their entire campaign.  We know that the truth is that, yes, through the years Microsoft's Exchange Server has had a particularly difficult relationship with security.  In short, it's pretty much been an utter disaster.  And not just for a while.  It's inexplicable that they've had this problem, especially when email is a trivial protocol.  I mean, there's hardly anything simpler than email.  But Microsoft has managed to make it deadly.



On January 3rd of this year, Bleeping Computer's headline read:  "Over 60,000 Exchange servers vulnerable to ProxyNotShell."  And few months earlier, on October 27th, Wired's headline, and I'm not making this up, was "Your Microsoft Exchange Server Is a Security Liability" was their headline, with the subhead "Endless vulnerabilities.  Widespread hacking campaigns.  Slow and technically tough patching.  It's time to say goodbye to on-premise Exchange."  As an aside, I'll remind everyone of just how conscientious Microsoft has been about Exchange's security.



In that article, describing the constant struggle over Exchange vulnerabilities, Wired wrote, they said:  "The latest reminder of that struggle arrived earlier this week, when Taiwanese security researcher Orange Tsai published a blog post laying out the details of a security vulnerability in Microsoft Exchange.  Tsai warned Microsoft about this vulnerability as early as June of 2021.  And while the company responded by releasing some partial fixes, it took Microsoft 14 months to fully resolve the underlying security problem.



"Tsai had earlier reported a related vulnerability in Exchange that was massively exploited by a group of Chinese state-sponsored hackers known as Hafnium, which last year penetrated more than 30,000 targets by some counts.  Yet according to the timeline described in Tsai's post last week, Microsoft repeatedly delayed fixing the newer variation of that same vulnerability, assuring Tsai no fewer than four times that it would patch that bug before pushing off the full patch for months longer.  When Microsoft finally released a fix, Tsai wrote, it still required manual activation and lacked any documentation for four more months.



"Meanwhile, another pair of actively exploited vulnerabilities in Exchange that were revealed last month still remain unpatched after researchers showed that Microsoft's initial attempts to fix the flaws failed.  Those vulnerabilities were just the latest in a years-long pattern of security bugs in Exchange's code.  And even when Microsoft does release Exchange patches, they're often not widely implemented, due to the time-consuming technical process of installing them."



Okay.  So let's first just be really clear here.  Any security problems that Exchange Server has are directly Microsoft's fault.  Right?  So now they're saying that they're afraid of receiving any email that previous versions of their software might send to their current versions.  It's true that the poor users of those previous versions are likely risking life and limb to continue running older and out-of-date versions of Exchange.  But that's their choice.  Or it used to be.  And it's they who are being put in danger by running Microsoft's perennially insecure offering, not people who receive the email it sends.  That's utter nonsense.  But it's the utter nonsense upon which, as I noted, this entire campaign rests.



So here's how Microsoft continues.  They said:  "We've said many times that it is critical for customers to protect their Exchange servers by staying current with updates and by taking other actions to further strengthen the security of their environment.  Many customers have taken action to protect their environment, but there are still many Exchange servers that are out of support or significantly behind on updates."



Okay.  So Microsoft has coined a new term of art for this which will be entering our industry's lexicon.  They are calling what they will be employing their "Transport-based Enforcement System."  They said:  "To address this problem" - which of course they've invented - "we are enabling a transport-based enforcement system in Exchange Online that has three primary functions:  reporting, throttling, and blocking.  The system is designed to alert an admin about unsupported or unpatched Exchange servers in their on-premises environment that need remediation, upgrading or patching.  The system also has throttling and blocking capabilities so, if a server is not remediated, mail flow from that server will be throttled - delayed - and eventually blocked."



LEO:  Wow.



STEVE:  "We don't want to delay or block legitimate email, but we do want to reduce the risk of malicious email entering Exchange Online by putting in place safeguards and standards for email entering our cloud service."  And then they said:  "We also want to get the attention of customers who have unsupported or unpatched Exchange servers and encourage them to secure their on-premises environments."



That's right.  "Encouragement" is what we call it here at Microsoft when your email server's outbound mail starts being refused, not due to any misbehavior on its part, which would be your problem in any event, but only because you have chosen to continue using an older version of Exchange Server - which you paid for - whose email output we have since decided we no longer want to accept.  Oh.  But if you purchase a new license, then all will be forgiven, and we'll happily receive your email.



They also explain about the actions they'll be taking about, you know, progressive amounts of delaying and how they'll send back an SMTP 450 message delayed message and then ending in an SMTP 550 error to the sender.



So after all this, the announcement then contains an FAQ to clarify, in Q&A format, what this all means.  The answers to a few of the questions they ask themselves are a bit chilling.  So they ask themselves:  What is a persistently vulnerable Exchange server?  Answer:  Any Exchange server that has reached end of life - for example, Exchange 2007, Exchange 2010, and soon, actually next month, Exchange 2013 - or remains unpatched for known vulnerabilities.  For example, Exchange 2016 and Exchange 2019 servers that are significantly behind on security updates are considered persistently vulnerable.



Is Microsoft blocking email from on-premises Exchange servers to get customers to move to the cloud?  No.  Our goal is to help customers secure their environment, wherever they choose to run Exchange.  The enforcement system is designed to alert admins about security risks in their environment - because their email can't get delivered anymore - and to protect Exchange Online recipients from potentially malicious messages sent from persistently vulnerable Exchange servers.  Because you know how bad the email is that's sent from those old Exchange servers.



Okay.  Why is Microsoft only taking this action against its own customers, customers who have paid for Exchange Server and Windows Server licenses?  Whoa.  We are always looking for ways to improve the security of our cloud and to help our on-premises customers stay protected.  This effort helps protect our on-premises customers by alerting them to potential significant security risks in their environment.  You know, created by our software.  We are initially focusing on email servers - oh, initially focusing?  We are initially focusing on email servers we can readily identify as being persistently vulnerable, but we will block all potentially malicious mail flow that we can.



Will Microsoft enable the transport-based enforcement system for other servers and applications that send email to Exchange Online?  We are always looking for ways to improve the security of our cloud and to help our on-premises customers stay protected.  We are initially focusing on email servers we can readily identify as being persistently vulnerable, but we will block all potentially malicious mail flow that we can.



If my Exchange Server build is current, but the underlying Windows operating system is out of date, will my server be affected by the enforcement system?  No.  The enforcement system looks only at Exchange Server version information. But it is just as important to keep Windows and all other applications up to date, and we recommend customers do that.  However, we haven't figured out how to make you do it yet.



Delaying and possibly blocking emails sent to Exchange Online seems harsh and could negatively affect my business.  Can't Microsoft take a different approach to this?  Microsoft is taking this action because of the urgent - that's right, we just made it up, but now it's urgent - because of the urgent and increasing security risks to customers that choose to run unsupported or unpatched software.  Over the last few years, we have seen a significant increase in the frequency of attacks against Exchange servers.  You bet.



We have done, and will continue to do, everything we can to protect Exchange servers; but unfortunately there are a significant number of organizations that don't install updates or are far behind on updates, and are therefore putting themselves, their data, as well as the organizations that receive email from them, at risk.  We can't reach out directly to admins that run vulnerable Exchange servers, so we are using activity from their servers to try to get their attention.  Our goal is to raise the security profile of the Exchange ecosystem.



Why are you starting only with Exchange 2007 servers, when Exchange 2010 is also beyond end of life, and Exchange 2013 will be beyond end of life when the enforcement system is enabled?  Starting with this narrow scope of Exchange Server lets us safely exercise, test, and tune the enforcement system before we expand its use to a broader set of servers.  Additionally, as Exchange 2007 is the most out-of-date hybrid version, it doesn't include many of the core security features and enhancements in later - oh, so it's less bad, even though it's way older.  Oh, no, it's more bad, more bad.  The newer ones are less bad.  Restricting the most potentially vulnerable - oh, it's the most potentially vulnerable - and unsafe server version first makes sense.  That's right.



Does this mean that my Exchange Online organization might not receive email sent by a third-party company that runs an old or unpatched version of Exchange Server?  Possibly.  The transport-based enforcement system initially applies only to email sent from Exchange 2007 servers to Exchange Online over an inbound connector type of on-premises.  The system does not yet apply to email sent to your organization by companies that do not use an on-premises type connector.  Our goals are to reduce the risk of malicious email entering Exchange Online by putting in place safeguards and standards for email entering the service and to notify on-premises admins that the Exchange server their organization uses needs remediating.



In other words, the answer is yes to this question.  If you're an organization using Exchange Online, you will stop getting email from these scofflaws who have decided to continue using the software they purchased a while ago.



How does Microsoft know what Exchange version I'm running?  Does Microsoft have access to my servers?  No, Microsoft does not have any access to your on-premises servers.  The enforcement system is based on email activity, for example, when the on-premises Exchange Server connects to Exchange Online to deliver email.  Anyway, it's in the headers, the version of Exchange and the service pack and update level and so forth.



And in this posting they have some additional back-and-forth between some people who feel pretty much as I do and a Microsoft guy.  But everyone's got the sense for this.



So the world is apparently full of past-end-of-life and non-updated Exchange Server instances that are day-in and day-out working perfectly well for their users who once purchased the software from Microsoft and who have felt for whatever reason no need to purchase new licenses.  The server's online and working for them the way they want it to, without problems.  And for whatever reason they have chosen not to upgrade their software.  Is it not their choice and their right to run the software that they have purchased as long as they wish and as long as it is serving their needs?  That software was not a subscription.  It did not come with an expiration date.  Its license's clear assertion was that they would be able to use it for as long as they wished.



The idea that email originating from out-of-date and unsupported email servers is itself inherently dangerous is utter nonsense.  There's no awareness out in the world that email from certain versions of this or that email software is known to be dangerous.  We would know that on this podcast if that was the case.  Microsoft just invented that out of thin air to suit their commercial purpose.  To the extent that there is any danger, it's to the organization running such software, not to those receiving its email output.



This is the shaky premise upon which Microsoft's policy rests, and it doesn't hold up.  But that doesn't matter because Microsoft is able to claim it, and will soon begin enforcing it.  Those perfectly good servers have been tirelessly working, apparently without problems, for many years.  But now that's about to change, not because they suddenly represent any actual danger, but because Microsoft figured out how they can extort some new revenue from their old customers.



Because most organizations will take the path of least resistance, which will be to upgrade, Microsoft surely knows that the net effect of this will be to generate additional revenue for itself.  Those wayward users have fallen off the Microsoft gravy train, and it's high time that they be brought back into the fold.  What better way than to create a new inability for those old servers to send mail to Office 365 and Outlook.com users whom Microsoft already utterly controls?  It's diabolically brilliant.  And so that it doesn't appear to be the bald-faced revenue extortion scheme that it is, couch the whole thing in their need to protect their paying customers from those evil email messages being generated by their own presumably now highly dangerous software.



And finally, it's worth noting that this is not a one-time event.  No one using any Microsoft Exchange Server software will ever again be able to fail to keep it updated, nor to avoid the purchase of future licenses, forever.  Now, I celebrate that idea moving forward, since keeping software updated, especially Exchange Server, is a good thing, and since new licensors will be aware that they will never again have any choice other than to be paying Microsoft forever under whatever terms and conditions Microsoft may choose.



The problem I have is with Microsoft's effectively unilateral revocation of the open-ended licenses they previously sold.  That's a clear violation of trust, and that's never going to be okay, even though it's going to happen.



LEO:  Yeah, I mean, I'll have to ask Richard Campbell about this.  He is an Exchange Server admin, is abandoning Exchange Server for his home system.  Anybody who has been using Exchange Server has been tortured long enough.



STEVE:  Yeah.



LEO:  But this seems like a really unseemly way - because you're basically breaking email.



STEVE:  Correct.  You are.  You are saying, here is a valid piece of email with a letter from your mom that has no malware on her system.  But because it was sent to an Office 365 or Outlook.com user through an old version of Exchange Server, they're not going to accept the email.



LEO:  That's not okay.



STEVE:  They're just going to say, oh, no, we're not going to accept the email.  I mean, Leo, it is so wrong.



LEO:  Yeah.



STEVE:  Talk to you in four weeks.



LEO:  All right.  Thanks, Steve.



STEVE:  Okay, buddy.



Copyright (c) 2023 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.






GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#917

DATE:		April 4, 2023

TITLE:		Zombie Software

HOSTS:	Steve Gibson & Ant Pruitt

SOURCE:	https://media.grc.com/sn/sn-917.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we answer questions which arose during the past week:  When is an attack not an attack?  When our AI overlord arrives, how shall we call him?  Why has Italy said NO to ChatGPT?  What does Twitter's posting of its code to GitHub tell us?  Why is India searching for commercial spyware less well-known than Pegasus, and what does the Summit for Democracy have to say about that?  Has the FDA finally moved on the issue of medical device security updates?  And seven years after the first "Hack the Pentagon" trial, the Pentagon remains standing; or does it?  Then, after addressing a quick bit of miscellany, listener feedback, and an update on my ongoing work on SpinRite, we use CISA's KEV database to explore the question of how exactly we define "Zombie Software," and answer the question of whose brains will the zombies eat?



SHOW TEASE:  This week on Security Now!, Mr. Steve Gibson joins me to share some great information about the wonderful world of cybersecurity.  And I've got to tell you, it was a little bit scary at first; but it got better, huh.  So what's going on with AI?  Are we going to name our AI overlords once they arrive?  Italy decides to say no, there's not going to be any more ChatGPT.  We're shutting that down.  Oh, boy.  What does that mean?  The Pentagon said, "Hey, hack us."  And you know what?  It worked.  It was not a bad idea.  So more details on that and doing bug bounties.  And lastly, Zombie Software.  Oh, this is such a great story from Mr. Gibson.  You don't want to miss it.  Got to stay tuned.



ANT PRUITT:  This is Security Now!, Episode 917, recorded Tuesday, April 4th, 2023, hosted by Ant Pruitt and Steve Gibson:  Zombie Software.  Hey, what's going on, everybody?  Hey, this is Security Now! here on TWiT.tv.



STEVE GIBSON:  Wait a minute.  You're not Leo.		



ANT:  Yeah, yeah, yeah.  I'm not Leo Laporte.  I am Ant Pruitt, y'all.  I'm going to be hosting today with the one-and-only Mr. Steve Gibson.  Steve, how are you, sir?



STEVE:  Hey, Ant.  Great to be with you for the first of our three weeks while Leo is off on some trip.  And we're going to hold the fort down here and just keep the ball rolling.  So this is...



ANT:  We shall do our best, sir.



STEVE:  Yeah.  Security Now! #917 for the 4th of April.  And as we've been doing recently, we're going to answer a bunch of questions which have arisen over the past week.  When is an attack not an attack?  When our AI overlord arrives, how shall we call him?  Why has Italy said no to ChatGPT?  What does Twitter's posting of its code to GitHub tell us?  Why is India searching for commercial spyware less well-known than Pegasus?  And what does the Summit for Democracy have to say about that?  Has the FDA finally moved on the issue of medical device security updates?  And seven years after the first Hack the Pentagon trial, the Pentagon remains standing.  Or does it?



ANT:  Oh, boy.



STEVE:  Then, after addressing a quick bit of miscellany, some listener feedback, and an update on my ongoing work on SpinRite, we use the CISA's KEV, the Known Exploited Vulnerabilities database, to explore the question of how exactly we define "zombie software," and answer the question of whose brains will the zombies eat.



ANT:  Oh, boy.  Oh, boy.



STEVE:  Those questions and more answered this week.



ANT:  I've got to go ahead and ask, Mr. Gibson.  You're saying what happens when our AI overlords arrive, what are we going to call them?



STEVE:  That's right.



ANT:  I thought they were already here.



STEVE:  Well, there's some concern about that.  And in fact unfortunately Washington has been caught flat-footed and are trying to say, wait a minute, uh, we don't understand what any of this stuff is, so everybody should stop.  Anyway, we'll be talking about that today.



ANT:  Oh, joy.  Oh, joy.  Well, folks, this is going to be a fun episode.  Mr. Steve, the Picture of the Week.



STEVE:  Our Picture of the Week.  I got a kick out of this.  Actually, we have a couple things that are sort of about this topic in general.  So we have a single-frame cartoon.  And a woman is staring at her computer screen, talking to someone standing behind her.  And the balloon above her head says:  "Congress finally has a better understanding of how TikTok works.  Almost half of them now know it's not a breath mint."  So, yes, not Tic Tac, folks.  We didn't say Tic Tac, we said TikTok.  Oh.  Oh, not - okay, yeah.  So that's right.  We're going to give the politicians control of technology and see how well that works.



ANT:  Does a younger person know what a Tic Tac is, or a TikTok is?



STEVE:  Yeah, Tic Tacs are still around.  Yeah, probably.



ANT:  They still are?  Okay.  All right.



STEVE:  Yeah.  I think in general where we have old fogies doing podcasts, there is a danger of a generation gap in some of the terminology.  Sometimes when I say somebody was asleep at the switch, I think, does anyone know what that means?  I mean, old people know that we're talking about train tracks.  But I'm not sure that - what?  What?  Asleep at the switch?  What switch?  What are you talking about?



ANT:  What switch?  Oh, goodness.



STEVE:  Okay.  So recall that two years ago all of the fuss, bother, and more than a little bit of horror arose over the attack at a Florida water treatment plant in Oldsmar, Florida.  Now, the report that the world received was that a caustic and potentially poisonous concentration of lye had been dumped into the facility's municipal water treatment plants, remotely over the Internet, that is, the control of the water treatment plant's equipment had been done remotely.  And that was conducted, we were told, by a disgruntled former employee whose remote access credentials had not been canceled and deleted after his departure.



And the story had so much flair and detail.  Recall, because we talked about this a lot then, that the reports of the incident described how a worker at the plant saw his computer being remotely accessed and controlled, like right in front of him.  His mouse, we were told, moved to open functions to control the plant's water treatment protocols, and then the amount of sodium hydroxide, which is the chemical formula for lye, in the water was changed from 100 parts per million, which was what it was supposed to be, to 11,100 parts per million.  Whereupon the operator, who saw this horror, immediately reduced lye's level back to its proper level and alerted his supervisor.



And everyone got in on the act.  The hack immediately gained worldwide notoriety after the local Pinellas County Sheriff held a press conference which, in turn, prompted an investigation led by the FBI and the U.S. Secret Service, as well as a joint federal advisory warning water treatment facility operators throughout the country of the dangers they faced from hackers, and urging them to upgrade their security systems.



Okay, now, gristy as that story was for our mill, what if that's not what happened at all?  That's right.  According to former Oldsmar City Manager Al Braithwaite, who was with the city at the time, the incident was never a hack or an attack at all.  It was just a case of an employee, still with the water treatment facility, mistakenly clicking the wrong button.



ANT:  Wait, what?



STEVE:  I know.  I know.  Then alerting his superiors to his error.  Now the former City Manager Braithwaite describes the incident as a total "non-event," which was resolved in two minutes.  But he said law enforcement and the media for some reason seized on the idea of this being a cyberattack and just "ran with it," he said.  The attention resulted in a four-month FBI investigation, which Braithwaite said reached the same conclusion, that employee error alone was to blame.  And somehow we never heard the result of that after four months.  That sort of slipped through the cracks.



ANT:  Wow.



STEVE:  Now, okay, yeah, wow.  I mean, this was a big deal.  Everyone was, like, you know, alarms were ringing, and water treatment plants elsewhere were being put on alert.  And actually there might have been some upside to all of this because all municipal water treatment plants likely did and probably do still need to be more alert and aware of threats to keep their security tight because the thing that was so galvanizing about this, it was like, whoa, you know, poisoning the municipal water supply, that's a big deal.



So if the report of this dastardly attack-which-wasn't happened to serve to get some inattentive management at other treatment plants in the nation to tighten up their grip, change employee discharge policies, perhaps delete a bunch of old and unused access credentials and so forth, then this bizarre inflation of this minor event probably helped security overall in the long term.  So it's not the way any of us want to be used for having security improved, but it's better to know I think ultimately what happened than not.



ANT:  Yeah, somebody had to really take a sacrifice, if you will, to help everybody else raise awareness.  And this is good here.  My only thought on that, though, Mr. Gibson, when that happened, I want to say I heard it was like through RDP or something like that.



STEVE:  Yeah, apparently it was Team Viewer.  And they had...



ANT:  Team Viewer, that's what it was, Team Viewer.  And so when you're using Team Viewer, if someone pops onto your screen and moves the mouse around, can't you move your mouse back to try to take control and kill the session?  I'm like, what's going on here?  So that story just kind of...



STEVE:  So your point is that it never really made that much sense to you; right?



ANT:  Right.  Right.



STEVE:  Right.  Right.



ANT:  Geez.



STEVE:  Okay.  So it's not going to be news to anyone that the sudden explosive popularity, adoption, and use of Large Language Neural Network Models such as ChatGPT - and I know, Ant, you've been following along a lot of this stuff - has caught pretty much everyone flat-footed.  Everyone's running around trying to figure out what it means, whether it's some sort of apocalypse, and like the subject that the public should be protected from.



So a couple of weeks ago, the U.S. Chamber of Commerce issued a report from the "AI Commission."  Apparently we have an AI Commission now.  The report claims to highlight the promise of AI - you almost want to put "promise" in air quotes because they're, like, they're not so sure - while calling for a "Risk-Based Regulatory Framework."  And the subhead of the report says "Report Finds Policymakers Must Enforce Existing Laws," okay, whatever they would be, and "Develop Policies to Steer the Growth of Responsible, Ethical AI."  Okay.  Ethical AI?  We've got a lot of undefined terms here.  So what's actually happening is kind of clear to see.  The politicians and the bureaucrats have no idea what any of this is.  I mean, most of us don't; right?



ANT:  Right.



STEVE:  I mean, it's kind of like a "wait and see what happens next" because this is so new.



ANT:  Right.



STEVE:  But not knowing scares the crap out of them.  And these people, you could say, confusing TikTok and Tic Tac, they're not the brightest bulbs, as our Picture of the Week explained.  So these people are apparently the ones who have appointed themselves to shepherd us into the land of ethical AI.  Again, as opposed to unethical AI?  I don't know.



ANT:  Right, right.



STEVE:  Okay.  So here's what the U.S. Chamber of Commerce is thinking.  They wrote:  "The use of artificial intelligence is expanding rapidly."  They like to start off with the obvious.  "These technological breakthroughs present both opportunity and potential peril."  Oh.  "AI technology offers great hope for increasing economic opportunity, boosting incomes, speeding life science research at reduced costs, and simplifying the lives of consumers.



"With so much potential for innovation, organizations investing in AI-oriented practices are already ramping up initiatives that boost productivity to remain competitive.  Like most disruptive technologies, these investments can both create and displace jobs."  Uh-oh.  "If appropriate and reasonable protections are not put in place, AI could adversely affect privacy and personal liberties, or promote bias.  Policymakers must debate and resolve the questions arising from these opportunities and concerns to ensure that AI is used responsibly and ethically.



"This debate must answer several core questions:  What is the government's role in promoting the kinds of innovation that allow for learning and adaptation while leveraging core strengths of the American economy in innovation and product development?  How might policymakers balance competing interests associated with AI - those of economic, societal, and quality-of-life improvements - against privacy concerns, workforce disruption" - there that is again - "and built-in biases associated with algorithmic decision-making?  And how can Washington establish a policy and regulatory environment that will help ensure continued U.S. global AI leadership while navigating its own course between increasing regulations from Europe and competition from China's broad-based adoption of AI?"



ANT:  Oh, boy.



STEVE:  And of course that's a concern.



ANT:  There's a buzzword.



STEVE:  Uh-huh.  You hog-tie U.S. innovators with needless fuzzy and ill-conceived regulations, and the rest of the planet is going to leapfrog the U.S. with their high-speed plans for unethical AI, apparently.



So they said:  "The United States faces stiff competition from China in AI development.  This competition is so fierce that it is unclear which nation will emerge as the global leader, raising significant security concerns for the United States and its allies."  Oh.  We might not win this.  "Another critical factor," they said, "that will affect the path forward in the development of AI policymaking is how nations historically consider important values, such as personal liberty, free speech, and privacy.



"To maintain its competitive advantage, the United States and like-minded jurisdictions such as the European Union" - you know, we like them - "need to reach agreement to resolve key legal challenges that currently impede industry growth.  At this time it is unclear if these important allies will collaborate on establishing a common set of rules to address these legal issues, or if a more competitive and potentially damaging legal environment will emerge internationally."  Well, that's interesting.  So, like, are we going to have different rules and regulations and different definitions of ethical AI?  We don't know.



ANT:  Right.  Who knows?



STEVE:  Nobody knows any of this.  "AI has the capacity to transform our economy, how individuals live and work, and how nations interact with each other.  Managing the potential negative impacts of this transition should be at the center of global public policy.  There is a growing sense that we have a short window of opportunity" - some people would say narrow, but okay - "to address key risks while maximizing the enormous potential benefits of AI," which we don't understand at all.  No, they didn't say that at the end.  Benefits of AI.



Wow.  So, you know, I doubt that anyone knows what any of that really means; right?  But, like, okay, somebody must have said  you need to publish a position paper on, uh, something.



ANT:  We need to make a statement.  Whatever that statement is, we need to make a statement.



STEVE:  Right.



ANT:  That's what they did; you know?



STEVE:  Yeah.  So it's clear it's coming on like a speeding train.  And it's unclear to me that there's any way to slow it down.  Obviously their concern is, if they impede the innovation in the U.S., then countries that don't impede their researchers, and we know who they are, they're going to get ahead of us and take over, and we're going to all be working for them.  So that's not good.  Anyway...



ANT:  And again, to their credit, like most innovative technology, things can be great for mankind.  But then in the hands of the wrong actor it could cause a lot of problems.  I'm glad they're not that naive.



STEVE:  Oh, my god, it's what this podcast is all about; you know?  Technology which is inherently neutral, we get a lot of leverage and benefit from it.  And, boy, is there an awful lot of bad guys who are spending so much industry on just, you know, trying to subvert it.



ANT:  Unfortunately.



STEVE:  So I should mention that at least one country, bizarrely, has chosen to adopt a much more, like an immediate kneejerk position on AI.  Believe it or not, Italy has banned ChatGPT.  Okay, I'm not kidding.  The news is that the Italian Data Protection Authority has issued a temporary ban on ChatGPT as the agency investigates a possible breach of its GDPR regulations, with the agency accusing the OpenAI service of "unlawful collection of data."



Okay, now, and that appears to be the point of this.  Someone has apparently freaked out Italian legislators because they believe that "The ChatGPT" is sucking up all of the Internet's data on their Italian citizens without any regard for their GDPR-guaranteed privacy.



ANT:  Hmm.



STEVE:  Now, yeah, it's odd.  The only thing I could find to bring some clarity to this was a press release originally written in Italian and poorly translated into English.  Now, of course, the translation would be great if they had just let ChatGPT do it.  But noooooo.  So here's what was announced, from which you can get a sense for their somewhat hysterical concern.  And again, pardon the translation, but this was a dumber bot did the translation.



So it reads:  "No way for ChatGPT to continue processing data in breach of privacy laws.  The Italian SA imposed an immediate temporary limitation on the processing of Italian users' data by OpenAI, the U.S.-based company developing and managing the platform.  An inquiry into the facts of the case was initiated, as well.



"A data breach affecting ChatGPT users' conversations and information on payments by subscribers to the service had been reported on March 20th."  Okay, now, that's like neither here nor there.  That's got nothing to do with this, right, and AI.  But okay.



Then they said:  "ChatGPT is the best known among relational AI platforms that are capable to emulate and elaborate human conversations.  In its order, the Italian SA highlights that no information is provided to users and data subjects whose data are collected by OpenAI; more importantly, there appears to be no legal basis underpinning the massive collection and processing of personal data in order to 'train' the algorithms on which the platform relies.  As confirmed by the tests carried out so far, the information made available by ChatGPT does not always match factual circumstances, so that inaccurate personal data are processed.



"Finally, the Italian SA emphasizes in its order that the lack of whatever age verification mechanism exposes children to receiving responses that are absolutely inappropriate to their age and awareness, even though the service is allegedly addressed to users aged above 13, according to OpenAI's terms of service."



Okay.  Whew.  So this is all a bit weird, right?  So far as anyone knows, no private data has been or is being harvested by ChatGPT.  The system is simply ingesting vast quantities of publicly available information and merging it into an evolving large language model which has an interactive conversational interface.  In a very similar fashion, Google's "Googlebots" crawl the web, following links and indexing everything they encounter.  So there's really no difference between what Google collects and what ChatGPT collects.



ANT:  Exactly.



STEVE:  The difference, yeah, is in the accessibility and presentation of the information.  Google maintains a massive index, whereas ChatGPT and similar systems maintain a massive model.  Anyway, I imagine that things will likely calm down in Italy once someone who knows something actually talks to those who need to know more than they apparently do at the moment.  One thing seems clear, though, which is that Italy would not want ChatGPT to be ignorant of everything that country has to offer.  It would represent a huge blind spot that would ultimately hurt them economically as the world's inevitable adoption of and reliance upon this emerging technology continues.  If someone asks their favorite AI bot for recommendations about where to stay in Rome, you don't want the world's AIs asking, "What's a Rome?"



ANT:  And totally, totally screwing up that economy there from all of those visitors.



STEVE:  That's right.  Where did all the tourists go?  Well, you told ChatGPT to forget about us, and it did.



ANT:  We spoke about another AI offered by Adobe here on the network last week.  And the key difference between what Adobe's doing and the other AIs out there is Adobe is scraping data based on information submitted to them directly from their subscribers to Adobe so they could figure out text phrases, text strings or what have you to apply it to art.  I wonder if that would be a problem in Italy or somewhere else over there in Europe with concerns of GDPR.  Even though there's a consent by the users of Adobe products to say, you know what, whatever you create in our software, we're going to use it to train our models.  But you can't opt out of it.  So I wonder if there's any connection there between what's going on in Europe there versus here.



STEVE:  Also this raises an interesting question, or you have, which is I wonder if the users' input into ChatGPT influences  the model.  I mean, we know that it influences in the short time; right?  And after a while it goes insane, and you have to restart it, you know, because it loses its mind, or it starts hallucinating.



ANT:  Right.  They say it hallucinates or something.  That's what they're saying now.



STEVE:  Yes, exactly.  And so I wonder like if children were talking to it and giving it information which is not on the public Internet, if the information they voluntarily disclose ends up being captured in the long term.  I don't know one way or the other.  But anyway, it just seems like everyone's having a different reaction to the craziness of this.



But, you know, the reason I was put in mind of this, if you ask the AI where's a good place to stay in Rome, is I was watching the live stream sometime in the last week, I don't remember which podcast it was on, but Leo was talking about ChatGPT-4 and demonstrating it, and he literally, he said, "We're going to be" - I think it was in Rome.  "We're going to be in Italy.  What would you recommend as an itinerary?"  Well, it spit out, like, half a page.  And Leo said, "Wow, I've got to copy that.  That looks really good."  



ANT:  Right.



STEVE:  And so, like, literally, it had some good ideas that he hadn't considered.  And in fact it estimated how long the various things would take.



ANT:  That's awesome.



STEVE:  So that it shows you, like, where during the day you would be.  And Leo said, "Oh, I've got to make a copy of this."  So the world is clearly changing.



ANT:  That would be awesome to have that, though, sort of a predetermined itinerary.  And I'm sure all of the merchants there in the locale would appreciate your patronage, too.



STEVE:  That's right.  So last Friday, Twitter officially posted the source code that it uses for selecting which tweets users will see.  This of course has been the source of much controversy and handwringing and, you know, social media postings everywhere but Twitter and on Twitter, since we've learned that the tweets people see turns out to have an outsized effect upon the beliefs they hold.  So for anyone who may be interested, it's now there on GitHub.  Since I'm not a Twitter surfer, I'm not very curious.



But I'm sure we'll see some analysis of the algorithm once those who do care have invested the time in learning what it all does.  Ars Technica shared their take after a very quick look, and from what they saw it mostly looked like what any student of social science would predict.  Watch what each person does, what they click, what they search for, how long they remain, how far down they scroll, and so forth.  Collect every possible meaningful scrap of data from their interaction, figure out what sorts of things they want to see, then show them more of that.



ANT:  Well, that seems logical; right?



STEVE:  That's basically the algorithm; right.  And of course, you know, we've talked a long time ago that the so-called "filter bubble" effect, right, is where if you show people things that they've seen, that they've already demonstrated they want to see, you end up amplifying their positions and hardening their positions and essentially radicalizing them to a position, as opposed to just showing them everything going on and exposing them to all possible positions.



So one observer did note that Twitter is actively burying tweets about Russia's invasion of Ukraine.  Apparently that's in the algorithm there.  So that mystery is resolved for anyone who might have wondered where those tweets went after Twitter became an Elon property because it's been observed that that suddenly disappeared.  We talked about how Elon's Starlink satellite system was turning out to be hugely instrumental in helping to keep Ukraine connected to the rest of the world as Russian missiles continued to batter Ukraine's communications and other utility infrastructures.



So, you know, he seems to be helping Ukraine.  I don't understand why burying those tweets, like specifically biasing against that is a thing that he would want to do.  But then it's a little hard to be in Elon's head.



ANT:  Right.



STEVE:  So why he does anything.  So Ant, I think we should tell our listeners about our second sponsor.



ANT:  Yeah.



STEVE:  And then we're going to talk about Israel's Pegasus and why India doesn't want to use it.



ANT:  Oh, boy.  Another, 'nother fun story coming up.



STEVE:  Okay.  So this one, I guess if I was going to title this story, I'd say, okay, we know it's illegal.  How much will it cost?  Now, if that was a criminal saying that he knows it's illegal, what'll the price be, that would be one thing.  But this is the government of India.  Who is reportedly seeking bids from as many as a dozen, I guess we would call them "me-too smartphone surveillance software purveyors" because the current preferred go-to spyware, the well-known Pegasus from Israel's NSO Group, has become too well known.  The use of Pegasus now is being frowned on by several annoyed nations, including the current U.S. administration, which has been quite vocal about it.  So now it appears to matter which super-secret hidden illegitimate illegal surveillance spyware a country chooses to use to implant into the smartphones of its surveillance targets.  Which again, like, what world are we in?



The news coverage of this in the Financial Times stated that India is hunting for new spyware with a lower profile than the controversial Pegasus system which has been blacklisted by the U.S. government.  Indian defense and intelligence officials have decided to acquire spyware from less well known and less publicly exposed competitors of the NSO Group, and they plan to spend around $120 million, they figure it's going to cost, for the replacement software which they feel they need.  And again, this is, like, illegal spying on other people's phones.



ANT:  I'm sitting here.  I know I'm not supposed to be laughing at this, sir, but this is hilarious.



STEVE:  It's mind-boggling.



ANT:  Spyware for hire.



STEVE:  Yeah.  These lesser-known alternatives are doubtless gleeful that Pegasus became so popular that it's now being frowned upon by those with a reputation to maintain.  Anyway,  this all seems so bizarre since it's, as I said, it's illegal.  But it's what everyone does.



The Financial Times noted that India's move shows how demand for sophisticated, unregulated, and illegal technology remains strong despite growing evidence that governments worldwide have abused spyware by targeting dissidents and critics.  Right?  That's only supposed to be for terrorists and, like, you know, you're only supposed to use it against criminals.  But it's being used against non-criminal critics of the government.



India, for their part, has never publicly acknowledged ever being a customer of NSO, but they would like their money back.  However, Pegasus spyware has been found on the phones of secular journalists, left-leaning academics, and opposition leaders around India, and this sparked a political crisis, ultimately.  As we know, Pegasus turns phones into surveillance devices which can collect encrypted WhatsApp and Signal messages surreptitiously.  And of course it doesn't crack their encryption, it's just there on the phone, so it gets it before it's encrypted and sends it off to wherever Pegasus sends things.  This is what the spooks inside governments want, claim they need, and are obviously willing to pay for.



And I loved this:  India's Modi government officials have grown concerned about the "PR problem" caused by the ability of human rights groups to forensically trace Pegasus - it's a PR problem, yeah - to forensically trace Pegasus, as well as warnings from Apple and WhatsApp to those who have been targeted, according to two people familiar with the discussions.  So, yeah, we're looking for something, they're saying, that is just as effective, but is also much less well known.  So where do we wire the funds?



ANT:  Wow.



STEVE:  Unbelievable.  You know, I mean, so we were talking, you know, the promise of being able to spy on people who are critical to your government has turned these nations into outlaw nations where they're explicitly using illegal, forbidden, you know, who's worried about ethical AI?  What about ethical nations?



ANT:  Right.



STEVE:  And governments.  We already don't have that.  Wow.



ANT:  Oh, man.



STEVE:  Okay.  Meanwhile, with timing that you couldn't make up, last Wednesday the so-called "Summit for Democracy" was held, after which a joint 12-nation statement condemning the proliferation of exactly the sort of commercial spyware that India is currently on the market for, happened.  The governments of, in alphabetical order, Australia, Canada, Costa Rica, Denmark, France, New Zealand, Norway, Sweden, Switzerland, the UK, and the U.S. published a joint statement on planned efforts to counter the proliferation of this commercial spyware.



Agreed countermeasures include tightening export controls, better information sharing to track the proliferation of such tools, and engaging with additional partner governments to reform and align policies on the use of spyware by their agencies.  Maybe they ought to give India a call.  Given how widespread the practice has become at the highest levels of nation-state intelligence tradecraft, you really wonder how many of the signatories of that letter are condemning on one hand and deploying exactly that spyware from their own agencies on the other.  So it's just, you know, it's just too tantalizing. 



ANT:  Tantalizing, scary, sad, all at the same time.



STEVE:  So on a happy front.



ANT:  Thank you.



STEVE:  Guidance has been issued - yes, we need some happy news.  Guidance has been issued by the U.S. Food and Drug Administration last Thursday, March 30th, which explains now that any and all medical devices being submitted to the FDA for approval will from now on be required to meet specific cybersecurity requirements.  This is a first.  We've never had this before.  These new requirements are part of the so-called "Consolidated Appropriations Act" which was signed into law late last year, 2022.  That new law contained a section titled "Ensuring Cybersecurity of Medical Devices," which is exactly what it sounds like.



ANT:  Right.



STEVE:  According to the FDA, submissions for new medical devices will need to include specific cybersecurity-related information such as the description of a plan for identifying and addressing vulnerabilities and exploits within a reasonable time.  Companies must also provide details on the processes and procedures for releasing post-market updates and patches that address security issues, including through regular updates and out-of-band patches in the case of critical vulnerabilities.  The information provided to the FDA must also include, and this is something that we're seeing more and more, a software bill of materials, an SBOM.



ANT:  SBOMs.



STEVE:  For commercial - yes - commercial, open source, and off-the-shelf components.  So that they will be disclosing what tools and components and subsystems were used to build this whole thing.  So again, to create some visibility into the individual offering.  The requirements apply to any "cyber device" that runs software, that's defined as a device that runs software, has the ability to connect to the Internet, and would be vulnerable thereby to cyber threats.  The new cybersecurity requirements do not apply to submissions prior to March 29th, 2023, so like not prior to last Thursday.  And the FDA will not reject out of hand applications solely failing this new requirement until October 1st.



So you've got some, you know, a grace period, essentially, from this announcement and this going into effect kind of semi-officially last Friday until October 1st.  And in the interim it will provide assistance to companies up until that date.  However, starting with October 1, the agency may start rejecting premarket submissions that do not contain the required information.  So...



ANT:  You know, I love this, that they make this very, very clear for everybody.  So there's no wiggle room.



STEVE:  Yup.  Nope.



ANT:  Just simplify this.  We're trying to make things better for everybody.  And here we have the government stepping in and making it just a simple, simple process.  Thank you.  Thank you.



STEVE:  Right.  We're announcing it now.  We're giving you till October 1st.  We'll help you until then, if anything about this is confusing.  And no reason why it would be.  You know?



ANT:  Right.



STEVE:  In our industry, in the security industry, the stories, for example, about insulin pumps being hacked and taken over, you know, have almost become a meme in the industry.  So this welcome legislation means that devices can no longer be sold and forgotten.  Now, this works easily today for medical devices, the marketing and sales of which are already highly regulated.  So like there's already FDA regulation mumbo-jumbo and paperwork and analogies and all that in place.  But it does seem likely that somewhere in the future, and I don't know how far in the future we're talking, anything that can connect to the Internet may need similar mandatory functionality.  Note that in the FDA's definition of "cyber device" it meant could connect to the Internet because...



ANT:  It doesn't get any simpler than that.



STEVE:  Right.



ANT:  You know, it doesn't get any simpler than that.  Just keep it that way.  None of that like legal jargon mumbo-jumbo that you see in all the other types of services.  This is great.



STEVE:  Yeah.  The problem, of course, is that with non-medical devices, we don't already have any kind of regulatory framework in place.  It's just like, you know, it's the Wild West out there.  And so the question is will that eventually happen.  It would be better if it was done voluntarily by manufacturers, and if consumers were to insist upon those features.  Unfortunately, most consumers feel that updates to Windows are a big pain in the ass.  It's interfering with their work and their job.



No one who listens to this podcast thinks that updates are a problem.  It's like, the only question is when do we update?  Do we wait to see if it causes any problems and so forth.  But most consumers don't have anything like that appreciation.  It's like, because they don't see a problem that's being fixed.  They just see that, you know, their computer had something spinning on the screen for an hour, and then they finally were able to get back to work.



ANT:  With this requirement in place, what do you think would be the biggest pushback, if any, from the OEMs?  Would it be listing the SBOM or something else?  Would there be any pushback for this security requirement?



STEVE:  Certainly the software bill of materials would require some disclosure which otherwise would not be necessary.



ANT:  Right.



STEVE:  So I remember, what was it, I think it was maybe it was the Kindle when I first - because the Kindle was from Amazon, was based on Linux, and a whole bunch of public stuff.  I remember seeing - oh, I know.  It was the requirement to disclose the licenses of the open source software that the project was using.  And you looked through it, and it was like, well, Amazon, did you write anything?



ANT:  Nothing.



STEVE:  So glue this together from stuff you've got on GitHub.  I mean...



ANT:  Skinned it.



STEVE:  It was crazy.  Yeah.  Exactly.  Oh, stamp a label on it.



ANT:  Wow.



STEVE:  Okay.  So seven years ago, in 2016, we covered here on the podcast the U.S. Department of Defense's first halting, experimental, we're not sure about this, "Hack the Pentagon" bug bounty effort.  No one was sure it would work, least of all the skeptical government bureaucrats, but work it did.  Since that program's launch, ethical hackers have helped the DoD find and fix more than 2,100 vulnerabilities, many of them which were very scary and good to find, which were identified by more than 1,400 hackers.



The news today is that the continually useful and successful program now has a dedicated website at www.hackthepentagon.mil, which the DoD will be using to continue to educate additional branches of the sprawling U.S. government about the benefits available from allowing good-guy hackers to take a crack at cracking.  The site will also continue to seek and sign up talented new hackers for the continually expanding program.  So bottom line, this was a win.  The government's networks and the DoD military government networks are more secure as a direct result of inviting ethical hackers, not anybody, you just can't go attack the government, you've got to sign up first and identify yourself and say, uh, may I please...



ANT:  At least you'd better identify yourself.



STEVE:  Yeah, exactly.  It doesn't work if you say, oh, but wait a minute, I was hacking the Pentagon, and they said, "We know."  Anyway, no, you need to let them know you're going to do that first.  So it's cool that it is succeeding.  And in general that's what we're seeing.  We're seeing, and I talked about this a couple weeks ago, in general bug bounty programs are a win for the security industry.  They have become, just as much as security updates have become, you know, they are now part of the ecosystem, and we're better for it.



Speaking of being better for it, since Firefox 110, which everyone should have by now - I'm at 111.0.1.  But ever since 110, Firefox has provided a new built-in facility for showing third-party DLLs, meaning those not signed by either Mozilla or Microsoft, which have arranged, sometimes by hook or by crook - and I mean "crook" in that sense - to have themselves injected into Firefox's address space.  If you're a Firefox user, you can type up in the address bar "about:third-party," hit ENTER, and you'll be looking at this new page.  When I did that I only had three DLLs, all which were signed by Intel.  Since I'm running on an Intel NUC machine, that wasn't surprising.  By clicking on the little folder icon to the right of each of the DLLs' names, Windows Explorer will be opened on that file, and it's then possible to right-click to look further into its properties.  I did that.  And sure enough, in my case, the three DLLs were Intel graphics drivers signed by Intel.  And actually they were also co-signed by Microsoft, so they probably shipped along with Windows 10 in that instance.



So this is very useful from a security standpoint since injecting DLLs into another process's process space and very usefully into a browser's process space is something that malware likes to do because it allows it to steal things like the passwords that are being entered, either by you or your password manager, into the browser fields, and the crypto addresses that you may be copy-and-pasting in order to send  money to various places.  But Mozilla's primary motivation was to help identify misbehaving DLLs that might be causing Firefox to become unstable and to crash.  So in that sense it was self-defense is really what was going on.



ANT:  Right.



STEVE:  You know, like people were reporting crashes.  And so Mozilla said, uh, and I'm sure they initially said type some bizarre command so that you can tell us what DLLs have been injected into the browser.  And they decided, okay, let's just make this a form, you know, a fully public known UI,  about:third-party.  Now you can see the DLLs.  And for DLLs that are not signed by Mozilla or Microsoft, and in this case these were co-signed, mine were, you'll find a little - another option there, a little red cross-out symbol that allows you to deny that DLL's ability to inject itself.  So you're able to turn off, to block the injection by DLL in order to see whether that might cure a problem that you've having.  So they've also made it nicely diagnostic.



ANT:  End-user remediation is what that sounds like; right?



STEVE:  Right, right.  Of course, nobody would know it's there until, you know, you've complained.  Or, you know, certainly in forums with other knowledgeable Firefox users.  They might say, oh, yeah, type about:third-party, and you'll find out.  And then, you know, turn that stuff off.  You don't know what it's for.  And remember, from a security standpoint, if something looks suspicious, then definitely go explore what that is that's been injected into your browser's process space.



Okay.  So not surprisingly, a great many of our listeners wrote after last week's Microsoft rant on my part.  Rather than share with everyone here what was essentially the same sentiment expressed many times in the feedback that I received, I'll just share one which is representative of all.  Matthew Hile tweeted me.  He said:  "Steve.  Listener since Episode 1 and SpinRite alpha tester.  Thanks."  He said:  "I think you need to reconsider last week's rant.  For years you have decried outdated, unpatched servers stuck in the closet."  I absolutely have.  "You have spoken positively of governments' recent efforts to locate and report vulnerable servers.  You have asked what it would take to get those servers updated, and would it ever happen.  Well, Microsoft's refusal to accept email from unpatched and unsupported Exchange servers is clearly a very dramatic way to accomplish your goal."



ANT:  Huh.



STEVE:  "Regardless of the dangers from email from those systems, this will force users to either upgrade or move from known vulnerable systems to a less vulnerable one, making the Internet safer for all."  Okay.



ANT:  That's an interesting point.  An interesting perspective, I should say.



STEVE:  Of course.  And I'm bringing it up because I completely agree with what Matthew wrote, and with what everyone who wrote something similar said.  I agree 100%.  Really.  This is without question an absolutely powerful and doubtless effective means for Microsoft to force the upgrading of their older Exchange servers.  As we said last week, refuse to accept any email from them, and those who are still running them will get the message.  And toward the end of last week's rant I did say exactly that, though I would not fault anyone for missing it since it certainly wasn't my main thrust.



What I said toward the end of my rant was:  "No one using any Microsoft Exchange Server software will ever again be able to fail to keep it updated, nor to avoid the purchase of future licenses forever.  I celebrate that idea from here forward, since keeping software updated, especially Exchange Server, is a good thing."  So anyway, I just wanted to make sure that everyone understood that I really do appreciate that aspect of this mess.  But in my mind, the fact that it would be effective still doesn't make it right because Microsoft is rendering otherwise useful servers unuseful, claiming that they represent a danger to them, and clearly what would generate revenue for them.  So, you know...



ANT:  Let me push back on you.  



STEVE:  Okay.



ANT:  Let me push back on you here.  So you said "clearly useful servers."  What would those uses be if they're unpatched and not necessarily secure?  How would they be considered useful?



STEVE:  Well, okay.  So there are Exchange Server 2007, 2010, and soon to be 2013, which actually expires now we're in April.  It's end-of-life.  So they're useful because they are online, and they are receiving and sending email.  They're doing the job for their licensors.  And so it is certainly the case that - and in fact this is actually what we're going to - this is the gist of our conversation we're about to have about zombie software, is what does it mean when software has left its useful service life, you know, like what does that really mean?  My argument is Microsoft is claiming that those servers represent a danger to them because they can send malicious email.  Well, every email server can send malicious email.



ANT:  Okay.  Good point.



STEVE:  Well, that's the nature of email.  And so Microsoft is singling them out, for one thing.  Why?  Oh, well, because what's the shortest path for somebody who has an out-of-date server that suddenly is no longer able to send email to any Office 365 or Outlook.com user?  Well, the shortest path is to upgrade that server from whatever they have to the current Exchange Server.  And that's not free.



ANT:  Right, right, yeah.



STEVE:  So what pisses me off here is that Microsoft is essentially rendering those functioning servers - yes, they're old.  Yes, they may be buggy.  Yes, they have security vulnerabilities.  They're not vulnerabilities to Microsoft.  They're vulnerabilities that affect the users of those servers.  And it's up to them if they want to update, not up to Microsoft to force them to do so.



ANT:  Yeah, that is a bit of an extortion.  I give you that.  I give you that.



STEVE:  Okay.  We already did this last week, and my blood pressure rises every time I start talking about this.  So on to a happier subject.



ANT:  Yeah.



STEVE:  It was Episode 887, which we recorded on September 6th of last year. And before I went back to look it up, I assumed it was longer ago because that means that it's only been seven months since I shared my discovery of this new-to-me author.  And in that time I've read his first two series totaling 24 books.  Our listeners will know that I'm referring to The Silver Ships series by Scott Jucha, J-U-C-H-A.



ANT:  J-U-C-H-A, mm-hmm.



STEVE:  Now, Leo was not a fan of the series since he felt that the main character, Alex Racine, was unrealistically portrayed as being too perfect.  Okay, I understand that.  Not everyone is going to like everything.  Not everyone likes the same sort of music, nor the same food.  But for what it's worth, I had a wonderful time reading the series; and I have heard from many of our listeners who felt the same way I did.  And I mean, rave reviews.  There was some terrific science fiction in there.  Scott is a great storyteller.



Now, when I look back over the 24 books, my main complaint would be that 20 books spent with the same characters is a lot of time with them.  I knew that was the case because, when I switched - I don't remember now, like maybe it was after Book 16 of the first 20 - I switched to the four-book Pyreans sideline series, I was a bit relieved to be meeting some completely new characters in an entirely different set of worlds.  And inevitably, any 20-book storyline is going to have some spots where you're wading through detail that just seems to be taking up time.



I'm mentioning the series again because I did love it, and I'm glad to have recommended it to everyone here.  If it wasn't your cup of tea, then no harm done.  And if it was, then you already know how much fun was contained within those pages.  Since the beginning of the year, the amount of time I've allowed myself for recreational reading has been significantly curtailed, so I must have really been reading a lot more during the first months of the series because today all I really want to do is to get SpinRite v6.1 finished and published.  So pretty much whenever I'm awake, except for the time spent assembling this podcast each week, SpinRite has my full attention.  But I did manage to finish the final 20th book in the Silver Ships series.



Since I really think that Scott is extremely pleasant to read - it's just comfortable reading, he writes well, he's a great storyteller, I like the way he develops characters - I have opened the first of his next eight-book Gate Ghosts series.  And so far I love it, too.  New people, new worlds, but the same very acceptable writing style and terrific storytelling.



Anyway, I should also note that, while I've been doing this reading of this new author, another favorite of ours, Ryk Brown, has been cranking away on the third of his 15-book arcs.  Remember he's laid out five arcs, each of 15 books.  And many of us have all read the first two.  Many of you may already be into the third arc of 15.  I read a couple, and then I was turned onto the Silver Ships series while I was waiting for the next book of Ryk's to happen.  I switched, and then I got sucked into that.  So anyway, he's continuing to tell the story of Nathan, Jessica, Cameron, Telles, Josh, Loki, and the rest.  Anyone who's dipped their toe into the Frontiers Saga knows all of those names quite well.  So I'm sure that once I eventually finish Jucha's eight-book Gate Ghosts series, I'll switch back to catch up with the ongoing Frontiers Saga, which is underway and continuing.



And speaking of SpinRite, very briefly, I'll just say that we're getting very, very close.  SpinRite has not misbehaved in a long time, since I adopted full protection from any of the many ways that BIOS firmware can misbehave and was misbehaving.  And as I mentioned before, there were many.  But I think that, if anything now, SpinRite is probably over-insulated.  But you can never have too much insulation.  The few problems remaining mostly surround how hard SpinRite - I guess they're not really problems, there are questions remaining - around how hard SpinRite should try to work with badly damaged drives.  When a drive is really very badly damaged, it's even difficult for SpinRite to verify that it is a drive, or that it's established communication with the drive.



So that's where we are now.  We're sorting through a few issues surrounding that.  On one hand, it's mostly of academic interest since none of those clearly dead and dying drives that SpinRite's testers have would ever be considered useful any longer for data storage.  But we're all curious, right, to know, like, what's going on exactly.  And for me, I want to make SpinRite as good as it can be, and I don't ever want to return to work on SpinRite once I declare that it's finished.



So I'm still willing to give it a little more time.  But, you know, we're talking a week or two, not much more than that because, I mean, it has actually for quite a while been working just great for I think we're up to 668 people that have been running the alpha releases through the wringer.  And there are a few people with clearly dead drives that, you know, SpinRite still - it's still fighting itself about whether it should wait a little bit longer for the drive to come back with a response or not.  Or somebody, like, when it's chugging along and gets stuck on a problem, how long should it - how hard should it work on that before it finally gives up.  So it's that sort of...



ANT:  It's trying to figure out the difference between a malfunctioning drive versus a broken drive is what you're saying; right?



STEVE:  Yes, yes, yes.  Okay.  Let's talk about our last sponsor, and then Zombie Software.



ANT:  All right.  So we're going to talk about "The Last of Us."  No, not "The Last of Us," zombies.



STEVE:  Yeah.  So we know that a few years ago CISA, after its formation, created their KEV, which is the, you know, KEV, Known Exploited Vulnerabilities list or database.  The idea was for this list to serve as a prioritization for any new entity exposed to the Internet.  So this wasn't like every vulnerability ever known.  The idea was that only vulnerabilities whose active exploitation has recently been observed in the wild would make it onto the list.



And as a prioritization mechanism, when you have 20 things, okay, that would be feasible.  Unfortunately, because so many vulnerabilities are under exploitation, and CISA committed to adding anything that they see being exploited to the list, the list has grown.  We talked about how the size of CISA's KEV ballooned last year, not because of many new vulnerabilities discovered with patches being made available in 2022, but rather because older vulnerabilities, in some cases almost ancient, were still seen in use and were therefore, as is KEV's charter, added to the list.



Okay.  Now, today, we have another shoe dropping, with a rather breathtaking report from a security firm known as, I guess it's Rezilion, R-E-Z-I-L-I-O-N, Rezilion?



ANT:  It could just be Rezilion, but a fancy way of spelling it.



STEVE:  Yeah, I mean, I would have two L's if it was Rezilion.  You know?



ANT:  Okay, yeah.



STEVE:  I guess.  So anyway, I like the way Rezilion sounds, so we'll go with it as if it was million, but it's Rezilion. Anyway, so Rezilion writes:  "Do you know KEV?  You should," as if it's a person, I guess.  Do you know Kev?  "You should, because hackers do.  Rezilion's research team just released a new report which highlights the critical importance of Known Exploited Vulnerabilities (KEV).  Specifically, our research," they write, "uncovers that although the KEV catalog vulnerabilities are frequent targets of APT (Advanced Persistent Threat) groups, many organizations are still exposed and at risk from these vulnerabilities because they're not patching them.  This gap in patching may be due to a lack of awareness, or a lack of patching resources, or both."  Or maybe priorities, who knows.



They said:  "The KEV catalog, maintained by the Cybersecurity and Infrastructure Security Agency (CISA), is a reliable source of information on vulnerabilities that have been exploited in the past or are currently under active exploitation by attackers.  According to our new research, there are over 15 million vulnerable instances in the KEV catalog, with the majority being vulnerable Microsoft Windows instances.  That's a massive number of systems exposed to attacks, leaving organizations vulnerable to exploitation from threat actors and Advanced Persistent Threat groups."



Okay.  So Rezilion scanned the Internet, checking specifically for systems exposing vulnerabilities to any of the now 896 individual vulnerabilities currently listed in the CISA KEV database.  What they found is what any bad guys who took the time to do the same could also find, which was more than 15 million systems worldwide currently in need of patching to prevent their easy exploitation.  Rezilion's report listed the top 10 most frequently exposed vulnerabilities in a table which I've included in the show notes.  I was curious to know more about the specifics of a few of these top 10 still most prevalent exposed vulnerabilities.



Okay.  So the list is sorted by the number of IP addresses expressing that vulnerability, where that vulnerability is now, today, present.  So as I said, I was curious to know more about the specifics of a few of these top 10 still most prevalent exposed vulnerabilities.  Number one of the top 10, which is currently present in 6,453,785 IP addresses, is CVE-2021-40438.  So that one is only two years old.



ANT:  Wow.  And a third of the cases.



STEVE:  Yes, yes, is one vulnerability.  And listen to what Rapid7 wrote about this back on November 30th of 2021:  Rapid7 said:  "On September 16th, 2021, Apache released version 2.4.49 of HTTP Server, which included a fix for CVE-2021-40438, a critical server-side request forgery (SSRF) vulnerability affecting Apache HTTP Server 2.4.48 and earlier versions," like all the way back.  "The vulnerability resides in mod_proxy and allows remote, unauthenticated attackers," meaning anyone on the Internet, "to force vulnerable HTTP servers," all six-plus million of them, "to forward requests to arbitrary servers, giving them the ability to obtain or tamper with resources that would potentially otherwise be unavailable to them."



They wrote:  "Since other vendors bundle HTTP Server in their products, we expect to see a continued trickle of downstream advisories as third-party software producers" - here we come to the software bill of materials we were talking about before, right, like what stuff does your product have in it - "as third-party software producers update their dependencies.  Cisco," they said, "for example, has more than 20 products they are investigating as potentially affected by CVE-2021-40438, including a number of network infrastructure solutions and security boundary devices.  To be exploitable, CVE-2021-40438 requires that mod_proxy be enabled.  It carries a CVSS score of 9.0."



Finally, they wrote:  "Several sources have confirmed that they have seen exploit attempts of CVE-2021-40438 in the wild.  As of November 30th, 2021, there is no evidence yet of widespread attacks.  But given HTTP's prevalence and typical exposure levels, and the fact that it's commonly bundled across a wide ecosystem of products, it's likely exploitation will continue and potentially increase."



Okay.  Now, this particular vulnerability would not be leveraged for any sort of widespread attack.  Maybe you could use it in an HTTP query reflection, but that's not clear due to HTTPS certificates being needed.  But it is exactly what a high-level determined and targeted attacker might be looking for.  Imagine a publicly exposed Apache web server on an enterprise's network boundary, which is where they are.  The web server sits on the enterprise's network, as it pretty much has to, and it fields public HTTP requests over port 443 incoming from the public Internet.



And that enterprise also maintains other servers, naturally, of various types for strictly internal use.  And those servers have no public exposure.  They're simply sitting on the internal network.  However, they are visible, because they're on the internal network, to the vulnerable Apache web server, since as I said they reside on the same internal corporate network.



ANT:  Right.



STEVE:  This vulnerability presents the perfect means for allowing a remotely located attacker to bounce queries off of the enterprise's public Apache web server and into the private corporate network behind it.  It turns the Apache server into an unregulated and unrestricted public proxy with access to the enterprise's internal private network.  If and when used in that way, it is a horrifying vulnerability.  And right now, of the 896 known vulnerabilities on CISA's KEV list, it holds the number one slot by being present at 6,453,785 known IP addresses spread around the world.



The presence of this vulnerability came to light only about 18 months ago.  And one of the problems, as noted by Rapid7, is that Apache is embedded in many networked appliances.  They noted that Cisco has at least 20 different devices that were, and probably to a distressing degree still are, subject to this vulnerability.  No fault of Cisco's.  I'm sure they created patches for all of them.  But as we know, having a patch and having the patch applied are two very different things.  So it's not just instances where someone is running a standalone Apache web server on a Unix or Linux box.  Those would be obvious and easily updated.



No.  The real challenge is all of the places where Apache has been embedded inside an appliance that appears to be humming along just fine.  Hey, Cisco has some updates for our network thingamajig?  Well, okay, but it's working fine just now.  We'll get back to it when we have some time.  Then suddenly your entire network is encrypted, the ransom note arrives, and you really don't have any time.  You wonder after the fact how they got in.  Well, maybe they pivoted off of a known but unpatched vulnerability, six-million-plus instances of which currently exist and were recently enumerated on the public Internet.



Okay, now, I'm not going to go through all 10 of these, but let's look closely at one more that's halfway down the list, occupying slot number five.  That's CVE-2015-1635.  The first thing we notice is the 2015, okay, so eight years ago.  And it's in slot number five of how many servers are currently vulnerable.



ANT:  How?



STEVE:  I know.



ANT:  Eight years old?



STEVE:  And the other thing you notice is the 1635, you know, a reminder of how quaint things were eight years ago when CVE numbers only had four digits.  Now we need five.  I hope we never need six.



ANT:  It was a simpler time, sir.



STEVE:  It was a simpler time.  So how many known vulnerable instances of that now eight-year-old problem are still presently exposed on the public Internet?  Well, 120,156.  And get this.  It's one of those quite rare vulnerabilities that's managed to earn itself a CVSS score of 10.0.  That's right.  It doesn't get any worse or more frightening for those who rate these things.  We have seen how difficult it is to score a perfect 10.  There are plenty of wannabe 9.8's out there, but the 10.0 remains a rare beast.  And who managed to bring that one home?  None other than Microsoft in their own web server, where its successful exploitation, simply by making the proper remote query, allows the attacker's code to run in the user's system with full system kernel level privilege, thus earning the full monty 10.0 score. Again, a full eight years downstream, and yet today 120,156 Microsoft web servers remain sitting ducks for this remote code execution vulnerability.  I know.  Okay.  Now, I'm a software developer.



ANT:  Why?  I mean, why?  This is known information.  Is it just lack of resources?  And when I say "resources" that means time and money where none of this stuff is getting patched?



STEVE:  You know, the thing that's missing from these scans, because there's really no way to get them, we can get demographics by country, and sometimes that is illuminating.  But we don't get demographics by size of company or number of employees or corporate annual revenue.  And it would be really interesting to see what the correlations were there.  And to answer your question, Ant, it's like, okay, what explains this?  Because at this point all we know is that that's the number.



Okay.  So I'm a software developer who's done a fair share of Internet programming.  SQRL is an Internet authentication system.  GRC's ShieldsUP and the DNS Spoofability services are persistently available on the Internet, and GRC's DNS Benchmark, now with 8.2 million downloads, which are now occurring at a rate of around 2,000 new downloads a day, has become the industry standard for measuring DNS server performance.  I published that benchmark 13 years ago, back in 2010, and it's never had a bug.  So I deeply and fundamentally object to what is clearly a growing presumption in the world that any software that's not being actively maintained is therefore inherently bug ridden.



And I see this.  I mean, it's in the air.  It's as if no one trusts software anymore that isn't in intensive care with multiple IV bags hanging overhead.  It has to be in serious trouble, on continuous life support, being monitored 24 hours a day, for anyone to think that it doesn't have serious problems. It's perverse.  And yet the most frustrating thing is I can't argue against that because it is the only sane and rational conclusion to draw from all the evidence that is presented to us every day.



ANT:  I mean, we hear from the likes of Microsoft, as big as they are, there's always some type of announcement about a zero-day or some other bug that scares the crap out of everybody.  So why would the public folks just trust that, oh, this software doesn't have any bugs?  I mean, why would we?  It's because there's always something in the news.



STEVE:  Now, there are, as it turns out, there are some beautiful exceptions to that.  SpinRite 7 will be based upon an embedded 32-bit real-time operating system, all ongoing support for which ended at the end of last year when its publisher went out of business; and yet I've selected it for SpinRite's future.  Why?  They didn't go out of business because the software was no good.  They went out of business because the software was too good.  It was done.  It was a perfectly functioning finished product, and no one was subscribing to updates for it anymore because it was feature complete, and there were no more bugs to be fixed.  None.  It was done.  Take it off the ventilator.  Pull the IV lines.  Hold your breath and count to 10.  Does it still have a pulse?  What do you know.  It's alive.  Works great.



Now, the idea that a company went out of business because its software didn't have any bugs is a bit chilling because nobody wants to use Windows after its life support has been terminated.  If it's still moving after that, the presumption is that it's become a zombie that wants to eat your brain.  So when we hear that Microsoft shipped a brand new edition of Windows - I don't recall which Windows it was.  I remember hearing it, maybe Vista, 7, 8, or 10, which at the time of its release had more than 10,000 bugs known to Microsoft, you have to wonder...



ANT:  That sounds like a Vista thing 'cause Vista was so bad.



STEVE:  Yeah.  You have to wonder whether the leaking of what should have been embarrassing information might not have been deliberate on their part.  If a company went out of business because their software worked perfectly and had no more bugs, that's not a worry that will ever keep anyone at Microsoft awake at night.



Everyone wants to use Windows, and everyone wants to make sure that Windows IV lines are continuously connected and that vital, life-sustaining fluids are flowing freely into it.  And the minute Microsoft cuts off that flow, whoa, time to upgrade.  And not because you necessarily want anything that the new Windows has.  Quite often, in fact, much of what they've done to it is noxious and unwanted.  Just look at Vista or Windows 8.  But what choice do you have?  Everyone knows that Windows is perpetually so sick that the only safe way to use it is with it lying in ICU, attached to a ventilator, with IV lines flowing and under constant monitoring.  You disconnect it from its continuous life support at your peril.



And Microsoft with Windows, or Exchange Server, or, well, I guess anything Microsoft creates, is only a slice of a much larger problem that this industry has.  This is the sad reality of today's software ecosystem.  Our insatiable desire for features, or in Microsoft's case their unrelenting need to seduce us with new bug-ridden features, means that we're never going to get ahead of this; and that, indeed, almost all software only remains viable while it's on perpetual life support.



So now we loop back to CISA's KEV list, and we see that what we have is a list of what can only be described as Zombie Software.  For one reason or another, that software has been disconnected from the life support that was, sadly and unnecessarily, but still in fact vital for its ongoing safe use on the Internet.  In the case of that eight-year-old Windows web server critical 10.0 vulnerability, that Windows 7 and Server 2008 era software is now more than three years past its end of support life, which Microsoft terminated in January of 2020.



It's worth noting, however, that the updates for those 120,156 very old servers are still online and are available from Microsoft.  So simply clicking "check for updates" on any of those machines, assuming that they're activated and validly licensed, will quickly bring them current with the last build of their server's code and would, in the process, cure any and all vulnerabilities that were known and for which patches were available when support for that server ended three years ago.



ANT:  Imagine that.



STEVE:  Yeah.  But that's one of the two oldest entries in the list.  The others, the newer eight are much more recent, and ongoing life support still remains available for all of those.  A perfect example of that is the other critical vulnerability we examined, Apache.  Any of those 18-month-old Apache servers could be updated immediately.  Yet 6,453,785 publicly exposed instances of Apache voluntarily remain unpatched today, despite the fact that updates are certainly readily available for every instance of them.



So where does this all leave us?  One thing I want to observe is that zombies eat the brains of those who are nearby.  So this is about self-responsibility.  If you don't want your zombie server to eat your brain, either shut it down or get it patched, if possible.  This is one, as I said, of my main complaints with Microsoft's plan to force other people to update their Zombie Exchange Servers.  Microsoft contends that their brain was going to be eaten by somebody else's remotely located zombie.  That's utter nonsense.  We all understand that we're the ones whose brains are in danger if we choose to use software after it's gone zombie.



But taking the broadest view possible, it is an unfortunate and sad truth that, with very few exceptions, almost none of today's software is able to stand alone without the presumption that periodic and continuing updates are required for that software's users to feel comfortable with the performance of their software.  The assumption of the availability of ever-present Internet, which provides a lifeline for would-be zombies, allowing them to remain on life support, has enabled a gradually growing laxity on the part of software publishers.  Today, few apparently feel the need to get it right the first time, since problems can always be fixed later.



In the extreme case of Microsoft, major products are shipped, despite reportedly being riddled with tens of thousands of bugs known to its developers at the time that it is shipped.  And as we saw with last week's Exchange Server issue, Microsoft's own self-fulfilling prophecy is that, once any of their software is removed from life support, not only everyone nearby, but also everyone else on the Earth is put in danger once it becomes a zombie.  It's a sad state of affairs which did not have to be.  But as we sometimes observe on this podcast, it's the world we live in.  The best we can do is to navigate it safely.



ANT:  Now, Mr. Steve, I'm going to come to Microsoft's defense for half a second; okay?



STEVE:  Okay.



ANT:  Google is just as bad on this front.  There's been a gazillion times Google will push out a version of Android that's gone through alpha, it's gone through beta testing and so forth.  And the second it gets on your device, which happens to be their flagship device, it is a craptastic experience until they run that first patch.



STEVE:  So actually you're supporting my position.



ANT:  Oh, I know, I know.  



STEVE:  You're adding another example of a company that is relying on updates rather than ever getting it right.



ANT:  Yeah.  I just didn't want it to come out as if you're just only beating up on Microsoft because there's a bunch of them out there.



STEVE:  Oh, yeah.  Yeah.  I'm just using them as a good example, and because they had number five on the list with a server that 120-plus thousand of them are right now today vulnerable to a remote code execution exploit which exists and, if anyone cared to, could take them over because you know, bugs were not fixed.



ANT:  Unbelievable.



STEVE:  Yup.  It's the world we live in.



ANT:  So Mr. Steve, this has been another, 'nother fun chat and discussion.  And yes, there was some sad stuff that happened in this, some scary stuff.  But you did give us a little bit of good news on this week's episode of Security Now!.  And I thank you for all of the information and tidbits that you provided with us this week, sir.



STEVE:  My pleasure, Ant.  And we'll be back for more next week.



Copyright (c) 2023 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.


GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#918

DATE:		April 11, 2023

TITLE:		A Dangerous Interpretation

HOSTS:	Steve Gibson & Jason Howell

SOURCE:	https://media.grc.com/sn/sn-918.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we seek answers:  What did Microsoft and Fortra ask from the courts, and what did the courts say in return?  When can chatting with ChatGPT leak corporate secrets?  Why has Apple suddenly updated many much older of their iDevices?  Why bother naming a six-year-old ongoing WordPress attack campaign?  Which Samsung handsets just went out of security support?  What two user-focused policy changes has Google just made for Android users?  And do we really have additional ChatGPT hysteria?  After answering those questions, and examining an example of the benefit of rewriting solid state non-volatile storage, we're going to take a rather deep dive into a tool that was meant for good, but which I fear may see more use for evil.



SHOW TEASE:  Coming up next on Security Now!, Leo Laporte is out; I, Jason Howell, filling in for Leo.  Of course, though, joined by Steve Gibson, who has all the news, all the information for you this week.  ChatGPT and the company secrets, very interesting stuff there.  Apple's sweeping update of older devices.  A WordPress attack campaign that's already six years old.  A trifecta of Android security news updates, close to my heart, anyways.  And the main event, the complexity of modern video codecs and the embedded security risk within as a result of all that complexity.  Steve breaks down that complexity next on Security Now!.



JASON HOWELL:  This is Security Now! with Steve Gibson, Episode 918, recorded Tuesday, April 11th, 2023:  A Dangerous Interpretation.



It's time for Security Now!.  That's right, we're going to talk all about this week's big security news.  And actually, what you probably already know by now, I'm not Leo.  I'm Jason, filling in for Leo.  Leo is deep in his vacation.  I actually just returned from vacation, so I'm like kind of clueless about a lot of the news.  That's why I am super thrilled that we have Steve Gibson, the man of the hour, to talk all about this week's security news.  What's up, Steve?



STEVE GIBSON:  Well, Jason, it is great to see you sitting there behind the microphone.  I'm glad to have you this week.



JASON:  Oh, that's right.  That's right, I forgot.  You're sitting right next to me.  You can help to troubleshoot these things on my phone.



STEVE:  Yeah, that's right.



JASON:  Could you tell me what you see there?  No, don't tell anyone what you see there.  It's my password, and no one needs to know that.



STEVE:  Oh, no.



JASON:  Good to see you, Steve.  Thank you for doing the show with me, allowing me to kind of crash the party.



STEVE:  Hey, glad to have you co-hosting with us.  So this is Security Now! Episode 918 for, what is this, April 14th?  And as we've been doing recently...



JASON:  April 11th.  April 11th.  Just for the record.



STEVE:  Oh, thank you.



JASON:  It's okay.



STEVE:  Hello.



JASON:  Your security news from the future.  You always have been very futuristic.



STEVE:  Good cover.  So as we've been doing recently, we're going to be seeking some answers this week.  What did Microsoft and Fortra - a sponsor of the network, by the way - ask from the courts, and what did the courts say in return?  When can chatting with ChatGPT leak corporate secrets?  Why has Apple suddenly updated many much older of their iDevices?  Why bother naming a six-year-old ongoing WordPress attack campaign?  Which Samsung handsets went out of security support, and what two user-focused policy changes has Google just made for their Android users?  And do we really have additional ChatGPT hysteria?  Yeah, believe it or not.



After answering those questions, and examining an example of the benefit that comes from rewriting solid-state non-volatile storage, we're going to take a rather deep dive into a tool that, well, it was meant for good, but I fear it may see more use for evil.  Then of course we do have a fun and engaging Picture of the Week.  So I think another great podcast for our listeners.



JASON:  Of course.



STEVE:  Oh, and today's podcast is titled "A Dangerous Interpretation."



JASON:  Excellent.  We've got so much to talk about.  Some of those topics close to my heart in the Android world, so I'm curious to hear your take on them.  All right.  Picture of the Week time, always a fun glimpse into the bizarre.  And this week is no different.



STEVE:  So I captioned this one "Why developers sometimes feel misunderstood."  So it's a two-frame photo.  The upper frame shows a picture of a display from a computer that's just been turned on.  And the display is coming from the BIOS or the firmware, and it says:  "Keyboard not found.  Press F1 to continue, DEL to enter Setup."  Which of course you can't do; right?



JASON:  No.  



STEVE:  Because there's no keyboard.



JASON:  Not a lot you can do.



STEVE:  And the second frame just sort of shows some random guy in a shirt and tie, apparently staring at his screen thinking...



JASON:  He's dead inside at that point.



STEVE:  And he's like, did I misread this?  "Keyboard not found.  Press F1 to continue."  And maybe he did press it a couple times.  Nothing of course happened because there's no keyboard.



JASON:  I mean, I see it in front of him.  It's on the desk.  Apparently it's just not plugged in or something.



STEVE:  Yeah, I don't...



JASON:  Is the reason that it gives - because this is ridiculous in and of itself.  But is the thought that like, once you plug in the keyboard, then you can proceed?  But I guess if that was the thought, like, I suppose it should spell that out, like keyboard not found.  Attach keyboard and then, dot dot dot.  I'm trying to make sense of...



STEVE:  Yeah, I think you're giving too much credit to the people who wrote this system.  The idea was that there are a wide range of things that can go bump in the boot-up process; right?  Like the CMOS may have discharged, and so there's no checksum on the CMOS setup information or who knows what.  So there's like a bunch of different things that could be wrong.  And the universal response to any of them is "Press F1 to continue, DEL to enter setup."  Unfortunately, one of those things that can go wrong is, we just checked, and there's no keyboard here.



JASON:  Prevents you from doing any of that.



STEVE:  So unfortunately, still get the default, how do we proceed?  Well, Press 1.  Okay.



JASON:  All right.  I'd be the guy in the tie looking dead inside, too.



STEVE:  Why developers sometimes feel misunderstood.  Well, you might say it's well deserved.  Okay.  So a few weeks ago we talked about how wrong it was that Sony had chosen to legally force the DNS provider 9.9.9.9 to remove DNS resolution from its service as a roundabout means of blocking access to pirated Sony intellectual property, which amounted to some MP3 tunes which were being hosted on the Internet.  The proper action, which we discussed at the time, would have been to legally pursue the provider hosting the content; or, failing that, maybe go to the illegal domain's registrar and shut down the site that way.  But Sony's complaint apparently said, well - well, in fact, their complaint to the court did say, "Well, they didn't return our calls."  So anyway, crazy.



I was reminded of this due to some recent news last week from Microsoft's Digital Crimes Unit, their DCU, from whom Sony could take a few lessons.  Three organizations - Microsoft's DCU; the cybersecurity company FORTRA, also happens to be sponsor here on the TWiT Network; and the Health Information Sharing and Analysis Center, which abbreviates to Health-ISAC - they've all joined together to take technical and legal action to disrupt cracked, illegal copies of Cobalt Strike and other abused Microsoft software, which have been used by cybercriminals to distribute malware, including ransomware.



So in a change in the way Microsoft's DCU has previously worked, they're teaming up with Fortra to remove illegal legacy copies of Cobalt Strike so they can no longer be used by cybercriminals.  And we've talked about Cobalt Strike before, but it's due for a bit of a review.



Although Cobalt Strike is frequently associated with malware  in fact that's the only context in which we're ever talking about it because of the nature of this podcast - Cobalt Strike is actually a legitimate Fortra software product.  It's a very popular and very powerful post-exploitation tool used to simulate adversaries.  And as it happens sometimes, older versions of the software have been altered by the bad guys who use its extreme power, not for benign Red Team testing exercises, but for their criminal purposes because, I mean, it's the real deal.  It actually does penetrate.  Because if you're trying to do Red Team exercises, well, you need something that's real; right?  But unfortunately, in some cases it's indiscriminately real if it's been hacked.



So "cracked" illegal copies of Cobalt Strike have been used to launch destructive attacks, such as those against the government of Costa Rica and the Irish Health Service Executive.  And countless more Microsoft SDKs and APIs are abused as part of the coding of the malware, as well as criminal malware distribution, which is used in the infrastructure to target and mislead victims.



Okay.  So specifically, the ransomware families that have been associated with or deployed by cracked copies of Cobalt Strike have been linked to more than 68 specific ransomware attacks impacting healthcare organizations in more than 19 countries around the world.  So that's been the focus of this work, and it's what earned them a court order.  These attacks have cost hospital systems millions of dollars in recovery and repair costs, plus interruptions to critical patient care services including delayed diagnostic, imaging, and laboratory results; canceled medical procedures; and delays in delivery of chemotherapy treatments and so on.



So, I mean, and we've talked about this before.  It's sort of despicable that healthcare organizations are targets of ransomware, and the bad guys couldn't care less.  Of course they're not located in the same country as those whose services are being denied.  So I guess that's to be expected.



So anyway, the Friday before last, March 31st, the U.S. District Court for the Eastern District of New York issued a court order which allows Microsoft, Fortra, and that Health-ISAC group to forcibly disrupt the infrastructure being used by criminals to facilitate their attacks.  The court order allows that trio to notify relevant ISPs and computer emergency readiness teams (CERTs) who will then assist in taking the infrastructure offline, which severs the connections between criminal operators and the infected victim systems.



So they're not just saying "pretty please," they're taking the steps to remove these systems from the Internet.  And it's international treaties and sort of the assumption that whatever is on the Internet has the right to be on the Internet that makes this difficult.  But it's necessary to elevate this to court order in order to make it happen.  And I would argue this is what Sony should have done.  They're certainly big enough to get a court's attention and explain the problem and say, "Hey, these guys aren't returning our phone calls, so give us the right to go to their hosting provider and have the hosting provider turn them off."



Anyway, as we know, rather than obtaining a court order against an innocent DNS provider - which also, by the way, didn't really solve the problem anyway, right, because anybody not using 9.9.9.9 would have still been able to get to and access Sony's content.  Anyway, Sony should have done whatever was necessary to take the offending pirate server off the air, just as this group is now doing with instances of malicious Cobalt Strike.



Microsoft noted that they're also expanding a legal method, that is, this is an expansion of a legal method used successfully to disrupt malware and nation-state operations to target and abuse security tools used by a broad spectrum of cybercriminals.  Disrupting cracked legacy copies of Cobalt Strike will significantly hinder the monetization of these illegal copies, because you can find them on the 'Net, and slow down their use in cyberattacks.  So there's going to be a second-order effect.  On the one hand, you actually disconnect the ones that are in use.  But at the same time people are going to be less likely to go purchase new cracked copies because they know that they're going to be jumped on, wherever they are in the world, thanks to this court order.  So they'll hopefully reevaluate and change their tactics away from using Cobalt Strike.



Fortra, the publisher of Cobalt Strike, noted that in recognition of the power of their tools, they've always taken considerable steps to prevent the misuse of their software, which includes stringent customer vetting practices. You know, you don't want to just sell this to anybody.  However, criminals are known for stealing older versions of security software, among them Cobalt Strike, creating cracked copies to gain backdoor access to machines and deploy malware.  Ransomware operators have been observed using cracked copies of Cobalt Strike and Microsoft software to deploy Conti, LockBit, and other ransomware as part of the newer ransomware, which we've talked about a lot, now being offered as ransomware as a service under that model.



Although the identities of those conducting the criminal operations are currently unknown, the group has detected malicious infrastructure across the globe, including in China, and in our good old USA, as well as Russia.  In addition to financially motivated cybercriminals, they've observed threat actors using cracked copies to act in the interests of foreign governments, including from Russia, China, Vietnam, and Iran.  Microsoft has said that its Defender security platform has detected - and this is a little sobering - around 1.5 million infected computers communicating with cracked Cobalt Strike servers over just the past two years, 1.5 million infected machines communicating with cracked Cobalt Strike servers just in the past two years.  And in 2020, a Recorded Future report found that more than 1,400 malware command-and-control servers were using Cobalt Strike as their backend at the time.



The Censys search engine currently returns around 540 Cobalt Strike servers hosted in the wild.  So again, as I said, they can be found.  Now Microsoft, Fortra, and the Health-ISAC have the means to go get them turned off.  And it's unfortunate that bad guys are using powerful software meant for training and attack simulation.  But I guess it's not surprising.  It's the world we live in.  And going after the bad guys' communications, since they're still needing to use the public Internet for that communication, is clearly the right solution when all other measures fail.  It makes a lot more sense than Sony compelling a single DNS provider to block a bad guy's DNS lookups.  That's just crazy, and it's not going to be very effective.



Okay.  So can ChatGPT keep a secret?  It's not surprising that we're talking about ChatGPT.  Everybody is.  I stumbled upon this, and I knew that our listeners would get a kick out of it.  It seems that some employees of Samsung Semiconductor were using ChatGPT to help in their diagnosis and repair of some problematic proprietary Samsung code.  But in order to do this, they needed to upload the code and some documents to ChatGPT so that it could see what was puzzling these employees.



The only problem was the uploads contained Samsung's sensitive proprietary information.  After finding three separate instances of this happening, Samsung warned its employees against using ChatGPT in their daily work.  The data uploaded to ChatGPT included internal documents and code meant to identify defective chips.  Samsung now limits the length of questions submitted to ChatGPT to 1KB, while the company develops its own internal ChatGPT-like AI for its own private use.



And this really hadn't occurred to me before, but it brings to light something that, as I said, I hadn't considered, which is that it will likely become quite common within an organization to want to be able to leverage the power of these new large language model AIs to aid their own internal proprietary work and research.  But what if the necessary details of that research cannot be allowed to leave the company's control?  So this is a very different application than AI-assisted travel planning, or we need some help coming up with some new creative evening meals for the family.



And this, in turn, suggests that before long we're going to begin seeing AI companies offering to sell standalone preprogrammed AI systems for exclusive use within and by a single organization.  And such systems will likely be compartmentalized so that the published AI side can be refined and upgraded over time while keeping any proprietary information that's incrementally informing that AI separate and safe.  And you know, Jason, it's still difficult for me to believe that we're actually talking about this, like we're talking about, like, okay, soon you'll be able to purchase an AI thing, like an AI cabinet, and stick it in the server room of your organization, and connect it into your network, and all your employees will be able to chat with this AI thing that you purchased.



JASON:  I think what's interesting to me is that I feel like for a while, quite a while in technology circles, chatbots in general have been kind of like a punchline because they've never worked nearly as well as promised.  And now suddenly they're like the...



STEVE:  Right.  It comes up in the lower right corner of your screen.  And it's like, you know, "Hi there," you know, "I see that you're alive.  Would you like some help?"  It's like, go away.  How do I get rid of this thing?



JASON:  Your help is brutal and not enjoyable or useful whatsoever.  And now suddenly it's the solution to all of our problems.  Suddenly it's good enough or pretends to be good enough.



STEVE:  It is just amazing how quickly this has changed around. 



JASON:  I think you're absolutely right, though.  And they've got to already be working on this; right?  Because again, in all of these different facets, the promise or the potential that proponents of AI are putting out there as far as the benefits, you know, companies in thinking about their businesses in different ways and everything, all of that hinges, like you point out, in particular there's pieces of information that hinges on the need for private, secure systems to hold onto that information so that Samsung's employees aren't putting out proprietary information in order to gain the benefits of the AI systems.



And I'd be really surprised if these companies aren't already working on, we've got an AI to bring all these benefits to your company, it's within your walls, and you can rest assured that the information you put in there never goes anywhere outside of your company.  They're working on that right now.  They have to.  There's so much money to be made from that.



STEVE:  It's that little cube in the corner with that weird blue glow.



JASON:  Feed view data.



STEVE:  Who's in there?



JASON:  Feed view data developer.



STEVE:  That's right.  So Last Friday, Apple released security updates for iOS, iPadOS, macOS, and Safari.  The updates will remove two flaws and thus terminate the use of a pair of zero-days that were being exploited in the wild.  If you check, you're looking for v16.4.1 in iOS, iPadOS, and Safari; and macOS Ventura will update to v13.3.1.  And sure enough, this morning I checked my iPhone and it was still lagging behind four days after the update's release on Friday.  I think this demonstrates that this is not a five-alarm fire, and Apple's goal is to eventually bring all devices current without it being any big emergency.



These individual updates are large, right, and there are a lot of Apple devices out there in the world which are all eager to remain current.  But there's just no way  and, I mean, actually almost never really any need  to send the same large chunk of code out to every wandering iDevice simultaneously.  I mean, that's just too much data.  So for things like this, Apple is clearly trickling the updates out unless a user specifically checks, as I did, to see whether they're current.  And immediately my phone said, oh, there is something new, would you like it?  And of course I said yeah.



Okay.  So the point is, we're not seeing mass attacks using new vulnerabilities because that would bring them to everyone's attention quickly and cause them to be found, identified, and immediately fixed.  Instead, those who are finding ways to penetrate today's mobile devices - and boy, we'll be talking about this at the end of the podcast - are leveraging the fact that they're unknown for targeted infiltration.  And it's only valuable to them as long as they stay unknown.  And for something like this, when they're found, well, yeah, they're found, but we don't know how long they were in use before they were found.  So that's always a big question.



In this case, the supposition is supported by the fact that these two flaws that were fixed were reported to Apple by Google's TAG Team (Threat Analysis Group) and Amnesty International's Security Lab.  The fact that Amnesty International's Security Lab was involved further supports the notion that the devices being used against individual high-value political targets are what were attacked.  There are headlines in the tech press about this urging their readers to update immediately due to the danger of being a few days late.  And I would say update when you get around to it, if you're worried.  Otherwise Apple will get around to it for you, and eventually you'll be asked to reauthenticate to your device after an otherwise transparent overnight auto-update.



The two vulnerabilities were a use-after-free issue in WebKit that leads to arbitrary code execution when processing maliciously crafted web content.  So that's frightening; right?  You just get someone to go to a web page, and you can execute arbitrary code on their phone.  Not good.  The second one was an out-of-bounds write issue in IOSurfaceAccelerator that enabled apps to execute arbitrary code with kernel privileges.  Okay, so that wasn't an over-the-Internet remote problem, but a malicious app that had snuck itself into the App Store, or was originally good, and then an update of the app made it bad.  It would be able to break out of the protection of the kernel and obtain kernel privileges.



So Apple indicated, you know, so the point is those are both bad.  I mean, those are what the bad guys would die to know about, so long as nobody else knows about them.  So Apple indicated that it has addressed the first with an improved memory management, and the second with better input validation, adding that it's aware that the bugs "may have been actively exploited."  And from all of this, I salute Apple for being as security conscious as they are.  But out of all of this, that's the only thing that annoys me.  No one is more proactive than Apple, and Google is clearly at parity with them.  But it would be nice if publishers were more forthcoming with their language.  And I suppose that their attorneys won't let them be.



Those were both zero-day code execution flaws, obviously having been used in the wild.  No one should blame Apple for this.  But I suppose the point is that someone would blame them if they were told the whole truth.  So at least we can be thankful that our devices are being kept up to date.  And it's also worth noting that further details about these two vulnerabilities have been withheld due to their active exploitation and to prevent additional bad guys from learning about and also abusing them. In other words, you know, they're in use right now.



One last point is that we can presume that Apple is aware that these flaws are being abused because older devices that would not normally receive updates are doing so.  Yesterday, Apple backported patches to fix these problems to older iPhones, iPads, and Macs that had not been otherwise receiving recent security updates.  Now, updates for these are also being made available for these older out-of-patch-cycle devices.  And we've seen Apple do this before; right?  When something is really a problem, they'll break their policy.



And in the interest of, maybe it's in the interest of their users, or maybe it's because they receive intelligence telling them that known older devices are being victims of these because they're not being patched.  In this case all models of the iPhone 6s and 7, first-gen iPhone SE, Apple iPad Air 2, the fourth-gen iPad mini, and the seventh-gen iPod Touch all get updates.  Well, it happens I have a seventh-generation iPod Touch sitting next to me here because it still has a headphone jack.  After reading that it might be getting an update, I checked, and again, kind of woke it up.  Oh.  Yeah, would you like that?  Anyway, so yes, I updated it, and it's current now, too.  Again, eventually Apple would have gotten around to it.



So I'm just saying that this is not something Apple does for a "may be actively exploited" vulnerability, where they're not sure.  No.  They know these flaws are being used against their selected users in the field.  And also note that even older macOS Big Sur is being updated to 11.7.6, and Monterey has been updated to 12.6.5.  So they're cleaning up some clearly very bad updates that the fact that Amnesty International was involved in finding these really suggests that they were used by the commercial spyware industry, sold to governments probably, and then being used illegally to infect people.



JASON:  All right.  I feel like it's probably safe to say that every episode that I fill in for Leo there's some sort of security story about WordPress.  But yet it's so everywhere.  And I guess that's the point.



STEVE:  Well, yes.  So it's a constant on the podcast because it turns out, and I'm always surprised by this statistic, 43% of the Internet's websites are built on WordPress.



JASON:  Wow.



STEVE:  It's amazing.  It's far and away the most common CMS, you know, Content Management System.  WordPress has 43%.  The runner-up is in distant second place, that's Shopify, with 4.1%.  So 43% for WordPress, 4.1% for the number two guy.  Third place is Wix at 2.3%, followed by Squarespace at 2%.  So, yeah, WordPress is the big target on the web.



JASON:  Far and away.



STEVE:  So not surprising that they're the ones that are taking all the incoming.  Last Wednesday the team at Sucuri (S-U-C-U-R-I) finally gave a name to a long-running WordPress exploitation campaign which they've been tracking for years.  They named it Balada Injector.  Their post last week was titled "Balada Injector:  Synopsis of a Massive Ongoing WordPress Malware Campaign."



And in their post they said:  "Our team at Sucuri has been tracking a massive WordPress infection campaign since" - get this - "2017, but up until recently we never bothered to give it a proper name.  Typically, we refer to it as an ongoing, long-lasting, massive WordPress infection campaign" - that's a mouthful every time you want to talk about it - "that leverages all known and recently discovered theme and plugin vulnerabilities.  Other organizations and blogs have described it in a similar manner, sometimes adding terms like 'malvertising campaign' or naming domains that it was currently using, which amount to several hundred over the past six years.



"This campaign," they wrote, "is easily identified by its preference for" - and this is a particular PHP function - "String.fromCharCode."  Anyway, it's used for obfuscation.  They said also it's known for or recognized by "the use of freshly registered domain names hosting malicious scripts on random subdomains, and by redirects to various scam sites including fake tech support; fraudulent lottery wins; and, more recently, push notification scams displaying bogus CAPTCHA pages asking users to 'Please Allow to verify, that you are not a robot.'  Since 2017," they said, "we estimate that over one million WordPress websites" - one million - "have been infected by this campaign.  Each year it consistently ranks in the top three of the infections that we detect and clean from compromised websites."  So the other top two of the top three come and go.  This thing just hangs in there, year after year after year.



They said:  "Last year, in 2022 alone, our external website scanner SiteCheck detected this malware over 141,000 times, with more than 67% of websites loading scripts from known Balada Injector domains.  We currently have more than 100 signatures covering both frontend and backend variations of the malware injected into server files and WordPress databases."  So, I mean, this thing is just doing, like, everything it can.  And I got a kick, well, I didn't get a kick out of the fact, I thought it was interesting that it leverages every known theme and plugin vulnerability.  So it's continuing to stay alive and to stay relevant and in the top three because it just keeps expanding its vocabulary as problems are found in themes, you know, security vulnerabilities in themes and plugins.



So they said:  "As you can imagine, referring to this massive infection campaign using generic terms has not been convenient.  However, assigning a name to this malware was never at the top of our priority list.  Our security researchers deal with dozens of new malware samples every day, so we typically don't dwell too much on well-known malware that's adequately covered by detection rules and only necessitates minor tweaks and adjustments when we spot a new wave."  So essentially it's sort of like it's sort of fallen into the background; right?  I mean, it's always there.  It's been there since 2017.  It's pervasive and prevalent.  But it's like, eh, you know, they're focused on new stuff, and they've already got this existing thing pretty well covered.



So they explained that:  "In late December last year, our colleagues" - their colleagues - "at Dr.Web shared some valuable information that led us to choose the name [finally] 'Balada Injector.'  A post published on December 30th of 2022 titled 'Linux backdoor malware infects WordPress-based websites' caught our attention, and it was widely circulated in Internet security blogs with titles like 'Linux Malware uses 30 plugin vulnerabilities to target WordPress sites.'  The article discusses two variants of the malware, Linux.BackDoor.WordPressExploit.1 and the same thing .2, and provides a comprehensive overview, including targeted plugins and various indicators of compromise.



"The interest generated by this information," they wrote, "prompted numerous inquiries from various sources, leading us to examine the post closely on New Year's Eve to determine if immediate action of some kind was required.  To our surprise, we instantly recognized the described malware as being the ongoing, massive campaign we'd been tracking for years.  Upon closer inspection, we found that the information provided was accurate, but the vulnerabilities, injected code, and malicious domains all dated back to 2019 and 2020.  Nevertheless, the post offered interesting details about how campaign operators searched for vulnerable websites and injected malware.  We soon obtained samples of the Linux binaries which were written in Go language from VirusTotal, where other researchers had been creating collections.



"Most of the samples were compiled with debug information, and even a simple 'strings' command provided quite insightful information:  names of functions, string constants, paths of files included in the project.  These files consist mostly of source code for various Go libraries, providing additional functionality such as conversion functions and support for Internet protocols.  However, the main malware code was located in the file C:/Users/host/Desktop/balada/client/main.go.  The file path balada/client implied that the developer could refer to this software as Balada Client."



And they said:  "We know that the malware sends data to a command-and-control server, so that would be a Balada Server component.  Whether our assumptions were correct or not, we adopted this name internally and think that it provides some convenience" - yeah - "when talking about a really long-lasting malware campaign.  In many languages, 'Balada' means 'Ballad.'  To avoid ambiguity, we added the word 'Injector' to reflect the nature of the malware campaign that injects malicious code into WordPress sites, hence Balada Injector."



So that was sort of interesting because they'd never run across that particular aspect of the campaign which they discovered thanks to the Linux-based malware that was using 30 vulnerabilities to inject Balada into the WordPress sites that were being hosted on that Linux-based server.  That allowed them to finally look at code that had been compiled with debug strings still in place, which basically it's a thing that allows a debugger, when a debugger hits a problem and shows you where it is, it makes much more sense for you to be able to see the names of things than only the hex addresses of things.  So it was arguably a mistake for the bad guys ever to fail to strip debug strings out of their code.  But they did fail.



So anyway, this was, I thought, some interesting background about a long-lived, multiyear, six years and counting, very aggressive and effective focused campaign against WordPress sites.  And, you know, it's easy to become inured to big numbers.  We're talking about big numbers all the time.  You know, 2^32 is 4.3 billion.  And how many stars are in the sky?  And so forth.  But here, one million individually infected WordPress sites is a lot of sites.  So I wanted to cover this now because I suspect that it won't be the last time that we're hearing about this quite determined Balada Injector WordPress malware that shows no signs of giving up and going away, unlike other malware that sort of is a flash in the pan.  You know, somebody is really dedicated to this.



Okay.  So Mozilla has updated their Firefox Monitor data breach monitoring and alerting service, giving it a dedicated website.  It's at monitor.firefox.com.  But the page you want to go to is monitor.firefox.com/breaches.  If you go to monitor.firefox.com/breaches, you're greeted with the caption, kind of modest:  "We monitor all known data breaches to find out if your personal information was compromised."  And then they said:  "Here's a complete list of all of the breaches that have been reported since 2007."



Okay, now, I'll get back to that list in a second.  It's on the screen now, scrolling slowly.  And it could probably scroll for the rest of the week before you go to the bottom of it.  And that's kind of the point here.  So as you might expect from a web page which boasts a complete listing of what amounts to every site breach ever, you'll be scrolling for a while.  Thankfully, the page is sorted from yesterday to ancient, meaning from most recent to least recent.  And for those who don't know - I see the scrolling has increased in speed, hopefully to finish by the end of the podcast.



For those who don't know, Mozilla's Firefox Monitor site performs the same sort of checking that Troy Hunt's "Have I Been Pwned" site offers, where registered email addresses are cross-referenced against the database of all previous datasets obtained from website breaches.  Troy's facility offers a feature that I appreciate as the owner of GRC.com.  Once I authenticate my ownership and control over the GRC.com domain, "Have I Been Pwned" will perform a wildcard search for any and all email addresses within the GRC.com domain.  So *@grc.com, you know, any email address that was presumed to be from GRC.com ever.  



But where breach notification is concerned, in my opinion there's nothing wrong with having more than one such solution.  So what Firefox is doing, what Mozilla is doing, is welcome.  This new dedicated Firefox Monitor breach listing page is a bit breathtaking to behold.  For example, there were two site breaches on March 31st, a couple weeks ago, one of a site called "Sundry Files" and, ironically, a site named "Leaky Reality" had a site leak, which is probably not the reality that they were intending to be leaking, whatever that is.  In both cases, what was leaked were email addresses, IP addresses, passwords and usernames.



Before that was a breach on March 24th of "TheGradCafe," which lost email addresses, genders, geographic locations, IP addresses, names, passwords, phone numbers, physical addresses and usernames.  Whoops.  Before that, on March 11th, the site "Shopper+" lost its visitors' dates of birth, email addresses, genders, names, phone numbers, physical addresses, and spoken languages.  And on the same day, HDB Financial Services was doubtless embarrassed to have lost control of its clients' dates of birth, email addresses, genders, geographic locations, loan information, names, and phone numbers.



So it really is quite eye-opening to scroll back through this listing to get a sense for just how continuous and frequent these breaches are.  Just those I just quoted were the last couple weeks.  And we don't talk about them here every week because it would be information overload and because no one specific site breach would be useful to most, if any, of our listeners.



But I strongly recommend that everyone who's listening to this podcast take a minute or two to check out Mozilla's page.  And it certainly makes sense to get your email addresses registered there so that you will be notified if or when your name pops up in a breach.  Up to five different email addresses may be registered per Mozilla account.  And I have an account at Mozilla since I'm a Firefox user.  It's free, and there's no downside to it.  And to me that seems like a no-brainer, to get your email addresses registered there.  Once again, monitor.firefox.com is the site you want to go to.



Okay.  We talked about this last week, or a variant of this.  Joining Italy's weird ban on ChatGPT, which is what we talked about last week, we have Canada's privacy watchdog now launching an official investigation into OpenAI's ChatGPT service.  The Canadian officials say they launched the investigation after a complaint alleging that the service was non-consensually collecting personal data, which is exactly what the Italians were worried about.  And I suppose this is mostly a case of maybe like a bright light suddenly being shown on something that had been going on, unnoticed and unremarked, for quite a while.



You know, we talked about this last week, and I commented that, you know, as far as we know, ChatGPT is sucking in publicly available data only, in exactly the same way that Google spiders the web and indexes all publicly available data.  So the thing that's a little bit unnerving about ChatGPT is you can have a conversation with it, and that seems to put it in a different class than a more passive Google search.  Anyway, it does appear that the industry's AI chatbots are going to need to start paying a little more attention to whom they're chatting with because there have been some saber-rattling by privacy advocates saying, you know, you shouldn't be having inappropriate conversations with 13 year olds.  And now it becomes important to know the age of the person on the other end of the chatbot.



And Jason, in honor of you, my cohost on the podcast, who is also one of the hosts of TWiT's All About Android podcast, we have three quick bits of Android news, two of which you are already up to speed on because, again, you're the Android bot.  So Samsung's line of 2019 smartphones has formally, last month, reached their end of life and will therefore no longer be receiving security updates.  March 2023 was the last security patch level for devices which include the Galaxy S10, S10+, and the Galaxy S10e.



JASON:  It's a good series of phones.



STEVE:  Yeah.



JASON:  I remember.



STEVE:  And this is the point, too.  This doesn't mean the end of the world; right?  But it's just a note that if any of our listeners may still be using a four-year-old phone who are also concerned about remaining current and, as I put it, hooked up to the life support IV line of constant security patches, occasionally some of which are critical, it might be time for a hand-me-down of that device to someone who is less worried than a Security Now! listener, and think about upgrading your device to something current that will get reattached to life support so that you can go forth with confidence.



JASON:  Hand that problem off to somebody else.



STEVE:  That's right.



JASON:  Although I should mention, this patch does contain the fix for the Exynos modem zero-day that you talked about in Episode 915.  That was March 21st, just a couple weeks ago.  So that's a good, like, parting gift, I suppose.



STEVE:  Exactly.



JASON:  It's like, well, we fixed that really serious thing, at least.  Here you go.



STEVE:  Google also has a couple policy changes.  They've moved to restrict the amount of personal data which loan apps may gather from Android users.  Although this new policy took effect on April Fools' Day, it's no joke.  You know, that's April 1st, of course.  According to the new rules, loan apps can no longer access photos and user contacts.  Google's policy change follows reports of some loan app makers engaged in predatory behavior, such as harassing borrowers and threatening to expose their private communications - this is unbelievable - and photos unless they paid their loans or agreed to higher interest rates.  Wow.



JASON:  Yeah.  Super scummy.



STEVE:  Yeah.  I don't think you want to get a loan from a phone app.  That seems like a bad idea.



JASON:  No.  And, I mean, this isn't Google's only efforts to kind of curb this going on.  Not too long ago they did a ban on apps offering annual interest rates that exceed 36%.  That's the kind of predatory behavior that's going on with a lot of these apps.  So don't look to these kinds of apps as the purveyors of respecting privacy and basically humanity.  You know, it seems like a lot of times these kind of apps can be all about, you know, how do we make the most amount of money and in whatever way is necessary.  And when it comes to data harvesting on devices, it's really easy to get access to a lot of this information that could be useful for them to shame you into paying down the line and just kind of scummy behavior.



STEVE:  So it's not going to be Luigi coming to break your kneecaps, but still...



JASON:  No, your kneecaps are intact.



STEVE:  You don't want the app to have this personal data.  Wow.



JASON:  Yeah, no.



STEVE:  So finally, Google also announced that in the future all Android apps which allow users to create accounts of any kind will by early 2024 also need to allow users to delete their accounts and any associated data; that makers must honor requests to delete accounts either directly through the app as you're getting ready to delete it, or through a web dashboard.  The web dashboard requirement allows the data of an already removed app to also be deleted without the app first needing to be reinstalled in order to request the deletion.  And this new requirement, as I said, will enter into effect sometime in early 2024.  So another good thing as Android continues to march forward.



JASON:  Yeah, absolutely.  Without something like this, it's really, you know, it was the Wild West as far as like how you get this information deleted.  You know, do you contact the developer directly if you have no way of otherwise canceling the account and everything?  I guess the flipside to this is, and the question that I have is, okay, great.  Give them an easy way to delete this information.  But again, the cynical side of me is like, but how do we know that random developer 5,000,042 out there has actually deleted that information?  What does that mean, to "I delete your account"?



STEVE:  Yes.



JASON:  Did you delete reference to it, or did you actually delete the information?  Did you delete and scrub the information?  What does it mean?



STEVE:  The user interface now has a button.



JASON:  Yes.



STEVE:  Did you connect it to anything?



JASON:  Right.  Yeah, you told us you don't want your account anymore.  Noted.  What's the next step?



STEVE:  It's like the doorbell that doesn't ring anything.  It's like, I'm pushing the doorbell.



JASON:  Button in the elevator that you push to close the door, and it's there to pacify you more than anything.



STEVE:  Uh-huh.  Okay.  So one last piece of non-security-related news, but something near and dear to my heart, and I know a lot of our listeners who are also SpinRite fans.  The guy that I quoted last week, Matthew Hile, his is the tweet that I shared reminding me that Microsoft's Exchange Server plans would have the beneficial effect of forcing older Exchange servers to upgrade, which I certainly acknowledge, although I still think it's slimy and extortion.  He also shared a recent experience that he had with SpinRite.  His tweet last week noted that he was a listener from the start and that he had also been a development tester of SpinRite.



So Matthew wrote.  He said:  "After hearing on Security Now! from the user that ran Level 3 on an SSD," he said, "I noticed the same slow response at the start of my 500GB Samsung SSD 860 EVO."  Matthew then showed the five-point benchmark performed by ReadSpeed.  For those who don't know, ReadSpeed was an early offshoot of the work on SpinRite's new device drivers.  And I created it in order to allow users to verify that the device drivers were working on their systems.  And benchmarks are very popular.  The DNS Benchmark that I created in 2010 is the most downloaded thing that I've ever made.  It's like 2,000 downloads a day now, and I think I quoted something like eight-something million downloads total just because people want to know about the speed of things.



Well, anyway, so ReadSpeed takes benchmarks at five locations on a drive:  the beginning, the middle, the end, and then at the one-fifth and four-fifth positions.  So zero-fifths, one-fifth,  two-fifths, three-fifths, and, wait, five fifths, so maybe it's fourths.  Anyway.  So he cited the before benchmark numbers:  457.2, 511.8, 520.9, 543.3, and 543.3.  So we could see that the end of the drive, both of the final two spots were reading at 543MB per second.  But the beginning at 457 was way slower.  But, like, wait, this is an SSD.  It's solid-state.  It's like, you know, RAM; right?  And then the next spot went from 457 to 511, still not 543.  And then the third spot was up to 520.  And again, still not 543.



Okay.  So then he ran a Level 3 pass over this very nice 500GB, half a terabyte, Samsung 860 EVO drive.  After doing that he re-ran the ReadSpeed benchmark and, in the same tweet, reported his findings:  542.7, 542.4, 542.3, 542.5, 542.4.  Flat access.  Even the front, that went from 457.2, is now at 542.7, the same speed as the end.  All of the drive is now responding equally fast.



And then he finished, saying:  "Even better, it seems to have stopped my frequent blue screens," he said, "(which still occurred after a Level 2 pass)."  And that makes sense because that's a read-only pass.  He said:  "After weeks of troubleshooting, I was at the point of seriously considering blowing Windows away and starting with a new copy.  So glad to avoid that draconian effort."  Okay, now, blowing Windows away and reinstalling it would have solved the problem.  But a 60-minute Level 3 pass with the alpha release of SpinRite, which is what he used, certainly saved him a ton of time, and he didn't have to reinstall anything, because it was the rewriting of the drive's data that's what was needed.



What's happening inside our SSDs is closely related to the Rowhammer-style DRAM problems we keep being dragged back to through the years, which are exhibited by today's ultra-high-density DRAM.  In both cases of DRAM and SSD, it's necessary to appreciate that the density of the storage cells and the size of the feature details etched into these chips of both technologies are absolutely, absolutely as small and tiny as they can possibly be.  It's a competitive world.  So if it were possible for those feature details to be any smaller, and thus for the devices to have a higher bit density while the devices still function, they would be smaller; right?  I mean, no one's giving anything away.  These things are as dense as they possibly can be.



So just as with DRAM, where the engineers were pressured to push it perhaps a bit too far, SSD technology has a widely known problem known as "Read Disturb."  If you google "Read Disturb," you'll find out all about it.  I did google "Read Disturb" just now, and the top result was a description from the ACM, the Association for Computing Machinery.  The description reads:  "Read disturb is a circuit-level noise in solid-state drives (SSDs), which may corrupt existing data in SSD blocks and then cause high read error rate and longer read latency."



Again, "high read error rate and longer read latency," which is exactly what Matthew was seeing.  Error correction is not perfect.  There's a limit to how much error can be corrected, and there's also a statistical probability that a particular set of bit errors will slip past undetected, which explains the blue screens that Matthew was occasionally seeing.  The reason the front of Matthew's SSD was so much slower was that that's where the operating system files are stored.  So they're being read over and over and over, and much less often, if ever, being rewritten.



Over time, the electrostatic charges which are stored in the SSD's bit cells drift away from their proper values due to the Read Disturbance caused by all of the adjacent reading activity going on.  It got to the point where Matthew's SSD was always needing to work much harder to read some of the data whose bits had drifted further from their proper values.  They always needed to be corrected through multiple rereads, varying thresholds, and more extensive error correction.



GRC's ReadSpeed benchmark, which samples the read performance of those five regions with highly repeatable accuracy, saw this difference.  And the cure for this was simple:  Simply read and rewrite the troubled data to reset the drifting bit values back to their proper states.  But the problem with doing that, as we know, is that writing fatigues SSDs.  It's not something you want to be doing all the time.  And you certainly don't want to do it if you don't need to.  Which brings us to the reason I've become so excited about what we discovered earlier in this work on SpinRite, which is that given the proper technology, this can be detected and fixed in a highly targeted fashion.



Unfortunately, the current SpinRite doesn't have the architecture to do this.  DOS and real mode doesn't provide the sensitivity for the fine-grained read performance measurement we need.  SpinRite will need to use the power of protected mode to detect the precise instant when writes are occurring in memory as the data is read back into that memory.  And that's where I'm headed as fast as I can get there with SpinRite 7.



But today's SpinRite 6.1, which is at least able to run at the maximum speed that mass storage media can go, can perform a full drive rewrite.  That's one of the several things that SpinRite's Level 3 does.  When benchmarking, and a drive's flaky behavior begins to show that it might be needed, doing that  and maybe only on the front of the drive, which could have been done - is still a lot better than waiting for the drive's data to degrade to the point of system failure.  And in the future, SpinRite will be able to locate the exact trouble area and perform a selective rewrite of only those spots that need it.  So SpinRite 6.1 first, and then it's on to SpinRite 7.



JASON:  All right.  Main event time.  The main event.  Dangerous interpretation.  Tell us all about it, Steve.



STEVE:  Okay.  So anyone who's been following this podcast for a year or two or, you know, 18...



JASON:  Who's counting?



STEVE:  That's right, will have encountered our frequent observation - actually, toward the beginning I think it probably wasn't yet an observation.  It grew over time when we kept seeing the same problems occurring over and over and over, which was the inherent security dangers created by interpreters.  It's a recurring theme here because the act of interpretation means following instructions to perform some sequence of actions.  What we typically see when an interpreter's security vulnerabilities are analyzed is that the interpreter's designer inevitably assumed that valid instructions would be received for their precious little interpreter to read.



JASON:  Of course.



STEVE:  But it turns out that's not always the case.  And when not, bad things happen.  A research paper was recently submitted for presentation during the upcoming 32nd USENIX Security Symposium.  That paper described the frankly amazing work that was done by a pair of researchers at the University of Texas at Austin with the help of another researcher at Oberlin College.  The interpreter in question, which these three set their sights on, is one that every one of us is surrounded by with multiple instances of throughout our day and our daily lives.  And that's video and thumbnail creation, specifically the ubiquitous H.264 video codec.



Although the title of their paper is intended to capture people's attention, the content of their paper shows that the title is not overblown.  The paper is titled "The Most Dangerous Codec in the World:  Finding and Exploiting Vulnerabilities in H.264 Decoders."  And one thing I want to point out at the outset is that, almost without exception, most of the research papers we discuss here talk about hidden and unsuspected problems that were found and then fixed.  Not so this time.  The problems this tool is designed to unearth are generally well known, but are too widespread and ubiquitous to have all been found.  The authors offer several convincing proofs-of-concept case studies, but I fear that since the tool is now open sourced on GitHub for anyone to use, it won't just be the good guys who are motivated to leverage its power for finding exploitable flaws across the industry's video decoding interpreters.  I have a bad feeling about this.



Their paper leads with this Abstract.  They wrote:  "Modern video encoding standards such as H.264 are a marvel of hidden complexity.  But with hidden complexity comes hidden security risk.  Decoding video in practice means interacting with dedicated hardware accelerators and the proprietary, privileged software components used to drive them.  The video decoder ecosystem is obscure, opaque, diverse, highly privileged, largely untested, and highly exposed - a dangerous combination.



"We introduce and evaluate H26FORGE" - clever, right, it's the H.264 decoder, so H26FORGE - "a domain-specific infrastructure for analyzing, generating, and manipulating syntactically correct but semantically spec-non-compliant video files.  Using H26FORGE, we uncover insecurity in depth across the video decoder ecosystem, including kernel memory corruption bugs in iOS, memory corruption bugs in Firefox and VLC for Windows, and video accelerator and application processor kernel memory bugs in multiple Android devices."



Okay.  So when they're talking about a "domain-specific infrastructure for analyzing, generating, and manipulating syntactically correct but semantically spec-non-compliant video files," they're saying that because the bugs that might be, and turn out to be, resident within H.264 decoders might be buried deep in the multilayer decoding process, the much more common and much easier practice of simply fuzzing won't work here.



As we know, fuzzing is the common practice of throwing mostly random crap at something, a codec or an API or whatever, to see whether anything that might be sent can result in a crash.  And then, once the nature of that crash is understood, the next question is whether it might be exploitable to obtain a more useful outcome than a simple crash.  The trouble is, H.264 decoding is so complex that random crap won't make it through the front door.



That's what they meant when they said that their FORGE would generate "syntactically correct but semantically spec-non-compliant video files."  In other words, it looks like and is an entirely valid video file.  It follows all of the rules and can be processed properly.  But it's also a Trojan horse file.  It appears completely correct and valid so that it can get into the inner sanctum where the real vulnerabilities may lie.



Here's a bit more from their paper to describe and set up the situation and the environment.  They explain:  "Modern video encoding standards are a marvel of hidden complexity.  As SwiftOnSecurity noted, the video-driven applications we take for granted would not have been possible without advances in video compression technology, notwithstanding increases in computational power, storage capacity, and network bandwidth.  But with hidden complexity comes hidden security risk.



"The H.264 specification is 800 pages long."  It is the densest stuff you have ever read.  And that's 800 pages long "despite specifying only how to decode video, not how to encode it.  Because decoding is complex and costly, it is usually delegated to hardware video accelerators, either on the GPU or in a dedicated block on a system-on-a-chip.  Decoding video in practice means interacting with these privileged hardware components and the privileged software components used to drive them, usually a system media server and a kernel driver.  Compared to other types of media that can be processed by self-contained, sandboxed software libraries" - like rendering a web page - "the attack surface for video processing is larger, more privileged, and," they said, "as we explain below, more heterogeneous," meaning each instance is different.



"On the basis of a guideline" - and I got a kick out of this because they're quoting something we talked about on the podcast.  "On the basis of a guideline they call 'The Rule Of 2,' the Chrome developers try to avoid writing code that does no more than two of the following:  parses untrusted input, is written in a memory-unsafe language, and runs at high privilege."  Right?  One would be not, well, none would be good.  One is okay.  Two, eh.  But never do all three.  Never parse untrusted input in a memory unsafe language at high privilege.



And then these guys go on to say:  "The video processing stack in Chrome violates the Rule of 2, and so do the corresponding stacks in other major browsers and in messaging apps because the platform code for driving the video decoding hardware on which they all depend itself violates the Rule of 2."  So, right, if you're going to call upon a component that is violating the Rule of 2, is doing all three of those things, none of which are good, then so are you.



"Because different hardware video accelerators require different drivers, the ecosystem of privileged video processing software is highly fragmented.  Our analysis," they wrote, "of Linux device trees revealed two dozen accelerator vendors.  There's no one single dominant open source software library for security researchers to audit."  As, for example, there was for OpenSSL, which got audited because there was only one of those.



"And the features," they write, "that make modern video formats so effective also make it hard to obtain high code coverage testing of video decoding stacks by means of generic tools.  Consider H.264, the most popular video format today.  H.264 compresses videos by finding similarities within and across frames.  The similarities and differences are sent as entropy-encoded syntax elements.  These syntax elements are encoded in a context-sensitive way.  A change in the value of one syntax element completely changes the decoder's interpretation of the rest of the bitstream."



Okay, now, think about that.  "A change in the value of one syntax element completely changes the decoder's interpretation of the rest of the bitstream."  This means that in working to trigger a flaw, that flaw might only present itself if the decoder is in a particular state which is determined by everything that has come before.  So it's clear why a sophisticated pseudo video file generator had to be built in order to find the bugs which only manifest when all of the planets are in proper alignment.  Elsewhere in their paper they put their FORGE into context by explaining how it compares with other tools, which fall short, that the researchers attempted to use in the past.



They said:  "H26FORGE maintains the recovered H.264 syntax elements in memory and allows for the programmatic adjustment of syntax elements, while correctly entropy-encoding the adjusted values.  No prior tool is suited to this task.  Most software that read H.264 videos - for example, OpenH264 and FFmpeg - focuses on producing an image as quickly as possible, so they discard recovered syntax elements once an image has been generated.



"Tools used to debug video files, like Elecard's StreamEye, do not allow the programmatic editing of syntax elements; they focus on providing feedback to tune a video encoder.  FORGE can be used as a standalone tool that generates random videos for input to a video decoder.  It can be programmed to produce proof-of-concept videos that trigger a specific decoder bug identified by a security researcher, and it can be driven interactively by a researcher when exploring 'what-if?' scenarios for a partly understood vulnerability."



At one point they begin to explain in some detail about H.264.  That is, okay, this is video file format itself.  And they say:  "The H.264 video codec was standardized in 2003" - so it's 20 years old - "by the International Telecommunication Union (ITU) and the Motion Picture Experts Group (MPEG).  Because of this joint effort, this codec bears two names.  It's called H.264 by the ITU, and AVC when provided by MPEG."  Then they explain that "We default to H.264 when possible."



They said:  "The specification describes how to decode a video, leaving encoding strategies up to software and hardware developers.  Video encoding is the search problem of finding similarities between and within video frames, and turning these similarities into entropy-encoded instructions.  The H.264 spec describes how to recover the instructions and reproduce a picture."



Okay.  Then they get way into the weeds with the details of H.264 encoding.  I imagine that Alex Lindsay would probably love it.  At least there would be lots of terms that he would have encountered through his years of working with this video format.  But getting into that here doesn't serve our purpose of wanting to understand what they found and how they found it.  Suffice to say that they explain about things like YUV color space, 16x16 macroblocks, slices, prediction, deblocking, residues, profiles, levels, syntax elements, entropy encoding, encoded value organization.  And then they finish by talking about additional H.264 features and extensions.



It's quite clear that in order to write something like they wrote, this H26FORGE - which is, by the way, now posted on GitHub for anyone to experiment with - these guys had to really and truly deeply understand an insanely complex data encoding system that's described by an 800-page specification.



Okay.  So now let's get to the crux of the matter, which is the decoding pipeline.  Had I dragged everyone through that detailed description of H.264 components, what I'm going to share next would actually make some sense.  But it's not going to because we don't need to really understand it.  But I still want to share their overview of the decoding pipeline because everyone will get a good sense of just how insanely complex the world's H.264 decoders are.  We take it for granted.  You don't want to have to write one.  Here's just one paragraph that will give everyone a sense for the decoding process.



They wrote:  "First, the decoder is set up by passing in an SPS and a PPS with frame and compression-related properties.  Then the decoder receives the first slice and parses the slice header syntax elements.  The decoder then begins a macroblock-level reconstruction of the image.  It then entropy decodes the syntax elements and passes them to either a residue reconstruction path or through a frame prediction path with previously decoded frames.  Then the predicted frames are combined with the residue, passed through a deblocking engine, and finally stored in the DPB, where the frames can be accessed and presented."  Right.  Piece of cake.  And then it does most of that again for the next frame, and so on.  And by the way, that DPB stands for  Decoded Picture Buffer, which serves as both the output of the decoder and as an image reference for subsequently decoded frames to refer back to.



Okay.  So by now everyone should have a good sense for what's required to really and truly get to the bottom of vulnerabilities that may exist in any H.264 family codec.  And I have to say that I wasn't super happy, was not super happy to read that they'd open sourced all of this work and dropped it onto GitHub.  On the one hand, this will make it available to other researchers, and also to vendors of these technologies.  And all that's for the good.  But that also means that bad guys also now have it, and they might well be more motivated to take advantage of it than anyone else.



Okay.  So exactly how vulnerable are we, collectively, we who inhabit the world?  They explain:  "A wide range of software systems handle untrusted video files, providing a broad attack surface for codec bugs. An important observation is that hardware-assisted video decoding bypasses the careful sandboxing that is otherwise in place to limit the effects of media decoding bugs.  Popular messenger apps will accept video attachments in messages and provide a thumbnail preview notification.  In the default configuration of many messengers, the video is processed to produce the thumbnail without any user interaction, creating a zero-click attack surface.



"There are many examples of video issues on mobile devices.  Android has had historical issues in its Stagefright library" - remember all that we talked about on the podcast - "for processing MP4 files.  Video thumbnailing and decoding constitutes an exploitable attack surface in Apple's iMessage application, despite the BlastDoor sandbox.  Third-party messengers can also be affected.  In September, WhatsApp disclosed a critical bug in its parsing of videos on Android and iOS.



"Web browsers have long allowed pages to incorporate video to play through the video HTML tag, leading to multiple vulnerabilities in video decoding.  For example, both Chrome and Firefox were affected by a 2015 bug in VP9 parsing.  Later we describe a new vulnerability we found in Firefox's handling of H.264 files.  Despite this track record, more video processing attack surface is being exposed to the web platform.  Media Source Extensions (MSE) and Encrypted Media Extensions (EME) have been deployed in major browsers.  The WebCodecs extension, currently only deployed in Chrome, will allow websites direct access to the hardware decoders, completely skipping over container format checks.



"Modern browsers carefully sandbox most kinds of media processing libraries, but they call out to system facilities for video decoding.  Hardware acceleration is more energy efficient.  It allows playback of content that requires a hardware root of trust, and it allows browsers to benefit from the patent licensing fees paid by the hardware suppliers."  Meaning the browsers don't need to pay the fees because they're not using the technology, the platform is, allowing us to have free browsers.



"Video transcoding pipelines, such as YouTube and Facebook, handle user-generated content, which may contain videos that are not spec-compliant.  This could lead to denial of service, information leakage from the execution environment or other processed videos, or even code execution on their cloud-based platforms."  



Okay.  What about hardware video decoding?  That's a big issue, too.  They wrote:  "Video decoding in modern systems is accelerated with custom hardware.  The media IP" - okay, and they use the term "IP" here a lot, IP as in Intellectual Property, not an IP address.  "The media intellectual property (IP) included in SoCs (systems-on-a-chip) or GPUs is usually licensed from a third party.  In one notable example, iPhone SoCs through the A11 chip include Imagination Technologies' D5500 media IP, as do the systems-on-a-chip in several Android phones we study, with very different kernel drivers layered on top.



"IP vendors build drivers for their hardware video decoders, which are then called by the OS through their own abstraction layer.  The drivers will prepare the hardware to receive the encoded buffers through shared memory.  While Stagefright is Android's media engine, Android uses OpenMAX (OMX) to communicate with hardware drivers.  OMX abstracts the hardware layer from Stagefright, allowing for easier integration for custom hardware video decoders.



"Other operating systems similarly have their own abstraction layer.  The Linux community has support for video decoders through the Video for Linux API v2.  Similar to OMX, it abstracts the driver so user space programs do not have to worry about the underlying hardware.  Windows relies on DirectX Video Acceleration 2.0, and Apple uses VideoToolbox.  Intel also has its own Linux abstraction layer called the Video Acceleration API; and similarly, Nvidia has the Video Decode and Presentation API for Unix."



They said:  "We list 25 companies we found that have unique video decode intellectual property IPs.  Some of these may license from other companies, or may produce their own video codec IP.  The companies include providers for Single-Board Computers, set-top boxes, tablets, phones, and video conferencing systems.  Some video decode IP companies describe providing drivers and models for incorporating the IP into systems on a chip.  We highlight all of these companies to showcase the heterogeneity of available hardware video decoders, and thus the potential for vulnerabilities to exist within or across products."



Okay.  And finally in this paper we get to the threat model.  They wrote:  "In this paper we assume an adversary who, one, produces one or more malicious video files; and, two, causes one or more targets to decode the videos.  Delivering videos to the user and having them decoded, with or without user interaction, is easy to accomplish in many cases.  This is the minimal set of capabilities an adversary needs to exploit a vulnerability in decoding software or hardware.  For information disclosure attacks, the adversary must be able to read frames of decoded video.  For malicious videos delivered via the web, for example, this can be accomplished via JavaScript."



Okay.  So, H26FORGE was written by this team in around 30,000 lines of Rust code, and has a Python scripting backend for writing video modification scripts.  It has three main components:  input handling, syntax manipulation, and output handling.  The input handling contains the H.264 entropy decoding.  Syntax manipulation has functions for modifying recovered syntax elements or generating test videos.  And output handling has the H.264 entropy encoding, which outputs videos.



Okay.  The best way to describe what they found would be to say, unfortunately, that everywhere they looked they found problems.  Many of them were readily exploitable.  And here are some examples.  They said:  "We found two bugs in the AppleD5500 kernel extension.  The first bug enables a partly-controlled heap memory overwrite.  The second bug causes an infinite loop and leads to a kernel panic.  These bugs have been confirmed, patched, and assigned CVEs by Apple.  We verified that they can be triggered by a web page visited in Safari.



"Through reverse engineering of the H.265 decoder in the AppleD5500 kernel extension for iOS 15.5, we discovered what appeared to be a missing bounds check potentially leading to a heap overflow in the H.265 decode object.  To verify this, we modified H26FORGE with enough H.265 tooling to produce a proof-of-concept video that causes a controlled kernel heap overflow.  Unlike the previously described bugs, we were able to trigger this bug only when playing a video, not through preview thumbnail generation."  Meaning that in the other bugs, yes, showing a preview thumbnail was enough to take over the system.



"Apple assigned this bug CVE-2022-42850 and patched it in iOS and iPadOS version 16.2.  By overwriting a pointer with the address of a fake object that itself points to a fake virtual table, we can arrange to have any address of our choosing called in place of a legitimate destructor.  We did not attempt to develop an end-to-end exploit chain; however, Apple's assessment was that this bug, like the first bug, may allow" - and, you know, "may" - "allow an app to 'execute arbitrary code with kernel privileges.'



"We tested generic videos on Firefox 100 and discovered an out-of-bounds" - now, when they say "generic," they mean videos they generically developed using their FORGE - "and discovered an out-of-bounds read that causes a crash in the Firefox GPU utility process and a user-visible information leak.  The issue arises from conflicting frame sizes provided in the MP4 container, as well as multiple SPSes across video playback.  Note that both the crash and information leak are caused by a single video.  To exploit this vulnerability, an attacker has to get the victim to visit a website on a vulnerable Firefox browser.  We reported this finding to Mozilla, and it has been assigned a CVE and patched in v105.



"On VLC for Windows v3.0.17, we discovered a use-after-free vulnerability in FFmpeg's libavcodec that arises when interacting with Microsoft Direct3D 11 Video APIs.  We found this by testing generated videos in VLC.  The bug is triggered when an SPS change in the middle of the video forces a hardware re-initialization of libavcodec.  If exploited, an attacker could gain arbitrary code execution with VLC's privileges.  We reported the issue to VLC and FFmpeg, and they have fixed it in both.



"We tested the videos produced by H26FORGE on a variety of Android devices with varying hardware decoders.  In doing so, we found issues that span different hardware manufacturers, and more serious vulnerabilities in hardware decoders and their associated kernel drivers.  To target a breadth of video decode intellectual property, we went with older, cheaper systems-on-a-chip; but note that some of our findings impact newer MediaTek devices, as well.  And the videos produced by H26FORGE can be used to test new and future devices."



In reporting the problems they discovered, most mainstream publishers like Apple and Google and Samsung and so on were quite responsive.  But in some cases, when they needed to get in touch with a vendor who only OEMs chips to other major customers, they received no answer to their repeated attempts to make those companies aware of the vulnerabilities that exist in their intellectual property.



They summarized their work finally by writing:  "We have described H26FORGE, domain-specific infrastructure for analyzing, generating, and manipulating syntactically correct, but semantically spec-non-compliant video files.  Using H26FORGE, we have discovered, and responsibly disclosed, multiple memory corruption vulnerabilities in video decoding stacks from multiple vendors.



"We draw two conclusions from our experience with H26FORGE.  First, domain-specific tools" - like what they created - "are useful and necessary for improving video decoder security.  Generic fuzzing tools have been used with great success to improve the quality of other kinds of media-parsing libraries, but that success has evidently not translated to video decoding."  The point being otherwise there wouldn't still be all these problems everywhere we looked.



They said:  "The bugs we found and described have been present in iOS for a long time.  We have tested that our proof-of-concept videos induced kernel panics on devices running iOS 13.3, released back in December of 2019; and iOS 15.6, released recently, in July of 2022.  Binary analysis suggests that the first bug we identified was present in the kernel as far back as iOS 10, the first release whose kernel binary was distributed unencrypted," so they were able to analyze it somewhat.



"We make H26FORGE available at github.com/h26forge/h26forge under an open source license.  We hope that it will facilitate follow-up work, both by academic researchers and by the vendors themselves" - I hope so, too - "to improve the software quality of video decoders."



Second finding:  "The video decoder ecosystem is more insecure than previously realized.  Platform vendors should urgently consider designs that deprivilege software and hardware components that process untrusted video input."  Again, should urgently prioritize deprivilege.  "Browser vendors have worked to sandbox media decoding libraries, as have message app vendors, with the iMessage BlastDoor process being a notable example.  Mobile OS vendors have also worked to sandbox system media servers.  These efforts are undermined by parsing video formats in kernel drivers.



"Our reverse-engineering of kernel drivers suggests that current hardware relies on software to parse parameter sets and populate a context structure used by the hardware in macroblock decoding.  It is not clear that it is safe to invoke hardware decoding with a maliciously constructed context structure, which suggests that whatever software component is charged with parsing parameter sets and populating the hardware context will need to be trusted, whether it is in the kernel or not. It may be worthwhile to rewrite this software component in a memory-safe language, or to apply formal verification techniques to it.



"An orthogonal direction for progress, albeit one that will require the support of media IP vendors, would redesign the software-hardware interface to simplify it.  The Linux push for stateless hardware video decoders is a step in this direction.  Similarly, encoders that produce outputs that are software-decoder friendly, such as some AV1 encoders, help reduce the expected complexity of video decoders."



Okay.  So the entire industry has just received the gift of an extremely impressive, powerful, and flexible new tool for generating correctly formatted yet subtly defective test videos for the purpose of finding and perfecting exploitable flaws in pretty much all current H.264 video decoders.  I remain more than a little bit worried that bad guys are going to jump on this and use it to locate powerful new vulnerabilities that can be turned into zero-click exploits which they will obviously not be reporting to the device's manufacturer.



The problem is, as always, one of motivation and economics.  Not much imagination is required to picture the NSO Group pouncing on this to enhance their next-generation of Pegasus smartphone penetration spyware.  And we learned just last week that the NSO Group has a couple dozen also-ran wannabe competitors, selling essentially, or trying to, essentially the same thing.  What was it, Greece, I think it was, that was like entertaining bids from some couple dozen of these competitors?  Everyone is going to be in on this, that is, in on this H26FORGE tool.  And the problem is that they're highly motivated to use it since there's a pot of gold at the end of the development of a new successful exploit.



So I wonder who's going to be that motivated over on the good guys' side?  Perhaps, since the discovery of one of these would be of tremendous value, a substantial bug bounty would be awarded for a powerful zero-click remote device takeover exploit.  At the end of today's show notes I have the link to the GitHub page, although anybody who heard it can find it:  github.com/h26forge/h26forge.  And also a link to their entire paper.  Believe it or not, I skipped most of it, even while sharing a bunch of it.  The link to the PDF is also here.  And I'm sure you can find it over on h26forge.



So anyway, as I said, most of the times that we're talking about a research paper, we're talking about something that was found and was fixed, and now it's safe to mention it.  I guess these guys went ahead because it's probably never going to be safe to mention this.  This is really bad.  I mean, we're talking about - when they talk about the heterogeneity of this, they're saying that fixing - it's not possible to fix the H.264 decoder because there isn't "the" H.264 decoder.  Everybody's got their own.  And so that means everybody's got their own bugs.  And that means that it's the guys targeting the attacks that will be able to target a specific device, go find a bug for that device that's known to be vulnerable in that device, and use it to exploit that person's handset.  As I said, not good.



JASON:  Not good, Steve.  Playing a video, is that inherent upon any of this actually...



STEVE:  No.



JASON:  No.  So it's not just a matter of turn off autoplay so videos don't play automatically.



STEVE:  Right.  You would have to turn off thumbnails, too.  And as far as I know, that's not even an option.  The exploit that Apple patched because it was a remote code execution was a thumbnail in iMessage.  And just showing the thumbnail was enough to exploit the phone.



JASON:  Oh, man.  That is brutal.  Well, okay.  So the Internet is lost.  We're done.  We'll just shut it all down.



STEVE:  Go back to Costa Rica, Jason.  It's better there.



JASON:  I'll stay a little bit more sane when I'm not exposed to the security news because it can be...



STEVE:  But don't leave till next week.  I need you back.



JASON:  I won't go back yet.  I've got to be here next week, and then I will go promptly back to the beach.  And all the security stuff can happen around me.  Steve, love your breakdowns of this information.  I've got to admit sometimes, and especially with this story, trying to follow along like, okay, you know what, I don't totally and completely understand this information as much as the people who wrote the report.  But I trust you.



STEVE:  And hopefully I've broken it down enough to get a good sense for it.



JASON:  Absolutely.  Absolutely.  And that's what you do so well.  So Steve, thank you so much, as always, for doing this.  Anybody who wants to follow Steve online and everything that  Steve is up to, just go to GRC.com.  You can find all sorts of Steve goodness there.  SpinRite, of course, which he talked about some excellent progress with SpinRite, which is the best mass storage recovery and maintenance tool out there.  You can get your copy at GRC.com.



Copyright (c) 2023 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.






GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#919

DATE:		April 18, 2023

TITLE:		Forced Entry

HOSTS:	Steve Gibson & Jason Howell

SOURCE:	https://media.grc.com/sn/sn-919.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  So what happened with last week's Patch Tuesday?  Was there anything of note?  If we took a quick overview of just a tiny bit of last week's news, what would that look like, and what would those stories all have in common?  What new developer-centric service is Google making freely available for the good of the open source community?  What moves are WhatsApp making to improve the security for the world's most popular secure messaging system?  What happens when a European psychotherapy clinic apparently doesn't care enough to provide even minimal security for the patients' records?  And finally, in this week's deep dive, we're going to answer the question:  What could researchers have found inside a piece of the NSO Group's Pegasus smartphone spyware that actually terrified them, and why?



SHOW TEASE:  It's time for Security Now! with Steve Gibson.  I'm Jason Howell, filling in for Leo Laporte one last time, I promise you.  He's going to be back next week.  But we have a lot of fun talking about the security news this week.  Google's free Assured Open Source Software service, Steve talks all about that.  Microsoft Patch Tuesday kind of feels like Patch Tuesday in perpetuity.  And in reality that's probably exactly what it is.  We check in on a European psychotherapy clinic that failed to provide minimal security for their patients' records, and the consequences attached to doing so.  And Steve Gibson takes a closer look at the ForcedEntry exploit for iPhones.  It's really fascinating.  That's next on Security Now!.



JASON HOWELL:  This is Security Now! with Steve Gibson, Episode 919, recorded Tuesday, April 18th, 2023:  Forced Entry.



It's time for Security Now! with Steve Gibson and - Leo is still not here.  I'm Jason Howell, sitting in for Leo.  But Steve, you are always the constant on this show, and we appreciate that about you.



STEVE GIBSON:  Glad to be back with you, Jason.  Glad you're here.



JASON:  Thank you.



STEVE:  Holding down the fort at your end.  And yes.



JASON:  Doing what I can.



STEVE:  Certainly glad to be back.  Okay.  So an interesting episode, I think, for us.  This is Security Now! #919 for the 18th, of April; right?  I got the month right this time, or whatever it was I got wrong last time.  Anyway, okay.  So we're going to, as we have been, answer some questions.  What happened with last week's Patch Tuesday?  Was there anything of note?  If we took a quick overview of just a tiny bit of last week's news, just sort of a quick scan, what would that look like, and what would all those stories have in common?  What new developer-centric service is Google making freely available for the good of the open source community?  And I'm very bullish about this.  It looks really cool.



What moves are WhatsApp making to improve the security of what is the world's most popular secure messaging system, you know, theirs?  What happens when a European psychotherapy clinic apparently doesn't care enough to provide even minimal security for their patients' records?  And finally, in this week's deep dive, we're going to answer the question:  What could researchers have found - these are Google Project Zero researchers who, you know, they've been around the block a few times; right?  You know, they've seen it all.  What could they have found inside a piece of the NSO Group's Pegasus smartphone spyware that actually and truly terrified them?  And why?



JASON:  That's intriguing.  They've seen stuff.



STEVE:  And what they found, they're like, oh, goodness.



JASON:  What they found may surprise you, as clickbait headlines might have.  You're just going to have to wait to find out.  That's what we know.



Picture of the Week time.  This one's not too difficult to decipher.  You look at it for a second, you figure it out.  You wonder, did that really happen?  Really?  Somebody did that?  At least that was my experience.



STEVE:  Okay.  So for those who don't have the advantage of video or the show notes, we've actually had a variation on this before.  There was one where bolt cutters were being held captive at a hardware store with like a thick gauge steel cable that was looped through them.  But again, it's a bolt cutter that you're trying to keep from being stolen, obviously, by looping something that this thing was designed to cut through its jaws.  And so we have a variation on that this week.  I thank one of my Twitter followers for sending it.  Here we have some needle-nose pliers where down near the hinge of the jaw they've got wire cutters.  And, I mean, this doesn't even take any imagination.  You don't have to move anything.  You just have to squeeze the handle on one of these because the thin metal cord, essentially, that is looped through them is going through the wire cutter portion of these needle-nose pliers.



JASON:  I mean, it couldn't be lined up more perfectly than literally just pull the handle.  All you need to do is pull the handle.



STEVE:  And looking at it, you know, looking at it, even if you were not a thief, and you encountered this, you'd be just tempted to squeeze the handle, just to say, okay, idiots.  I'm not taking this, but you should take a lesson.  Anyway, wow.



JASON:  Yeah, I guess instead of looping it - well, no, I guess there's no real way to do that.  If it's got a wire cutter there, you've just got to come up with a different system.



STEVE:  You're in trouble.  I know that you produce the podcast every week, whether you're on-camera or not.  Did you see the picture of the bicycle that was locked to a yellow post that, like, had no top?



JASON:  Had no top; right.



STEVE:  So you just lift the bicycle off.



JASON:  I mean, I guess security in that case is like, can we slow down the majority of people who might do something?  But that's, like, pretty low effort right there.



STEVE:  Or maybe an IQ test?  Like, okay, you know, if you want to steal the bike, but you can't figure out how, you probably should not be riding a bicycle.



JASON:  But if you can, hey, free bikes.



STEVE:  That's right, yeah.  Wow.  Okay.  So last Tuesday was our monthly celebration of all things we know of - hopefully all the things we know of - that we wish were different in the details of our Windows software.  I would like to say that, to that end, we received new and improved software.  But since we never appear to be getting ahead, we're basically just treading water, hoping to keep our heads above, what that suggests is that we actually received last week software that is broken in new and different ways than what we had before.  You know, what's different is that we haven't yet discovered all of the new ways that this replacement software is broken, but we've eliminated a bunch of the ways that we knew the old software was broken.



So we'll be back next month, and the month after that, and the month after that, ad infinitum.  This will never end.  Think about that.  This shows no sign of ever ending.  It's not as if the number of problems is diminishing over time, patch counts are dropping, and there's a chance of us not needing one of those monthly rituals.  No.  I'm not really up on pop culture.  Leo typically helps me with these things.  But I think that the official technical term for this monthly dance that ultimately goes nowhere is Whac-A-Mole.



So, yes.  This month we reportedly cured 97 flaws in our software from Microsoft, including one actively exploited zero-day flaw.  There were seven flaws which received a critical rating because they allowed for remote code execution.  However, overall, nearly half of this month's 97 vulnerabilities allowed for remote code execution.  Okay.  So here's the breakdown.  Of the 97, 45 were remote code execution vulnerabilities; 20 were elevation of privilege vulnerabilities; 10 information disclosure; nine denial of service; eight security feature bypass; and six were spoofing vulnerabilities.



Okay, now, as we know, the two most potent vulnerability classes are remote code execution and elevation of privilege.  So together that was 45 plus 20.  They formed 65 out of the total 97 patched problems.  Oh, and that count is separate from the 17 newly discovered vulnerabilities which Microsoft fixed for us the week before in their Edge browser.  The zero-day vulnerability is an elevation of privilege problem in the Windows Common Log File System Driver.  This tends to be a problematic component of Windows.  We've run across problems in it before, and we hear about it too often.



The problem with Windows EoP (elevation of privilege) bugs in core component drivers, such as the common log file driver, is that they're always there.  That is, the drivers are always there, and they're in the kernel.  They're installed on every Windows computer.  Consequently, they allow attackers with few or no significant access privileges to promote themselves directly to the system account level, you know, root, which gives them free rein over the machine.  This is why elevation of privilege, which sounds less scary than remote code execution, can be at least as troublesome, and certainly at least as handy to attackers.



One of the critically rated remote code execution bugs may be a bit extra worrisome.  Microsoft disclosed a remote flaw in their Microsoft Message Queue system, known as MSMQ.  It provides a robust means for intersystem message-passing communications when absolute reliability in messaging is critical.  You know, you just don't send it and hope that it got there.  It's got, you know, delivery confirmation and receipts and all that stuff added to it.  But since it's a bit special purpose, it's not always enabled.  But when it is, it can be trouble because Microsoft gave it a 9.8 CVSS score.  Which we know it's very difficult to get, I mean, rare do you get a 10; 9.8's still way bad.



And apparently from Microsoft's description they said:  "To exploit this vulnerability, an attacker would need to send a specially crafted malicious MSMQ packet to a MSMQ server.  This could result in remote code execution on the server side."  Okay.  So we've been around long enough to know that's not good.  There's some exploitable buffer overflow-ish sort of thing where just sending a packet at the server can allow it to execute your code.  So not good.  So let's hope that MSMQ servers are entirely constrained, as they should be, within enterprise Intranets, and that none are exposed on the public Internet.  On the other hand, we know reality, too.



We now know with absolute certainty that it's not only security researchers who compare pre-patched software to post-patched software in order to reverse engineer the flaws that were fixed by the patch and thus discover the vulnerability that wasn't known until the patch was made available.  So it won't be long before bad guys are working to gain entry into enterprises that, A, have not yet applied the April patches to their border servers; and, B, have the MSMQ service publicly exposed where you almost sort of feel like they deserve what they're going to get because this really should not be, you know, there's no point, there's no reason for it to be out on the Internet.



And as a matter of fact, another worrisome critical bug also should not be out on the Internet.  It's a remote code execution flaw in Microsoft's DHCP service.  As we know, DHCP is the Dynamic Host Configuration Protocol which is the way most of the computers in the world receive their IP addresses.  It's a slick system that allows a machine that knows absolutely nothing about the network it's connected to, to emit a totally generic Ethernet broadcast packet out onto whatever network it's attached to, wired or wireless.  That packet, which essentially says, "Hey, I'm here, help," will be received by a listening DHCP server, which is then able to reply with all of the instructions and settings necessary to get that machine's IP stack configured with a unique, non-conflicting local IP address, the IP of the network's gateway so it's able to talk to the rest of the world, and the addresses of some DNS servers which are available to the hosts on the Intranet.



Okay.  So my own server network at GRC uses 100% statically configured IP addresses because there's just no need, nothing is coming and going, you know, people are not attaching to that network on an ad hoc basis, unlike at Starbucks and airports and in our homes.  At home I have a combination of static IPs and DHCP.  Some machines I want to always have at the same location; whereas things like iPhones, iPads, Rokus, you know, IoT thermostats and whatnot, they don't matter, and there's no point in tying them down.



One of the cool things that a DHCP server can be configured for is the range of IPs that it is allowed to allocate from.  So, for example, in a /24 network like most of us have, most residential users have, which typically begins with 192.168.0 or .1, the final byte can range from 0 to 254.  So a residential network's DHCP server could be configured to automatically assign IPs only from the upper range, like where the last byte runs from 100 to 254 - or I'm sorry, 253, 254 is typically the gateway - which leaves the lower 100 IPs, 0 to 99, guaranteed to always be available for manual static assignment.



In any event, DHCP server flaws have historically and understandably been a little extra frightening because, if you stop to think about it, the entire nature of the way DHCP servers operate is inherently unauthenticated.  By design, they're supposed to be there for anyone who asks for their help, without question.  This means that DHCP servers must be robust enough to accept and reply to packets from unknown, untrusted, and potentially hostile devices.  You know, they've got to be sturdy.  Fortunately, although we've seen very serious problems in the past with DHCP servers, today's problem only rates an 8.8 out of 10 since the flaw turns out to be a post-authentication problem, so different than just querying for your IP address and stuff.  That's still bad, but at least it's less so.



Microsoft said:  "An authenticated attacker could leverage a specially crafted RPC call (remote procedure call)" - so I guess that would be remote procedure call call, RPC call - "to the DHCP service to exploit this vulnerability," they said.  And being DHCP, it's going to be on the LAN, not the WAN, so exploitation of this vulnerability requires an attacker first have access to the local internal network, then have some form of authentication which is recognized by the DHCP server, and then be clever enough to craft some RPC call in order to take it over.



So we did not have, this month, any serious post-patch meltdowns, which we've covered in prior months - actually, I don't think any this year, but lots were last year.  Microsoft had a tough time, especially at the beginning of last year.  But none of those have been reported this month.  So there's no obvious reason not to proceed, update our machines to eliminate the latest batch of known problems while apparently, given all  the evidence, introduce at least as many new unknown problems.  Which, you know, we'll get to fixing eventually.  So we'll see you next month for more of the same.



Okay.  So I was perusing the news of the past week, right, to pick out those things that I thought were most interesting and worthy of some conversation.  There's a section which Risky Business News calls "Breaches and Hacks."  And as I just scanned it, none of them were gripping, none of them I wanted to go into in any detail.  But still, they just sort of had me shaking my head.  I remind myself that our listeners are only aware, at least through this podcast, of those things that rise to the top of the week's news which, you know, seem worthy enough for mention, you know, and some further exploration and, you know,  deeper dives here.  But that can have the effect of inadvertently obscuring what the broader view looks like of just how much nonsense is continually going on out there in the wider world.



So Risky Business's tag line is "It's a Jungle Out There."  So I wanted to share briefly what I encountered in just this one short section covering the past week's news under the topic of "Breaches and Hacks."  So I'm just going to read the headline and the short little blurb, just to give you a sense for it.



So NCR gets ransomwared:  "NCR, the world's largest banking and payments software maker, has confirmed that a recent data center outage was caused by a ransomware attack.  According to NCR, the incident has impacted the availability of its Aloha POS payments platform.  The intrusion was claimed by the AlphV ransomware gang, which temporarily listed the company on its dark website."



Also, cyberattack on irrigation systems:  "A cyberattack is suspected of having disrupted the operations of irrigation systems in Israel's Upper Galilee region.  Water controllers for irrigating fields in the Jordan Valley and control systems for the Galil Sewage Corporation were down last week during yearly cyberattacks aimed at Israeli targets known as OpIsrael.  While several hacktivist groups announced their participation in OpIsrael this year, the identity of the attacker remains unknown, the Jerusalem Post reported."



Also, Hyundai data breach.  "Hyundai's Italian and French branches have leaked customers' details.  Exposed data includes email addresses, telephone numbers, physical addresses, and vehicle chassis numbers."  MSI ransomware attack: "Taiwanese hardware vendor MSI has confirmed a security breach after the Money Message ransomware gang claimed to have breached and encrypted some of the company's systems."



Hundred Finance crypto heist:  "The Hundred Finance platform was hacked for $7.4 million following a flash-loan attack."  GDAC crypto heist:  "South Korean cryptocurrency platform GDAC was hacked earlier this month for $13 million worth of assets."



Bitrue crypto heist:  "The Bitrue cryptocurrency exchange says it was hacked and lost $23 million worth of assets following an attack on one of its hot wallets on April 14th.  Bitrue says it's currently investigating the incident and doesn't know how the attack took place."



Yearn Finance crypto heist:  "A threat actor exploited a bug in the Yearn Finance platform to steal $11.6 million worth of cryptocurrency assets."  And finally, Terraport crypto heist:  "The Terraport DeFi platform was hacked for $4 million worth of crypto assets."  



Okay.  What a week.  19 years ago, when Leo first proposed this podcast to me, none of this sort of nonsense was going on.  You know, we had cute little email macro viruses that liked to send out email to all of the contacts in someone's address book.  You'd receive a bizarre email from your mom and call her up and say "Uh, Mom.  Unless we have a Nigerian cousin that you've never told me about, I think that maybe your computer has a virus."  You know, and that was pretty much the extent of it; right?  Simpler times.



But thinking about that brief news blurb summary that I just shared, you know the one thing that most of them have in common?  The thread that winds and weaves through them all is cryptocurrency.  That's today's common factor.  Partly it's because it enables ransoms, payments which are now facilitated by the inherent anonymity offered by cryptocurrency.  But even more so, it's the cryptocurrency itself.  The software being used to implement cryptocurrency systems is apparently, given the evidence, no better than the software that does anything else.  But rather than just getting the font wrong when an item is selected in a word processor, when a hole is found in cryptocurrency software, millions of dollars of actual tradable fiat currency drain out through that hole.  And where there's money, there's bad guys.



JASON:  That's exactly it.  Where there's money, there's bad guys.  Yup.



STEVE:  Yeah.  Okay.  So this is cool.  We're in need of some good news at this point, and we have some courtesy of Google.  Last Wednesday the 12th, Google announced that their so-called Google Assured Open Source Software service would now be generally available to all.  Here's what Google posted and explained.



They said:  "Threats to the software supply chain and open source software continue to be major areas of concern for organizations creating apps and their developers.  According to Mandiant's M-Trends 2022 report" - so from last year - "17% of all security breaches start with a supply chain attack, the initial infection vector second only to exploits."



They said:  "Building on Google's efforts to improve open source security, we're announcing the general availability of the Assured Open Source Software" - which they call Assured OSS - "service for Java and Python ecosystems.  Available today at no cost, Assured OSS gives any organization that uses open source software the opportunity to leverage the security and experience Google applies to open source dependencies by incorporating the same open source software packages that Google secures and uses in their own developer workflows."



Wow.  Okay.  So in other words, just to be clear, this new service will provide free use of the same vetted and carefully reviewed Java and Python packages that Google themselves use internally for their own applications.  Google explained:  "Using Assured OSS, organizations can obtain their OSS packages from a trusted and known supplier; know more about their ingredients with Assured Software Bill of Materials provided in industry standard formats like SPDX and VEX; reduce risk as Google is actively scanning, finding, and fixing new vulnerabilities in curated packages; increase confidence in the integrity of the ingredients they're using through signed, tamper-evident provenance; and choose from more than 1,000 of the most popular Java and Python packages, including common machine learning and AI projects like TensorFlow and Pandas.



"Since our public preview announcement in May of last year (2022), and integrating Assured OSS as a key component in Software Delivery Shield the following October, we've received an overwhelmingly positive response and interest from our customers."  They then cite one.  Jon Meadows, managing director and Citi Tech Fellow, Cyber Security at Citi said:  "Citi has been an advocate and active leader in the industry's efforts to secure enterprise software supply chains.  Both Citi and Google see untrusted and unverified open source dependencies as a key risk vector.  This is why we've been excited to be an early adopter of Google Cloud's new Assured OSS product.  Assured OSS can help reduce risk and protect open source software components commonly used by enterprises like us."



Okay.  So Google said:  "Assured OSS guards OSS packages against attacks and risk by continuously mirroring key external ecosystems to manage end-to-end security without creating forks; managing the security and integrity of the mirrored repos and end-to-end build tool chain with tamper-evident provenance and attestations; continuously scanning for, fuzz testing, and fixing critical vulnerabilities, which are then quickly contributed back upstream to limit the exposure time and blast radius; and, finally, operating a critical patching team to support covered packages."



Another early adopter of this, of Google's, is Melinda Marks, a senior analyst with ESG.  She was quoted saying:  "As organizations increasingly utilize OSS for faster development cycles, they need trusted sources of secure open source packages.  Without proper vetting and verification or metadata to help track open source software access and usage, organizations risk exposure to potential security vulnerabilities and other risks in their software supply chain.  By partnering with a trusted supplier, organizations can mitigate these risks and ensure the integrity of their software supply chain to better protect their business applications."



Okay.  And then Google finishes:  "There are significant security benefits to Assured OSS adopters and the larger community from the curation process.  Since our Assured OSS team curated the first 278 packages, we have been the first to find 48% of the new vulnerabilities (CVEs).  Each of these CVEs has been fixed and upstreamed."



So to me, this seems like all good news.  I do have the sense for, like, those who have a cross to bear, an axe to grind with Google, that this might cause free software open source purists to blow a gasket if they sense any sort of effort to in any way privatize or taint the openness and freeness of open source software.  But we all know too well now that public open source software registries are under constant and growing attack.  They are being polluted with similarly named, deliberately malicious packages, and people are downloading them.  So while the idea of having an open community of like-minded contributors is terrific, the reality is that not everyone is like-minded.  So I, for one, think that it makes all kinds of sense to have access to a vetted and curated source of open source software.  So to Google I say "Bravo."  I think this is a big win.



JASON:  All right.  WhatsApp?  What's happening with WhatsApp?  What's happening?



STEVE:  I'm not a big social media user.  As Leo often notes, it took him quite a while to get me into Twitter.  And he's not really even trying to move me over to Discord.  I think he knows that's not going to happen.  I just, you know, I don't need - I can't handle one more outlet that I need to be checking all the time.



JASON:  I understand, Steve.  Totally get it.



STEVE:  Okay.  And also the fact is I'm not moving state secrets securely.  And of course I'm skeptical that it's actually possible to do so with any of our modern mainstream technologies.  We've often joked that if you really want to tell someone a secret, go out into the middle of a football field, both of you huddle under a thick blanket, and whisper into each other's ear.  Oh, and you have to be naked so that there's no recording devices anywhere on either of you.  And then you have a chance of actually conveying a secret and not having it leak out anywhere.  Otherwise, eh, I don't think so.  Anyway, the only encrypted messaging service I bother with is iMessage, and it's just because it's there.  It really wouldn't matter much to me if it wasn't encrypted.  I have some friends who have green bubbles on my iPhone, which means they're using SMS.  And, you know, those messages fly out in the clear.



Okay.  But all that aside, when the world's leading secure messaging platform - which for better or for worse is WhatsApp - announces security improvements, it's something that we need to at least take note of.  So last week WhatsApp received three new security features.  They called them Account Protect, Device Verification, and Automatic Security Codes.  And we'll look at these things one at a time.  So here's how WhatsApp described the first one of those, Account Protect.



They said:  "If you need to switch your WhatsApp account to a new device, we want to double-check that it's really you."  Okay, good.  They said:  "From now on, we may ask you on your old device to verify that you want to take this step as an extra security check."  To which I thought, you're not doing that already?  That seems like a really good thing to do.  Anyway, they said:  "This feature can help alert you to an unauthorized attempt to move your account to another device."  As I said, that one seems kind of obvious.  We often see that when we're changing an email account, a notification is sent to both the old and the new accounts to help prevent a malicious email change.  This is the same sort of thing.  And sort of similarly, when a new trust relationship is being established, both ends are typically asked to affirm that they both wish to trust the other.



So this so-called Account Protect feature is obviously useful, but you have to wonder why it's only happening now.  And we know, right, that anything that is done to increase security will have some degree of backlash, some trouble.  You know, people have moved their WhatsApp account from device to device in the past, and the old device never asked if they wanted to lose access to it in favor of another one.  Now it's going to.  So, okay.  That'll cause some confusion.  But it sure does seem like it's worth the additional security of not, you know, suddenly discovering that WhatsApp has moved to a different device that you did not authorize.



Okay, next up we have what WhatsApp is calling Device Verification.  Describing this, they write:  "Mobile device malware is one of the biggest threats to people's privacy and security today because it can take advantage of your phone without your permission and use your WhatsApp to send unwanted messages.  To help prevent this, we have added checks to help authenticate your account, with no action needed from you, and better protect you if your device is compromised.  This lets you continue using WhatsApp uninterrupted."



Okay.  So I had to do a little more digging into that one.  This Device Verification is some backend technology designed to prevent malware which crawls into a user's phone from obtaining the user's authentication token and then impersonating them in subsequent secure messages.  Okay, now having said that, that's not actually what it does.  But exactly what's going on here isn't yet clear.



So here's how WhatsApp describes the situation.  They said:  "WhatsApp uses several cryptographic keys to ensure that communications across the app are end-to-end encrypted.  One of these is the authentication key, which allows a WhatsApp client to connect to the WhatsApp server to reestablish a trusted connection.  This authentication key allows people to use WhatsApp without having to enter a password, PIN, SMS code, or other credential every time they turn on the app.  This mechanism is secure because the authentication key cannot be intercepted by any third party, including WhatsApp.  If a device is infected with malware, however, the authentication key can be stolen."  Okay, so the point is that's different than using it locally on the app and, like, sending out messages through the client's WhatsApp app.  This is obtaining the authentication key and exfiltrating it.



So they said:  "We are primarily concerned about the popularity of unofficial WhatsApp clients that contain malware designed for this purpose.  These unofficial apps put users' security at risk, and it is why we encourage everyone using WhatsApp to use the official WhatsApp app.  Once malware is present on user devices, attackers can use the malware to capture the authentication key and then use it to impersonate the victim to send spam, scams, phishing attempts, et cetera, to other potential victims.  Device Verification," as they're calling this, "will help WhatsApp identify these scenarios and protect the user's account without interruption."



Okay, now, I'll just note, before we go any further, that WhatsApp, in writing this, is being optimistic, if not disingenuous, since serious spy malware, such as we will be looking at closely in our final topic today, is purpose-designed to do this with the official WhatsApp app.  So they say, oh, yeah, it's only the third-party apps that are like, you know, third-party WhatsApp apps that are a problem.  Clearly, the vast majority of WhatsApp users are using the WhatsApp app, and that's what the spyware's targeting.  Okay.  In any event, the problem is certainly real.  So here's how they're solving it.



They wrote:  "WhatsApp has built Device Verification to benefit from how people typically read and react to messages sent to their device.  When someone receives a message, their WhatsApp client wakes up and retrieves the offline message from WhatsApp's server. This process cannot be impersonated by malware that steals the authentication key and attempts to send messages from outside the user's device."  Okay.  That's not quite true, but when we understand what they're doing, we'll be able to sort of dissect this.



They wrote:  "Device Verification introduces three new parameters:  a security-token that's stored on the user's device; a nonce that is used to identify if a client is connecting to retrieve a message from WhatsApp server; and an authentication challenge that is used to asynchronously ping the user's device."  They said:  "These three parameters help prevent malware from stealing the authentication key" - again, that's not what it does, but okay - "and connecting to WhatsApp server from outside the user's device.  Every time someone retrieves an offline message, the security token is updated to allow seamless reconnection attempts in the future.  This process is called 'bootstrapping the security token.'"  Okay.  And giving it a fancy name doesn't make it fancy.



"Every time a WhatsApp client connects to the WhatsApp server, we require the client to send us the security token that's on their device.  This allows us to detect suspicious connections from malware that's trying to connect to the WhatsApp server from outside the user's device.  An authentication challenge is an invisible ping from the WhatsApp server to a user's device.  We only send these challenges on suspicious connections.



"There are three possible responses to the challenge.  Success:  The client responds to the challenge from the connecting device.  Failure:  The client responds to the challenge from a different device.  This means the connection being challenged is very likely from an attacker, and the connection will be blocked; or, finally, no response:  The client doesn't respond to the challenge.  This situation is rare and indicates that the connection being challenged is suspicious.  We retry sending the challenge a few more times.  If the client still doesn't respond, the connection will be blocked."



Okay.  And then they finish by adding:  "Device Verification has been rolled out to 100% of WhatsApp users on Android and is in the process of being rolled out to iOS users.  It enables us to increase our users' security without interrupting their service or adding an additional step they need to take.  Device Verification will serve as an important and additional tool at WhatsApp's disposal to address rare key theft security challenges.  We will continue to evaluate new security features to protect the privacy of our users."



Okay.  So here's what's actually going on.  What they've described, when you sort through and read out the mumbo-jumbo, is a simple, but certainly useful protocol for detecting when two physically separate clients are both checking in for messages.  The idea is that every WhatsApp client will now be maintaining some state, a transient state, in the form of this new token which they have received from the WhatsApp server.  It's probably just a random nonce.  That's all it needs to be.  It was the random nonce that was most recently received from the WhatsApp server when the client last connected.  This nonce will be changed by the server and sent every time a WhatsApp client connects to transact messages.  And at the start of each new connection, which after all the clients initiate, the client will return the last nonce it received to the server.



Okay, clearly, if only one client is ever doing this for a given account, the server will always receive the nonce that it had previously sent to the client for that account.  But if a user's WhatsApp identity authentication has been stolen and is being used by a physically separate client, then two clients will both be returning nonces, and only one of them will have the nonce that was most recently sent to that user's account by the server.  As soon as the client that didn't most recently connect to the WhatsApp server does so, it will provide an obsolete nonce to the server, and the server will know that something's not right.  So, yeah, it's a clean and simple solution to this cloned client problem.  Again, not that big a deal, and certainly a good thing to do.  Again, one wonders why this wasn't already in there, but it's good that it is now.



And lastly, this third feature they call Automatic Security Codes.  They describe it this way.  They said:  "Our most security conscious users have always been able to take advantage of our security code verification feature, which helps ensure you're chatting with the intended recipient.  You can check this manually by going to the encryption tab under a contact's info.  To make this process easier and more accessible for everyone, we're rolling out a security feature based on a process called 'Key Transparency' that allows you to automatically verify that you have a secure connection.  What it means for you is that when you click on the encryption tab, you'll be able to verify right away that your personal conversation is secured."  Okay, now, once again, in kind of this touchy-feely boilerplate overview, they don't really explain what's going on, and they gloss over some of the, like, important stuff.



I read all the way through WhatsApp's description of this thing, what they're calling Automatic Security Codes.  And you can thank me later for not dragging you through it.  What it amounts to is a public, auditable, append-only - meaning nothing can ever be deleted; immutable is the other fun phrase for that - immutable log of the use of WhatsApp user account public keys.  Now, the WhatsApp app already allows users to display, share, and verify their public keys with either a QR code or by verifying a 60, six zero, digit number.  You know, which is each endpoint's public key.



But doing that requires that both parties arrange to interact in real time, preferably face to face, with their phones to exchange QR codes.  And as we know, this was a feature that Threema, the secure messaging app, has used since its inception.  In Threema you had like a little stoplight.  You had green, yellow, and red.  Or I guess better put it would be red, yellow, and green, where green is the highest level of verification which is only obtainable if each phone gets shown to the phone that you're peering with, and they're able to exchange their identities, like literally face to face.



Okay.  So what WhatsApp is now creating under the name Key Transparency is a global public directory of user accounts and their matching public keys which will allow one-sided verification of the public keys of a user's contacts on WhatsApp.  So, you know.  And that's the key, and that's certainly useful.  The point is that, in the encryption tab on WhatsApp for one of your WhatsApp contacts, you could, if you clicked it, you got a QR code or a 60-digit number for that contact's public key.  So you're seeing that.  But in order to verify it, so all you know is you've got a public key; right?  You don't know that it's actually theirs.  In order to verify it, you both need to be available in real-time where your contact is able to read out their public key, and you verify that it matches the key that you believe is their public key.  So thus a double-sided verification requirement.



What WhatsApp has done now is they're creating a global automation accessible, immutable append-only reference for all of the public keys of all of the WhatsApp users.  So the users' experience is that now they will be able to press a button to verify the public key of any of their contacts.  The contact gets looked up in this public key directory, the so-called Key Transparency Directory, and it's verified with the phone on a one-sided verification, not needing the other side.  So it's another nice feature improvement, you know, in a way that makes sense.  And, you know, good.  I'm glad they have it.  Again, WhatsApp is the number one global leader in secure messaging.  By far and away they are in the lead, thanks to Facebook's prevalence.  So it needs to be easy to use, and it needs to be as secure as possible.



Okay.  The last thing I wanted to share before we get into the thing that terrified Google's Project Zero folks is a follow-up to a previous story.  Remember back near the beginning of February of this year, it was Episode 909, Leo and I talked about a particularly astonishing and horrifying data breach which had occurred at a psychotherapy clinic in Europe.  Here's how I described it then.



I said:  "The news was that French authorities have detained a 25-year-old Finnish national who's accused of hacking the Vastaamo Psychotherapy Center.  For reasons we'll see," I said, "this hack of Vastaamo is considered to be one of the worst in the country's history.  Okay.  Now, it occurred back in 2018 and 2019, so I guess this kid was, what, 20 years old then, when he allegedly stole the personal medical records of the clinic's patients and attempted to extort the clinic.  To put pressure on the company, the hacker leaked extremely sensitive client files on the dark web.  When that failed, he sent emails with ransom demands to more than 30,000 of the clinic's patients, asking them each for 200 euros and threatening to publish their medical records if they did not pay up."  To which Leo replied:  "Oh, boy."



And I continued, saying:  "Uh-huh.  Finnish authorities formally identified the hacker in October last year when they issued a European arrest warrant for his arrest, and they detained him last week."  Okay.  And I said:  "Okay, so this is brazen and bad; right?  The hacker obtained extremely sensitive personal medical information and chose to use it to extort both the clinic and its past patients, all 30,000 of them.  And it was that number of files and patient histories that raised my eyebrows," I said, "30,000.  Okay.  No matter how large and busy this clinic might be, they cannot be currently treating 30,000 patients.  And in fact there are 260 working days a year, five times 52.  So if the clinic averaged 10 new patients per day, which seems like a high-side number, 30,000 patient records would be 11.5 years' worth of patient files at the rate of 10 per day."



Okay.  So that was in early February.  We have an update.  For one thing, the Finnish psychotherapy clinic is now bankrupt.  What a surprise! That's what'll happen when apparently the entire past hyper-confidential history of your clients is publicly exposed to the Internet.  Who's going to make an appointment after that?  So the clinic is gone.



What's a bit more interesting is that the ex-CEO of the clinic also faced criminal charges.  And it's difficult to feel sorry for him since, as we know, mistakes can happen, right, by mistake; but policies do not happen by mistake.  They happen by policy.  So someone needed to be held accountable for this clinic's online preservation of the past records of 30,000 patients, which, you know, as I said back in February, if you really have to keep it for, like HIPAA-equivalent regulations, okay.  But take it offline.  Don't leave it on, you know,  exposable, accessible to the Internet, where it was.  It turns out that this kid, this 25 year old, didn't hack into their network.  He used a password in order to access the database over the Internet.  So that's just unconscionable.



Okay.  In the process of digging around within the facts of this data breach, it came to light that in addition to failing to take this sort of data security, like any sort of data security precautions that any medical patient would reasonably assume were in place, and that the law would both expect and require to have in place, the clinic's CEO knew about his company's sloppy cybersecurity for up to two years before the blackmail which took place in 2020.



It turns out that those earlier attacks that I mentioned in 2018 and 2019 were separate events that were previously known.  They happened under this guy's watch.  The clinic suffered several previous data breaches, as I said, in 2018 and 2019, and never reported them, presumably hoping that no traceable cybercrime would arise as a result, and thus the company would never be held to account.  Of course, the final breach was what in the security industry we call "a doozy."  So there was no sweeping that one under the rug.  When you send email to 30,000 previous clients asking them for 200 euros each or their private psychotherapy records will be made public, that's not good.



Okay.  Moreover, current breach disclosure and data protection regulations, such as our favorite GDPR in Europe, make it very clear that data breaches can no longer simply be ignored with the hope that no one will find out.  No.  They must be promptly disclosed for the greater good of all.  So the final news from Finland is that our arguably criminally negligent CEO was, in fact, found by the courts to be criminally negligent and has been convicted and given a prison sentence.  And one wonders whether he'll have the cell next to the young hacker who stole his data.



The hope is that this will serve to remind other business leaders that merely promising to look after other people's personal data is not good enough.  You actually have to at least be able to demonstrate that you tried to do so.  And that clearly did not happen in this case.  And the fact that after multiple breaches, no improvements were made in the clinic's security posture.  Nothing was done to fix those previous problems.  So, yeah, it's good, as we know, that people are beginning to be held accountable because nothing will change otherwise.



Okay, Jason.  One last mention of why we're here, and then we're going to look at ForcedEntry, the thing that the Project Zero guys at Google discovered that really did keep them awake at night.



JASON:  That may surprise you, dot dot dot.



STEVE:  Terrified.



JASON:  I keep going to that because apparently it works for clickbait.  All right.  We've done a lot of really big setup for this main event here.  I'm super curious to know what they've found because it doesn't sound good.



STEVE:  It's not good.  Okay.  So, but it's way interesting, so that's of course the criteria for it making the podcast.



JASON:  Yeah.



STEVE:  My original title for today's podcast was "KingsPawn."  Now, that's capital P, not capital S, not KingSpawn, KingsPawn.  That's the name that's been given to a powerful piece of spyware being offered by one of those Israeli spyware purveyors other than the NSO Group who, as we know, offer now their "too well known for comfort" Pegasus spyware to various governments.  But while doing the background legwork for that story, I ran across the fascinating technical details of another earlier piece of Spyware that was used by both this second group, who we will be talking about soon, and by NSO Group with Pegasus.  I knew our listeners would find the technical details of this very interesting.  So I decided to push our discussion of KingsPawn to next week, unless something even more juicy comes up.  But we'll get to it.



Today, I want to take us through what Google's Project Zero team discovered about their so-called "ForcedEntry" exploit that has been successfully deployed by several malware vendors to gain entry into Apple iPhones.  Citizen Lab, which is at the Munk School of Global Affairs and Public Policy at the University of Toronto in Canada, provided Google's Project Zero team with a sample of the ForcedEntry exploit; and Apple's Security Engineering and Architecture group collaborated with Google in working through exactly how it was possible for a GIF thumbnail image to take over any iPhone, despite all of Apple's many layers of protection which work to make that impossible.  So what we had here with ForcedEntry was the holy grail of exploits providing attackers with remote code execution via a zero-click iMessage.



Citizen Lab somehow arranged to capture an NSO Group-generated, iMessage-based, zero-click exploit while it was being used to target a Saudi activist.  At the time that Project Zero wrote about this, they said:  "Based on our research and findings, we assess this to be one of the most technically sophisticated exploits we have ever seen, further demonstrating that the capabilities NSO provides rival those previously thought to be accessible only to a handful of nation states."



And of course what makes this so disturbing is that this is for sale, so that it's not only a handful of nation states that now have access to this sort of technology.  Any qualified government - and some unqualified governments, unfortunately - can now purchase this advanced targeting malware.  Okay.  So here's how Project Zero described, just for reference, the NSO group.  They wrote:  "NSO Group is one of the highest profile providers of 'access-as-a-service,' selling packaged hacking solutions which enable nation-state actors without a homegrown offensive cyber capability to 'pay-to-play,' vastly expanding the number of nations with such cyber capabilities.



"For years," they wrote, "groups like Citizen Lab and Amnesty International have been tracking the use of NSO's mobile spyware package Pegasus.  Despite NSO's claims that they 'evaluate the potential for adverse human rights impacts arising from the misuse of NSO products,' Pegasus has been linked to the hacking of the New York Times journalist Ben Hubbard by the Saudi regime, hacking of human rights defenders in Morocco and Bahrain, the targeting of Amnesty International staff, and dozens of other cases.



"The United States has added NSO to the 'Entity List' to severely restrict the ability of U.S. companies to do business with NSO, and stating in a press release that 'NSO's tools enabled foreign governments to conduct transnational repression, which is the practice of authoritarian governments targeting dissidents, journalists, and activists outside of their sovereign borders to silence dissent.'"



And they finished:  "Citizen Lab was able to recover these Pegasus exploits from an iPhone, and therefore this analysis covers NSO's capabilities against iPhone.  We are aware that NSO sells similar zero-click capabilities which target Android devices.  Project Zero does not have samples of these exploits; but if you do, please reach out."



Okay.  So in previous cases analyzed, targets were sent links,  you know, like links in SMS messages.  And while, as we know, these sorts of phishing-style attacks are often successful, individuals at the highest risk levels who are probably more technically savvy, or have received training about, like, where not to click, are less apt to click on something that doesn't really look absolutely 100% legitimate and pass a very stringent smell test.  So although a one-click exploit might work, the holy grail is the exploit that requires zero action on the part of the target.  The exploit just works silently in the background.



Short of not using a device, there really is no way to prevent exploitation by a zero-click exploit.  It's a weapon against which there is no defense.  Since the initial entry point for Pegasus on iPhone using this attack, that is, this ForcedEntry attack, is iMessage, this means that a victim can be targeted just using their phone number or their Apple ID username, which is typically not difficult to find or obtain for high-profile individuals.



Okay.  iMessage offers native support for GIF images which can be directly sent and received in iMessage chats.  They show up in the chat window's chat log timeline.  As Leo and his Discord denizens are all too aware, GIF images can also be animated by causing the GIF to contain a series of frames which are displayed in succession, creating animation.  Apple wanted to make those GIFs, which don't naturally loop, loop endlessly, rather than play through only once.



So very early in the incoming iMessage parsing and processing pipeline, after a message has been received, but well before the message is displayed, iMessage calls a method in the IMTranscoderAgent process to which it passes any image file it receives with the extension .gif.  The method called is named "IMGIFUtils," with "CopyFromPath" and "ToDestinationPath" parameters.  As suggested by the selector's name, the intention was presumably to just copy the GIF file before editing the loop count field to make the GIF repeat endlessly.  The method uses the Core Graphics APIs to render the source image to a new GIF file at the destination path.  And although the source filename does need to end in .gif, that doesn't mean it's actually a GIF file.



So apparently we're going to learn this lesson all over again, or at least a variation of it.  Apple's Image I/O library is used to determine the actual format of the source file, ignoring the file extension, and then parse it, completely and deliberately ignoring the file extension.  This was actually done to prevent file type confusion attacks, which we've seen in the past.  But in this instance, the protection backfired.  Thanks to the use of this "fake gif" trick, over 20 different image processing codecs are now suddenly part of the iMessage zero-click attack surface.



Unfortunately, this includes some very obscure and complex formats.  As a consequence of this unintentional chain of dependencies, hundreds of thousands of lines of code are now remotely exposed.  Remember last week's podcast, "A Dangerous Interpretation."  That was just one example of how difficult it can be to get complex codecs exactly right.



Okay.  So what codec do you imagine the NSO Group chose to invoke with their "fake GIF in an iMessage" attack?  Would you believe a PDF?  Yes, believe it or not.  The unintended chain of dependencies in iMessage's processing of incoming GIF images allowed external remote malicious parties to send anyone a PDF - but it's a PDF with a .gif extension; right? - which the receiving iPhone would then attempt to parse and display.  For how many years were we, as an industry, dealing with malicious PDFs?



JASON:  Yeah.



STEVE:  You know?  Wow.



JASON:  Lesson learned.



STEVE:  Oh, my god.



JASON:  Or so we thought.



STEVE:  Yes, exactly.  Wouldn't you think?  So as we know, PDFs were one of the more popular exploitation targets due to the file format's ubiquity and the complexity of its - wait for it - its interpreter.  And on top of the difficulty of rendering a document image from a typesetting specification, which is what PDFs are, PDFs later acquired the ability to interpret JavaScript, which is where we all intone in unison:  "What could possibly go wrong?"



Okay.  Fortunately, Apple's Core Graphics PDF parser does not appear to interpret JavaScript.  So we can be thankful for that.  But those evil geniuses - and as you'll soon see, I'm not using the term "genius" casually.  I rarely use the term.  The evil geniuses at the NSO Group found something sufficiently powerful lying inside the Core Graphics PDF parser.



Turning the clock way back to the late 1990s, we can all remember a time when bandwidth and storage were much more scarce commodities than they are today, and when it made much more sense to actually code in assembly language.  I'm just saying.  But back then, a compression standard known as JBIG2 was created.  JBIG2 had a very narrow and very specific purpose, but it was very good at it.  It was an image codec designed to highly compress monochrome images where pixels were only black and white, and where there was an extremely high degree of redundancy of what was being compressed.  It was developed to achieve extremely high compression ratios for scans specifically of text documents, and was implemented and used in high-end office scanner and printer devices.  If you were to use the direct "Scan to PDF" feature of such a machine, a PDF file would be produced which was mostly just a thin PDF wrapper around a JBIG2 compression stream.



As it happens, the PDFs produced by those scanners were exceptionally small, often being on the order of only a few kbytes.  Okay.  You might ask, how is this possible?  There were two novel techniques which JBIG2 used to achieve its extreme compression ratios, which are relevant to the exploitation we're talking about here.



The first technique was known as Segmentation and Substitution.  Okay.  Segmentation as in looking at the individual black blobs on the page, meaning the text characters; and substitution as in let's not compress all of them.  Let's just compress one of them and then point to the others.  If you think about it, a text document contains a bunch of text.  And in a language such as English, having a relatively small alphabet, the same character, like "s," "t," and "e," will appear all over the page.  Believe it or not, JBIG2 actually segments documents to be compressed into their individual glyphs and uses a simple pattern recognition matching to match and collect all of the glyphs that look almost the same.



Then, rather than storing all of those near-duplicate glyphs, JBIG2 pretends that they are all the same and replaces all of them with just one.  This replacement of all occurrences of similar-looking glyphs with a copy of just one often yields a document which remains entirely legible and enables very high compression ratios since the compressor just needs to store the coordinates of each of the same-looking glyphs rather than the glyphs themselves each time.  The output remains perfectly readable, but the amount of information to be stored is significantly reduced.



Again, rather than needing to store all the original pixel information for the whole page, we only need one compressed version of the "reference glyph" for each character shape, and the relative coordinates of all of the places where its copies should be placed.  This means that the decompression stage treats the output page like a canvas onto which it "paints" that one copy of the glyph at each of the stored locations.



There is one big problem with such a scheme.  It's possible for a poor encoder to accidentally get confused and exchange similar-looking characters.  This explains why the format fell out of favor and became obscure.  It does not explain, though, why Apple never thought to remove its decompression codec from their Image I/O library.  Most of Apple's core PDF decoder appears to be Apple's own proprietary code; but the JBIG2 implementation is straight from Xpdf, whose source code is freely available.  This means that the NSO Group effectively had the full source code for an extremely complex and not well vetted interpreter, to which they were able to provide any amount of data and cause to be invoked on any unsuspecting target's iPhone.



The vulnerability that was found in the code and exploited was a classic integer overflow, which occurs when the code processes a deliberately malformed document containing a JBIG2-compressed image description.  This flaw allows the contents of the image to be executed as code, allowing the attackers total freedom to inject and run any code of their choosing.  The only problem is that the messages sent remain on the target's device.  And it was the persistence of the attacking code, that is, on this Saudi individual's phone, which they were able to provide to Citizen Lab, that triggered the unraveling of this entire scheme.



Okay, now, the extent to which the NSO Group went in their implementation of this vulnerability is somewhat astonishing because the rendering of this was sandboxed by - I can't remember the name of it, Blockade or something, that Apple has that sandboxes this.  So they needed a sandbox escape.  And doing that is what upped the ante for this entire attack.



Since replacing all of an image's text with identical duplicate versions of just one master reference character will result in an inherently lossy compression, right, because the decompressed image, that is, the reconstructed, essentially, image will not be identical to the original; right?  Because one instance of "e," the character "e," has been used everywhere.  So it's close, but it's not identical.  So there was an additional mode added to JBIG2 to allow for lossless compression.  An error mask would be created to represent the difference between the original image and the lossy super-compressed image.  And it turned out that this error mask was not that large, either.



However, this capability, the capability of an error mask, gave the decompressor the ability to perform XOR operations since the mask was an XOR mask which was able to flip individual bits in the image as needed to make the final output identical to the original input.  This meant that the decompressor's interpreter had access to AND, OR, XOR, and XNOR logical operators.  And it turns out that, given those four operations, any computable function can be computed.  It is, as we say, "Turing Complete."



JBIG2 does not itself have scripting capabilities.  But when combined with a vulnerability which they found, it does have the ability to emulate circuits of arbitrary logic gates operating upon arbitrary memory.  So these genius attackers must have thought, okay, why not just use that to build an entire computer architecture from scratch and script it?  And believe it or not, this is exactly what the NSO Group's ForcedEntry exploit does.



By using over 70,000 JBIG2 interpreted segment commands defining logical bit operations, they defined, they essentially built an entire small computer architecture with features such as registers and a full 64-bit adder and comparator which they then use to search the iPhone's memory to perform arithmetic operations.  It's not as fast as Javascript, but who cares?  It's fundamentally computationally equivalent.



Okay.  So here's what the Project Zero guys wrote to describe their amazement and their reactions to what they discovered once they had reverse engineered this exploit.  They wrote:  "The bootstrapping operations for the sandbox escape exploit are written to run on this logic circuit, and the whole thing runs in this weird, emulated environment created out of a single decompression pass through a JBIG2 stream.  It's pretty incredible and, at the same time, pretty terrifying."



So from what I've just described, it's clear why the Project Zero guys, who are, you know, they're immersed in and live this sort of stuff day and night, they've seen it all, nevertheless they've never seen anything like this.  They described this as the single most sophisticated attack they had ever encountered.  On the one hand it amazed them, as it would anyone.  And it also terrified them to think that there are people out there somewhere who are not only willing to go to these lengths, but who have the capability to do so.  This is a tour de force in fundamental computer science.  These people built a working emulation of a 64-bit computer, replete with registers and math capability, out of the pixels being manipulated by a JBIG2 image decompressor.



You know, and impressive as this is as an example of astonishing computational mastery, it's also a tragedy that the entire goal of this effort, the sole purpose was to subvert iPhones belonging to journalists and human rights defenders for the purpose of illegally spying on their actions and communications.  It really seems a shame for such talent and capability to be spent toward such an end.



JASON:  Yeah.  John just whispered in my ear a very, you know, a truth that the purpose also was to sell the tools to people who wanted to do that.  So it's finding something that is of value for others; right?



STEVE:  Good point.  Very good point.



JASON:  Yeah, that is fascinating.  That is really, like, remarkable and, like they say, terrifying.  Although every time I heard you say JBIG, I was reminded that was my football nickname back in high school, JBIG.  Not really.  I didn't play football.  But it could have been.  Yeah, that's pretty awesome and crazy stuff there, Steve.



STEVE:  Wow.  So, yeah, to me, as someone who loves computing, I mean, this could be a Ph.D. thesis.  You would be Dr. Abraham or whoever if you made this thing public.  I sure hope that whoever did this got a lot of money themselves, I mean, evil as it is, and that they were appreciated for doing the impossible.  And, you know, the thing I forgot to add is that also this makes you wonder.  They would not have gone to this length if there was an easier way to skin this particular cat.



JASON:  Yes, it's a good point.



STEVE:  Which suggests, my god, the bar has been raised by all of the security infrastructure that Apple has managed to cram into the iPhone.



JASON:  If this is what you have to do to get there, then they've done a pretty darn good job.  And, you know, as you've said many, many times on this show, there is no perfect security; right?  Any system, I think, at some point there is some way in.  It's just a matter of finding it.  It turns out in this case you've got to go to great lengths.  But it really is remarkable what they were able to accomplish to make that happen.  Where there's a will, there's a way.



STEVE:  Wow.



JASON:  Yeah, pretty incredible.  Well, Steve, thank you so much for breaking that down, this and all the news.  And every single week, week in, week out, all the security news, that's what Steve is great at, putting it into terms that you and I can understand because sometimes these stories, if I was to just read the article about this, I'd be lost.  But thanks to Steve breaking it down, I have a better understanding of what's exactly going on here.  And if you find yourself in the same position, and you want to support Steve and everything that's he's doing, go to GRC.com.



Copyright (c) 2023 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#920

DATE:		April 25, 2023

TITLE:		An End-to-End Encryption Proposal

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-920.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week's look at the past week's most interesting security news answers the question of whether Apple's Lockdown Mode does anything that's actually useful.  Just how big is the market for commercial "Pegasus-style" smartphone spyware?  Why exactly has the dark web suddenly become interested in purloined ChatGPT accounts, and is "purloined" a word one uses in mixed company?  What trove of secrets did ESET discover when they innocently purchased a few second-hand routers?  And speaking of routers, what was the mistake that users of old Cisco routers really wish Cisco hadn't made, and whose fault is its exploitation today?  What's the story behind the newly established Security Research Legal Defense Fund?  Then, after a few quick update and upgrade notes, we look at two opposing open letters written about the coming end-to-end-encryption apocalypse, and consider whether I may have just stumbled upon a solution to the whole mess.  So I doubt that anyone's going to be bored this week!



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  We're going to talk about a couple of zero-click exploits discovered.  NSO Group was selling them.  Some of them might even get around lockdown mode on the iPhone.  We'll also talk about why you should wipe your routers before you sell them or give them away.  And speaking of routers, a flaw in Cisco's IOS that hasn't been patched in some older routers and why you should fix it now.  It's all coming up next, a lot more, too, on Security Now!. 



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 920, recorded Tuesday, April 25th, 2023:  An End-to-End Encryption Proposal.



It's time for Security Now!, yay, the show we wait all week for.  In my case, all month for.



STEVE GIBSON:  Guess who's back?



LEO:  I'm here, kids.



STEVE:  Yes, indeed.



LEO:  And I missed you, Uncle Steve.



STEVE:  Well, Leo, Ant and Jason did a great job of holding the fort down.



LEO:  Thank you, guys.  I really appreciate that.	



STEVE:  The one piece of news that really had me thinking of you was the news that we covered in February of that ridiculous psychotherapy clinic in, I don't remember where, in Europe somewhere - Finland, I think - where a hacker got in and stole the records of 30,000 of their previous clients.



LEO:  Oh, yeah, I remember.



STEVE:  And when the clinic didn't succumb to the ransomware threats, the hacker emailed all the clients, saying, "I've got all your data.  I need 300 bucks from each of you."



LEO:  Oh, god.



STEVE:  "Or I'm going to expose you to the Internet."  Well, the good news is the CEO is convicted and in jail.



LEO:  Oh, good.



STEVE:  So, yeah, I was thinking of you when we were...



LEO:  They take that stuff seriously there.



STEVE:  Yeah, yeah.



LEO:  I've got to tell you one thing, though, that's extremely annoying.  You think these cookie banners are bad in the states?



STEVE:  Uh-oh.



LEO:  Every page you go to it covers the page.  You can't do anything until you see this big - it's much bigger, much more elaborate, lots more clauses.  And every single place you go to you've got to click that.  So they take that law seriously, too.  And it's so stupid because what are you going to click but accept, accept, accept; right? 



STEVE:  Yeah.



LEO:  It's just dumb.  Anyway, I'm glad to be back, and I'm so glad to see you.  I did, now don't take this amiss, but I did, you know, we have an AI Leo now in our...



STEVE:  I saw that you asked ChatGPT how many - when would I be doing the last episode of Security Now!.



LEO:  I'm just trying to do some planning.



STEVE:  It was quite wonderful.



LEO:  Yeah.  I said, because as you said, you've said this again and again, 999 is the last episode because you don't have four digits in you.  And we're doing 920 today.  So I asked, first I asked ChatGPT, in fact GPT-4, the official Open AI one, to tell me, well, when will Steve's last show be?  It went on and on and on, "To calculate the date that's 79 weeks from April 25th, 2023, we follow these steps," and came up with the wrong freaking answer.



STEVE:  Well, and I love it, too, because then it begins moving month to month, trying to figure out how many days each month has.  I mean...



LEO:  And then it gives up.



STEVE:  Yeah, it's sort of amazing that it got, like, it thought it knew what it was doing.



LEO:  It's bizarre.



STEVE:  And it is the point that is worth making.  "60 Minutes" did a piece on all of this, I think it was Sunday before last.  I mean, I think it was Scott Pelley who had - it was the first half of the show.  And he was astonished by some of the things that this thing came up with, like they gave it, like, Hemingway's shortest writing, which was three two-sentence words, and asked it to expound on it or create a short story from it.  And it did this amazing job.



LEO:  Yeah.  Well, that's what it does.



STEVE:  And then someone said, "And we'd like it as prose."  And it turned it into a poem.



LEO:  Sure.



STEVE:  Lorrie had tears running down her face because it was just so eloquent and amazing.  And then, but the point is, then they said, "Give us your forecasts for the economy."



LEO:  Oh.



STEVE:  And again, it produced this beautiful...



LEO:  Very credible, yes.



STEVE:  ...fantastic-looking result.  And then said, "For additional information, you might check these five reference texts," none of which were actual.  It made them up.



LEO:  It makes stuff up, yeah, because it's just fancy autocorrect.  I did go to Wolfram Alpha, which can do math.  And just in case anybody wants to know, unless we miss an episode because of a Best Of or whatever, October 29th, 2024, will be 999.



STEVE:  Well, and you actually keep the counter, you keep the counter going through those.



LEO:  Yeah, I think we should - it should be accurate, yeah.



STEVE:  Well, yeah.  And because, I mean, I've had my count off when I forgot to add one across the holidays.



LEO:  Yeah, we fix it, yeah.



STEVE:  So it's like, uh-oh.



LEO:  We try to get it, you know, so you have 52 episodes a year.  And if that happens, it'll be, I'm sad to say, sorry ChatGPT, not August 30th, but, thank goodness, October 29th.  It'll be Halloween weekend.



STEVE:  By that point you could probably just plug it in in place of me, Leo, and just let ChatGPT, you know...



LEO:  Well, it kind of knows now.  It's got a deadline.  Yeah, get going.  So what's coming up today?



STEVE:  Okay.  So, oh, I've got something.  I've got something, I think.  So this is Episode 920 for - this is our last episode of April, believe it or not, Leo.  You missed, like, most of...



LEO:  I missed April, yeah.



STEVE:  Now, here we are, it'll be May before we know it.  Not May Day, we're going to miss that by one, but that's good.  Today's topic is An End-to-End Encryption Proposal.  I think I may have solved this whole problem with government spying and all that, and the tension between problems.



LEO:  Oh, that would be good.



STEVE:  That would be, yeah.  Anyway, we're going to get to that.  We're going to look at the past week's most interesting security news answers for the questions which have arisen, like whether Apple's Lockdown Mode actually does anything useful.  How big is the market for commercial Pegasus-style smartphone spyware?  Why exactly has the dark web suddenly become interested in purloined ChatGPT accounts?  And is "purloined" a word one uses in mixed company?



What trove of secrets did ESET discover when they innocently purchased a few second-hand routers?  Whoops.  And speaking of routers, what was the mistake that users of old Cisco routers really wish Cisco had not made, and whose fault is its exploitation today?  What's the story behind the newly established Security Research Legal Defense Fund?  And then, after a few quick update and upgrade notes, we look at two opposing open letters written about the coming end-to-end-encryption apocalypse, and consider whether I may have just stumbled upon a solution to the whole mess.  So I doubt anyone's going to be getting bored this week.



LEO:  Yeah, I can't wait.



STEVE:  And of course we do have a great picture.



LEO:  Yes, we do.  Yes, we do.  Picture time.  This is hysterical, by the way.



STEVE:  Oh, okay.  So yes.  We have a picture.  Thank you, one of our listeners, for tweeting this to me.  Very valuable.  This is a 16-port D-Link, barely recognizable router, or rather an unmanaged switch, yes.



LEO:  It's a switch; right?  A switch, yeah.



STEVE:  An unmanaged switch.  If the guy in charge of the set of the Addams Family had been overpaid, I think this is...



LEO:  This is a ceiling or the basement somewhere and just...



STEVE:  Oh, god.



LEO:  Look at this.  Ugh.



STEVE:  Yeah.  So as I said, it's barely recognizable.  I had to, like, squint to, okay, so we've got, you know, we have, on the upper side we have four - we have two groups of four ports, and I'm sure it's the same thing below.  It looks like there is a little 4x4 grid of LEDs.  And they're lit up.  A lot of them are green there.  So I gave this the caption, "By some miracle, it's still working.  Don't touch it."  Because, boy, I mean, yeah.  So I don't know where, probably like some office building, like sort of a low-end office facility where...



LEO:  It's so dusty and cobwebby, though.  I mean, that's disgusting.



STEVE:  Yeah.  And why would one ever put this in the attic where it's, you know, I mean, you can just see it's like there's some sort of a heating duct passing by in the very far lower left corner.  You sort of see the ribs of the duct.  And many a spider has hopefully set up its operations here and thought, okay, I'm going to get lucky.  It doesn't look like anything happened there except the web collected a lot of dust, you know, thus the Addams Family.



LEO:  It's ridiculous.



STEVE:  So anyway, yes.



LEO:  Oh, my.



STEVE:  Another great piece of technology that somehow is keeping the whole world online.  If the Internet goes through there, then yeah, you're in danger.



LEO:  Wouldn't that be funny if you unplugged it, and the whole Internet went down?



STEVE:  Yeah, exactly.  Turns out that's the hub.



LEO:  That's it.



STEVE:  That's the main crux, yeah.  So last Tuesday, the forensic security research group Citizen Lab reported on three iOS 15 and 16 exploits attributed to Israel's NSO Group's Pegasus smartphone spyware system.  And now, just as a reminder,  Citizen Lab is at the University of Toronto's Munk School of Global Affairs and Public Policy.  They've been doing some serious forensic work.  Last week's main topic was ForcedEntry, which for our listeners and Leo, I'm bringing you up to speed, it was a fascinating example of the details of a zero-click exploit against iOS devices which Google's Project Zero researchers reverse engineered and dissected, thanks to Citizen Lab, who found a live sample of it and sent it to Project Zero to take a look at.  They found it, Citizen Lab found it on a phone of a Saudi political activist.



So my eye was caught by Citizen Lab's mention just last week of the apparent successes of Apple's Lockdown Mode, which of course we previously described when Apple announced it.  I think it was brand new in iOS 16.  I think that's where it first appeared.  As we know, it rather significantly restricts many features of an iPhone for the express purpose of thwarting exactly these sorts of targeted attacks against high profile users of their iOS, of Apple's iOS devices.



Last Tuesday they published an extensive description of several zero-click attacks they discovered, "they" being Citizen Lab, discovered being deployed against users of iOS 15 and 16.  So not old, out-of-service version 13, or 12 and 13, 14.  These are today's phones.  I'm not going to get into all of the details of those, but their intersection with Lockdown Mode I thought was interesting.  So to give you some sense, before we get to the intersection, they summarized their finds, the things they found, in seven bullet points which are short, and so I think worth sharing.



They said:  "In 2022," so just last year, "the Citizen Lab gained extensive forensic visibility into new NSO Group exploit activity after finding infections among members of Mexico's civil society, including two human rights defenders from Centro Prodh, which represents victims of military abuses in Mexico.  Our ensuing investigation led us to conclude that, in 2022, NSO Group customers widely deployed at least three iOS 15 and 16 zero-click exploit chains against civil society targets around the world."  And just so everyone is on the same page here, "zero-click exploit" means the user does nothing.  We covered one of them in detail last week where just the receipt of an iMessage, without any acknowledgment of any kind, was enough to take over the target's phone.



LEO:  And I'm sure what this is covering was that Apple was patching these.



STEVE:  Yes, right.



LEO:  I mean, very aggressively.  I think they had three patches in a few weeks.



STEVE:  Right, right.  And they've been like, again, they're doing everything they can.  But it just seems that more problems keep being found.



LEO:  It's aggressive, yeah.



STEVE:  Yes.  So Citizen Lab said, speaking of these:  "NSO Group's third and final known 2022 iOS zero-click, which we call 'PwnYourHome,' was deployed against iOS 15 and 16 starting in October of 2022.  It appears to be a novel two-step zero-click exploit, with each step targeting a different process on the iPhone.  The first step targets HomeKit" - thus PwnYourHome - "and the second step targets iMessage."  So somehow the exploit against HomeKit primes iMessage to then be exploitable by the second step in the two-step zero-click.



LEO:  And that's not unusual.  I mean, Pwn2Own we see multiple steps sometimes.



STEVE:  Chains.



LEO:  Yeah, chains, yeah.



STEVE:  Chains, exactly, yeah.  So then they said:  "NSO Group's second 2022 zero-click, FindMyPwn, was deployed against iOS 15 beginning in June 2022.  It also appears to be a two-step exploit.  The first step targets the iPhone's Find My feature" - thus FindMyPwn - "and the second step targets iMessage."  They said:  "We shared forensic artifacts with Apple in October of  2022, and additional forensic artifacts regarding PwnYourHome in January of this year, 2023, leading Apple to release several security improvements to HomeKit in iOS 16.3.1.  Once we had identified FindMyPwn and PwnYourHome, we discovered traces of NSO Group's first 2022 zero-click, LatentImage, on a single target's iPhone.  This exploit may also have involved the iPhone's Find My feature, but it utilizes a different exploit chain than FindMyPwn."



Okay.  And so here's the final comment that I wanted, with all of that background, to share:  "For a brief period, targets that had enabled iOS 16's Lockdown Mode feature received real-time warnings when PwnYourHome exploitation was attempted against their devices."



LEO:  That's great.



STEVE:  Yes.



LEO:  That's amazing.



STEVE:  Yeah.  They said:  "Although NSO Group may have later devised a workaround for this real-time warning, even so, we have not seen PwnYourHome successfully used against any devices on which Lockdown Mode is enabled."  So props to Apple.



The first thing Apple notes when they're talking about the limitations that they impose when Lockdown Mode is enabled, they say:  "Most message attachment types will be blocked, other than certain images, video, and audio."  They said:  "Some features, such as links and link previews, will be unavailable."  And so of course we know from, as I said, our examination last week of ForcedEntry that the way entry was forced was by sending the target a PDF with the .gif file extension, which caused iMessage to attempt to render a very cleverly manipulated JBIG2 image that had been embedded in the PDF.  It seems pretty certain that with Lockdown Mode, Apple has switched to a "Default Deny" with then highly selective "Allows" being permitted.  So ForcedEntry would likely have also been nipped in the bud.



The trouble with something like Lockdown Mode, unfortunately, is that to be effective it really does need to be a restrictive service, I mean, it needs to be restrictive.  As we've seen, exploits are everywhere.  And that might annoy the people who need it the most, enough for them to turn it off due to its interference with the things they need to do.  So just this is really, of everything we've seen, Lockdown Mode hasn't been around for that long.  But the ability to find the forensic evidence of attacks which are this targeted, as highly targeted as Pegasus is, it's a very rarified environment there.  So this is neat feedback to have.  And we have fresh evidence that countries are busily using these patently illegal tools, governments.  Yeah, I know.



NCSC stands for the National Cyber Security Centre in the UK, which is exactly what it sounds like.  Last Wednesday, the center published a report titled "Cyber experts warn of rising threat from irresponsible use of commercial hacking tools over the next five years."  Which of course begs the question, what would "responsible use" be?  It's like, okay, I mean, this is all illegal; right?  And the countries, most of the countries, probably not all of them because they're certainly used in non-Democratic repressive regimes, but you have to imagine that a lot of the countries who said, oh, no, no, no, that's bad, it's like, okay, how much does that cost?  They just don't seem to be able to say no.



Okay.  Here are some selected pieces from the report, which I think provide some conclusions which serve as a useful reality check.  So the UK's NCSC said:  "The commercial proliferation of cyber tools and services lowers the barrier to entry to state and non-state actors in obtaining capability and intelligence that they would not otherwise be able to develop or acquire."  I mean, and that really is the key; right?  It's not like you need to have your own NSA-level capability anymore.  You just go to the NSO Group and say, "How much?" and you get what you need.



They said:  "The sophistication of some commercial intrusion cyber products and services can almost certainly rival the equivalent capabilities of some state-linked Advanced Persistent Threat groups. The bulk of the commercial cyber sector is highly likely focused on satisfying domestic state demand from law enforcement and government agencies."  Right?  I mean, the providers don't exist in a vacuum.  They're fulfilling a need.  They're selling this to people because people want it.



"However," they wrote, "over the last decade a growing number of enterprises have emerged offering a range of products and services to global customers.  They include off-the-shelf capability" - known as Hacking-as-a-Service - "bespoke hacking services" - Hackers-for-Hire - "and the sale of enabling capabilities such as zero-day exploits and tool frameworks.  Over the past 10 years, at least 80" - eight zero - "80 countries have purchased commercial cyber intrusion software, or spyware.  For dozens of states without a skills base, the commercial sector is almost certainly transformational, allowing cost-effective access to capability that would otherwise take decades to develop.



"While products vary in capability and application, commercially available spyware for mobile devices can offer the ability to read messages, listen to audio calls, obtain photos, locate the device, and remotely operate the camera and microphone.  Some states are likely to procure multiple commercial cyber tools to meet their requirements."  Wow.  "Devices can be compromised in a number of ways, including phishing, but also 'zero-click' attacks which do not require user interaction, making it more difficult for victims to mitigate.



"While these tools have been used by states against law enforcement targets, spyware has almost certainly been used by some states in the targeting of journalists, human rights activists, political dissidents and opponents, and foreign government officials.  This is almost certainly happening at scale, with thousands of individuals targeted each year.  While current products focus on mobile devices and intelligence gathering, as the sector grows and demand increases, products and services will likely diversify to meet demand."  I mean, we're talking about a whole ecosystem which is emerging, on the DL, being sold to governments.



They write:  "Hacker-for-hire groups carry out cyber activity for paying clients.  As well as providing information of traditional espionage value to states, hackers-for-hire are also reportedly used for legal disputes, intellectual property theft, insider trading, and the theft of other private data.  Hackers-for-hire differ in skill and capability, ranging from low-level cybercrime activity to technically complex and effective network compromises that may go undetected.  Some groups operate in criminal circles, some portray themselves as commercial companies, and others operate anonymously.



"Hacker-for-hire groups that focus on stealing information use phishing and other social engineering attacks, exploits against publicly reported vulnerabilities in computer networks, and sometimes zero-day attacks to compromise victims.  The greatest threat comes from higher end hacker-for-hire groups whose abilities and impact are similar to those of capable state actors.  Hackers-for-hire pose a potential corporate espionage threat against organizations and individuals with privileged or valuable confidential information in multiple sectors.



"While less skilled and cybercriminal hackers-for-hire almost certainly carry out Denial of Service (DoS) attacks for a fee to temporarily disrupt a target website or server on a customer's behalf, additional law enforcement attention probably deters higher skilled hackers-for-hire from conducting destructive or disruptive operations."  In other words, the really high-end guys, they're not mucking around down in the less skilled areas of DDoS attacks.  And those tend to be much more easily attributable, as we see.



"However," they said, "a growing market and the extra financial incentive raise the likelihood of hackers-for-hire accepting this type of tasking over the next five years."  So that might be changing.  "Hackers-for-hire also raise the likelihood of unpredictable targeting or unintentional escalation through attempts to compromise a wider range of targets, particularly those seeking valuable information to sell on, as opposed to 'working to order.'"  So they might be like out doing their own reconnaissance work, hacking companies just, you might say, on spec, as opposed to under contract.  They said:  "It's likely that potentially significant financial rewards incentivize state employees or contractors with cyber skills to become hackers-for-hire, risking the proliferation of cyber techniques from state to non-state actors."  So as you might expect, these skills spread over time.  They're not, you know, containment is not being maintained from where they originated.



"Historically, underground criminal markets have facilitated the exploit trade.  Since the early 2000s, a lucrative market for zero-day exploits has emerged in the commercial space.  The large sums of money involved for critical zero-day exploits for commonly used systems and processes mean opportunities for profit are significant and have driven commercialization."  And of course we've often talked about the likes of Zerodium, who are purchasing these exploits for resale.  And there's no accountability.  There's no sense for to whom they are selling them.  But Zerodium makes no bones about it.  It's like, yeah, we want to buy your exploits.  We'll pay you dollars.



They said:  "Critical zero-day exploits and vulnerabilities are almost certainly transformational to actors with the skills to make use of them.  States, or commercial cyber intrusion companies providing products to states, are the dominant customers for the commercial zero-day market and are highly likely to remain so for the next five years.  The growth of the commercial sector facilitating this trade has likely increased the number of states able to access critical zero-day capability, directly or indirectly.



"Some well-funded cybercrime groups have highly likely purchased lower priced zero-day exploits for less well-used systems from underground exploit marketplaces. However, purchasing high-cost, critical zero-day capability from the commercial marketplace is unlikely to appeal to most cybercrime groups.  Financial motivation makes it more likely that they prioritize lower cost exploits developed from disclosed zero-day vulnerabilities, albeit as early as possible after disclosure, to maximize the number of unpatched systems they can target."  Of course this all follows from the things we're talking about on this podcast constantly.



"Customizable tool frameworks are developed by cybersecurity software developers to emulate threat activity to enable penetration testing of networks."  And we were just talking about that with red team attacks.  They said:  "They're usually sold under license, but some are also publicly available or available in versions where the license has been removed.  These frameworks are being used or repurposed by state and non-state actors, highly likely enabling a cost-effective uplift in cyber capability.  It's highly likely that their constant evolution and the ability of actors to customize and repurpose these frameworks means widespread misuse of these frameworks will almost certainly continue over the next five years.



"State and non-state actors also have access to capability developed and sold for cybercrime.  In recent years, cybercrime marketplaces have grown and become increasingly professional, in part driven by demand from ransomware actors.  One example is Malware-as-a-Service (MaaS), which is a service that provides use of malware, eliminating the need to create and develop the software, as well as reducing the knowledge threshold required to operate the malware.  Offering these services as a package is attractive to less skilled cybercriminals, and as such has almost certainly expanded the number of victims."



So, anyway, I thought they did a great job of sort of encapsulating everything that's going on there and the trends that all of the evidence points to as where we'll be headed in the future.  And so they concluded with just four points, saying:  "Over the next five years, increased demand, coupled with a permissive operating environment, will almost certainly result in an expansion of the global commercial cyber intrusion sector, driving an increased threat to a wide range of sectors.  Second, it is almost certain there will be further high-profile exposures of victims against whom commercial cyber tools or hacker-for-hire operations have been deployed.



"Third, oversight of the commercial intrusion cyber sector will almost certainly lack international consensus, be difficult to enforce, and subject to political and commercial influence."  Right?  I mean, the people who you would like to be providing the oversight are the customers.  And they finished:  "However, it is likely that many commercial cyber companies will be incentivized to vet and limit their customer bases, should effective oversight and international norms on development and sale of commercial cyber capability emerge."  So, you know.



Last week we took a deep dive, as we know, into ForcedEntry, which led us to appreciate the insane level of effort that the NSO Group's spyware developers, whomever they are, and what we just shared suggests maybe they're not even NSO Group developers.  They might have developed a zero-click independently and said, "How much is this worth to you?"



LEO:  I thought the NSO Group disbanded.  That's why I'm surprised to see their name again.



STEVE:  They did get pounded down, well, because their stuff was highly publicized.



LEO:  Right.



STEVE:  But they're still around.



LEO:  Can't kill cockroaches.



STEVE:  Exactly.



LEO:  They come back again.



STEVE:  So anyway, what Google's Project Zero researchers who reverse-engineered that work reported was that the sophistication they discovered in that exploit terrified them.  They used the word "terrified."  It was as if they had discovered alien technology lurking within a terrestrial device.  Which, you know, gives you some pause to note the obvious commercial value of this sort of technology.  And now we're seeing, almost not surprisingly, a growing black market for ChatGPT accounts, which I thought, what? 



The best way for me to introduce this next topic is to just read what Check Point Research posted last week.  Their headline was: "New ChatGPT 4.0 Concerns:  A Market for Stolen Premium Accounts."  And they said:  "Since December of 2022, Check Point Research has raised concerns about ChatGPT's implications for cybersecurity."  Remember we talked about some of this before, where they used it both to try to reverse engineer for them some code, which it did not do a good job at; or writing code, which unfortunately we know it does do a good job at.



So they said:  "Now, Check Point also warns that there is an increase in the trade of stolen ChatGPT Premium accounts, which enable cybercriminals to get around OpenAI's geofencing restrictions to obtain unlimited access to ChatGPT.  The market for account takeovers" - generically ATOs - "stolen accounts to different online services, is one of the most flourishing markets in the hacking underground and in the dark web.  Traditionally, this market's focus was on stolen financial services accounts - banks, online payment systems, and so forth - social media, online dating websites, emails, and more."



They said:  "Since March of 2023" - which was four weeks ago - "Check Point sees an increase in discussion and trade of stolen ChatGPT accounts, with a focus on Premium accounts, including leak and free publication of credentials to ChatGPT accounts, trading of Premium ChatGPT accounts that were stolen, brute forcing and Checker tools for ChatGPT" - meaning tools that allow for brute forcing - "that allow cybercriminals to hack into ChatGPT accounts, and ChatGPT Accounts as a Service, dedicated service that offers opening ChatGPT Premium accounts, most likely using stolen payment cards."



So why is the market of stolen ChatGPT account on the rise?  What are the main concerns?  They said:  "As we wrote in previous blogs, ChatGPT imposes geofencing restrictions on accessing its platform from certain countries including Russia, China, and Iran.  Recently we highlighted that using ChatGPT AI allows cybercriminals to bypass different restrictions, as well as use of ChatGPT's Premium account.  All this leads to an increased demand for stolen ChatGPT accounts, especially paid Premium accounts.  In the dark web underground, where there is a demand, there are smart cybercriminals already taking advantage of this business opportunity.



"Meanwhile, during the last few weeks there have been discussions of ChatGPT's privacy issues, with Italy banning ChatGPT, and Germany now considering a ban, as well.  We highlight another potential privacy risk of this platform.  ChatGPT accounts store the recent queries of the account's owner.  So when cybercriminals steal existing accounts, they gain access to the queries from the account's original owner.  This can include personal information, details about corporate products and processes, and more."  And Leo, on the first of the three podcasts where Ant co-hosted, we covered the story of Samsung's employees on three separate instances uploading, wanting to get the advantage of ChatGPT's "thoughts," you know, in air quotes, about something.



LEO:  What are your thoughts?



STEVE:  What are your thoughts?  They uploaded Samsung proprietary information in order to get ChatGPT to tell them what it thought.



LEO:  Oh, lord.



STEVE:  And after three instances of that, Samsung...



LEO:  Oh, god.



STEVE:  I know, has shut down.  They imposed a 1K limit on the transactions with ChatGPT and said they are looking into measures for bringing a ChatGPT facility into their internal corporate network so that you won't have to go on the public Internet in order to use it.



LEO:  I wonder, though, if there's any evidence that ChatGPT does save that information.



STEVE:  I think not evidence.  But, for example, just here was the point being made that, if an account is stolen, then the past queries are apparently available...



LEO:  Yeah, it saves a certain number of past queries.  Not all of them.



STEVE:  Right, through the API.



LEO:  It resets it, yeah.  This is what I got in Italy.  [Italian].  And then this is actually from the ChatGPT folks.  They posted an English one, as well, which is nice.  But, yeah, I thought, well, I heard that.  So I said, well, let me try to use it because I have an account.  It said "No, we regret to inform you we've disabled ChatGPT for users in Italy."



STEVE:  Oh, that's right.  I forgot that's where you just were.



LEO:  I was in Italy.  I tried to use it.



STEVE:  Because we covered the news that Italy was saying it represented a privacy threat, and so they were just not going to make it available.  Sorry about that.  We're just saying no.



LEO:  It's the same question, though, is - and ChatGPT denies it.  They say, well, you know, we don't scrape everything.  We scrape stuff that's publicly available.



STEVE:  Right.



LEO:  So if they don't save stuff, and they only scrape public stuff, I don't know if there really is...



STEVE:  It's just, yeah, in fact, that was the point we made on the podcast was it's really just a different kind of spider than Google.



LEO:  Yeah.  It's a spider.



STEVE:  Which is indexing and saving the entire web.  It's just it has a conversational interface, instead of one that's just sort of, you know, put in some keywords and see what hits you.



LEO:  Exactly.  This stuff will shake out.  I don't think it's, yeah, I think it's a short-term issue.



STEVE:  I think we should take our second break.  Then we're going to talk about what ESET found in some decommissioned routers that they purchased.  Whoopsie.



LEO:  Whooooo.  Reminds me of when Simson Garfinkel bought a bunch of hard drives on eBay, and they were un-erased hard drives from old ATM machines and had all sorts of bank account information and stuff like that.



STEVE:  ESET made a very interesting observation in their posting on Tuesday, titled "Discarded, not destroyed:  Old routers reveal corporate secrets."  Uh-huh.  Get a load of what they wrote.  They said:  "Taking a defunct router out of an equipment rack and sliding in a shiny new replacement is probably an everyday occurrence in many business networking environments.  However, the fate of the router being discarded should be as important, if not more so, than the smooth transition and implementation of the new kit in the rack.  Unfortunately, this appears often not to be the case.



"When the ESET research team purchased a few used routers to set up a test environment, there was shock among team members" - shock, I tell you - "when they found that, in many cases, previously used configurations had not been wiped.  And worse, the data on the devices could be used to identify the prior owners, along with the details of their network configurations.  This led us to conduct a more extensive test, purchasing more used devices and adopting a simple methodology to see if data still existed on the devices.  A total of 18 routers were acquired.  One was dead on arrival.  Two were a mirrored pair, so we counted them as a single unit.  After these adjustments, we discovered configuration details and data on over 56% of the devices.



"In the wrong hands, the data gleaned from the devices - including customer data, router-to-router authentication keys, application lists, and much more - is enough to launch a cyberattack.  A bad actor could have gained the initial access required to start researching where the company's digital assets are located and what might be available.  We're all likely aware what comes next in this scenario.  The change in recent years to the methods used by bad actors to conduct cyberattacks on businesses for the purposes of monetization is well documented. Switching to a more advanced persistent threat style of attack has been cybercriminals establishing an entry point and then a foothold into networks.  They then spend time and resources conducting sophisticated extraction of data, exploring methods to circumvent security measures, and then ultimately bring a business to its knees by inflicting a damaging ransomware attack or other cyber nastiness.



"The initial unauthorized incursion into a company network has a value.  The current average price for access credentials to corporate networks, according to research by KELA Cybercrime Prevention, is around $2,800.  This means that a used router purchased for a few hundred dollars, which without too much effort provides network access, could provide a cybercriminal with a significant return on investment.  And that's assuming they just strip the access data and sell it on a dark web market, as opposed to launching a cyberattack themselves.



"A concerning element of this research was the lack of engagement from companies when we [ESET] attempted to alert them to the issues of their data being accessible in the public domain.  Some were receptive to the contact.  A few confirmed the devices had been passed to companies for 'secure destruction and wiping'" - whoops - "a process that had clearly not taken place," they wrote, "and others just ignored our repeated contact attempts."  They said:  "The lessons that should be taken from this research are that any device leaving your company needs to have been cleansed, and that the process of cleansing needs to be certified and regularly audited to ensure your company's crown jewels are not being openly sold in public secondhand hardware markets."



They said:  "We have published the details, except the companies' names and data that would make them identifiable, in a white paper.  The white paper also contains some guidance on the processes that should be followed, including references to NIST special publication 800.88 Revision 1, 'Guidelines for Media Sanitization.'  We strongly recommend reading the details and using our findings as a nudge to check the process in your own organization to ensure no data is unintentionally disclosed."



So I've got in the show notes, I have a link to ESET's white paper which is titled "How I (could've) stolen your corporate secrets for $100."  And I also have a link to NIST's special publication 800-88 Revision 1, "Guidelines for Media Sanitation."



Anyway, in that white paper they provided a summery breakdown, just some bullet points of what they found by percentage.  They said 22% of the routers contained customer data.  33% exposed data allowing third-party connections to the network.  Think about that.  One in three of the routers they purchased for 100 bucks off eBay or somewhere exposed data allowing third-party connections to the network.  44% had credentials for connecting to other networks as a trusted party.  89% itemized connection details for specific applications.  89% also contained router-to-router authentication keys.  100% of them contained one or more IPSec/VPN credentials, or hashed root passwords.  And finally, 100% had sufficient data to reliably identify the router's former owner and operator.



So again, you know, wow.  Just a heads-up to make sure, if you're in charge or know somebody who is in a sufficiently sized organization, I would not trust a third party.  I mean, how difficult is it to reinitialize the configuration in a router?  That's, you know, return it to factory settings.  It's not difficult.  So don't pull it out and toss it on a pile to some third party that in this case apparently wasn't doing anything.  And Leo, to the point you were making, I'll note that through 2023, four months so far, I've been having some very similar experiences of my own.  I've been purchasing specific old drives from eBay.



LEO:  Oh, yeah, of course, for SpinRite, yeah.



STEVE:  Right, when a SpinRite tester has reported that they've had some weird behavior from a specific old drive.  And as SpinRite is running, one of its more popular screens is one which flashes up snapshots of the data which it's obtaining from the drive it's scanning.  You just see it flashing by, but it's kind of mesmerizing, the way we used to watch all the blocks on a defragmenter move around the screen.  It's sort of like that.



LEO:  Steve, I've spent many hours staring at that screen.  I know exactly what you're talking about.



STEVE:  We all have, Leo.  It's an embarrassing truth.



LEO:  No, but that DynaStat, I love that stuff, man.  That's cool, man.  I love it.



STEVE:  Yeah, yeah.  So NTFS file system metadata has a particular look to it which I've learned to recognize.  And these drives are full of it.  I have no interest in the contents of those drives beyond watching SpinRite recover whatever data they might contain.  But they contain someone's data.  Oh, there you go, Leo.



LEO:  And we're defragging here.



STEVE:  We're defragging.  Wow.  Love it.



LEO:  That's interesting.  They didn't erase those, either, yeah.  I can understand how you would think, oh, well, a drive I have to erase.  But a router?  What could that possibly contain?



STEVE:  Right.  Turns out those are the crown jewels of accessing inside of a corporate network.  Makes sense when you think about it, but just make sure that you do think about it.



Okay.  While we're on the topic of routers, let's take a look at last week's report, again from the UK's NCSC, regarding "Jaguar Tooth," a Cisco...



LEO:  That's a terrible name.



STEVE:  I know, Jaguar Tooth, a Cisco router targeted malware. And this serves as a perfect case study.  Jaguar Tooth is a system of backdoor Trojan malware developed via exploitation of a long-since-patched SNMP vulnerability.  It's CVE-2017-6742.  So there are two things here.  First of all, 2017 tells you that it's now, what, six years ago, 2017.  And 6742 reminds us of those quaint days six years ago when CVEs had four-digit numbering for their individual CVEs.  We have to use scientific notation these days.  This vulnerability was first announced by Cisco on the 29th of June in 2017, when updated and repaired software was made available by them.  Cisco's published advisory included details of workarounds, including some of limiting access to SNMP from trusted hosts - imagine that - or by disabling several vulnerable SNMP API branches, which are known as MIBs.



Okay.  So this amounts to another of those issues I so often have about policies versus mistakes.  Backing up a little bit, SNMP is the Simple Network Management Protocol.  It's essentially a network API, a very powerful network API, which allows for the complete configuration state querying and configuration management of SNMP-capable networked devices.  The point is, it should never be publicly exposed to the wider Internet.  That's just nuts.  It is meant to be used on the internal Intranet for internal management.  And if by some weird network configuration need a router's SNMP traffic does need to transit the public Internet, then it would certainly only ever need to be seen by a specific single targeted remote public IP, never all public IPs.  There's no conceivable reason for a router's SNMP service to be globally available.



And yes, SNMP has an authentication layer.  But it's old, and its lame, and it's barely adequate for the purpose of keeping insiders out.  You know, it's just ridiculous.  Let alone outsiders, keeping outsiders out.  If external SNMP packets cannot reach the SNMP service, then vulnerabilities in that service will never become an issue in the first place; right?  So even if there's a mistake, if your policy is to firewall the SNMP service so that it isn't available, then no problem.



So once again, policies versus mistakes.  Mistakes happen by mistake; okay?  But policies happen by policy.  In other words, on purpose.  And mistakes don't need forgiveness, but policies don't deserve any.  And I don't mean to harp on this, but to me this delineation seems important, and it is too often confused. Anyone who's responsible for any Cisco corporate router in 2023, which is still running a version of Cisco's OS from 2017, should be immediately, summarily, and disgracefully discharged from their responsibilities and their employment.  It's unconscionable.  But we also know that, unfortunately, many such routers will nevertheless exist.



So what do we know about this specific problem?  The vulnerability in SNMP from six years ago enables a stack-based buffer to be overflowed - whoever heard of such a thing - enabling control of the instruction pointer which can be used to gain remote code execution.  This exploit uses Return Oriented Programming, the idea there is that until you're able to get your own code running, you need to use the code that's already there.  And it turns out that an operating system like Cisco's IOS is full of subroutines.  At the end of a subroutine is a return instruction which returns to where it came from, from where the subroutine was called.  And first bad guys and then good guys figured out that the last few instructions just before the return can be useful.  They can load something in a register.  They can add something to a register.  They can do useful things.



So clever hackers who have access to the same operating system as the one that they're attacking look at all the return instructions, and all of the instructions just in front of them, that are just before a return.  And they're able to knit together the execution of code they want by jumping, deliberately jumping to just before subroutine's end, do a couple things, and then return to them, and then do it again, and again, and again, and again.  Using just little snippets at the end of all the subroutines that exist in order to get done what they want done, which in this case is to incrementally assemble their malware in RAM.  Again, it's a little frightening how sophisticated these attacks have been.



Turns out the vulnerable function targeted by this exploit is reached using the SNMP Object Identifier, the so-called OID, which corresponds to alpsRemPeerConnLocalPort.  By appending additional bytes to the end of the OID, a stack-based buffer can be overflowed, which tells us that the expected length of this OID is how much space was created on the stack to hold it.  But they didn't check to see if the OID was actually that long.  They just went ahead and parsed it.  So you literally - an OID is a weird-looking thing in SNMP.  It's 1.3.6.2.27.14 dot.



So the idea is each of those dots is a branch in a tree.  And so SNMP represents all of the little settings that you might have in a router by the end points of this incredibly richly branching tree where you address it by following this crazy dotted syntax all the way out to the end.  Well, it turns out you could just keep adding things with dots in this broken version of IOS from 2017 and put a bunch of stuff on the stack, which then you can cleverly design what's there in order to go out and execute little code snippets for you.



One of the side effects of this vulnerability is that any ASCII characters in that additional OID bytes are converted to uppercase, which can be inconvenient, so the attackers get around that.  Jaguar Tooth is deployed by writing custom shellcode to memory which can be used to write an arbitrary 4-byte value to any specified address.  This shellcode is then called repeatedly to incrementally write Jaguar Tooth into RAM, 4 bytes at a time.  Once the Jaguar Tooth payloads have been copied into memory, they're individually executed by overflowing the return address of the vulnerable function with their location in memory.



Once Jaguar Tooth is running, it uses TFTP, that's the Trivial File Transfer Protocol, like a really reduced subset of FTP, to exfiltrate pretty much everything the router knows about all of the peers that touch it, its configuration, and all those things we were just talking about that you don't want to decommission, you don't want to leave in your decommissioned router.  The router's ARP table, for example, is dumped to obtain the MAC addresses and IPs of all the internal machines that have recently touched the router.  And of course the bad guys now have a foothold in a border router, able to run whatever they choose.  From there on, it's going to be pretty much bad news.



So as I noted earlier, Cisco responsibly updated their IOS, that's what they call it, the Internetwork Operating System, back in June of 2019.  The bad guys probably assumed that there would be routers, correctly assumed there would be routers that had not been updated in six years and were still running this IOS software from back then.  So sure enough, a malicious actor group known as APT28 has been detected actively conducting reconnaissance and deploying their malware on the world's routers, which are still running that vulnerable version of IOS.  So another example of the problem that we have in our industry that we have not been able to figure out.  We are unable to write software that doesn't have these kinds of problems, no matter how much we try and how much focus and attention we give to it; nor are we able to essentially recover all of the software that's already out there with known problems which nobody is taking the measures to fix.



Okay.  Time for some happy news.



LEO:  Well, it's about time.  I've been waiting for this for 18 years.



STEVE:  We may have some really happy news at the end of this.



LEO:  Oh.  Oh, good, okay.



STEVE:  But first some interim happy news.  Something known as the Security Research Legal Defense Fund is in the process of being created, and it is what its name suggests.  The organization's website domain is also its name,  SecurityResearchLegalDefenseFund.org with no spaces or hyphens or anything.  They explain themselves in one long line.  They said:  "We aim to help fund legal representation for persons who face legal issues due to good faith security research and vulnerability disclosure in cases that would advance cybersecurity for the public interest."  Which is very cool.



So they break this down in three statements.  The first is their mission statement:  "The Security Research Legal Defense Fund will be a nonprofit organization whose mission is to promote social welfare by providing financial assistance for legal representation of good faith security researchers and vulnerability disclosure."



For background they say:  "Society depends on secure digital communications and devices, but cyberattacks and system failures increasingly endanger physical safety, consumer privacy, and the operation of critical services.  The public benefits when security vulnerabilities in software and systems are discovered and fixed before malicious actors can exploit them.  In many instances, individuals have acted independently and in good faith to find and report vulnerabilities for mitigation, thereby strengthening the cybersecurity of products and services for the good of the community.



"While recognition from governments and businesses of the value of good faith security research and vulnerability disclosure is growing, individuals continue to meet with legal threats when their vulnerability research and disclosures are unwelcome or misunderstood.  Such threats can ignore individuals' rights or misconstrue facts, creating a chilling effect on beneficial security research and vulnerability disclosure, especially for individuals without the resources to finance legal counsel."  So, yay, you know, this is great news.



Finally, under "How It Works," they explain:  "The Security Research Legal Defense Fund may donate to good faith security researchers' choice of counsel to represent them in defending against legal claims related to good faith security research and vulnerability disclosure.  The Defense Fund does not provide direct legal representation at this time.  The organization's Board of Directors will consider potential guarantees and vote on distribution of funds.  To help ensure funds are used in the public interest, the recipients of legal defense funds would be required to meet eligibility criteria.  The eligibility criteria is subject to revision by the Board, and aims to reflect alignment with legally accepted definitions of 'good faith security research.'



"The eligibility criteria to apply for grants from the Defense Fund is anticipated to include:  The grantee demonstrates financial need.  Funds donated from the Security Research Legal Defense Fund would go towards representation in legal matters related to good faith security research or vulnerability disclosure, and not such illegal behavior as extortion."  Okay, duh.  "Also the 'good faith security research or vulnerability disclosure' was performed for the purpose of good faith testing, investigation, correction, or disclosure of a security flaw or vulnerability.  It was carried out in a manner designed to avoid harm to individuals or the public, and the information derived from the activity was intended to be used primarily to promote the security or safety of computers or software, or those who use such computers or software.  And, finally, Board approval."



So I think this is great.  You know, through the years here we've talked about this problem where well meaning, typically amateur hackers who are not backed by an organization attempt to inform an organization of some significant problem that they've stumbled upon and identified, only to have the organization's management freak out and aim their law enforcement and their attorneys at the hapless hacker.



LEO:  That's what happened to Randal Schwartz at Intel, and he was arrested and prosecuted.



STEVE:  Yes.  Yes.  I mean, it's so wrong.  So it seems like, you know, this would be a terrific backstop for such situations.  I did a little bit of more research and legwork.  SC Magazine knew a little bit more about this.  It turns out that Google is like the main anchor for this.  They said:  "Google and other companies will develop and stand up a pair of new initiatives that will provide policy guidance to governments and legal protection to security researchers engaged in 'good faith' vulnerability research and disclosure," and they said, "while the tech giant also said it would formalize an internal policy to be publicly transparent when bugs in Google products are exploited in the wild."



Anyway, they go on at some length.  The council, that is, this Board of Directors, will include representatives from bug bounty firms HackerOne, Bugcrowd, Intigriti, and Luta Security, as well as Intel and Venable, a law firm that specializes in cybersecurity law and policy matters.  So anyway, I just think this is great news, that there will be this sort of a formal legal defense fund that will be backed by people who understand the nature of the business and will be there to support people who get themselves in trouble when they're absolutely not being black hat, not attacking, not doing anything but trying to let someone know that they've got a problem, and end up being attacked and sued in the process as a result.



Okay.  So a couple quick updates.  Last week Firefox users moved to v112.0.1 to fix exactly one problem, which was like, I don't think I've ever seen that happen.  But because it seemed to be of great concern, Mozilla immediately moved on it.  Mozilla wrote:  "Fixed a bug where cookie dates appear to be set in the far future after updating Firefox.  This may have caused cookies to be unintentionally purged."  Now, that can happen, right, because cookies are meant to expire.  And so if the cookie dates were set into the far future, which shouldn't happen, that could be a problem, and it would be a big problem for people who expected to be, you know, to remain signed in.  I didn't notice any problem.  And I am once again, after using Google and Bing browsers for a spell, I'm happily back at Firefox, Leo, where I assume you still are.



LEO:  Never moved.  Never moved.  So I've tried, but I never moved.



STEVE:  I know, yeah.  



LEO:  They kept giving me, they said, you should try this or that or Opera or Brave or...



STEVE:  I've been watching Paul.  He's been, like, roaming all over the place.



LEO:  He goes all over.  Yeah, Firefox.  You know why?  Because I want to support an ecosystem that has more than one browser engine.  Everything else is Chrome, or Chromium.



STEVE:  Yes.  And Paul is a little reminiscent of Jerry, who used to say, "I do all these dumb things so you don't have to."



LEO:  Yes.  You're talking about, of course, the great Jerry Pournelle, who is much missed, I have to say.



STEVE:  Yeah.  He was great.



LEO:  He was a great guy.



STEVE:  Also you'll be glad to hear this.  Kubernetes received a security audit.  The NCC Group concluded and published a new security audit of the Kubernetes automation platform.  Nothing significant found.



LEO:  That's huge because it's very widely used yeah.



STEVE:  Yes.  That's very cool.  And finally, Chrome fixed a zero-day.  They released 112.0.5615.137 or 138, which fixes eight security flaws, including a new zero-day.  And I'm not sure I understand this, unless it's a - oh, yeah.  I'm sorry, I was looking at the number rather than the date.  A zero-day, CVE-2023-2136.  So it was discovered by Google's TAG team, so internally, although being a zero-day, they're not like Microsoft who calls it a zero-day if they learn of it by surprise.  It's a zero-day when it's found being exploited in the wild.  So anyway, this was patched last week.



LEO:  And I wasn't aware of that.  I did not know that.  That's interesting.



STEVE:  Yeah.



LEO:  Microsoft has a different definition.



STEVE:  Yes, they've got their own private definition, which is weird.  It allows them to, I mean, you wouldn't think they would be declaring...



LEO:  No, it makes more things zero-day than you would normally expect.



STEVE:  Exactly.  Exactly.  Which, you know...



LEO:  They're not in the wild yet.  It's still a zero-day?  Wait a minute.  Yeah.



STEVE:  Yeah.  Google did note that it was under abuse by a surveillance vendor.



LEO:  Oh.



STEVE:  So just exactly the type of slime that we were talking about before, somebody selling these things to third parties.



LEO:  A data broker type?



STEVE:  Yeah, a surveillance - well, no, like an NSO Group.



LEO:  Oh, geez.  Oh, god.



STEVE:  It's a vendor selling surveillance capability.



LEO:  Terrible.



STEVE:  Yeah.  Okay.  I'm very excited about what we've got coming next, an end-to-end encryption proposal.  Let's do our final break.



LEO:  Yeah.  We'll do a little sponsor - yeah.



STEVE:  I'm going to share two open letters and then an idea that I had that might just work.



LEO:  You've proposed things in the past.  But boy, now more than ever we need it.  EARN IT is back.  The UK is about to do this.  I mean, we need something.  We need a solution that works for everyone.  And if anybody could come up with it, it'd be  you, Steve.  So I look forward to hearing this.  But meanwhile I just want to tell, first of all, I want to thank all of our Club TWiT members because you are the salt of the earth.  You're the people who, you know, you could, and most of you do, like 95%, I think it's more like 98% of all of our listeners are very happy to listen for free.  We've been doing it that way for 18 years.  That's fine.  Did you know, Steve, we had our 18th anniversary while we were gone, on April 17th?  18 years.



STEVE:  I know because you and I are in - yeah.



LEO:  Right along in there, yeah.  We were the second show; right?



STEVE:  Yup.



LEO:  Yeah.  So, and, you know, I'm happy to have you, and please listen for free as long as you want.  But I really have to say appreciation to the listeners who say, you know, I like what you do, and we want to support it.  We want to keep it on the air.  We want you to grow.  And they contribute.  And it's not much.  It's seven bucks a month.  That's one, you know, half caff, half decaf cappuccino frappuccino with a twist.  That's it.  We really love our club members.  They're the people who are most engaged, most involved, really support what we do.  And thank you.  Because of you, we're able to do more than ever before, and keep the lights on.  TWiT.tv/clubtwit.



Now let's talk about Steve's proposal that's going to solve all our problems.



STEVE:  Okay.  Maybe it's a good idea.  Anyway, we'll see.  We have, of course, been covering the fascinating and escalating debate over the presence of ubiquitous end-to-end encryption, which took another step with the UK's Online Safety Bill, which is currently winding its way through the United Kingdom's legal system.  But it's on its way to becoming law in the UK.  As we know, this is the legislation that's being promoted as a means of protecting children from online threats of all sorts by requiring secure messaging providers to somehow arrange to monitor and filter the images, videos, audio, and textual communications of their entire user base, regardless of whether individuals are suspected of illegal behavior or not.



Okay.  So while assembling today's podcast, I encountered two opposing open letters, which I'll share here in a moment.  What's surprising is that, after reading and placing these open letters into the podcast's show notes, I was summarizing the current situation and working through the dilemma, and I may have actually come up with a workable solution to this whole encryption mess.  We'll see.



LEO:  Let's submit it for their approval, yes.



STEVE:  I'll share it with everybody, and we'll see what you think.  But okay.  First things first.  Last week's news is that the CEOs of the secure messaging firms have collectively authored and co-signed an open letter to the UK government.  Represented were the heads of Element, Session, Signal, Threema, Viber, WhatsApp, and Wire.  Since this open letter contains a few juicy bits - they make some really great points - I want to share what the heads of today's secure messaging companies just wrote to the UK.  They addressed it to anyone who cares about safety and privacy on the Internet.  Okay, a little bit loaded there, but okay.



They said:  "As end-to-end-encrypted communication services, we urge the UK government to address the risks that the Online Safety Bill poses to everyone's privacy and safety.  It is not too late to ensure that the bill aligns with the government's stated intention to protect end-to-end encryption and respect the human right to privacy.



"Around the world, businesses, individuals, and governments face persistent threats from online fraud, scams and data theft.  Malicious actors and hostile states routinely challenge the security of our critical infrastructure.  End-to-end encryption is one of the strongest possible defenses against these threats.  And as vital institutions become ever more dependent on Internet technologies to conduct core operations, the stakes have never been higher.  As currently drafted, the bill could break end-to-end encryption, opening the door to routine, general, and indiscriminate surveillance of personal messages of friends, family members, employees, executives, journalists, human rights activists, and even politicians themselves, which would fundamentally undermine everyone's ability to communicate securely.



"The bill provides no explicit protection for encryption and, if implemented as written, could empower OFCOM" - that's the UK's communications regulator - "to try to force the proactive scanning of private messages on end-to-end encrypted communication services, nullifying the purpose of end-to-end encryption as a result, and compromising the privacy of all users.  In short, the bill poses an unprecedented threat to the privacy, safety, and security of every UK citizen and the people with whom they communicate around the world, while emboldening hostile governments who may seek to draft copy-cat laws.



"Proponents of the bill say that they appreciate the importance of encryption and privacy, while also claiming that it's possible to surveil everyone's messages without undermining end-to-end encryption.  The truth is that is not possible.  We aren't the only ones who share concerns about the UK bill.  The United Nations has warned that the UK government's efforts to impose backdoor requirements constitute 'a paradigm shift that raises a host of serious problems with potentially dire consequences.'  Even the UK government itself has acknowledged the privacy risks that the text of the bill poses, but has said its 'intention' isn't for the bill to be interpreted this way."  What?



LEO:  Yeah, it doesn't matter.



STEVE:  How are we supposed to interpret it? We're not interpreting it, we're just reading it.



LEO:  You shouldn't interpret that, you know.



STEVE:  Wow.



LEO:  It's, yeah.



STEVE:  And then they said:  "Global providers of end-to-end encrypted products and services cannot weaken the security of their products and services to suit individual governments."



LEO:  There you go.



STEVE:  "There cannot be a British Internet, or a version of end-to-end encryption that is specific to the UK.  The UK government must urgently rethink the bill, revising it to encourage companies to offer more privacy and security to its residents, not less.  Weakening encryption, undermining privacy, and introducing the mass surveillance of people's private communications is not the way forward."  And it was signed by, they said, "those who care about keeping our conversations secure."  And it was the CEOs of the companies I first mentioned.



Okay.  So there's the open letter from the encryption providers who argue, convincingly, I think, that forcing surveillance capability into all communications is not workable.  Now we have an open letter published last Wednesday by a group known as the Virtual Global Taskforce.  They describe themselves as an international alliance of 15 law enforcement agencies.  Now, I was a bit suspicious because the chair of the organization is the UK's National Crime Agency.  So I was wondering how global they were.  But Wikipedia knows all about them and explains.



"The Virtual Global Taskforce is a group of law enforcement agencies from around the world who operate together to stop online child sex abuse.  The VGT is made up of the following organizations:  We have Australian Hi-Tech Crime Centre and the Australian Federal Police; Child Exploitation and Online Protection Centre in the UK; Colombian National Police; the Cybercrime Coordination Unit of Switzerland; the Dutch National Police; Europol; Interpol; the Italian Postal and Communication Police Service; the Korean National Police Agency; the Royal Canadian Mounted Police; the New Zealand Police; the Ministry of Interior for the United Arab Emirates; the Philippine National Police; and U.S. Immigration and Customs Enforcement (ICE), which is a division of the DHS."



Okay.  So this group of actual police forces, law enforcement, have collectively authored and sent their own open letter, this time to Meta, asking Meta to reconsider adding end-to-end encryption features to Facebook and Instagram.  The letter argues, of course, that this would hinder their own and Meta's efforts to fight the proliferation of CSAM on the platform, you know, Child Sexual Abuse Material.  So here's what they said.  This is their open letter.



"The Virtual Global taskforce is calling for all industry partners to fully appreciate the impact of implementing system design decisions that result in blindfolding themselves to child sexual abuse occurring on their platforms, or reduces their capacity to identify CSA and keep children safe.  It is time to confront these concerns and make tangible steps towards possible solutions that we know exist.



"The Virtual Global Taskforce is an international alliance of 15 dedicated law enforcement agencies, of which the National Crime Agency is the chair, working alongside affiliate members from private industry and non-governmental organizations to tackle the threat of child sexual abuse.  The VGT issued its first position statement on end-to-end encryption in 2021.  This statement highlighted the devastating impact end-to-end encryption can have on law enforcement's ability to identify, pursue, and prosecute offenders, when implemented in a way that affects the detection of CSA on industry platforms.  It is important to update the VGT position on end-to-end encryption in the context of impending design choices by industry.  As outlined in our previous statement, there is no doubt that encryption plays an important role in safeguarding privacy; however, this must be balanced with the importance of safeguarding children online.



"The VGT encourages industry to respond and consider the following:  Only to implement platform design choices, including end-to-end encryption, at scale alongside robust safety systems that maintain or increase child safety.  And where the child user base and risk is high, a proportionate investment and implementation of technically feasible safety solutions is paramount."  They said:  "The abuse will not stop just because companies decide to stop looking.  We all have a role to play in protecting children in online spaces, and we strongly urge industry partners to take active steps toward this goal.



"The scale of online child sexual abuse is increasing worldwide.  The WeProtect Global Alliance have identified it as one of the most urgent and defining issues of our generation.  The number of reports of CSA from industry continue to be staggering, but demonstrates the key role that industry plays both in protecting children online and in reporting cases to law enforcement for action.  The National Center for Missing and Exploited Children (NCMEC) received 29.3 million reports of suspected CSA in 2021, a 35% increase from 2020.  Of this 29.3 million, over 29.1 million reports came from electronic service providers.



"Although these reports result in a range of different outcomes globally, what is consistent is that they significantly contribute to positive outcomes for child safety.  These figures demonstrate the current success of industry partners in detecting and reporting CSA occurring on their platforms, resulting in victims being identified and safeguarded.  Design and investment choices implemented in a way that interfere with the effectiveness of such safety systems threaten to undermine these successes which have been consistently built upon over previous decades."



Finally:  "The announced implementation of end-to-end encryption on Meta platforms Instagram and Facebook is an example of a purposeful design choice that degrades safety systems and weakens the ability to keep child users safe.  Meta is currently the leading reporter of detected child sexual abuse to NCMEC.  The VGT has not yet seen any indication from Meta that any new safety systems implemented post end-to-end encryption will effectively match or improve their current detection methods."



Okay.  So everybody's gearing up here and staking out their positions.  It feels like it's coming to a head.  It's unclear what's going to happen.  Legislation is probably going to be passed since it's easy for politicians to write laws which tell others what they can and cannot do.  But it's difficult to see any of the providers of end-to-end encryption backing down from their positions, especially not those like Telegram, Signal, and Threema, whose entire purpose is providing secure end-to-end encryption.  As we know, Apple proposed a solution that would be minimally invasive, but the public freaked out over the idea of anything like a library of known child pornography being resident on their phones, and the sentiment is understandable.  And Apple's solution would not handle the whole text messaging "grooming" problem.



So this all led me to revisit the question we touched upon once previously, which was whether some form of good old-fashioned parental control might be the only answer.  Perhaps we should, you know, we would need to decide that these social media devices are just too dangerous for children to have.  And that led me to an interesting idea that I haven't seen suggested anywhere.  Why don't we arrange to only monitor children?  The pending laws and legislation would be changed to only require exactly what those governments claim is their reason and motivation for needing to compromise full end-to-end encryption, which is sexual abuse material content and behavior screening for minors.  Then we implement that legislation with technology so that the devices children use in countries that require it are aware of the date when they will no longer be subject to monitoring for their own protection.



When any device is first set up and configured with an account, the setup process determines whether the user of this device resides within a country whose government has mandated the surveillance of minors.  If not, that's the end of it.  But if so, the setup process is then informed of whether the user is already an adult.  If so, again, that's the end of it.  But if the user is currently a minor in their local society, governed by laws which mandate the protection of minors online, the setup process asks the user's date of birth and the age at which they will no longer be considered a minor in their world.  This sequence of steps sets and stores an immutable date which subsequently governs the behavior of all encrypted services available for the device.  Encrypted services query for a binary value, whether or not its user requires the protection provided by side-channel content moderation.



While users are young, any government-mandated surveillance will be conducted in the background without interfering with the use of any applications.  It will be entirely transparent to its young users.  But on the day of their birthday, when they reach the age of majority, all such background side-channel surveillance automatically terminates, in full compliance with the laws governing their use of encrypted services within their society.  And, importantly, this solution means that no user who is already an adult - none of us, for example - will ever be subjected to this monitoring.



So think about the problems this solves.  Children don't lose any functionality.  Everything works for them as it always has.  Yes, sure, in the margins they're sacrificing some of their privacy in the interest of their protection from online predation, but only while it's in their best interests to be protected.  As soon as it's no longer needed, it disappears.  And since there's no observable effect from its presence, there's no great pressure for them to cheat the system.  Children are never inconvenienced.  Everything works perfectly for them, and the side-channel monitoring is completely invisible.  Parents can take some relief in knowing that, whatever it is their kids are doing online, it's being monitored for their safety, while preserving as much of their privacy as possible.  So parents who are in the position to oversee and set up this system in compliance with their local laws are able to enforce its presence.



Adults, who are not endangered by online exploitation, enjoy the privilege of truly private unmonitored end-to-end encryption without any fears of Big Brother eavesdropping.  The fact that adults are never monitored dispels the worries about eventual government overreach and the presence of hidden government surveillance agendas.  Only children are ever monitored.  The online slimeballs who seek to take advantage of youthful trust and innocence know that all of their communications with an underage target is being monitored, so that hopefully pours some cold water where it may do some good.



The concern of whether such surveillance might be a slippery slope, and whether governments are actually using "but think of the children" as a stalking horse to mask their real interest in perpetrating more widespread surveillance is resolved by this.  No adult is monitored.  Only young users whose electronic devices are aware of their monitoring cutoff date are protected.  If governments have secret intentions to expand this monitoring beyond sexual exploitation of minors, then that's fine, too.  But whatever they do, it will only work on kids.  The monitoring cutoff date system could be entirely local to the device, as I described above, set up under parental supervision when the device is first brought online and never subject to change.  Or it might be set by the device's service provider, such as a cell provider, or by the device's account provider, such as Apple, Google, Samsung, et cetera.  In the future, if governments require some form of oversight verification that minors are being protected, that, too, could be implemented.



But the crux of the idea is clear.  We've come to loggerheads and are approaching an impasse because both sides have been taking absolute all-or-nothing positions.  Using technology, a compromise is possible that should satisfy everyone.  Governments and law enforcement agencies say that they want to monitor children for their own protection.  Fine, that can be arranged.  Adults are adamant that they do not want to ever be monitored.  Fine.  They won't ever be.  Everyone worries that governments have a hidden agenda for this monitoring.  This makes that impossible.



I've been thinking about this since it occurred to me yesterday, and I can't find much fault to it.  The need to embed the date when surveillance will no longer be needed is new.  But so what? New features are being added to our phones continually.  If necessary during a transition period, or when a device does not yet offer the "Monitor Me" flag, the age determination could be individually distributed among encrypted service providers when accounts are created.  But it would be cleaner to have this built into the device and queryable by encrypted apps.  If a device is shared by multiple children, the age of the youngest user among them would be chosen so that the youngest user remains protected, and all remain in compliance with local laws.



In all the coverage we've given of this mounting encryption standoff, I've never seen any mention of something like this that appears to be a workable compromise.  Both sides sound and appear to be absolute in their positions.  But this would appear to offer a compelling middle ground that would not be objectionable to either adults or pre-adults.  And it feels like a compromise that even the encryption absolutists could live with.  And they may have to if they want their services to remain legal where monitoring of minors is required by law.



LEO:  Okay.  I don't think they're actually proposing monitoring kids.  They want to monitor adults for trafficking CSAM.



STEVE:  Well, oh.  So, okay, I guess I missed that.



LEO:  I think you missed the point.



STEVE:  So I thought it was to...



LEO:  It's to protect children.



STEVE:  ...keep it from children.



LEO:  No.  Well, that's, I mean, that was a minor part of what Apple was proposing.  But that's not what all these other things are about.  They're about, whether this is true or not, but...



STEVE:  I guess I'm too far out of the loop, Leo.  I really don't...



LEO:  The thing they're trying to prevent...



STEVE:  I don't understand what the bad guys are doing.



LEO:  The thing they're trying to prevent is adults from trafficking in child sexual abuse material.  That CSAM database Apple was going to put on your phone comes from NCMEC, the National Center for Missing and Exploited Children, is a database of child pornography that adults are trafficking in.



STEVE:  That's really creepy.



LEO:  Oh, Steve, you are very innocent.  I did not realize you didn't know about this.  Yeah, no, that's the whole - of course it's horrible.  It's horrific.  But it's not about monitoring children.  I mean, that's part of it, I guess.  You know, Apple has a thing where...



STEVE:  I guess I was focused on the whole grooming aspect of it.  So, wow.



LEO:  Yeah, that's not the issue.  The issue is they really want us - the problem is all of these cloud services are storing tons of data.  They don't want them to be storing child sexual abuse material.  That's what CSAM is.  It's images, graphic child porn images.



STEVE:  It's hard to even imagine that. 



LEO:  It's not kids sharing these, it's adults sharing these.  That's the people they're going after.



STEVE:  What was it on "Saturday Night Live" where at the end of something, someone said, "Never mind"?



LEO:  Roseanne Roseannadanna.  No, there is a part of this, you're absolutely right.  And Apple, by the way, is doing this.  So if a child receives a naked picture - the parents could turn this on - or wants to send - this is actually what they're really trying to stop is children sexting each other - or wants to send a naked picture of themselves, Apple will say, "Oh, you don't want to do that," to the kid.  And the parent could turn on the thing where it will warn the parent that this is going on.  That's...



STEVE:  We have talked about this before.  I didn't realize that that was actually in place, that that is in place now?



LEO:  That's in place now, yeah.  They turned that on.  But that's not what all of this - that's not what the FBI or the UK wants.  What they want is to break encryption for all of us so that pedophiles can't communicate.



STEVE:  Oh, Leo.  It's hard to even picture that happening.



LEO:  Oh, it's horrible.  And that's why, by the way, it's a very useful tool for law enforcement because nobody's in favor of that, and you don't want to say, well, don't break encryption because they'll say, well, are you in favor of child porn?  No.  But the problem is you can't break encryption for pedophiles and still leave it intact for us.



STEVE:  Yeah, I certainly understand the mistake I made.



LEO:  Well, you protected the kids, which I agree is a good idea.  You've proposed in the past, quite a few years ago, an escrow system which I think still might not be a bad idea, third-party escrow system, like Apple holds the keys.  But most of the people on that letter, for instance, just don't want any backdoors at all, or any escrow system, or any keys.  These should be end-to-end means only you and the recipient have the key, no one else.  And the problem is, what law enforcement says, is well then pedophiles can exchange all of this stuff, and we won't know.



STEVE:  Wow.



LEO:  Yeah, I know.  I'm sorry, I didn't mean to burst your innocence, Steve.  I want to pat you on the head.



STEVE:  Wow.



LEO:  Yeah, there's some bad people out there.  There's some really bad people out there, more than we think, I think.  But nevertheless.



STEVE:  Well, okay.



LEO:  That's why it's a complicated thing; right?



STEVE:  I don't have any answer.



LEO:  Yeah.  That's why it's a really complicated thing.



STEVE:  I mean, it's hard to embrace the problem.



LEO:  Yeah, I know.



STEVE:  Ugh.



LEO:  Oh, Steve, I'm so sorry.  I didn't know you didn't know.  Go talk to Lorrie; would you?  Ask her.



STEVE:  Oh, god.



LEO:  Ask her about it.  Steve, thank you for a wonderful show.  You have a solution for that one area of it; right.  And in fact that's basically what Apple's doing, by the way.



STEVE:  Well, and I was going to share these two open letters because what we are seeing is both sides escalating this thing.



LEO:  Right.  Oh, they're at loggerheads, and that's why it's tough.  It's really tough because nobody wants to say, oh, yeah, we want to facilitate child porn.  But it's much more than that.  And this is, you know, Phil Zimmermann, creator of PGP, I did a Triangulation...



STEVE:  I think I'm in favor of monitoring, Leo.  This is just so horrible.



LEO:  Well, that's one of the reasons why that's what they use.  That's the specter that they raise because who's going to be in favor of that?  But I personally don't think that's the only thing they want.  I mean, look, law enforcement says, if we could see everything going on, there would be no crime.  We could stop crime, nip crime in the bud.  The Constitution says, yeah, that's true; but people deserve - they have a right to privacy in their own home.



STEVE:  Yeah.



LEO:  So this is the tension.  This is why it's very difficult.  I interviewed Phil Zimmermann, and we raised - this has always been the issue with - people said with PGP, look, that lets criminals exchange information freely.  And Phil said, look, don't fool yourself.  Law enforcement will say to you, oh, we're going dark.  In fact, the FBI put out that paper, "Going Dark."  They're not going dark.  Technology's given them far more means of surveillance.



STEVE:  And what you're referring to when you talked about the thing I proposed a long time ago was the idea, a means of coming up with the equivalent of a search warrant.



LEO:  Right.



STEVE:  Where under warrant a specific individual's communications could be monitored appropriately.



LEO:  Right.



STEVE:  And again, though, we do have the problem of it not getting out of control, of it not being abused.



LEO:  And that's the premise that WhatsApp and Signal and everybody else says, if you've got a backdoor, it will leak out.  We know that historically.  It will leak out.  And so you can't have any backdoors.  It's not safe.  It's, look, Steve, I sympathize with your feelings because it's a horrible thing.  Nobody's supporting pedophiles.  But again, law enforcement, yeah, we could give them the right to see everything.  That's what they'd like.  But that would also mean that no one would have any privacy at all.  It would eliminate crime, but at what price?



I'll let you think about this for a while, Steve, and we'll come back next week.  Steve Gibson is the man.  GRC.com is the site, the Gibson Research Corporation.  Go there, get SpinRite.



STEVE:  I'm going to go back to work on SpinRite.



LEO:  Yeah, stick with SpinRite.  This is why you didn't want to do an - you were working on an encryption tool.  And you knew...



STEVE:  Yeah, CryptoLink because, yeah, I knew the government was going to say...



LEO:  They would come for you.



STEVE:  They'd be unhappy with absolute encryption.



LEO:  Yeah.  GRC.com has this show.



Copyright (c) 2023 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.


GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#921

DATE:		May 2, 2023

TITLE:		OSB OMG and Other News!

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-921.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week, because the UK's Online Safety Bill continues to stir up a hornet's nest of worries and concerns within many industries, we're going to examine WhatsApp's reaction to Signal's "We plan to walk" position and Wikipedia's concerns over the bill's age verification requirements.  And, undaunted, I have another idea that might be useful!  We also have a new UDP reflection attack vector, a welcome (and late) update to Google Authenticator, more NSO Group client news, a Russian OS?, the unintended consequences of releasing updates for routers that won't actually be updated, a smart move by Intel with pre-release security auditing, yet another side-channel attack on Intel CPUs, cURL's maintainer implores Windows users not to delete it, and VirusTotal gets AI.



SHOW TEASE:  it's time for Security Now!.  Steve Gibson is here with some really interesting topics.  Can the UK force companies to break their encryption?  And what is to be done about it? We'll also talk about an unusual case, but some evidence that maybe it isn't always a good idea to rush those security updates out.  And a word, if you don't mind, from the creator of cURL.  It's all coming up next, plus a lot more, with Security Now!.	



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 921, recorded Tuesday, May 2nd, 2023:  OSB OMG and Other News.



It's time for Security Now!, the show where we cover the latest news in security.  The show that gets longer every week because there's never any lack.  Mr. Steve Gibson.  Good day, sir.



STEVE GIBSON:  It is funny.  Sometimes I have an occasion to go back in time and look at some of our earlier shows.



LEO:  The 20-minute versions, yeah.



STEVE:  Yeah.  And I think, how did we get anything done in 20 minutes?  You know, we're still busy saying hi to each other.  And by the way, Leo, have you run across "Barry" on HBO?



LEO:  You know, I've watched the first three seasons, and I saw that a fourth has arrived.  So, yeah, we'll be watching it.  You like it.



STEVE:  Yes.  It starts a little slow.  We're not sure.  And what was weird was I was used to seeing the name Alec Berg, and I couldn't place it.  So I googled him.  Well, he was the guy behind "Silicon Valley."



LEO:  Right.  A talented writer, yeah.



STEVE:  And it does have - yeah.



LEO:  And Bill Hader's hysterical.  So I love Bill Hader.



STEVE:  What a good job.



LEO:  And is the bald guy in it still?



STEVE:  Yeah.  He's in it all the way through.



LEO:  I love him.



STEVE:  And they are, you know, they're making him increasingly gay, so they're having fun with all of that.



LEO:  He's such a great character.  He's, what, a Chechen mobster, but sensitive.  He's a sensitive...



STEVE:  He has style.



LEO:  He has a lot of style.



STEVE:  A lot of style, yeah.



LEO:  Yeah, I really, I quite enjoy that.



STEVE:  They really blinged him out in one of the episodes in the fourth season.



LEO:  Did they really?  Oh, that's great.



STEVE:  Yeah.  And the way I found it was that somehow I clicked - I saw something that I think it was on Vox or something that said the show on HBO that you're not watching just started its fifth and final season, or is about to start its fifth and final season.  And I thought, oh, that's interesting.  We're ready for something.  Because we're rewatching "House" because we're sort of out of things.  So anyway.



LEO:  Well, you and I after - and every once in a while I'll get a text from Steve saying "You've got to watch this."  You and I after maybe we'll compare notes because we've found a few shows that we're quite enjoying.



STEVE:  Ah.



LEO:  I know you're a "Succession" fan, so you've got to be watching this on Sunday nights.



STEVE:  Yes, well, actually, we're holding off.



LEO:  Oh, that's smart.



STEVE:  So that we can do the whole thing.  Because these cliffhangers...



LEO:  But it ain't easy because there's a lot of spoilers on the Internet, and there's some real spoilage that can happen.  So just close your eyes.



STEVE:  Well, the good news, Leo, is I don't use the Internet.



LEO:  Oh, smart.  Yeah, stay off Twitter, and you're okay.



STEVE:  Yes.  I do not think...



LEO:  I do not have that luxury.  I wish I did.



STEVE:  I don't think WiFi works, and I don't use the Internet.



LEO:  You know what's funny, yesterday I was telling Lisa, because we've been having IoT troubles and all sorts of stuff, and I said, "I can't wait till I can retire and I don't have to use technology anymore.  We're going to move to a cabin in the woods.  All I need is hot and cold running water and electricity.  Forget the Internet."  She doesn't believe me.



STEVE:  So there, Leo, is half of our show from the old days.  That consumed 10 minutes.



LEO:  Right there.  The happy talk about - yeah.



STEVE:  None of our listeners will ever be able to get back.  Okay.  So this week, because the UK's Online Safety Bill continues to stir up a hornet's nest of worries and concerns within many industries, we're going to examine WhatsApp's reaction to Signal's "We plan to walk" position.



LEO:  Oh, yeah.  Oh, yeah.



STEVE:  And Wikipedia's concerns over the bill's age verification requirements.  And, undaunted, I have another idea that might be useful.



LEO:  Never stop coming up with ideas, Steve.  We love you for that.



STEVE:  That's right.  So we also have a new UDP reflection attack vector, lots of amplification in it; a welcome, and late, update to Google Authenticator; more NSO Group client news; a Russian OS?; the unintended consequences of releasing updates for routers that won't actually ever be updated; a smart move by Intel with pre-release security auditing; yet another side-channel attack on Intel CPUs; cURL's maintainer implores Windows users not to delete it; and VirusTotal gets AI.  So I think a great podcast.  I titled this "OSB OMG and Some Other News."



LEO:  Well, we'll get to it PDQ.  The Picture of the Week, Steve.



STEVE:  Okay, so...



LEO:  This is wild.



STEVE:  And Leo, it has to have been photoshopped.  Don't you think?



LEO:  Yeah.  Nobody would do this on purpose.



STEVE:  No, no one could build this.  So for those who don't have the advantage of video, what we have is a bizarre swing set where just past the bar that holds the chains mounting the swings is a brick wall so that, you know, you can't swing because the moment you would go past center you'd be hitting this brick wall.  Literally.  Maybe kids could sit there, like push off of the brick wall with their feet?  I mean, I just don't know what is...



LEO:  But as soon as it came back, boom, you're going to hit the wall.



STEVE:  It's not good.



LEO:  That's crazy.  That's crazy.



STEVE:  Yeah.  So but the picture captivated me, so it made it into this week's show.



LEO:  I believe it, Steve.  I think it really happened.



STEVE:  Do you?



LEO:  No.  I'm just trying to give you credibility.  No, that's definitely real.  What I like is it's like the best built swing set you ever saw.  I mean, this thing is...



STEVE:  It's very sturdy.



LEO:  Sturdy.



STEVE:  Except it doesn't do anything good.  Anyway, so it came with the caption that I used, which is "Security should never be added as an afterthought."



LEO:  Love it.



STEVE:  As if to say that the brick wall was added after the fact and, whoops, you know, it broke the swings.  But I'll bet the swings wouldn't work without the brick wall.  So, you know, anyway, I just - it was wacky enough that I thought, okay, we don't really have an explanation for this, but we'll just put it in the show notes.



Okay.  So before we get into everything that happened since last week's podcast, I wanted to follow up on last week's topic about the clear collision of encryption ideologies we're in the midst of witnessing.  While I don't have anything completely novel to say, I want to go on record:  I adamantly hope that not one of the encryption providers backs down from their absolutist position, which I do believe is the only tenable position for our industry to take.



No one wants to provide cover to any community of law breakers of any kind, certainly not anyone trafficking in child pornography or terrorism.  But the fact is that, while it's not zero, the illegal use of technology represents an infinitesimal minority of the technology's total user base.  Everyone else, who are law abiding users, will obtain clear benefits from access to technology which protects their privacy to the maximum degree possible.



For the past several weeks we've been talking about the thriving marketplace for commercial smartphone spyware, a market created by the very same governments who want to expand their ability to monitor, not just targeted individuals, but everyone's private communications.  Sadly, government bureaucracies are too large and too unaccountable to be trusted.  Edward Snowden provided a wakeup call, and the revelations have never stopped since.



This podcast draws lessons from events, which is I think much more useful and enduring than a dry recitation of the weekly news.  One such lesson we have seen demonstrated time and again is that, if it is possible for privacy to be breached, it will be breached.  We must not willingly and knowingly provide deliberately breachable tools to unaccountable governments.



The final piece of this argument is that, as we know, and as we've often said, it would not work anyway.  If fully private encryption is outlawed, only outlaws will be using fully private encryption.  As we've often observed here, now that such encryption already exists, it's never not going to exist again.  So the only thing that will happen if the tools everyone uses should lose their privacy by law, is that privacy will be obtained outside the law.



Okay.  So this leaves us with a question.  What's going to happen?  No one knows, which is I think what makes this so interesting.  There doesn't appear to be any possible way to compromise.  Either governments have no officially sanctioned way to monitor communications, or they do.  It's glaringly binary.  If the UK's current proposal were to be enacted into law, there would presumably be some length of time provided for encrypted service providers to come into compliance.



Then there would be three choices.  One, a company could choose to tough it out and call the UK's bluff by simply ignoring the law and continuing to offer fully encrypted and unmonitored communications.  Or, two, they decide to comply, and during the grace period they add the technology for side-channel monitoring to their product.  This complies with the UK's requirement by copying all communications to a central repository for content screening.  It's still unclear how the requirement to prevent "grooming" is accommodated, but it begins with monitoring.  Or, three, they just say no to compromising their users' privacy.  They choose to boycott the UK, removing access to their service from all UK users.



Okay.  Option one, toughing it out and calling the UK's bluff, doesn't appear to be practical.  A piece of related news I'll share in a moment from the BBC contains the line:  "If a service does not comply with the bill, there can be serious consequences potentially including large fines, criminal sanctions for senior staff, or restricting access to a service in the UK."



JD Supra, a legal news site, had this to say about the Online Safety Bill's penalty provisions.  They wrote:  "Penalties available include fines of up to 18 million pounds or, if higher, 10% of global turnover" - meaning their global revenue - "and which for the larger providers could be significant sums.  In addition, it can impose business interruption measures including, ultimately, service restrictions.



"A particularly controversial measure has been the availability of up to two years' imprisonment for senior managers who suppress, destroy, or alter information requested by Ofcom" - remember that's the UK's communications regulator - "who fail to comply with, obstruct, or delay Ofcom when exercising its powers of entry, audit, and inspection; for providing false information; or for employees who fail to attend or provide false information at an interview.  A recent amendment also provides a further offense where a senior manager has 'consented or connived in ignoring enforceable requirements, risking serious harm to children.'



"For these purposes, a 'senior manager' is if the individual plays a 'significant role in the making of decisions about how the entity's relevant activities are to be managed or organized; or, B, the actual managing or organizing of the entity's relevant activities."  And they finish:  "The OSB is in the latter stages of the legislative process; and while substantive amendments may still be made, it is likely to receive Royal Assent by midyear.  The OSB may also have the unintended effect of causing terms of service to be watered down as to what content a service may contain.  Ultimately, some providers may decide it is simply too difficult to comply with, and instead block UK users."



Okay, well, there's obviously no way Tim Cook or Mark Zuckerberg are going to prison over this.



LEO:  Yeah.



STEVE:  You know?  Nor are they going to give up 10% of their respective companies' annual revenue.



LEO:  No, that's like, for Apple, that's like $36 billion.  That a massive amount of money.



STEVE:  Not going to happen.  So ignoring the law doesn't appear to actually be a practical option.  And note that the law was clearly crafted to make exactly that fact exceedingly clear.  But I wanted to cover that case since it was theoretically one of the three possibilities.



Okay.  So it turns out that I wasn't totally out in the weeds last week with my proposed design for a device-centric age verification solution, and something like that may actually be the right answer.  Even if the Online Safety Bill (OSB) fails, as we all hope it will, in enforcing the monitoring of all private communications within the UK...



LEO:  There is now a U.S. bill called the CSAM Act because they know, you know, nobody's going to vote in favor of child porn.  So this is going to be a problem everywhere, not just in this case.



STEVE:  Yes, that was the thing that the Senate enacted on Wednesday that you're talking about?



LEO:  Yeah, yeah.



STEVE:  Yeah.  So, you know, regardless of how in this case the UK bill's authors want it to be "interpreted," you know, what the Bill requires is...



LEO:  A back door.  A back door.



STEVE:  ...side-channel monitoring, yes.  Its extensive provisions, which require the moderation of all content made available specially to those under the age of 18, those provisions appear very likely to survive.  So it appears that some answer to the unsolved challenge of online age verification is going to be needed.



Okay.  So this was highlighted by some news that was covered four days ago by the BBC in their piece titled "Wikipedia will not perform Online Safety Bill age checks."  Okay.  So here's what the BBC wrote.  They said:  "The Wikipedia Foundation says it will not comply with any age checks required under the Online Safety Bill.  Rebecca MacKinnon, of the Wikimedia Foundation, which supports the website, says it would 'violate our commitment to collect minimal data about readers and contributors.'



"A senior figure in Wikimedia UK fears the site could be blocked as a result.  But the government says only services posing the highest risk to children will need age verification.  Wikipedia has millions of articles in hundreds of languages, written and edited entirely by thousands of volunteers around the world.  It is the eighth most-visited site in the UK, according to data from analytics company Similarweb."



LEO:  Yeah.  And of course many of those are kids.  And it's just a matter of time before they decide that, oh, no, Wikipedia counts. 



STEVE:  Right.  Exactly.



LEO:  And I'm sorry, yeah.



STEVE:  And that is the concern here.  "The Online Safety Bill currently before Parliament places duties on tech firms to protect users from harmful or illegal content and is expected to come fully into force sometime in 2024.



"Neil Brown, a solicitor specializing in Internet and telecom law, says that under the bill, services likely to be accessed by children must have 'proportionate systems and processes'" - I know - "designed to prevent them from encountering harmful content.  That could include age verification.  Lucy Crompton-Reid, chief executive of Wikimedia UK, an independent charity affiliated with the foundation, warns some material on the site could trigger age verification.  'For example,' she said, 'educational text and images about sexuality could be misinterpreted as pornography.'



"But Ms. MacKinnon wrote:  'The Wikimedia Foundation will not be verifying the age of UK readers or contributors.'  As well as requiring Wikipedia to gather data about its users, checking ages would also require 'a drastic overhaul' to technical systems."  In other words, they don't have any capability to do that.  "If a service does not comply with the bill, there can be serious consequences potentially including large fines, criminal sanctions for senior staff, or restricting access to a service in the UK.  Wikimedia UK fears that the site could be blocked because of the bill and the risk that it will mandate age checks."



Okay.  Now, "Ms. Crompton-Reid wrote:  'It is definitely possible that one of the most visited websites in the world, and a vital source of freely accessible knowledge and information for millions of people, won't be accessible to UK readers, let alone UK-based contributors.  There are currently 6.6 million articles on Wikipedia,' she said.  It was 'impossible to imagine' how it would cope with checking content to comply with the bill.  She added:  'Worldwide, there are two edits per second across Wikipedia's 300-plus languages.'



"The foundation has previously said that the bill would fundamentally change the way the site operated by forcing it to moderate articles rather than volunteers.  It wants the law to follow the EU Digital Services Act, which differentiates between centralized content moderation carried out by employees and the Wikipedia-style model by community volunteers.  Last Tuesday, the House of Lords debated an amendment from Conservative peer Lord Moylan that would exempt from the Online Safety Bill services 'provided for the public benefit,' such as encyclopedias.  Heritage Minister Lord Parkinson said he did not think this would be feasible, but added that Wikipedia..."



LEO:  Ha.  Ho.



STEVE:  Uh-huh, "...was an example of how community moderation can be effective."  Meaning figure it out, and you guys do it.  "He said the bill did not say that every service needed to have age checks, and it was expected that 'only services which pose the highest risk to children will be required to use age verification technologies.'  Ms. Crompton-Reid told the BBC that while Lord Parkinson's remarks 'reassured' her, the charity did not want to be relying on" - as you said, Leo - "on future goodwill and interpretation of legislation.  She said they would continue to urge that protections to community moderation were in the bill through measures such as the exception for public benefit websites like Wikipedia.



Okay.  So our takeaway from that is that, while Wikipedia will refuse to comply with age-related regulation, it hopefully is unlikely that Wikipedia would actually be required to do so.  You know, it's not a porn site.  They would appreciate receiving confirmation that this will not be necessary in order to remove any uncertainty.  But there certainly are services that will be required to provide age verification, such as legal websites which make it their business to provide extremely adult sexual content.



LEO:  Oh.  This just happened in Utah.



STEVE:  Uh-huh.



LEO:  And Pornhub has withdrawn from the state.



STEVE:  Exactly.  Yeah, exactly.



LEO:  The problem is, if it's just England, you know, honestly, fine.  Bye-bye.  But it's not going to just be England, and that's the problem.  It really is.



STEVE:  Well, and that's why I'm hoping that we're going to see, I mean, so this stuff is separate from encryption providers.  I think encryption providers are going to just decide this is the hill they want to die on, and they're just going to say no.



LEO:  That's what Signal says, yeah.



STEVE:  Yes.



LEO:  Signal says no, we're not - it's not going to happen.



STEVE:  So last month...



LEO:  And I think honestly encryption is going to end up being civil disobedience.  Using encryption's going to be a form of civil disobedience.  The problem is that's for individuals.  It doesn't solve the problem for these big companies.  I'm not sure how they solve this.



STEVE:  Right, right.  So last month the Guardian published what they called an "Explainer" about the age-related aspects of the impending Online Security Bill.  Their piece was titled "Will UK's online safety bill protect children from adult material?"  And their subheading was sort of the crux of it, saying "Legislation puts duty of care on tech firms to protect under-18s, but does not mandate use of specific age-checking technology."



Okay.  So they said:  "The online safety bill is due to become law this year, and it imposes a duty of care on tech companies to protect children from harmful content.  However, there are calls from campaigners and peers to toughen the legislation's provisions regarding pornography.  Here's what the act proposes to do on adult material.  The bill requires all pornography websites, such as Pornhub, to ensure children do not encounter their content.  This will require age-checking measures.  The legislation refers to stringent age verification - checking a user's age via government ID or an authoritative data source such as a person's bank - as a means of doing so.  Breaches of the act carry the threat of a fine of up to 10% of a company's global turnover or, in extreme cases, blocking a website altogether."



So what are the rules currently?  They wrote:  "MPs have described the legal approach to pornography in the UK as a 'loose patchwork' comprising more than a dozen laws.  It is a criminal offense to publish work under the Obscene Publications Act that is deemed 'obscene,' and it is illegal under the Criminal Justice and Immigration Act to possess an 'extreme' pornographic image.  It is also an offense to make, possess, or distribute indecent images of a child.



"The primary regulator of legal pornography offline is the British Board of Film Classification, which gives pornography age ratings - R18 for the most extreme but legal content, or 18 - but it has no control over online content.  Ofcom, the communications watchdog, already has the power to regulate UK-based 'video-sharing platforms' such as TikTok, Snapchat, and OnlyFans.  These platforms are required to protect under-18s from videos containing R18 material such as pornography.  The age-appropriate design code was introduced in 2021 and is designed to prevent websites and apps from misusing children's data.  Under its terms, social media platforms would be breaching the code if their algorithms served adult material to under-18 year olds.



"Age verification has been a troublesome issue for the government.  Age checking for pornography was announced as a Conservative policy in 2015.  However, plans to introduce a nationwide age verification system for online pornography were abandoned four years later in 2019.  The bill will not mandate use of specific technologies for age checking, although Ofcom will issue codes of practice on age assurance, which is the umbrella term for assessing the age of people online.  Age verification is the term for the toughest measures, such as requiring proof of official ID.



"One solution is to use age verification companies that vet a user's age, via a range of methods including checking offline ID or bank statements, and then notify the porn provider that the person wishing to access their service, who is anonymized, is over 18 years old.  Ofcom has said it will launch a consultation on protecting children from pornographic content, including on user-generated platforms such as OnlyFans, in the autumn.  The government has indicated that there will be clear instructions to mainstream social media sites and search engines to prevent children accessing pornographic content on their services."  That's right, search engines, of course, is another  problem.  "The bill requires sites to prevent children encountering what it terms 'primary priority content.'  Because it qualifies as a 'user-to-user' service, subscription site OnlyFans is also covered by this part of the bill.



"We will not know what is primary priority content officially until it is defined in a statutory instrument that will be published after the bill becomes law.  However, pornography is expected to be on that list, and it was listed as primary priority content by the previous culture secretary, Nadine Dorries, in a parliamentary statement last year.  According to a timeline published by Ofcom, though, it could be more than 18 months after the bill is passed before these provisions come into effect.  Social media sites and legal pornography sites will also be required to shield all users from illegal pornography such as obscene content and child sexual abuse material.



"The bill will update the law on sharing intimate images without someone's consent.  In England and Wales there will be a new 'base offense,' where it is an offense to share an intimate image of a person if they do not consent, and the perpetrator does not believe they have consented.  Currently, these offenses apply if the image is shared in order to cause humiliation or distress.  The base offense will now apply regardless of the motivation, including sharing it as a joke, for social status, financial gain, or 'where there is no motivation at all.'"



Okay.  So from all of that, the article's subheading seems to me the most pertinent to those of us who care and are interested in how these things work, and how they're done.  The Guardian wrote:  "Legislation puts duty of care on tech firms to protect under-18s, but does not mandate use of specific age-checking technology."  So said another way, "We don't know how you're going to arrange to do what the new laws we have just written require you to do.  But that's not our problem.  It's yours because we said so."



LEO:  This is because the old law required that you go to a pub, and I think there might have been like post offices, but somewhere where they had a system of checking ID, but most of the time it sounded like a pub, to verify your age.  And everybody said, "You're going to send 15 year olds to a pub to verify their age?"  And, oh, what about somebody who says "I'm an adult, and I want to view porn.  I've got to go to a pub and say can I have a porn license, please?"  I mean, this was a terrible plan, and they haven't solved it.  They've just said, well, it's not our problem.



STEVE:  Yeah.  Yeah, exactly.  They're saying "We don't know how you're going to do it."



LEO:  Just do it.



STEVE:  It's very much like the encryption problem; right?  Well, we don't want you monitoring everyone's content, but you have to.  You know?



LEO:  I'm so depressed by all this, Steve.  I really - it's just...



STEVE:  I know.  I know.  It's the collision that we've seen coming for a long time.  So whether it's the perceived need to monitor everyone's communications all the time, in the off chance that something illegal might pass by, or the need to impose strict age restrictions on access to Internet content and behavior, it's apparent that the UK's legislators believe that they can ask for whatever they want, leaving it up to the tech companies to figure out how to do it, while all the while imposing penalties if they fail to achieve what might well be impossible, or at least impractical, like as you said, the previous legislation that they had passed.



 So after last week's podcast I received a DM from someone who said:  "Please don't go spending another seven years solving the age verification problem like you did with the online login authentication problem."  To which I will formally respond:  "Fear not.  As they say, 'Fool me once.'  I have quite thoroughly learned my lesson."



LEO:  Oh, no, no.



STEVE:  And I'm having far too much fun working on new technology for SpinRite, where I can actually make a difference. So, you know...



LEO:  You did propose some years ago, I'll have to find the episode, a kind of some sort of third-party key escrow.



STEVE:  Yeah, and that's still - that could still be done.  But it doesn't solve - so that would allow privacy to be maintained and search warrants to be served.  That is, so that was that.  So the idea there was to try to bring into the end-to-end encryption space the similar U.S. constitutional protection where you need to prove to a court that you've done something wrong.  On the other hand, now we see what the courts have been doing lately.  So it's like, oh.



LEO:  And also, as others pointed out, if you have a backdoor, those keys leak.  I mean, it's hard to keep it.  And you proposed a very good system, you know, you had - it was well thought out.  I thought it was a very good idea.  And it may be our last best hope because it may be that, you know, it's the better of two bad alternatives.



STEVE:  But it doesn't solve this problem that the UK wants to solve.  They literally want to look to somehow screen all text messaging in case it might be grooming children.



LEO:  Yeah.  See...



STEVE:  And every image that you send in case it might be illegal content.



LEO:  So it's not - it's everything.  It's a phishing expedition.



STEVE:  Yes.



LEO:  It's a broad net catching everything.



STEVE:  Yes.  It is side-channel monitoring.



LEO:  Yeah, it's terrible.



STEVE:  That's what this online safety bill requires.  And that was the point at the beginning of this is that no one wants to provide any cover to those creeps to allow them to do what they're going to do.



LEO:  No, of course not.



STEVE:  But they will do it using illegal technology if we open up the encryption.



LEO:  Right.



STEVE:  And so that we're doing back-channel monitoring.



LEO:  Right.  It's not going to solve the problem, and it's going to compromise everybody else's privacy.  And if you think a government only cares about CSAM and grooming, you're not paying attention.



STEVE:  Well, yes.  You haven't been noticing where the cash for Pegasus is coming from.



LEO:  Yeah, exactly.



STEVE:  It's coming from governments.



LEO:  Exactly.



STEVE:  So in a little bit of happy news, recall that Signal's President Meredith Whittaker made some headlines when she told BBC News that Signal "would absolutely, 100% walk," and stop providing services in the UK, if required by the Online Safety Bill to weaken the privacy of its encrypted messaging system.  That's the only stance that any entity like Signal, Threema, or Telegram could take; right?  Because their entire existence is encrypted communications.  But what's the position of the number one most popular and largest messaging app in the UK, WhatsApp?  WhatsApp is used by more than seven in 10 adults who are online, according to the UK's communication regulator Ofcom.



Okay.  So the BBC asked Will Cathcart, the head of WhatsApp. Will replied that WhatsApp would refuse to comply if asked to weaken the privacy of encrypted messages.  Period.  Full stop.  He said that WhatsApp would rather be blocked in the UK than undermine its encrypted messaging system, if required to do so under the Online Safety Bill.  He said:  "We won't lower the security of WhatsApp.  We have never done that, and we have accepted being blocked in other parts of the world."  And he feared the UK would set an example, as you've said, Leo, other nations might follow.



Will added that undermining the privacy of WhatsApp messages in the UK would do so for all users.  He said:  "Our users all around the world want security.  98% of our users are outside the UK.  They do not want us to lower the security of the product.  We've recently been blocked in Iran, for example.  We've never seen a liberal democracy do that.  When a liberal democracy asks, 'Is it OK to scan everyone's private communication for illegal content?,' that emboldens countries around the world that have very different definitions of illegal content to propose the same thing."



After Will went on the record with WhatsApp's position, Signal's Meredith Whittaker tweeted, "Looking forward to working with @wcathcart and others to push back"; after which Will replied on Twitter, "And very important we work together, and honored to get to do so, to push back."  So what appears to be forming is a bit of an insurrection, and this may be where the encrypted services companies decide they need to take a stand.  Last week we saw their open letter to the UK regulators.  So they all know each other, and they have each other's email addresses, and they're talking.  That's all for the good.



I wondered what Apple might do, since iOS's always-encrypted iMessage is so deeply integrated into their products.  Then I considered the green bubbles.  Assuming that Apple also decides to "just say no" to government communications monitoring, they could simply drop the use of iMessage encryption and fall back to SMS whenever they're communicating inside the UK.



LEO:  There you go.  Yeah.



STEVE:  So it would mimic the way iOS devices currently operate when messaging outside of Apple's closed and encrypted ecosystem to Android devices.



LEO:  UK or Android.  Both green.



STEVE:  Yup, exactly.  And Leo, speaking of green, let's make a little green.



LEO:  All right.  We need some green right now.  This is such a good subject.  I have a feeling there's going to be mass civil disobedience.  You know, I hope you're saving your crypto code, all of you, and coding what you need to have it because it's just not going to be okay.  The problem is all these companies - eventually companies are going to have to give in.  If it's just the UK, maybe they can write it off.  If it becomes the EU, or the EU plus Australia, or the EU plus Australia plus the U.S., you know, the Five Eyes, the companies are going to comply in the long run.  They have to.  In which case it's going to be up to individuals to preserve their own privacy.



STEVE:  Yeah.  And I have - this is where my next idea comes in.  We'll get to it in a minute.



LEO:  Can't wait.  Coming up.  Yeah, that's exciting.  Yes.



STEVE:  Here comes my idea.



LEO:  Good.  Steve's idea.



STEVE:  I want to get off this topic, but there's one more thing I need to share.  It's just a concept and observation that I want to plant in everyone's mind.  Given WhatsApp's stance, which aligns with all of the other encryption providers, I doubt this idea will be needed.  I truly hope that's the case, and I believe that if everyone just says no, that's likely to work.  I doubt that the citizens of the United Kingdom would choose to be without all of their messaging capabilities with each other and with the rest of the world, especially when having that happen would only serve to drive the creeps further underground.  But, if just saying "no" doesn't work for some reason, we may need a fallback.



One of the mixed blessings of today's technologies is that most people have no idea how they operate.  And for the most part that's good.  You should not need to be a car mechanic to drive a car.  That's the leverage provided by technology.  But this also means that most car drivers have very little idea what's going on under the hood.  If they don't need to know, then not knowing is fine.  But if there's a problem, some knowledge could come in handy when it comes to making decisions.



For quite a long time, third-party cookies lived in obscurity.  They were always there, but by design they remained part of what was under the hood, out of sight and out of mind.  And third-party cookies liked it that way.  But no one who was asked whether they wanted to have third-party tracking cookies said that they thought that would be a great idea.



Tracking has a similar history.  For years it's been going on largely unseen, often aided by those same third-party cookies, and it has enabled an entire online web surveillance industry.  But when Apple began requiring iOS applications to obtain explicit permission to allow the apps' users to be tracked outside of the application, the result was an overwhelming and resounding cry of "No, thank you."  And that's putting it politely.



And I had my own firsthand experience with people being unhappily surprised.  Leo, you used to introduce me by mentioning that I discovered the first spyware and in the process coined that term.



LEO:  Right.



STEVE:  What happened was I discovered that a freeware utility I had installed on my Windows machine - as I recall it was an early version of WinZip - wasn't as free as I was led to believe.  It was an "ad-supported application," and so it brought along an advertising DLL from a third-party company named Aureate.  What I discovered was that this Aureate spyware was inventorying my machine, monitoring my actions.



LEO:  Oh, I remember this, yup.



STEVE:  Monitoring my actions and usage, and then phoning home without ever obtaining any permission from me.  I had no idea it was there, and to say that I was unhappy when I found something communicating behind my back without my knowledge or permission would be an understatement. So I created "OptOut," the world's first spyware removal tool, a bit of freeware which successfully removed Aureate and several other early forms of stealth spyware.



The reason I bring this up is that the management at Aureate shared with me some of the way-over-the-top enraged and nearly psychotic emails they were receiving from PC users who were more than just a bit unhappy to have used OptOut to discover that their machines had also been infected.  The Aureate people said that the ad-supported software packages which installed their spyware - I mean their adware - were supposed to explain the situation and obtain their users' consent.  I asked why they didn't have their DLL present its own permission dialog.  They didn't reply.  Anyway, the name Aureate had been ruined by my crusade, so the company renamed itself to Radiate, and not long after ceased operations.



LEO:  Yeah.  Victory.



STEVE:  The whole concept was really never viable, and after this no freeware developer wanted anything to do with them.



Okay.  So what does all this have to do with the UK's Online Safety Bill?  What occurred to me was that, if encrypted applications were going to be required by law to arrange some sort of side-channel government-mandated monitoring, you know, eavesdropping and surveillance, they should make very clear to their user that that's what they are doing, and not repeat past mistakes of doing things that people would find objectionable if they were clearly informed of what was going on.



So the presence of state-mandated communications surveillance should be placed front and center for every UK resident, and anyone they communicate with.  The top of any application that's being forced to break into its users' privacy in order to comply with the UK's Online Safety Bill should clearly display against a red background the message:  "This communication is being monitored by your government."



I imagine that the presence of that notice at the top of any communicating application might provoke a reaction similar to what happened when the news of the Aureate spyware broke.  The UK government will clearly wish the fact of this being hidden from its citizens' view.  But it should be there to serve as a constant reminder of what the country's politicians have decided is in the best interests of their citizenry.  And could you imagine, you know, a little red banner up at the top, "This communication is being monitored by your government."  That'll provoke some change.



Okay.  So we've got some Closing-the-Loop feedback from our listeners.  TWS tweeted:  "Hi, Steve.  Are you still using the ZimaBoard you mentioned many episodes ago?  Would you still recommend?  With the shortages of Pi's I'm considering them.  Thanks for making SpinRite and the SN podcast."



In a word, yes, yes, yes.  Oh, okay, wait, that's three words.  The ZimaBoard is the best thing I've found for my own work during the development of SpinRite.  It is perfect.  And while I haven't taken a show of hands in the Spinrite.dev newsgroup, I keep seeing people referring to their ZimaBoard in passing.  So I know I'm not alone.  But also for other uses, it is really a perfect little machine.  It's been around long enough that there are now a ton of YouTube How-To videos covering pretty much anything you can think of.  Just go to YouTube and put in Zima, Z-I-M-A-B-O-A-R-D, and you'll see.



And while working, as it happens, to assemble today's show this morning, I received an email from them announcing a Star Wars Day discount of 20%.  Their email said:  "May the 4th be with you."  Apparently there's a little bit of lisp.  And their emailing said that the sale runs for three days, from tomorrow, May 3rd, through Friday the 5th, offering, again, a 20% discount.  So yeah, ZimaBoard.com.  And if I didn't already own five of them, I'd be purchasing some more.  It was a real find.



But remember, unlike the Raspberry Pi, which is ARM-based, the ZimaBoard is Intel-based.  That makes it incredibly useful to me, but you'll want to be sure that whatever you want to do with it you can do with Intel-based software.  And definitely check out YouTube.  And I just wanted to make sure that everyone knew.  We have a bunch of news we'll be covering after I deal with a couple of these little blurbs from feedback from our listeners.



David Schofield, yeah, he said:  "Good morning, Steve.  SN listener since 2007, SpinRite user since my ST-225s."  Which of course were Seagate 20MB drives.



LEO:  Oh, yeah, I remember them.  I had them, too.



STEVE:  And he said:  "Encouraging news about Microsoft rewriting parts of Windows in memory-safe Rust."  So his note is a perfect segue for me mentioning a bit of news I had encountered.  David's direct message to me linked to an article in The Register.  Here's just the top of their piece.  They said:  "Microsoft is rewriting core Windows libraries in the Rust programming language, and the more memory-safe code is already reaching developers.  David Weston, director of OS security for Windows" - he's got to have an interesting job - "announced the arrival of Rust in the operating system's kernel at BlueHat IL 2023 in Tel Aviv, Israel last month.  He said:  'You will actually have Windows booting with Rust in the kernel in probably the next several weeks or months, which is really cool.  The basic goal,' he continued here, 'was to convert some of these internal C++ data types into their Rust equivalents.'"



So The Register continued:  "Microsoft showed interest in Rust several years ago as a way to catch and squash memory safety bugs before the code lands in the hands of users.  These kinds of bugs were at the heart of about 70% of the CVE-listed security vulnerabilities patched by the Windows maker" - meaning Microsoft - "in its own products since 2006.  The Rust toolchain strives to prevent code from being built and shipped that is exploitable, which in an ideal world reduces opportunities for miscreants to attack weaknesses in software.  Simply put, Rust is focused on memory safety and similar protections, which cuts down on the number of bad bugs in the resulting code.



"Rivals like Google have already publicly declared their affinity for Rust.  Amid growing industry support for memory-safe programming, Microsoft's exploration of Rust has become more enthusiastic.  And last September it became an informal mandate:  Microsoft Azure CTO Mark Russinovich declared that new software projects should use Rust rather than C and /C++."



And of course, as we know, all of the evidence suggests that we're not really making any headway with simply trying to be more careful using powerful but unsafe legacy languages.  Our software is growing more and more complex, and people make mistakes.  The adoption of newer languages which prevent those mistakes from proving fatal to a system's security looks like the only way we're ever going to get to the point where we start removing more existing bugs than we are introducing new bugs as we go forward.  So yay to Microsoft for making this move.



Frank S. tweeted:  "Dear Steve.  I have two kids, and I am very happy that they are under seven.  This gives me and the market some time to find best, or better, practices for children using the Internet.  As a parent, I want my children to be safe, both physically and online.  However, I don't think that we can stop existing CSAM images that are already out there.  What we can and should do is try to prevent new cases and victims.  I believe it is up to the parents to take better care and guide our children when growing up.  And I don't want the government to spy on them.  As a parent, I would like more tools to keep my children safe online."



So I wanted to use Frank's note as a catalyst to thank all of our listeners who took the time to write after last week's podcast.  It's clear that everyone understands that the Internet is a true mixed blessing when it comes to our youth, who haven't yet obtained the life experience which would allow them to place some of the horrific crap they might encounter on the Internet into its proper context.  Unfortunately, bad and taboo things can be exciting, and excitement can be addictive.



I was brought up short by a tweet I received which noted that, unfortunately, giving parents control and oversight over their children's communications could be harmful to the child when the child's nature is rejected and not understood by their parents.  In such situations, having private communications creates a potential sanctuary.  I think my answer to that situation, which I can certainly imagine and empathize with, is to observe that the Internet didn't create such problems.  It's just another part of a complex world.  And I think that it's easy to make the mistake  which I would argue UK legislators are making  of assuming that all such problems can be solved by the proper application of technology.  I'm certain that's not true, and I think it's possible to get ourselves tied up in knots trying to whack every mole.



LEO:  Well, and I sympathize with Frank and as every parent.  But I have to say the problem is not people like Frank or his kids or their access to the Internet.



STEVE:  Right.



LEO:  The problem is from parents unlike Frank who don't care, who exploit their children.  Most abuse of children comes from relatives and people they know or their parents know.  It is not from caring parents like Frank.  And frankly, it's not from the Internet.  And I think that all this focus on, oh, they're groomers, they're going to get you in an AOL chatroom, and then all of a sudden you're going to go out and do child porn, is really, I think, misdirecting it.  And maybe it's easier for legislators to attack technology.



STEVE:  Well, and it's call a "straw man"; right?



LEO:  It's a straw man.



STEVE:  Yup.  Which is not to say that it can't or doesn't happen.



LEO:  Right.



STEVE:  But that the purpose is not sincere.



LEO:  No, the real danger is inside the house when it comes to children.  It's relatives and people they know.



STEVE:  Yup.



LEO:  Protect them from those people.  Frank, you're doing a great job.  I don't think you have to worry about a technological solution to protect your kids.  If you're just paying attention, that's all you need to do.  It isn't that hard.



STEVE:  Yes, be involved.



LEO:  Be involved.  But the problem is not there.  I mean, there are a lot of parents who aren't involved, who don't care, or worse, participate.  And, you know, we do need to catch those people. 



STEVE:  Yeah.



LEO:  But there are ways to do that.  And all you're going to do is drive these things underground.



STEVE:  Yup.



LEO:  That's not going to solve it either because, if you're motivated, you'll find a way to get it done.  Anyway, I agree with you, Steve.



STEVE:  Yeah.  So we've been focused upon the UK so far, but this is probably a good time to mention what you had said earlier, Leo, a new U.S. federal bill was announced and unveiled last Wednesday aiming to regulate access by age to social media platforms in the U.S.  Here's a bit of CNN's coverage of this new proposal.



They said:  "A new federal bill unveiled Wednesday would establish a national minimum age for social media use and require tech companies to get parents' consent before creating accounts for teens, reflecting a growing trend at all levels of government to restrict how Facebook, Instagram, TikTok, and other platforms engage with young users.  The proposed legislation by a bipartisan group of U.S. senators aims to address what policymakers, mental health advocates, and critics of tech platforms say is a mental health crisis fueled by social media.



"Under the bill, known as the Protecting Kids on Social Media Act" - that's PKSMA, okay, that doesn't say anything, that's good - "social media platforms would be barred from letting kids below the age of 13 create accounts or interact with other users, though children would still be permitted to view content without logging into an account, according to draft text of the legislation.



"Tech platforms covered by the legislation would also have to obtain a parent or guardian's consent before creating new accounts for users under the age of 18.  The companies would be banned from using teens' personal information to target them with content or advertising, though they could still provide limited targeted recommendations to teens by relying on other contextual cues.  It's the latest step by lawmakers to develop age limitations for tech platforms after similar bills became law this year in states such as Arkansas and Utah.  But the legislation could also trigger a broader debate, and possible future court challenges, raising questions about the privacy and constitutional rights of young Americans.



"Speaking to reporters Wednesday, Hawaii Democratic Senator Brian Schatz, an architect of the federal bill, said Congress urgently needs to protect kids from social media harms.  Schatz said:  'Social media companies have stumbled onto a stubborn, devastating fact.  The way to get kids to linger on the platforms and to maximize profit is to upset them, to make them outraged, to make them agitated, to make them scared, to make them vulnerable, to make them feel helpless, anxious, and despondent.'"



And I'll just note that this discovery is not unique to social media platforms appealing to children.  Precisely the same observation has been made by cable news outlets about how to engage and enrage their audiences to encourage viewership.  I'm not sure what "gen" we're on now, Leo, X, Y, or Z.  I've lost track.



LEO:  Me, too.



STEVE:  But young people have grown up with the Internet and hundreds of cable channels, each with their own agenda.  I hope they can figure out how to handle the mess we have made of this.



So speaking of messes, and then we're going to get to the week's news, Simon Zerafa, a good friend of the show, he tweeted:  "Shodan.  Twitter canceled our API access, which broke the ability to log into Shodan via Twitter using single sign-on.  Email us at support@shodan.io if you're currently logging in via Twitter and would like to migrate to a regular Shodan account instead of using single sign-on."  Yikes.



And this reminds us that aside from the well-appreciated privacy implications of using OAuth-style "Sign in with" whatever, Facebook, Google and so forth, authentication, if that third-party authentication service should ever become unavailable for any reason, you're hosed since the only way you're known by the site you're wishing to log into is courtesy of that other third-party entity which may no longer exist.  So, yeah, another downside of the super convenient logon using somebody else.  You'd better hope those "else" people stick around.



LEO:  I had a little tiff with Dave Winer a few months ago because all of his online services, which are very cool, use Twitter OAuth.  And I said, "Dave, can you just do something else?"  He said, "No, no, this is good."  I wonder how he feels now.



STEVE:  Yeah.  All the people that were using that for free.



LEO:  Not free.



STEVE:  Yup, exactly, not free.  Let's take our last break, Leo, and then we're going to plow into the News of the Week.



LEO:  Yeah.  His defense was it's easy.



STEVE:  Uh-huh.



LEO:  I'm not a fan, as you know, of third-party OAuth.



STEVE:  No.  So we've previously talked about UDP reflection attacks. Unlike TCP connections, which are inherently bi-directional and therefore require packet round trips between the endpoints to establish byte numbering and other connection parameters, the UDP protocol is often referred to as  "connectionless" because, although it's still possible to establish connections by mutual agreement, UDP doesn't have that baked into its protocol.  This makes UDP the perfect protocol for DDoS bandwidth flooding attacks since the sender of a UDP query can spoof the UDP packets that they're sending out, spoof its source IP so that the recipient of a query will redirect its reply to the victim of the attack.



What's then needed are publicly exposed and available UDP protocol services which will generate a large answering reply from a very small query.  This is known as the UDP query amplification factor.  How many times larger is the reply than the query?  A not very exciting example of such a service is good old DNS.  A relatively small query for a DNS record can return a significantly larger answer.  But DNS is optimized for very small size, and its internal compression is really quite clever.  So as I said, DNS is not very exciting.



We're revisiting this subject today because security researchers from Bitsight and Curesec have stumbled upon a way to exploit a network service that was only ever intended for internal LAN use; but for which, for some reason, about 70,000 instances are currently exposed on the public Internet.  The service in question is the Service Location Protocol (SLP).  And by cleverly abusing it through the means that these researchers discovered, and unfortunately have now published in full with all the exploit details, UDP query amplification factors as high as 2200 to 1 can be achieved.  This makes this technique one of the largest amplification factors ever discovered. And since the service is available over UDP, it is ready made for DDoS flooding.



Because of the protocol's huge potential for DDoS attacks, both Cloudflare and NetScout have said that they expect the prevalence of SLP-based DDoS attacks to rise significantly in the coming weeks, once threat actors learn to exploit it.  The only good news is that since SLP is transported over port 427, and since it has no business being exposed on the public Internet, like it's for printers and things to find each other, it's a way for a printer to broadcast its existence and be found, so it would only make sense in a LAN context.  It's only exposed publicly due to the typical mistakes of having ports exposed that shouldn't be.



Anyway, because it has no business being out on the public Internet, I would expect that many savvier carriers like Cloudflare will already be proactively blocking that port traffic at their borders.  They just don't need to allow port 427 stuff to get even close to its targeted victims.  There's no reason for it.  But that won't help any unprotected targets.  So DDoSers have had another arrow added to their quiver.  And of course they don't lack for arrows, unfortunately.



Google Authenticator, which was first released 13 years ago in 2010, has just been updated with an extremely useful new feature which I've had in my favored iOS OTP Auth app from the start: Cloud backup.  In their announcement of this groundbreaking technology, they wrote:  "One major piece of feedback we've heard from users over the years was the complexity in dealing with lost or stolen devices that had Google Authenticator installed.  Since one-time codes in Authenticator were only stored on a single device, a loss of that device meant that users lost their ability to sign into any service on which they'd set up two-factor authentication using Authenticator."  Very much like using Twitter to log on.



"With this update we're rolling out a solution to this problem, making one-time codes more durable" - that's what you want in your one-time codes, some durability - "by storing them safely in users' Google Accounts."  What a concept.  "This change means users are better protected from lockout, and that services can rely on users retaining access, increasing both convenience and security."  Talk about upselling a small feature.  Anyway, that's great.  Somewhat odd that it took them this long to get it, but I wanted to make sure that everyone listening who might still be using Google Authenticator - I was at one point, before I moved over to OTP Auth - would know of this critically useful new feature.  And I would imagine you definitely want all of the private secret keys that are in Authenticator to be backed up so that should you need them somewhere else, you can get them.  And as you know, I'm...



LEO:  It's not currently end-to-end encrypted.  When are they going to do that?



STEVE:  What is not?  Google...



LEO:  The backup.



STEVE:  Oh, no kidding.



LEO:  Yeah.



STEVE:  So maybe that's what they added was some means of having a secret key.



LEO:  So it syncs, but it syncs unencrypted.  So Mysk, M-Y-S-K, was the group or person who discovered this.  And Google has confirmed it.  And they said, well, we're going to add that later.  But we thought it would be just faster just to put this out right now.



STEVE:  Because, Leo, after 13 years, we suddenly need to hurry.



LEO:  Yeah, why not?



STEVE:  That's right.



LEO:  So you haven't seen anything that - so data syncs between devices with the new Google Authenticator update could be viewed by third parties.  Google says the app works as planned.  Christian Brand of the Google Group Product Manager Identity and Security tweeted that this is our intention.  Because I guess E2E would be hard.  I don't know.  The lack of end-to-end encryption also means Google has a transparent view into what services each account owner uses as it's being transmitted.



STEVE:  Ah, that's true.  It's not client-side encrypted.  So they're getting that.



LEO:  Mysk found the app does not expose the 2FA credentials associated with the user's Google account.  So that's still secure.



STEVE:  Okay, everybody, let me tell you about OTP Auth. 



LEO:  Tell us what the name is of the one you use.  OTP Auth?



STEVE:  OTP space Auth, and the logo is a simple gray padlock.  Very modest logo.  And it does all of this correctly.



LEO:  Yeah.  And I've been using one that's open source called 2FAS Authenticator.  And the way it works is you encrypt it client-side, and then it will put it on your iCloud or your Google Drive in an encrypted blob.



STEVE:  Right, as an encrypted blob, right.



LEO:  Which you then can download and so you can move it around.  Both you and I have this problem, well, me maybe more so than you, of moving from device to device like a butterfly sampling nectar.  And so I have to do this all the time.  So I for a long time...



STEVE:  That would be a good description, yes, Leo. 



LEO:  Yes.  A long time I used Authy, but Authy has that - it does encrypt, but has a disadvantage of storing it in the cloud, their cloud.  So I like this 2FAS.  And it is open source, which I like, so it's free.



STEVE:  Good.



LEO:  Anyway.



STEVE:  Apparently multiplatform?



LEO:  Yes.  Two good choices, though.  And, yeah, some day Google says we're going to add end-to-end.  And when they do...



STEVE:  That will be great.



LEO:  Yeah.  Bran said on Twitter:  "The extra protection offered by end-to-end encryption was set aside to balance against 'the cost of enabling users to get locked out of their own data without recovery.'"  Which is always the excuse for not using encryption; right?



STEVE:  So in my drawer I have all of my QR codes printed.



LEO:  You print them out, and you put them in a notebook; right?  



STEVE:  That's right.



LEO:  Still doing it? Yeah.



STEVE:  They're in a safe place.  And if it ever comes to the point where I need to set up a new authenticator, not a problem.  I just scan the QR codes once again, and we're back in business.  So the other thing to look for is an authenticator that will allow you to do that because it is nice to have hard copy backup.



LEO:  I agree.  I agree.



STEVE:  I have encrypted my most important accounts with two-factor authentication.  And it's the right thing to do.



LEO:  Oh, god, yes.



STEVE:  Okay.  So I suppose we should not be surprised that Israeli law enforcement is apparently using their own homegrown NSO Group's spyware to spy on their own citizens, in response to reports in Israeli media, which claim that their police have been using a reduced-strength version of the NSO Group's Pegasus spyware known as Saifan.



LEO:  Saifan Lite.



STEVE:  I guess, yeah, I guess you pay less for it, Leo, if it uses a reduced-strength.  You don't need the government or the  military-grade Pegasus.  So you're just some cops, so we're going to give you the reduced...



LEO:  Reduced grades.



STEVE:  Yeah, Pegasus Lite, Saifan, to target activists, business figures, reporters, and politicians.  And so, perhaps to save face, the Israeli government has announced the formation - oh, we'll be so glad to hear this, Leo - of a commission.



LEO:  Oh, well.



STEVE:  To probe into the use of spyware by police forces to hack the smartphones of Israeli citizens.  Which made me think, isn't the definition of a "commission" the place where sensitive political issues go to die?



LEO:  Yes.  So they actually admit that they're hacking politicians' and reporters' smartphones.  That's stunning.  At least they're using the Lite version.



STEVE:  Yes.  The announcement of the commission makes everyone happy, and no one can say that the government isn't doing anything.  "Hey! There's a commission for that."  But then, after a few years of inaction, it will be quietly disbanded.  In any event, if the Israeli media reports are accurate, it was interesting that Israel's own police are also getting in on the act.  But the privacy of activists, business figures, reporters, and politicians is being breached.



LEO:  Unbelievable.



STEVE:  Yeah.



LEO:  Oh, my gosh.



STEVE:  Unfortunately, too believable.



LEO:  Imagine if the, I mean, I'm sure our government does it, too.  But imagine if they admitted that.  The uproar.



STEVE:  Ooh.



LEO:  Yeah, the FBI's monitoring the smartphones of reporters and activists and politicians.  Ay ay ay. 



STEVE:  Yeah.  So rarely have I wanted a Russian translation of anything.  But the article in Russia's Kommersant news was written in Russian, and I didn't see any easy way to translate it into English.  And by the way, Leo, those Russian characters are really...



LEO:  Cyrillic.



STEVE:  Glyphs and fonts, it's weird.  They're weird-looking.



LEO:  Yeah.



STEVE:  But the news is that the Russian government is working on a law to force retailers to pre-install Russian operating systems on all new PCs sold in the country, instead of Windows. The first wave of feedback claims that this will lead to an increase in laptop and PC prices across Russia.  Now, despite efforts to get Russian companies and users to move to Russian operating systems, Windows' market share remained the same in Russia as it was last year.  And it would be interesting to see what a Russian operating system looks like.  I think it has to be a derivative of Linux.  You know, that's the only thing that I can imagine would be feasible in this day and age.  We've talked about how you just can't start from scratch and create Russki OS.  That just - I don't how you'd do that.  But if it's a derivative of Linux, why would it be more expensive than Windows?  You know, why would it increase the cost?



LEO:  Because capitalism.



STEVE:  Right.  So.



LEO:  By the way, you should watch the Apple TV show "Tetris" about the history of Tetris.  It's fascinating.  Speaking of capitalism.



STEVE:  Oh, cool.  I think we did watch it, Lorrie and I, yeah.  It was really fun.  I completely agree, it was really neat.



Okay.  During the Toronto Pwn2Own hacking contest, which we covered last December, one of the successfully exploited devices was a fully patched, at the time, TP-Link router.  After the exploit was created and demonstrated during the contest event, it was assigned a CVE; and the contest organizers, ZDI, the Zero-Day Initiative, responsibly disclosed the vulnerability to TP-Link.  TP-Link found and fixed the trouble and released a patch for it this past March.  And unfortunately, that patch was all the operators of the Mirai DDoS botnet needed in order to reverse engineer the change to discover the original flaw for which the patch had just been released.  They immediately then set about taking over and hijacking every TP-Link router that had not yet been updated.



So we have a story where everyone did everything right.  Everyone acted correctly.  A problem was found.  It was demonstrated.  The details were kept secret, and the underlying flaw was responsibly reported to the product's publisher who, in a somewhat timely manner - and it hadn't really mattered how quickly - fixed the trouble and made an update available to their devices.  But despite everyone doing everything exactly right, the bulk of TP-Link routers were almost certainly never updated.  And with today's Internet scanner databases, discovering the locations of those routers is no longer difficult.



The evidence suggests something that's obvious in retrospect.  Bad guys are watching every minute of hacking contests such as Pwn2Own.  They're just waiting to see someone hack something where there will be a large "patch gap" which exists between the eventual release of an update and those updates being installed into vulnerable gear.  And probably nowhere is the "patch gap" larger and more glaring than in consumer routers.  When was the last time any of us checked to see whether our router had new firmware available?  I just checked, this was last night, and sure enough, my ASUS consumer router has newer firmware available.  But how would I know that? I'm not obsessively checking it every day.



LEO:  Don't routers auto-update nowadays?  No?  Yours does not?



STEVE:  Mine doesn't.  And it's a feature that is making its way.  But I'm sure you could, if it's even enabled by default; and of course we know if it's not, it might as well not exist.



LEO:  Right.



STEVE:  So in retrospect and perversely, it would almost have been better if TP-Link had not published a public update for their firmware because the act of doing so painted a big red bull's-eye on every publicly exposed vulnerable TP-Link router.  And the phrase "publicly exposed" is redundant for a router, since that's what they are almost by definition, you know, publicly exposed.  Assuming that the problem was present in their current product line, TP-Link might have simply fixed it there and then and never published and pushed out a fix for the problem.



And I know this goes against everything we think and believe about fixing and updating known problems.  But if patches cannot reasonably be expected to be applied, then what will happen is what just did.  Because, I mean, this is not theoretical.  Mirai exploded into all the vulnerable TP-Link routers and pwned them.  And they were all enslaved into this Mirai botnet.  



And, you know, I appreciate the controversy surrounding this, but I think that generic consumer routers all need to occasionally phone home to check for updates and be willing to take themselves offline for an autonomous update cycle.



LEO:  Yes.  Yes, I agree.  Yup.



STEVE:  The clear prevalence of bad guys who are now waiting to receive and reverse engineer router patches, coupled with the fact that router owners don't know to patch, I think that tips the balance clearly in favor of all such consumer routers being autonomously self-updating by default.  Let the owner turn it off if they want.  But ship this thing with that checkmark turned on.  And for 99.999% of routers, that's the way it's going to stay.  They phone home, and they know what time of day it is.  They do it in the middle of the night.  And they're also monitoring traffic.  So they do it when there's like, no - like they find the sweet spot, the block of time where traffic is minimal, and that's when they go download their firmware.  There are all kinds of embedded OS solutions, dead man switches and watchdog timers where, if a firmware upgrade were to fail, the hardware could automatically roll back to the previous firmware.



So, I mean, there are ways to do this safely.  We talked about this years ago.  There was somebody who was, I think, as I recall, someone you knew, Leo, who was involved in the IoT aspect of this, and looking for a safe way to deal with automatic - with IoT devices being able, being empowered to update themselves.  This is a perfect classic example of where we really do need that.



LEO:  Yeah.  My Ubiquiti system will auto-update.  And sometimes it's a pain.  You know I have it set to do it at 2:00 in the morning.  But we have ceiling-mounted WiFi access points.  There's one in the bedroom.  The light on it is turned off.  But when it's updating, it blinks a bright blue.  [Indiscernible] woke me up last month saying "There's something wrong.  Something's going on.  What's that?"  I said, oh, it's just the router updating.



STEVE:  That's very cool.



LEO:  But, yeah, and that's nice.  And now one of the problems with Ubiquiti stuff is some of the beta versions are notoriously awful.  So I have it set for only stable releases.



STEVE:  Yes, yes.



LEO:  But it does it automatically.  And I think that's how - frankly, Stacey says, and I agree with her, don't buy home automation stuff that doesn't auto-update.



STEVE:  That's really good.



LEO:  In this day and age you need it.



STEVE:  Yup.  I'm glad that that's beginning to be the word that is spread.  I would not have a consumer router on the public Internet.  All of mine are behind a pfSense firewall.  So I have a separate box that is in front.  And actually I need that because I need - I do a bunch of crazy stuff with port mapping in order to link my two sites and get around Cox's blocking of ports that are useful to have open.



LEO:  Right.  And does pfSense auto-update?  It must; right?



STEVE:  No, it doesn't.  It will check for updates, but it won't do it by itself.



LEO:  Yeah, I guess that's the theory is we need to be 100% uptime.



STEVE:  Yeah, exactly.  And I think that's something we're going to have to get over.  And then of course the other problem is what if the update fails, and the router stumbles, and now you're offline, and you have no connectivity. 



LEO:  Right.



STEVE:  Well, again, there are ways to roll back from a failed update.



LEO:  Yeah.  And Ubiquiti will do that automatically.



STEVE:  Perfect, perfect.



LEO:  I'm very happy with this gear.  It's been very good.



STEVE:  So we have a bit of happy news from Intel.  We first need to know what Intel's TDX is.  TDX stands for "Trust Domain Extensions."  Intel describes it as:  "Intel Trust Domain Extensions is introducing new architectural elements to deploy hardware-isolated virtual machines called Trust Domains.  Intel TDX is designed to isolate VMs from the virtual-machine manager/hypervisor and any other non-TD software on the platform to protect Trust Domains from a broad range of software."  So anyway, it's further virtual walls in order to control security.



They said:  "VM isolation with Intel TDX is a key component of Intel's Confidential Computing portfolio, which also includes application isolation with Intel SGX and trust verification with our upcoming service code-named Project Amber.  Confidential Computing uses hardware to protect data in-use from a wide variety of threats, and enables organizations to activate sensitive or regulated data that may have otherwise been locked down and idle."  So, okay.  So it's more security stuff that they're building into their baseline hardware.  



After this announcement, Intel brags that before releasing this new tech to the world, they ran it through a very useful security gauntlet.  They wrote three points.  First:  "In our first-ever pre-release activity, we also took Intel TDX through Project Circuit Breaker, part of Intel's Bug Bounty, where we challenged a community of elite hackers to find bugs in some of our top technologies.  Using simulation software, the community went through two rounds of bug hunting over several months, earning bounties to help us find potential vulnerabilities so we could mitigate them.



"We then took it to security experts at Google Cloud and Google Project Zero to conduct a deep security review.  They looked for security weaknesses while evaluating the expected threat model for any limitations that would inform Google's decisions.  The nine-month collaboration resulted in 10 security issues and five defense-in-depth changes that were mitigated."



And finally:  "Intel offensive researchers also spent considerable time reviewing the product.  Their job is to apply an attacker mindset to evaluate security technologies.  They were able to find and mitigate potential vulnerabilities like the use of memory disturbance errors.  Threat modeling, penetration testing, and hackathons were all applied during the research."



Okay.  So the good news is that even though the authors of Intel's code were as sure as any code authors ever are that their code was correct, they nevertheless subjected it to pre-release third-party scrutiny.  Naturally, Intel put a happy face on the results, saying that it had succeeded in improving the code quality. What we learn from other sources is that, indeed, vulnerabilities were uncovered during the security audits that could have resulted in arbitrary code execution, cryptographic weaknesses, and denials of service.  So I hope that somebody who's pulling the strings over there recognized fully how much benefit they got from this, and that this becomes standard practice because it's a great idea.



And in what Intel wrote, they didn't say something that I did see elsewhere which is they provided the source code.  They made the source code available to Google's Project Zero guys so they could really take a look at it and weren't being forced to reverse engineer it and just guess at what was going on.



On the flipside, an academic paper was just published titled "Timing the Transient Execution:  A New Side-Channel Attack on Intel CPUs."  And since we've entered the world of "yet another side-channel information leakage from Intel CPUs," I'm not going to spend undue time digging into this one.  But in case it might wide up being important, which hopefully is unlikely, I wanted to at least share the brief description of these authors.  They were a bunch of Chinese researchers, but located domestically.  But you'll hear in their description that some of their word choices are a little confusing.  But still, we can see what's going on.



They wrote:  "The transient execution attack is a type of attack leveraging the vulnerability of modern CPU optimization technologies.  New attacks surface rapidly.  The side-channel is a key part of transient execution attacks to leak data.  In this work, we discover a vulnerability that the change of the EFLAGS register in transient execution may have a side effect on the Jcc" - that's Jump on condition code - "instruction after it in Intel CPUs."  And EFLAGS is just, you know, all processors have a status register, a FLAGS register.  "E" stands for Extended because it used to be 16 bits long, and now it's 32 bits long.



Anyway, they said:  "Based on our discovery, we propose a new side-channel attack that leverages the timing of both transient execution and Jcc instructions to deliver data.  This attack encodes secret data to the change of register which makes the execution time of context slightly slower, which can be measured by the attacker to decode data.  This attack does not rely on the cache system and doesn't need to reset the EFLAGS register manually to its initial state before the attack, which may make it more difficult to detect or mitigate.



We implemented this side-channel on machines with Intel Core i7-6700, i7-7700, and i9-10980XE CPUs. In the first two processors, we combined it as the side-channel of the Meltdown attack, which could achieve 100% success leaking rate.  We evaluate and discuss potential defenses against the attack.  Our contributions include discovering security vulnerabilities in the implementation of Jcc instructions and EFLAGS register, and proposing a new side-channel attack that does not rely on the cache system."



So, yeah, once again, aspects of our modern processors which were developed to improve their performance over a long period of time in common contexts, and which existed for years with no one worrying, are one by one turning out to each be exploitable to leak information wherever hostile code might be sharing hardware with targeted code which contains secrets.  And I'll note that these days virtually all operating systems contain valuable data that needs to be kept secret.  They've all got keys.  But that's the key.



And the reason not to get too overworked about this is that it does require that hostile code already be present.  Certainly in our own personal workstations, if something's in them that's evil, it's already too late.  The reason concern is in all of the virtualization which is going on now where you might have multiple companies' systems sharing a common set of hardware.  And there it's cross-OS or cross-VM leakage that you need to be concerned about.



LEO:  There have never been any in the wild, that we know of, examples of these side-channel exploits, like Meltdown and Spectre.



STEVE:  No.  Still...



LEO:  It's hard to do.



STEVE:  Yes, I mean, even Heartbleed, you know, it required that you pound on the server for a long time, and maybe you got something.  And remember when Heartbleed first came out, the discoverers recognized and called it a "theoretical problem," but doubted it could ever actually happen.  And then it did.



LEO:  Right.



STEVE:  So, yeah.  So the real problem here is that what we're seeing Intel being forced to do is they're having to back out of very useful performance optimizations driven by the world that doesn't want to have any known possibility of problems.



LEO:  Well, and the problem is mostly on shared servers; right?  You said you had...



STEVE:  Only.  Only on shared servers.



LEO:  Only on shared servers.



STEVE:  Yeah.



LEO:  So if you're, you know, it's not your home machine you have to worry about.  



STEVE:  Right, exactly.  So early last week, Daniel Stenberg posted two messages at Mastodon.social regarding some recent hysteria about cURL.  Daniel's feelings about cURL are significant because cURL is pretty much his baby.  His own bio says:  "I am the founder and lead developer of cURL and libcurl, an Internet protocol geek, an open source person, and a developer.  I've been programming for fun and profit since 1985.  You'll find lots of info about my various projects on these web pages and on my GitHub profile.  My name appears in products."  Daniel was also the 2017 winner of Sweden's prestigious Polhem Prize for his work on cURL.



LEO:  cURL is the single probably most useful program for interacting with the Internet there is.  I mean, it's incredible.



STEVE:  Yes, yes.  So here's what Daniel posted to Mastodon.  His first post said:  "Do NOT."  NOT in all caps.  "Do NOT.  I repeat.  Do NOT remove curl.exe from your Windows System32 folder to silence a (stupid) security scanner.  It will lead to tears and sorrows.  And if you do, please don't ask me for help when you've broken your Windows install.  I can't fix that."



And he followed up the tweet with:  "Why do people remove it?  Because NVD has exaggerated a cURL security flaw to an inflated level, and now 'security scanners' [he has in quotes] insist that the bundled cURL executable has a 'high severity' [again in quotes] security flaw and scaremongers people into removing it."



LEO:  Wow.



STEVE:  "And then they realize Windows Update refuses to work."



LEO:  Oh, interesting.



STEVE:  And then he finishes:  "Are we sure this is the best we can do?"



LEO:  So Windows Update uses cURL, I guess.



STEVE:  Apparently some facet of Windows Update does, yeah.  And I didn't realize it's sitting there, a curl.exe in the System32 folder.



LEO:  Wow.



STEVE:  Yeah.  Very cool.



LEO:  Well, certainly it's on all Linux distros and all Macs.



STEVE:  Oh, yeah.



LEO:  Yeah.



STEVE:  Yeah, I'm a big Wget user.  So that's a...



LEO:  I always replace it with Wget because it's easier, but...



STEVE:  Yes.  And just a little tip for Wget fans.  There was something I needed to download.  Actually I'm sure it was some network driver, long forgotten, for a card that a motherboard had that I needed to get on the network in order to do network-based debugging of SpinRite.  So wherever it was, I think it was on IBM, actually, on IBM.com there was an archive.  When I clicked on it, no luck.  It was FTP.  And then I remembered.



LEO:  Yes.



STEVE:  That Wget does FTP.



LEO:  Yeah, yeah.



STEVE:  So.



LEO:  cURL supports every possible - you can do posts and gets and everything with cURL.  It's awesome.



STEVE:  Yeah.  And finally, AI comes to VirusTotal.  During last week's annual RSA security conference, Google announced "VirusTotal Code Insight."  Code Insight is a new feature for VirusTotal that uses AI and Machine Learning to generate simple natural language summaries from submitted malware and source code samples.  Google says that, at present, the new functionality is deployed to analyze PowerShell files submitted to VirusTotal.  The company says it plans to expand the service with additional file formats in the future.



So I think this is potentially very cool and useful.  However, increasingly real-world evidence is suggesting that AI, at least in the form of our well-known ChatGPT, always sounds absolutely convincing and authoritative while being completely wrong with its facts.  So at least at this early stage, anything and everything that's produced should be regarded with skepticism and carefully vetted and verified.  By all means, you know, see what the AI has to say, but then go do your homework.  You know, use that as a starting off point and verify it yourself.



On the other hand, since this is a Google effort, and they know their technology, it would be truly cool to have something where you could update this, and it would provide an automated analysis of whatever malware you provided.  So I hope that this gets better and continues to grow in the future.  Sounds like a good idea.



LEO:  Yay.



STEVE:  And Leo?



LEO:  Well, well, well.  Here we are.



STEVE:  That's our show.



LEO:  Once again at the very end.  Sadness rules the land.



STEVE:  And remember, I think it was on Windows 95 there was that really bizarre comment that would come up.  Maybe it was the paper clip?  And it said:  "Never run with scissors?"



LEO:  No, I don't remember that.



STEVE:  Oh, yeah, it was like...



LEO:  That sounds like Clippy, Clippy always looking out for you, yeah.



STEVE:  I just wanted to tell anyone, never be in a hurry when you're shaving with a razor.



LEO:  Oh, my gosh.  It's like Jack the Ripper hit you.



STEVE:  Because, yeah, well, it looks worse than it is, you know, it's just a paper cut.  Just a flesh wound.  I'll be fine.



LEO:  You used to be an adventurer.  Then you took a razor to the chin, and it's all over.



STEVE:  I haven't cut myself a lot for a long time.



LEO:  Steve, always a thrill, always a pleasure.  It's great to see you.  I'm glad you've got your power back at your headquarters, your world headquarters.



STEVE:  Yes.  Power is at both locations, so I will be retiring this emergency - we call this, what was it on the Enterprise?  It was the Auxiliary Bridge.



LEO:  Auxiliary Bridge shutting down for the week.



Copyright (c) 2023 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#922

DATE:		May 9, 2023

TITLE:		Detecting Unwanted Location Trackers

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-922.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  Last week Google activated their Passkeys support.  What does that actually mean?  Do TP-Link routers auto-update by default?  What trouble did a secretive branch of the U.S. Marshals get into?  When and why will Chrome be eliminating the padlock icon?  Were you prompted by Apple's new Rapid Security Response?  What did Elon Musk do to upset WordPress, and why is it a win for Mastodon?  How many fake news AI-driven websites have been spotted so far, and are they convincing?  What's this about Russia dropping TCP/IP in favor of their own Russian network protocol?  What three mistakes does Vint Cerf, co-designer of the Internet Protocols, think he made?  And finally, in the first half of our two-part very deep dive into the design of the next-generation location tracking devices, will you be put off when you learn that law enforcement is able to query for the identity of any device's owner?  Fasten your seatbelts for



another interesting Security Now! podcast brought to you by TWiT, the itch that Leo scratched.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  We're going to talk about Google and Passkeys.  They activated support.  But how do Passkeys work?  What are their problems?  What are the things they solve?  We'll also talk about Vint Cerf, the father of the Internet, and the three things he thinks he did wrong when he designed it.  And then Steve will dive into an Apple proposal to make AirTags and other Bluetooth trackers more secure?  Or is it less?  Security Now! is next.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 922, recorded Tuesday, May 9th, 2023:  Detecting Unwanted Location Trackers.



It's time for Security Now!.  Oh, I know you've been waiting all week for this.  Here he is.  He's here.  Mr. Steven Gibson.  You've never missed a show, Steve, have you.



STEVE GIBSON:  No.



LEO:  Never got sick.



STEVE:  No.  I've actually scheduled some surgery in between shows, specifically.



LEO:  Wow.  Even when you traveled to wherever, Scandinavia, you never missed a show.



STEVE:  Nope.  Never missed a show.



LEO:  In fact, Steve gets mad when we tell him you've got to take a week off during Christmas so we can do a Best Of.



STEVE:  Well, it's why I'm so happy that I'm on Tuesday and not on Monday.  Because those three-day weekends used to piss me off.  I'm like, wait a minute.



LEO:  Well, we're glad that you don't miss a show, and you are now in Episode 922.  It's going to be a good one, I think.



STEVE:  It is actually going to be a good one.  I expected to be able to finish today's topic, and I could not, for reasons that we're going to see.  So this is actually Part 1 of a two-parter titled "Detecting Unwanted Location Trackers."  So lots to talk about.



Last week Google activated their Passkeys support.  What does that actually mean?  Do TP-Link Router auto-update by default?  What did a secretive breach of the U.S. Marshals get into?  When and why will Chrome be eliminating the padlock icon?  Were you prompted by Apple's new Rapid Security Response last week?  What now has Elon Musk done to upset WordPress, and why is it a win for Mastodon?  How many fake news AI-driven websites have been spotted so far, and are they convincing?  What's this about Russia dropping TCP/IP in favor of their own Russian network protocol?



LEO:  Oh, lord.



STEVE:  I know.



LEO:  I've heard about that.



STEVE:  Oh, there's some good stuff here.  Okay.  What three mistakes does Vint Cerf, co-designer of the Internet protocols, think he made?  And finally, in the first half of our two-part very deep dive into the design of the next-generation location tracking devices, will you be put off when you learn that law enforcement is able to query for the identity of any device's owner?  So fasten your seatbelts for another interesting Security Now! podcast brought to you by TWiT, the itch that Leo scratched.  



LEO:  As long as it doesn't leave you itching, we're okay.  No red rashes from listening to the show.  You didn't mention, but we have another Picture of the Day in a continuing series.  I love this.  I love this one.  Okay, Steve.



STEVE:  Next time you talk about Kolide, because you're right that the LastPass, the cause of the LastPass breach is a perfect example of this, the developer did have a publicly exposed instance of Plex server for which a patch had been available for years that was never applied.



LEO:  Yup.



STEVE:  And that's how the bad guys got in.



LEO:  That's how it can happen.  I mean, it's that easy.  And that's a hole that you, fortunately, you can patch.  If you're using Okta, you can use Kolide, and you're done.  I guess they would.



STEVE:  Okay.  So our Pictures of the Week...  



LEO:  Oh, boy.



STEVE:  ...have been elevated to a new level, Leo.  One of our listeners was driving down the street in Texas when he saw this and pulled over and stopped and took a picture.



LEO:  We've trained them to look for this stuff now.  That's awesome.



STEVE:  So for those who aren't looking at the video, we've got yet another ridiculous gate, like in the middle of nowhere.  What I love about this, so there's like a big concrete driveway coming up, like across a sidewalk which is set back from the curb a good distance.  And then there's a big gate with both sides hinged such that it would, you know, they would swing apart in order to allow you access to the full width of the drive.  Okay.  And of course the problem is that there's nothing on either side of the posts that anchor this gate.  And there's some evidence of, as there always is, of people trotting or driving, like, around it.  So you can sort of see it's a little bit worn over there to the left.  But so this gate is sort of just out in the middle of nowhere with, like, nothing to prevent anybody from thinking, uh, okay, if I want to go behind it, I can.  But what I love most is that, if you zoom in on the picture, there is a security cable and a padlock which is, like, locking the two halves of this gate together.



LEO:  Well, you wouldn't want anybody opening the gate.  I mean, come on, man.  That's  hysterical.



STEVE:  No, no, no, no.  You want to make sure that both halves stay firmly closed.  Wow.



LEO:  Oh, that's crazy.



STEVE:  Okay.  So Google and Passkeys.  Last week, Google formally launched their support for Passkeys.  I popped onto Jason's Tech News Weekly show last Thursday and spent some time placing this announcement into perspective.



LEO:  Oh, good.  Oh, good, I was hoping.  We need your input, yeah.



STEVE:  We had a great time.  I explained, and there's a picture here in the show notes, Leo, how 50 years ago, five zero, 50 years ago, in 1973, when I was at UC Berkeley, I sat in front of that, I mean, I found the picture of it, and I knew exactly what it was.



LEO:  It's a Hazeltine 2000 dumb terminal; right?



STEVE:  Correct.



LEO:  That's a dumb terminal, yeah.



STEVE:  Correct.  I logged onto the campus mainframe sitting in front of that Hazeltine 2000 dumb video terminal.  You'd hit the Break button, which was the second one back from...



LEO:  Oh, yeah.  Oh, yeah, I remember that, yeah.



STEVE:  ...from the right, in order to get the attention of the mainframe, after which a few seconds later it would prompt you  for your username and then your password.  And if those were both known to the system, you'd be logged in.  So my point was, that I was making on Jason's show last Thursday, is nothing has significantly changed during the intervening 50 years.  That was 50 years ago.



And, you know, today, 50 years later, every web server and website on the Internet prompts for, I mean, with yes a few notable exceptions, but largely prompts for a username and password, or at least can, then verifies that they're correct.  Now, we've gotten much better about protecting the user's password secret using hashing and brute force resistant PBKDFs, which we've had a lot of fun talking about and explaining on the podcast.  But servers are still essentially holding and comparing user secrets.  And that is what is finally changing with the introduction of WebAuthn.  WebAuthn finally moves us from that 50-year-old symmetric secret key model to a modern asymmetric public key model.



Under this new model, servers no longer keep secrets.  They keep public keys which allow them, and only allow them, to verify the signature of a randomly generated challenge that they just freshly sent to someone who's logging on.  That Hazeltine 2000 terminal I used 50 years ago was, as you know, as we just said, a "dumb terminal."  So there was no way for it to perform the complex math that's required to answer a cryptographic challenge by signing it with a private key.  But none of us are using Hazeltine 2000 terminals anymore.  We're all logging into a remote computer with a local computer.  So these two quite capable computers that are communicating are able to have a mindbogglingly complex, heavy math interaction which we are able to take for granted.



So in one sense, last week's news from Google was insignificant because it's not Google who needs to make Passkey support available, it's everyone else in the world.  And that's the hurdle.  Just as with any major change in our industry, this is going to be an extremely slow process.  At the same time, Google's announcement was significant inasmuch as it represents another necessary incremental step in the right direction.



But it's going to take a long time for this to become universal, if it even ever is.  Think of it like today's one-time passwords.  I just checked my OTP Auth app. I have 18 one-time passwords registered.  How long ago, Leo, were you and I talking about that one-time password football that was supported by VeriSign.



LEO:  Oh, yeah.  PayPal.  I got mine with PayPal, yeah.



STEVE:  Yeah, exactly.  It was accepted by PayPal.  It must have been at least a decade ago.



LEO:  Ages.



STEVE:  Yet today, very few of the websites I log into offer one-time passwords as an additional authentication option.  You know, those where logon security is most important generally do.  But, you know, I use mine at...



LEO:  That's TOTP.  It's the same thing; right?  The football, yeah, yeah.



STEVE:  Right, right.  That's exactly what it was.  But I suspect that Passkeys will be similar.  It's going to take a while.  But that said, without any question, the biggest and best news is that we now have an option.  We have an open and modern public key-based means of authenticating users over networks.  And 50 years from now, I expect everyone to be using it because, you know, it'll end up being built into servers, and it'll just be there and easy to use.



LEO:  Answer me this, though.  This is something I've been kind of trying to figure out.  As far as I can tell, the FIDO and WebAuthn spec don't have any provisions for exporting or moving your keys; right?  And I can understand why that might be a security flaw.



STEVE:  Right.



LEO:  But it means that Apple, for instance, with their iCloud key, has a lock-in.  There's no way I can get it out of Apple and move it to Android; right?



STEVE:  Correct.  The only thing you can do is use your iPhone to briefly bridge to a different device and then have it create its own new unique Passkey there.



LEO:  So I've gone to Steve's Marvelous Site and set up a Passkey with my iPhone.  But now I want to get an Android phone.  So you're saying I could log into Steve's Wonderful Site with my iPhone.  Can I then tell Steve's Wonderful Site, generate a new  Passkey, and set it up with my Android phone?



STEVE:  Uhhh...



LEO:  No.  I have to keep my iPhone around to login; right?



STEVE:  So, okay.  So this is one of the reasons that I focused, when I was talking to Jason...



LEO:  Yeah, you solved this with SQRL.  I know you did.



STEVE:  Well, I did.  But I focused on the backend side with Jason because these user-facing sides, they're sort of - they're up in the air.  They're open.  For example, it would be possible for you to sign into Steve's Wonderful Site with your iPhone, then tell Steve's Wonderful Site that the next sign-in is also from you, using a different device.  So then your Android could identify itself to Steve's Wonderful Site, and now Steve's Wonderful Site would have two Passkeys, each for a different device.



LEO:  Oh, so there is - okay.  I've never seen this offer.



STEVE:  Well, no.  And that's my point is it's up to, like, it's up to the UI...



LEO:  Steve's Wonderful Site has to offer that; right?



STEVE:  Yes, yes.



LEO:  And one thing we found out, at least initially, was a lot of these sites that claim to be offering Passkeys were actually not doing it themselves.  They were using a third party.



STEVE:  Yes, yes.  Which is like, you know, punting.  And immediately - and we could expect that to change over time.  The other thing, the other observation that Jason made was that, since Pixel has been creating, or Android and Google on Pixel phones have been creating Passkeys for a while, the moment it got activated, he looked in Google, and there were like all of his different devices were listed with their own individual Passkeys.



LEO:  Yeah.  Both my Google Pixel and my Galaxy S23 with - I mean, I guess I seem to dimly remember them saying something about this.  But both created Passkeys.  And they were preexisting.  That's what they're supposed to do.



STEVE:  Well, that is - yeah.  That is the model.  So I got a tweet from Chris Smith which was interesting and apropos.  He said:  "Passkeys:  Going through the details, can they be used if I don't own a computer or phone?"



LEO:  Oh.  Well, what would you be logging in with?



STEVE:  Well, that threw me.  So he said:  "Does this amount to an economic barrier to better security and privacy?"  And so, you know, that tweet and question caught me by surprise.  At first I thought, exactly as you did, how could you be doing anything if that required...



LEO:  Oh.  He went to the library.



STEVE:  Yes, yes, yes, yes.



LEO:  And a lot of people who don't have their own technology do use technology at libraries.  Okay.



STEVE:  Exactly.  I remembered Internet cafs, public libraries, hotel business centers, senior citizen centers, and other such facilities which provide computers and Internet connectivity for those who don't have any sort of computer with them.  So, yeah.  Chris, I think, makes a point.  As I noted above, the biggest change brought by Passkeys to the traditional username and password model is the presumption that the user's end of the link will have powerful crypto math capabilities.



LEO:  Yes.



STEVE:  While any PC on the Internet will have that capability, even an Internet caf machine, Passkeys' other requirement is the ability to store any number of individual public key pairs, one pair for every Passkey association that the user has ever made to individual websites.  And that's not going to be something that someone logging onto a machine in any sort of shared caf or library setting will have.  Now, that brought me to...



LEO:  Will there always be a password fallback, though?  Or do you think at some point...



STEVE:  Yes.  No.  There will always be.  Which actually is, it creates the weak link; right?  I mean, we've often talked about the weakest link being the one that gets you.



LEO:  Right.



STEVE:  Well, if you still actually have usernames and passwords, then site breaches will still be leaking your password.



LEO:  Well, I guess the presumption is that any new site, I'm going to create a Passkey.  I won't have a password on that site; right?



STEVE:  If they let you not have a backup.



LEO:  They may say create a fallback, yeah.



STEVE:  Yeah.



LEO:  Oh, god.  I bet they will, too, because they don't want to field customer support calls.



STEVE:  Exactly.  Exactly.



LEO:  Oh, this is a mess.



STEVE:  It is a mess.  So that brought me to the question of a personal FIDO dongle because this user might not have a phone or a computer, but what about a dongle?  But it's unclear, I think, what the future of hardware dongles is now, after they essentially failed to achieve market critical mass traction, and the FIDO Alliance finally deigned to soften authenticator requirements to allow smartphones and PCs to qualify as sufficiently secure hardware authenticators.  It seems to me that this dramatically reduces, if not almost eliminates, the pressure to purchase a separate freestanding dongle for most use cases.  There could still be exceptional enforcement of the requirement for a separate freestanding authenticator in selected ultra-high security applications.  But we already know that's not going to be the norm.  I have a smartphone.  Why do I want to purchase and carry a redundant dongle?  I don't, and I won't.



Again, some users might want that, but that can now be expected to become much less common.  And if that's the case, then what of longer term support for dongle authenticator logon?  It's unclear that hardware authenticator support will automatically be added to websites as Passkey support is added.  After all, if almost no one is carrying them around, why clutter up and further confuse the user's experience with a rarely used and largely redundant option?



LEO:  Well, and as Google pointed out, they're going to support this in their advanced protection program, Passkeys.



STEVE:  Right, right.



LEO:  Although when I log into my Google Passkeys thing, my YubiKeys are there, too.



STEVE:  Right.



LEO:  But that's because I have YubiKeys.



STEVE:  Exactly.  And who's going to be...



LEO:  Who's going to buy them, yeah. 



STEVE:  ...buying a YubiKey when you can use your phone.



LEO:  I feel bad for Stina, really.



STEVE:  So apparently they cashed out, so I think she's just fine.



LEO:  Oh, good.  Yeah, I saw she stepped out or up.



STEVE:  Yeah.



LEO:  And what about password managers?  That puts them out of business, too; right?



STEVE:  Well, password managers will still be the holders of our usernames and passwords.  And there's nothing to prevent a password manager from being your Passkey holder.



LEO:  Well, do I want to do that?  I mean...



STEVE:  I agree.  I agree.



LEO:  1Password says they're planning to do that.



STEVE:  I know.  And what it does solve is the cross-ecosystem problem; right?  Because the password manager won't care if you're Apple or Google.  I mean, it'll be a third-party password manager, essentially.



LEO:  Right.  But it's not a hardware enclave.  It's software.



STEVE:  No.  No.



LEO:  Which makes it less secure, I presume.



STEVE:  Yeah, exactly.  So I think that on balance Chris's point is valid.  This move to higher security authentication does require some form of dedicated per-user authenticator capable of storing all of the user's public Passkey pairs and negotiating with remote websites for their use.  I cannot see any way for a shared PC-only usage model to accommodate that.  So it's good news that username and passwords will never go away, and users who are unable to own their own authentication device will need to operate without the benefit of public key crypto authentication.



Okay, now, I just said that I could not see any way for public key logon to be made practical.  But that's not completely true.  Not to belabor the point, but this highlights another example of what SQRL's design got right and the FIDO Alliance got wrong.  I'll explain, not from a position of sour grapes - that ship has not only sailed, but sank - but only to highlight that other and arguably...



LEO:  Aw, Steve, you're going to make me cry.



STEVE:  Well, wait till you hear this, and you're going to cry.  I want to highlight that other and arguably superior solutions are possible.  In a SQRL world, the Internet caf or library shared PC user could simply carry their single lifetime SQRL identity printed on a one-inch square QR code.  They could have it plastic laminated for longevity.  That's all they would need.



LEO:  So that's their private key, basically, in a QR code.



STEVE:  Yes.  But it's a universal private key instead of needing a...



LEO:  Instead of one per.  Yeah.



STEVE:  One per everything.



LEO:  That's a lot of keys, yeah.



STEVE:  So they sit down in front of a shared PC, which is configured to reboot after use and to never make any permanent changes, as all of those caf-style machines are.  The user would click the SQRL icon, hold their QR code up to the PC's camera, and import their globally unique identity into SQRL.  After then entering their one single password which is used to decrypt their SQRL identity, from that point on, every site they visit would be able to obtain their per-site anonymous identity for logon.



LEO:  That's so cool.  So much better.



STEVE:  I know.  So unfortunately, unlike SQRL, Passkeys' operation requires potentially unlimited storage of individual public key pairs.  So that rules out its use by any user who cannot provide some form of personal public key pair storage.



LEO:  It's like they started that process that you started, when that light bulb went on seven years ago, but they didn't finish it.  They didn't think it all the way through.  Because there's no reason why they couldn't have done it better, to me.



STEVE:  No.



LEO:  Yeah.



STEVE:  No.



LEO:  That's very frustrating.



STEVE:  You know, I solved it all the way out, all the possibilities.



LEO:  You thought all the edge cases through, and that's the thing.



STEVE:  Yeah.  For example, say that the guy who sat down for some reason wants to appear as a different user, but he only has his one QR code.  Well, there's a provision in the SQRL spec to allow you to append something to your identity to create a branch identity just for that one time.



LEO:  We want that, too.  We want that, too.  Yeah.



STEVE:  I know.



LEO:  This got support, somebody's saying in the IRC, DryHeat, seems obvious they wanted the smartphone to be the authenticator.  If you think of who was behind this - Google, Apple, Microsoft, too - but Google and Apple did have a dog in that hunt.  They want a hardware enclave that you buy from them.  Plus the lock-in really bothers me.  You cannot...



STEVE:  Yeah.



LEO:  You're locked into that platform.  I mean, Apple makes it possible to go to a new iPhone, but not to an Android.



STEVE:  The whole model is create another identity.  You know, create another key pair.  So it just doesn't transport the way we would like it to.



LEO:  So frustrating.  Oh, well.



STEVE:  Okay.  I received some firsthand feedback from Kevin Kinneer, one of our listeners, who owns and knows his way around TP-Link routers.  Here's what Kevin wrote.  He said:  "I have several of the TP-Link AX21 (AX1800) routers you were talking about last week, and they all upgraded automatically."



LEO:  All right.



STEVE:  Yeah.



LEO:  Yeah.



STEVE:  He said:  "In its initial setup wizard, auto-upgrade is offered, and you would have to intentionally choose to turn it off."  Yay.  He said:  "Perhaps when this model first came out this was not a feature; but from what I've seen lately, they are doing it right."  So Leo, that supports your belief that auto-updating is beginning to appear in routers, and it supports my prayers that when the option is available, it will be enabled by default so that less-informed users will assume that it's a good thing, and they'll leave it enabled where it belongs.  And thank you, Kevin, for taking the time to provide that feedback.



The advice that follows from this would be to no longer purchase any router that does not offer autonomous upgrading in the future.  And I'm also certain that many of our listeners are considered to be their friends' and families' influential tech guru.  So find a good auto-updating router to recommend to those who depend upon your opinion because this is clearly where the future is, especially for, I mean, consumer routers are like in this weird, like, worst of all possible worlds; right?  They are, by definition, they're exposed to the public Internet, so they're not on a LAN.  They're bridging the WAN and the LAN.  And the fact that they're a bridge makes them enticing for attackers.  The fact that they're on the public Internet means that, well, they can be poked at and prodded and searched for and found.



And they're not exciting.  You know, you don't turn them on every day to watch news on them or anything.  They're dusty.  They're sitting in a closet somewhere.  So they're like the least obvious thing anyone is ever going to think about like updating and keeping current; right?  It's like, you know, the forgotten stepchild.  It just does its job.  And so it's like the profile of it from every direction is as bad as it could get. 



LEO:  By the way, I would extend it.  You should not buy an IoT device of any kind that isn't auto-updating.  I mean, period.  Doorbell, light bulb, router, they should all auto-update.  That's got to be the standard going forward.  Clearly.



STEVE:  Yes, yes.  Yeah.  I would argue that we're probably leaving the first generation of devices that just came out quickly because they would work.  And we're now beginning to get into it like, okay, we're saying, but what if?  And it's like, okay, so it's time to get smart.



Okay.  So I stumbled upon a story that became more interesting the more I dug.  Ten weeks ago, back in the middle of February of this year, a computer system run by a special and somewhat secretive and elite unit of the U.S. Marshals Service, known as TOG  their Technical Operations Group  fell victim to a breach followed by the inevitable ransomware attack and the followon demand.  The U.S. Marshals refused to pay the ransom.  And so today, a full 10-plus weeks and counting later, that entire system remains offline.  And when you hear what it was doing, ho ho.  Okay.  What makes this otherwise somewhat now generic story extra interesting is the back story of this particular system, which is no longer on the air.  



It turns out that this TOG, their Technical Operations Group, is a secretive arm within the U.S. Marshals Service that uses technically sophisticated law enforcement methods to track criminal suspects through their cell phones, email, and web usage.  Its techniques are kept secret to prolong their usefulness, and exactly what members of the unit do and how they do it is a mystery even to some of their fellow Marshals personnel.



The system that went down and remains down manages a vast amount of court-approved tracking of cell phone data, including location data, and this system has existed outside regular Justice Department oversight and computer systems for years, essentially leaving it unnoticed.  After the breach and attack occurred, the technical group proactively wiped the cell phones of everyone who worked with and had anything to do with the breached system, deleting all of their contacts and emails.



Since the action was taken without notice on a Friday night, it caught everyone by surprise.  One staffer was working the security detail for a Supreme Court justice when they discovered that their phone had been wiped of all of its data.  Although the phone still worked, the person had no emails or contacts.  It would appear that the technical group either believed or knew that someone had clicked a bad link and had fallen victim to a phishing attack which allowed the bad guys to then gain entry.



However, the most significant consequence of the system going down, and remaining down for more than 10 weeks, is that one of the Marshals' best tools for finding fugitives, often used on behalf of state and local law enforcement agencies, has been completely incapacitated.  It's just gone.



Okay.  Sounding somewhat, and understandably, defensive, a spokesperson for the Marshals Service said that:  "The breach data has not impacted the agency's overall ability to apprehend fugitives and conduct its investigative and other missions.  Most critical tools were restored within 30 days of the breach discovery."



LEO:  Oh, well, that's a relief.



STEVE:  Yeah.  But 30 days?  Wait.  Okay?  He said - and it doesn't sound like that's true anyway.  We have some details in a second.  He said:  "Further, USMS (U.S. Marshals Service), USMS soon will deploy a fully reconstituted system with improved IT security countermeasures."



LEO:  We had to buy all new computers.



STEVE:  Yeah.  Well, and where's the backup?  Right?  You know, we have no details about the computer system.  But it sure does sound as though the network was either without protected backups, or that these backups were also destroyed.



LEO:  Oh, boy.



STEVE:  Because, you know, 10 weeks.  Okay.  So the Technical Operations Group has helped the Marshals hunt down high-value suspects in the United States and in other countries, including Mexican drug kingpin Joaquin Guzman, better known as "El Chapo." A great deal of the hunting is done through what is called pen register/trap and trace, a means of cell phone surveillance that has evolved along with phone technology.  In the era of landlines, a so-called PR/TT (Pen Register Trap and Trace) meant getting a record of all the incoming and outgoing calls from a phone.  Today, PR/TTs can also be applied to email accounts, and apparently also to an individual's web browsing usage, all of which can provide information critical to a manhunt.



Unlike a wiretap, a pen register/trap and trace does not monitor the contents of phone conversations.  A PR/TT order for a phone's data requires law enforcement to convince a judge only that the information is relevant to an ongoing investigation, so it's a low bar, not the higher legal standard of probable cause, which is needed for a full wiretap.  In their reporting about this, The Washington Post quoted Orin Kerr, a law professor at the University of California Berkeley, who specializes in criminal procedure and privacy.



He said:  "In a world where everyone has a cell phone, it's a way to track cell phones, and it's a way to track account usage.  We're all on these devices all day, so it's a way to  with a court order  track not the messages that people are sending, but the information" - you know, as we would call it, the metadata - "about them," he said, "which is helpful in finding them."



The Technical Operations Group does so many real-time pen register/trap and trade data searches that in many years it collects more of that data than the FBI and DEA combined.  The people said that that office's use of the technology typically generates more than 1,000 arrests over a 10-week period.  So that suggests that over the past 10 weeks, maybe 1,000 fewer arrests?  Well, apparently.  Since the ransomware attack took the system down in mid-February, the TOG has not been able to do that kind of real-time collection, which people familiar with the situation said has had a major impact on fugitive-finding efforts.  Meanwhile, the U.S. Justice Department has judged the computer intrusion to be a "major incident" and notified Congress.



Some within the Marshals Service have complained for years that the TOG is too unsupervised and secretive, being described as a cowboy arm of a law enforcement agency.



LEO:  Oh, boy.  Great.  Have you watched the HBO show on the Watergate Plumbers, G. Gordon Liddy and...



STEVE:  No, I haven't yet.  I love Woody Harrelson.



LEO:  It sounds like TOG, I've got to say.  Oh, lord.  Oh, boy.



STEVE:  So, and unfortunately this has brought unwelcome scrutiny; right?  They didn't want to be on, you know, yeah.  So in particular its activities in Mexico have been the subject of concern within the agency and whistleblower complaints.  Questions about cell phone surveillance by the Marshals and other law enforcement agencies led Obama's administration to tighten up the rules governing how federal agencies use such technology.



LEO:  Oh, yeah.  We've been complaining about pen registers for ages.



STEVE:  Yeah.  They're very easy to get.  And they tell you...



LEO:  There was a point where all the major cell phone companies had portals for law enforcement.  They would log into the portal, give them a buck fifty, and get the information.  I mean, it really - I'm glad to hear you now need at least some court supervision because that didn't use to be the case.



STEVE:  Yeah.  Yeah.  So other law enforcement officials describe the TOG as full of technical wizards unencumbered by red tape, whose skills at data extraction and surveillance to find and track targets are a model, not just for law enforcement, but also for the military.



LEO:  Yeah, but maybe not completely a model.



STEVE:  Yeah, maybe not a model on the IT sector.



LEO:  Yeah.  



STEVE:  Yeah.  Anyway, I thought that some of the  interesting back story was fun for a ransomware breach.  Apparently the entire incident illuminated the operational details of a system that the U.S. Marshals Service was hoping to keep under wraps so that their "cowboys" could continue operating unimpeded.  They may have been technical wizards, but it sure sounds like they got caught without their systems backed up in a way that would allow them to quickly recover and then remain on the DL, which, you know, they blew that.  Leo, speaking of blowing - wait, no.



LEO:  I've been blowing things.  No, I have an example right here in my hands.



STEVE:  Oh, perfect.



LEO:  Of me blowing something.  So apparently I have an inbox, which I didn't know about.  And this letter has been sitting in my inbox.



STEVE:  You're holding paper in your hands.



LEO:  Yes, because it came in Thanksgiving timeframe from a guy named Jim who apparently has McGruff the Crime Dog stationery.  So I don't know what Jim's background is.  But he says:  "Hello, Leo.  Thank you so much for hosting, and sometimes roasting, Steve Gibson.  He's a major asset to the security community.  I'm sending a check for his podcast.  Thank you.  I'm not signing up for TWiT, just paying for Steve."  And he sent me a check which, because I didn't find this in time, is now, I think - I'll try to cash it.  But for $40.  Thank you, Jim.



STEVE:  Take Lisa out.



LEO:  Yeah.  No, I'm going to send you two crisp $20 bills.  All right.  Steve.  On we go.



STEVE:  Okay.  While we're on the topic of breaches, we've also talked in the past about the distressing trouble T-Mobile appears to have with data breaches which is far out of proportion to their peers.  They've just suffered their second major breach of this year, 2023.  In a letter filed with the Maine Attorney General's Office, T-Mobile says that a threat actor gained access to the account data of its customers, including their account PIN codes.



LEO:  Oh, lord.



STEVE:  I know.



LEO:  I set up the PIN.  I have a T-Mobile account.  I set up the PIN code to keep from getting SIM-jacked.



STEVE:  Yup.  So now it enables that.



LEO:  Great.



STEVE:  The breach took place between February 24th and March 30th this year.  And while this is their second breach incident just this year, it's their ninth breach since 2018.



LEO:  It's ridiculous.  That's ridiculous.



STEVE:  Which is why we were talking about them years ago, yes.  It is like, guys, get it together.



LEO:  Get it together, yes.



STEVE:  Because wow.



LEO:  [Growling]



STEVE:  So the familiar padlock icon, which has been used to connote private and authenticated web browser connections ever since Netscape first deployed SSL in their Netscape Navigator web browser back in 1995, will be leaving Chrome with Release 117 in early September.  It's being replaced with a "settings" icon that I just noticed in my Firefox.



LEO:  Yeah, Firefox has had this for ages.  But it doesn't do the same thing, so that's going to be really confusing because Firefox still has a padlock.



STEVE:  Yeah, thank you for [crosstalk].  And so, yeah, exactly.  So, and in Firefox I noticed that it seems to come and go, depending upon which page I'm viewing.  It actually stands for "You have given this page additional capabilities" or privileges or something.



LEO:  Yes.  It's not on every page, yeah.



STEVE:  Right.



LEO:  Oh, that's interesting.  Okay.



STEVE:  So we first lost any visual indication of the use of Extended Validation certificates, which pretty much killed them, since they were much more expensive, and they were not allowed to be used with wildcard domains.  Since I have www.grc.com, forums.grc.com, dev.grc.com, sqrl.grc.com and so forth, it made so much sense to be able to put "grc.com" and "*.grc.com" into a single certificate.  So I switched over to DigiCert's Organization Validation (OV) certs.  So no more display of EV-ness, and soon not even a lock.  Of course we can guess why.  But here's a lightly edited version of what Google explained.



They wrote:  "Browsers have shown a lock icon when a site loads over HTTPS since the early versions of Netscape in the 1990s. For the last decade, Chrome participated in a major initiative to increase HTTPS adoption on the web, and to help make the web secure by default.  As late as 2013, only 14% of the Alexa Top 1M sites supported HTTPS.  Today, however, HTTPS has become the norm, and over 95% of page loads in Chrome on Windows are over a secure channel using HTTPS.  This is great news for the ecosystem.  It also creates an opportunity to reevaluate how we signal security protections in the browser.  In particular, the lock icon.



"The lock icon is meant to indicate that the network connection is a secure channel between the browser and site, and that the network connection cannot be tampered with or eavesdropped on by third parties.  But it's a remnant of an era where HTTPS was uncommon.  HTTPS was originally so rare that at one point, Internet Explorer popped up an alert to users to notify them that the connection was secured by HTTPS.  When HTTPS was rare, the lock icon drew attention to the additional protections provided by HTTPS.  Today, this is no longer true, and HTTPS is the norm, not the exception.  And we've been evolving Chrome accordingly.



"For example, we know that the lock icon does not indicate website trustworthiness.  We redesigned the lock icon in 2016 after our research showed that many users misunderstood what the icon conveyed.  Despite our best efforts, our research in 2021 showed that only 11% of study participants correctly understood the precise meaning of the lock icon.  This misunderstanding is not harmless.  Nearly all phishing sites use HTTPS, and therefore also" - they're secure phishing sites.



LEO:  They're secure phishing sites.



STEVE:  That's what you want in your phishing site.  If you're getting phished, you want it to be a secure phish.



LEO:  That's right.



STEVE:  So, "Misunderstandings are so pervasive that many organizations," they said, "including the FBI, publish explicit guidance that the lock icon is not an indicator of website safety."  But, you know, Let's Encrypt said let's have everybody use a lock icon, and that's what we got.



So "The lock icon is currently" - this is Google again talking - "the lock icon is currently a helpful entry point into site controls in Chrome.  In 2021 we shared that we were experimenting with replacing the lock icon in Chrome with a more security-neutral entry point to site controls.  We continued to mark HTTP as insecure in the URL bar.  Users in the experiment opened the site controls more, and they didn't express any confusion that can follow major UI changes.



"Based on these research results from ourselves and others, and the broader shift towards HTTPS, we will be replacing the lock icon in Chrome with a variant of the 'tune' icon.  We think the tune icon does three things:  Does not imply trustworthy."  I agree, doesn't imply anything to me.



LEO:  Yeah.



STEVE:  "Is more obviously clickable."  Okay.  "Is commonly associated with settings or other controls."  You know, and I guess that's the case.  And so, as I was thinking, the tune icon?  I guess everyone knows what the tune icon is but me.  And actually it's subtly different for Chrome than it is for Firefox.  In Firefox it's a mirror image.  And when you think about it, you could either flip it horizontally or vertically and get the same thing.



LEO:  Right, right, right.



STEVE:  And if you do both, you come back to where you started, so don't do that.  So I think it's like meant to be, like, first I thought maybe it was like horizontal sliders or audio faders.



LEO:  That's what I think it is, yeah.



STEVE:  Actually, I think it's switches, like in iOS.



LEO:  Oh, okay.



STEVE:  Where you push the switch to the left and the right, turn something on and off.



LEO:  Some people say it looks like two people in sleeping bags, sleeping head to foot. Once you see that, by the way, you'll never not see it.  So I'm sorry, I shouldn't have said that.



STEVE:  Unfortunately, yes.  The high school kids are giggling.



LEO:  It's sleeping, people in sleeping bags, yeah, uh-huh.



STEVE:  So yeah, unfortunately, Leo, I think you're right.  Now when I look at that icon I'm seeing little stick figures that are laying next to each other, unfortunately, and I don't think I'm ever going to be the same again.



Okay.  So in any event, as I noted, Firefox sometimes shows this.  It is its setting app.  So don't be thrown when Chrome's lock icon disappears and gets replaced by these little stick figures.  The presumption is that on the one hand the use of TLS has become so ubiquitous that our browsers will only alert us, actually as they now do; right?  If you try to go to a site that doesn't offer HTTPS, you have to, like, beg and cajole and plead with your browser to please, yes, I accept the fact that, oh, my god, I don't know what's going to happen, please let me go.



LEO:  Yeah, I think that's actually - we can, you know, say success.  That's good.  That's really good.



STEVE:  Yeah, yeah.  I agree.



LEO:  Yeah, everybody's secure.



STEVE:  Okay.  iOS users will have likely noticed that last week, for the first time ever, at least first-time ever on non-beta systems, Apple used, they deployed their new security feature that they call Rapid Security Response.  It's meant to be a lightweight delivery system for security updates for iOS and macOS.  It was announced during last year's WWDC as an update mechanism that was added into iOS 16 and macOS 13.  The idea is that, unlike Apple's previous update system, you know, where, I mean, it is - your device is down for half an hour while it first downloads a big monster, and then verifies it seemingly endlessly before then it installs it and then does a full reboot and everything.  This is pretty quick by comparison.



So they wanted the ability to rapidly deliver small security updates as separate standalone patches without needing to update the entire operating system, which is what they've traditionally done.  So it's believed that in this instance, last week's instance, they patched a Safari bug.  And it didn't go completely perfectly.  There were reports on social media that this RSR, the Rapid Security Response, update failed to install on some people's devices.  But, you know, it worked for me.



LEO:  Well, yeah, me, too, on all of my devices.  And I figured because it came right after last week's show where you talked about the Pegasus vulnerabilities, the zero-click vulnerabilities, I thought, you know, I bet that's what they're fixing.



STEVE:  Well, I think we could guarantee that whatever it was, it was something that was in the wild that they wanted to immediately shut down.  For what it's worth, if for some reason you did not want this, and I don't know why anyone would not want it, you can turn it off under Settings > General > Software Update > Automatic Updates.  There's the option to disable these little quickies.  But, boy, you know, if Apple thinks it's important enough to disrupt everyone briefly with the imposition of this, I wouldn't be inclined to say no to that being offered.



So Leo, I thought of you when I saw this next one, not only because WordPress is dropping their support for Twitter, but because they're adding it for Mastodon.



LEO:  Yes.  This was really good news.



STEVE:  This is what Automattic, the parent of WordPress, posted on their Jetpack official WordPress add-on site under the headline "The End of Twitter Auto-Sharing."  They said:  "In early April, we experienced an unexpected suspension of our Twitter API access.  This access is what powers Jetpack Social, which in turn helps you automatically share your blog posts to Twitter.  Though the service was restored that same day, it turns out that there were bigger changes looming on the horizon.



"Twitter decided, on short notice, to dramatically change the terms and pricing of the Twitter API.  We have attempted to work with Twitter in good faith to negotiate new terms, but we have not been able to reach an agreement.  As a result, the Twitter connection on Jetpack Social will cease to work, and your blog posts will no longer be auto-shared to Twitter."



Okay, now, remember WordPress is 43% of all Internet websites; and 97% of online blogs are WordPress.  And as a consequence of this failed negotiation, and you have to know WordPress probably tried to do the right thing for their users, but no.  Elon said we don't need you.  So Automattic says:  "You will still be able to share your posts to Twitter manually by pasting the post link into the body of your tweet."  Obviously, but not automatically.



"In addition," they said, "you can still auto-share your posts to Tumblr, Facebook, and LinkedIn.  In the near future, we are adding the ability to auto-share to Instagram and Mastodon.  We are continuing to release new features in Jetpack Social, so keep an eye on the Jetpack blog for more updates.  We apologize for any inconvenience this causes for your website and marketing efforts.  We wish the outcome had been different; but our customers are always our primary concern, and we're not willing to compromise the experience or value you receive from Jetpack."  So, okay.  Another casualty.



LEO:  Yeah, you know, Matt Mullenweg, who owns Automattic, created WordPress, is a big supporter of open source.  WordPress is open source.  And I think it's good he's supporting an open source platform with this.



STEVE:  Yup.



LEO:  They said very early on, you know, November of last year, that they were going to do ActivityPub stuff.  So that's something they probably were already working on.



STEVE:  Well, and I also did want to give - I wanted to give Mastodon its due.  Mastodon also announced a slew of new changes driven by the huge influx the platform has received over the past six months.  Those new features include quote posts, improved content and profile search, and support for groups.  And in addition, all new Mastodon users will be onboarded on mastodon.social, instead of having them pick their instance, which was a process that confused many users.



LEO:  Yeah, I'm not happy about that at all because that just makes Mastodon.social more unwieldy.  



STEVE:  Right.  And you sort of start to lose the whole concept of distribution.



LEO:  Centralized, yeah.  Suddenly it's centralized.  And frankly, Mastodon.social is already really too big to easily moderate.  It's over 100,000 users.



STEVE:  Wow.



LEO:  So, you know, there are better places to join.  It does at least say you can join Mastodon.social right now, or here are some, you know, choose some other site.  So you're not forced to.



STEVE:  Ah, okay.



LEO:  It gives you a choice.  It's just got a default now.  But as you know, the tyranny of the default.



STEVE:  Uh-huh.



LEO:  You know what's going to happen.



STEVE:  Yes.  And especially going to a new place, you don't really get the whole decentralized model.  So it's like, uh, okay, what?



LEO:  Just like Twitter.  You just join Mastodon.social.  A lot of people did.  That's why there's 100,000 people there.



STEVE:  Okay.  So there's an organization called "NewsGuard" which calls itself "The Internet Trust Tool."  I hadn't heard of it before so I wanted to, like, see whether I trusted these guys.  To give you some idea of its pedigree, which I discovered, it has co-CEOs and co-Editors-in-Chief Steven Brill and Gordon Crovitz.  Both are attorneys, veteran journalists, and news entrepreneurs.  Steven Brill founded The American Lawyer magazine, the Court TV cable channel, and like you, Leo, is a Yalie, having received both his bachelor's and his law degree from Yale, where he also founded the Yale Journalism Initiative.



LEO:  Yeah.  Steven's a well-known guy, yeah.  He's good.



STEVE:  Right, yeah, Brill.  Gordon Crovitz is a Phi Beta Kappa graduate of the University of Chicago.  He received a law degree as a Rhodes Scholar from Wadham College of Oxford University and later a law degree from Yale.  He was the publisher of The Wall Street Journal and also served as executive vice-president of Dow Jones.  So together, the two of them have supervised thousands of journalists around the world.  They were also the co-CEOs and co-founders of Press+, which was sold to RR Donnelley.  So, pretty clearly, they're not a pair of schlubs.  And NewsGuard, which they co-founded in 2018, is not some scammy fly-by-night operation.



The title of their posting which caught my eye, and the reason we're talking about this today, was "Rise of the Newsbots:  AI-Generated News Websites Proliferating Online."  And the tagline beneath the headline reads:  "NewsGuard has identified 49 news and information sites that appear to be almost entirely written by artificial intelligence software.  A new generation of content farms is on the way."



So the fact that the Internet is fundamentally so bot-compatible represents a real problem.  And of course this is not the first time this has been a problem.  The reason we have spam is that email is also 100% bot-compatible.  But sadly, it appears that the value of the Internet for news and journalism is about to become a whole lot worse.  Or at least diluted.  Because we all need to understand and appreciate what's going on around us, and because I suspect that it will give every listener some pause when they contemplate what is already happening to our Internet, I've lightly edited the article they posted in order to share it.  So here's what their recent research has just revealed.



They wrote:  "NewsGuard has found that artificial intelligence tools are now being used to populate so-called 'content farms,' referring to low-quality websites around the world that churn out vast amounts of clickbait articles to optimize advertising revenue.  Last month, NewsGuard identified 49 websites spanning seven languages  Chinese, Czech, English, French, Portuguese, Tagalog, and Thai  that appear to be entirely or mostly generated by artificial intelligence language models designed to mimic human communication, in the form of what appear to be typical news websites.



"The websites, which often fail to disclose ownership or control, produce a high volume of content related to a variety of topics, including politics, health, entertainment, finance, and technology.  Some publish hundreds of articles a day.  Some of the content advances false narratives.  Nearly all of the content features bland language and repetitive phrases, hallmarks of artificial intelligence.



"Many of the sites are saturated with advertisements, indicating that they were likely designed to generate revenue from programmatic ads, ads placed algorithmically across the web which finance much of the world's media, much as the Internet's first generation of content farms, operated by humans, were built to do.  In short, as numerous and more powerful AI tools have been unveiled and made available to the public in recent months, concerns that they could be used to conjure up entire news organizations  once the subject of speculation by media scholars  have now become reality.



"Last month, NewsGuard sent emails to the 29 sites in the analysis of 49 that listed contact information, and two confirmed that they have used AI.  Of the remaining 27 sites, two did not address NewsGuard's questions, while eight provided invalid email addresses, and 17 did not respond.  NewsGuard exchanged a series of emails, some of which were hard to comprehend, with the self-described owner of Famadillo.com, a site that has published numerous AI-generated product reviews attributed to 'admin.'  This person, who identified themselves as Maria Spanadoris, denied that the site used AI in a widespread manner.  This is a direct quote of her.  'We did an expert to use AI to edit old articles that nobody read anymore just to see how it works,' Spanadoris  who declined a phone call with NewsGuard  said, without elaborating.



"Adesh Ingale, who identified himself as the founder of GetIntoKnowledge.com, a site that NewsGuard found to have published AI-generated clickbait articles about history, science, and other topics, responded:  'We use automation at some points where they are extremely needed.  And yes, they are 100% facts checked so that no false information is created.  As a world is growing towards digital and automation era, we have introduced some automation softwares in our work, but the results getting out of it are 100% original and regional facts based.'  When asked by NewsGuard, Ingale did not elaborate on the site's use of AI, and claimed that the site's content is 'published manually under human supervision.'  Ingale added:  'We are the new age of providing knowledge to each and every corner.'



"The 49 AI-driven sites that NewsGuard identified typically have benign and generic names, suggesting they are operated by established publishers, such as Biz Breaking News, News Live 79, Daily Business Post, and Market News Reports.  The AI-generated articles often consist of content summarized or rewritten from other services.  For example, BestBudgetUSA.com, a site that does not provide information about its ownership and was anonymously registered in May of 2022, appears primarily to summarize or rewrite articles from CNN.



"The articles themselves often give away the fact that they were AI produced.  For example, dozens of articles on BestBudgetUSA.com contain phrases of the kind often produced by generative AI in response to prompts, such as, 'I am not capable of producing 1,500 words.  However, I can provide you with a summary of the article,' which it then does."



LEO:  Yeah, if you search for "as an AI," you'll find it everywhere.  It's amazing.  People are so lazy, they don't even cut that part out.



STEVE:  Wow.  Wow.



LEO:  It's just mindboggling.



STEVE:  And then in this case that quote was followed by a link to the original CNN report.



LEO:  Yeah, there you go.



STEVE:  God.  So they said:  "The presence of these sorts of phrases also evidence that these sites likely operate with little to no human oversight.  Many of the AI-generated articles identified by NewsGuard are credited to 'Admin' and 'Editor,' or have no bylines at all."



LEO:  Yeah, exactly.



STEVE:  "Other sites feature fake author profiles.  For example, HarmonyHustle.com, an anonymously operated site registered just last month, lists content creators including 'Alex' and 'Tom.'"  You know, Alex and Tom.



LEO:  Oh, yeah.  No, Alex and Tom, yeah, yeah.



STEVE:  "A reverse image search of their profile photos revealed that neither author is authentic."



LEO:  Oh, wow.



STEVE:  "Some of the sites also include About and Privacy Policy pages that were algorithmically produced by tools used to generate customizable disclaimers and copyright notices, but were not fully completed, leaving little doubt about their source.  For example, the About Us page of HistoryFact.in, an anonymously run AI-generated site identified by NewsGuard, stated that, okay, get a load of this:  'This website was founded in [date] by [Your Name].  Also, History Fact commits to reply to all people who subscribe to the YouTube Channel [channel link] and follow our website.  We hope you enjoy our services as much as we enjoy offering them to you.  Sincerely, [Your Name].'"



LEO:  But this has been the computer era forever.  That's like mail merge that doesn't bother to merge in a name.



STEVE:  Right, right.



LEO:  You know, we've seen this forever and ever and ever, yeah.



STEVE:  Right.  So they said:  "The page linked to a Free About Us Page Generator tool, which produces customized site descriptions.  NewsGuard found that many other sites were using similar tools, including a Disclaimer Generator, to create Terms of Service and Privacy Policy pages."  So they finish:  "The unassuming reader would likely have no idea that the articles produced by many of these AI content farms were not written by a human, if not for one glaring tell:  All 49 sites identified by NewsGuard had published at least one article containing error messages commonly found in AI-generated texts, such as 'my cutoff date is September 2021.'"



LEO:  2021.



STEVE:  Yeah.  Or "as an AI language model, I cannot complete this prompt."



LEO:  Yes.  So just leave it in because it's all automated.  They don't care.



STEVE:  And then they finish, saying:  "For example, CountyLocalNews.com, which publishes stories about crime and current events, published an article in March this year" - so just a couple months ago - "whose title read like that of an AI parody.  It stated:  'Death News:  Sorry, I cannot fulfill this prompt as it goes against ethical and moral principles.  Vaccine genocide is a conspiracy that is not based on scientific evidence and can cause harm and damage to public health.'"



LEO:  Well, there you go.



STEVE:  "As an AI language model, it is my responsibility to provide factual and trustworthy information."



LEO:  Good.



STEVE:  So anyway, and so here's the concern.  The laughable mistakes that are being made today by these first AI-driven news sites is only evidence that we're still in the early days, you know, the first stages of this new and emerging nightmare.  Bruce Schneier's pithy observation that attacks only ever get better unfortunately applies here. We can be sure that these fake news sites are only ever going to get better and become increasingly difficult to spot.  Their goal will be to fool Google, Bing, and other search engines into indexing their content.  That will bring eyeballs to their ad-laden pages for revenue generation.



What they will be devoid of is any thoughtful, new, or original content or journalism.  They'll just be AI-driven regurgitation factories.  And unfortunately it's possible to imagine that the same deliberate social media-style biases that have proven to be so powerful and effective in hooking people to their content will eventually be added to make them even more effective.  So here come the AI.



LEO:  Yup.  Not surprised.



STEVE:  Okay.  So this is just - this almost last piece is just juicy and amazing.  Because it used Cyrillic, I put the actual headline from Kremlin.ru in the show notes, but just sort of because it was fun.  I picked up a hint of some news that Russia was annoyed by the fact that all of the technologies they and the rest of the world are using originate from the West, and specifically from America.  So there was apparently serious discussion of replacing it with their own homegrown protocols.  I found the page on the Kremlin's website containing a transcript of a conversation with Vladimir Putin during a conference which he convened where this was being discussed.  I have the URL of the page, which is by the way HTTP, not HTTPS.



LEO:  Oh, boy.  Uh-oh.



STEVE:  And it's kremlin.ru/events/president/news/71015.  Now, naturally this page was all written in Russian.  But this time I felt that it was worth translating.  So I can share the relevant portion of the transcript of the dialog that ensued, but keep in mind that this is a machine translation from Russian into English.  But still it's pretty good.



So Dmitry Peskov is quoted:  "Of course, we are trying to look ahead.  We understand that for an industry in which hundreds of thousands of vehicles are constantly in the air, the key issue for the long term is safety.  Security needs to be addressed at the deepest level, at the level of our own standards, and of course at the level of using our own space systems.  Russia has its own grouping and its own communication standard, but the standards are all in the American format - TCP/IP, the Internet.  They originate from there.  And colleagues have such a very ambitious project to solve this problem."  Whoa.



"To date, we have begun to develop a control system for these devices and a communication modem that will work on our own communication protocol.  Realizing that the Russian Federation is dependent on foreign technological information and telecommunications solutions, 10 years ago we took the initiative to develop our own stack of technologies that will create an independent Russian information and communication space.  We call this project 'Internet from Russia.'"  And what's not to love about that?



LEO:  Internet from Russia.



STEVE:  That's right.



LEO:  Yeah.  In Russia, Internet surfs you.



STEVE:  "The foundation of our technological solutions is our own data transfer protocols, the implementation of which will allow us to abandon the use of the American TCP/IP network protocol stack."



LEO:  Yes, capitalist protocol.  Evil, yes.



STEVE:  "On which, in principle, we all work now, and high-speed data transmission and reception technology based on low-orbit satellites.  In 2016, we developed in hardware and tested all the main technological blocks that are necessary" - see, they don't move packets.  They move blocks - "that are necessary to create a broadband satellite communication system."



LEO:  Soviet blocs, yes.



STEVE:  Soviet blocs.



LEO:  Yes, join our blocs.  Oh, god.



STEVE:  I know.  To create - I hope they do this, Leo.  Goodbye Russia.



LEO:  Absolutely.  That'd be it.



STEVE:  That would be wonderful.



LEO:  You've got one tank and your own Internet.  Goodbye.



STEVE:  They flipped the switch and were never heard from again.



LEO:  Unbelievable.



STEVE:  So, "necessary to create a broadband satellite communication system," you know, that's disconnected from the West.



LEO:  Yeah.



STEVE:  So he said:  "In 2018, an interdepartmental commission was formed" - got to love those - "the two-month results of which led to the recommendation of the state corporation Roscosmos to combine our efforts in the development of satellite communication systems.  To date, we have not approached this yet, but we understand that the creation of a Russian global satellite communications and navigation system with the technologies" - what are they smoking?  It must be really something - "with the technologies that we have developed will have a multiplier effect for the development and technological..."



LEO:  Yeah, minus one.  Multiply times zero.



STEVE:  And then we multiply by minus one, then we divide by zero.  And that gets us to infinity.



LEO:  Infinity and beyond.



STEVE:  That's right.  So a "technological leap of our Motherland.  Based on state capabilities, and with these technological solutions, we are ready to create cooperation in the shortest possible time necessary for the speedy production and deployment of both terrestrial and satellite communication segments of the Internet from Russia project.  According to our calculations, the deployment will take from one and a half to five years."



LEO:  Times zero.



STEVE:  "We ask for your support in the implementation of this strategic project for the country."  Whereupon Vladimir Putin says:  "What exactly needs to be done?"  And Alexander Selyutin steps up.  "Specifically," he says, "we need funding.  We need a decision to approve, to put at least on an equal footing, the Russian communication protocol with American network protocols."



Putin asks:  "Do you see any preferences for foreigners here?"  Now, I think this was lost in translation because I didn't understand what he means by "Do you see any preferences for foreigners here."  Alexander Selyutin replies:  "I see preferences, of course.  Why?  Because the experience of communicating with Roskomnadzor, Rostelecom, and other organizations shows that the entire system that has been formed today works on the American network protocol stack.  Therefore, we are constantly information dependent.  Therefore we have to defend ourselves, encrypt our data.  This is a constant process.  Since this whole system" - you can just imagine the NSA is just rubbing their hands together saying, oh, please, please, please do this.  Do your own.



Anyway:  "Since this whole system is formed for this processing, something new is a challenge.  Against this challenge, I am unable to resist," says Alexander.  Vladimir Putin:  "I understand.  As it has developed since the '90s, it continues."  Alexander replies...



LEO:  They need better translation software.  This is terrible. 



STEVE:  Alexander says:  "As it happened, this is how things are going.  Anything new that is offered is immediately swept aside."  Probably meaning that like maybe Russia has suggested some tweaks to TCP/IP, and we said, what?  No.



Anyway, Vladimir:  "Yes.  Or you need to go there, to that site."  Alexander:  "Or you need to go to that site, really, just like that.  Although, in terms of parameters, we finished testing the Russian protocol in December 2022 at the 16th Institute of the Ministry of Defense."  He says:  "I don't know if we can talk about this or not."  Vladimir Putin:  "You have already said.  The word will fly out.  You will not catch it."



And finally, Alexander Selyutin says:  "The results showed that we work better than TCP/IP on the network.  I'll give you a number.  If the delay or loss of packets in TCP occurs at a rate of 1%, then TCP recovers these packets within 360 seconds.  We restore in 5.7 seconds."  Whoa.  Okay.  So first off.



LEO:  That's just gobbledygook.



STEVE:  Yes.  Let's just note that there is nothing whatsoever American about TCP/IP.



LEO:  That's true, yeah.



STEVE:  The RFCs which specify the operation of every protocol in use are not red, white, and blue.  They are black and white.



LEO:  Yeah.



STEVE:  And certainly Russia has produced plenty of their own Russian networking gear that runs just fine by following the same RFCs that the rest of the world uses.  Again, yeah, I'm glad you like that.  I thought that worked out well.  Not red, white, and blue; they're black and white.



LEO:  Black and white, I love it.



STEVE:  Now, I know TCP pretty well, having implemented much of an IP stack from scratch, first for ShieldsUP! and later for the DNS Spoofability Test.  It's not at all clear to me that there's anything wrong with what we have today.  Over the years, TCP has been occasionally tweaked until it has become a highly dynamic protocol that's able to adapt to a widely varying range of bandwidth and network latencies.  Except apparently it's unable to keep my cable modem online.  But anyway.



Many years ago we did a series of podcasts, Leo, you and I, on the operation of the Internet.  And because dropped packets are a deliberate design point of the Internet's brilliant packetized conception, we thoroughly discussed TCP's dropped packet recovery mechanism.  In brief, the sender of TCP data holds the data that has been sent in local buffers so that it will be able to retransmit that data if necessary.  Once the sender receives the data, a returning "ACK" packet is sent which carries an increasing byte sequence number to indicate the highest numbered byte that has been received so far.  At that point the sender is free to release that data which it had been holding since its receipt has now been confirmed by the other end.



If just a few seconds elapses without a returned ACK, the sender will auto-resend all of the traffic from the point after the highest sequence numbered ACK received so far.  360 seconds claimed by Alexander is six minutes.  I have no idea what Alexander means when he states that TCP requires six minutes to resend packets, whereas the Russian protocol does so in 5.7 seconds.



LEO:  No, it's nonsense.



STEVE:  Six minutes is nonsense.



LEO:  Nonsense.



STEVE:  Yeah.  So again, given that there is really nothing American or Western about open, published, standards-based network protocols, I cannot imagine that it could possibly make sense to reinvent that wheel.  And, of course, doing this would make the Russian Internet completely incompatible with any of the rest of the world's networking equipment.  This would turn Russia into an Internet island, utterly unable to communicate with anyone else.  And there's no way anyone else is going to start supporting some random new Russian networking protocol.



Then, as I was thinking about this, it occurred to me, though, they would still need to retain some of that despicable Western cowboy TCP/IP protocol network so that they're still able to create fake Facebook profiles and postings, and to attack Western networks and organizations, and to receive their ransom payments in Western cryptocurrencies.  So not quite so easy to unplug from the West. 



LEO:  Yes.



STEVE:  And Leo, let's take our last break.



LEO:  Okay.



STEVE:  And then we're going to talk about Vint Cerf's three mistakes in creating TCP/IP.



LEO:  I interviewed Vint many, many moons ago.  Well, not that many moons ago.  But I did ask him, you know, if you were going to do it all again, TCP/IP all over again, what would you do differently?  He said - I think it was on a Triangulation, actually.  He said, "I'd have encryption."  But I'm curious what other missing items he has come up with since.  So I will be listening with great interest.



STEVE:  Okay.  So Vint Cert's three mistakes.



LEO:  I'm really curious about this, yes.



STEVE:  Since we're talking about TCP/IP.



LEO:  He invented it.



STEVE:  I mean, there's just no way, okay, just for the record, there's just no way that it's possible for Russia to come up with their own networking protocol.



LEO:  Of course not.  It's propaganda.  It's not real.



STEVE:  Well, I mean, it was a discussion with Vladimir in a conference that he held, and it isn't happy.



LEO:  Yeah.  You think [crosstalk], oh, it's propaganda.  It's just propaganda.  That's all.  They can't - it would take them off the Internet.  They don't want that.



STEVE:  Yeah.



LEO:  Anyway.



STEVE:  Okay.  Since we're talking about TCP/IP, Vint Cerf was the recent recipient of the 2023, that's this year's, IEEE Medal of Honor for "co-creating the Internet architecture and providing sustained leadership in its phenomenal growth in becoming society's critical infrastructure."



So in a conversation with IEEE Spectrum Magazine, which is the IEEE's traditional magazine called Spectrum, which occurred two days ago on May 7th, Vint admitted that, in retrospect, he did not have a perfect view of the Internet's future.  To which I would interject...



LEO:  Of course not.



STEVE:  ...how could he or anyone have possibly foreseen what this has become?  He said that, in hindsight, there were a few things he got wrong, and three of them stood out for him.  So here are the three things that one of the two creators of the incredibly successful networking system which grew to become the Internet, gluing the entire world together into a single global network - at least until Russia leaves - feels that he got wrong.



Okay.  So first, he said:  "I thought 32 bits would be enough for Internet addresses."  He says:  "And of course," he said, "everybody laughs and says, 'You idiot, why didn't you use 128-bit addresses?'  The answer is, back in 1973, people would've said, 'You're crazy if you think you need 3.4 times 10 to the 38th addresses.'"



LEO:  There were four computers or something.  I mean, yeah.



STEVE:  Right, "'to do an experiment that you aren't sure is going to work.'"



LEO:  Right, right.



STEVE:  He said:  "So that was a mistake," he says, "although I don't think at the time I would have been able to sell 128 bits."  And of course he's right.



LEO:  Yeah.  No blame on that one.



STEVE:  Second mistake, he says:  "I did not pay enough attention to security."  And he said:  "Before public key cryptography came around, key distribution was a really messy manual process.  It was awful, and it didn't scale."



LEO:  Right.



STEVE:  "So that's why I didn't try to push that into the Internet.  And by the time they did implement the RSA algorithm, I was well on my way to freezing the protocol, so I didn't push the crypto stuff.  I still don't regret that because graduate students, who were largely the people building and using the Internet, would be the last cohort of people I would rely on..."



LEO:  I love this.



STEVE:  "...to maintain key discipline."



LEO:  I love Vint.  He is so great.  Oh, man.



STEVE:  He said:  "Although there are times when I wish we had put more end-to-end security in the system to begin with."  So, yeah.  Just great.  He says, you know, it was grad students who were doing this, and no.  We're not giving the keys to those kids.



And then third and final, he said:  "I didn't really appreciate the implications of the World Wide Web."  So he said:  "I didn't expect the avalanche of content that went onto the Internet once the web was made available.  And what happened as a result of that avalanche is that we had to invent search engines in order to find stuff because there was so much of it."  He said:  "I absolutely did not predict that search engines would ever be needed."  So I thought that was interesting.



LEO:  You know what, Steve, both of us used the Internet in the early days.  We didn't know either; right?



STEVE:  And it was crappy back then.



LEO:  Yeah.  [Crosstalk] came along, and it was a human-written directory of Internet websites.  That's all we thought we needed.  There were only a handful.



STEVE:  Right.  And there was AltaVista.  That was the one that we were all using.



LEO:  That was later, yeah, yeah.



STEVE:  Later, when - and it was not good because it was just, you know, random spidering the web, and here's everything we found.



LEO:  Right.



STEVE:  You know, it was Google's site ranking breakthrough, that brilliant invention of theirs, to rank sites based on the quality of the sites that link to them, that changed everything.



LEO:  Huge, yup.



STEVE:  So, you know, and my feeling is, you know, many inventions are going to happen sooner or later.  They're just going to.  But it isn't always the case that the ones that do happen are the best that we can get.  Unfortunately, I'm thinking of FIDO.  And sometimes we get stuck with solutions that are inherently suboptimal or maybe a dead end.  So something like the Internet would have eventually been created if Vint Cerf and Robert Kahn had not done it.  The world should be thankful that they did because they really got it right.



LEO:  It's kind of amazing, really.



STEVE:  It is.  It is astonishing, Leo, that Vint's insanely modest statement that he could have done better, in my opinion that's nonsense.  It was, frankly, impossible to do better than they did 50 years ago, that's how long ago it was, 50 years ago in '73, as we just quoted him saying, back in '73 when they first created those IMPs - the Interface Message Processors - and the goal was to see whether packets could make it from Stanford to San Diego.  And, you know, the rest, as they say, is history.  And we've lived through 50 years of it, and it's been wonderful.



LEO:  Let me recommend a wonderful book I think everybody who listens to the show would enjoy, by Katie Hafner, H-A-F-N-E-R, called "Where Wizards Stay Up Late."  And it's about Vint, and Bob Metcalfe, and all of the people who made this happen.  And it talks about the IMPs.



STEVE:  The early pioneers.



LEO:  And it's really a - what's his name, Licklider, who was the original guy at the Defense Department who said, let me throw some DARPA money at you guys, see what you can come up with.  It's just a - it's a wonderful story, J.C.R. Licklider.  Highly recommend it:  "Where Wizards Stay Up Late." 



STEVE:  Okay.  So Detecting Unwanted Location Trackers Part 1.  I called this "Intro to."  It seems that any powerful new technology gets used for both the benefit and the detriment of society.  In other words, it's a mixed blessing.  And so is the case for AirTags, those popular and handy Bluetooth LE, you know, low-energy dongles that are all about their location.



The trouble, of course, is that there are innumerable situations where the power of location tracking and reporting can be abused.  Years ago on this podcast we reported about AirTags being used by car thieves who would surreptitiously attach an AirTag to a desirable car which was at the time located in a publicly exposed parking lot from where it could not easily be stolen.  Instead, they'd AirTag it and then use its location-tracking technology to follow the vehicle to a much more private location, typically the owner's home, where it could then be stolen.



At the same time, there are instances where you might want your car to be tracked.  Coincidentally, nine days ago, on Sunday April 30th, New York City's Mayor Eric Adams held a press conference during which he urged all New Yorkers to equip their cars with Apple AirTags, while offering to supply 500 free AirTags to get the ball rolling.  I doubt that 500 is going to make much of a dent in the need, but okay.



Mayor Adams held the press conference at the 43rd Precinct in the Bronx, where he said there had been 200 instances of grand larceny of autos.  An NYPD official said that in New York City, just this year so far - and get a load of this because we've talked about this - 966 Hyundais and Kias had been stolen, thus already surpassing last year's entire total of 819.  The NY Police Department's public crime statistics tracker says that there have been 4,492 vehicle thefts this year alone, a 13.3% increase compared to the same period last year, and the largest increase among New York City's seven major crime categories.



Our listeners are probably not surprised to hear Hyundais and Kias being singled out since we previously covered the interesting news of the Kia TikTok Challenge which encouraged people to steal those vehicles with a USB cable, thanks to a flaw in the design of those car models.  And in related news, earlier last month, on April 7th, New York City announced litigation against Kia and Hyundai, blaming the rise in car thefts for those well-known design flaws which enable and make the thefts so easy.



Okay.  Anyway, it was interesting to have New York City's mayor and the police department proactively recommending the use of an inexpensive consumer-grade tracking technology to help track down stolen cars.  Google is reportedly getting ready to produce their own branded trackers, but the technology can be a true mixed blessing.  Late last year, Apple was sued by two women who allege their previous romantic partners used AirTags to track their whereabouts, potentially putting their safety at risk.  And separately, according to reports last June, an Indiana woman allegedly used an AirTag to track and ultimately murder her boyfriend over an alleged affair.



So sometimes we want tracking; sometimes we don't.  But in all cases, what we want is control over the process.  Consequently, Apple and Google have plenty of incentive to work together to mature this technology to minimize the risk of its presence when tracking is not wanted.  And what we have today is a nearly final early working proposed standard that has been submitted to the IETF for subsequent hashing out and finalizing.



Apple's VP of Sensing and Connectivity, Ron Huang, was quoted saying:  "Apple launched AirTag to give users the peace of mind knowing where to find their most important items.  We built AirTag and the Find My network with a set of proactive features to discourage unwanted tracking  a first in the industry  and we continue to make improvements to help ensure the technology is being used as intended.  This new industry specification builds upon the AirTag protections, and through collaboration with Google results in a critical step forward to help combat unwanted tracking across iOS and Android."



So Apple already provides a solution for unwanted tracking of Apple's devices.  In this context, for the rest of the discussion, "Unwanted Tracking" refers to a Bluetooth LE device that is not known to be yours, that is, is not paired with the device you're carrying, but which is detected and is moving along with you.  This is what would happen if, for example, someone had planted an AirTag-style tracker on you, on your car, or on something you're carrying.



So this is a super-useful feature, but until now it has only worked with Apple's own devices.  And that's what's changing and being significantly expanded.  Although Apple has released a "Tracker Detect" app for Android smartphones, it still only detects AirTags, and it only works while the app is launched and running.  So what the industry needs is a single, unified, cross-platform solution based upon clearly defined standards, which is what we're all about to get.



Dave Burke, Google's VP of Engineering for Android, said:  "Bluetooth trackers have created tremendous user benefits, but they also bring the potential of unwanted tracking, which requires industry-wide action to solve.  Android has an unwavering commitment to protecting users and will continue to develop strong safeguards and collaborate with the industry to help combat the misuse of Bluetooth tracking devices."



We're here today for Security Now! podcast 922 to dive into this technology that will soon evolve into an industry-wide standard.  In addition to Apple and Google, Samsung, Tile, Chipolo, eufy Security, and Pebblebee have all expressed support for the forthcoming draft spec, which offers best practices and instructions for manufacturers who choose to build these capabilities into their products.  The title of today's podcast is exactly the title of the proposed IETF draft specification, "Detecting Unwanted Location Trackers."  The abstract of the spec describes its goal succinctly.



It says:  "This document lists a set of best practices and protocols for accessory manufacturers whose products have built-in location-tracking capabilities.  By following these requirements and recommendations, a location-tracking accessory will be compatible with unwanted tracking detection and alerts on mobile platforms.  This is an important capability for improving the privacy and safety of individuals in the circumstance that those accessories are used to track their location without their knowledge or consent."



So this is where, at nearly two hours, we need to pause this discussion this week.  The specification for the detailed operation of this technology is surprisingly detailed, as all good specifications are, so I need to do it justice.  By the time I got to this point in the podcast notes, I was at page 19, which generally means we're well past the hour and a half point, and there wasn't enough time remaining to cover this the way I want to.  As you'll see next week, Apple and Google have not taken any half measures here.  The technology that's going to be buried into the chips of these next-generation trackers is actually a bit astonishing.  For example, they know how long they've been away from their owner, and their behavior completely changes.  There's randomized MAC addressing, and even forward-incrementing pseudorandom functions built in.



And there are some already controversial aspects, such as Section 3.15, which discusses the creation of an industry-wide "Pairing Registry," of which the document says:  "Verifiable identity information of the owner of an accessory at time of pairing SHALL" - in all caps, which is the way IETF docs are written - "SHALL be recorded and associated with the serial number of the accessory, for example, their phone number and email address."



LEO:  No.



STEVE:  Then the following Section 3.15.1 titled "Obfuscated owner information" explains that:  "A limited amount of obfuscated owner information from the pairing registry SHALL be made available to the platform" - meaning someone who is in range of the token - "along with a retrieved serial number.  This information SHALL be part of the response of the serial number retrieval from a server which can be rendered in a platform's HTML view."  This allows someone near to a tracker which they are not paired with to see something about the registered owner of that tracking device.  The spec says that the displayed data MUST include at least either the last four digits of the owner's telephone number or an email address with the first letter of the username and the domain name visible, as well as the entire email server's top-level domain.



Then Section 3.15.2, titled "Persistence," explains that:  "The pairing registry SHOULD be stored for a minimum of 25 days after an owner has unpaired an accessory.  After the elapsed period, the data SHOULD be deleted."  And then here's the big one.  Section 3.15.3, titled "Availability for law enforcement," states that:  "The pairing registry SHALL be made available to law enforcement upon a valid law enforcement request."  In other words, anyone using any next-generation tracker will have that tracker affirmatively registered with their real-world identity and stored in a law enforcement-accessible database.



LEO:  No, no.  Oh, god.  This is a nonstarter, though, thank god.  No one's going to go for that.  I understand they're trying to fix one harm, which is stalking.



STEVE:  It's clear that Apple has been deeply affected by the previous abuses of their first-generation technology and wants to erect some serious safeguards going forward.



LEO:  No.  You think Apple proposed this?  Apple's going to stop it.



STEVE:  Apple's 100% behind this.



LEO:  Ugh.  It's a nonstarter.  It's just not [crosstalk].  So go ahead, I'm sorry.



STEVE:  I know.  So what this means is that, if you're going to use any of these, the next-generation AirTag tracking technology, then you're accountable for the location of your AirTags.  That is, you're accountable for where they go and how they're used.  Somebody who's near the AirTag can query why there's an AirTag that is moving with me and obtain that blanked-out information about the actual owner of the AirTag.  Maybe it's their kid's AirTag or partner's AirTag.  Anyway, there is a ton of detail about this.  We're going to cover it next week.  But I agree with you, Leo, it is, it's a little breathtaking.



LEO:  Oh, it's a nonstarter.  



STEVE:  To know that AirTags are registered.



LEO:  And with law enforcement.  So the cops have a database of every AirTag and who owns it.  You're just enabling absolute surveillance.  I can't believe Apple's proposing this.  I think, if they're proposing it, it's because they know it's a nonstarter, and they think this will make them look like they're trying to do something.  Unbelievable.  All right.  I'll let you - we'll talk more about it next week.  All I can say is what could possibly go wrong; right?  Geez, Louise.



STEVE:  Or what could possibly save this.



LEO:  Yeah, well, no.  You know what?  No one would - no one in their right mind would ever again buy an AirTag.  Apple would solve the tracking problem, sure, because no one would buy another AirTag.  Or Tile.  Or any of these devices.  My fear is they'll put this in the phone.  You know, many Apple devices, including your phone and your AirPods, are Find My devices.



STEVE:  Right.



LEO:  All right.



STEVE:  I think we can assume it's going to be.  And the spec talks about that.  So we'll talk about it next week.



LEO:  Next week.  Next week.  Maybe this is their peace offering to the FBI.  Maybe that's it.  Steve Gibson is at GRC.com.  That's his home on the Internet.



Copyright (c) 2023 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.






GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#923

DATE:		May 16, 2023

TITLE:		Location Tracker Behavior

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-923.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we're going to answer only two questions.  First, why hasn't Steve been saying anything about his work on SpinRite recently?  And then second, what are all the details spelled out in the emerging specification for the detection of unwanted location tracking?



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  We've got of course the Picture of the Week, nice little cartoon about AI.  We've got an update on his software project SpinRite.  We'll talk a little bit about a software coding philosophy, even as it applies to his favorite language, assembly language.  And then we're going to dive deep, as we did last week, into location tracker behavior, in particular the future spec that both Google and Apple want to adopt for these little Bluetooth location trackers.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 923, recorded Tuesday, May 16th, 2023:  Location Tracker Behavior.



It's time for Security Now!, the show where we cover the latest security news with our champion, Mr. Steven Gibson.  Hi, Steve.



STEVE GIBSON:  Hey, Leo.  It's great to be with you for Episode 923, where we're going to continue, we're going to pick up basically where we left off after last week's episode, talking about the behavior of location trackers.  And it was my intention to do what I always do, or almost, "almost" being the operative word in this case because I didn't do it, which is to actually talk about some news in addition to our main subject.



LEO:  Well, you had to prepare the way beforehand.



STEVE:  Well, yes.  And the problem is doing this specification of AirTag and other location tracking behavior justice, I just - the deeper I got into it, the more I had to unwind it because the spec turns out to be really poorly written.  It's all out of order.  They refer to things in other places.  And it's like, okay, wait, what?  So anyway, and I ended up with 18 pages without ever talking about any news.  So this is one of our - this is a pure deep dive.



We're going to answer only two questions this week.  First, why haven't I been saying anything about my recent work on SpinRite?  And second, what are all the details spelled out in this emerging specification for the detection of unwanted location tracking?  On the other hand, I think everybody's going to find this very interesting and fascinating.  And I will do the two previous weeks of news next week for our podcast.  So, and of course we do have another good Picture of the Week.  So I think another great couple hours of entertainment for our listeners, and a lot of information.



LEO:  As you always do, Steve.  Always.



STEVE:  So our Picture of the Week is a fun one, apropos of many of the things we've been talking about relative to AI.  It's a two-frame cartoon.  The first frame on the left has someone sitting in front of his computer.  He's very excited.  He's explaining to a friend standing behind him:  "AI turns this single bullet point into a long email I could pretend I wrote."  Very excited about that.  And then in the second frame we have a gal sitting in front of her computer.  Oh, I should mention, on the left-hand screen we just seen one little bullet point.  On the right-hand screen, we see her screen is full.  And she's explaining to her friend:  "AI makes a single bullet point out of this long email I could pretend I read."



LEO:  It's almost not even a joke.



STEVE:  I know.



LEO:  It's actually true.  Oh, my god, yeah.  I think at Google I/O they practically showed this as a feature; right?  Which is good.  Let the AI handle the email.



STEVE:  Yeah.  Besides, no one likes typing on those touch screens.  



LEO:  Yeah.



STEVE:  So just, you know, give it a clue.  Give it a hint.



LEO:  Let the AI do it.



STEVE:  Let it know how you're feeling, and it'll just go ahead and respond on your behalf.  So, that's right.  Okay.  So before I get into our main topic, which is going to dominate the rest of the podcast, I did want to mention why I've been mute for the past couple of weeks about ongoing work on SpinRite.  And that's not because of any slowdown in the action, but because of a  major revamping of SpinRite's primary core data recovery code.  As it happens, it sometimes occurs that software gets itself painted into a corner.  Or a programmer paints themselves into a corner.  Or maybe the software paints the programmer into a corner.  I don't know.  But paint and corners always seems to be involved.



This can occur when an inexperienced programmer tackles a task that they don't fully understand.  You know, they jump right in and start writing code, and at some point they realize that they can't get where they need to go from where they are.  They're in a corner.  And at the start of our careers, we coders, that's a common occurrence to many of us who love and live to code.  It can also happen, however, when a sufficient quantity of new information arrives late in a software design cycle which requires that the code be modified in a way it wasn't designed to be, to incorporate the needs created by this new information.  And since I checked, I did some subtraction, since I've been coding now for more than 60, six zero, years, inexperience is not my shortcoming.



What happened is that I - and SpinRite's first Alpha release - were wholly unprepared for the shocking myriad ways that today's and yesterday's highly distressed mass storage devices might and do fail.  Everyone's heard me mentioning for many months that nearly all of SpinRite's currently 683 registered development testers have mostly been bored.  Everything has always just worked for them.  That's why I released an Alpha, as I thought, okay, here we go.  It looks great.  But while SpinRite is valuable to those whose drives are still healthy - it's proven through several decades to be particularly adept at helping to keep drives that way - it really comes into its own when it's working to resurrect the data on drives that are far from healthy, for whatever cause, you know, it got dropped by mistake, or just got too old, or the power failed in the middle of it writing some data or something.



So I was so close to being able to declare this work finished when a final couple of unexpected surprises landed.  For example, though I have never seen this happen, a literal reading of the latest ATA-8 specification, which specifies the exact operation of mass storage drives, states that if an error occurs during a data transfer, the drive will abort the transfer and indicate the sector of the first error it encountered.  Well, right.  That's fine.  That's what you'd expect.  But it then goes on to say that nothing can be inferred about what data may have been transferred before that error occurred.  Now, as far as I know, all drives will transfer all of the data preceding the erroneous sector.  But the specification says they no longer need to.  And who knows what technology might be used in the future, or how far behind in transferring its data a drive might be when it decides to stop due to encountering an error.  And since SpinRite 6.1 obtains its breathtaking speed by transferring 16MB in a single request, that could be important.



So from a practical standpoint, this means that when an error occurs, to be safe, SpinRite needs to identify the trouble, then re-request all of the data up to, but not including, the trouble.  Get all that data settled, then deal with the sector that caused the transfer to halt.  That was the straw that finally broke the camel's back, as they say.  It wasn't just that.  I could have once again fixed that.  It was the "once again" part.  SpinRite's all-new core data recovery code, which I've been so pleased with, and which I have had every intention of moving directly from SpinRite 6.1 into SpinRite 7, had become a mess over the past six months as I had needed to keep poking and prodding it as we kept discovering new and surprising ways that drives could misbehave.



On top of that, SpinRite users had drives that were so badly damaged that they were hanging.  So they wanted to be able to interrupt SpinRite at virtually any point to immediately get out and try another drive.  SpinRite wasn't ever built to support that sort of emergency user escape because we weren't seeing that.



I was so close to being done that the last thing I wanted to do was to sacrifice that investment.  But my code had become so clogged up with special cases, exception testing, and early-out, you know, like bailout code from inner loops, that it started to feel brittle rather than robust.  So I scrapped it.



I recognized that the real investment that had been made was in what I had learned since that first Alpha-1 release.  So I settled down with my favorite text outliner, and I completely reconceived SpinRite's entire data recovery system.  I'm willing to confess this today because I completed that entire rewrite last Sunday night, two nights ago, and it's been a long time since I have been so happy with any code.  It is now immaculate.  All of the worker code has exactly zero tests for any special case exceptions.  I moved that handling into SpinRite's common IO abstraction.



There are two "top level" places in SpinRite during this data recovery, one where SpinRite is starting to work with a block of sectors, and the other where SpinRite is starting to work on a single sector.  Immediately upon entering the top of either of those worker routines, I checkpoint the system's stack by saving the current stack pointer in a static global variable.  Then, if anything happens that requires us to immediately surface from wherever we are, no matter how deep and how nested with routines we are, the IO abstraction system, through which all control passes on its way to and from the drivers, now has the ability to simply reset the stack to its previously saved state.  When it returns, that will have the effect of unwinding and discarding all current call stacks and local variable storage.  It immediately terminates anything that was going on, whatever it was.



That design expedient allowed me to have exactly zero tests or concerns for anything that might go sideways throughout the rest of the data recovery code.  Now, as a consequence, it is breathtakingly clean because the data recovery code no longer needs to test for anything.  And it now also adds support for the wacky ATA-8 spec in case drive designers ever do take the spec as literally as it says they could.



And there's a broader lesson here that has probably occurred to anyone who codes a lot.  There really is an art to coding.  And Leo, I know you appreciate this.  And the act of solving a complex problem teaches the coder so much about how to approach and solve the problem that it's really only after they finally have their program running that they understand how it should have been done.  



A couple hours ago, purely by coincidence, during MacBreak Weekly's pre-show, Alex Lindsay told the story of the four months he first took to solve a complex program in 3D graphics, which in this case was rotating a 3D pyramid on a Commodore 64.  This was obviously a while ago.



LEO:  That's a long time, yeah.



STEVE:  Shortly after he finally had it working, after four months, the machine crashed, and he lost all of his work.  You know, maybe it was the cassette tape that we used back then...



LEO:  Yeah.  [Sound effects]



STEVE:  ...to save and load our programs.  In any event, after recovering emotionally from the loss, Alex told the story of how he reimplemented what he originally had in just three weeks.



LEO:  And it was much better, probably, because that's - you know?



STEVE:  Exactly, that which had taken him four months to do the first time.  And, he said, it was only after that, that he then realized how it should be done.  So he rewrote it again in four days.



LEO:  Yeah.



STEVE:  And the way Alex expressed it was perfect.  He expressed it as three phases.  He said:  "Do it.  Then do it better.  Then do it right."  And he said that's like his mantra now.  That's the way he approaches things because he recognizes things that are hard, you know, the reason we're engaged by them is that they're hard.



LEO:  Right.



STEVE:  But they're not boring because we're learning from them.



LEO:  Right.  I love it.  That's one of the reasons I use Lisp -  I'm not recommending it, well, I mean, I would, but no one will ever do it - is because it has that kind of iterative style built into it.  It's a very natural way to work in Lisp, especially because of the REPL and the way the debugger works.  You can actually change a program while it's running.  So it's very easy to - it's a good tool for experiment.  This is how I write.  I'm not writing real code like you or Alex.  But I like to experiment and try stuff and write a first iteration that I know isn't going to be the final version.  And then you're not attached to it.  It's easier for you to kill it.



STEVE:  And you're able to get in.



LEO:  Exactly.



STEVE:  It's often just getting in.



LEO:  Just the first line is the hardest, yeah, yeah.  That's right, yeah.



STEVE:  So to wrap this up, in my present case, you know, in retrospect, because again, the other reason we code is we're like, okay, what happened?  How did this happen?  When that first little innocuous exception was needed, the quickest answer, which seemed right at the time, was to simply add a couple of tests for it.  And when it turned out to be needed in other places, I had already established a template for how to, quote, "solve" that need, in air quotes.  So I applied the same tests elsewhere.  Then when another type of exception emerged, I did the same since by then I'd established a precedent.



But after six months of that, my code had become a mess.  I knew how to solve it, which is what I just did.  But right up until the moment that I finally bit the bullet, it was more important to me to be almost finished.  The good news is that now I'm almost finished again, although the gang in the .dev group will have a lot of fresh virgin code to pound on while we verify that it's working exactly the way we want.



LEO:  "Almost finished," when a programmer says that, means something completely different.  I just want to point out.



STEVE:  Exactly.  And it's why, you know, my dear wife is saying, "So, honey, how much longer do you think?"  And I just say, you know, "I have no idea."  I would tell everybody if I did.  It's not like I'm holding out.



LEO:  Tell them what Michelangelo told Pope Julius when he was painting the Sistine Chapel.  "It's done when it's done.  Get out of here.  I'm working."



STEVE:  Right.  So anyway, this time I will have a beautiful legacy, which SpinRite 7 will be able to inherit from SpinRite 6.  And just as I was reading this, I realized that the other cool thing about this is that I have now sequestered all of that special case crap, that exception handling, in one location.  So if any additional, like, need for special casing occurs, it's got a happy place to live.



LEO:  Also a good idea, yeah.



STEVE:  And it won't ever muck up the rest of my code.



LEO:  Higher level languages I think kind of facilitate that a little bit better.  It's probably really tempting with assembly language to just do it when you think of it, stick it in there and it's all over the code base.



STEVE:  It's three bytes.  It's three bytes.



LEO:  But you're disciplined, and I'm sure that you'd do that.  But assembly language doesn't give you any - it's like bowling without bumpers.  You really can't - you could throw a gutter ball pretty easy.



STEVE:  Well, there is in high-level languages now, because the need for this has been recognized, it's called "structured exception handling," where you create a block of stuff where you know there may be a problem.  And if there is, then it's like without constantly having to test for it throughout your code, you've told it where you want to go in the event that that happens.  And that's a little bit like what this is, though as you said, in assembler I have the advantage of knowing the structure of the stack, knowing all of what's on it.



I did have to make sure that my code, there wouldn't be any side effects from just aborting, like at any time, the inner loops of my code makes a call for some IO with the drive which may never come back.  And that's essentially the power that I gave the IO abstraction layer is, if it sees a problem, it just discards all of the stack.  And when it returns from it, it goes back to the top level.



LEO:  Yeah, that's another thing Lisp does really well, what they call their common Lisp condition system, but it has exception handling built into it.  And it's very powerful.  It's a really - it's kind of, I mean, I'm sure that what they're doing in other languages is kind of inspired by it.  It's really quite amazing.



STEVE:  I would love to know whether McCarthy put that in in version 1.



LEO:  He did not.  No, he did not.  And that's what's - I look forward to the day when you don't have anything to do.  And then I have a friend who says, don't learn golf until I can teach you.  Don't learn Lisp until we can talk about it.  I can't teach you a thing.  But Lisp is kind of amazing.  So McCarthy was really taking symbolic logic and algebraic expressions and turned it into a programming language.



But the common Lisp now has evolved like 30 or 40 years of evolution since McCarthy thought of that in 1956 and is really, I think, where a lot of academic research and a lot of academic thought about what's the right way to write code kind of ended up in common Lisp and its successor Scheme.  And Racket's a very good example of that. But the common Lisp condition system is kind of famous for exactly that, taking all the errors and making sure that you can handle them.  And then you don't have to worry about errors because everything will be okay.  We'll see.



STEVE:  Okay.  Last week, I opened our look at the forthcoming standardization of consumer location tracking technology by writing:  "It seems that any powerful new technology gets used for both the benefit and detriment of society.  In other words, it's a mixed blessing.  And so is the case for AirTags, those popular and handy Bluetooth LE (Low Energy) dongles that are all about their location."



Now, as for popularity, I should note that Apple announced their $29 tags two years ago last month in April of 2021.  So in the past two years, they alone have sold more than $1 billion dollars worth of AirTags, amounting to around 55 million of those little buggers.  And Tile, another tracking tag company, told Wired magazine last year that they had sold about 40 million of their popular little devices.  So we know that consumers love the concept; and, when tightly integrated with smartphones, they also love their ease of use.  A single Apple account can be associated with up to 16 AirTags.



As our listeners also know, we ended last week with the controversial tidbit that the forthcoming "Detecting Unwanted Location Trackers" IETF specification requires that all qualifying devices be registered in an online database that could be queried by law enforcement.  Leo's immediate and certainly understandable reaction was that this would be a deal breaker for the technology.  I then posed this question over in GRC's Security Now! Newsgroup to gauge the feelings of those there, and many people felt the same way, often quite vocally.



I would argue that nothing is going to stop the sales of these handy little things for use in the legal and ethical tracking of people's own property.  I doubt that anyone using them to keep track of their car keys or backpack will care.  And I believe that the specter of a registration database - actually, I hope that the specter of a registration database - that can be queried by law enforcement when probable cause has been established is what's needed to help curtail the abuse of what really is a very powerful consumer technology.



So I think it's perfect that Leo and I may have differing feelings about this and that we'll be able to discuss the pros and cons which will make for a more interesting and balanced podcast for our listeners.  It's already clear that the issues are not entirely black and white.  But one thing that did arise from my posting into GRC's newsgroup was that there was significant mis-presumption about many aspects of this technology.  That's understandable since this technology has not yet been elucidated.  Which is why we're here today.



One example of mis-presumption is that people assumed that Apple themselves would be aware of the location of everyone's AirTags and could track them.  The technology that Apple has deliberately designed makes that explicitly impossible.  And it has always been possible for anyone discovering an AirTag to directly query the device to obtain the last four digits of its owner's phone number.  This is done to help people determine whether a Tag they may discover might belong to someone obvious, who they know.



Now, I recognize that querying an AirTag you have found is very different from some evil database in the sky.  We'll get back to that by the end of our technical walkthrough.  To me, and I may be alone in this, which is fine, the idea that casual consumers should be entitled to the absolute private tracking of others, which is illegal in many jurisdictions, seems against society's best interests.



Okay. Before we get any deeper into the pros and cons, let's learn how this technology operates and what it means for users and abusers.  Once I had fully absorbed this detailed 22-page specification, which I mentioned was a hodgepodge, I mean, it is - there's typos.  There's grammatical errors.  I was wishing that I had an editor open on it rather than just reading a PDF because...



LEO:  That's kind of discouraging.



STEVE:  Yeah, I mean, it's rough, and all out of sequence.  You're having to, like, wait a minute, why are we talking about that here when it should go over there?  So, you know, let's hope they get this fixed.  But more importantly, I was very disappointed by what was missing from it.  I was hoping that we were going to get a full working technical specification for the operation of Bluetooth Low Energy trackers with all of the crypto and the other details spelled out.  I wanted that because I wanted to be able to present a detailed walkthrough of that level of the technology which Apple has developed to ensure the privacy of the various parties.  Instead, what we have is a small subset of that whole.  And I've been unable to locate anything more complete.  I don't think that's been published yet.



This document does, however, provide us with the common behavior that's required from small tracking devices to enforce their owners' privacy and the privacy of anyone they may be near.  There's still plenty of interesting stuff to talk about, but we're going to need Apple or Google or someone to publish another specification to fully satisfy our curiosity.



What today's specification does is outline how location tracker accountability is created.  And it's also about enforcing the privacy of the users of location trackers.  In other words, users should not misuse tracking power, and there should be fair accountability if they do.  But neither should their legal tracking in any way compromise the privacy of what they are tracking.  As I got more deeply into this, and brought myself up to speed about the features and operation of Apple's current AirTag technology, that is, what we've been using for the last two years, it appeared that most of this has already been implemented by Apple.



So this began to feel more like Apple finally taking the covers off of some of the existing AirTag technology that they had already developed as a means of encouraging industry-wide standardization.  For all of these low-power, short-range Bluetooth and NFC tags, this is important since, in this entire model, "location" is determined by others in a crowd-sourcing model.



Whereas fancy and expensive, technically you need to have a cellular account which costs something every month, traditional GPS-style trackers, they use both GPS satellite positioning and built-in cellular telephony for reporting back to headquarters.  Compared to that, all of these little Bluetooth Low Energy tags, you know, BLE tags, rely upon having their periodic broadcasts picked up and their position relayed by other bystanders' smartphones.  This means that enlisting all of Android and potentially all other devices into one big happy crowd-sourced family - pardon me.  I've been suppressing a cough there.



LEO:  Let it go.  Let it all out.



STEVE:  This means that enlisting all Android and potentially other devices into one big happy crowd-sourced family will significantly improve the tracking experience and responses for everyone.  One New York Times reporter, writing last year about this exploding industry in consumer tracking devices, planted multiple sets of three different types of trackers on her husband with his full consent and knowledge.



And she noted that, while he was near their sparsely populated home, she didn't get much updating on his position with any of the non-GPS trackers.  But the moment he ventured into "the city," presumably New York, the tracking information available exploded and became absolutely real-time.  That was due to the fact that so many people were carrying iPhones which were autonomously pinging the AirTag he was carrying, or rather it was pinging them, and they were all simultaneously reporting its location as being near to them at that moment.  And since iPhones are adept at knowing where they are, the tag's location could be inferred from that.



For those who are not familiar with RFC-style terminology, I should preface what comes next by noting that the spec in RFC style uses all-caps qualifiers so that its reader understands how various requirements should be taken.  Specifically, the capitalized phrases used are "MUST" - whose implication is obvious - "MUST NOT," "REQUIRED," "SHALL," "SHALL NOT," "SHOULD," "SHOULD NOT," "RECOMMENDED," "NOT RECOMMENDED," "MAY," and "OPTIONAL."  So keep that in mind.



And with that in mind, it was interesting to see the section on Applicability where the specification states that these best practices are REQUIRED (in all caps) for location-enabled accessories that are small and would not be easily discoverable on their own in the world.  For large accessories, such as a bicycle, these best practices were merely RECOMMENDED, as opposed to REQUIRED.  Accessories are considered easily discoverable, and thus recommended but not required, if they meet one of the following three criteria:  The item is larger than 30 centimeters in at least one dimension.  Okay, so that's about a foot.  If any dimension is larger than a foot, it's recommended but not required.  Or the item is larger than 18x13 centimeters in two dimensions.  That's about 5x7 inches.  Or the item is larger than 250 cubic centimeters in three-dimensional space.  If it was a cube, that would be 6.3 centimeters on a side, or about 2.5 inches on a side.



LEO:  So if it's the size of a refrigerator, you don't have to do it.



STEVE:  If it's bigger than a breadbox, Leo, that's right.



LEO:  And that's because everybody knows it's there.



STEVE:  That's right.  It's not going to, you know, it's like...



LEO:  Not hiding.



STEVE:  Why is R2D2 following me?  Well, at least you know it is.



LEO:  Yeah.



STEVE:  So, you know, this detecting unwanted tracking specification is meant to protect from itty-bitty trackers, you know, like Apple's AirTags, which are disks 2.25 inches in diameter by about .3 inches thick.  As I said, if it's bigger than a breadbox, it doesn't need to rigidly follow the specification.



One of the people who responded to my GRC posting noted that he had a pair of very nice electric bikes stolen from their garage, and he was wishing that they had been trackable.  With the rapidly growing popularity of this technology, I think it's foreseeable that such high value, desirable, and inherently mobile objects, such as a very nice bicycle, will incorporate tracking as a sales feature.



LEO:  Oh, there already are for ebikes, yeah.



STEVE:  Exactly.  Wondering whether that had already been done... 



LEO:  It has.



STEVE:  A quick google revealed a story from just last month which was titled "Why a Bike with Built-in Find My Capabilities Is Total Genius."  



LEO:  Apple showed it, I think, when they showed off AirTags.  They were talking about third-party uses of this technology.



STEVE:  Yes, yes.  So, and the article had three bullet points.  It said:  "Velotric's" - bad name, Velotrics.



LEO:  I think it's Velotric's.



STEVE:  Okay, that's somewhat better.



LEO:  Because bicycles were originally called Velopeds.  So it's an electric Veloped.



STEVE:  Velopeds and Velotric's.  Got it.



LEO:  Yeah, electric Veloped.



STEVE:  Yeah.  Anyway, so it's their new...



LEO:  You sound skeptical.



STEVE:  Their new - I don't.  I was calling that router the MikroTik router for a long time.



LEO:  Yeah, for a long time, yeah.



STEVE:  I don't think they appreciate that.



LEO:  I don't know how they pronounce it.



STEVE:  MikroTik's sounds like something you take medicine for.



LEO:  Yeah.



STEVE:  No, not good.  So anyway, this thing is the Thunder 1, the ST ebike, has a built-in AirTag feature, says you can track your bike just like you can track your iPhone and your AirPods.  And finally it said:  "Find My is potentially more private and much cheaper than a dedicated cellular GPS tracker."  And to that I would add that having it built in, it can run off the ebike's battery, so avoiding the annual otherwise needed battery replacement.  And more importantly, assuming that the technology is integrated into the ebike's electronics, it cannot be discovered and removed.



The final point is that hopefully the ebike's console will prominently advertise the fact that it incorporates built-in antitheft AirTag tracking so that the lazy thief who wants a free ride will think twice and just walk away instead.  So there is every indication that for such items, especially when they become cross-platform for compatibility with both iOS and Android, that we're going to begin seeing AirTag tracking becoming a competitive marketing feature for these kinds of devices where a little bit of incremental price or cost increase justifies the cost.



Okay.  So moving on.  I want to next share some formal definitions which the specification uses.  It'll help in understanding what comes later, although I'm going to also be deobfuscating this as we go.  So this is more than just defining terms since a lot of reading between the lines is also possible.  So the spec says:  "Throughout this document, these terms have specific meanings."  The term "platform," which we sort of have a sense for, in this case, in this document, it's used to refer to mobile device hardware and associated operating systems like phones, tablets, and laptops.  So the platform is what we're carrying in our pockets, not the AirTag, which they insist on calling an "accessory."



The term "owner device" is a device that is paired to the accessory and can retrieve the accessory's location.  You know, so a smartphone.  Then the term "non-owner device" refers to a device that may connect to an accessory, but is not an owner device of that accessory.  The term "location-tracking accessory" refers to any accessory that has location-tracking capabilities, including but not limited to crowd-sourced location, GPSS, WiFi, cell tower, et cetera, and provides the location information back to the owner of the accessory via Internet, cellular connection, and so forth.  So that sort of just broadly location-enabled tracking accessories, non-AirTag-ish things, and also of course AirTags.



The term "location-enabled state" refers to the state an accessory is in where its location can be remotely viewed by its owner.  The term "location-enabled advertisement payload" refers to the Bluetooth advertisement payload that is advertised, which is the term, you know, broadcast, when an accessory has recently, is currently, or will in the future provide location updates to its owner.  And I have no idea how it knows it will be able to in the future, but okay.  The term "unwanted tracking" refers to undesired tracking of a person, their property, or their belongings by a location-enabled accessory.  The term "unwanted tracking detection" refers to the algorithms that detect the presence of an unknown accessory traveling with a person over time.



"Unwanted tracking alert" refers to notifying the user of the presence of an unrecognized accessory that may be traveling with them over time and allows them to take various actions, including playing a sound on the accessory if it's in Bluetooth Low Energy range.  And there's a lot more we'll be getting to.  And finally, the term "platform-compatible method" refers to a method of communication between the platform, which, you know, is the thing we're holding in our hand, our pad or whatever, and the accessory/accessory manufacturers to exchange information, including but not limited to Bluetooth, advertisements, HTTP, and so forth.  So that's sort of a dumb term.  I don't know why it's even there, but it is.



Okay.  So I find it awkward to refer to tags as "accessories," which is what the spec does.  So I will tend to use the simpler and clearer term "tag."  But when I'm quoting the specification I'll use its language of referring to these tags which are "accessories."



So tags can be in one of two major modes, and this affects their behavior.  They can be in so-called Near-Owner Mode or Separated Mode.  And the tag knows.  Each tag periodically emits a broadcast hoping to be heard by some passing smartphone or Bluetooth-enabled device.  And later we'll see that by "periodically" we mean every half second to two seconds.  So these things are quite chatty.  If the tag is within range of the owner's smartphone, and if actually any of the owner's multiple devices that are on the same account, with which it has previously been paired, the owner's smartphone notifies the tag that its owner is nearby.  So that's what places the tag into its Near-Owner Mode is it does a ping, and the owner's device says yeah, you know, you're here.  That, among other things, suppresses its location broadcasts.



The specification states:  "The accessory SHALL" - again, all caps, SHALL - "transition from Separated to Near-Owner mode if it has reunited with the owner device for a duration no longer than 30 minutes."  But I suspect this transition is typically immediate because there's no reason for it not to be.  The specification is likely leaving some latitude there.  "Conversely," the specification states, "the accessory SHALL transition from Near-Owner mode to Separated Mode if it has been physically separated from the owner device for a duration no longer than 30 minutes."  Okay, now, since BLE's range is limited, it makes sense for a tracking device to not immediately flip into Separated Mode.  But this says that it must do so within 30 minutes of being away from its owner.



One of the definitions we just noted above was for "location-enabled state."  The specification further explains that by saying:  "The accessory SHALL maintain an internal state that determines when its location is, or has been, available to the owner via a network.  This state is called 'location-enabled state.'  Misuse of location-enabled accessories can occur when the owner's device is not physically with the accessory.  Thereby, the accessory SHOULD maintain a second internal state, denoted the 'Near-Owner state'" - which is being a little redundant here - "which indicates if the accessory is connected to or nearby one or more of the owner's devices.  Near-Owner state can take two values" - you know, true or false - "either Near-Owner Mode or Separated Mode.  Near-Owner Mode is denoted as the opposite of Separated Mode."  They're being very clear about this.



When the device is in "location-enabled state," it is broadcasting a payload containing the most recent location it has been able to obtain.  This payload is, not surprisingly, called the "location-enabled payload," and the spec says:  "It is RECOMMENDED that the location-enabled payload is only advertised when the accessory is in its Separated state.  The reasoning behind this recommendation is that unwanted tracking detection relies on the Bluetooth Low Energy advertisements emitted while in the location-enabled state to determine if an unknown accessory is traveling with someone who is not the owner.  If the location-enabled payload is advertised only in the Separated state, that minimizes false-positive unwanted tracking alerts."



Okay, well, now, that's sort of obvious.  The location-enabled payload contains the most recent location information that the accessory tag was able to obtain from some nearby smartphone, which is what let it know where it was.  So it would make no sense for a tag to be broadcasting its location when it's nearby and therefore in its Near-Owner Mode.  The spec adds:  "The accessory SHALL broadcast the location-enabled advertisement payload if the location is available to the owner or was available any time within the past 24 hours.  This allows unwanted tracking detection to operate both between and beyond the specific moments an accessory's location is made available to the owner.



It's this so-called "location-enabled advertisement" that is the primary indication of tracking.  That's the Bluetooth advertisement that sets off alarms in nearby phones.  The idea being that the accessory tag has somehow obtained a location fix within at least the past 24 hours.  And being a good little tracker, it's hoping to find someone who will dutifully relay its location back to its owner.  In other words, this is tracking.  It's not necessarily unwanted tracking, but it's definitely some form of tracking.



This should be disabled when the device is with its owner, since it makes no sense to be broadcasting its location in that situation.  The false positive unwanted tracking alerts referred to earlier, that are being eliminated by the suppression of location-enabled advertisements, occurs, for example, if two or more people, each having smartphones, are together, and somebody also has a tag somewhere that belongs to one of them.  If they're all traveling together, the tag is also traveling with the non-owners.  So if that tag were to be emitting its location-enabled advertisements, then the non-owners' phones would believe that they were being tracked and would generate false positive alerts.  So the fact that you have the proximity of the owner placing the tag into its Near-Owner mode, and thus suppressing these overt tracking announcements, is a good thing.  And that's where you get your false-positive suppression.



And speaking of these Bluetooth Low Energy advertisements, as I mentioned before, they are quite frequent.  The specification's "advertising policy" formally specifies an announcement interval  of between half a second and two seconds.  So, you know, lots of this happening.  Bluetooth Low Energy devices have MAC addresses, and therein lies a problem.  In the spec, we're wanting to prevent the tag from being tracked by an adversary.  So on one hand, there's not having the tag being with someone whose owner is using it to track.  But then there's also just the tracking of the tag itself.  That wants to be prevented.  So here's what the spec has to say about that.



"The Bluetooth Low Energy advertisement payload SHALL contain a resolvable and private address for the accessory, which is the 6-byte Bluetooth LE MAC address.  The address MUST be private, and it MUST rotate periodically and be unlinkable.  Otherwise, if the same address is used for long periods of time, an adversary may be able to track a legitimate person who is carrying the accessory.  A rotation policy aims to reduce this risk.



A general approach to generate addresses meeting this requirement is to construct them using a pseudorandom function, taking as input a secret key established during the pairing of the accessory and either a counter or a coarse notion of time.  The counter or coarse notion of time allows for the address to change periodically.  The secret key allows the owner devices to predict the sequence of addresses for the purpose of recognizing its paired accessories.



"An accessory SHALL rotate its resolvable and private address on any transition from Near-Owner state to Separated state, as well as any transition from Separated state to Near-Owner state."  So, like, as soon as it realizes it's no longer with its owner, bang.  Immediately the MAC address changes.  And similarly, as soon as it is back within the region of its owner, the MAC address changes again.  But it also changes autonomously.  When in near-owner state, the accessory SHALL rotate its resolvable and private address every 15 minutes.  This is a privacy consideration to deter tracking of the accessory by non-owners when it is in physical proximity to the owner.  Since it is nearby, the owner device is able to maintain synchronization so that it's able to recognize and remain paired with its known accessories.



"When in a Separated state, the accessory SHALL rotate its resolvable and private address every 24 hours.  This duration allows a platform's unwanted tracking algorithms to detect that the same accessory is in proximity with it for some period of time, when the owner is not in the tag's or accessory's physical proximity."



So this seems well thought out.  Devices rotate their 8-byte MAC address on a schedule as directed by a secret key.  This is exactly analogous to the TOTP one-time passwords many of us are using today, though with six 8-bit bytes rather than only 6 digits.  By knowing each device's secret key, all future MAC addresses can be determined, and no one tracking a device based on its periodic broadcasts will be able to determine any tag's next MAC address.



It's worth noting that the devices have no actual native MAC addresses.  Unlike the anti-tracking technology we've developed for smartphone WiFi, there is not one rotating spoofed MAC address used when roaming and another actual physical fixed MAC address used when associated or paired with a WiFi home base or access point.  So these tags are all simply using the traditional 8-byte, 48-bit MAC as a short-term rotating ID.  And there's a galaxy of them, you know, a hundred million of them out there now and growing, all occasionally changing their identifier following a predictable pattern that only their owner knows.  Another intriguing aspect of this specification is what non-owned devices, that is, the tags that are in their Separated state, are required to do to reveal themselves.  The spec calls this "Non-Owner Finding," and it has a number of components.



The spec says:  "Once a user has been notified of an unknown accessory traveling with them, it is REQUIRED that they have the means to physically locate the accessory.  This is called non-owner finding of the accessory.  These capabilities are both REQUIRED and RECOMMENDED and reflect hardware to be incorporated into the accessory to enable non-owner finding."  And now we'll explain it.



"Motion detection:  The accessory SHOULD include a motion detector that can detect accessory motion reliably, for example, an accelerometer.  If the accessory includes an accelerometer, it MUST [all caps] be configured to detect a change in orientation of plus or minus 10 degrees along any two axes of the accessory.  After some number of hours between eight and 24, chosen randomly from a uniform distribution" - so that would be an average of 16 hours, so no fewer than eight, no more than 24, but an average of 16 - "the accessory being away from its owner and in a Separated state, the accessory's motion detector will be enabled.  While the motion detector is enabled, it must be able to detect motion within 10 seconds."  Okay, so it's sampling its position at 10-second intervals, presumably to conserve power.  The spec says:  "If motion is not detected within the 10-second period, the accessory MUST stay in this state until it exits Separated state."



Okay.  So accessory leaves the owner.  After some time between eight and 24 hours from then, the motion detector is activated.  And at 10-second intervals, it starts sampling its angular position in space.  The spec says:  "If motion is detected within the 10 seconds between samples, the accessory MUST play a sound.  After any motion is detected, the movement detection period is decreased from 10 seconds to half a second."  Okay, so it starts sampling much more quickly after first detecting any motion.  "The accessory MUST continue to play a sound for every detected motion.  The accessory SHALL disable the motion detector for six hours under either of the following two conditions:  Motion has been detected for 20 seconds at the half-a-second sampling rate, or 10 sounds have been played."



Okay, so after this thing has become motion sensitive, which after an average of 16 hours of being aware from its owner, then any motion of more than 10 degrees on two axes will cause it to make a sound, and will also cause it to shorten its sampling to half a second, and to continue making sounds if it continues being moved, until either 20 seconds have passed or 10 sounds have been played.  After that point it then goes into a six-hour, they call it a "back off," so it goes to sleep for six hours.  



The spec says:  "If the accessory is still in its Separated state at the end of the six-hour back off" - which has gone silent, essentially, it's not going to just keep beeping or screaming, whatever it's going to be doing - "the unwanted tracking behavior MUST restart."  So again, after six hours of quiet time, if it is again moved, it starts making sounds.  "Any Bluetooth LE connection from a paired device MUST reset the Separated behavior and transition the accessory to connected state."  In other words, the instant it's back with its owner, then it's paired, and it shuts this down, and it goes back into near-owner state.



And finally they said:  "The accessory MUST include a sound maker, for example, a speaker of some kind, to play sound when in Separated state, either periodically or when motion is detected.  It MUST also play sound when a non-owner tries to locate the accessory by initiating a play sound command from a non-owner device when the accessory is in range and connectable through Bluetooth LE.  The sound must be loud, and the sound must be played for a minimum of five seconds each time."  And in the spec it goes into a detailed specification of the measuring of loudness the devices are able to have.



Okay.  So this gives us a system where, after a tracking device has been separated from its owner for an unknown interval of time, randomly chosen between eight and 24 hours, its motion detector activates.  At that point it begins taking readings of its angular orientation in three space every ten seconds; and if it finds that it's been rotated by more than 10 degrees through any two axes between successive position samples, it will emit a clearly audible sound for five seconds.  And if any qualifying movement is detected within those 10 second intervals, essentially that will have roused it so that it will start sampling twice per second to allow it to make its noise much more responsively to someone who may be attempting to discover its location by moving things around.



All of this occurs whenever a tag is separated from its owner for between eight and 24 hours.  It's about requiring anything that is small that can track to deliberately reveal itself periodically, and in doing so using simple sound which does not require any technology where the tag is located.



Note that a lot of attention has been given to detecting unwanted tracking tags with another smartphone.  But not everyone has a phone that's smart enough to do so today.  Today, unless that AirTag Apple app is launched and running on an Android phone, no Android-carrying user would be able to detect an unwanted nearby tag.  This spec will be changing that soon for new Android devices.  But we know that there will remain many non-upgraded Android devices in use for years, and there are also non-smart cellular phones.



I was recently listening to a talking head on some show suggesting that one way to keep young people away from the perils of social media would be to equip them with only a dumb phone capable of making and receiving telephone calls, texting, and taking pictures with its camera.  Now, I don't know whether this person has ever actually been around any young kids, but that would be a tough sell when they're surrounded by their peers who are gleefully deep into the social Internet.  So good luck with that.  But still, the point being not everybody is carrying a cell phone.



So there's clearly a need to expose trackers through some low-tech means, and sound is the obvious choice.  This is not what someone wishing to track someone stealthily would choose, right, because the device is going to give itself away.  After that initial period of eight to 24 hours of separation from all of its owner's devices, any tag that remains separated will generate attention-getting sounds whenever it's significantly moved.  And once it has done so 10 times or for 20 seconds of movement in its faster sampling mode, it will go quiet for six hours, after which it will again reawaken and notify of any movement.



The obvious weakness in this system is that tracking requires radio, but not sound.  So arranging to enclose a tag inside some sort of acoustic suppression container which is transparent to radio might defeat the tag's audible "Help me! I've become separated from my owner!" cries for help.  In addition to physical movement which will trigger sounding, any nearby device within radio, you know, Bluetooth LE radio range of a tag, whether a tag's owner or non-owner, is able to remotely command any tag to emit its sound.  So if, for example, a suspected tracker is detected, the detecting smartphone's user interface for managing tags can request any unseen tags to sound off to aid in determining their location.



Tags also contain a wealth of queryable information.  This includes a unique 8-byte token, a UUID, that serves as a unique identifier for the accessory make and model.  The 8-byte value will be listed in a public registry so that the tag's issuing company can be determined.  Tags also contain the manufacturer's name in an up to 64-byte field, so plenty of length.  And also another 64 bytes for the model's name, so that can be made clear.  There's also an 8-byte accessory category indicator.  Only the first single byte of the eight bytes is presently defined, though eight have been set aside; and many of the values of that byte are already defined.



To give you a sense for how the publishers expect these tags are going to be used, the enumerations of the one byte that's been defined are things like a generic Finder.  Then there's also Other, also Luggage, Backpack, Jacket, Coat, Shoes, Bike, Scooter, Stroller, Wheelchair, Boat, Helmet, Skateboard, Skis, Snowboard, Surfboard, Camera, Laptop, Watch, Flash Drive, Drone, Headphones, Earphones, Inhaler, Sunglasses, Handbag, Wallet, Umbrella, Water Bottle, Tools or Tool Box, Keys, Smart Case, Remote, Hat, Motorbike, Consumer Electronic Vehicle, Apparel, Transportation Device, Sports Equipment, and Personal Item.



LEO:  Well done.  You just passed our cognitive test.  You do not have dementia, Steve.  Congratulations.



STEVE:  And had I been able to do that from memory, Leo, I would agree with you.  So anyway, that gives us some sense for what the people behind this are thinking about the future and about the wide potential for this location tracking technology.  I suspect that there's every chance that, as I said, that many higher end consumer products like that ebike, which are prone to misplacement, loss, or theft, or which might need to be located with some urgency, such as an inhaler, may eventually incorporate this consumer location technology as a sales feature.



The last item of the queryable data is a 4-byte, 32-bit value which enumerates a tag's capabilities.  Only four bits are currently defined.  Those are supports sound, supports motion detection, supports serial number lookup by Near Field, and supports serial number lookup by Bluetooth.  There's also some interesting specification about deliberate disablement.  The spec says:  "The accessory SHALL have a way to be disabled such that its future locations cannot be seen by its owner.  Disablement SHALL be done via some physical action, for example, a button press, a gesture, or the removal of the battery.



"The accessory manufacturer SHALL provide both a text description of how to disable the accessory, as well as a visual depiction, for example, image, diagram, animation, et cetera, that MUST be available when the platform, you know, the thing that, you know, the smartphone, is online and OPTIONALLY when offline.  Disablement procedure or instructions CAN change with accessory firmware updates.



"A registry which maps product data to an affiliated URL supporting retrieval of disablement instructions SHALL be available for platforms for reference."  You know, again, remember that "platforms" in this spec refers to smartphones, pads, and similar devices with full user interfaces.  Then they said:  "This URL must return a response which can be rendered by an HTML view."



So this says that, if someone discovers an unwanted tracking device, that 8-byte registered product ID data, which can always be retrieved directly by querying the device over Bluetooth, will, among other things, point to a URL which returns HTML that any smartphone can render to obtain clear and updated instructions from the device's manufacturer about how to manually disable the tracker.  And this is all in, you know, capital MUSTS.  But this always requires physical access to the device.  At least at this point in the evolution of the specification, it cannot be done remotely over radio.  It's foreseeable that this might be allowed over NFC, since that requires essentially physical proximity, and it might be simpler.  As we'll see, the spec does makes clear distinctions about what can be done via NFC and what can only be done over Bluetooth.  Or rather Bluetooth and only under NFC.



All devices are also serialized.  The spec explains:  "The serial number SHALL be printed and be easily accessible on the accessory.  The serial number MUST be unique for each product ID.  The serial number payload SHALL be readable either through NFC tap or Bluetooth LE."  So again, that can be done at a distance.  "The serial number payload SHALL be readable either through NFC tap or Bluetooth LE."  Note that this does not mean the serial number itself.  I'll get to that in a second.



They said:  "For privacy reasons" - oh, they're going to say it.  "For privacy reasons, accessories that support serial number retrieval over Bluetooth LE MUST have a physical mechanism, for example, a button, that SHALL be required to enable the remote 'Get_Serial_Number' opcode command.  The accessory manufacturer SHALL provide both a text description of how to enable serial number retrieval over Bluetooth LE, as well as a visual depiction - again, image, diagram, animation, et cetera - that MUST be available when the platform is online and OPTIONALLY when offline.  The description and visual depiction CAN change with accessory firmware updates.  A registry which maps Product Data to an affiliated URL that will return a text description and visual depiction of how to enable serial number lookup over Bluetooth LE SHALL be available for platforms for reference.  This URL MUST return a response which can be rendered by HTML view."



So we have the same set of requirements for enabling the retrieval of a device's serial number remotely over Bluetooth as we do for disabling the device.  In both cases, some sort of physical action must be taken with the device by an individual to prove that it is in their physical possession - either to disable it and/or to learn its immutable serial number.  It's obviously required for the serial number since, unlike its MAC address, which is nothing but a keyed pseudorandom sequence, the serial number never changes.  And if it were available remotely without contact with the device, it could be used for long-term tracking.  But as I noted before, NFC, which is, as its name suggests, a Near Field contact technology, is allowed greater latitude.



The spec says:  "For those accessories that support serial number retrieval over NFC, a paired accessory SHALL advertise a URL with parameters.  This URL SHALL decrypt the serial number payload and return the serial number of the accessory in a form that can be rendered in the platform's HTML view."



Okay.  So first of all, NFC.  If the tag and user device both support NFC, then obtaining the device's serial number is as simple as tapping the smartphone against the tag which you have in your possession to query it.  But we also just encountered the phrase "decrypt the serial number payload."  Okay.  So here's what that's about: 



"The 'Get_Serial_Number' opcode is used to retrieve serial number lookup payload over Bluetooth LE or NFC.  This MUST be enabled for five minutes upon user action on the accessory," you know, the button press for, for example, 10 seconds to initiate serial number read state.  "When the accessory is in this mode, it MUST respond with 'Get_Serial_Number_Response' opcode and Serial Number Payload operand.  For security reasons, the serial number payload returned from an accessory in the paired state SHALL be encrypted.  A registry which maps Product Data to an affiliated URL which will decrypt the serial number payload and return the serial number SHALL be available for platforms to reference.  This URL MUST return a response which can be rendered by an HTML view."



So this tells us that the data contained within the public registry is encrypted, and that only the tag device itself contains the information required to decrypt its own publicly stored serial number.  When a user encounters a tag that's in its Separated state, and either presses a button on the tag or taps it for a Near Field exchange, that smartphone receives a URL from the tag which contains the address of the lookup and also the decryption keying information needed to decrypt the publicly stored and encrypted serial number.  Thus only someone who is in physical possession of a tag may use the tag to obtain its own serial number electronically.



But remember that the serial number must also be physically printed on the device.  This suggests that this electronic fallback is provided in case some would-be malicious tracker person thinks they're getting very clever by sanding off the device's exposed and obvious serial number to make it anonymous.  Well, that won't work.



Since the serial number is an important piece of the whole, the spec further explains:  "Serial number lookup is required to display important information to users who encounter an unwanted tracking notification.  It helps them tie the notification to a specific physical device and recognize the accessory as belonging to a friend or relative.  However, the serial number is unique and stable, and the available partial user information can further make the accessory identifiable."  And we'll get to that in a second.  This spec has a lot of, as I said, out-of-sequence content.



The spec says:  "Therefore, the serial number SHOULD NOT be directly available to any requesting devices.  Instead, several security and privacy preserving steps SHOULD be employed."  It says:  "The serial number lookup SHALL only be available in Separated mode for a previously paired accessory.  When requested through any long-range wireless interface like Bluetooth, a user action MUST be required for the requesting device to access the serial number.  Over NFC, it MAY be acceptable to consider the close proximity as intent for this flow.



"To uphold privacy and anti-tracking features like the Bluetooth MAC address randomization, the accessory MUST only provide non-identifiable data to non-owner requesting devices.  One approach" - oh, and by the way, it doesn't want to be repeatable, either; right?  Because that would also be like having something that's not changing.  So whatever it is that the device provides has got to be changing.  "One approach,"  they say, "is for the accessory to provide encrypted and unlinkable information that only the accessory network service can decrypt.  With this approach, the server can employ techniques such as rate limiting and anti-fraud to limit access to the serial number.  In addition to being encrypted and unlinkable, the encrypted payload provided by the accessory SHOULD be authenticated and protected against replay.



"The replay protection is to prevent an adversary using a payload captured once to monitor changes to the partial information associated with the accessory, while the authentication prevents an adversary from impersonating any accessory from a single payload.  One way to design this lookup encryption," they wrote, "is for the accessory to contain a public key for the accessory network server.  For every request received by a device nearby, the accessory would use the public key and a public key encryption scheme to encrypt a set of fields including the serial number, a monotonic counter or a one-time token, and a signature covering both the serial number and counter or token.



"The signature can be either a public key signature or symmetric signature, leveraging a key trusted by the network server which MAY be established at manufacturing time or when the user sets up the accessory.  Some additional non-identifiable metadata MAY be sent along with this encrypted payload, allowing the requesting device to determine which accessory network service to connect to for decryption, and for the service to know which decryption key and protocol to use."



So possession of the tag, which would usually allow someone to simply look at its serial number, is essentially duplicated electronically.  The tag won't directly inform even a nearby device of its serial number.  But it will provide a URL, once a button is pressed or Near Field has been used, with embedded decryption data that such a device can then use to query and then obtain the device's true plaintext serial number.  Again, it was also printed on there, so it's not a big deal.  But this allows it to be obtained electronically.  By asking a remote service for this information, its disclosure can be controlled.  And that's a cool part, too; right?  So since someone is having to query a server using a URL that provides the data, you can do rate limiting, and you can do various anti-fraud things.



Okay.  And so now we get to the so-called "pairing registry," where we ended our truncated discussion last week.  To reiterate against the background of what we now understand, the spec says:  "Verifiable identity information of the owner of an accessory at time of pairing SHALL be recorded and associated with the serial number of the accessory, for example, phone number, email address, and so forth."



Okay.  So the process of beginning to use a new tracking tag is to pair it with the user's so-called "platform" device, you know, phone, a tablet, whatever.  And most users will, in turn, have their real-world identity and billing account information associated with their device.  So this pairing associates the tag's globally unique serial number with the user's real-world identity.  The point of this being to create an accountability link from the tag to the real-world user with whom the tag has been paired.



Later, under the heading "Persistence," the spec says:  "The pairing registry SHOULD be stored for a minimum of 25 days after an owner has unpaired an accessory.  After the elapsed period, the data SHOULD be deleted.



We covered in detail the steps required to obtain, decrypt, and display a device's serial number.  But when that serial number is displayed, so too is some deliberately obfuscated owner information that's obtained from the pairing registry.  In other words, when you go through getting the serial number electronically, you also get some obfuscated information from the pairing registry.  The spec says:  "A limited amount of obfuscated owner information from the pairing registry SHALL be made available to the platform" - the user's device - "along with a retrieved serial number.  This information SHALL be part of the response of the serial number retrieval from a server which can be rendered in a platform's HTML view.



"This MUST include at least one of the following:  the last four digits of the owner's telephone number," you know, asterisks for everything except -5555 in their example; "or an email address where the first letter of the username and the first letter of the domain name are visible, as well as the entire top level domain."  So as we said last week, s****@g**.com, if my email address were steve@grc.com, which it hasn't been for several - for quite a while.



LEO:  So don't send him email at that address.



STEVE:  Yeah, that won't help.  Okay.  Elsewhere, under the heading "Privacy Considerations," the spec says:  "In many circumstances when unwanted tracking occurs, the individual being tracked knows the owner of the location tracker.  By allowing the retrieval of an obfuscated email or phone number when in possession of the accessory, this provides the potential victim with some level of information on the owner, while balancing the privacy of accessory owners in the arbitrary situations where they have become separated from those accessories.



So, you know, the idea being that someone who might immediately react with, like, "Who the 'F' is tracking me," this allows the individual to immediately obtain some information.  It's just a matter of pressing a button on your phone.  The phone will get the serial number, make the query, the remote server decrypts it, returns it in HTML to the phone, and you will get the serial number and some information that hopefully provides you with some relief.  It's like, oh, that's where that tracker went or whatever.  So it may often be someone that you know or can recognize.



So we have a pairing registry which associates the real-world identity of the pairer with any of the tracking tags that they have paired.  Even if a third party discovers or obtains one of these tags and uses the system to get as much information about the tag's owner as possible, the absolute limit of that information will be heavily obfuscated identity information such as the last four digits of the registered phone number, an email address and so forth.  So who has access to the fully unobfuscated pairing registry?  And the answer is law enforcement.  But there are some caveats.  Paragraph 3.15.3, titled "Availability to law enforcement," simply reads:  "The pairing registry SHALL be made available to law enforcement upon a valid law enforcement request."



Nothing in this technical specification indicates how high the evidentiary bar would be set for such law enforcement queries to be honored.  But the point is that the identity of any tag-pairing party is tightly controlled for non-authorized access to this information.  The only information that anyone possessing a tag they are not paired with can obtain is that obfuscated identity information.  So one purpose of the pairing registry is to aid in immediately resolving the question of who might be tracking you.  But since the pairing registry itself contains fully non-obfuscated identity information, it's clear that the reason for this is to deliberately and explicitly create end-user accountability for the use of what is quite powerful consumer tracking technology.



I said earlier that I had the sense that at least part of this was Apple formalizing and publishing the technology that they had already established and deployed in the interest of making it an industry-wide and worldwide standard.  In any crowd-sourced model, bringing Android into the fold would make the system far more useful for everyone.  So I did some further digging.  And actually it was earlier this morning.  And what I found was that none of this is new, and that this law enforcement accessible pairing registry has always existed.



In one instance Apple writes - this is Apple:  "AirTag, AirPods, and other Find My network accessories include features to guard against unwanted tracking.  They should not be used to track people, and should not be used to track property that does not belong to you.  Using these products to track people without their consent is a crime in many countries and regions around the world.  If an AirTag, set of AirPods, or Find My network accessory is discovered to be unlawfully tracking a person, law enforcement can request any available information from Apple to support their investigation.



And since even that was a bit noncommittal, I dug some more.  On February 22nd of last year, so 15 months ago, February 22nd of 2022, 15 months ago, in their article titled "An update on AirTag and unwanted tracking," they make this very plain and explicit.  They said:  "We have been actively working with law enforcement on all AirTag-related requests we've received.  Based on our knowledge and on discussions with law enforcement, incidents of AirTag misuse are rare; however, each instance is one too many."  This is Apple.



"Every AirTag has a unique serial number, and paired AirTags are associated with an Apple ID.  Apple can provide the paired account details in response to a subpoena or valid request from law enforcement.  We have successfully partnered with them on cases where information we provided has been used to trace an AirTag back to the perpetrator, who was then apprehended and charged.  Law enforcement has shared their appreciation for the assistance we've provided in helping them find the source of unwanted tracking.  We've identified additional improvements we can make in the information we share and the educational resources we provide, and we will be taking action, including making updates to our law enforcement documentation."



Well, the surprise in this specification, of a registry whose real-world identifying data can be made available under due process of law, caught us off guard.  And I had picked up on it because the tech press was highlighting it as being a surprise.  So actually no one should have been surprised.  It's not a new thing.  This does, of course, beg the question, who knows?  And who would care if they knew?  We know that Apple has sold more than one billion dollars' worth of AirTags, and that Tile has sold more than 40 million units of their own tags.  No one appears to care about tag ownership tracking.  But I suspect that not many people know or have stopped to wonder.



LEO:  Yeah.  I didn't know, or I might have thought twice.



STEVE:  Right.  So one reason is that even if they have no idea how any of this works, they implicitly trust Apple.  And Apple has never given us any reason to suspect that trust is misplaced.  If I were to trust anyone to work hard to keep my information private, based upon all the evidence we've seen during the 17-plus years of this podcast, it would be Apple.  So the question is, would anyone particularly care if they knew?



I know that if I had something of value that I needed to track, like a bicycle or my luggage, I would not hesitate now to pair those tags to my real-world identity, knowing that any query about the ownership of those tags would need to get past Apple's legal department first.  And I have to say that I've never really seriously thought about AirTags before, but I have now.  Even though we've covered them on this podcast whenever they've made the news.  Now knowing what I know, I am absolutely going to purchase some AirTags, and I am never going to again travel without having them in my luggage.



We know that they won't begin squawking until they've been separated from us for a minimum of eight hours, or on average 16 hours.  So if they're in the belly of a plane along with us, they're going to be remaining silent until we pick them up after a flight.  And just knowing that wherever they might be, we'd have some way to know, seems like a bargain to me for 29 bucks each.  And I think that, being a bit sneaky, I would have one AirTag exposed and visible hanging out on an AirTag dongle, which are available, with another tag deeply hidden and tucked away rolled up in a sock so that if anyone nefarious did want to make off with our stuff, they might stop looking after removing the obvious tracker.



I'm glad that we took the time to look into this deeply since I've never really thought about it.  And I'm glad that Apple is making the effort to work with Google and their Android platform on this, so that we wind up with a single global standard which, due to the crowd-sourced nature of positioning, will make it much more powerful and attractive.  And next week, back to the news.



LEO:  I've got a hammer.  And I've got an AirTag.  Which, by the way, I've got to point out I don't see any serial number on this. 



STEVE:  It's not micro-etched around the back?  



LEO:  No.



STEVE:  How about if you open it up and look in the battery compartment?



LEO:  Yeah, but it's supposed to be visible; right?  Screw that.  [Hammering sounds]



STEVE:  Oh, oh, oh, oh.



LEO:  I don't want anybody - this thing's as tough - oh, yeah, hide that.  This thing is tough.



STEVE:  You know that we know where your phones are, Leo; right?



LEO:  Yeah, but I have this on my keys.  Yeah, that's already gone.  [Hammering sounds]



STEVE:  Ooh, ooh.  Okay, wait.  Turn the hammer over and use the horrible nail pull side.  Oh.



LEO:  Whoa.  I don't know where it went.  It has gone flying.  No, I don't want to carry that around.



STEVE:  So even after this, you still feel this way.



LEO:  Yeah, because Apple's already said they're handing this information over to law enforcement.



STEVE:  You're carrying a phone around.



LEO:  Yeah, I know, but that's a phone.  This is attached - I carry this on my person.  If I want to murder somebody, I've got to make sure...



STEVE:  Your phone is on your person.



LEO:  Well, I won't carry the phone when I go murder the person.  But it's on my keys.



STEVE:  You're going to bring an AirTag with you when you murder someone?



LEO:  Yeah, because it's on my keys.  I'm taking it off right now.  It could be in my bag because I - but, you know, basically, yeah, I guess you're right.  Your phone is doing the same thing, and I have the phone with me all the time.



STEVE:  And your car.  You've got technology out the wazoo, Leo.  Your headphones probably are tracking you.



LEO:  I'm mad.  Well, they didn't tell us, though.  That's the other thing.  I mean...



STEVE:  And you've registered your DNA with 23andMe.



LEO:  Yeah, I knew that my...



STEVE:  Your biology is smeared all over the map.



LEO:  I knew that my phone was being tracked; right?



STEVE:  Okay.



LEO:  I knew that.  They didn't tell me that they have - I had no idea that that...



STEVE:  Did you read the fine print?  I'll bet it's in the fine print. 



LEO:  Fine print.



STEVE:  I'll bet it is.



LEO:  But I think you're right, most people who carry AirTags don't know that that is broadcasting their exact location to all iPhones in the vicinity.  And then we always...



STEVE:  Wait, wait, broadcasting the tag's location.



LEO:  Yeah, and we were told that that was going to be private; right?  But the fact that Apple is now, is apparently saving that information about your location and then offering it to law enforcement upon request, that's - I think that's a violation of what they told us.



STEVE:  Okay.  So it's not the AirTag's location.  That's encrypted.  They don't get that.



LEO:  Oh.



STEVE:  It's the AirTag's ownership.  That's all we get.



LEO:  Oh, I don't care about that.



STEVE:  Well, that's all they have.



LEO:  Well, what good is that to law enforcement?



STEVE:  To know who paired the AirTag?  That's very useful.



LEO:  Oh, I guess if you're using it to stalk somebody.  So you're not saying, okay, now, I think my AirTag actually is still okay.  You're not...



STEVE:  Good, because it's...



LEO:  You're not saying that...



STEVE:  First of all, you've just demonstrated how robust the little suckers are.



LEO:  I tried to kill it, and I couldn't.



STEVE:  Wow.



LEO:  So you're not saying that the AirTag is sharing my location.



STEVE:  No, absolutely not.



LEO:  Because they were very clear about that, that it didn't do that.



STEVE:  Absolutely not.  It is deeply encrypted, and only the paired - it is encrypted so that only the device which paired the AirTag is able to determine its location. 



LEO:  Okay.  That's what they were - that's what I thought because they were very clear about that.



STEVE:  Yeah, only the owner of the AirTag is what Apple is able to reveal.



LEO:  So when they hand this over to law enforcement, they merely said, oh, that AirTag that somebody found stuck in their tailpipe or whatever, that's owned by Leo.  That kind of thing.



STEVE:  Yes.



LEO:  Yeah.  Well, that's fine.  I don't have a problem with that.



STEVE:  Oh, good.  Then this was all just a misunderstanding.



LEO:  It's just a misunderstanding.  I'm sorry, little AirTag.  I didn't mean to hit you with a hammer.



STEVE:  Aw.  I'll tell you, if that thing had any sort of a G-force sensor, you know, the Apple security people are going to be knocking at your door, saying, "What are you doing to our AirTag?"



LEO:  I'm just seeing if Find My still knows where my AirTag is.  Uh-oh.  It hasn't seen it in four minutes.  That's a bad sign.



STEVE:  Yeah, I think that's when the pounding began.



LEO:  I think I might have killed it.



STEVE:  You know, probably.  It might deliberately take that as a hint and just stop, like turn itself off.



LEO:  Take the hint.  Yeah, because it...



STEVE:  That might be one of the instructions.  Well, you can press the button for 10 minutes, or you can just ask Leo to smash it for you with a hammer.  That will disable its tracking function.



LEO:  There you have it, ladies and gentlemen.  I misunderstood.  I feel bad now.  You can have your hammer back, Burke.  I went searching all over this - this is a good hammer, I might add.  Some significant leverage.



STEVE:  It is AirTag approved.



LEO:  I think I killed the AirTag.



STEVE:  It's AirTag certified.  I hope you did.



LEO:  Hasn't been heard from in five minutes.



STEVE:  Yeah.



LEO:  I think I killed it.  It's all right, I didn't want it anyway.



STEVE:  If it starts beeping after 16 hours, you'll know that some part of it is still alive.



LEO:  Now, that's Apple.  Presumably doesn't mean Tile or Chipolo or any of the other Bluetooth trackers are as private.  But Apple has always said we're not - that location's not being shared anywhere.



STEVE:  That's a very good point, and I actually meant to put that in the notes so I wouldn't forget to mention it.  That's why I'm using AirTags, not weasel tags.  I do not want, you do not want to use a weasel tag on your stuff because weasel will know where you are.



LEO:  And all that law enforcement gets is my email, my phone number.



STEVE:  Yes.  That's all they have is the pairing registry data.



LEO:  Yeah, yeah.  Sorry, AirTag.



STEVE:  Yay.



LEO:  Haven't heard from it in six minutes.  I'm thinking it's...



STEVE:  It's not looking good, Leo.  Not looking good. 



LEO:  It didn't smash it.  But apparently the insides were a little munged.



STEVE:  Oh, I think it's, yeah, it's done.



LEO:  You did it again.  You scared the pants off of me.  In this case not necessarily needed.  I've put my pants back on.  And thank you, Steve Gibson.



STEVE:  Well, the world has lost one AirTag.  But it has 55 million other ones, busily beeping away.



LEO:  I mean, honestly, that really is the thing to underscore is that everything you have is beaconing, as you point out.



STEVE:  Yes.  There is no privacy.  It's all an illusion.



LEO:  Yeah.  And furthermore - by the way, I don't know what's in front of you.  Can you fix that, somebody, anybody?  Steve is now peering at us through his address card.  Hello, Engineering.  I sent the whole engineering team off to find your hammer, and now they're gone.  Okay.  Fixed.  And you did mention 23andMe.  I just recently swabbed my cheek and sent it off to a company called Nebula.  It does my whole gene sequencing, not just sampling.



STEVE:  Nice.



LEO:  Does 100% of Leo's genome.  So anybody who's looking for that, you'll find that online.



STEVE:  And now we know where.



LEO:  Nebula.org.  They've got the whole thing.  Steve Gibson is at GRC.com.  That's where you find him.  You'll also find copies of this show.  He's got two unique versions.  Normally we'd have a 64Kb audio version.  Steve's got that.  But he also has a 16Kb audio version for people who want to shrink it down.  What is that, one fifth, something like that, one fourth as big.  He also has - that's my AirTag there on the floor.  Could you...



PERSON:  I'm not touching that thing.



LEO:  Okay.  We also have - he also has very nice human-written transcripts that Elaine Farris does such a good job with.  And there's one more thing he's got, which is important.  He's got the show notes.  Now, we link to it in our show page at TWiT.tv/sn, so you can click there.



While you're at Steve's site, pick up a copy of SpinRite v6.0, the world's finest mass storage maintenance and recovery utility.  As you heard, he's working hard on 6.1.  When that comes out, you'll get that for free.  But you've got to buy 6.0 now:  GRC.com.  You can leave Steve DMs at the Twitter.  He's @SGgrc.  Let me put that on the screen:  @SGgrc.



Copyright (c) 2023 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#924

DATE:		May 23, 2023

TITLE:		VCaaS - Voice Cloning as a Service

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-924.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week, we'll lead off with a tracking device follow-up, then answer some questions including:  What happened when I updated my own ASUS router?  And what happened when HP attempted to update all of their OfficeJet Pro 9020e-series printers in the field?  What did the Supreme Court have to say, if anything, about Section 230?  How concerned should KeePass users be about this new master password disclosure vulnerability?  What's Apple's position on ChatGPT?  What's Google been quietly doing about its "user profiling without tracking" Privacy Sandbox technology?  What disappointing news did the Senate Intel Committee just reveal about the FBI, and why did the Python Foundation suddenly close all new registrations of users and packages?  Then, after I announce and explain the discovery and fix for a longstanding bug that has always existed in SpinRite 6.0, probably extending as far back as SpinRite 3.1 in the mid '90s, we're going to finish by examining the emergence of new "Voice Cloning as a Service" Dark Web facilities.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  Coming up, what happened when HP attempted to update its Office Pro 9020e printers?  It wasn't good, I'll give you a hint.  We'll also talk about a flaw in KeePass that you probably don't need to worry about.  A bug in SpinRite 6 that's been there for years.  We'll talk about why it's also probably not something to worry about.  And then the rise of Voice Cloning as a Service.  Should you be worried?  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 924, recorded Tuesday, May 23rd, 2023:  VCaaS - Voice Cloning as a Service.



It's time for Security Now!, the show where we cover the latest in security news with Mr. Steven "Tiberius" Gibson.  I think it now says that on your Wikipedia page, by the way.



STEVE GIBSON:  I think it does.



LEO:  From GRC.com.  Hello, Steve.  Good to see you.



STEVE:  Hello, my friend.  So I was initially going to call this podcast - I wasn't very inspired because it was mostly just news.  So I was going to call it "News, Views, and No Snooze."  Which, you know...



LEO:  Is cute.



STEVE:  ...wasn't great.  But, well, yeah.  And we may have one in the future that's called that.  But I ran across a story about the emergence of voice cloning as a service.  You know we've been talking about ransomware as a service, so like to make it much easier for the bad guys to deploy ransomware.  Well, once that, you know, now of course we've got voice cloning as a service.  So that took over the title, and we're going to wrap up by talking about that.



But we're going to lead off with a tracking device follow-up, then answer some questions, including what happened when I updated my own ASUS router, and what happened when HP attempted to update all of their OfficeJet Pro 9020e series printers in the field.  What did the Supreme Court have to say, if anything, about Section 230?  How concerned should KeePass users be about this new master password disclosure vulnerability?  What's Apple's position on ChatGPT?



What's Google been quietly doing about its user profiling without tracking privacy sandbox technology?  What disappointing news did the Senate Intel Committee just reveal about the U.S. FBI?  And why did the Python Foundation suddenly close all new registrations of users and packages?  Then, after I announce and explain the discovery and fix for a longstanding bug that has always existed in SpinRite 6.0...



LEO:  What?



STEVE:  ...long thought to be bug-free...



LEO:  Long thought to be perfect!



STEVE:  I know.  Probably extending as far - this bug extends as far back as SpinRite 3.1, as far as I know.



LEO:  Wow, wow.



STEVE:  In the early '90s.  I did confirm it in 5.0.  We're going to finish by examining the emergence of this new Voice Cloning as a Service dark web facility.



LEO:  Uh-oh.



STEVE:  And, oh, we've got a terrific Picture of the Week.  I am being overloaded with fences and gates that go nowhere.  It turns out...



LEO:  They're very common.



STEVE:  Turns out the world is full of this, like what is going on here?  



LEO:  It's really a thing.



STEVE:  Today we've got a really fun Picture of the Week that will be of interest.  It's just so clever, about a mistake that people can make in coding.



LEO:  Ah.  I'm going to guess the mistake you made in SpinRite, it was not a buffer overflow bug.  I'm going to guess.



STEVE:  It wasn't, and I will defend myself because it wasn't a bug back then, either.



LEO:  Oh, interesting.



STEVE:  But it's something that it became one, so it's interesting.



LEO:  See?  Yeah, that's an interesting, yeah, example.  Well, we'll talk about that in a second.



STEVE:  So, oh, okay.  So in programming in general, there are often times where you want to do something multiple times.  You want to repeat something.  And, you know, it's called a "loop."  So you are looping in order to continue doing something.  Well, there's a question then, like how do you know when to stop looping.  The two standard constructs are to test at the top of the loop before you do whatever it is that you're going to do; or to test at the bottom of the loop so that, after you've done something, you check to see if you should do it again.  And the reason both exist is that they, you know, although they're very similar, right, you are looping, and you're testing to see if you should keep looping, the test at the top of loop may result in never doing something, like if you don't want it to happen at all.



Testing at the bottom of the loop normally is used when you always want to do something once, at least once, and then you test to see if you should do it again.  So anyway, so we have a cartoon here with the Road Runner and the Wile E. Coyote.  And the loop that is controlling the Road Runner says "while not edge, run."  So it's going to check that the Road Runner is not at the edge; and, if not, the Road Runner's going to run, and then check again, and run.  However, unfortunately, the Wile E. Coyote, his actions are controlled by the other sense of loop, which says "do run while not edge."  The problem being that running is happening before the check for the edge.  So as often happens to this unfortunate Wile E. Coyote, he ran right off the edge.  



LEO:  Aw, right off the edge.



STEVE:  Then after he was in the air, you know, and it's always odd how he's suspended there for an unusually long amount of time.  It's like, what has happened?  Where's the gravity here?  Anyway, so we all have a chance to see that the Coyote's in trouble.  Then we check to see, oops he's not over the edge anymore.  And then, whew, and then we switch to the long-distance, little dust explosion down at the bottom of the canyon.  But the good news is he seems always undaunted, and he springs back to life to harass the Road Runner further.



Okay.  As I mentioned last week, GRC's Security Now! newsgroup was holding a great deal of discussion surrounding all of the AirTag tracking issues.  In that vein, a person named "Li" (L-I) posted that they had stumbled across a YouTube video which explained how to, without much trouble, remove the little beeping speaker from an AirTag.  So he posted that, and I asked him whether he had any impression from the video about why the person making this video thought that that would be useful.



Li replied:  "The reason is to prevent alerting a thief to the presence of the AirTag."  And he said:  "The YouTuber begs people not to misuse the hack."  Well, there's been some related discussion in the newsgroup questioning how an AirTag that's beeping can be used for stolen item recovery since the beeping will inherently disclose the hidden presence of the tag.  You know, at which point the attacker might either, well, Leo, as you did last week, smash the crap out of it with a mallet in order to render the tag inert, or perhaps be more sneaky and tuck it down into the back seat of an Uber driver's car so that the tag will drive off with the car, and good luck finding it.



The salient point here being that we have two use-cases which are inherently at odds with one another.  Someone trying to locate something that is not in the hands of someone nefarious wants help locating the object.  So in this context, beeping to call attention to the not-stolen tagged object is desirable.  Whereas someone trying to locate something that has been stolen doesn't want the nefarious thief to know that they have been tagged and are being tracked.  So in that context beeping, which calls attention to the presence of a tracking tag, is undesirable.



Since the tag's paired owner might know which of the two contexts is proper - did I leave my car keys somewhere or has my scooter been stolen? - I'm sure that at some point someone must have suggested that it would be possible technologically to give the tag's paired owner remote control over the beeping since they would know which context applied and whether the tag should be set to beep or not.  But of course we can't do that either, since that control assumes that the tag's paired owner is honest, honorable, and ethical, which would not be the case when someone wishes to deploy the tracking tag, not to keep track of their own stuff, but to surreptitiously track someone else or somebody else's stuff.



So the point I'm hoping to highlight here is that a fundamental and I think irresolvable, you know, irreconcilable tension inherently exists for which there is no single perfect answer.  And that is this tension that apparently was reflected in this forthcoming IETF tracking specification which we went over in detail last week.  The spec never explains the why of any of its requirements; right?  We didn't see that.  It only lays out the what.  So we're left to guess.  But, for example, the provision that tags must wait a random length of time - from eight hours to 24 hours - after being physically separated from their owner before they're allowed to start making any noise, and then only when they are jostled, feels like a compromise between the need for stealth in the event of a tagged item's theft and the need for disclosure in the event that someone else's tag is being used for nefarious tracking.



Now we learn that disabling a tag's beeping speaker is easy to do, and using somewhat more precision surgically than, Leo, you did last week.  Clearly, someone who tags their own ebike, for example, would desire to keep their tag quiet since "locating" a tag attached to a large bicycle won't ever require a speaker, and preventing a thief from discovering the tag could be extremely useful. In fact, if I were to tag something big like that today, knowing what I know now, I would disable its tiny speaker, too.



Unfortunately, the obvious flipside to this is that low budget bad guys who wish to track others surreptitiously using AirTags, could also disconnect the device's tiny speaker in order to keep the tag from revealing itself.  There's no way to win on this one.  These Bluetooth LE tags are a powerful consumer tracking technology that is inherently prone to abuse.  But, you know, presumably that is offset by the fact that they are also very useful, and we know that a hundred million plus have been sold so far in just the last two years.



Okay.  So automatic IoT device updating, an issue we've been talking about recently, and two really interesting little tidbits.  Well, one somewhat interesting and other really interesting.  So we know that some of it is here already, and that more is clearly coming.  Much of the forthcoming legislation and pending regulation surrounding the behavior of connected IoT devices makes mention of the need for the provision for automatic autonomous firmware updates.



I recently mentioned that when I asked the ASUS RT-AC68U WiFi router at one of my locations to perform a self-check for newer firmware, it said that there was an update available.  I mentioned this in the context a couple weeks ago of the problem of a lack of automatic autonomous firmware updating, wondering how many of us were incessantly checking for the availability of updated firmware for our various devices, and presuming that most of us generally had more pressing things to do.  And thus the problem that routers may not be getting updated.  The next week we learned from a TP-Link-using listener that those vulnerable TP-Link routers, at least some of them, did have autonomous update capabilities which was enabled by default.  So that was great news.



Then I received a Twitter DM from another of our listeners.  Shaun Merrigan tweeted:  "Steve.  Regarding auto-update routers, ASUS added this feature into this now 'old' router [and he had 'old' in quotes] RT-AC68U [the one I just mentioned, the one that I use] a couple of years ago."  He said:  "And they're still updating the firmware regularly."



Well, that was interesting to me.  I knew from having recently checked, new firmware was available for that router, though I hadn't taken any action on that yet.  In defense of my apparent laxity, I'll remind everyone that this consumer router is not on the frontline with its WAN port facing a hostile Internet.  It sits safely behind the front line router which is running pfSense since I play lots of static port translation gymnastics to avoid Cox Cable's annoying consumer bandwidth port filtering.



But Shaun's tweet came at a perfect moment.  So I logged into the ASUS RT-AC68U and asked it to please update itself.  And it did that.  And lo and behold, what missing feature do you think I found had been added to this router since its last update?  Yup, automatic firmware updating.  My router now has the ability to update itself autonomously at 2:00 a.m. whenever it detects that ASUS has some new firmware for it.  So that's the good news.



But this good news comes nicely paired with the most catastrophic possible outcome of autonomous auto-updating.  I labeled this story HP 9020e - error code 83C0000B.  A post dated May 8 of obviously this month, at 7:30 a.m., on HP's customer support forum, addressing HP OfficeJet Pro 9022e All-in-One Printer.  The post is:  "Hi there, my printer is offline with a blue screen" - and didn't you know they had to choose blue as the background - "and error code 83C0000B.  I have read through other posts and tried the power drain reset, but to no avail.  Can someone send me instructions to perform a semi-full reset, please?  Many thanks."



That particular forum posting, which launched a thread, generated 132 replies all on its own, such as:  "I have the same problem and have just been going around in circles trying to find a solution."  Or "Ditto.  No coincidence that we're all having the same issue on the same printer on the same day (software issue). HP please help!"  Or "Same problem here too - did anyone get a solution?  My printer is only three months old, and it is used for business so I need it to work ASAP!"



"Has anyone had any luck resolving this? I have the same issue."  "Hi there, did you get any resolution as the exact same thing has happened to me this morning."  And a final three:  "I had the same problem, brand new printer.  I've tried unplugging the power cable and waiting for a minute.  The error is still there in a blue screen and a power button sign.  HP, please find a solution as our work is affected."  Also, "Hello.  Same problem on my 9022e, 4 months old.  I switched it off for few hours, removed the power cable, but still blocked on 83C0000B error message."



And lastly:  "I have an OfficeJet 9022 Pro under warranty and with an HP cartridge subscription.  I have same error code since yesterday, 08/05/2023.  After three calls to HP after-sales service, more than an hour and 30 minutes, no solution.  'The problem is being studied by HP.'  I am awaiting instruction.  If no return info, I return it, and it could end with a refund via distributor."  And he says:  "Amazon France.  Thank you."  Oh, and one last one:  "This is insane, and not a word from HP?  I came here to make a post about this, and it seems it's basically a global issue!  Can someone from HP provide an update?  This is terrible.  We cannot print anything.  Stuck with this error!"



Okay.  So now let's turn to our friends at BleepingComputer for a succinct summary of what's going on:  BleepingComputer's headline reads "HP rushes to fix bricked printers after faulty firmware update."  And note that this was posted just this past Saturday, 12 days after all of their affected printers were turned into expensive bricks.  BleepingComputer writes:  "HP is working to address a bad firmware update that has been bricking HP OfficeJet printers worldwide since it was released earlier this month," meaning the firmware update was released.  



"While HP has yet to issue a public statement regarding these ongoing problems affecting a subset of its customer base, the company told BleepingComputer that it's addressing the blue screen errors seen by a 'limited number' of users.  HP told BleepingComputer:  'Our teams are working diligently to address the blue screen error affecting a limited number of HP OfficeJet Pro 9020e printers.  We are recommending customers experiencing the error contact our customer support team for assistance at support.hp.com.'"



BleepingComputer wrote:  "Impacted printers include HP OfficeJet 902x models, including HP OfficeJet Pro 9022e, 9025e, and the 9020e and 9025e All-in-One Printers.  Affected customers report that their devices display blue screens with '83C0000B' errors on the built-in touchscreen.  Since the issue surfaced, multiple threads have been started by people from the U.S., the U.K., Germany, the Netherlands, Australia, Poland, New Zealand, and France who had their printers bricked, some with more than a dozen pages of reports.  One customer said:  'HP has no solution at this time.  Hidden service menu is not showing, and the printer is not booting anymore.  Only a blue screen.'  Another added:  'I talked to HP Customer Service, and they told me they don't have a solution to fix this firmware issue at the moment.'



"Others have said that the only way to address the issue is to send the printer for servicing to HP and that:  'The firmware doesn't even load partially.  It instantly fails.  HP remotely bricked our devices.'  Some users said that HP would be sending out a replacement.  Since the buggy update," BleepingComputer finishes, "seems to install automatically onto Internet-connected printers, HP customers are advised to disable their devices' Internet connection and wait for a firmware update to fix the bricking issue."



So, as it happens, it's not that often that we actually get a horrific real-life answer to our often posed question:  What could possibly go wrong?  But, oh boy.  Wow.  Apparently, the only reason some of these susceptible model printers were not blasted into oblivion was that they had not phoned home to check in for a firmware update.  In this instance they had a blessed lack of connectivity.



We do need to expand upon our normally rhetorical "What could possibly go wrong?" question to also ask "How could this possibly happen?"  Right?  I mean, this would lead anyone to ask:  "Did HP not ever actually test the firmware that they sent out to every available printer which then proceeded to kill all of them?"  It must be that HP had at some point working firmware in the lab.  So we have to conclude that somewhere between the lab and all of those end users, that firmware image file became corrupted somehow.  Perhaps it was a corrupted file transfer to the update server, or somehow the file on the update server itself became corrupted.  But without information that only HP has, and which we will probably never have, it's impossible to know exactly how and why this blew up. All we can do is examine the debris field.



But regardless of what happened upstream of these printers, which will almost certainly forever remain unknown, what we do know today is that HP's autonomous updating system is not well designed, because this should have been not just unlikely to happen, but impossible to have happen.  We're witnessing the obvious downside of poorly designed autonomous updating, which is that, if you're not careful, you can potentially hose your entire installed base of devices, whatever and wherever they may be.  And I suspect that since HP has not yet been forthcoming with an explanation, though I'm sure by now they know just how deep in the doo-doo they are, we're never going to know exactly what happened.  Today's HP is a massive enterprise, which means that they have squadrons of well-paid attorneys who immediately clamped down on all communication in an attempt to minimize perceived culpability, even though it's going to be difficult to explain this one away.



Today's HP is not the HP I grew up admiring.  HP, as many of I'm sure our old-timers know, was founded by a pair of electrical engineers working in a small garage in Palo Alto, California.  Back in 1939, Bill Hewlett and David Packard correctly divined that what the world really needed was a high-quality analog signal generator.  So they built and sold some and then they built and sold many more.



The reason for my annoyance is not just HP's sloppy software, although their printer software, at least for Windows, is a true atrocity now.  But what appears to be the case, is that HP has just bricked all of these printers - rendering them unusable at least for the time being - in order to update the printer's detection of non-HP printer ink cartridges in a move to further thwart the use of significantly less expensive and just as effective third-party ink.  Ars Technica also covered this month's HP printer bricking debacle, and at the end of their coverage they added some additional interesting insight.



Ars wrote:  "It's unclear what exactly prompted the firmware update that broke HP's printers.  HP's support pages for the 9020e and 9025e series only emphasizes the printers' use of HP's much maligned 'Dynamic Security.'"  They have that in quotes.  "HP," they write, "uses Dynamic Security to stop printers from working with non-HP brand ink cartridges.  HP's notoriously sudden issuance of Dynamic Security to printers has abruptly rendered piles of ink useless.  Officially, HP says it uses 'Dynamic Security measures to protect the quality of our customer experience, maintain the integrity of our printing systems, and protect our intellectual property.'  But since debuting in 2016, it has resulted in class-action lawsuits and irate customers stuck with otherwise functioning ink that HP decided should not work.



"HP's Dynamic Security page notes that Dynamic Security printers require periodic firmware updates to 'maintain Dynamic Security effectiveness' [I'll just bet they do] and that 'updates can improve, enhance, or extend the printer's functionality and features, protect against security threats, and serve other purposes.'"  So Ars finishes:  "HP's poor handling of printer firmware updates can deter people from future updates that may be important.  Meanwhile, HP has traumatized countless customers, and many will now think twice before depending upon an HP printer again."



So if we wonder why HP printers which employ "Dynamic Security" might require periodic security updates, it may be because HP feels that their control over their printers allows them to avoid playing on a level playing field where they need to compete with other suppliers of ink cartridges.  So they're engaged in a game of cat-and-mouse with the third-party ink vendors who are continually working around HP's "Dynamic Security" measures in order to get their ink to flow.  That then necessitates HP again updating their non-HP ink defenses with new firmware.  It's a sad game really, and this time it appears that the joke's on HP.



But regardless of why this happened, it's going to be quite interesting to learn whether this can be fixed in the field.  The printers do all have a USB port.  So if the printer is able to boot from a properly formatted USB thumb drive even with its firmware in this apparently badly blasted state, HP might be able to avoid replacing all of the destroyed printers in the field by giving their users a recovery image to download onto their own thumb drive or express mail a ready-to-boot thumb drive to anyone who requests one.  But we don't yet know whether that can be done.  Since customers would probably not have shipping boxes, the shortest path might be to send a new printer in a box and ask the user to box up their dead printer and return it in the same box.



No matter what, it's a mess.  The ASUS router upgrade page says:  "In case the upgrade process fails, RT-AC68U enters emergency mode automatically.  The LED signals at the front of the router will indicate such a situation.  Please visit ASUS Download Center to download ASUS Firmware Restoration utility for a manual update.  Check FAQ for more instructions."



It doesn't appear as though HP has provided for such a contingency.  But the observation I'd like to make is that today's flash memory is very inexpensive, so there is no excuse for not incorporating a factory backup image of printer firmware that the system can boot, can fall back to...



LEO:  Oh, that's a good idea, yes.



STEVE:  ...and boot into in an emergency simply by pulling the most significant address line of the flash memory high, thus switching to a secondary memory bank containing the backup image.  Then trigger a reboot, and you're back in business.  Processors used in embedded environments typically incorporate what's known as a "watchdog timer."  It's a sort of a dead man's switch.  The idea is that running software will periodically reset the watchdog timer, always making sure to do so before the timer has the chance to expire.  This is typically easy to do, since the watchdog generally doesn't need to respond instantly to any problem.  It's the response of absolute last resort.



So whereas the code in the device's main execution loop might reset the watchdog timer every second or so, the timer might wait for 20 seconds without a reset occurring before it has the chance to expire and declare an emergency.  The first time an emergency occurs, the watchdog might just perform a reset to recover the device from a random glitch, you know, a power line hiccup or a cosmic ray.  But it would remember that that's what it had done.  And if it got called back again into action soon, or several more times, it would pull that address line high, switch to the backup firmware, and bring the printer back online.  If HP had done this, a couple of third-party ink cartridges might have slipped past due to not running the latest anti-third-party ink firmware, but a major economic and reputational disaster would have been averted.



So the point I want to leave everyone with is that, yes, of course, autonomous updating can be done wrong.  And we are apparently seeing a glaring example here.  But it can also be done in a way that will always fail safe rather than fail dead.  And on that happy note, Leo, let's take our second break.  And then we're going to talk about Section 230.



LEO:  Back to you, Mr. Steve.



STEVE:  So Leo, I know you had some Section 230 legal experts on the Sunday show.



LEO:  We had a great show on TWiT, yeah, because that's all we talked about, pretty much, yeah.



STEVE:  Yeah, I was able to listen to just the beginning.  I've not listened to the whole thing, but I plan to.  As we know, the bit of law that's become famous for its section number, 230, is what essentially allows Facebook, Twitter, YouTube, and anyone else who hosts content provided by others to avoid liability for any consequences which might arise from their hosting of that third-party material.  So perhaps the biggest news of last week, or at least a relief for a lot of the Internet, was that the Supreme Court, which first surprised many court watchers when they agreed to weigh in on the appeals made to the Ninth Circuit Court's earlier decisions, produced their conclusion in May rather than waiting until the end of the current term, which is in June.  And to the great relief of practically everyone, Section 230 held up under the Supreme Court's review and remains, at least for the time being, in full force and effect.



Both tech and non-tech press covered the news.  I felt that the EFF was a bit snarky in their coverage, which they headlined "The Internet Dodges Censorship by the Supreme Court."  Yeah.  Dodged it by 9 to 0.  So I would say that...



LEO:  It's a unanimous dodge, yes.



STEVE:  That's right.  Techdirt's Mike Masnick dug deeply into the details.  It's way more than we need here, but I linked to his coverage in the show notes for anyone who really wants a deep dive.  Reuters, however, provided a very good overview of the issues.  And since this is of crucial importance to the way the Internet evolves, and since this may not be settled law forever, I think it's worth looking at the way Reuters summarized the Court's involvement.



So they wrote:  "The U.S. Supreme Court handed Internet and social media companies a pair of victories on Thursday, leaving legal protections for them unscathed and refusing to clear a path for victims of attacks by militant groups to sue these businesses under an anti-terrorism law.  The justices in a case involving Google's video-sharing platform YouTube, part of Alphabet, sidestepped making a ruling on a bid to weaken a federal law called Section 230 of the Communications Decency Act that safeguards Internet companies from lawsuits for content posted by their users.  They also shielded Twitter, Inc. in a separate case from litigation seeking to apply a federal law called the Anti-Terrorism Act that enables Americans to recover damages related to 'an act of international terrorism.'



"In both cases, families of people killed by Islamist gunmen overseas had sued to try to hold Internet companies liable because of the presence of militant groups on their platforms or for recommending that content.  The justices in a 9-0 decision reversed a lower court's ruling that had revived a lawsuit against Twitter by the American relatives of Nawras Alassaf, a Jordanian man killed in a 2017 attack during New Year's celebration in a Istanbul nightclub claimed by the Islamic State militant group.



"In the case involving YouTube, the justices returned to a lower court a lawsuit by the family of Nohemi Gonzalez, a college student from California who was fatally shot in an Islamic State attack in Paris in 2015.  The justices declined to address the scope of Section 230, concluding they did not need to take that step because the family's claims appeared likely to fail given the Twitter case decision.  Section 230 provides safeguards for 'interactive computer services' by ensuring they cannot be treated for legal purposes as the 'publisher or speaker' of information provided by users.



"Calls have come from across the ideological and political spectrum, including Democratic President Joe Biden and his Republican predecessor Donald Trump, for a rethink of Section 230 to ensure that companies can be held accountable for content on their platforms.  This case marked the first time the Supreme Court had examined Section 230's reach.  Google's General Counsel said:  'Countless companies, scholars, content creators, and civil society organizations who joined with us in this case will be reassured by this result.  We'll continue our work to safeguard free expression online, combat harmful content, and support businesses and creators who benefit from this content.'



"Critics have said Section 230 too often prevents platforms from being held accountable for real-world harms.  Many liberals have condemned misinformation and hate speech on social media.  Many conservatives have said voices on the right are censored by social media companies under the guise of content moderation.  The massacre at Istanbul's Reina nightclub killed Alassaf and 38 others.  His relatives accused Twitter of aiding and abetting the Islamic State by failing to police the platform for the group's accounts or posts in violation of the Anti-Terrorism Act.



"Gonzalez's family argued that YouTube provided unlawful assistance to the Islamic State by recommending the group's content to users.  In their brief ruling, the justices wrote that they 'decline to address the application of (Section 230) to a complaint that appears to state little, if any, plausible claim for relief.'



"Chris Marchese, an attorney with NetChoice, a technology industry group that includes Twitter, Meta and Google as members, said:  'Even with the best moderation systems available, a service like Twitter alone cannot screen every single piece of user-generated content with 100% accuracy. Imposing liabilities on such services for harmful content that unintentionally falls through a crack would have disincentivized them from hosting any user-generated content.'



"The Twitter case hinged on whether the family's claims sufficiently alleged that the company knowingly provided 'substantial assistance' to an 'act of international terrorism' that would allow the relatives to maintain their suit and seek damages under the anti-terrorism law.  After a judge dismissed the lawsuit, the San Francisco-based Ninth U.S. Circuit Court of Appeals allowed it to proceed, concluding that Twitter had refused to take 'meaningful steps' to prevent Islamic State's use of the platform.



"Conservative Justice Clarence Thomas, who authored the Supreme Court's ruling, said the allegations made by the plaintiffs were insufficient because they 'point to no act of encouraging, soliciting, or advising the commission' of the attack.  'These allegations are thus a far cry from the type of pervasive, systemic, and culpable assistance to a series of terrorist activities that could be ascribed as aiding and abetting each terrorist act,' Thomas added.  In the Twitter case, the Ninth Circuit did not consider whether Section 230 barred the family's lawsuit.  Google and Facebook, also defendants, did not formally join Twitter's appeal."



Okay.  So as with many of the decisions reached by the Supreme Court, the result was not terribly satisfying.  The Court presumably obtained the outcome they wanted, which was to leave Section 230 in place, as is, in its entirety.  But they did so not by directly addressing the issue, but by focusing on procedural distractions, like complaining that someone had forgotten to tie their shoes.



At the end of his long and interesting analysis, Techdirt's Mike Masnick concluded.  He said:  "Finally, speaking about money, time, and resources," as Mike put it, he said, "a * ton of all three were spent on briefs from amici for the Gonzalez case, in which dozens were filed, including one from us.  And the end result was a three-page per curiam basically saying 'We're not going to deal with this one.'"  He said:  "The end result is good, and maybe it wouldn't have been without all of these briefs.  However, that was an incredible amount of effort that had to be spent for the Supreme Court to basically say, 'Eh, we'll deal with this some other time.'  The Supreme Court might not care about all that effort expended for effectively nothing, but it does seem like a wasteful experience for nearly everyone involved."



Okay.  So anyway, 230 stands unweakened, but still likely assailable in the future if the right challenge is brought.  And Leo, as I said, I didn't have a chance to listen to your experts on Sunday.  Did they have a different take?



LEO:  Well, Cathy Gellis, who was the person who wrote that Mike Masnick's Techdirt amicus, you know, there were two cases.  There was Google, and there was Taamneh.  And the reason that they put aside one is because they said our ruling in Taamneh applies to both.  So the ruling, and I think Mike's point in Techdirt was that the, you know, it was Clarence Thomas who wrote the opinion in the Taamneh case, basically made the case for Section 230 without actually mentioning Section 230.  They ended up deciding more on what aiding and abetting meant in the Terrorism Act and whether this was aiding and abetting.  And they said it didn't.



So it didn't, yeah, I mean, it's true, it doesn't really, one way or the other, it doesn't confirm 230.  But what Thomas wrote certainly can be used in future cases to justify 230.  One of the issues in the Google case was the idea of an algorithm.  Or I guess it was Twitter case.  If a company uses an algorithm - no, it was Google.



STEVE:  To recommend.



LEO:  To recommend a video, is that the same as them publishing the video?  And that was the contention of the plaintiffs.  But in fact the court said no, algorithms are used everywhere all the time.  That does not make it something that the company published.  That's important for you, too, because you run forums on your website.



STEVE:  Oh, yeah.



LEO:  Section 230 protects you.  If it didn't exist, you would probably think twice about running forums, as I would.  Not just forums, but our Mastodon instance, our chatrooms.  We wouldn't do it because we would be liable.  What Section 230 does that's so important is it provides immediate injunctive relief.  You say, no, no.  Somebody sues you, you don't have to really defend yourself.  You just say "Section 230, Your Honor," and the judge will throw it out immediately.  So, and that's how it's supposed to work.  None of that was impacted by the Supreme Court's decision.  But the opinion, Masnick says, and I think he's right, it makes a very strong case for why 230 exists.  You have to have it.  So I think, yeah, dodges in the sense that they didn't really address 230 is probably not inaccurate.  But what they did do did not in any way weaken it, and perhaps even strengthens it.



STEVE:  Good.



LEO:  There'll be another case, I'm sure.



STEVE:  Yeah, there will.



LEO:  This is the right, I mean, if you listen to the arguments in the Google case, which I listened to, they were, at one point they were saying, well, Google made thumbnails of this video.  That makes them liable.  And it's like, no, no, no.  They just - it was poorly argued, frankly.  So anyway, victory.  Declare victory and move on.



STEVE:  Yes, yes, yes.  Last week the entire tech press community lost its collective mind over the news generated by a hacker that KeePass was insecure, and that it was possible to obtain the KeePass master password and thus everything else that it was protecting.  Infosecurity Magazine's headline was "KeePass Flaw Exposes Master Passwords."  Oh, that doesn't sound good.  The Hacker News reported under the headline "KeePass Exploit Allows Attackers to Recover Master Passwords from Memory."  SC Magazine said "KeePass bug lets attackers extract the master password from memory."  And actually they had by far the coolest graphic, which I put in the show notes.



TechTarget wrote:  "KeePass vulnerability enables master password theft."  Anonymania:  "Passwords at Risk.  Major KeePass Flaw Unearthed."  TechRadar:  "This top password manager apparently has a major security flaw that could spill all your logins."  And finally, HelpNetSecurity:  "KeePass flaw allows retrieval of master password.  Proof of concept is public."  Uh-oh.



Okay.  So here's the one and only thing that this podcast's well-informed listeners need to know:  It's an entirely local attack.  In other words:  News flash.  If your machine has malware running amok, it's not safe to use a password manager.



LEO:  No.



STEVE:  Any password manager, not just KeePass.



LEO:  Right.  Right.



STEVE:  By the very nature of what any password manager is and needs to do, no password manager is safe from local attack.  And they don't try to be because it's a fool's errand.  Recall how almost all commercial video DVDs were encrypted to prevent the theft of their contents.  But DVD players were fundamentally unable to keep their keys secret because they needed to use them in order to decrypt the DVDs.  It was so dumb.  In an exactly analogous way, a password manager that's going to fill-in blank username and password fields must have access to that password manager's decrypted password database.  Even if you set it up in an annoying way to require you to provide the master password each and every time you use it, it would still, briefly, need to decrypt and read the master database to obtain the required password.



The reason we were so upset with LastPass was not over something like this, which, as I said, is an inherent design point for any client-side password manager.  We got upset with LastPass because they lost everyone's encrypted data all at once, and then we learned that a lot of it had never been encrypted, and that what was encrypted might not have been very well encrypted.



KeePass is a popular, free, and open source password manager.  I don't use it, but I know from Twitter that some of our listeners do.



LEO:  Most of them, I would hope, use KeePassXC, which is not by the way vulnerable to this.



STEVE:  Yes.  It is a fork that is not vulnerable.  So you have to imagine that KeePass's lead developer, Dominik Reichl, who has recently been attacked from every angle, has got to be asking himself about now why he's been working so long and hard on this currently thankless project.



LEO:  Sad.  We do not treat our open source developers very well, do we.



STEVE:  No.



LEO:  No.



STEVE:  So here are some details to give everyone some sense for what's going on in this particular case.  The issue affects the software's custom text box control, which is used for entering the master password and other passwords during editing.



For the, and it's called "KeePass 2.X Master Password Dumper," proof-of-concept tool to work, you need some access to the system RAM through a system RAM snapshot, so on a Windows machine that would be the process dump, the pagefile.sys swap file, the hiberfil.sys hibernation file, or a RAM dump of the entire system.  The researcher whose work has generated so much click-bait said:  "The flaw exploited here is that for every character typed, a leftover string is created in memory.  Because of how .NET works, it is nearly impossible to get rid of it once it gets created."



So this sounds like a case where a developer, using a very high-level language framework, .NET, is being betrayed by the low-level way some of the language environment functions behind the scenes, in this case the environment's automatic string management.  It wasn't really meant for implementing super secure systems, at least not without giving it extra explicit thought.  There's just too much automatic stuff happening in the background.



You know how when entering a password in iOS the most recent character typed can be seen for confirmation while all previous characters are blanked out with a round bullet character?  It appears that this is what KeePass does, but that the custom control that its author initially created is discarding each of the intermediate strings, which leaves them floating around in RAM.



The exploit's developer said: "For example, when the password  'Password' is typed, it will result in these leftover strings:  a, which is the second character.  And then s, s, w, o, r, d."  He says:  "The proof of concept application searches the RAM memory dump for these patterns and offers a likely password character for each position in the password."  Except for the first character, which is not available.



The vulnerability affects the KeePass 2.X branch for Windows, and possibly for Linux and macOS. It has been fixed in the test versions of KeePass v2.54 whose official release is expected in two months, in July.  Presumably, in this updated release, Dominik is no longer using intermediate strings, or he's blanking the intermediate string before releasing its storage back to the environment so that released strings will not contain any sensitive information.



But stepping back from the details, the broader lesson here is to always keep a secure system's security model in mind.  There are things that the model provides and things that it doesn't.  The security model for a password manager is security and protection across the network.  No one acting remotely should be able to obtain any secret information.  In return for using a password manager, impossible-to-remember, or even enter, passwords can be used to provide today's highest generally available level of security for remote network logon. But only from across the network.



Password managers do not protect themselves against local attacks.  And they really cannot because users demand operating systems that are fundamentally insecure.  A truly secure operating system is no fun to use because you really can't get any work done.  Right?  You spend all of your time manually authorizing the system to do anything you want.  For a long time, users were running with root privileges because they'd grown tired of always needing to log off of their low-privilege account and log back on as a root administrator to install a program.  Then they'd have to switch back again.  Microsoft finally developed Windows' split-token UAC, right, the User Account Control system which minimizes the grief of normally running as a user who only has safe permissions.  It is a terrific compromise.  So yay.



But anyway, I was just astonished by the amount of nonsense headlines that was generated by this.  And poor old Dominik, I mean, he was quoted all over the place saying this is not a problem.  This is like everybody else does it.  And it's the reason he's not rushing out like some emergency update.  He's saying it's fine, and I'll be releasing this in July.  So everybody take a breath, you know, you don't want malware on your computer.  If you've got it, don't use anything.  Get rid of the malware.



Anyway.  AppleInsider reported that Apple has joined the growing ranks of companies that are banning the use of AI conversational language model AI inside the company.  And we know that Apple runs a tight ship.  Internal documents and anonymous sources have leaked details of Apple's internal ban on ChatGPT and similar technology, as well as some tasty bits regarding its inevitable plans for its own Large Language Model.



As we know, despite their popularity, chatting with Large Language Models can be unpredictable and do tend to leak the data that's being fed to them.  Even if dialog logging is disabled, this information is still leaving the local facility to travel out over the Internet.  It's not a friend you're sitting in front of, or in the box you're holding in your hand.  It's a massive remote cloud compute farm.  A report from The Wall Street Journal details an internal Apple document which restricts Apple employees from using ChatGPT, Bard, or any similar Large Language Models.  Anonymous sources also shared that Apple is working on its own version of the technology, though no details were provided.  And maybe we'll get some in Apple's big event next week.



We previously noted that Samsung had publicly banned their employees from using ChatGPT after three incidents where employees had uploaded very sensitive proprietary corporate information in order to get ChatGPT's opinion.  So Apple has also joined the ranks of Amazon, Verizon, and likely many other lesser-known organizations in telling their employees that there will be no "chatting with the bots" during work.



As for Apple's own internal Large Language Model work, I haven't heard anyone assembling a fully interactive speech interface for one of these textual chatterboxes.  But, boy, it sure does seem like the next obvious thing to do.  Being able to have a verbal conversation with this sort of simulated intelligence would take a voice assistant to an entirely new level.  So someone's going to do it sooner or later.  We'll see who gets there first.



Although we covered Google's next-generation user interest profiling technology in full detail when its detailed mechanism of operation was first announced, we haven't focused upon it for some time.  Yet Google has been diligently and quietly at work getting the technology ready for release.  Google will be rolling out this new Privacy Sandbox technology, their name for it, in the third quarter of this year, with the release of Chrome 115.  This rollout will take the form of a set of new built-in browser APIs that JavaScript code running in ads hosted on websites will be able to use to obtain categories of interest for the website's visitors.



The idea is that online advertisers will be able to deliver ads without collecting any personal information about users and without tracking their movements across the web using third-party cookies.  Google has said that, as the rollout proceeds, it will also be removing support for third-party cookies from Chrome next year.  This will be done incrementally and through careful testing.  Initially, in the first quarter of 2024, support for third-party cookies will be removed from 1% of Chrome's user base.  And by the third quarter of 2024, Chrome and its users will be 100% third-party cookie free.



The message of course to advertisers is we're going to be releasing the API.  You'd better switch over to it because the number one browser on the Internet that has the lion's share is going to be preventing all of the tracking technologies that it's able to in favor of this new API, which will be in the Chromium core, which I can imagine then other browsers may choose to support or not.  We'll see.



I really want to believe, Leo, that the agents of the FBI are good, law-abiding people.  I really do.



LEO:  Yes.  Yes.



STEVE:  And for the most part, I'm sure they are.  I wonder how many of our listeners know the name Efrem Zimbalist, Jr.



LEO:  Oh, I do.



STEVE:  Yup.



LEO:  Because we're old.



STEVE:  We're old.  He was the lead in "The F.B.I.," which was a television series I grew up watching every Sunday night from 1965, when I was nine, and I think that makes you seven, Leo, through 1974.



LEO:  Yup.



STEVE:  In any event, recently declassified documents have revealed that the real U.S. Federal Bureau of Investigation improperly searched the personal communications of Americans more than - is everyone sitting down? - more than 300,000 times between 2020 and early 2021, according to, as I said, newly declassified documents which were released by the Senate Intel Committee.  I don't know any details, but it seems disappointing.  And I'm quite sure that Efrem Zimbalist, Jr. would have had no part in that.



LEO:  I am sure he didn't even know what a pen register was.  I am sure.



STEVE:  No.  The inappropriate searches occurred under FISA, our Foreign Intelligence Surveillance Act, which may have been, you know, somewhat inappropriate for U.S. citizens.  You know?



LEO:  Good point, yeah.



STEVE:  That foreign part?  FISA is currently up for renewal by Congress.  So, you know, it keeps getting renewed.  I imagine it will again.  The documents reveal that the FBI used the massive trove of communications data gathered using FISA powers to search for information on participants who attended the George Floyd protests - ouch - and the January 6 storming of the U.S. Capitol.  The FBI also searched the names of more than 19,000 donors to political campaigns for plausible connections to foreign governments.  Hearing this causes me to think that, even though my own communications are quite boring, it's my right as a U.S. citizen for them to remain truly private unless and until a court orders otherwise.  So utterly unbreakable end-to-end encryption?  Yes, please.



Over in the increasingly fraught open-source registry supply chain world, things have grown so bad that the Python Foundation was finally forced to suspend the creation of new accounts and new packages.  The Foundation has suspended all new user registrations as well as new package uploads to PyPI, the official Python Package Index.  The organization's security team says PyPI saw another wave of malicious packages being uploaded on the platform over the past week.  PyPI staff says the incident occurred while some of its members were on leave, and that the smaller group of personnel on duty were quickly overwhelmed by the large number of reports and malicious packages they had to remove.  So they threw the switch and said "No more of anything" for the time being.  Existing users can still access and update their projects.  It's just new accounts and new projects that are barred for the time being.  What was that about why we can't have nice things?  I'll have more to say about these disturbing trends in this week's closing comment.



But first, I have some interesting news about a longstanding SpinRite 6.0 bug to share.  It turns out that my claim that SpinRite 6.0 has had no known bugs for the past 19 years has not been correct.  Though I suppose it would depend upon how you define "known."  Certainly it hasn't had any "appreciated" bugs, but it definitely has had a bug.  And thanks to the work of an independent coder named Paul Farrer (F-A-R-R-E-R), GRC is now offering a pair of patch utilities which fix this bug that SpinRite has had, perhaps since the mid-'90s, with SpinRite 3.1.



Since it only occurs on drives larger than 549GB, when SpinRite's DynaStat system kicks in to perform data recovery and repair, and since this behavior has been present since at least SpinRite 3.1, what is now seen as an overflow was very likely my deliberate design decision at the time, since drives of that size were not even a dream back when 50MB was a large drive.  So what likely happened was that as I evolved SpinRite through the decades, I never revisited the parameters surrounding this one division operation to notice that modern drives might now cause it to overflow.



Through the years, we've had reports of SpinRite halting with a division overflow error.  Somehow, I got it into my head that the location reported by SpinRite of this overflow was the segment where the error was occurring, not the offset.  So "B04E" would not be in program space.  It's in the region of memory that was once set aside for the monochrome display adapter, which is now completely unused.  So I assumed that this error was occurring in a chunk of code that the system's BIOS had mapped into that unused region.  And this belief was supported by the fact that GRC's tech support guy, Greg, has developed a collection of workarounds for SpinRite's users who encounter this error, things like "Try running SpinRite with that drive on another machine" - which he says often works.



In fact, over the weekend, when I wrote to him to tell him that we had a patch for this longstanding problem, his reply was:  "Since we are getting closer to 6.1, I'll probably use this as the last thing to try as all the other 'fixes' we have in place are much less technical."  So my point is, this hasn't been a big deal or issue for us.  But I do know for certain that it has been so for some users, and that's not okay.  So I also now understand why moving to a different machine may have helped, since part of the issue surrounding the BIOS's mapping of cylinders, heads, and sectors to a drive's linear sector number was what was going on.  And that mapping is one of the many things that different BIOSes might do differently.



In any event, Paul had just finished developing a different patch for all of those buggy AMI BIOSes which I talked about before that we discovered were blasting main memory when anything attempted to access sectors past 137GB on USB-connected drives.  Out of an abundance of caution, which I feel is warranted, SpinRite 6.1, the forthcoming SpinRite, will refuse to go any further than the first 137GB on any USB drive.  But Paul had access to our newsgroup, which was full of people who had machines with these buggy, it turned out to be AMI only, BIOSes.



So working with those users, he's created a tiny utility that can be run before SpinRite, that is, before SpinRite 6.1.  If it finds an AMI BIOS that it recognizes, it patches it.  And then, with my blessing - we've discussed how this should be done to be stable - his utility will remain in RAM and cause SpinRite 6.1 to believe that USB devices are SCSI devices, thus lifting 6.1's cautionary clamp on USB drive size.  So that little utility will be made available for users to use at their own risk if they choose.  SpinRite 7 will not use any BIOS, so all of this will be going away as soon as we move there.



After doing that work, Paul became curious about that B04E error.  Without my bias of assuming that this was the segment of the problem, and therefore in the BIOS and not in SpinRite, he assumed that I had been reporting the offset - as I was.  So he looked into SpinRite's running, in-memory code; and, sure enough, he found a division instruction at that offset in SpinRite.  He then proceeded to reverse engineer that region of SpinRite's code to figure out what was going on there and why.



At one point I provided him with the relevant chunk of SpinRite's source so that he could be sufficiently confident that he knew exactly what was going on.  He had it exactly right.  Drives had become much larger.  And the math that I had not revised for decades, which decomposed a linear sector number into cylinders, heads, and sectors for the BIOS was no longer able to handle today's larger drives.



So Paul has produced another patch utility which fixes this problem for SpinRite 6.0.  He created both a DOS driver which can be loaded through CONFIG.SYS, and a DOS TSR - terminate and stay resident - that can be run before running the current version of SpinRite 6.  After testing it thoroughly he provided me with his source code to review, and it was immaculate.



So thanks to his efforts we have a patch for this bug that's always been in SpinRite since its very early days.  Out of curiosity, I checked SpinRite's source code for SpinRite 5.0, which is dated February 11th, 1996.  And it's the same code that SpinRite 6.0 is still using.  So I never changed it for SpinRite 6.0 since again, at the time, it was not a bug.  But it is today.



Hitting this error, which can only occur on drives over 549GB, and only when DynaStat engages, does not endanger or damage any of a user's data.  But it does mean that SpinRite cannot proceed with its recovery and repair.  So my advice to all SpinRite users listening to this podcast would be to grab Paul's patch and to add it to SpinRite's boot media.  The file is called MDFYSR60 - as in modify SpinRite 6 - .zip, and it has a menu entry under GRC's Main Menu under SpinRite and Knowledge Base B04E.  It's also in our freeware collection.  And it's now referred to near the top of the SpinRite FAQ.  My hope is that anybody who hits the problem will be able to patch SpinRite and then move on.  And of course I've told Greg about it. 



As I was writing this up for today's podcast, I suddenly became curious about what the code for this looks like now, since a lot of SpinRite is still the same.  But we already know that 6.1 doesn't contain this bug.  We've all been running lots of DynaStat recoveries on lots of multi-terabyte drives without any trouble for the last six months.  So I was pleased to see when I went to look that I had completely replaced that old code with a new routine which is capable of handling a full 64 bits worth of sectors.  That's 18,446,744TB.



LEO:  Oh, you'll be okay for a while.



STEVE:  Yeah.  More than 18 million terabytes.



LEO:  What is that, 18 yottabytes or...



STEVE:  Oh, who knows?  So, even though releases of SpinRite tend to live for a long time, we should be good for a while.



LEO:  I think you'll be all right, yeah.



STEVE:  I think we'll be okay.



LEO:  And you'll know where to look if suddenly 18 yottabyte drives come out.



STEVE:  That's right.



LEO:  You'll know what to work on.



STEVE:  Since this bug is already long gone from SpinRite 6.1, since 6.1 is so vastly superior to 6.0, is nearly finished, will be free to all 6.0 owners, and since this is never destructive to any SpinRite user's data, I plan to get 6.1 finished, rather than delay it in an attempt to announce this patch to all of 6.0's current owners.  After I put all this online Sunday night, I updated Greg so that he'll be pointing anyone who encounters this problem to the patch.  And when I do announce 6.1, I'll also inform 6.0's users of this patch so that they'll have it for 6.0, just for the record, even though it will largely become obsolete, as will 6.0.  So a big public thanks to Paul Farrer for his terrific work on this.



LEO:  Wow.  That's nice.



STEVE:  One final...



LEO:  Just a fan; right?



STEVE:  Yeah.



LEO:  Just a user; yeah.



STEVE:  Yup.  Just a talented coder.  One final point:  Something else I had forgotten from 19 years ago which recently came to light was that for a couple weeks after SpinRite's initial public availability, its first time that it went online, I was still finding and fixing some final bits of debris.  No showstoppers, apparently, or I would have made a big deal about it.  I mean, I was posting publicly in the newsgroups about this.  Everyone knew.  But I was updating SpinRite's downloadable code on the fly.  I didn't have the mature version-stamping system that all of my recent work carries, and which SpinRite 6.1 will have, so all of those early editions just say 6.0 without any indication of any sub version or build number.  Nothing has changed since in 19 years.  But if you believe that you may have downloaded SpinRite 6 within weeks of its first release in 2004, and never again since then, you might want to update your copy until 6.1 is ready.  I'm just saying.



And Leo, let's take our last break, and we're going to look at the emergence of Voice Cloning as a Service for, you know, the ne'er-do-wells.



LEO:  Something we've needed for so long.  So glad.  I did the voice cloning, and Apple's going to offer this, as well, but it was for a good reason.  If you have ALS or some other debilitating disease, and you can't talk anymore, it's good to record your voice now, like Stephen Hawking had, so that if you...



STEVE:  Apple's going to announce this?  



LEO:  They did announce it.  It'll be in the next version of iOS.  And there's a company called Acapela.  I just did this with Acapela.  You record, in the case of Acapela, about 200 phrases like "Could somebody open the window" and "Please be patient, I'm typing this."  It does it in your voice.  The idea if you have to use a voice synthesizer as Hawking did.  He intentionally chose a robotic voice, but you could use your own voice, and the theory being that would make it be a little more appealing.  It's not good enough, at least I don't - I haven't heard the Apple one.  Acapela is not good enough to fool anybody.  But the Apple one, you know, it's Apple.  It might be.  Wouldn't be surprised.



STEVE:  Well, and we know where this is going, Leo.  It's going to be good enough to fool anybody.



LEO:  Oh, sure, yeah.  Oh, sure.



STEVE:  I assumed that Hawking was using that because he'd had it for so long.



LEO:  No, he chose.  He chose.  And I can't remember the rationale.



STEVE:  Because it's creepy.



LEO:  Yeah.  I think he didn't want it to sound like him.  He wanted people to understand it's a synthesized voice.  I'll look it up.  I remember this conversation.



STEVE:  Well, you're good with your facts, so I'm sure you're right.



LEO:  We also talked about it on Sunday because one of our guests had tried the Apple product and was talking about that.



STEVE:  I know from our communication with our audience, I mean, it's a great, high-end group.



LEO:  Oh, they're sophisticated, absolutely, yeah, yeah.  Very smart.



STEVE:  Okay.  So I suppose it was inevitable, if we follow the "anything that can be done will be done" rule.  My original title, as I mentioned for today's podcast is was "News, Views, and No Snooze."  I think that's been delivered.  Which I felt was not particularly inspired, but accurate.  That was right up until I hit upon the news that the barrier to entry for malicious voice cloning had just dropped with the inevitable, in retrospect, appearance of Voice Cloning as a Service.  So although today's coverage of this will not be a deep dive, I thought it merited some special attention, thus we gave it the title of the podcast.



Last Thursday, the security firm Recorded Future published a 16-page Cyber Threat Analysis report which was titled "I have no mouth, and I must do crime."  A link to this full PDF is in - that's a great title, yeah.



LEO:  It is.



STEVE:  "I have no mouth, but I must do crime."  I've got a link to the PDF in the show notes for anyone who wants to read the whole thing.  It's got lots of detail.  I'll just share the executive summary which it leads with.



They wrote:  "Deepfake voice cloning technology is an emerging risk to organizations, which represents an evolution in the convergence of artificial intelligence threats.  When leveraged in conjunction with other AI technologies  such as deepfake video technology, text-based large language models such as GPT, generative art, and others  the potential for impact increases.  Voice cloning technology is currently being abused by threat actors in the wild.  It has been shown to be capable of defeating voice-based multifactor authentication, enabling the spread of misinformation and disinformation, and increasing the effectiveness of social engineering.  We are continuously monitoring the emergence of deepfake technologies and their use in cybercrime, as detailed in our April 29, 2021 report 'The Business of Fraud:  Deepfakes - Fraud's Next Frontier.'



"As outlined in our January 26, 2023, report 'I, Chatbot,' open source or 'freemium' AI platforms lower the barrier to entry for low-skilled and inexperienced threat actors seeking to break into cybercrime.  These platforms' ease of use and out-of-the-box functionality enable threat actors to streamline and automate cybercriminal tasks that they may not be equipped to act upon otherwise.  Many of the voice cloning platforms referenced in this report are free to use with a registered account, thus lowering any financial barrier to entry for threat actors.  For those that are not free, premium prices are available, rarely more expensive than $5 per month.



"Voice cloning samples that surface on social media, messaging platforms, dark web sources often leverage the voices of public figures  such as celebrities, politicians, and Internet personalities (influencers)  and are intended to create either comedic or malicious content.  This content, which is often racist, discriminatory, or violent in nature, enables the spread of disinformation, as users on social media are sometimes deceived by the high quality of the voice cloning sample.  This 'proof-of-concept' work shared by threat actors has inspired a trend on dark web and social access sources, with threat actors speculating about the emergence of voice cloning as an attack vector.  Conversations among threat actors often reference executive impersonation, callback scams, voice phishing, now known as 'vishing,' and other attacks that rely on human voice.



"One of the most popular voice cloning platforms on the market is ElevenLabs' Prime Voice AI, a voice-based, text-to-speech software that allows users to upload custom voice samples for a premium fee.  While there are a number of voice cloning platforms referenced in this report such as MetaVoice, Speechify and so on, ElevenLabs is one of the most accessible, popular, and well-documented, and thus serves as the case study for this research."



Then they wrap up their summary with five bulleted key findings:  "First, voice cloning technologies such as ElevenLabs lower the barrier to entry for inexperienced English-speaking cybercriminals seeking to engage in low-risk impersonation schemes and provide opportunities for more sophisticated actors to undertake high-impact fraudulent schemes.  Second, currently the most effective use of voice cloning technologies is in generating one-time samples that can be used in extortion scams, disinformation, or executive impersonation.  Limitations to the use of voice cloning technologies, especially for enabling real-time extended conversations and generating prompts in languages other than English, mean that extensive planning is required for fraudulent operations with a higher impact.



"Third, threat actors have begun to monetize voice cloning services, including developing their own cloning tools that are available for purchase on Telegram, and the emergence of Voice Cloning as a Service.  Fourth, public interest in AI, including voice cloning technology, has prompted an interest on dark web and special-access sources in AI platforms' potential for abuse.  Threat actors are also interested in leveraging multiple AI platforms in concert, thus enabling the convergence of AI threats.  However, not all threat actors are confident in their ability to leverage such platforms in their current state.  Threat actors that do not have an expert grasp of the English language may have the greatest hesitation about using these new technologies.



"And finally, in order to mitigate current and future threats, organizations must address the risks associated with voice cloning while such technologies are in their infancy.  As these technologies will only get better over time, an industry-wide approach is required immediately in order to prevent further threats from future advances in voice cloning technology."  And sadly, to that I say, well, good luck with that.



I went over and took a look at ElevenLabs.io, which is their URL, E-L-E-V-E-N-L-A-B-S dot io.  They call their web-based service "Prime Voice AI:  The most realistic and versatile AI speech software ever.  Eleven brings the most compelling, rich, and lifelike voices to creators and publishers seeking the ultimate tools for storytelling."  So we need to be clear that there are, and will be, all manner of public-benefiting wonderful applications for this emerging technology.  But that, once again, as always, along with the good comes the bad.



They mention applications - "they" ElevenLabs - for storytelling, audiobooks and news articles.  They boast having indistinguishable speech quality.  They claim:  "Our AI model is built to grasp the logic and emotions behind words.  And rather than generate sentences one by one, it's always mindful of how each utterance ties to preceding and succeeding text.  This zoomed-out perspective allows it to intonate longer fragments convincingly and with purpose.  And finally you can do this with any voice you want."



So I played a bit with their online tool, and I have to agree that it was producing far better, more believably human, less robotic-sounding speech than we normally hear from text-to-speech.  But I wouldn't say that it's at the point where it's going to fool anyone who might be at all suspicious.  As for cloning well-known voices, under their "Voice Lab" they say: "Voice Lab is your creative AI toolkit.  Clone voices from samples or clone your own voice.  Or design entirely new synthetic voices from scratch.  Our cloning model learns any speech profile based on just a minute of audio, without additional training.  And our generative model allows you to create completely new voices that never spoke before."



And then at the bottom of the page they do have a statement under the topic "Ethical AI," where they write:  "At Eleven, we believe that we should strive to make the most of new technologies, but not at all costs.  As we develop them, we make every effort to implement appropriate safeguards which minimize the risk of harmful abuse.  With this in mind, we're fully committed both to respecting intellectual property rights and to actioning misuse."



So I want to end with this observation.  A macro trend that we appear to be seeing with increasing frequency is that bad guys are now jumping onto and exploiting new technologies every bit as quickly as the good guys.  Over the lifetime of this podcast we have watched "doing bad on the Internet" evolve from being a curiosity, perhaps a basement hobby, into truly full-blown industries.  Under our watch, the dark web went from being a fanciful idea, you know, just a theory, to a widely inhabited, active, and quite busy underground.  So it's going to be interesting to see what comes next.  



LEO:  I mean, I don't know how you - you can't shut it down.  It's not going to stop.



STEVE:  No.  There's no way.  It's like the genie that's out of the box, Leo.



LEO:  It's like deepfakes.  It's like photoshop.  It's here.



STEVE:  I agree.  I agree.



LEO:  So what you've got to do is just train people that you can't trust voices.



STEVE:  Yes.  Raise a level of awareness that trust no longer means identity.  I mean, the voice...



LEO:  Voice, yeah.



STEVE:  ...no longer means identity.



LEO:  I mean, Charles Schwab uses voice ID to log in.  The companies that are doing that need to stop, I would suggest.  Maybe, I don't know, but I think, you know, look, it's here.  We've used ElevenLabs to generate my voice.  I have a AI Leo that has my voice.  Spotify just announced through the ringer that they're going to use AI voices to have host-read ads that weren't read by the host, but sound like they were.



STEVE:  It'll be nice to have audiobooks being better.



LEO:  Oh, yeah.  They already are.  Apple has AI-generated audiobooks using I think very high-quality voices.  They sound very good.  And you can buy, if you look at the Apple Bookstore, the audio bookstore, it'll say some of them, I can't remember exactly the wording, but it's generated by computer.  And they're very good.  And that means there's a lot of books that would not have an audio version will now.  Which is great.



STEVE:  Right.



LEO:  As always, Steve, you cover the changing world around us.  This is the place to go to find out what's happening.



Copyright (c) 2023 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#925

DATE:		May 30, 2023

TITLE:		Brave's Brilliant Off the Record Request

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-925.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week, before we address what I think is a brilliant new idea from the Brave Browser's Privacy Team, we're going to see why people are suggesting that the initials HP stand for "Huge Pile."  What was Google thinking when they created the .zip TLD that no one was asking for?  How has the Python Foundation responded to attacks and subpoenas?  Do we believe a VPN service when it promises that no logs are saved anywhere?  Will Twitter be leaving the EU?  Does Bitwarden now support Passkeys?  Who just got fined 1.2 billion euros, and why so little?  What feature did WhatsApp just add?  And what's the story about Google's new bug bounty for their Android apps?  Then, after answering those questions, and a brief bit of good news about SpinRite, we're going to look at Brave's Brilliant "Off the Record" request concept and new feature.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here with lots of great things to talk about:  why the .zip TLD is a really terrible idea; why Meta got fined $1.2 billion by the Irish and what they're going to do about it.  And then Steve's going to applaud a really nice move from Brave to improve your privacy.  It's all coming up next in Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 925, recorded Tuesday, May 30th, 2023:  Brave's Brilliant Off the Record Request.



It's time for Security Now!, the show where we cover the latest news in the security sphere.  And that's thanks to this guy right here, Mr. Live Long and Prosper, Steve Gibson.  Hi, Steve.



STEVE GIBSON:  And you know, Leo, we can make a whole meal out of today's Picture of the Week.



LEO:  I haven't seen it.  You told me, "Do not look at it until we're live."  So I have not looked.



STEVE:  Only because I want our audience to be able to experience you seeing this for the first time.



LEO:  Oh, dear.



STEVE:  So that they will be incented to get it themselves, to track it down in the show notes because this is the most inspired thing, I mean, I didn't think anything was going to beat the green ground wire stuck into the pail of dirt.



LEO:  I did enjoy that, yes.  I did enjoy that.



STEVE:  Yeah.  This one, this is cleverness to a whole new level.



LEO:  Is it a visual pun again?



STEVE:  It is.  It'll take you a minute to visually parse it.  You'll look at it, and you'll go, what?  And then it's like, OMG.



LEO:  Okay, I can't wait.  



STEVE:  So today, this is Podcast 925 - I will next see you in June; this is our last podcast of May - titled "Brave's Brilliant Off the Record Request.  Rarely do we come across something that is a simple idea that's new and really offers value.  And the Brave - Brave as in the Brave browser.  Their privacy team came up with something that I am really hoping that the other browsers adopt, and this becomes an industry standard.  So we'll have a lot of fun talking about that.  But first we're going to see why people are now suggesting that the initials "HP" stand for Huge Pile.  We're going to ask what Google was thinking when they created the .zip top-level domain, which nobody asked for.



LEO:  Nobody asked for.



STEVE:  Like nobody said, oh, gee, that's what we really need.  How has the Python Foundation responded to attacks and subpoenas?  Do we believe a VPN service when it promises that no logs will be saved anywhere?  Will Twitter be leaving the EU?  Does Bitwarden now support Passkeys?  Who just got fined $1.2 billion euros, and why so little?  What feature did WhatsApp just add?  And what's the story about Google's new bug bounty for their Android apps?



Then, after answering all those questions, and sharing a brief bit of good news about SpinRite, we're going to look at Brave's brilliant Off the Record request concept which is a new feature that I hope, as I said, that the industry will adopt.  And after you tell us about our first sponsor, Leo, you're going to be able to pull back the curtain on probably one of the best Pictures of the Week this podcast in its, I don't remember when we began the Pictures of the Week, but it's been decades.  This is a topper.



LEO:  That's saying a lot.  The best ever.  All right.  I am ready.  I'll tell you what.  Let's do this all together.  Everybody, oh, I've got to switch this over.  All together now, we're going to look at the Picture of the Week.  I might have to pull it up over here.  Oh, no, there it is.  Okay, you ready?  We're going to look together.  I'm going to scroll - oh, wait a minute.  Do you want to see me as I scroll up?



STEVE:  No, we'll be hearing you.



LEO:  You'll hear me?  Okay.



STEVE:  It's pretty good.



LEO:  We're going to watch together as we scroll up "The old-school way to add remote control [laughing] to a light switch."  Well, well, well.  I bet it works.  Even when the power's out it works.



STEVE:  So if you don't want Russia or China to have control of the power...  



LEO:  If you're on the Battlestar Galactica, and you don't want the Cylons to get into your grid.



STEVE:  I think if we were to rewire the U.S. electrical grid using this technology, then there wouldn't be any concern about, like, IoT and SCADA attacks and so forth.



LEO:  So tell us how this works.



STEVE:  It's just brilliant.  So this would be if our grandfathers were inventive, and the switch that they needed to control was in the other room.  It was like on the wrong side of the wall.  So, huh, what do you do?  So you drill two holes through the wall about six inches apart, one so that it comes out above the light switch and the other one so it comes below the light switch.



LEO:  Yeah, yeah.



STEVE:  Then you run some half-inch PVC tubing through and put elbows on the ends in order to bend it down from the top and bend it up from the bottom.



LEO:  It's a little over-engineered, but it gets the job done.  Okay.



STEVE:  Then you thread a piece of string all the way through this contraption, drill a hole through the bat toggle of the light switch, tie the string to it, and then the string continues on down and back through the wall.  So on the operator side of the wall...



LEO:  It's two strings.



STEVE:  You've got two strings, yeah.  And you pull the upper string, and it goes through the wall, pulls the switch up, and you notice while you're pulling the upper string that the lower string kind of goes in a little bit.



LEO:  Yeah.



STEVE:  So it's like, okay.  And now the lights are on, apparently in the room where you are, right, because apparently - it must be that this switch is controlling the wrong room.  So someone said, huh.  What are we going to do about that?  You know, we want to control the lights in our room.



LEO:  Hysterical.



STEVE:  So anyway, yeah, this is just...



LEO:  Oh, my god, Steve, you've got a winner.  You've got a winner.



STEVE:  Well, we have to thank - one of our listeners saw this and thought, okay, this is definitely going to make it for Picture of the Week.  And they were correct.



Okay.  So I had never heard it suggested that HP stood for "Huge Pile" until I went to catch up on the current state of those HP OfficeJet Pro 9020e series inkjet printers, as we know, all of which were effectively destroyed at the firmware level more than now three weeks ago.  That was on the morning of Monday, May 8th, so three weeks and a day, right, 22 days ago.  As a consequence of an aberrant autonomous firmware download and install.  And to add insult to injury, it may be that the entire purpose for this badly failed update was simply to tighten HP's grip over the use of non-HP printer ink.  But whatever the motivation, the entire world, which has now been waiting, as I said, for 22 days, is filled with HP-branded dead printers which no longer print - using anyone's ink.



And what's becoming more and more glaring with each passing day,  now at 22, is that, incredibly, there has been very unsatisfying communication from HP about this issue.  Any call to HP Service replies that HP is aware of the issue and, fear not, is working diligently on a solution.  But we're talking three weeks now, and a day.  So no matter what solution is eventually forthcoming, this has not been very impressive responsibility-taking on HP's part.  I fully expected that when I checked back for today's podcast I would get news of, oh, here's what we're going to do.  Static.  Silence.  Nothing has been said, which is just - it's astonishing that HP says, oh, yeah, go to whatever it is, support.hp.com and join the crowd.  Wow.



Okay.  So not quite four weeks ago, back on May 3rd, Google, which is now a fully fledged domain name registrar, announced their eight new top-level domains with a posting that was headlined "Eight new top-level domains for dads, grads and techies."



So the TL;DR on this is "Google Registry launches eight new top-level domains:  .dad, .phd, .prof, .esq [short for esquire], .foo, .zip, .mov, and .nexus."  For the dads they said:  "Knock, knock.  Who's there?  With Father's Day right around the corner, .dad is here for the jokes, the games, and the advice.  Whether you're a fit.dad, a gay.dad, or a dude.dad, .dad is the place to celebrate fatherhood."  Leo, I think maybe they've gotten a little carried away over there in the Google domain registration world.



Anyway, they said:  "Check out these interesting .dad websites."  We've got Classic.dad, where you get to play a fun eight-bit game where your goal is to successfully mow the lawn while dodging pets and obstacles, and preventing weeds from spreading.  Or Dear.dad.  They say:  "Check out this media platform dedicated to telling stories of Black fathers."  Or Daily.dad.  It's a new book from Ryan Holiday that provides 366 accessible - you know, in case it's leap year - meditations on parenthood, a manageable slice for each day.



Okay.  So for the grads they're saying:  "May means graduation season for many in higher education.  We're celebrating graduates and the professors who taught them well by launching .prof, .phd, and .esq.  Whether sharing legal advice for everyday life or teaching courses on behavioral science, these new domains are perfect for showing off your credentials.  Hats off to these early adopters," says Google.



And so we have Erika.esq.  And they tell us that "Erika Kullberg is an attorney and money expert who is passionate about better positioning people for success."



LEO:  Lot of attorneys would use esq.  That would be popular, yeah.



STEVE:  Right.  And so I think this brings a whole new meaning to the notion of vanity domains.



LEO:  Well, exactly.  When I started using weird domains, in other words, anything but .com, .net, and .org, people would always say, like I had Leo.fm.  They'd say, oh, you mean Leo.fm.com.  And I said no.  And then you'd also see sites that would reject emails because they said, well, there is no .email domain.  Yeah, there is, yeah.



STEVE:  Right.  So we also have Casey.prof.



LEO:  Which is good, for professors.  That's fine.



STEVE:  Yeah.  And that's Professor Casey Fiesler.



LEO:  Or proofreaders would be good for.  Maybe Elaine would like that.



STEVE:  Well, she would complain about the lack of two o's.  You really, you know, proof with one O?



LEO:  That's not good.



STEVE:  So Casey Fiesler is a technology ethics educator and science communicator, who apparently thought, hey, I need my own prof domain.  And then we have Rafael.phd.  Rafael is an expert in post-quantum cryptography.  Fully homomorphic...



LEO:  Well, he deserves his Ph.D.



STEVE:  Yeah.



LEO:  Holy cow.



STEVE:  Well, also if you understand how fully homomorphic encryption works, you're a doctor.



LEO:  Yeah.



STEVE:  And also privacy-enhancing technologies, and the application of these constructions.  So, okay.  But the real worry, and the reason for my bringing this up today is the new TLDs that Google chose to create for techies, that third category.  Under the heading of Techies, they write:  "May is also the month of Google I/O [as we know now], our annual developer conference.  Whether you're learning to code, deploying a helpful tool, building your portfolio, or starting a new community, .foo, .zip, .mov, and .nexus have you covered.  Here are some examples from our developer community."  So we've got gamers.nexus.



LEO:  Which is their actual name.  That's the name of the company, the publication, yeah.



STEVE:  Right.  Yeah, they said:  "Use gamers.nexus to review computer hardware and plan your next gaming PC."  Or "helloworld.foo:  Learn how to code 'hello world' in...



LEO:  You'd better explain why "foo" is for coding Hello World, though.



STEVE:  Leo, you tell us.  You'll have fun with this.



LEO:  All right.  So there is an old military acronym FUBAR, effected up beyond all repair, I guess, I don't know.  FUBAR.  And so that's been around since World War II, right, along with SNAFU and a few other choice military - recognition.  FUBAR is beyond all recognition.  Okay.



STEVE:  Yes, recognition.



LEO:  So programmers, when they were making up dummy variables in their texts, like Dennis Ritchie and Brian Kernighan and the C programming language, would use FU and BAR as...



STEVE:  Correct.



LEO:  ...phony dummy variable names.  But my question is, there's a .foo.  Where is the .bar?



STEVE:  Yeah.



LEO:  Foo without bar is like a...



STEVE:  You would think they would want to raise the bar, Leo.



LEO:  .Foo is fun.  I don't know who's going to use .foo.  I might.  Maybe should I go get Leo.foo?  Maybe I will.



STEVE:  Yeah, you don't have enough.



LEO:  I pity the foo.



STEVE:  And url.zip, they say.



LEO:  Uh-oh.  Uh-oh.



STEVE:  So there is apparently a domain, url.zip.



LEO:  Uh-oh.



STEVE:  They said:  "Create short, powerful, and trackable links with url.zip."  So I guess somebody created, registered the domain and created a URL shortener called url.zip.  And then, finally, david.mov, which is "Watch videos by David Imel in this liminal space."



LEO:  Well, that's a $5 word.



STEVE:  Oh, boy.  So, yeah.



LEO:  Okay.



STEVE:  And just to finish off their posting, before we look at not only what could possibly go wrong, but what did go immediately go wrong, catastrophically, Google finished, saying:  "Starting today" - this was like May 3rd; right?  "Starting today, you can register all of these new extensions as part of our Early Access Program for an additional one-time fee.  This fee decreases according to a daily schedule through the end of May 10."  So the 3rd to the 10th, so that's seven days.  So presumably on day one you had to really want one of these in order to belly up to the bar and pay whatever they were asking, which dropped down successively on each day until it was back to the normal price on May 10th.



They said:  "All of these domains will be publicly available at a base annual price through your registrar of choice."  Although I went over to Hover, and they don't offer ZIP, so they said, "Huh?  What?  No."  



"To make it super easy for anyone to get their website live, we've worked with Google Sites to launch new templates for graduates, professors, and parents."  Oh, good, you don't even have to do any, like, website coding, just drop a template in.  "To learn more about pricing or our participating partners, visit registry.google."



Okay.  So it occurred to me that, if those grads are looking for jobs in Internet security, protecting the assets belonging to those dads, then Google's difficult-to-understand decision to offer domains under the .zip top level domain may create the full employment guarantee that those new grads have been looking for.



The malicious exploitation of these .zip TLDs took exactly zero time since their abuse was so obvious to everyone except for, apparently, Google.  The new phishing technique is already going by the name "file archiver in the browser," as just one of many examples.  It can be leveraged to "emulate" file archiver software in a web browser when a victim visits a .zip domain.



"mr.d0x" is a security researcher we haven't quoted in some time, although we did some time ago.  He recently said:  "With this phishing attack, you simulate file archiver software, for example, WinRAR, in the browser and use a .zip domain to make it appear more legitimate."  In other words, threat actors are creating realistic-looking phishing landing pages using modern web tools - HTML, CSS, JavaScript - which mimics legitimate file archive software, hosted on a .zip domain, to elevate social engineering campaigns.  The point is when you click on a zip file, and most people don't know if it's a single-click or a double-click, so they go with two because, you know, that's better than one.



So whether you single-click or double-click, if it's a URL, you're going to be taken to a site ending in zip, and the site is going to pop up and look like an archiving program.  So you think you've opened the zip file on your local machine, when in fact you've opened your browser mimicking an archiver, and now you're under the control of this site when you click anything else that you want to use the archiving software for.



So the search bar in the Windows File Explorer will also emerge as a sneaky conduit where searching for a nonexistent .zip file opens it directly in the web browser should the file name correspond to a legitimate .zip domain.  Our Mr.d0x said:  "This is perfect for this scenario since the user would be expecting to see a zip file.  Once the user performs this, it will auto-launch the .zip domain which has the file archiver template and is able to appear legitimate."



The problem, of course, that created all this is that the new .zip and .mov top level domains are also both legitimate file name extensions.  So this invites confusion when unsuspecting users mistakenly visit a malicious website when they believe they've opened a file.  They could then be misled into downloading malware.



Trend Micro agrees that this is anything but a good idea.  They said:  "Zip files are often used as part of the initial stage of an attack chain, typically being downloaded after a user accesses a malicious URL or opens an email attachment.  Beyond ZIP archives being used as a payload, it's also likely that malicious actors will use zip-related URLs for downloading malware.  The crux of the concern," they said, "is that with the introduction of TLDs that are identical to well-known file extensions, bad guys are going to cook up clever new ways to take advantage of this resulting confusion and ambiguity.  If nothing else, it clearly equips bad actors with another new vector for phishing."



Then I got a kick out of Malwarebytes Labs.  I mean, the tech security industry just went nuts over the idea that we now have a .zip TLD because it was so clear to everyone how bad this was going to be.  Malwarebytes Labs, which titled their recent discussion of this ill-advised move by Google "Zip domains, a bad idea nobody asked for."  They wrote:  "If you heard a strange and unfamiliar creaking noise on May 3rd, it may have been the simultaneous rolling of a million eyeballs.  The synchronized ocular rotation was the less than warm welcome that parts of the IT and security industries," he said, "this author included, gave to Google's decision to put .zip domains on sale."



Okay.  So then the author had some additional useful things to say which I think is worth sharing.  He said:  "Domain names and filenames are not the same thing, not even close; but both of them play an important role in modern cyberattacks, and correctly identifying them has formed part of lots of basic security advice for a long, long time.



"The TLD is supposed to act as a sort of indicator for the type of site you're visiting.  .Com was supposed to indicate that a site was commercial, and .org was originally meant for non-profit organizations.  Despite the fact that both .com and .org have been around since 1985," he says, "it's my experience that most people are oblivious to this idea.  Against that indifference, it seems laughable that .zip will ever come to indicate that a site is "zippy" or fast, as Google intends."  And it's like, so that's what zip - it's like, oh, it's because it's zippy.  Okay.  Quoting Google:  "When you're offering services where speed is of the essence, a .zip URL lets your audience know that you're fast, efficient, and ready to move."



LEO:  Oh, please.  Well, but anybody can do it.



STEVE:  That's literally what they said, yes.  There's no - you don't have to pass a speed test, Leo, in order to get the zippy .zip URL.



LEO:  That's just a moron copywriter who wrote that; right?  Just some marketing idiot.  Because obviously it's dopey.



STEVE:  Yes, it is.



LEO:  But I wonder what the technical thinking is.  Maybe, whoa, let's make some money.



STEVE:  I just have no idea.  He says:  "Meanwhile, plenty of users already have a clear idea that .zip means something completely different.  Since the very beginning, files on Windows computers have used an icon, and a filename ending in a dot followed by three letters to indicate what kind of file you're dealing with.  If the three letters after the dot spell z-i-p, then that indicates an archive full of compressed  zipped-up - files.  The icon," he says, "even includes a picture of a zipper on it because reinforcement is good, and confusion is bad."  Unfortunately, Google missed that message.



"As it happens," he says, "cybercriminals love .zip files and the last couple of years has seen an explosion in their use as malicious email attachments.  Typically, the zip file is first in a sequence of files known as an 'attack chain.'  In a short chain, the zip file might simply contain something bad.  In a longer chain it might contain something that links to something bad, or contain something that contains something that links to something bad, or contain something that links to something that contains something that links to something bad.  Anyway, you get the idea," he says.  "The key to it all is misdirection.  The attack chain is there to confuse and mislead users and security software."



Criminals use other forms of misdirection in file extensions, too.  An old favorite is giving malicious files two extensions, which of course we spoke about at the beginning of this podcast 18-plus years ago, like evil.zip.exe.  The first one, .zip in this case, is there to fool you.  The second is the real one, a dangerous .exe executable file type.  Given the choice of two, users have to decide which one to believe.  Most aren't even faced with that choice because Windows helps with the subterfuge by hiding the second file extension, the .exe, the one you really should be paying attention to, by default.  So you just see something or other .zip.



"Domain names get the same treatment.  Criminals make extensive use of open redirects, for example, web pages that will redirect you anywhere you want to go, to make it look as if their malicious URLs are actually links to Google, Twitter, or other respectable sites.  Less sophisticated criminals just throw words like 'PayPal,' or anything else you might recognize, into the beginning of the link and hope you'll notice that bit and ignore the rest."  Because, again, a lot of links just look like gobbledy-gook.  You kind of look at it and go, what the heck?  Oh, there's PayPal, okay, good.



He says:  "Against that backdrop, Google inexplicably decided to introduce something that will generate no useful revenue, but will give cyber crooks an entirely new form of file and domain name misdirection, to add to all the others we're still wrestling with today.  So what could criminals do with this new toy?"  He says:  "There is no better example than that provided by security researcher Bobby Rauch, in his excellent article 'The Dangers of Google's .zip TLD.'"  In it, Rauch challenges readers to identify which of the following two URLs is a malicious phish that drops evil.exe on their machine?



And I've got the two links in the show notes.  They begin:  https://github.com/kubernetes/kubernetes/archive/refs/tags/v1.27.1.zip.  And then the exact same URL except that in front of the final v.1.27.1.zip there's an @ sign.  And he says:  "The answer is that the evil link is the second one.  The top one opens a zip file called v1.27.1.zip which was downloaded from github.com domain.  The second one would go to the domain v1.27.1.zip, which in this hypothetical example triggers the download of whatever that site wants to provide.



LEO:  I did not know that about URLs.  So the @ sign changes the parsing.



STEVE:  Yes.  And it turns out those forward slashes are not actually forward slashes.  There's a Unicode character that looks the same.



LEO:  Oh.  So this looks like it is rooted at GitHub.com.



STEVE:  Yup.



LEO:  But in fact those are not forward slashes until we get to the...



STEVE:  Exactly.  Exactly.



LEO:  I see.  That makes sense.  So it isn't just the @ sign.  It's you have to use some tricks.



STEVE:  Yes.  Unfortunately, all the browsers display that Unicode as a forward slash, and it passes right through the browser.  So here's the problem for this audience.  I would be hard-pressed to look at that second URL and think that that's a problem.  It looks absolutely legitimate, https://github.com.  That's all you need to know; right?  Anything to the right of that is going to be somewhere underneath GitHub.com.



LEO:  We've taught everybody and we've trained ourselves to look for that root TLD.  That's where it's rooted.  And now we can ignore the rest.  But no.



STEVE:  Yup.



LEO:  Wow.



STEVE:  So me speaking:  In other words, the Internet just became a whole lot more dangerous, even for those of us who take some pride in understanding all this stuff, in knowing how to examine URLs, and knowing our way around.  What chance do our friends and relatives have?



Anyway, I've placed a link to Bobby Rauch's full write-up which explains in detail how simple it is to craft .zip URLs that look convincingly like URLs for well-known sites that are offering .zip archives for download, whereas in fact you are going to a site that somebody registered under the .zip TLD.  You know, it is really a problem.  And I just can't understand it.  In fact, I've skipped over a bit of the Malware Labs stuff.  But he said:  "It's also possible that .zip will simply die on the vine if enough companies choose to block it.



LEO:  Just block it, yeah.



STEVE:  Yes.  Last week, Citizen Lab's John Scott-Railton urged his nearly 200,000 Twitter followers to simply "block it all," saying:  "The chance that new .zip and .mov domains mostly get used for malware attacks is 100%."  So, he said:  "It's for you and your organization to decide if you should block it, but I will point out that if you are going to, the best time to do it is now.  Almost nobody is currently using it, and nobody is going to use it in the future if it's routinely being blocked.



LEO:  Little tip of the hat, too, to Firefox because I'm doing this in Firefox.  This is the forged slashes.  And in Chrome this would take you to a bad website.  In Firefox, it says, uh, what?



STEVE:  Oh.



LEO:  I don't think you're going where you think you're going, buddy.



STEVE:  Nice.



LEO:  So that's Firefox I guess has decided to ignore those, or not treat those Unicode characters as slashes.



STEVE:  Well, and I mean, the well-known author Robert Lemos, writing for Dark Reading, his coverage was titled "Google's .zip, .mov Domains Give Social Engineers a Shiny New Tool."



LEO:  Oh, yeah.



STEVE:  Wired headlined "The Real Risks in Google's New .Zip and .Mov Domains."  TechTarget:  "The potential danger of the new Google .zip top-level domain."  And Ycombinator:  "The Dangers of Google's .zip TLD."  So, you know, you just don't see that when new top level domains come out for, you know, like .pizza and things.  Who cares?



LEO:  No, who cares?  One other problem is, and this is another thing people should do, at least on the Mac, and I think on many browsers, they're set by default to "open safe files."  So, and zip is normally considered a safe file.  So when Safari sees a zip file, if you haven't unchecked that box, it will try to open it.  And so you can go to a .zip and get what looks like a .zip that tries to open, and it's malicious.  So another thing everybody should do, and I don't - on Windows, I think just as a convenience, browsers just assume, well, a .zip is safe.



STEVE:  Windows now puts it in the tree.  It's in the File Tree.



LEO:  Oh, that's terrible.



STEVE:  I hate that.



LEO:  Oh, my, my, my.



STEVE:  It is so - I actually, I have subfolders where I put my zips because I don't want them to be expanded in this tree, the regular directory tree.  Again, Microsoft wanted to make it easy.  So thank you very much.  Probably some way to turn that off, but I just - ugh.



LEO:  Certainly if you're on a Mac, disable "open safe files by default" because that's terrible.



STEVE:  So Leo, after our next break we're going to talk about PyPI and something interesting that just happened to them.



LEO:  Oh, yeah, because we talked about them last week.



STEVE:  They were under attack, and they've responded.



LEO:  Yeah, yeah.  It wasn't just last week.  We've talked about them for many, many weeks.  



STEVE:  Yeah.



LEO:  Constant problems with these supply chain attacks.



STEVE:  Last week I noted that the Python Software Foundation had been forced to take the unprecedented step of temporarily shutting down all new account and Python package creation due to an overwhelming automated attack on the world's leading Python software registry.  In response to the attack, plans are now underway to require two-factor authentication for all PyPI accounts; and, even more interestingly, to reduce the instances where PyPI portal needs to store a user's IP address.  Okay, well, we'll get to that second one in a second because that, I thought, what?



Anyway, in a major, but welcome and probably inevitable policy change, by the end of this year, all accounts that maintain a Python library on the PyPI portal must choose some two-factor authentication method or have their access to PyPI features limited.  The Foundation says that the move comes to improve their supply chain security of the Python ecosystem.  The requirement for two-factor authentication is intended to help block account takeovers, which has been a problem in the past, and also to raise the bar for those automated attacks like the ones that PyPI has just been subjected to over the past few months.



Two-factor authentication has been supported and available, but its use has been optional, and there's not been much uptake because people just haven't cared that much.  So that's what's changing.  Last year the Foundation offered free security keys to the maintainers of its Top 1% of all packages in a move to show the PyPI devs that two-factor authentication would not be a serious disruption that some for some reason believed it would be.



Okay.  But it's the second piece of news that I thought was the most interesting.  As I was reading the reports that the Foundation was making, well, they said they were making an effort to purge IP addresses from their server logs, replacing them with hashes of the IP addresses, or in some cases geographic data provided by Feedly, which is their CDN.  I didn't understand what that had to do with anything.  The Foundation said that the only systems that will see plaintext versions of a user's IP address will be their rate-limiting systems, and even those they will work on replacing.



Providing some additional detail, the Foundation explained that they would be salting their hashes because the IPv4 address space, being relatively small at only 32 bits, would allow for the creation of hash-reversing lookup tables.  As we know, the normal practice with salted hashes is to pick a unique per-hash salt at random, mix that in as you're hashing the thing you want, and then store the salt alongside the hashed value.  This would allow a later IP comparison by using the same salt against a candidate IP to see whether the hashes match when you do the same thing.



But in this case, that normal approach doesn't provide the level of protection that the Foundation requires due to the fact that the 32-bit IPv4 search space is still so small compared to the speed of today's GPUs that are able to do very high-speed hashing.  So instead, the Foundation said that they're going to use a single secret salt, that is, not a public salt, a secret salt which will not be stored in the database, and that they would be taking steps to protect it against any leakage.



Okay.  So that was all well and good, but it still left unanswered the question of why the Foundation was bothering to hash their database's IP addresses?  You know, what's the point?  The answer that came was somewhat chilling.  It's the same reason ExpressVPN, a sponsor of this network, goes to great lengths to protect their users' IP addresses.



Okay.  Get a load of this:  The news of this major engineering effort came two days after the Foundation revealed that it had been subpoenaed by the U.S. Department of Justice to reveal information on five PyPI accounts.  The requested information included the users' IP addresses, but also the IP addresses of all users who had downloaded packages from those five users.  The Foundation wants to eliminate their own ability to respond to such subpoenas in the future, thus protecting the IP addresses and thus to some degree the identities of those who are uploading and downloading Python libraries.



So the problem here is essentially identical to the issue with encryption.  While no one wants to provide care and comfort to criminals, there's also a reasonable presumption and expectation of privacy.  And unfortunately we keep seeing instances of government overreach, such as the other story we covered last week where the U.S. FBI abused the powers of the FISA data collection to improperly search the personal communications of Americans more than 300,000 times in a little over one year, according to the U.S. Senate Intel Committee.  If the government wants the trust of its citizens, the government needs to first be worthy of that trust.  These revelations, you know, about them not being worthy don't engender much trust.  And now we have very strong privacy enforcing technologies that are going to move their ability to have the information that they want, even under subpoena, out of their reach.



Other package registries have also announced plans to require their users to enable two-factor authentication.  And while the Python Foundation's move to purge IP addresses from logs and databases is a novel solution, it may develop into more standard practice in the future.  So as a fly on the wall, I'm curious why the Department of Justice produced a subpoena that required that the Python Foundation comply.  In their full-disclosure posting they said they were caught by surprise, they had the information, and they complied because at that point they had no choice.  But they're going to move that information out of their own grip so that they are not able to comply in the future.



And speaking of not being able to comply in the future, an object lesson story appeared in the news last week.  The so-called Super VPN service by the publisher SuperSoftTech, whose Android app - and there also is one from iOS.  The Android app has been downloaded more than 100 million times, was exposing more than 360 million records due to its Chinese developer leaving the app's database open and exposed to the Internet.  And that database contained all of the information the VPN provider explicitly promised would never be exposed, including information on all of its paid customers, including details such as emails, the customers' real physical IPs, geolocation information, and VPN servers they have connected to.



The database was secured only after it was discovered and reported to the developer by security researcher Jeremiah Fowler.  And it's worth noting that Jeremiah's findings contradict the app's Google Play Store listing at the time - it's been since removed - where the service claimed "no logs saved anywhere" as one of the benefits and features of its use.  Our obvious takeaway lesson here is that it's very easy for anyone to make the claim to not be retaining any logs.  No one using a VPN wants their usage to be logged.  There's zero benefit to the user from having their VPN provider logging their activity.  But it's obvious that the anti-logging claim needs to be true for it to have any value whatsoever.  So our takeaway is that it really does matter who is making the claim.



Well, Twitter may be facing stiffer headwinds with the EU's 27-nation bloc after Twitter last Friday chose to drop out of a voluntary European Union agreement to combat online disinformation.  The Associated Press reported that European Commissioner Thierry Breton tweeted - actually he used Twitter, right, he tweeted - that Twitter had pulled out of the EU's disinformation "code of practice" that all other major social media platforms, including Google, TikTok, Microsoft, Facebook, and Instagram, among others, have all pledged to support.  He added that Twitter's "obligation" remained, referring to the EU's tough new digital rules taking effect in August.  Breton said: "You can run, but you can't hide."  Okay, well, so a little bit of tough talk there.



There were early signs that Twitter was not prepared or planning to live up to its commitments.  The European Commission blasted Twitter earlier this year for failing to provide a full first report under the code, saying that it provided little specific information and no targeted data.  Breton said that the new digital rules that incorporate the code of practice fighting disinformation will become a "legal obligation."  Okay.  But, you know, Elon as we know is notorious for ignoring any legal obligations that he doesn't actually need to heed.



Breton concluded by adding, somewhat ominously:  "Our teams will be ready for enforcement."  Uh-huh.  It will be interesting to see what, if anything, that amounts to.  Perhaps some monetary fines that Elon will ignore.  I think that's going to be about it.  I doubt that the EU has the huevos to block all Twitter access outright.  So I think Elon is probably just going to be successfully calling their bluff.



There was also a flurry of confusion in the tech press last week over Bitwarden's announcement of their increased support for Passkeys.  I read some misreporting, and I thought, wow, I mean, before I knew it was misreporting I thought, great, I can tell everybody that Bitwarden will now be able to operate on Passkey-based sites.  No.  The actual news was not what we've been hoping for, at least not yet.  Apparently it's coming soon.  Bitwarden is saying that end users will be getting Passkey support for their Bitwarden clients sometime later this summer.



What was the news was that their Passwordless.dev site, which we referred to at the time shortly after Bitwarden acquired that group that they acquired not long ago, that they are offering APIs and code libraries now formally available to enable enterprises to add backend Passkey WebAuthn support into their authentication flows.  It's all open source, and it's going to make it much easier to do that.



And there was also news that Bitwarden client users would be able to use Passkeys to authenticate to their Bitwarden client, which makes the entire chain, as soon as the client is able to authenticate to websites, will make that whole authentication chain complete.  So, you know, Bitwarden doing good stuff, making WebAuthn more accessible.  But we don't yet have a client that we can use through Bitwarden in order to log into websites.  And again, even if we did, where would we log in because there's no one yet really supporting it.  



As I was scanning recent news for interesting updates and discussion, the phrase "1.2 billion euro GDPR fine" was difficult to ignore.  It appears that Ireland's Data Protection Commission wanted to get everyone's attention.  So they levied a 1.2 billion euro fine against Facebook's parent, Meta, for their failure to comply with the EU's GDPR laws.  The Irish officials claim that Meta illegally transferred the personal information of EU users to the U.S. without their approval.



What's apparently meant by this is the issue of where Facebook data resides, as in, you know, in a data center, sometimes there, sometimes here.  So this IDPC (Ireland Data Protection Commission) has ordered the company to cease all data transfers and delete existing user data within six months.  And in case anyone was wondering, yes, 1.2 billion is indeed the largest fine so far imposed - I guess I would say requested?  I don't know.



LEO:  They're going to appeal it, obviously.  It's going to be a while, yeah.



STEVE:  Yeah, under the EU's GDPR.  Now, to my mind this represents a failure of imagination, Leo, on the part of those plucky Irishmen.  If they wanted to really get everyone's attention over a fine that was never going to be paid anyway, they should have gone directly to 1.2 gazillion euros.



LEO:  Well, they can go as high as, I think, 10% of revenue.  They can go pretty high.



STEVE:  That's true.



LEO:  Yeah.  Not a gazillion.  I think no one makes that much money.



STEVE:  Well, why settle for a mere billion when you could ask for a gazillion?



LEO:  Well, here's my question.  And we talked about this on Sunday.  You know, email requires that my message goes from the server in my country to the server in your country.  I presume Facebook Messages and Facebook posts have the same thing.  It doesn't make sense in a modern Internet to say your data has to stay in our country.  I understand why they would want that.  But doesn't it break the Internet?



STEVE:  Meta, for their part, said:  "Let us think about it.  Okay, no."



LEO:  That's probably the right answer, actually.



STEVE:  To exactly no one's surprise, Meta said that they would be appealing, as you said, Leo, this nonsense.  In somewhat more detail, Meta explained that they were being unfairly singled out for attack because thousands of businesses, maybe even a bazillion businesses and organizations rely on the ability to transfer data between the EU and the U.S. in order to operate and provide their everyday services.  They explained that the real issue was not about one misbehaving company's GDPR-violating privacy practices; but rather that there are fundamental conflicts of law between the rules in the U.S. and in the EU, and that it's those laws that need to be brought into alignment.



And, they said, notably, respective policymakers are expected to be resolving all of this in this coming summer.  So yes, Meta formally stated that they would be appealing the ruling, including the unjustified and unnecessary fine, and seek a stay of the orders through the courts.  And meanwhile, Facebook continues unimpeded throughout Europe.  So, wow.  Just, I mean, crazy.  I guess, you know, if you have the GDPR, then you want to try to use it.



LEO:  Yeah.  I mean, I don't - I honor their desire to protect people's privacy.  I understand that.  And, you know, some fines, like the fine against Google for saying we turned off location tracking but didn't...



STEVE:  Yeah.



LEO:  That doesn't bother me at all.



STEVE:  Yeah.



LEO:  I just think that - I don't understand how the Internet's supposed to work if everything has to stay - it's like, well, your email will only work in Ireland.  Like you can only email people in Ireland.  Huh?  You can only message people in Ireland.  I don't understand what they're thinking.



STEVE:  Well, that's why it's called iMessage.  It's Ireland Message.



LEO:  Ireland Message.



STEVE:  That's right.



LEO:  Apple knew this all along.



STEVE:  That's right.  Okay.  So while we're on the subject, very briefly, of Meta and Facebook, I wanted to note that WhatsApp messaging announced on Monday, actually Monday before last, not yesterday, that they would be adding a feature that iOS users received with the update to the "16" versions of iOS and iPad OS, which is the ability to edit after the fact, for up to 15 minutes, the content of any recently sent message.  In the case of iOS, Apple allows up to five edits of a message during that 15-minute having been sent window, which I suppose is useful if you're really having a difficult time getting the message right.



Anyway, as it happened, I was glad to be reminded of this new feature in iOS 16 since I have a buddy who frequently follows up messages with little asterisks and like single words where he rereads the message after he sent it and goes whoops, and then lets me know that he realizes there was a typo.  Anyway, since I put this in the show notes last night, I sent him a text and said, "Hey, guess what, Mark, you're able to edit your messages after you send them."  So he's very excited.  So just a reminder to everybody else who's using WhatsApp or iOS and iMessage and iPad OS, you can now edit things after you send them.



And finally, Google has announced a new bug bounty program aimed at their own franchise's Android apps.  They've named it Mobile VRP for Mobile Vulnerability Reward Program.  And in describing the program they said:  "Google's Mobile Vulnerability Rewards Program (Mobile VRP) focuses on first-party Android applications developed or maintained by Google."  And by that they mean "or Google property."



"The Mobile VRP recognizes the contributions and hard work of researchers who help Google improve the security posture of our first-party Android applications.  The goal of the program is to mitigate vulnerabilities in first-party Android applications, and thus keep users and their data safe.  Only apps published by the developers in the list below, or apps in the Tier 1 list, are in scope for the Mobile VRP."  So that's apps by Google LLC, Developed with Google, Research at Google, Red Hot Labs, Google Samples, Fitbit LLC, Nest Labs Inc., Waymo LLC, and Waze (W-A-Z-E).  



So there are three general classes of vulnerabilities that qualify under their rewards program.  The first of the three is Arbitrary Code Execution where they explain:  "Vulnerabilities of this type allow an attacker to execute arbitrary code in the context of the vulnerable application.  In order to qualify, the ACE (Arbitrary Code Execution) should allow an attacker to run native code of their choosing on a user's device without the user's knowledge or permission, in the same process as the affected app."  Meaning there is no requirement that the OS sandbox needs to be bypassed.



And they provide three examples:  an attacker gaining control of an application, meaning that code can be downloaded from the network and executed; or overwriting a .so file with a malicious .so file which is then executed by the victim app; or executing Java code in order to call the "exec" function which is then able to run arbitrary native code.  They said that merely tricking a user into installing an app and executing code within that app itself does not qualify.  So it's got to be your ability to execute your own arbitrary code within one of Google's family of apps.



The second of the three qualifying classes is the theft of sensitive data, which includes vulnerabilities that lead to unauthorized access to sensitive data from an app on an Android device, where the scope of the qualifying data is data that enables unauthorized access to a user's account, you know, login credentials, authentication tokens that are able to perform sensitive state-changing actions that result in non-trivial damage to the victim.  Or sensitive user-generated data:  contact list information; photos unless they've been made public by default; content of a user's messages (email, IMs, or texts); call/SMS logs; web history; or browser bookmarks.  So basically getting data that you're not supposed to be able to get from any of Google's apps.  And finally, information that is linked or linkable to an individual, such as medical, educational, financial, or payment data, including employment information.



So they did note that location information alone does not qualify.  They don't feel that's significant enough.  But all the other stuff that you should not have access to does.  And lastly - those were the first two classes.  The last one is just additional vulnerability types which are in scope:  path traversal and zip path traversal vulnerabilities which lead to an arbitrary file write; intent redirections leading to launching non-exported application components; vulnerabilities caused by unsafe usage of pending intents; and orphaned permissions.



The range of rewards is as high as $30,000.  You would need to do a zero-click arbitrary code execution in one of their apps.  But if you can do that, you've got $30,000.  And they go down to about 750 bucks, depending upon severity and class of app.  So anyway, what we have witnessed is the emergence and clear success of the concept of bug bounties as like a functioning chunk of the security infrastructure in our industry where good guys are paid for finding and responsibly disclosing previously unknown and important bugs.  And if you're good at it, you can make a living.



I wanted to just quickly note that since last week I finished the full rewrite of SpinRite's mass storage data operations backend, and I finished testing it all by the end of the weekend.  So Sunday evening I released the 28th alpha, Alpha-28, to the gang over in GRC's spinrite.dev newsgroup, and they have begun putting it through its paces.  I have not been looking obsessively at it yet because I've been working on this podcast.  But it looks like it's going pretty well.  You know, I'm sure I'll have something to report about that next week.



In my own testing, I can say that it felt significantly more solid and responsive than it had before, and it did incorporate many new features, essentially thanks to all of the revelations that I was experiencing over the last six months.  So I'm very glad that I took that month to scrap and reengineer that very important part of SpinRite.  That's where all the actual work happens.



LEO:  All right, Steve.  I've been kind of playing with the Brave browser.  In fact, a couple of weeks ago Paul Thurrott made it his app of the week.  Sounds like maybe it's a good browser to use for privacy.



STEVE:  Well, they've done something I think is very cool.  Last Wednesday the Brave browser's privacy team announced a slick new forthcoming feature that as I said at the top of the show I would love to see obtain some traction within the wider browser community, meaning also being adopted by Google for Chromium and Mozilla for Firefox.  It's called "Request OTR," where OTR is short for "Off the Record."  The shortest summary of this interesting new idea is that this feature allows websites to suggest to the browser that this user's visit to the site should probably be forgotten and thus remain "off the record."



So here's how the Brave Privacy Team explained their idea.  They said:  "Starting in version 1.53, Brave will begin rolling out a new feature called "Request Off the Record."  This feature aims to help people who need to hide their browsing behavior from others who have access to their computer or phone."  Now, initially this sounds like incognito mode; right?  But it's not.



So they said:  "For example, a person who's the victim of intimate partner violence who needs to find support services without their partner knowing, or someone needing to find personal healthcare without others in their home finding out.  Request OTR allows websites to optionally describe their own content as 'sensitive.'  The browser can then ask the user if they would like to visit the site in OTR mode, where the site is visited in a clean, temporary storage area.  Sites visited in OTR mode are not saved to your browsing history, and any cookies, permissions, or other site data do not persist to disk.  Meanwhile, all other normal sites visited are stored and treated normally, hiding the fact from anyone who may access the device later that any 'unusual' behavior happened.



"Brave intends to work with other browser vendors to standardize OTR, so that at-risk browser users can be private and safe across the web, regardless of which browser they're using.  This feature has been designed with the input of, and in collaboration with, several civil society and victim advocacy groups.  We agree with Mallory Knodel, the CTO at the Center for Democracy and Technology, who said:  'Brave Browser's attention to detail with OTR Mode, where users can more easily choose which websites are recorded in their browsing history, is an important privacy innovation that can protect users in "attacker you know" situations or anyone who wants more control over what their browser remembers and what it doesn't.  This feature empowers people who browse the web - all of us - and gives us more agency over content consumption.'"



Okay.  So Brave said:  "Some users need to hide their browsing from people who have access to their device.  Most often, when people talk about web privacy, they're talking about protecting personal data from other websites, for example, blocking Google from recording the sites visited.  However, web users have other privacy needs, too, needs that are currently poorly served by most browsers."  They said:  "Consider Sarah, a hypothetical web user who lives with Stan, a physically abusive partner.  Sarah needs to use the web to learn about legal, medical, and other support services in her area so she can safely exit their relationship.  Stan, though, suspects Sarah may be planning to leave, and begins monitoring Sarah's phone, computer, and other devices to see if she's contacting support services.



"Unfortunately, not only do browsers fail to protect users like Sarah, they actually make it easier for abusers like Stan to digitally surveil them.  Browsers record a wealth of information about our browsing behavior and interests, both explicitly (browsing history, DOM storage, and cookies) and implicitly (cache state, saved credentials, URL autocomplete).  Worse still, the tools browsers do include to protect people like Sarah are incomplete and/or difficult to use correctly.



"Browsers currently provide some tools to help users hide their activity on sensitive sites.  However, these tools are insufficient to protect people whose safety depends on hiding visits to specific sites from people who have access to their device.  Existing tools either hide too much, thus inviting suspicion from abusers; too little, thus allowing abusers to recover browsing history; or are otherwise difficult to use successfully.



"Private, also known as incognito, windows allow users to browse the web without their browsing activity being permanently recorded.  Unfortunately, private windows do a poor job protecting users from on-device surveillance.  It's easy to forget to open a private window before visiting a site, especially under stress, thus causing the site visit to be permanently recorded.  It's equally as easy to forget to close the private window, and thus continue browsing in the private window beyond just the target sensitive site.  This can reveal to the abuser that private browsing modes have been used, which on its own may elicit suspicion or put the victim at further risk.



"Similarly, some browsers include advanced browser controls that could be used to delete browser storage for specific sites.  This approach has the drawback of needing to be performed after the site was visited, instead of protecting the user during the visit, which may put the user at risk if the browser needs to be closed quickly.  Additionally, these controls are often difficult to find, and more difficult still to use correctly by non-technical users.  And finally, these browser controls typically only allow the user to delete stored values for the site, for example, cookies or permissions; but do not allow the user to delete other traces of the site, for example, browser history or caches.



"Finally, some sensitive sites include quick-exit buttons in the site themselves, which allow a visitor to quickly navigate away from the site in a way that may be semi-difficult for an abuser to detect.  While useful, this approach is also incomplete.  Quick-exit buttons cannot delete many types of site data, for example, permissions or caches; and are constrained in their ability to modify the browsing history.  Further, they depend completely on the correct implementation by the site.  The browser is unable to protect the user.  In contrast, Brave's "Request OTR" approach provides a comprehensive way for sensitive sites to request to be omitted from a user's browsing history and local storage.



"Any site can request to go off the record, and the user is prompted to determine whether they would like to do so.  If so, the Brave browser creates a temporary storage area for that site, and does not record the site visit in the user's browsing history.  The OTR session is tied to the site, and any other sites the user visits, even in the same tab, along with any sites visited in any other tabs, are recorded in browsing history as usual.



"Brave's implementation of Request OTR protects the user in the following ways:  The user is prompted and proactively asked upfront whether they wish to have their visit forgotten after they leave.  The user is protected the entire time they're visiting a sensitive site.  They don't need to attempt to scrub their browsing history later.  Other, non-sensitive sites are recorded as usual, which prevents the appearance of large gaps in browsing history that might look suspicious to an abuser.  All target site behaviors are prevented from persisting to disk, including cookies, caches, browsing history, permissions, et cetera.  And finally, OTR prevents sites from abusing the feature.  A site cannot go off the record unless a user explicitly gives the site permission to do so.



"Brave has developed Request OTR specifically to help people suffering from intimate partner violence, or people otherwise needing to hide visits to sensitive sites from their browsing history.  However, OTR is intentionally a general browser feature and is intended to be usable by any site on the web.  There are currently two ways for a site to request to go off the record in Brave.



"The primary, intended way is for the site to include the header Request-OTR: 1 in the website's response to the initial navigation request to a site.  If the browser receives this header, the browser will halt the navigation and ask the user if they would like to visit the site off the record.  If the user says yes, then the browser does two things.  It does not record the site visit in the browser history, and it creates a temporary storage area for all caches, cookies, and permissions.  The browser continues using this temporary storage area for all subsequent pages visited within the same tab, within the same site.  When the user closes the tab or navigates away from the site, the temporary storage area is discarded, and browsing behaviors return to being recorded as usual.



"The second way a site can request to go off the record is to be included in Brave's preloaded list of 'request off the record' partner sites.  These are sites that serve victims of intimate partner violence and have told Brave they're interested in being considered a sensitive site, by default, thanks to the browser, in the browser.  This list is intended as a bridge measure until all such sites have the opportunity to implement the previously mentioned header approach.



"There are a few caveats:  Users should be aware that Brave's Request Off the Record feature cannot protect users from other software on their computer that might record information about what sites they visit.  Examples of software the Brave browser cannot hide browsing history from include browser extensions; network spying; malware or spyware installed on the device; information saved by sites before or after you visit off the record, in other words, that's obviously just out of scope; operation system level logging; and crash logs."  They said:  "Brave is exploring what additional protections can be provided using such threats, but users should be aware that as with systems like private browsing mode, Brave's Request Off the Record mode only prevents recording of core browsing behaviors and data."



They finished, saying:  "We're excited to release Request Off the Record in the upcoming version 1.53 of our desktop browser, with an Android version coming in 1.54.  We'll be rolling it out to users shortly, though people interested in testing the feature can now enable it by visiting brave://flags and enabling #brave-request-otr-tab."  They said:  "Please note this should only be done if you understand the risk of testing experimental browser features," blah blah blah.



"We're also excited about the next steps we're taking to further improve the Request OTR feature.  First, we're working with experts and researchers at George Washington University and Paderborn University to evaluate how Request-OTR is understood by users, and how we can further convey to users exactly what protections the feature does and does not provide.  We will both share the research that results from this collaboration on this blog, and incorporate it into future versions of Request-OTR in Brave."



And finally:  "We're interested in working with other browsers, organizations, and web companies to potentially standardize Request-OTR so that users of other websites and browsers can benefit from the protection.  Our current implementation is the result of working with a wide range of abuse advocates, technologists, browser specialists, and NGOs; and we're eager to continue working with similar organizations to best support web users."



So to all of that foregoing, I say a huge bravo.  As I said, in 2023, with our web browsers at their current level of maturity, it is extremely rare to encounter something so clear, so clean, and so simple that is, I would argue, able to offer new and useful benefits to a user's browsing experience.  And this new feature is such a thing.  The user's experience of this is perfect.  It could be baked into every browser, and most of us would never have any idea that it was present.  But the moment someone who was in an environment that might make them vulnerable, visited a website that understood that by virtue of the services it offers, its visitors might need the site's help in protecting themselves, the website could immediately prompt the browser to proactively ask the visitor whether they would like all of their use of this site to remain off the record and be immediately forgotten after they've left.



You know, it's clean and simple and I think a wonderful addition to the traditional incognito mode of browser usage.  So I hope that the clarity and the objective goodness of this idea captures the imaginations of those at Google and Mozilla, as well, and that it would grow into a World Wide Web standard that I think it deserves to be.  Just you know, simple, but, you know, and easy to do, but I think really a worthwhile benefit.



LEO:  There's some question in the chat about how this differs from incognito mode or a private browsing mode.



STEVE:  I would say that it's that that requires you to take that action explicitly.  It also requires that you not do other things.  The problem is while you're in there, your browser's not logging.



LEO:  Right.



STEVE:  So if somebody was suspicious and looked at your history, they would see a block of time when they knew you were on the computer and doing something that there was no record.  Also, if you suddenly got interrupted, you have to close the incognito mode.  You might forget and leave it open.  So, and most of what I like about this is that somebody innocent who wasn't thinking in advance is proactively asked, oh, you're visiting us.  Would you like this to be forgotten?



LEO:  Right.



STEVE:  And they think, oh, yes, thank you.



LEO:  Yes, yes.



STEVE:  And so it's that proactive - and again, no sites would ever display this to us unless the site itself knew that the type of services they were offering were those that they...



LEO:  Right, like a women's health clinic or, yeah.



STEVE:  Yes.



LEO:  Of course, yeah, yeah.



STEVE:  Yes. 



LEO:  I think that's great.



STEVE:  I think it's just brilliant.  It is simple.  It's a tiny little thing, trivial to do.  And so what it is, is whereas incognito mode is mostly the things I'm doing I don't want Google to remember or my browser to remember, whatever it is, you know, any tabs that you open.  This is just it's more selective just for the site that you're visiting.  So other things you do during the time, other tabs you visit, other links you click, they'll all be there, just not this one selective site.  And for me also, the fact that it's proactive is the key, that this type of site says, would you like this not to be remembered by your browser?  Oh, yes, thank you.  Of course you're also free to decline it.  You say, no.



LEO:  I don't care.



STEVE:  I'm fine, yeah.  I just think it's very clean.



LEO:  Yeah, very nice.  Steve, always a fascinating and informative podcast.  Thank you so much.  Episode 925 is in the books.



Copyright (c) 2023 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#926

DATE:		June 6, 2023

TITLE:		Windows Platform Binary Table

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-926.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we're back to answer a collection of burning questions which we first pose, including:  What news from HP?  What is Microsoft doing for Windows 11 that promises to break all sorts of network connections?  What's OWASP's new Top Ten list of worries about?  Did Apple help the NSA attack the Kremlin, and what crucially important revelation does this incident bring?  What new hacking race has Google created?  And what misguided new U.S. legislation will hopefully die before it gets off the ground?  What is TOR doing to protect itself from DoS attacks?  How much are educational institutions investing in cybersecurity?  And what can go wrong with civilian cameras in Ukraine?  Are we seeing the rise of cyber mercenaries?  What is the Windows Platform Binary Table, why should we care, and how can we turn it off?



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  Coming up, OWASP's suggestions for things to watch out for when you're using artificial intelligence.  Did the NSA and Apple work together to hack the Kremlin?  Steve doesn't think so, but the story's quite interesting.  And then we'll talk about the Gigabyte motherboard flaw that could infect hundreds of millions of people.  Yup, Security Now! is next.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 926, recorded on Tuesday, June 6th, 2023:  The Windows Platform Binary Table.



It's time for Security Now!, the show where we cover the latest security, and the security news, and how things work, and laugh a little, and love a little.  And no hugging.  Mr. Steve Gibson.



STEVE GIBSON:  And why not?		



LEO:  And why not.  Steve from GRC.com, longtime host of this show.  Great to see you.



STEVE:  And, you know, I had an occasion to go back to 2015, we'll get to that in a while, in one of - I think it was Episode 542 or something.  And I was reading the transcript, and I thought, you know, I am really glad that this has evolved into a script that I'm reading because I'm not good when it's with just completely freeform, and I'm just like...



LEO:  Oh, is that how we used to do it, where I would like just - we would just gas about stuff?



STEVE:  Well, just like all the other podcasts that you have.



LEO:  Oh, yeah.  Yeah, that's true.



STEVE:  Yeah.  But, you know, but you guys are good.  I'm stumbling around, and I'm like, my god.  And Elaine had to transcribe this.  I'm sorry, Elaine.  Anyway, we've got a great one today.  This one is titled Windows Platform Binary Table.  And I loved your little quip earlier, like random words that you have to memorize to show that your brain is still functioning.  It sounds a little bit like that.



But we're going to start off by answering a bunch of questions.  What news from HP?  What is Microsoft doing for Windows 11 that promises to break all sorts of network connections?  What's OWASP's new Top 10 list of worries about?  Did Apple help the NSA attack the Kremlin?  And what crucially important revelation does this incident bring?  What new hacking race has Google created?  And what misguided new U.S. legislation will hopefully die before it gets off the ground?  What is TOR doing to protect itself from DOS attacks?  How much are educational institutions investing in cybersecurity?  And what can go wrong with civilian cameras in Ukraine?  Are we seeing the rise of cyber mercenaries?  And what is the Windows Platform Binary Table?  Why should we care?  And can we turn it off, please?



LEO:  Ooh.  Well, I look forward to this episode of person, woman, man, camera, TV.  See, I remember them.  I remember them.



STEVE:  That's good.  And we do have another Picture of the Week, and it's one that we've seen many times now, but this one sort of pushes it over the top.  So it made the grade mostly because of how way more than normally ridiculous it is.  



LEO:  I haven't looked at it yet.  We have a new protocol.  I don't look at the Picture of the Week until the actual time so you can see my...



STEVE:  The listeners can share your experience.



LEO:  ...natural reaction; that's right.



STEVE:  That's right.



LEO:  All right.  Steve, I'm ready.  My eyes' blinders have been removed.  This is the Picture of the Week.  I'm going to scroll it up, and then I'm going to look over and see it.  Are you ready?  Here we go.  The Picture of the Week [laughing].  That is as bad as you can get.



STEVE:  So, yeah.  The reason - so we're being - I have enough pictures of ridiculous gates...



LEO:  I bet, I bet.



STEVE:  ...in the middle of, like, fields and things, that we'll have those for the end of time.  And we've had so many of them recently that I thought, well, I'm not, you know, we're going to back off on that theme a little bit.  But this one just - it made it to the podcast because it goes the extra mile.  It's, you know, it's - so we have this path, like, some steps are going down in the foreground, and there's a path cutting across them going into the distance.  And there's this gate which has been placed on the path.  And it's not just that there's a gate there, which of course you could just walk around, because like choose which way - it's probably a little more convenient to go around it onto the right because the hill gets a little steep descending to the left.



LEO:  It's not about a gate, it's just a sign holder, basically.



STEVE:  So what this thing has the audacity to say to somebody who is confronting it is, in red signage:  "Strictly No Access."



LEO:  Oh, no, absolutely not.



STEVE:  No, really, strictly none, despite the fact that, like, what?  You could just walk around it.



LEO:  There's also, I'm sure you noted this, there's a - is that a power outlet or what is that?  I mean, it seems like they maybe electrified the fence.



STEVE:  That was my thought, or that they're monitoring it in case somebody opens it and goes through.



LEO:  Don't you dare.  That is the weirdest thing ever.



STEVE:  And the only thing I could think was that this was there so that, if somebody was caught on the path behind the gate, they could say, well, you were warned.



LEO:  You were warned.



STEVE:  Strictly no access.  Anyway, I just - this was just a hoot.



LEO:  That's hysterical. 



STEVE:  Because it's like, you know, pushes this thing really over the top.  Right.  Not, I mean, the gate being there would be enough.  But no, you really - it doesn't matter how much you want to go down that path.  No.  Can't.



LEO:  Oh, lord.



STEVE:  Okay.  So I started off this week, as I do every week, looking to see whether there was any good news for the tens of thousands of owners of HP OfficeJet 9020e series inkjet printers which were all bricked, as we know, that Monday morning, now four weeks and a day ago.  Again, total silence from HP.  Against that backdrop, the first two headlines that popped up in my search yesterday for any updated news were stories initially published when this was news was fresh:  "HP rushes to fix bricked printers after faulty firmware update," and "HP Races to Fix Faulty Firmware Update That Bricked Printers."



So we have "rushing" and "racing."  Both of those and many similar headlines certainly made sense at the time, but they're not aging well.  Despite what we hope and assume from HP, their response, even after now more than a month, has been a big zero.  So, boy.  Wow.  Just it's crazy.  And again, this wasn't a small event.  This didn't happen to a few people.  This was global.  And I refer to it a little bit later in the show.



Okay.  So Microsoft has announced that future versions of Windows 11 will require all SMB, as in Server Message Blocks,  messages to be cryptographically signed, regardless of the message type.  Now, that's a rather significant change, and it's going to be interesting to see what it breaks.  SMB is commonly known, it's the protocol that many things in Windows now use.  Notably, Windows File and Printer Sharing runs on top of Server Message Blocks.  The requirement for cryptographic signing is different from, and a further extension of, Microsoft's policy of stepping away from the oldest versions of this aging and sadly troubled protocol.



Just over a year ago, Microsoft announced their related intention to remove all SMBv1 support from Windows 11.  Back then, at that time, Ned Pyle, the Microsoftie who was making this announcement into Microsoft's Tech Community, he wrote:  "I had to save this behavior for last.  It's going to cause consumer pain among folks who are still running very old equipment, a group that's the least likely to understand why their new Windows 11 laptop cannot connect to their old networked hard drive."  He said:  "I'll spread the word throughout consumer channels as best I can.  I hope you can help me with friends and family who rely on you for technical expertise."



Okay.  So Microsoft is, you know, changing things, and they feel that they need to.  There are currently three major versions of SMB - 1, 2, and 3.  And actually I think it's 3.1.1 currently.  And by default, Windows 10 has always disabled support for the original SMBv1 due to its total lack of security, but the code to support both the client and the server roles has remained present in Windows 10.  Remember that SMBv1 was present in the MS-DOS Windows for Workgroups add-on which allowed DOS machines, which were still very much in use in the early days of Windows, to participate as either clients or servers on a Windows LAN.



Since my own work on SpinRite 6.1 is still DOS-based, and DOS only knows about the original SMBv1, I was hugely relieved to find that support for v1 could be enabled under Windows 10.  That's what I've been using to network my DOS machines.  It's allowed me to write and assemble SpinRite's code on a Windows 10 workstation, then give the DOS machine access to that Windows 10 shared directory which contains SpinRite's source and executables so that I'm able to do native source level debugging under DOS.



Last year Microsoft took this further by removing the binaries for SMBv1 from at least some of Windows 11.  So it was disabled under Windows 10, but you could go under that - it's under Control Panel.  Oh, it's Windows Features where you're able to, like, turn things on and off, like for example a web server.  You're able to say, yeah, I'd like to have an IIS web server on my Windows workstation, and turn it on, and it sort of adds that to Windows.  So that's where this SMBv1 stuff is in Windows 10 that you can still turn on.  Microsoft is, you know, needing to keep moving things forward.  So they've decided that you're not even going to be able to that in the future.



And apparently this still breaks things, that is, removing SMBv1.  So users of the higher end editions even of Windows 11 will be able to use an unsupported binary from Microsoft to add this back.  I'm just telling everybody so in case this happens to you, and for some reason you still need v1.  You know, there are things, like I think the version of the Sonos software that I had wasn't able to operate unless it had v1.  So then there were instructions for how to go turn that on.



But anyway, this next announcement will be interesting, and it's likely to create further issues for connectivity with devices that cannot or do not support SMB message signing because that's the news of the week.  And that makes me suspect that this signing requirement is also likely to be somewhat soft and probably can be overridden because Microsoft's just - they're really reluctant to break things that have happened before.



So last Friday that same guy, Ned Pyle, who a year ago said what I first quoted, last Friday he said:  "Heya, folks.  Ned here again.  Beginning in Windows 11 Insider Preview Build 25381, the Canary release, Enterprise editions, SMB signing is now required by default for all connections.  This changes legacy behavior, where Windows 10 and 11 required SMB signing by default only when connecting to shares named SYSVOL and NETLOGON, and where Active Directory domain controllers required SMB signing when any client connected to them."



So SMB signing is a simple but a useful security measure that's been around since actually, believe it or not, Windows 98 had it, and Windows 2000 over on the NT platform.  But it's never been forcibly enabled by default because the signing overhead it adds to every message used to be too much.  Without signing, nothing detects or prevents the alteration or spoofing of SMB messages.  There's no security in them otherwise.  So signing protects against NTLM, NT LAN Manager, relay attacks which have been a constant thorn in Microsoft's SMB implementation.  So signing is definitely a good thing.



But, as always, moving into the future for the sake of security also means removing some of the past, which is always somewhat painful.  I'm mentioning all of this as a heads-up since there's a very good chance that this next move by Windows 11, and it sounds like it may be only Enterprise editions, so it may not be affecting Pro users and Home users, which is probably a good thing.  But even at the Enterprise it could break some things that our listeners are using, you know, things that either don't currently have SMB signing enabled or maybe don't support SMB message signing at all.  So there.



OWASP, O-W-A-S-P is the "Open Web Application Security Project."  And it has, for the past 20 years, since 2003, maintained the often quoted "OWASP Top 10" list of the most worrisome web application vulnerabilities.  I have a special warm spot for OWASP, since it was several European chapters of OWASP who hosted my trip to Europe once SQRL was completed to give me the opportunity to introduce it to their members in person, which I really appreciated.  We're talking about this today because OWASP has announced their work on a completely different list.  It is the "OWASP Top 10" for Large Language Model applications, which is interesting that they've decided, okay, there's enough issues with LLM stuff that they're going to start tracking and maintaining a similar Top 10 list of things to be careful about.



So in their announcement they said:  "The OWASP Top 10 for Large Language Model Applications project aims to educate developers, designers, architects, managers, and organizations about the potential security risks when deploying and managing Large Language Models.  The project provides a list of the Top 10 most critical vulnerabilities" - and actually it's vulnerability classes - "often seen in LLM applications, highlighting their potential impact, ease of exploitation, and prevalence in real-world applications.  Examples of vulnerabilities include prompt injections, data leakage, inadequate sandboxing, and unauthorized code execution, among others.  The goal is to raise awareness of these vulnerabilities" - again, vulnerability classes - "suggest remediation strategies, and ultimately improve the security posture of LLM applications."



They said:  "The following is a draft list of important vulnerability types for Artificial Intelligence applications built on Large Language Models."  So we've got 10 itemized problems or classes.  The first is prompt injections.  "Prompt injections bypass filters," they write, "or manipulate the LLM using carefully crafted prompts that make the model ignore previous instructions or perform unintended actions."  And we have already seen applications of that.



Number two, data leakage:  "Accidentally revealing sensitive information, proprietary algorithms, or other confidential details through LLM responses."  Number three, inadequate sandboxing:  "Failing to properly isolate Large Language Models when they have access to external resources or sensitive systems, allowing for potential exploitation and unauthorized access."



Fourth, unauthorized code execution:  "Exploiting LLMs to execute malicious code, commands, or actions on the underlying system through natural language prompts."  Fifth, SSRF, Server Side Request Forgery Vulnerabilities:  "Exploiting LLMs to perform unintended requests or access restricted resources, such as internal services, APIs, or data stores."



Number six, over-reliance on LLM-generated content:  "Excessive dependence on LLM-generated content without human oversight can result in harmful consequences."  You're right.  Seven, inadequate AI alignment.  They said:  "Failing to ensure that the LLM's objectives and behavior align with the intended use case, leading to undesired consequences or vulnerabilities."  Number eight, insufficient access controls:  "Not properly implementing access controls or authentication, allowing unauthorized users to interact with the LLM and potentially exploit vulnerabilities."



Nine, improper error handling:  "Exposing error messages for debugging information that could reveal sensitive information, system details, or potential attack vectors."  And finally, 10, training data poisoning:  "Maliciously manipulating training data or fine-tuning procedures to introduce vulnerabilities or backdoors into the Large Language Model."



Okay.  So some of these are old and generic, like unauthorized code execution, you know, that's not something anyone is going to want, you know, except bad guys.  And similarly, data leakage, inadequate sandboxing, server-side request forgeries, insufficient access controls, or improper error handling.  You know, those are like, yeah, right, sort of common generic classes of problems.  But those common problems are on this list because this is a new context, and there might be some tendency to let those slip by, thinking that those old rules no longer apply.  So I agree that it's certainly worth reinforcing and remembering to think specifically about some of those oldies but goodies.  There's a reason they have become so well known, and they never quite disappear.  



And then we have a handful of new problems that are quite specific to this application class.  "Prompt Injection" is a new one.  That didn't exist before we had conversational chatbots, where we quickly learned that you could lead them astray.  You could get them to do things that they weren't wanting to do.  In one of our very first conversations about this we were talking about how some bad guys managed to make one of the early ChatGPTs do something, and it sounded like they did it just by like asking more and getting mad at it.  And finally it said, yeah, oh, oh, yeah, okay, I'll give you what you need.  So there's that.



We have "Overreliance on LLM-generated Content," which as we have seen examples, we talked about one - I guess it was here, or maybe it was just, you know, other TWiT podcasts - where an attorney used ChatGPT to generate a complete legal brief, citing cases which were fictitious.  It just made them up.  So, yeah, overreliance on LLM-generated content.  We've got "Inadequate AI Alignment" and "Training Data Poisoning."  So in addition to all of the usual suspects, we also have a few new worries that we've never had before.



The Original OWASP Top 10 has for 20 years been a useful benchmark against which many projects have been measured and just checked.  If nothing else, coders of any stripe, you know, back then and for the last 20 years, have been able to use it to double-check that they hadn't overlooked some obvious things.  I expect that this LLM-specific Top Ten list will serve a similar role.  Someone should create one.  OWASP is at work doing so.  And I think they said that they were at 0.1 at this point, so not yet ready for primetime.  But they have announced that they're going to be working on this. 



LEO:  It looks pretty good.  I mean, I would agree with every single one of those.  They all look pretty accurate.



STEVE:  Yeah, yeah.  And, you know, it's so easy just to deploy these things without giving sufficient thought to things that you should give sufficient thought to.



LEO:  Yeah, yeah.



STEVE:  So good to have it.  Okay, now, this is a two-parter.  And this ended up with something really interesting, an observation that we're making for the first time anywhere.  And it's, I think, very important.  So we start off with what led me into this, which was the question, did Apple help the NSA attack the Kremlin?  It should have been given the subheading "Why does anyone take anything Russia says with even a grain of salt?"  Now, I was unable to read the original - I've got the link in the show notes, but it's HTTP, right, because Russia doesn't believe in "S" - :// and then www.fsb.ru, and the rest is in Russian.  And it didn't seem worth bothering to get it translated because I was able to use the translation from the Risky Business newsletter.  So I'm going to rely on that.  And here's what Risky Business explained.



"Russia's FSB intelligence service claims to have uncovered a U.S. intelligence operation that hacked the Apple smartphones of diplomatic missions and embassies in Russia.  The operation allegedly targeted thousands of devices, including the devices of Russian citizens and diplomatic representatives from NATO countries, the post-Soviet bloc, Israel, China, and South Africa.  The attacks exploited a vulnerability in Apple smartphones.  The FSB attributed the hacks to the U.S. National Security Agency and claimed Apple cooperated with the NSA for the attacks."  And actually it was that allegation that caught my attention since, well, that would be huge if it turned out to be true, which seems unlikely in the extreme to me, given everything we know about Apple, and fortunately the country that we still live in.



Anyway, Risky Business continues, saying:  "The Russian cybersecurity firm Kaspersky says the same attacks  which the company tracks as Operation Triangulation  also targeted its employees.  Kaspersky said they found compromised devices as far back as 2019 and said that the attacks are still ongoing.  According to Kaspersky and to a technical report released by FSB's National Coordination Center for Computer Incidents, the attacks involve an iOS zero-click exploit delivered as a file attachment via iMessage.  The attachment executes without user interaction as soon as it arrives on a device and starts downloading additional files.  Kaspersky described the final payload as a 'fully-featured APT (Advanced Persistent Threat) platform.'  Unlike the FSB, Kaspersky did not link the activity to the NSA or any other APT group, and could not say if the attacks targeted other organizations.



"News of the attacks came after, in March, the Kremlin's security team instructed their presidential staff to dump their iPhones by April 1st, 2023."  Coincidentally April Fool's Day.  And we talked about that Kremlin activity at the time on the podcast.  "Employees of the Kremlin were told to get an Android device, either from a Chinese vendor or one running Rostelecom's Aurora OS.  Kremlin officials cited security considerations for their decision, claiming iPhones were 'more susceptible to hacking and espionage by Western experts compared to other smartphones.'" 



LEO:  Which is not true.



STEVE:  Which is not supported by any evidence we've seen.  "Russian officials asked the Prosecutor General's Office to start a formal investigation into Apple employees and U.S. intelligence officials."  Okay, right.  Okay.  And, unsurprisingly, in an email that the Risky Business newsletter received after its initial publication of this news, Apple formally denied the FSB's accusations, writing:  "We have never worked with any government to insert a backdoor into any Apple product, and we never will."  And as I said earlier, given the entire history of Apple's actions and the design of their devices which we have often examined as closely as we could, I certainly believe Apple's assertion far more than the Kremlin and the FSB who we catch frequently spewing state-sponsored propaganda.



Okay, now, that said, if Kaspersky first saw this attack as early as 2019, assuming that the same zero-click exploit was in use then as is in use now, that would suggest that an exploit has remained undiscovered for the past four years.  So I was interested in additional details, and something that an actual security firm did in terms of research.  So I tracked down Kaspersky's thoughts about this.



They wrote:  "While monitoring the network traffic of our own corporate WiFi network dedicated for mobile devices using the Kaspersky Unified Monitoring and Analysis platform" - which they call KUMA, K-U-M-A - "we noticed suspicious activity that originated from several iOS-based phones.  Since it is impossible to inspect modern iOS devices from the inside, we created offline backups of the devices in question, inspected them using the Mobile Verification Toolkits, and discovered traces of compromise.



"We are calling this campaign 'Operation Triangulation,' and all the related information we have on it will be collected on the Operation Triangulation page.  If you have any additional details to share, please contact us at triangulation@kaspersky.com.  Mobile device backups," they wrote, "contain a partial copy of the filesystem, including some of the user data and service databases.  The timestamps of the files, folders, and the database records allow the rough reconstruction of the events happening to the device.  The mvt-ios utility produces a sorted timeline of events into a file called 'timeline.csv,' similar to a super-timeline used by conventional digital forensic tools.



"Using this timeline, we were able to identify specific artifacts that indicate the compromise.  This allowed us to move the research forward, and to reconstruct the general infection sequence.  One, the target iOS device receives a message via the iMessage service, with an attachment containing an exploit.  Two, without any user interaction, the message triggers a vulnerability that leads to code execution."



LEO:  A zero-click.



STEVE:  Yes.  "Third, the code within the exploit downloads several subsequent stages from the command-and-control server that include additional exploits for privilege escalation.  Fourth, after successful exploitation, a final payload - a fully featured APT platform - is downloaded from the command-and-control server.  And fifth, both the initial message and the exploit in the attachment are deleted."



They said, finishing:  "The malicious toolset does not support persistence, most likely due to the limitations of the OS."  Or actually the strength of the protections that iOS is giving us.  So it's living in RAM, no persistence.  "The timelines of multiple devices indicate that they may be reinfected after rebooting.  The oldest traces of infection that we discovered happened in 2019."  Okay, now, so that's a correction to what my initial presumption was.  I had presumed they first saw this in 2019.  No.  In looking back at forensic evidence that was being collected, the oldest traces of the infection they discovered in their records occurred in 2019.



"As of the time of writing," they said, "in June 2023" - so that's now - "the attack is ongoing, and the most recent version of the devices successfully targeted is iOS 15.7."  So also current iOS.  Well, almost current.  "The analysis of the final payload is not finished yet.  The code is run with root privileges, implements a set of commands for collecting system and user information, and can run arbitrary code downloaded as plugin modules from the command-and-control server."



Okay.  So one of the things that gives this apparently long-running attack campaign so much power and longevity is, as Kaspersky wrote:  "It is impossible to inspect modern iOS devices from the inside, so we created offline backups of the devices in question."  To that observation we add the fact that this exploit fully covers its own tracks by deleting the exploitive attachment and the original attachment-carrying iMessage.



Now, add to that the fact that iMessage is end-to-end encrypted using private decryption keys that are only present in each of the endpoint devices' secure enclaves, and that these attacks are all individually targeted at their victims.  That means that communications traffic monitoring cannot be used since all anyone on the outside, like Kaspersky, who wishes they could see what was going on, will ever see is pseudorandom noise flowing back and forth.  Taken together, what all this means from a practical forensics and remediation standpoint is that this thing can never be caught.



And that brings up an interesting point that has never been observed during the 17-plus years of this podcast, which is, to exactly the same degree that Apple's seriously super-strong security is protecting the privacy of its users, it is equally protecting the privacy of exploits like this from discovery.  Now, as we know, the Pegasus exploits are left behind in their targets' phones to later be discovered, reverse engineered, patched, eliminated, and rendered inert.  But even as skilled a forensics team as Kaspersky can only observe an historical log of file modifications made by iPhone backups that indicate that something may have been happening to them, and is happening now, and for the past four years, with no idea by whom or to what end.  And no one, Kaspersky or anyone else, can take this any farther.



So I'll reiterate, since it seems like an important observation:  To exactly the same degree that Apple's seriously super-strong security is protecting the privacy of its users, it is equally protecting the privacy of exploits like this one from ever being discovered and eliminated.



LEO:  You know, it's funny, I saw that same sentence when I read the story days ago.  I didn't put two and two together.  It's an interesting problem, though.  But, I mean, you want them to have encrypted backups; right?



STEVE:  Oh, yeah.  I'm not saying that this is bad.



LEO:  It's not a bad thing, but it does prevent the discovery of the provenance.  Or removing it, yeah.



STEVE:  Yes.  And so the difference here is that this thing can delete itself.  And apparently the Pegasus exploits don't have that ability, or they certainly would.



LEO:  Right.



STEVE:  You know?  They get left behind, and then we can figure out who got attacked and what the attack was and what the exploit is, and we reverse engineer it, and Apple gives this an emergency iOS update so that none of us can have that happen to us, even though it's unlikely to because it's targeted.  This thing is four years in duration.  And it's, like, someone's got it.  Someone's using it.



LEO:  Right.



STEVE:  And iOS is protecting it.



LEO:  Now, if the victim unlocks the phone for the forensic analysis, couldn't that make it possible?  Or no?



STEVE:  The whole thing happens while the phone's locked.



LEO:  If it's gone already, there's nothing you can do; right?



STEVE:  Right.  It's received, executes, downloads some stuff...



LEO:  And erases itself.



STEVE:  ...and immediately deletes itself.



LEO:  Which is, by the way, a tale as old as time.  I mean, that's what hackers have always done is erase their tracks.



STEVE:  Right.  You immediately remove your own intervention from the logs in classic computer attacks.



LEO:  Right, right.



STEVE:  But anyway, so something is out there.



LEO:  Scary.



STEVE:  It is.  That's, yeah.  And it can go and get anybody it wants, is what this amounts to.  Anybody whose phone or iOS account is known can receive an iMessage and, in the blink of an eye, this thing is in and out and leaves behind a running advanced persistent threat alive in their phone until it's restarted that has apparently free rein.



LEO:  Apple could implement some form of logging that would make note of this and not be addressable by the hack.  I would think.



STEVE:  Yeah, I would think that's true.



LEO:  Yeah, some sort of syslog that is, you know, encrypted and protected.  Maybe they should.  I mean, that would solve it; right?



STEVE:  Or maybe, I mean, so do they know about this?  It's been going on for four years.



LEO:  Well, it's in the news.



STEVE:  Yeah, I mean, they know now.  



LEO:  Oh, you're thinking as Russia is asserting that they do know about this.  And they're intentionally...



STEVE:  No, no, no.  I still believe, I mean...



LEO:  Apple wouldn't do that.



STEVE:  I don't think they would.  Everything we know about them says that they - sorry, no, NSA, you're going to have to find your own way in.  And I can't [crosstalk]...



LEO:  If they're not going to help the FBI, they're not going to help the KGB.



STEVE:  Now, the way this might go away is just naturally.  There could be some new features added to iMessage in iOS 17.



LEO:  Right, right.  That just happened to fix it.



STEVE:  Yes, exactly.  They do a recompile, or they reimplement it, and it breaks the thing that they were using for so long.



LEO:  Right.



STEVE:  On the other hand, how many iOSes have we had over the last four years, and this thing apparently has stuck around across multiple iOS major version changes.



LEO:  And it's often in messages or the browser that you find these exploits because they have rendering engines that are basically code engines.



STEVE:  Right.



LEO:  And we've talked about that before.  It seems like every operating system should have some sort of forensic log that is permanent and unmodified.



STEVE:  Immutable, yeah.



LEO:  Yeah, immutable forensic log.  But maybe that's too much to ask.  I don't know.  You know, it certainly would be a burden on resources.  That's interesting.



STEVE:  These things have so much RAM now.



LEO:  Yeah, we got us some resources.  I think my phone, frankly, is barely in use.  I mean, it's just sitting there waiting to do something.



STEVE:  Yeah, you know, it may be the targeted nature of it, the fact that it deletes itself, you know, I mean, and like what, you know.  And so Apple just doesn't have the opportunity to get their hands on this.  Or I don't know.  Really, really, really interesting, though.



So last Thursday Google announced what they called their "Chrome Browser Full Chain Exploit Bonus" program.



LEO:  Woohoo.  That sounds good.



STEVE:  Yeah.  Yeah, it's a bonus.  Here's what they said.  They said:  "For 13 years, a key pillar of the Chrome Security ecosystem has included encouraging security researchers to find security vulnerabilities in Chrome browser and report them to us through the Chrome Vulnerability Rewards Program, the Chrome VRP.



"Starting today" - that was last Thursday, June 1st - "until December 1st, the first security bug report we receive which provides a functional full chain exploit resulting in a Chrome sandbox escape, is eligible for triple the normal full reward amount."



LEO:  Woohoo.



STEVE:  Your full chain exploit could result in a reward up to $180,000.



LEO:  Yikes.



STEVE:  And potentially more with other bonuses.  And because I didn't want people to stop after the first one, or to be discouraged, they said:  "Any subsequent full chains submitted during this time are eligible for double the full reward amount.  So $120,000 each."  So essentially Google's creating an extra incentive race among bug hunters.  Anything found before this coming December yields - anything found yields double the normal bounty, and the first person to supply an unknown exploit is rewarded for their trouble with triple the normal payment.



So they said:  "We've historically put a premium on reports with exploits" - they call them "high quality reports with a functional exploit" - "is the highest tier of reward amounts in our Vulnerability Rewards Program.  Over the years, the threat model of Chrome browser has evolved as features have matured, and new features and new mitigations, such as the MiraclePtr, have been introduced.  Given these evolutions, we're always interested in explorations of new and novel approaches to fully exploit Chrome browser, and we want to provide opportunities to better incentivize this type of research.  These exploits provide us valuable insight into the potential attack vectors for exploiting Chrome, and allow us to identify strategies for better hardening specific Chrome features and ideas for future full-scale mitigation strategies."



So, you know, 180 grand, that's not chicken scratch.  And if you're a competent reverse engineer hacker researcher person, there's an extra year's worth of income for you in however long it takes you to find something if you can be first.  And even if not, as long as what you find is unique, 120 grand.  So anyway, I've got a link in the show notes with the full participation details for anyone who might be interested.



Okay.  Oh, boy.  Exactly one week ago, last Tuesday, three quite conservative senators who belong to the Republican party introduced their "Know Your App Act."  Their announcement carries the headline "Wicker, Scott, Lankford Introduce Bill to Increase Transparency [and of course] Better Protect [the] Children Online."



The announcement of this new legislation begins:  "WASHINGTON - U.S. Senators Roger Wicker, Tim Scott, and James Lankford introduced the Know Your App Act.  The bill would require online app stores to display the country where apps are developed and owned."  So this is, I suppose, the natural follow-on from all the controversy surrounding TikTok, once it became clear to them that TikTok was not a breath mint.  They then go on to explain this new proposed legislation's intent, starting with quotes from each of the three senators.



Roger Wicker leads with his quote:  "Our adversaries will exploit every available tool, including popular apps that gather huge amounts of data on Americans, to gain an advantage over the United States.  It is crucial for users to take steps to limit their exposure and be made aware of the risks associated with using foreign-controlled apps."  I know.  It gets worse, Leo.



"The Know Your App Act would bring much-needed transparency to app stores, empowering Americans to safeguard their families from exploitation."  You know, from the Commie bastards.  Wow.  Then we hear from Tim Scott:  "Americans should be able to make informed decisions about the online services they use in order to protect their data and security.  Requiring app stores to display an app's country of origin is a commonsense solution that can help them do just that.  Parents shouldn't fear that their family's online privacy and security could be compromised when unknowingly using an app owned by a foreign adversary."



And finally, James Lankford adds:  "Seeing 'Made in China' on nearly any product nowadays is frustrating to Oklahomans" - he's a Republican Senator from Oklahoma - "frustrating to Oklahomans trying their best not to prop up the Chinese Communist Party and Chinese government with their hard-earned money.  We already see the ways the TikTok app is a dangerous extension of the CCP that is collecting every user's personal data and all of their contacts.  I want the 'Made in China' label and labels for any other countries where apps like TikTok originate to be clearly marked when and where they are downloaded.  Americans should remain free to buy items from wherever they want."



LEO:  Please.



STEVE:  I know, "But the least Big Tech can do is label where Americans' money" - these are all free, by the way - "is going when they download in the app store."  Okay, now, aren't all iPhones and iPads...



LEO:  Made in China, yeah.



STEVE:  ...and pretty much iAnything also made in China?



LEO:  Although they, by law, they do have to put on it Made in China.  Anything made in China by law has to be labeled that way.  The problem is that software development's not like manufacture.  Most programs are written all over the world by a variety of teams; right?



STEVE:  Yeah.  You know, and the guts of our cars.



LEO:  Oh, yeah.



STEVE:  It's why that COVID-related Chinese chip shortage messed up U.S. and foreign automobile manufacture and jacked up the cost of used cars that already had all their chips built in.  So but it's worse.  The legislative announcement continues with:  "As of March 2023, four of the five most popular apps in the U.S." - this is the proposed legislation from last Tuesday - "four of the five most popular apps in the U.S. were developed in China.  This is particularly concerning given that China's national security laws provide a pathway for the Chinese Communist Party to compel application developers to control an application's content or user data.  The Know Your App Act responds to this risk by requiring online app stores to display prominently the country where apps are developed and owned, allowing users to make informed decisions about the applications they access."



And then it gets worse:  "The bill also requires the U.S. Department of Treasury and U.S. Department of Commerce to produce a list of adversarial governments that may have undue control over application content moderation, algorithm design, or user data transfers.  App stores would be required to provide users the ability to filter out applications from the identified adversarial countries and" - get this - "warn users about the risk of downloading one of the foreign applications on these lists.  If a developer fails to provide sufficient information to the app store about its country affiliation, the app store would be required to issue multiple warnings over a designated period.  If the developer still refused to comply, the app store would be required to remove the app from its store."  Says the party that doesn't want the government to be involved in stuff.



LEO:  I mean, this is minor.  I guess it's possible, and it's certainly not a hardship, I would guess.



STEVE:  Well, yeah, I mean...



LEO:  It's meaningless.  It's not going to change anything.



STEVE:  Right.  So products, as we know, as you said, Leo, are routinely marked with their country of origin.  So that's not any new big deal.  But here we're implicitly saying that any applications made in China are inherently dangerous for that reason, which is what really feels wrong to me.  And it's not going to work anyway.  If someone wants Temu, TikTok, CapCut, or Shein, those are the top four or five.  Meta's thing I'm blanking on right now is the fourth of the top five.  Instagram.  That rounds out the top five.  If they want those apps, that's what they're going to download.



No one who needs to use these apps for their intended purpose is going to care where they came from.  And the idea of requiring an app store to caution and warn a user that an app was developed in China with an "Are you sure?" before it can be downloaded seems to me really unfair.  Perhaps I'll be proven wrong in time, but today this seems really over the top.  Increasing the tension and division between two of the world's superpowers doesn't seem like a winning strategy for anyone.  Wow.  Made in China for apps.



Okay.  The Tor Project is testing a new Denial of Service mitigation feature for their network, where servers will require their connecting clients to solve a puzzle to access its resources.  So, you know, it's basically a CAPTCHA; right?  Now, it won't normally be enabled, but it will become active when a server is being overloaded with bogus attack requests.  The idea is to allow authentic users to connect to a Tor service while weeding out automated DDoS attacks, which in recent times have become a big problem for Tor servers.



The new feature is currently being tested in the Tor alpha software and is expected to roll out to most Tor nodes later this year.  Work on the feature started last year after Tor node operators reported DDoS attacks against their infrastructure.  And, you know, probably hacker kiddies who were attacking nodes because they don't like what some other hackers are able to do there, you know, like competing something as a service, you know, fill in the blank.  You know, Malware as a Service, Hacking as a Service, you know, anything as a service stuff.  So, fine.



When I caught this recent news of a just-published survey, it helped to resolve the mystery surrounding why was it that so many school districts were failing and falling to ransomware attacks.  Get a load of this.  According to a study published by the Consortium for School Networking, a professional association of school systems technology providers, fully two out of every three school districts - and this is districts, not just individual schools.  Two out of three districts do not employ the full-time services of someone specializing in cybersecurity.  Districts.  And one in eight districts don't even allocate any funds for cybersecurity defense whatsoever.  So to me, in this day and age, that's unconscionable and astonishing.



Think of the challenge that they're facing.  Large enterprises have it tough enough where they're servicing employees on their networks who are at least trying to do the right thing by not clicking on everything they receive in email.  But a school district of any appreciable size would be an insanely complex network to secure, especially given that the interior of its network is filled to the brim with rambunctious children and teenagers, half of whom are probably attempting to hack their school's network from the inside.  I just can't imagine how you secure a school district's network.



And as an aside I'll just note that it's a VERY GOOD THING, in bold capitals, that the Internet didn't happen while I, well, was there, that it didn't happen until after I'd already attended my high school's 10th reunion.  As it was, remember, I told the story of the "Portable Dog Killer" adventure that had the district's technicians climbing around in the rafters trying to locate the source of the near ultrasonic sounds that everyone was hearing that day.



And I mentioned at the end of the story that Vice Principal Archibald knew me on sight.  Remember he faked me out by suddenly spinning around after he was deliberately walking away, and then he pointed at me.  Now, I don't recall whether I mentioned that one of the many reasons that Vice Principal Archibald knew me by name was because at one point I was caught holding a copy of the master janitor's grand master key to the entire San Mateo Union High School district which unlocked every single school door district-wide.



LEO:  Oh, boy.



STEVE:  So, yeah, they knew my name.  So I shudder to think what would have happened had I been there once the school was networked because, boy, that would have been - would've been a lot of fun.



LEO:  Oh, man, you would have had a fun time.



STEVE:  Oh, yeah.  Anyway, so the idea that today so little attention is being paid to cybersecurity in two out of every three school districts, well, that's just nuts.  That's, I mean, I don't want to say they deserve what they're getting.  Nobody deserves ransomware attacks and having all your servers scrambled and encrypted.  But, boy, you know, really.  Maybe the message is finally sinking in.



Okay, now, normally innocuous civilian security cameras can be trouble, it turns out, in times of conflict.  The Ukrainian Security Service, the SSU - probably Security Service for Ukraine - has asked its citizens to please disconnect any security cameras they may have which are aimed at public spaces.  The SSU says Russia is exploiting vulnerabilities in modern security cameras to access these cameras' feeds, and they have proof that Russia is using these feeds to time their launching of missile attacks and to adjust attack targeting in real-time.



After the SSU sent SMS messages to all Ukrainian citizens carrying this request last week, several Russian military bloggers suggested that the agency may be trying to mask the movement of its own troops, that is, Ukrainian troops, leading up to its impending Russian counteroffensive.  And there may be some truth to that, too.  But in any event, wow, talk about unintended side effects of otherwise innocent cameras that may be looking out into public areas and may not be as secure as people would hope they were.



We've recently been looking at the NSO Group with their Pegasus smartphone malware and other malware-for-hire groups.  One of the worries surrounding the availability of off-the-shelf spyware tooling is that those who could never manage to do this themselves now only need money to purchase what they want, thus empowering them.  A similar set of services has been emerging over the past several years which, for lack of any better term, we might call "cyber mercenaries," you know, hackers-for-hire.  Exactly three years ago, in June, Reuters published an exclusive piece of reporting titled:  "Obscure Indian cyber firm spied on politicians, investors worldwide."



The first two lines of their full report state:  "A little-known Indian IT firm offered its hacking services to help clients spy on more than 10,000 email accounts over a period of seven years. New Delhi-based BellTroX (B-e-l-l-T-r-o-X) InfoTech Services targeted government officials..."



LEO:  That sounds made up.  That sounds like a movie name.  All right.  All right.



STEVE:  BellTroX Infotech Services.



LEO:  Infotech Services.



STEVE:  Well, it does sound like an Indian IT name, I think.  They have fun names - "...targeted government officials in Europe, gambling tycoons in the Bahamas, and well-known investors in the United States, including private equity giant KKR and short seller Muddy Waters, according to three former employees, outside researchers, and a trail of online evidence."



And today, The New Yorker magazine has a wonderful profile of this same firm, BellTroX.  It's actually - so it's Be-l-l-T-r-o-X.  So I don't know.  I would think you'd say BellTroX.  But, you know, funky capitalization.  And what the New Yorker describes as India's budding cyber mercenary market, which outsources hacker-for-hire services across the globe, with the Indian government's tacit acceptance.



Now, okay.  This podcast could make a full meal out of this coverage, but it's time for us to get to this week's very interesting discussion of the backdoors that have been carelessly designed into many of today's most popular motherboards.  So I've provided the links to both of these fascinating stories for anyone who wants to take the time to learn more.  Suffice to say it is now possible for those without any cyber hacking skills to simply rent such cyber skills from mercenaries of any skill level required to obtain whatever cyber outcome might be needed, for a fee.



I have two quick Closing-the-Loop pieces from our listeners.  James Brooks said:  "While I agree that the 'Request OTR' is a noble idea" - that was the topic of last week's podcast, the idea that the Brave browser has implemented, which would allow a website to proactively ask a browser to ask its user if they would like to have their use of the site kept private and flushed the moment they leave.  Anyway, so he says:  "It's a noble idea."  He says:  "I have concerns that sites looking to victimize my kids will include this header.  All they need to do is prep the user for the prompt, and my kids are on a dangerous site with little chance of me having visibility into it."



And so I thought that was an interesting point.  I guess I would say that relying on browser history is probably not strong protection either.  And I would be using a DNS service that didn't resolve dangerous sites, you know, as a good means of protection.  But I certainly do take the point that this could be used by other sites that want to victimize their users and not leave a trail of that having happened.



Mark Newton, he said:  "What are some good ways to block TLDs such as .zip and .mov if people don't have the ability to block it at their firewall?  I was thinking the hosts file and adding a 127.0.0.1 with a *.zip.  Any thoughts?"  Okay.  Unfortunately, the Windows HOSTS file won't handle wildcards.  Be nice if it did.  It doesn't.  I'd say that the best solution would be to use an external security-oriented service such as NextDNS.  You can set up your configuration in, for example, NextDNS to block entire TLDs.  And that's something that I know our listeners, because I have some other feedback, have already done.  So I think that's what I would do.  The idea of having a security-oriented DNS provider for many purposes, both the ones that James Brooks brings up and that Mark Newton was asking about, I think makes a lot of sense.  What does not make a lot of sense, Leo, is Windows Platform Binary Table.



LEO:  Even though it has a great name.



STEVE:  Even though it's so catchy, you know, you can't even say the abbreviation quickly, WPBT.  It's too bad it's not BLT.  That'd be good.  Windows BLT.



LEO:  Microsoft is terrible at naming things.



STEVE:  Platform Loader Table, that'd be good, Windows BLT.  But no.  Binary Loader Table.  No.



LEO:  No, no, no.



STEVE:  The biggest news of the week, that rocked the security world, was the revelation that the very popular motherboards made by Gigabyte - and I'm typing these show notes on one right now, sitting in front of a machine that's based on a Gigabyte - were found to be secretly downloading code that the motherboard would then cause Windows machines to execute.  And what was extra disturbing was that the TCP connection over which this download took place was neither authenticated nor encrypted.



LEO:  Yeesh.



STEVE:  Yeah.  To everyone's shock and horror, and the source of a great many terrific headlines, this meant that it would be trivial for bad guys to intercept these communications to install their own rootkit malware.  Doesn't sound good.  But if my demeanor makes you wonder whether you're detecting that I'm somewhat less scandalized by these revelations than the rest of the security press, you would be correct.  And that's not because all of the above is not true.  It is all true.  It's because we're all completely fooling ourselves in the belief that Windows operating systems actually offer any true security in the first place.



LEO:  That's an angle I didn't really think of.  Motherboard security, who cares?  The operating system's full of holes.



STEVE:  Yeah, I mean, we're just glad it boots.  Windows has never, never been a secure operating system, and to meet the demands of the marketplace it never can be.  This should have been obvious when Windows was first placed onto the Internet and openly published everyone's C: drive to the entire world.  It sounds insane to say that now, but we know it happened.  I was ridiculed certainly after the birth of this podcast for suggesting that the original Windows metafile format (WMF) deliberately supported the ability to execute native code that was carried in the metafile.  I'm absolutely certain that it did, just as I'm sure that it once made sense to the developer who had added that escape function.



But years later, the world was a very different place, so the industry was horrified by that discovery and thought that the only possible way it could happen was by mistake.  Mark Russinovich reverse engineered the Windows' metafile interpreter, as I had; and he said, "It sure does look deliberate."  There was never any doubt.  My point is, context matters, and the world is constantly changing.



Last week I said that it's not fun to use a truly secure operating system because you can't actually get any work done.  First of all, it's not at all clear that we even know how to create a secure operating system because we haven't figured out how to create secure software.  So we're resorting to erecting various types of fencing around our admittedly insecure attempts by using hardware-enforced protection levels and sandboxes and virtual machine containers and so on.  So, you know, we're still stumbling forward.



Microsoft has recently made headlines by announcing that they're going to rewrite the Windows kernel in RUST for much better security.  That's great.  But are they also going to then prohibit the use of any third-party peripheral device drivers, all of which run alongside their shiny new RUST code in the kernel?  If not, then any notion of true security is just marketing hype.  That's not to suggest that rewriting the kernel in RUST will not be useful.  If you plug a hole in a large block of Swiss cheese, you do at least have one fewer holes.



So I wanted to kick off our discussion today of the Windows Platform Binary Table with a wee bit of a reality check.  Make no mistake:  What Eclypsium discovered was not good.  So another previously unsuspected hole in the Swiss cheese has been plugged.  But no one should imagine that doing so meaningfully increases Windows' actual security.  That said, we do need to keep trying.  The quest for security will, I think, guarantee employment for anyone who is capable and competent in the field.  Even if school districts are not hiring, everyone else is.



Okay.  So what's this Windows Platform Binary Table?  It's a facility which Microsoft first defined and implemented 11 years ago, back in 2012, and it was first supported in Windows 8.  It defines a clear, clean, and well-documented means for the platform from which Windows is booted - our motherboards - to provide Windows with its own code, previously stored within its firmware, which Windows, ever since Windows 8, will look for and execute when it's present as part of the Windows boot process.



Now, if your first thought is that this also perfectly describes the operation of a motherboard-based rootkit, you would be correct in your thinking.  Because it was foreseeable that advanced motherboards might need to have the capability to reach up into the operating system to take advantage of its rich array of advanced services and connectivity, like downloading and installing their own firmware interface drivers, or perhaps updating their own firmware itself, and since Microsoft did not want motherboards each inventing their own horrible kludges in order to do this, Microsoft formalized this capability in what's known as the Windows Platform Binary Table.



And since this podcast likes to stay on top of such things, I should note that this is not the first time we've talked about this here.  Security Now! Episode 521 which we recorded on August 18, 2015, was titled "Security Is Difficult."  Okay.  I don't know if that was a new concept for us back then, but that's what we called that podcast.  And during that podcast we discussed the Windows Platform Binary Table facility.  This was in the context at the time of Lenovo laptops which were found to be behaving badly.



And this is also not the first time that Eclypsium has surfaced with some worrisome news regarding the operation of Windows WPBT.  Eclypsium's posting of September 23rd, 2021, so about a year and a half ago, not quite, was titled "Everyone Gets a Rootkit."  I'll share their Executive Summary from that posting since it serves to set the stage for what follows today.



Not quite two years ago, they wrote:  "In a connected, digitally transformed age, the term 'no good deed goes unpunished' could perhaps be rephrased as 'no feature goes unexploited.'"  Or as we like to say here on this podcast, what could possibly go wrong?  "The protocol, called Advanced Configuration and Power Interface (ACPI) was introduced in the early 2000s when it became apparent that the energy consumption of billions of rapidly proliferating computing devices was a significant and increasing drain on national and regional energy supplies."



I remember that.  It was like this was - remember you could buy green computers.  That was a big thing.  Everyone was like, green monitors and green computers.  It's like, okay.  "So ACPI was designed to efficiently manage energy consumption on PCs, along with several additional well-meaning use cases.  As laptop usage and portable computing became universal demands, ACPI became a de facto standard for nearly all systems."  And yes, I think it's on everything now.



"With the advent," they wrote, "of Windows 8, the protocol evolved to include an object called the Windows Platform Binary Table and has since been included in every single Windows OS shipped since 2012.  In June of 2021, so what's that, that's exactly two years ago, Eclypsium researchers discovered significant flaws in WPBT.  These flaws make every Windows system vulnerable to easily crafted attacks that install fraudulent vendor-specific tables.  These tables can be exploited by attackers with direct physical access, with remote access, or through manufacturer supply chains.  More importantly, these motherboard-level flaws can obviate initiatives like Secured-Core and Secure Boot because of the ubiquitous usage of ACPI and WPBT.  Security professionals need to identify, verify, and fortify the firmware used in their Windows system."



Okay.  So that was their overview.  A bit later, here's their description of the issue at the core of this problem:  "The Eclypsium research team" - this is, again, two years ago, two and a half - "has identified a weakness in Microsoft's WPBT capability that can allow an attacker to run malicious code with kernel privileges when a device boots up.  WPBT is a feature that allows OEMs to modify the host operating system during boot to include vendor-specific drivers, applications, and content.  Compromising this process can enable an attacker to install a rootkit compromising the integrity of the device.



"The issue stems from the fact that, while Microsoft requires a WPBT binary to be signed, it will accept an expired or revoked certificate.  This means an attacker can sign a malicious binary with any readily available expired certificate.  This issue affects all Windows-based devices going back to Windows 8 when WPBT was first introduced.  We've successfully demonstrated the attack on modern, Secured-Core PCs that are running the latest boot protections.  This weakness can be potentially exploited via multiple vectors, for example, physical access, remote, and supply chain; and by multiple techniques, malicious bootloader, DMA, et cetera.  Organizations will need to consider these vectors and employ a layered approach to security to ensure that all available fixes are applied, and identify any potential compromises to devices."



And then they quote Microsoft:  "Microsoft recommends customers use Windows Defender Application Control (WDAC) to limit what is allowed to run on their devices.  WDAC policy is also enforced for binaries included in the WPBT and should mitigate this issue.  We recommend customers implement a WDAC policy that is as restrictive as practical for their environment."  Yeah, it's easy for Microsoft to say.  It's difficult to live with, as I said.  If you have a really secure computer, you can't get any work done.  Then they said:  "You can find documentation on WDAC," and then we provide a link.



So, okay.  It makes sense that WPBT-based code execution would have these problems.  First, all certificates expire, and all code-signing certificates expire.  Right now, at this moment, my latest code-signing certificate from DigiCert has expired.  I haven't renewed it since I've been working on SpinRite in DOS, so there's been no need.  But the moment I get the DOS side finished, I'll be integrating it into its Windows executable.



So the first thing I'll do is renew that cert.  But right now it's expired.  And the Windows executables it signed - SQRL, InControl, and the DNS Benchmark - all run right now without complaint and show that the signature is valid.  The reason for this is that, unlike for HTTPS TLS connections which, you know, unlike those for code signing, the only requirement is that the certificate be valid at the time of that signing.  That's the test that's employed.  So it's unsurprising that Eclypsium discovered this, although it is a bit distressing.  And it turns out that's not even a requirement for signing code that is loaded through WPBT.



The second issue, of continuing to honor proactively revoked code-signing certificates is also worrisome.  As we know, when valid certificates are found to have escaped into the wild, their revocation until they naturally expire on their own, which wouldn't help us either, is our only recourse.  What Eclypsium found was that Windows is not checking for certificate revocation at this early time during Window's booting.  Perhaps it's unable to do so.  So this means that once someone obtains a code-signing certificate - which is not a high bar to jump over if you want to actually get your own, and which has a three-year lifetime - even if that certificate is discovered being misused in the wild, and it's revoked, Windows will continue to honor any boot time code that was signed by it.



And the situation might even be worse, since it doesn't appear that even a certificate's validity, as I mentioned, at the time of signing is required.  Here's what they said.  They said, and this is all still two and a half years ago:  "WPBT requires any binaries to be properly signed.  Microsoft's WPBT code-signing policy states" - this is Microsoft - "All binaries published to Windows using the WPBT mechanism outlined in this paper must be embedded signed and timestamped.  These images should be linked with the /INTEGRITYCHECK option and signed using the SignTool command with the /nph switch to suppress page hashes."



Then Eclypsium said okay, that's all well and good, that's what they're saying.  "However," they said, "our testing revealed that this check will pass even if the code-signing certificate has been explicitly revoked.  To highlight this, we signed our malicious test code using a Hacking Team code-signing certificate that was revoked in 2015."  Okay, and they did this in 2021 after that cert would have expired.  So not only revoked, but expired.



They said:  "This and many other expired or revoked certificates are readily available to anyone on GitHub.  This particular cert was revoked when the company's tools including a UEFI rootkit which was exposed in a major breach.  We used this certificate just to highlight a particularly egregious example, but the same process would work with other revoked certificates, as well.  This attack is also significant because it allows an attacker to drop a malicious file in the OS, even while BitLocker encrypts the disk.  BitLocker would prevent other types of threats such as the Hacking Team implant, which used firmware to directly write the malicious code to disk.  However, in the case of WPBT, an attacker could avoid BitLocker since the malicious file is pushed to the OS and stored in c:\windows\system32 and run on startup, regardless of BitLocker."



Okay.  So in other words, this entire WPBT mess is a huge hole in Windows boot security.  Like I said, security?  You know, marketing hype.  Windows is not secure and is arguably not securable.  Essentially, motherboards must be absolutely and utterly trusted because they, with Microsoft's full blessing, and a spec for how to do this, are able to completely subvert the security of the Windows boot process.



And as we also saw, Microsoft's only solution and recommendation is for those concerned by this behavior to manually apply their Windows Defender Application Control, which, for what it's worth, is active at this early boot stage.  So it can be used to govern what the motherboard is able to inject and then cause to run.  Unfortunately, WDAC is typically used in a blacklisting mode, not in a whitelisting mode, because whitelisting isn't at all practical in the real world, just like true security isn't practical.  This means that the specific filename being used and injected into the system32 directory would need to be known, and you would need to know that the firmware was never going to change that name to something different, and how would you know?



So with this background two and a half years ago, we're caught up with today, which makes the conclusion I previously drew all the more chilling.  I said:  "Motherboards must be absolutely and utterly trusted because they are able to completely subvert the security of the Windows boot process."



What put Eclypsium back in the news Wednesday of last week and generated so many hysterical headlines was that they discovered that Gigabyte motherboards were not injecting and running implant code into Windows - no, wait, I've got a "not" in there that shouldn't be.  They discovered that Gigabyte motherboards, but on the other hand all other motherboards are, too, I mean, all manufacturers are doing this now.  They discovered Gigabyte motherboards were injecting and running implant code into Windows, and that this code was then downloading and running code from non-secured web servers over HTTP, which, you know, that means they could be in Russia, right, where they use HTTP; or unenforced HTTPS.



Here's what Gigabyte posted.  They said:  "Recently, the Eclypsium platform began detecting suspected backdoor-like behavior within Gigabyte systems in the wild.  These detections were driven by heuristic detection methods, which play an important role in detecting new, previously unknown supply chain threats, where legitimate third-party technology products or updates have been compromised.



"Our follow-up analysis discovered that firmware in Gigabyte systems is dropping and executing a Windows native executable during the system startup process, and this executable then downloads and executes additional payloads insecurely.  It uses the same techniques as other OEM backdoor-like features like Computrace, a.k.a. LoJack DoubleAgent, which is abused by threat actors and even firmware implants such as Sednit LoJax, MosaicRegressor, and VectorEDK.  Subsequent analysis showed that this same code is present in hundreds of models of Gigabyte PCs.  We are working with Gigabyte to address this insecure implementation of their app center capability."



They wrote:  "In the interest of protecting organizations from malicious actors, we are also publicly disclosing this information and defensive strategies on a more accelerated timeline than a typical vulnerability disclosure."  In other words, this is so bad it can't wait.  "This backdoor appears to be implementing intentional functionality and would require a firmware update to completely remove it from affected systems."  In other words, it's not a bug, it's a feature.



"While our ongoing investigation has not confirmed exploitation by a specific threat actor, an active widespread backdoor that is difficult to remove poses a supply chain risk for organizations with Gigabyte systems.  At a high level, the relevant attack vectors include compromise in the supply chain, compromise in the local environment, or malware persistence via functionality of this firmware in the systems.  A more detailed analysis of these risks is provided with suggested mitigations.  After a more traditional vulnerability disclosure timeline, we plan to publish details about how this works."  Which will be, you know, why?  We all know how it works.  



"There are two important aspects of our findings.  First, Eclypsium automated heuristics detected firmware on Gigabyte systems that drops an executable Windows binary that is executed during the Windows startup process.  And second, this executable binary insecurely downloads and executes additional payloads from the Internet.  So that's the key issue here.  Without any ability to intervene, observe, or control, hundreds of Gigabyte motherboard models have the ability [and are] causing Windows to go out onto the Internet to fetch and run additional Windows executables every time a system is booted.  And what's worse, this is done by fetching those files over unauthenticated and unencrypted HTTP."



So in their analysis of the firmware which is dropping the OS executable they said:  "An initial analysis of the affected UEFI firmware identified the following file."  And I can't even read this.  It starts at 8ccbee6f7858ac6b9, and that's about the first quarter of it.  And it ends in 0c715971b2.bin.  So that's the file, a .bin file with a ridiculous name.



They say:  "This is a Windows Native Binary executable embedded inside of UEFI firmware binary in a UEFI firmware volume.  This Windows executable is embedded into UEFI firmware and written to disk by firmware as part of the system boot process, a technique commonly used by UEFI implants and backdoors."  In other words, it's doing what bad things do.  They said:  "During the Driver Execution Environment (DXE) phase of the UEFI firmware boot process, the WPBTDXE.efi firmware module loads the embedded Windows executable into memory, installing it into a WPBT" - that's the Windows Platform Boot Table - "ACPI table which will later be loaded and executed by the Windows Session Manager Subsystem."  That's smss.exe.  That's the thing that actually does this.



And they go on to explain that although this setting appears to be - oh.  This is important.  I shouldn't skip this.  The WPBTDXE.efi module checks if the APP Center Download and Install feature has been enabled in the BIOS/UEFI setup before installing the executable into the WPBT ACPI table.  They said:  "Although this setting appears to be disabled by default, it was enabled on the system we examined."  So that's first thing to know.



This behavior is something that appears to be configurable by the user in the firmware setup.  And they're saying it was off by default, which is fabulously good news.  APP Center Download and Install feature was on on the one that was generating this traffic, they say disabled by default.  So that's good news.  I guess the only danger would be if someone's setting it up, or maybe a consultant who sets it up and wants this to be all taken care of, thinks that this is a good thing to turn on, so it may be enabled.  You'll have to see.



This executable uses the Windows Native API to write the contents of an embedded executable to the file system at, you know, the system root, typically c:\windows, then \system32\, and it's called GigabyteUpdateService.exe.  It then sets registry entries to run this executable as a Windows Service.  So it's creating a service which Windows will be running.  The mechanism described here is similar to the methods used by other UEFI firmware implants such as LoJax, MosaicRegressor, MoonBounce, and VectorEDK, referenced previously.



Also note that this is not then transient.  It does it presumably every time, but this Gigabyte update service, it's sticking around.  It's physically written into the system32 directory, and it's defined as a service that will be started every time the system boots, whether or not you have subsequently disabled this APP Center Download and Install.  So that's important.  If you found it on, and you turned it off, you're not out of the woods because it being on previously meant that GigabyteUpdateService.exe was written to system32 and defined as a service.



Okay.  So then secondly they talk about downloading and running further executables.  The dropped Windows executable, that's that Gigabyte update service, is a .NET application.  It downloads and runs an executable payload from one of the following locations, depending upon how it's been configured.  And they give us three different URLs.



The first one is http://, and it starts with mb.download.gigabyte.com/, then FileList/, and then Swhttp and then /LiveUpdate4 is the last directory.  The second URL is an HTTPS URL, so that's interesting.  And otherwise the URL is identical.  So that says they're running servers on both port 80 and on port 443 and accepting connections either insecurely on HTTP or over TLS with HTTPS.  And the third one is also interesting.  It's not a fully resolved public domain.  It's https://software-nas/Swhttp/LiveUpdate4.  So that would be a locally resolved IP that would allow Gigabyte motherboards to go download stuff inside of an internal LAN, not reaching out into the public.



They said:  "Plain HTTP, the first bullet" - the first URL - "should never be used for updating privileged code [duh] as it is easily compromised via machine-in-the-middle attacks.  However, we noticed that even when using the HTTPS-enabled options, remote server certificate validation is not implemented correctly.  Therefore, MITM (man-in-the-middle) is possible in those cases also."  In other words, insecurely.



"The firmware does not implement any cryptographic digital signature verification or any other validation over the executables it downloads.  The dropped executable and the normally-downloaded Gigabyte tools do have a Gigabyte cryptographic signature that satisfies the code-signing requirements of Microsoft Windows, but this does little to offset malicious use, especially if exploited using Live-off-the-Land techniques like in the recent alert regarding Volt Typhoon attackers.  As a result, any threat actor can use this to persistently infect vulnerable systems either via MITM or compromised infrastructure.



"These issues expose organizations to a wide range of risks and attack scenarios."  And we've got four bullets.  "First, abuse of an OEM backdoor by threat actors.  Previously," they say, "threat actors have taken advantage of legitimate but insecure/vulnerable OEM backdoor software built into the firmware of PCs.  Most notably, Sednit group, which is APT28, also known as Fancy Bear, exploited Computrace LoJack to masquerade as legitimate laptop antitheft features.



"Secondly, compromise of the OEM update infrastructure and supply chain.  Gigabyte does have documentation on their website for this feature, so it may be legitimate.  But we cannot confirm what is happening within Gigabyte.  In August of 2021 Gigabyte experienced a breach of critical data by the RansomExx group, and then experienced another breach in October of 2021 by the AvosLocker group."  In other words, their security's not that great, apparently.  And all these Gigabyte motherboards that have this thing enabled are downloading whatever happens to be on the server at the time.  



They say:  "Third, persistence using UEFI Rootkits and Implants is another vector.  Fourth, man-in-the-middle attacks on firmware and software update features.  And finally, ongoing risk due to unwanted behavior within official firmware.  Backdoors hidden within UEFI or other firmware can be hard to remove.  Even if the backdoor executable is removed, the firmware will simply drop it again the next time the system boots up.  This challenge was demonstrated before when trying to remove Computrace LoJack and related to vulnerabilities in Lenovo Service Engine on notebooks and laptops."



Okay.  So that's what was discovered, that Gigabyte motherboards were insecure in their downloading of whatever happened to be available on those URLs, and that Windows would then run those things.



The following day, immediately, on last Thursday June 1st, Gigabyte responded with their posting "Gigabyte Fortifies System Security with Latest BIOS Updates and Enhanced Verification."  Their posting is short, so I'll share it.  They said:  "June 1st, 2023.  Gigabyte Technology, one of the leading global manufacturers of motherboards, graphics cards, and hardware solutions, has always prioritized cybersecurity and information security."  Okay.  All evidence to the contrary.  "Gigabyte remains committed to fostering close collaboration with relevant units and implementing robust security measures to safeguard its users.



"Gigabyte engineers have already mitigated potential risks and uploaded the Intel 700/600 and AMD 500/400 series Beta BIOS to the official website after conducting thorough testing and validation of the new BIOS on Gigabyte motherboards.  To fortify system security, Gigabyte has implemented stricter security checks" - or you might say any security checks - "during the operating system boot process.  These measures are designed to detect and prevent any possible malicious activities, providing users with enhanced protection.



"First, Signature Verification:  Gigabyte has bolstered the validation process for files downloaded from remote servers.  This enhanced verification ensures the integrity and legitimacy of the contents, thwarting any attempts by attackers to insert malicious code.



"Two, Privilege Access Limitations:  Gigabyte has enabled standard cryptographic verification of remote server certificates."  What a concept.  "This guarantees that files are exclusively downloaded from servers with valid and trusted certificates, ensuring an added layer of protection.  BIOS updates for the Intel 500/400 and AMD 600 series chipset motherboards will also be released on the Gigabyte official website later today, along with updates for previously released motherboards.  Gigabyte recommends that users regularly visit the official Gigabyte website for future BIOS updates."  So that's great.  If you have a Gigabyte motherboard, now would be a good time to update the BIOS.  There are very insecure practices still in firmware until the BIOS is updated.



So we have some takeaways.  If you have a Gigabyte motherboard, and you dislike the danger that's inherently presented by having it reaching out to anyone other than Microsoft to obtain unmanaged updates to your system, you'll likely want to disable APP Center Download and Install if it's present in your motherboard firmware.  And as I noted, you'll also want to look in the system32 directory for the Gigabyte updater which has been installed as a service, and shut that down as a service.  You could look in Windows Service Manager for it and stop it and then set it to disabled, and that would keep it from running.



And finally, the best news from all of this is there is a fully generic undocumented registry entry that can be added to Windows, any Windows 8 through presumably ever, to shut down all of this Windows boot behavior.  The registry key, it's under HKEY_LOCAL_MACHINE\SYSTEM\CurrentControlSet\ Control\Session Manager, because remember it's the Session Manager because remember it's the session manager that's responsible for this behavior.  And if you set - and I've got this all documented - "DisableWpbtExecution," you create a DWORD with that name and set it to one, Windows will no longer do this.  No matter what motherboard you are running under, this will no longer happen.



The Windows Session Manager component looks for this registry key and will abort the execution of anything being provided by the motherboard if this setting is present.  So that's the number one most robust thing that any Windows user can do to shut down and disable this almost certainly unwanted Windows behavior.  Because it's universal and safe, I've made this registry file this week's GRC Shortcut of the Week.  So just put "grc.sc/926" into the URL of any browser.  It will obtain a zip file named DisableWPBT.zip, whose contents is exactly what's shown above, a .reg file which, when double-clicked, and then you confirm that it's what you want to do, will add that DisableWpbtExecution DWORD into the local machine's registry, giving it a value of one.



The reason that adding this disablement key to your Windows Registry is probably the right thing to do is that this Gigabyte event should serve us as a wakeup call.  While this focused upon just Gigabyte, as I said before, all systems are now known to be using this mechanism for maintaining their firmware.  Microsoft officially created this and blessed it and said this is what you should do.



And as for what could possibly go wrong, just ask those tens of thousands of HP OfficeJet 9020e printer users whether they wish they had not had their printer connected to the Internet early last month.  In other words, supply chain problems can and do occur.  As Eclypsium noted, Gigabyte has been previously penetrated twice.  So having every Windows system happily downloading third-party software from who knows where, you know, seems like an unwarranted and unnecessary risk.  Grc.sc/926.  That will get you the zip file containing the reg file that you can apply to your Windows instance to stop it from doing that.  And as we say, Leo, that's the show.



LEO:  I'm glad you covered this.  Yeah, we've been talking about it a little bit, as well.  Of course the fact that they offered a patch firmware update is great.  But a lot of people will never update their firmware.  A lot of people don't listen to this show.  And there's a problem.  Therein lies the problem; right?



STEVE:  Yeah, yeah, big problem.  



LEO:  Yeah.  That's why you listen to the show, and you should keep listening to the show every Tuesday.



Copyright (c) 2023 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.


GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#927

DATE:		June 13, 2023

TITLE:		Scanning the Internet

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-927.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we examine what happens to your monthly cloud services bill if you're infected by cryptomining malware.  And speaking of cloud services, is Elon paying his bills?  Just how fast are IoT-based DDoS attacks rising?  What was the strange tale of wayward Chinese certificate authority?  What useful new privacy and security features will Apple be adding to their services with their net OSes this fall?  And why has France headed in another direction?  How does Russia feel about foreign Internet probes and what can they do about it?  And after a bit of miscellany, listener feedback, and a SpinRite update, we're going to take a deep dive into the back story and current capabilities of the Internet's premier scanning and indexing service, Censys.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here with a big, big, big show for you coming up in just a little bit.  Cryptomining, who pays for all that cryptomining?  Who doesn't pay for their Google Cloud services?  A Chinese certificate authority that you really shouldn't be using if you can avoid it.  And then Steve looks at Apple's security announcements from last week's WWDC.  That and a whole lot more, coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 927, recorded Tuesday, June 13th, 2023:  Scanning the Internet.



It's time for Security Now!, the hatless Steve Gibson.  But you don't need a hat because he's inside to do the show.  Steve Gibson's here, our master of ceremonies and all of that.  Hi there.



STEVE GIBSON:  Hello, Leo.



LEO:  Hey there.



STEVE:  And as we know, glass is a blocker of UVB radiation, which is why we should all be taking some Vitamin D because we're not getting it when we're inside.  And that's why when you hang your arm out the window, when the windows rolled down in the old days...



LEO:  Get the farmer's tan.



STEVE:  Yeah, that's right.  You've got the arm sunburned and nothing else.



LEO:  I know that, because I have those new transitions lenses on my eyeglasses, that UVB darkens them.  But you're sitting in the car, and they don't darken, which is kind of a problem.  Anyway, we're not here to discuss optics.  We're here to discuss security.  What's up?	



STEVE:  We are.  We're going to - I think this is going to be a fun one, not that that's any surprise, I hope.  We're going to examine what happens to monthly cloud services billing if you get infected by a cryptomining malware.  It's not good, but Google has an answer.  And speaking of cloud services, is Elon paying his bills?  And what's about to happen?  Just how fast are IoT-based DDoS attacks on the rise?  What was the strange tale of a wayward Chinese certificate authority, and why it's good what happened?  What useful new privacy and security features will Apple be adding to their services with their next OSes in the fall?



We've got an after-WWDC report from last week's event.  And why has France headed in another direction from Apple?  How does Russia feel about foreign Internet probes entering their IP space?  And what, if anything, can they do about it?  And after a bit of miscellany, some listener feedback - one of which drives me into a brief sci-fi reading retrospective for listeners, we haven't talked about that for a long time - and then a quick SpinRite update, we're going to take a deep dive into the back story and the current capabilities of the Internet's premier scanning and indexing service, which is known as Censys (C-E-N-S-Y-S).  Thus today's title is "Scanning the Internet." 



LEO:  Oh, Censys with a Y.



STEVE:  Censys with a Y.



LEO:  That's because there's no U in it.



STEVE:  That's right.  And as they say, there's no U in Internet.



LEO:  That's right.



STEVE:  It's not about you.  It's about the world.



LEO:  Me.  All right.



STEVE:  That's right.



LEO:  Time for the Picture of the Week, Steve.



STEVE:  So this is sort of astonishing.  And maybe it's obvious in retrospect.  But I have to salute the unnamed individual who thought, you know, clearly somebody who was very much into the generative AI image stuff, like could I use Stable Diffusion to morph a real-world photo, an image, into something that a QR code reader would perceive as a valid QR code?  And the answer was yes.  This thing tore through Reddit last week.  People were just blown away by it.



So our Picture of the Week shows an outdoor sort of a vegetable market with some stands and various shoppers and purveyors, some sort of underneath some sun protective awnings.  And it is also a valid QR code.  I mean, it looks maybe a little odd, but surprisingly not.  And but then you notice the three standard QR code targets, two on either upper corner and one in the lower left, where if you sort of get your eye to look at it right, you can see that.  And then like down on the sidewalk, down toward the bottom of the picture, it sort of looks like shadowing, but it's actually part of the QR code.  And the more you look around, the more you can sort of see how the Stable Diffusion engine fit the needs of the QR code into the image.  And these were so cool that I have a second page of them in the show notes, showing just two more examples, or actually four more examples, of images that were morphed using Stable Diffusion to create what's arguably beautiful QR codes.



Anyway, so in case anyone was interested, I've got a link in the show notes:  stable-diffusion-art.com/qr-code.  And at the top of the page it said:  "A recent Reddit post showcased a series of artistic QR codes created with Stable Diffusion.  Those QR codes were generated with a custom-trained ControlNet model.  Just like another day in the Stable Diffusion community, people have quickly figured out how to make QR codes with Stable Diffusion without a custom model."  And then this page goes on to show you how you can indeed create your own.  Give it an image you want and a QR code you have, and presto, here's like this amazing hybrid.  So anyway, just I thought it was very cool and wanted to share it with our listeners.



Okay.  So what happens, more often than not, when a cloud computing account is compromised?  Turns out the bad guys waste little time setting up and running a cryptocurrency mining operation.  The bad news for unwitting users is that, as we know, the reason this is being done is to mine on someone else's dime.  And the more computational resources that are available, the greater the rate of currency minting.  Consequently, minting on stolen accounts is typically not throttled, and it can consume massive amounts of compute time in a short bit of real time.



And that brings us to the question:  Who pays for that stolen compute resource usage?  Well, Google's June 8th announcement was titled "New Cryptomining Protection Program offers $1 million for costly cryptomining attacks."  It's like, yes, I mean, you could get hit with a bill that just astonishes you at the end of the month.  And it's funny because what I remember was like in the early days of the Internet, there was this notion, it was called 90 - well, actually it still exists when you're dealing with top-tier providers, 95-5 Internet bandwidth billing.



The idea was that the ISP wanted to make available higher bandwidth in bursts, but didn't want to get taken advantage of.  So over the course of a month all of the - essentially the amount of bandwidth that a customer was using would be sampled in small pieces, then all of those samples would be sorted from greatest to least amount of bandwidth used in that sample.  And then the 95th percentile was taken.  Maybe I've got it sorted backwards.  I think it was sort of the other direction.  Anyway, the idea was that if you had a - if you were using high periods of bandwidth, which wasn't like all month, it wasn't average bandwidth, it was weighted so that if you crossed more than 5% of the samples, the amount of bandwidth used at that 5%-95% percentile point, that was then taken to be your bandwidth usage for the entire month.



So the point is that, if you weren't careful, you could similarly get hit with an astonishing bandwidth charge from an ISP that was billing in this way.  So anyway, this is sort of the same.  It's like, okay, you're not paying attention.  A bot crawled onto, you know, got into your cloud computing account and has spent like all month grinding away, generating cryptocurrency, and then Google says "You owe us $5 million."  It's like, what?



Anyway, so their announcement reads more like a promotional advertisement, but it contains some useful information so I wanted to share it.  Here's what they said.  They said:  "Cryptomining is a pervasive and costly threat to cloud environments.  A single attack can result in unauthorized compute costs of hundreds of thousands of dollars in just days. Furthermore, the September 2022 Threat Horizons Report published by Google's Cybersecurity Action Team revealed that 65%" - okay, so just shy of two out of every three - "compromised cloud accounts experienced cryptocurrency mining."  So as I said, if they can get into your cloud account, that's what they do.



Google said:  "Stopping a cryptomining attack requires effective detection, which is why we've made it a focus of Security Command Center Premium, our built-in security and risk management solution for Google Cloud.  To strengthen our customers' confidence in their ability to quickly detect and stop cryptomining attacks" - basically they're going to give us indemnification.  They said:  "We are introducing a new Cryptomining Protection Program which offers financial protection up to $1 million to cover unauthorized Google Cloud compute expenses associated with undetected cryptomining attacks for Security Command Center Premium customers."  In other words, basically they're saying sign up for this, pay for this, and we'll indemnify you because we're sure we're going to be able to detect when this is happening on your account.



So they said:  "We're able to offer financial protection because Security Command Center Premium includes specialized detection capabilities that are engineered into the Google Cloud infrastructure.  To detect cryptomining attacks, Security Command Center scans virtual machine memory for malware.  It does this without agents, which can slow performance and increase an organization's attack surface.  Our approach enables us to detect attacks that could be missed by bolt-on security tools that rely on analysis of cloud logs and information gathered from APIs.



"Security Command Center can also detect compromised identities, which allow attackers to gain unauthorized access to cloud accounts and quickly deploy cryptomining malware.  This means Security Command Center can detect possible threats before an adversary can exploit compromised information to begin an attack.  This full set of advanced detection capabilities for cryptomining can only be delivered by a product built into the cloud infrastructure," blah blah blah.  So, yeah.  A commercial service, but just sort of brings to mind, you know, this is the kind of thing, unless you're really watching your cloud system closely, or really do have your own technology for detecting when suddenly your CPU usage gets pinned, and the meter starts running fast at Google, this would make sense, I think.



They quoted a guy named Philip Bues, who's IDC's research manager for cloud security.  Of course it's pro Google.  But he said:  "Cryptomining attacks" - this is IDC.  "Cryptomining attacks continue to be a serious security and financial issue for organizations who do not have the right preventive controls and threat detection capabilities in their cloud environment."  So just don't go setting it up and forgetting about it and assuming everything's going to be fine. 



He said:  "Google Cloud is taking an important step by providing built-in threat detection of unauthorized cryptomining, backed by real financial protection available to Security Command Center Premium customers.  If an attacker evades their detection defenses, they're there to back you up.  This shared fate approach to cloud security helps increase confidence among enterprise buyers when moving to the cloud."  And of course this is also, you know, this helps Google; right?  Because they've got, in their whole infrastructure, everybody is sharing this pooled compute resource.  So they don't want it to be drained off by a bunch of undetected malware which is reducing the total amount of compute that is available to everybody who's paying for the cloud services.



LEO:  They may not mind if you pay for it.  But still, it's probably a good thing.  You've got to wonder, I mean, this is kind of a shame that crypto exists at this point.  I feel like it's the negatives of crypto have really outweighed any benefits.



STEVE:  You know, I hadn't thought that, Leo, but you're absolutely right.  We talked about it, gosh, years ago.



LEO:  We thought it was interesting, you know, it was a really cool, you know, Satoshi's original paper and all of that.  But as it turns out it just really enabled ransomware and this kind of cryptomining hijacking.



STEVE:  And all the speculation.  People are losing their shirts.  In fact, it didn't make it into the show notes this week, but the FCC...



LEO:  Coinbase and Binance have both been sued by the FCC.



STEVE:  Yes, yes.



LEO:  For unregulated securities exchanges.



STEVE:  Yes.  But also the very first exchange that fell, Mt. Gox, it turns out that $1.7-some billion of current value in Bitcoin was slowly siphoned out of Mt. Gox over years before it finally went under.  So it's interesting.  I hadn't thought of that, Leo, but you're absolutely right.  I agree.



LEO:  Not to mention the environmental impact of these servers cranking up at high speed.



STEVE:  Yeah, Niagara Falls is much warmer now than it used to be.



LEO:  There's not going to be any ice this summer in the Arctic, and I think you can blame Bitcoin.  I plan to, anyway.  Geez.



STEVE:  Well, speaking of blaming people, while we're on the topic of Google Cloud Services billing, and even though this is a bit more gossipy than our usual fare, but since it is potentially an intriguing event in our industry, I decided to share the news that Elon Musk's Twitter has reportedly been refusing to pay both its Google Cloud and Amazon AWS bills, in an apparent strong-arm play to renegotiate its preexisting multiyear contracts which Twitter had signed with both service providers.  And this has been going on long enough now to lead both companies to independently begin threatening termination of services.  In the case of Google Cloud, this reportedly leaves Twitter's trust and safety systems, which maybe Elon doesn't care that much about, hanging in the balance as Twitter's contract with Google Cloud Services comes up for renewal this month.  And we're about mid-month at this point.



Although Twitter hosts some services on its own servers, the company has long contracted with both Google and Amazon to complement its infrastructure.  And prior to Musk's acquisition of Twitter last year, Twitter had signed an extensive multiyear contract with Google to host services related to, among other things, fighting spam, removing child sexual abuse material, and protecting its users' accounts.  So those facets of Twitter's services might be, well, you know, we'd like to have them; but, you know, Google wanted us to pay.  So, sorry.



Anyway, after acquiring Twitter, Musk reportedly issued a blanket mandate to his minions, requiring them to cut $1 billion from Twitter's infrastructure costs.  So it may be that he feels that playing hardball with Google and Amazon is the way to at least begin the process of renegotiating those agreements which predated his acquisition.  You know, we've previously heard stories about Twitter choosing to default on its existing physical office lease agreements.



LEO:  Rent, yeah, yeah.



STEVE:  And if Twitter manages to cut a billion dollars from its infrastructure costs, as I said, it may be eliminating the cost of various protective services it's able to provide to its user community.



LEO:  Yeah, well, who needs those?



STEVE:  Wow.  Yeah, you know.



LEO:  Needless expense.



STEVE:  Kiddie porn, well, what are you going to do?  You know, we can't look at everybody's feed.  Over on the Amazon side, since Twitter has also been delaying its payment for Amazon's Web Services, Amazon has reportedly been threatening now to withhold its advertising payments to Twitter, which would, of course, impact Twitter's revenue.  So maybe that'll get Elon's attention.  Anyway, it all seems like a big mess.  But, you know, messes seem to follow Elon around.  So we'll see what happens.  Anyway, I just thought - I ran across it, I thought, well, okay, we're on the topic of paying for cloud services.  So Elon says that we don't have to.



As our listeners know, the risks posed by the rapid uptake and proliferation of today's not-yet-really-secure IoT devices has been a constant source and topic of concern here.  Last Wednesday, June 7th, a report published by Nokia's Threat Intelligence team gave these concerns some numbers.  Here's what Nokia's report explained.



They said:  "The latest Nokia Threat Intelligence Report released today" - which was last Wednesday - "has found that IoT botnet DDoS traffic, originating from a large number of insecure IoT devices with the aim of disrupting network services for millions of users, increased fivefold over the past year, following Russia's invasion of Ukraine and stemming from the growing increase in profit-driven hacking collectives operated by cybercriminals."  And Leo, yes, paid for by cryptocurrency.



LEO:  Uh-huh.



STEVE:  In cryptocurrency.  "This sharp increase, also supplemented by the increased use of IoT devices by consumers around the world, was first noticed at the beginning of the Russia-Ukraine conflict, but has since spread to other parts of the world, with botnet-driven DDoS attacks being used to disrupt networks as well as other critical infrastructure and services.  The number of IoT devices (bots) engaged in botnet-driven DDoS attacks rose from around" - okay, so this is the number of IoT devices - "rose from around 200,000 a year ago to approximately one million devices, generating more than 40% of all DDoS traffic today."



So once it was that you were commandeering big iron servers where you could because they had big pipes connecting them to the Internet, and lots of network juice, and you could pump out a lot of packets.  Now we're distributing.  We've got a million IoT devices, each on residential Internet connectivity.  But, boy, you take the typical residential Internet and multiply that by a million, and, well, we've been talking about the size of recent DDoS attacks.  They're just astonishing in size.



So Nokia said:  "The most common malware was found to be a botnet malware that scans for other vulnerable devices, a tactic associated with a variety of IoT botnets.  There are billions of IoT devices worldwide, ranging from smart refrigerators, medical sensors, and smart watches, many of which have lax security protections.  The Threat Intelligence Report also found that the number of trojans targeting personal banking information in mobile devices has doubled to 9%, putting millions of users around the world at heightened risk of having their personal financial and credit card information stolen.



"The report, however, did find some encouraging news, showing that malware infections in home networks declined from a Covid-high of 3%" - but still, 3% of home networks infected.  It was cut in half to 1.5%.



LEO:  Wow.



STEVE:  And they said - still, yeah:  "Close to the pre-pandemic level of 1%."  So it hasn't even dropped back down.



LEO:  That's a good job done by Windows, I think, primarily, by Microsoft.



STEVE:  I would think that's probably the case, yes, yes.



LEO:  Yeah.



STEVE:  They said:  "As malware campaigns targeting the wave of at-home workers tapered off, and more people returned to office work environments.  These findings are based on data aggregated from monitoring network traffic on more than 200 million devices globally" - that's Nokia's view into the Internet - "where Nokia NetGuard Endpoint Security product is deployed."



Nokia's Senior Vice President for Business Applications said: "The key findings in this report underline both the scale and sophistication of cybercriminal activity today."  And of course that's something we've been noting here constantly.  This is, you know, the world has changed.  It was at the beginning of this podcast, it was oh, look, I wrote a worm; you know?  Now it's, okay, like there's now a darknet which is saying, okay, we've got 200,000 IoT bots in our net.  Pay us, and we'll beam there wherever you tell us to.  It's a completely different complexion today.



He says:  "A single botnet DDoS attack can involve hundreds of thousands of IoT devices representing a significant threat to networks globally.  To mitigate the risks, it is essential that service providers, vendors, and regulators work to develop more robust network security measures, including implementing threat detection and response, as well as robust security practices and awareness at all company levels."  And, you know, we've often observed, Leo, that a lot of this is spoofing IPs.  Unfortunately, when you've got a million individual IoT devices, they don't even need to spoof their IP.  No one cares if they get identified.  It's some light switch, or some plug.



LEO:  Right.



STEVE:  Just like there's too many of them to deal with.  You just have to do what Cloudflare does and bring up transient defenses in order to block connections at the perimeter and prevent them from getting through.



LEO:  The darker interpretation of this is not that our computers are more secure, but the malware creators are going to the softer targets in IoT.  And there's no softer target than IoT.



STEVE:  Right.  They don't care if it's a toothbrush that's generating IoT traffic or a mainframe.  To them all they are is packets, packet missiles that they're able to launch at a temporary enemy.



LEO:  I suspect that really that's the reason is just these targets are easier to compromise.



STEVE:  Well, and now, why is it happening?  They're getting paid.



LEO:  Oh, there's money in it.  Oh, yeah.



STEVE:  Yes.  It's now a business.  It's a business.  Remember in the beginning the original botnets were script kiddies who were blasting their competitors off IRC servers.



LEO:  Yeah.  Have you read the new, or are you going to read the new book about Fancy Bear, or it's called "Fancy Bear"?  It's really a good book ["Fancy Bear Goes Phishing"].  I recommend it.



STEVE:  Oh, okay.



LEO:  It starts with the Robert Morris worm.  But he takes five well-known malware attacks, Fancy Bear is one of them, and talks about it.  His final conclusion is interesting.  Because these are Turing machines, they cannot be secure.  It's just the nature of it.  They can do anything, including malware, and they always will be.  But I didn't, you know, pro or con on that, the stories themselves are great.  And Fancy Bear, to your point, which eventually became the Mirai Botnet and all of that, the kid who did Mirai - it wasn't Fancy Bear.  It was the kid.  Fancy Bear's the Russian group; right?



STEVE:  Right.



LEO:  The kid who did Mirai was a Rutgers students in his freshman year.  He was pissed off that Rutgers wouldn't allow underclassmen to take the computer science courses he wanted, so he wrote his first DDoS attack to take the Rutgers network down so nobody could register for any classes.  But to your point, and then later became more malicious, and he did Mirai, and eventually got caught by the FBI.  It's a really good story.  That's been excerpted.  You can read it around if you want.



STEVE:  Cool, sounds great.  Yeah, really.



LEO:  The book is called "Fancy Bear."  That's why I was thinking of Fancy Bear.



STEVE:  Right, right.  That's a good title for a book.



LEO:  Yes, it is.



STEVE:  It's really much more catchy than APT28.



LEO:  Yeah.  Yeah, I agree.



STEVE:  And that's not going to jump off the shelves.



LEO:  Or the GRU or whatever.  Yeah, I agree, 100%.



STEVE:  Okay.  So a guy named Matt Holt wrote a nice little web server in the Go language.  He calls it Caddy Server, you know, as in "Caddy Shack," Caddy Server, and describes it as an extensible, cross-platform, open-source web server written in Go.  The name "Caddy" refers to both a helper for tedious tasks, like someone carrying someone's golf clubs, and a way to organize multiple parts into a simplified system.



Okay.  So we've established that Matt knows his way around web server technology.  He was experimenting with ACME, which is, as we know, the protocol created by the EFF's Let's Encrypt project to automate the issuance of TLS certificates, the idea being that it allows the server to proactively ask a certificate authority to please refresh its cert because under the Let's Encrypt model, certificates only last 90 days.  So you want to automate that process so that you're not having to do it constantly.



Okay.  A low-budget Chinese certificate authority named HiCA  (H-i-C-A) only supported one particular ACME client.



LEO:  Oh, that's suspicious.



STEVE:  Uh-huh, for its customers' servers; right?  Because if it's an open protocol, and it is, it should not matter.  Matt found this odd, just as you did, Leo.  Now, this client, this one particular ACME client, is open source, and it's over on GitHub as ACME.sh.  So here's a bit of what Matt wrote.  He said:  "HiCA's documentation" - that is, the documentation with this certificate authority - "explains that it only supports acme.sh as a client.  This was curious to me, so I tried to learn why.  If it's using ACME" - and this is the thing that really got him, the ACME logo - "it should be basically compatible with the majority of ACME clients.  While observing a certificate using ACMEz, I discovered that the discovery was blocked" - and this is like details of the protocol we won't get into because you'll get the gist of this - "the discovery was blocked unless the User-Agent is set to a string that starts with Mozilla or acme.sh/2.8.2.



"Once I faked the User-Agent in my own client and got that working, certificate issuance still failed.  Curiously, the error message involved trying a URL of dot dot" - meaning, you know, backup in a hierarchy - "../pki-validation.  This doesn't make any sense to me, even though that kind of appears in their docs, because it's not standard ACME.  So I dug a little deeper to figure out what the Challenge object consisted of that would cause my client to be making a request to ../pki-validation.



"It turns out that the Challenge object looks unusual, and it became immediately obvious to me why HiCA only supports acme.sh.  They are not conforming to ACME at all."  And he says:  "(Bugs the heck out of me that they're using the official ACME logo on their site, even though they don't implement the ACME standard.)"  And he says:  "Instead, HiCA" - you sitting down, Leo?  "HiCA is stealthily crafting curl commands and piping the output to bash.  Acme.sh is being tricked into..."



LEO:  What could possibly go wrong?



STEVE:  Oh.  "Acme.sh is being tricked into running arbitrary code on the remote server."  Okay.  So let me make that a bit more clear and fill in some additional details.  A small Chinese certificate authority requires their clients, their users, their customers, to only run a specific acme.sh ACME client, specifically because this particular open source client has a bug which the CA has been exploiting to cause their clients' web servers, their customers' web servers, to remotely execute arbitrary code and commands on their own servers.



LEO:  Oh, my god.



STEVE:  Wow.  Now, obviously, no one should ever run code, meaning ACME clients, that they don't trust on their servers.  And if some certificate authority tells you that they support ACME, but only one specific ACME client, even if their certificates are free, run away as fast as you can.



LEO:  So this is probably the Chinese Communist Party, the Chinese government.



STEVE:  Oh, Leo, it's a plot.  They're trying to take over our children.



LEO:  No.  But they're hoping that some foolish industrial enterprise will use the server and so they can keep an eye out; right?  I mean, I don't think it's hackers.  It's probably the government.  But who knows; right?



STEVE:  No.  Well, based on the - I read the GitHub thread discussion.  The guy behind this HiCA got involved when this became a controversy.  And he appears to be benign and goodhearted.  And of course if it was actually evil Commies, then he would.  He explained that doing this allowed him to have more flexibility.



LEO:  Oh, yeah.



STEVE:  And if he's able to run whatever code that he wants on your server, that's flexible.



LEO:  Could be benign.  It could really be just some [crosstalk].



STEVE:  Yes, yes.  I think it was just he saw a bug, and he decided to use it.  The acme.sh maintainers immediately fixed the bug that this HiCA guy was exploiting for their service, and HiCA shut down and closed its doors.  It wasn't a big deal.  He was affiliated with some other CA.  It was not quite free, but it was like $3.12 or something.  So he wasn't making a lot of money.  And he said, okay, fine, I'll just stop this because this isn't fun anymore.



LEO:  Yeah.



STEVE:  But still, you know, everybody who was using his certs was having his code running on their computer.



LEO:  Jiminy Christmas.  So it wasn't the server, it was the certificate authority.  Interesting.



STEVE:  Yes.  The certificate authority has a...



LEO:  He found a bug.  I get it.  He found a bug in the server that he was able to exploit.  I get it now.



STEVE:  Yeah.  He found a bug in the acme.sh client.



LEO:  Right.



STEVE:  So his customers had to run, by his instruction, the acme.sh client in order to get the certs from him.  And the way they were getting them from him is he was saying, okay, you want a new update?  I need to run a little code in your server in order to make that happen.  What could possibly go wrong?



LEO:  And we never saw a malicious attack.



STEVE:  No, no.



LEO:  So maybe it was completely benign.



STEVE:  Right.  I think so.  But again, this is the way we keep these things from escalating.



LEO:  Yes, exactly.  And bravo to what's his name, Matt Holt, for finding this and writing it up.  And use Let's Encrypt, please.  Just use Let's Encrypt.



STEVE:  Exactly.  Why do anything else?  Use the one everybody uses, that's been vetted and had real security people writing it, maintaining it.  I don't get the need for additional ACME clients.  But again, everything's open, and it should be.  But it does not prevent bad ones from being written.  And, you know, I see some cars, Leo, that have been painted a color which should not have been allowed to leave, like, the garage.



LEO:  Cars, forget it.  We've got Victorian houses painted that way up here.  Fluorescent.



STEVE:  So, yeah.  If they build it, somebody will...



LEO:  Somebody will use it, yeah.  That's right.  So what a world.



STEVE:  Okay.  So Apple's 2023 World Wide Developer Conference is now behind us, and it was very interesting.  Apple did not disappoint with their continuing focus upon the privacy and security of their users.  It's very clear that they intend to offer both privacy and security as features of their products and technologies.  For example, during the presentation of their new mixed-reality vision goggle system, they made a point of noting that the system's quite powerful eye-tracking technology creates an inherent privacy risk, which at first you think, huh?  But social scientists have long understood, where a user's eyes look when confronted with an image reveals, with surprising fidelity, the innate emotional power of the content of various parts of an overall image.



Now, interpreting what that means exactly may be problematic, but it's still an unintended gateway into a user's mind.  So when we ask the question, do you want a web page you visit to know where you looked on that page, less privacy-centric developers might think that would be quite cool, and might sell it as a feature.



LEO:  Oh, yeah.  We do heat maps to see what people are looking at.  Yeah, it's a usage thing, to find out how they use the site, yeah.



STEVE:  Exactly, like having a virtual mouse pointer automatically jump to that location where then a page's JavaScript is able to obtain its coordinates and relay them back to the mothership.  But that's not the way Apple thinks.  Apple was quite clear in their presentation that where a user's eyes were looking was private information that would never leave that device.



LEO:  Good.  Good.



STEVE:  Yes.  And again, this is Apple on our side.  Only when they looked and clicked, which means did a little finger pinch thing, would the location of that click be returned, just like clicking the mouse pointer on something.  So it's this pervasive attitude across Apple that led me last week to opine that there's no way Apple has deliberately supplied anyone, including our own NSA, with a robust backdoor through iMessage to launch iDevice malware.  That just isn't something they're going to do.



Okay.  So back to last week's WWDC.  Although this year's advancements did not explicitly focus upon user security nearly to the degree that they did last year, that was a big focus of WWDC 2022, Apple still demonstrated that this continues to be a selling point for them, and they're going to sell it.  So for their Safari browser, Apple says that they've added additional tracking and fingerprinting protections which go even further to help prevent websites from using the latest techniques to track and identify a user's device.  This is the constant cat-and-mouse game, and they're just continuing to tighten it up.



Also, Safari's Private Browsing mode now locks when it's not in use to allow a user to leave private tabs open even when they've stepped away from the device.  Safari will now show a locked browsing window and request a Touch ID or password or Face ID in order to unlock and view those tabs.  And Safari's Private Browsing windows now automatically lock as a whole, if they've not been in use recently.  So nice moves there.



In Photos, a new embedded photos picker can help users share specific photos with apps while keeping the rest of their library private.  So they've made it more granular.  When apps ask to access the user's entire photo library, the user will be shown more information about what they'll be sharing, along with occasional reminders of their choice.  And I think that is so important.  This notion of occasional reminders that previous permissions remain in effect represents a significant advancement in our understanding of the human factors side of how to offer security and privacy.  It's so easy for us to grant a permission in the moment when we want to make something specific happen, but then to leave that permission enabled well after it's no longer appropriate.  So a gentle nudge to ask, "Huh, is this still want you want?"  That's just brilliant privacy-enhancing tactics.



So in the case of Photos, the Photos permission prompt now tells users how many photos and videos they would be giving access to, as well as providing a sample of those photos.  Apple is also moving to curtail the surreptitious link tracking which occurs in Messages, Mail, and Safari's Private Browsing.  It's becoming commonplace, more commonplace, for websites to append extra information onto to their URLs as a means of tracking users across sites.



We've talked about this years ago, how the "Referer" header informs advertisers of the URL of the page which is pulling the ad.  If this URL is needlessly embellished with enhanced tracking info, that information gets sent.  And there's been no way to automatically limit this.  Apple says that they're changing this, or will be this fall in iOS 17, by silently removing this unnecessary information from the links users share in Messages and Mail, and from the links in Safari's Private Browsing.



And what Apple calls Communications Safety is also being further advanced.  Communication Safety, which has been designed to warn children when receiving or sending photos in Messages that contain nudity, now also covers video content in addition to still images.  And a new API lets developers integrate this Communication Safety into their own apps.  So this would allow these warnings to be present in non-Apple apps, as well.



And this Communication Safety with now also help keep kids safe when they're sending and receiving an AirDrop, a FaceTime video message, and when using the Phone app to receive a Contact Poster and the Photos picker to choose content to send.  So basically they launched this under 16, verified that it's working, and now they're confident enough to extend this out into additional apps that could be problematic.



All image and video processing for Communication Safety occurs only on the device, so that neither Apple nor any third party gets access to the content.  And as we've talked about before, these warnings will be turned on for the child accounts in their Family Sharing plan, which can be disabled by the parent.  A Sensitive Content Warning is shown in Messages in the iPad Pro.



What Apple calls Communication Safety is what protects children receiving or attempting to send videos or photos, and the same protections are available for adult users in the form of a Sensitive Content Warning.  The feature is optional and can be turned on by the user in Privacy & Security settings.  And as with Communication Safety, all image and video processing for Sensitive Content Warnings for adults occurs on the device, meaning that nobody else gets to see it.



And there was some conversation of this, as well, I think I saw it on some of the podcasts, Leo, probably on MacBreak Weekly.  Apple has also added Passwords and Passkey sharing with the creation of sharing groups, which totally makes sense to do.  Users can now create a group to share a set of passwords, and everyone in the group can add and edit passwords to keep them up to date, as needed.  And in a slick new feature that I want to see in action, this will be really interesting to see.  Apple says that one-time verification codes received in Mail will now automatically autofill in Safari without the user leaving the browser.



Okay.  So okay.  It sounds as though you're on a web page that sends you a link, where the web page wants you to authenticate your email address by giving it a six-digit code or whatever.  So it says we sent you email, please populate this field with your code.  Apparently iOS will be observing that empty and waiting field and also notice that you've just received email in the background which contains a code.  So it will parse the email for the code and populate the one-time code field in the browser so that it just appears, and then you just click on, yeah, there's your code.  And it happens.  So wow, I can't wait to see that happen, you know, in front of me.  That'll be very cool.



We've talked about Apple's "Lockdown Mode" which significantly reduces the iPhone attack surface by dramatically restricting the content that the phone will accept and process.  Apple is pushing this technology now even further.  This was also the first time I've seen the term "mercenary spyware" anywhere.  I love the term.  I'm going to be using it.  These new lockdown protections encompass safer wireless connectivity defaults, media handling, media sharing defaults, sandboxing, and network security optimizations.



So now, when we get this in iOS 17, enabling Lockdown Mode will further harden device defenses and strictly limit functionality, all in the name of security.  And I think it makes a lot of sense.  Oh, and it's also coming to watchOS.  When you enable Lockdown Mode on your iPhone that is paired with your watch, the watch also gets locked down.  So, you know, it's not like they've left a way in by sneaking in through your watch.



Also, Apple has something that they're calling "Check In," which is an interesting new feature.  I'll just share how Apple described it.  They said:  "Check In makes it easy for users to let friends or family members know they've reached their destination safely.  Once turned on by the user, Check In automatically detects when the user has reached their intended destination, and will let selected contacts know via Messages.  In the case that something unexpected happens when the user is on their way, Check In will recognize that the user is not making progress toward their declared destination and check in with them.  If they don't respond, the feature will share useful information  like the user's precise location, battery level, cell service status, and the last active time that they used their phone with the contacts and the users selected.



"In addition to making it easier to get help if needed, Check In is designed around privacy and security" - of course, it's Apple - "keeping the user in control by letting them choose with whom to share their information, including the destination and time duration that they set.  Users can end the Check In session at any time," you know, canceling it.  "Information sent with Check In is end-to-end encrypted so only the user's family member or friends who have been authorized are able to read it, not Apple or anyone else."  So again, just kind of somebody at Apple saying, what new service, how else could we help people leverage the technology that they're now carrying in their pocket?  And so they created another one.



"NameDrop" is another new feature which allows for tightly controlled contact information sharing from one device to another, presumably enabled through NFC since the devices basically need to be in super close proximity, if not touching.



And, finally, a brilliant innovation which they call "Live Voicemail" allows the recipient of a phone call that they have chosen to let go to voicemail observe a real-time textual transcript of the voicemail as it's being recorded on their phone, and then change their mind on the fly to pick up the call.  It's the brilliant modern equivalent of how we used to use residential telephone answering machines to screen calls, and then we'd grab the phone receiver to pick it up, you know, claiming that we had just walked in the door while they were leaving the message and heard them leaving it, after we heard who it was or what the call was about.  So now we have the same thing on our smartphones.



All these goodies will be arriving later this year, presumably with iOS 17.  And, you know, big props to Apple for continuing to raise the bar on security and privacy and, I think, doing everything they can for their users, and clearly selling these features as part of their product offerings.



Now, not exactly following Apple's example, we have France, who last Wednesday evening, June 7th, the French Senate passed an amendment to its so-called "Keeper of the Seals" justice bill.  And I have no idea where that name came from.  The approved changes, which passed last Wednesday, now allow law enforcement agencies to secretly activate the cameras and microphones of remote devices, and specifically smartphones, without notifying the device's owner.



Officials say they plan to use this new provision to capture sound and images of suspects of certain types of crimes.  The measure would be reserved for cases of delinquency, organized crime, and terrorism.  Delinquency?  Okay.  The same update to the bill text would also allow law enforcement agencies easier access to geolocation data to track criminals suspected of committing offenses punishable by at least 10 years in prison.



What's not mentioned is exactly how they intend to make this actually happen in practice.  I had Google translate the French news web page, and they were saying that without this provision, investigators would need to plant physical bugs on the premises of their investigation targets.  So this was being sold as a safer means for their investigators to accomplish the same already legal surveillance, like if you get a warrant to plant a bug, by instead targeting their targets' phones, and turning the phones and other devices into surveillance equipment.



Now, we know that Apple's iPhones will actively resist any such abuse.  There's no way to ask an iPhone to do that.  But one wonders whether this might be paving a legal framework for the use of, to use Apple's new term, "mercenary spyware" such as Pegasus, which would subvert smartphone protections and would then, within the bounds of this legislation, no longer represent illegal spying which the country needs to deny and be ashamed of.  So France did this last Wednesday evening.



LEO:  Hmmm.



STEVE:  Yeah.  Okay.  So it should come as no surprise to anyone that Russia has decided to begin blocking foreign vulnerability scanning at the incoming border of RuNet.  Very much like their continuing use of Microsoft Windows, my reaction to that is, "You're only getting around to doing that now?  Really?"  So, you know, there are services like Shodan and the one we're going to talk about at the end of the show, Censys, typical services.



There's also other security companies and proprietary scanners which are more or less, we know this, constantly poking around the entire Internet to see what they can find.  When some security firm notes that, for example, some new vulnerability in a Cisco device affects more than 34,000 of them, well, that number comes as a result of scanning.  That's how we know there's 34,000 of them out there hanging out on the Internet just waiting to hopefully get updates. 



So it's entirely reasonable for an increasingly hostile foreign nation, like hostile to the rest of the world, not to want anyone poking around in their backyard.  And wouldn't you know it, the responsibility for limiting such scans falls to our favorite Russian Internet watchdog, Roskomnadzor.  In their announcement of this plan, they stated that more than 10 such services are constantly scanning inside their Russian RuNet for vulnerable systems that are then exploited in cyberattacks.  And that number of scanners, 10, okay, that sounds about right.



The trouble is, to at least some degree, the scanners you know about are not the scanners you need to worry about.  Shodan and Censys operate above board and scan from publicly known blocks of IP space.  So blocking them, if one chose to, would not be difficult.



But as anyone knows who's ever tried logging all of the individual IP packet traffic arriving at any arbitrary IPv4 address, today there's more or less a continual flux of incoming noise.  And as our long-term listeners know, long ago I coined the term "Internet Background Radiation" to remind us of exactly that.  My point is, all of the IP space of Russia's RuNet is also constantly receiving this random noise flux.  And it doesn't make sense to block it all, even if you could.



There's no way, for example, for any central authority to know which traffic to which port is part of the services legitimately being offered there and for whom those people want those incoming packets.  Look at what a mess, for example, some cable providers make when they decide to block some ports that they don't think their subscribers should be using.  It's not always good.  So my point is, if some of those random-seeming packets were actually carefully aimed NSA probes, Russia would never be the wiser.  The packets they do need to worry about would never be the ones belonging to the well-known public scanning services, which they've now said, okay, we don't want you anymore, and so they'll be blocked.



Okay.  We've got some, well, actually one piece of Miscellany and some Closing the Loops.  Tavis Ormandy, whom we've quoted through the years, he's @taviso, Tavis Ormandy at Google.  On Friday, June 9th, last Friday, he tweeted:  "Quick personal update.  It's nearly 10 years since @scarybeasts and I started Project Zero.  A lot has changed since then, and I've decided there are teams where I can have a bigger impact.  I'm still at Google, and still working on vulnerability research.  I'm going to work on CPU security with Google ISE."  That's the Independent Security Evaluators.  "We've already got" - and he's got like three flame emojis.  "We've already got zero-day reports on the way."



So we're going to be seeing some interesting, and especially interesting if they're CPU zero-day reports.  You know, that's different than Spectre and Meltdown, which are, you know, this might not be good someday.  These are, uh, we just hacked your CPU.  So stay tuned.  Tavis moved, but he's certainly not gone.



Dave Johnston sent:  "Hey, Steve.  Big fan of the show.  I heard your stat in Episode 926" - so that was last week - "about school districts lacking security staff.  Having worked in K-12 and community colleges, I have some background.  Many school districts are small, i.e., one or two schools.  They might have one or two technology staff running the whole show.  My last job was IT Director for a district with 7,000 students, 14 schools, which had two desktop techs, server admin, network admin, database admin, secretary, and a director."



He says:  "That's not an unusual load.  It's not that the districts don't care about cybersecurity.  They're having a tough time just keeping all the technology running on a daily basis."  And I can well imagine that.  So David, thank you for the viewpoint from the trenches, from the front lines.



Mrlinux11, he said:  "Getting error going to grc.sc/926."  And this was sort of interesting because it was in a tweet, and there was a link.  I clicked the link, and I got the error, too.  And I thought, uh-oh.  So I looked, and the browser had https://www.grc.sc/926.  And there's no www.  So that's an interesting sort of bad behavior on browsers because I never said www.  But we do know that, and this is one of the things we've talked about, some web browsers have taken it upon themselves to probe www dot, you know, like to add that to the URL.  Now, that won't resolve.  On GRC's DNS there's no resolution to that.  So you'd hope that the browser would back away.



Anyway, I'm not sure how the www dot got there.  But for any of our listeners who may have had trouble, that was the shortcut for the registry, the little tiny registry reg file which turns off Windows querying and running any startup code when your machine boots, which are being supplied by the motherboard.  So no www in front of the grc.sc/926.  So thanks to Mrlinux for bringing that to my attention.



Andrew Drapper sent:  "While the iPhone is quite locked down, the Mac less so.  iMessage accounts are synced.  Why can't these self-deleting messages be captured on a Mac?"  And I thought that was a great observation.  Now, the problem, of course, is that these attacks are highly targeted.  So it'd have to be the coincidence of a user who was targeted having a Mac.  And what we don't know is how long the actual attack took, that is, the iMessage arrives.  It has an attachment.  The attachment auto executes.  It causes some other stuff to be downloaded from the command-and-control server.  Those execute, go persistent, and then delete the attachment and the message.  So that could be a few seconds; right?  I mean, this is all blink blink blink blink, and now the malware is in place, and the message is gone.  So presumably iMessage deletion also propagates, and there may not be any chance to grab it.



But if it is the case that deletion doesn't propagate, and somebody had a Mac synchronized to their phone, logged into their account, and was a target, then yes, the planets could align, and maybe be possible to capture that.  One has to wonder, though, you know, Leo, you sort of mentioned this last week, and I think you had a really good point.  Apple might not add some quiet forensic stuff in order to like just capture stuff.  The problem is they're end-to-end encrypted; right?  They don't want to break their own users' privacy.



LEO:  You know, I have to remember this.  So this is the very important distinction.  Messages are end-to-end encrypted unless you use iCloud storage for them.  Then they are stored using a key that Apple has access to.



STEVE:  Right.



LEO:  So that's the difference.  When they say...



STEVE:  Unless we do that other thing that they just talked about; right?  Where they did remove that last key that they have from iCloud so that they no longer have it.  Historically they have.  But there was something that we talked about, like a month ago.  



LEO:  Yeah.  That was if you turned on Advanced Protection, though.



STEVE:  Right.



LEO:  So...



STEVE:  I think that takes it out of their hands.



LEO:  But it's not on by default.



STEVE:  Right.  That is true.  And it's not easy to turn it on.



LEO:  It's not easy.  And one of the reasons it's not is because you have to have everything up to date.  Which tells me that there's some sort of key sharing, as you point out, key sharing mechanism built into the latest operating system.



STEVE:  That the older devices don't have.



LEO:  That's right.



STEVE:  Yeah.



LEO:  This is unclear.  We talk about this sometimes on MacBreak Weekly because Apple's not fully forthcoming.  But we know for instance, I mean, it's true of Telegram, as well.  Once you back up Telegram to iCloud, it's unencrypted, or it's available.  We know because Apple provided Telegram messages, Paul Manafort's Telegram messages, to a subpoena to the law enforcement.



STEVE:  In response to a subpoena.



LEO:  In response to a subpoena.  So they have access to that.  I think unless you turn on Apple's whatever they call it, Advanced Protection...



STEVE:  Yeah, the Advanced Data Protection.



LEO:  Yeah.  I bet you that those iCloud messages are encrypted but not end to end.



STEVE:  And then you've got to wonder how deep is the delete?  Like when it deletes it, does the delete propagate?



LEO:  Well, I would think so because they don't want to store stuff unnecessarily anyway, really.



STEVE:  That's my thought, too.  That's my thought, too.



LEO:  Yeah.  But again, this is something - Apple kind of believes in security through obscurity.  And so they're very hesitant to really reveal - maybe there's a whitepaper somewhere, but I've never seen it - exactly what's going on; you know?  Yeah, it's an interesting question.  I don't know.  I'll ask around.  Rene Ritchie used to be our expert on that, but not anymore.



STEVE:  Right, right.  So, well, and they do maintain that beautiful security whitepaper.  We used to go over it in detail.



LEO:  Yeah.



STEVE:  But we've just been too busy lately.



LEO:  Yeah.  And that's a very good source.  But it's my understanding that it's only end-to-end encrypted if you turn on Advanced Data Protection.  I would guess, yes.



STEVE:  Yeah.  So Jared Neaves says:  "Hi, Steve.  Hope you're well.  I was wondering if you have any more book recommendations for us.  I've enjoyed all the ones I've heard so far from you, but the last one I think I heard you talk about on Security Now! was the Bobiverse series, which was a while ago.  P.S.:  Just re-read one of my all-time favorites, 'The Mote in God's Eye' and its sequel, 'The Gripping Hand.'  Amazing to think they conceptualized smartphones and AI home assistants back in the '70s."



LEO:  Jerry was amazing.  Yeah, he really knew his stuff.



STEVE:  Yup.  That was the team of Jerry Pournelle and Larry Niven.



LEO:  I often asked Jerry.  He says, "I conceptualize."  So the ideas come from Jerry, and then Larry would do most of the writing out, I think.



STEVE:  Yeah.  Some of my favorite books of all time came from those guys.



LEO:  So good, yes, so good, yeah.



STEVE:  So I'd already forgotten about the Bobiverse novels.  They were definitely fun, and I read them all.  We talked about them at the time.  But somehow they didn't stick with me as much as some of the others, both old and new, have.  So I'm just going to - I'm going to quickly bring people up to speed.  The series that I most recently read was Scott Jucha's, and that's spelled J-U-C-H-A, his "Silver Ships" series.  It was recommended to me by one of our listeners, and I am glad.  There are 24 books in that series, 20 in the main line and four that are an offshoot, which then merge with the timeline as that original series of 20 finish.  That whole series had some truly wonderful moments and many terrific new ideas.  So I'm glad that I read all 24 of those.



There were an additional six books in his so-called "Gate Ghosts" series that caused me to do something that I almost never do, which was to quit without finishing.  Now, this tendency of not quitting without finishing annoys my wife because we'll start in on some video streaming series which, after a few episodes, turns out not to be very good.  She'll want to abort, but my inclination is to see it through to the end.  I want to know.  I'm into the story.  I want to know what's going to happen.



LEO:  I'm an optimist.  I always think it's going to get better.  It rarely does.



STEVE:  No.  So it's very unusual for me to quit any sci-fi story in the middle, though I did quit the Gate Ghosts after I think two of its six books.  I also, for that matter, quit Apple's "Foundation" series.



LEO:  Yup, me, too.



STEVE:  To my wife's great relief.  She says, "Oh, thank god we don't have to keep watching this."



LEO:  It was pretty bad.  Season 2 is imminent, by the way.  You might want to finish it.



STEVE:  It was so disappointing.  But I can't wait for the second half of...



LEO:  Yes, finish Season 1 so that you can be disappointed by Season 2.



STEVE:  There we go, yes.  "Dune," I can't wait for the second half of "Dune."



LEO:  Oh, yeah.  Lisa's saying, "Let's watch 'Dune 1' again because 'Dune 2' is coming up." 



STEVE:  Yes, yes, yes, absolutely.



LEO:  What a great movie that was, yes.



STEVE:  Anyway, the reason I dropped out of Gate Ghosts was a clear lack of action with no sign of any impending action.  It turned into a mostly political narrative about the rights of sentient AIs and cloned humans as slaves.  You know, I'm going to get bored after a while unless something blows up from time to time.  Which leads me nicely back to my absolute favorite number one series in a long time, which is the Frontiers Saga by Ryk Brown, where Ryk is spelled R-Y-K.



Now, Ryk was a bit slowed down by a heart attack he suffered at 2:30 in the morning last December 4th.  Fortunately, thanks to very good EMS response, he suffered almost no lasting cardiac damage.  But the cause was severe blockages in three of his coronary arteries.  So he underwent triple-bypass surgery on January 16th.  Happily, his recovery was complete, and he picked up right where he left off with his ambitious plan for 75 full-length novels.  And maybe we're going to get them all.



So while reading the Silver Ships series, I had fallen four books behind when I switched over to, well, as I said, the Silver Ships.  Now I'm caught up.  And if I may have been just a bit unsure after starting into this third 15-book arc, that was quickly dispelled once we got to book three.  I would love to share a bit about what happens, but there are some fabulous surprises awaiting anyone who still has some catching up to do.  I'll just say that in Ryk's work there is no lack of action, and plenty of things are blowing up all the time.



LEO:  That's what I look for in a book.



STEVE:  Yeah.  Got to have some stuff blow up.  So those are the most recent two series.  Earlier in this podcast I talked about the Honor Harrington novels by David Weber.  There are 13 of those.  They are wonderful, and she's a great character.  There's also Jack Campbell's Lost Fleet series, which is also a ton of fun.



Nearly 10 years ago, in a PDF which is dated December 19th, 2013, so, yep, almost 10 years ago, I captured all of my favorite reading recommendations at the time, which we had discussed here on the podcast.  I captured it into a Sci-Fi Novels Guide which just now, yesterday as I was writing this, I reviewed.  It has the Honor Harrington series and the Lost Fleet series and all of our Peter F. Hamilton discoveries back then.  So I've given it a shortcut using GRC's shortcut service, and it is - no www - it is just https://grc.sc/ - oh, and it just occurs to me, I could have done the sc from the dot sc.  Anyway, it's .sc/scifi.



LEO:  It would have been very confusing if you...



STEVE:  That would have been, yes, too confusing, grc dot scifi.  So grc.sc/scifi, S-C-I-F-I.  That will give you a PDF which is not just a list of books.  It's also some commentary about how I felt about each book or series to sort of give you a little bit of guidance.  And I'm going to eventually, at some point, need to update that because I would definitely like to add certainly the Frontiers Saga, which is just so much fun, and Silver Ships - although, Leo, I know you were not a fan of Silver Ships.



LEO:  It was just a matter of style.  I'm not a fan of - yeah.



STEVE:  Okay.



LEO:  I don't have to have things blow up regularly in my books.



STEVE:  Oh, well, okay.  As long as you don't mind when they do.



LEO:  I don't mind when they do.  Not too much, though.  Just a little bit of blowing up.



STEVE:  You know, and all of those early Daniel Suarez novels, those were quite...



LEO:  Whoa.  "Daemon" had a lot of explosions.  That was an action-rich novel, yeah, I agree.  And I didn't mind that.  It had a pace.  It had a real pace to it, yeah.  I just, you know, the thing I don't like about Silver Ships is just the hero.  He's just too Captain America for me.  I just can't...



STEVE:  I get it, yup.  I do.



LEO:  Too perfect.  I like my heroes imperfect.



STEVE:  Well, speaking of which, I don't recall sharing any SpinRite testimonials since I began work on SpinRite 6.1.  But we received one yesterday which I thought was interesting.  Meir (M-E-I-R) in Montreal wrote, said:  "Hello, Steve & Team.  My son became a university lecturer and was very busy developing his course materials.  After many hours of work, he discovered that the server he was using had not synced the data reliably, so he pulled out his backup 1TB USB memory stick, only to discover that the computer could not even see it.  He asked me for help.  I pulled out my copy of SpinRite 6.0; and it, too, did not identify the device.



"I then remembered that a few years ago you read a testimonial from a user who cooled his hard disk in a freezer.  After three hours in the freezer, SpinRite had no problem seeing the USB device.  It told me it would take 25 hours at Level 2, so I left it for the night.  But in the morning, I saw this."  And he sent me a picture of, and it's in the show notes, "Division Overflow Error.  A critical error occurred at" - and I knew what it was going to say, B04E.



LEO:  Uh-oh.  How did you know that?  How did you know that?



STEVE:  So he said:  "Disappointed, I told myself maybe it recovered some of the data.  And it did.  All the 35 or so gigabytes of it.  Freezing a USB stick?  It worked.  Thank you, Steve.  Anxiously awaiting version 6.1.  Meir in Montreal."



Okay, so first of all, the error that Meir encountered was that infamous problem in 6.0 that we now have a fix for.  It's the result of SpinRite from 2004 encountering a drive 16 years later that it was never designed to handle.  Thanks to the reverse engineering work of Paul Farrer, we have a simple patch utility that can be run before SpinRite 6.0 which will prevent this from ever occurring.



Secondarily, it is because of recoveries like this one that I decided to continue moving SpinRite forward even past 6.1.  Although the days of spinning mass storage may be numbered, though that's been predicted for quite some time, and hard drives are still there and still make sense for many applications, I've previously shared many user stories of SpinRite recovering and repairing solid state drives.  I remember initially being surprised by those reports.  Now we just shrug and say, yeah, it works on those, too.



And what's even cooler is that, while not everyone always needs recovery, everyone could always use more speed.  Over the weekend, one of our 6.1 testers from Australia, a guy named Peter Hancock, ran SpinRite on one of his thumb drives.  I have a clip from the SpinRite log, this is SpinRite 6.1's log, which  he posted in our GitLab.  Among SpinRite 6.1's many new features is the ability to have it automatically run pre- and post-benchmarks on the drive.  Although that option is not enabled by default, I wanted to provide this option since it helps SpinRite users to recognize that SpinRite is really not only about data recovery.  This is clearly valuable maintenance for thumb drives.



So the numbers are in the show notes.  Look at the read transfer rate at the front of Peter's drive.  Before running SpinRite on it, it was reading at 1.077 megabytes per second, 1.077 megabytes per second.  After running SpinRite it jumped to 14.021 megabytes per second, 13 times faster.  And this is not some distant future SpinRite X.  This is the free upgrade that everyone's getting.  So dramatic improvement in the performance of solid state media.  And this is not an isolated incident.  Everybody who's running SpinRite on their SSDs is seeing this.  And as for putting a thumb drive in the freezer, well, that's interesting, too.  Meir in Montreal may have discovered something that will turn out to be useful for recovery.  We'll have to see if we're able to repeat that experience, and then we'll add it to our tips and tricks for SpinRite.



LEO:  Now we talk about scraping the Internet.  There was just, it's interesting, I don't know if it's related, but there was just a story, came out today in Wired, saying that the United States intelligence divisions are illegally buying data broker information to spy on the entire populace.



STEVE:  Yeah, huge amount of...



LEO:  Huge amount.



STEVE:  ...of proprietary independent data broker, yes, that was...



LEO:  And it's technically not legal for them to do.  The U.S. is openly stockpiling dirt on all citizens.  The article today in Wired by Dell Cameron.  But that's not exactly what you're talking about.  You're talking about a different kind of scanning.



STEVE:  Correct.  So with the news that Russia wants to block scanning of its internal networks  and again, who could blame them for that?  I thought it would be interesting to take a look at a modern state-of-the-art Internet scanning service, actually THE modern state-of-the-art Internet scanning service,  to see where that state of the art is today.  How quickly can the entire Internet be scanned?  What ports are checked?  What controls are available to be placed on such scanning?  Can Roskomnadzor ask them to please not scan Russia?



So the story of the Internet's current state-of-the-art scanning begins 10 years ago, back in 2013, when a Turkish-American Ph.D. student at the University of Michigan named Zakir Durumeric looks at the existing, and at the time quite famous, though also quite an old kludge, NMAP network scanner and thinks, quite correctly, this could be done much better.  Four years later, after being one of 14 students to receive a Google Ph.D. Fellowship in Security for the 2014-15 academic year, Dr. Durumeric finishes his Ph.D. thesis which is titled "Fast Internet-Wide Scanning:  A New Security Perspective."  The result of this work is a new scanner which Zakir names ZMap.



Okay, now, having created GRC's ShieldsUP! service back in 1999, which required the creation of an IP stack from scratch for that purpose, I know my way around packets, and I've seen a packet or two.  So I can attest that Zakir's work is beautiful.  His thesis demonstrates that he has an absolutely thorough grasp of  many various problems, asks all the right questions, performs the right experiments, and winds up developing extremely robust whole-Internet scanning and assessment technology.  It's really not rocket science, but no one had taken the time to sit down and really do it right until he did.  And he did.



Now, in fairness to NMAP, which is many people have a strong fondness for, the world had changed dramatically since NMAP was first conceived.  The biggest change was to the bandwidth available to such a scanner.  At the time that Zakir came along, gigabit Internet connections were common and affordable.  When I first created ShieldsUP!, my 1.54Mb T1 line was the envy of my friends.  Since no scanner wants to create its own bandwidth denial of service on itself, the available bandwidth dictates everything else about the system's architecture.  So Zakir was able to reconceptualize Internet-wide scanning at a time when doing so was feasible, like it really hadn't ever been before.



To gain an appreciation for the potential importance of the ability to have true near-real-time visibility into the Internet, I want to share the Introduction to Chapter 7 of Zakir's 216-page Ph.D. thesis.  He wrote a book.  It's a topic that all long-term Security Now! listeners will be able to relate to, since it happened on our watch.  Zakir's Chapter 7 is titled "Understanding Heartbleed's Impact."



He writes:  "In March 2014, researchers found a catastrophic vulnerability in OpenSSL, the cryptographic library used to secure connections in popular server products including Apache and Nginx.  While OpenSSL has had several notable security issues during its 16-year history, this flaw, the Heartbleed vulnerability, was one of the most impactful.  Heartbleed allows attackers to read sensitive memory from vulnerable servers, potentially including cryptographic keys, login credentials, and other private data.  Exacerbating its severity, the bug is simple to understand and exploit.



"In this work, we analyze the impact of the vulnerability and track the server operator community's responses.  Using extensive active scanning, we assess who was vulnerable, characterizing Heartbleed's scope across popular HTTPS websites and the full IPv4 address space.  We also survey the range of protocols and server products affected.  We estimate that 24 to 55% of HTTPS servers in the Alexa Top 1 Million were initially vulnerable, including 44 of the Alexa Top 100.



"Two days after disclosure, we observed that 11% of the HTTPS sites in the Alexa Top 1 Million remained vulnerable, as did 6% of all HTTPS servers in the public IPv4 address space.  We find that vulnerable hosts were not randomly distributed, with more than 50% located in only 10 ASes - those are top-level ISPs, right, AS is Autonomous System number - 50% located in only 10 ASes that do not reflect the ASes with the most HTTPS hosts.



"In our scans of the IPv4 address space, we identify over 70 models of vulnerable embedded devices and software packages.  We also observe that both SMTP plus TLS and Tor were heavily affected.  More than half of all Tor nodes were vulnerable in the days following disclosure.



"Our investigation of the operator community's response finds that, within the first 24 hours, all but five of the Top Alexa 100 sites were patched; and within 48 hours, all of the vulnerable hosts in the Top 500 were patched.  While popular sites responded quickly, we observed that patching plateaued after about two weeks, and 3% of HTTPS sites in the Alexa Top 1 Million remained vulnerable almost two months after disclosure."



Okay.  Now, think about how valuable it is to have this sort of information in the wake of a significant Internet-wide security event like Heartbleed.  When this happened in 2014, there was no other source of this information, other than Zakir's new Internet scanner, which he had written as a freshman at University of Michigan.  There's only one way to get this kind of information, which is to have the tools that are able to go out onto the Internet and look at it.  And they need to be fast.



Okay.  So Chapter 4 of Zakir's thesis, which I won't go into here, is titled "Detecting Widespread Weak Keys in Network Devices."  It's another example of how crucial having this sort of visibility into the Internet can be.  A new vulnerability and/or attack is discovered on some core aspect of our global Internet, and we need to be able to assess its impact and to begin to know how to remediate its effects.  Where to look.  Who to call.  What to do.



In an interview which Zakir gave to the Turkish American Scientists & Scholars Association, he was asked:  "Could you describe your innovation in layman's terms, and how it relates to everyday life?"  He replied:  "The cornerstone of this research is ZMap, a tool that I introduced in 2013 that enables researchers to rapidly measure how every device connected to the public Internet is configured.  ZMap reduces the time required to perform Internet-wide measurements from months to minutes, 10,000 times faster than previous techniques, and allows us to reason about the devices that make up the Internet for the first time.  Previously, many decisions were made anecdotally or through sampling.  Now we're able to perform comprehensive measurements, which has allowed us to uncover new types of bugs and understand some of the more complex interactions between devices at scale."



Okay, now, what happened to Zakir?  Today he's an Assistant Professor of Computer Science at Stanford University, and Chief Scientist of Censys, which is the inevitable commercial spin-off of his work.  But before we get to that, let's look at the non-commercial side, which is the ZMap Project.  It's located at zmap.io.  And no www in front of that, either.  Just zmap.io.



The ZMap project describes itself as "a collection of open source tools" - all of this is open source - "open source tools for performing large-scale studies of hosts and services on the Internet.  The project was started in 2013 with the release of ZMap, a fast single-packet scanner that enabled scanning the entire public IPv4 address space on a single port in under 45 minutes.  A year later, we released ZGrab, a Go application-layer scanner that works in tandem with ZMap.  Since then, the team has expanded, and we have built nearly a dozen open source tools and libraries for performing large-scale Internet measurements.  Continued development is supported by the National Science Foundation, the U.S. NSF."



So the Project has published a series of papers that describe how the suite of ZMap tools are designed.  There's ZMap:  Fast Internet-Wide Scanning and its Security Applications.  There's another paper, ZDNS:  A Fast DNS Toolkit for Internet Measurement.  There's ZLint:  Tracking Certificate Misissuance in the Wild.  And LZR:  Identifying Unexpected Internet Services.  And I found this one particularly interesting.  That last paper, the LZR, identifying unexpected Internet services, was delivered during the 2021 USENIX Security Symposium.  The synopsis of this "Identifying Unexpected Internet Services" paper has a couple of surprising findings.  



It says:  "Internet-wide scanning is a commonly used research technique that has helped uncover real-world attacks."  Well, it's common now.  It wasn't common in 2013.  "Helps uncover real-world attacks, find cryptographic weaknesses, and understand both operator and miscreant behavior.  Studies that employ scanning have largely assumed that services are hosted on their IANA-assigned ports."  Right?  Like HTTP, port 80; HTTPS, port 443; Telnet, port 23; and so on.  Those are the IANA-assigned ports.  The idea being that if you're given the IP of a service, you know what the service is, you know which port it's expected to be on at that IP address.



So they go on:  "Overlooking the study of services on unused ports.  In this work, we investigate where Internet services are deployed in practice and evaluate the security posture of services on unexpected ports.  We show protocol development is more diffuse than previously known, and that protocols run on many additional ports beyond their primary IANA assignment.  For example," they say - get this - "only 3% of HTTP and 6% of HTTPS services run on ports 80 and 443."  What?  3% of HTTP is on port 80, and only 6% of HTTPS is on 443.



LEO:  Well, where else is it?



STEVE:  Everywhere else.  Like on the other 65534 ports.



LEO:  But it starts on 80.



STEVE:  No.  Well, like we know, Leo, that to run a server on port 80 you have to have root.  Right?



LEO:  Right.



STEVE:  Because you can only run - you're only able to access ports below 1024, that is, 1 to 1023, if you have root privilege.  That's why userland servers often use port 8080.



LEO:  8080, right, right.



STEVE:  As their HTTP.  So the point is these services are scattered all over hell and gone.



LEO:  The public-facing servers have to use 80 and 443.  Otherwise you wouldn't get there.



STEVE:  Well...



LEO:  No?



STEVE:  Or you have to know to put a colon something after the URL.



LEO:  Right.



STEVE:  Which, you know, sometimes happens.



LEO:  Sometimes, sure.



STEVE:  You will sometimes see a URL that says, you know, :12345.  It's like, okay.  So they conclude:  "Services on non-standard ports are more likely to be insecure, which results in studies dramatically underestimating the security posture of Internet hosts.  Building on our observations, we introduced LZR ('Laser'), a system that identifies 99% of identifiable unexpected services using five handshakes and dramatically reduces the time needed to perform application-layer scans on ports with few responsive expected services."  And they give an example of having achieved a 5500% speedup identifying MongoDB on port 27017.



Okay.  So again, who would have imagined that only 3% of all of the actual HTTP service is on port 80?  The other 97% are on other ports.  And similarly...



LEO:  I think it's misleading.  All public HTTP servers are on 80, and HTTPS servers are on 443.  Right?



STEVE:  No.  This is a scan of the public Internet.  All of this is the public Internet.



LEO:  Yeah, but like when I set up a localhost thing, it's on a different port, you know, when I'm writing stuff for a web server or whatever.



STEVE:  Right.



LEO:  But my website has to be on port 80; doesn't it?



STEVE:  Yes.  But other people can have other things on other ports.



LEO:  But I'm not surprised they're insecure because those are often just like somebody messing around and stuff.



STEVE:  Or, you know, what's that horrible NAS that keeps getting hacked?



LEO:  QNAP?



STEVE:  Yeah, QNAP.  It's probably got a web server running on some port; right?  So there's a public service running on a non-80 port, for example.



LEO:  It shouldn't shock you, though, if they're running on non-canonical ports, that they might be insecure, too, because, like, why; right?



STEVE:  And the point is we never looked.  We never knew.



LEO:  Right.



STEVE:  So what happened was, when we looked, it was like, oh, my god, look at all this crap everywhere else that is, like, listening on these bizarre ports that no one ever thought to look at.



LEO:  Right.



STEVE:  So two papers later, the ZMap Project's list of published papers, we encounter Censys, C-E-N-S-Y-S.  "Censys:  A Search Engine Backed by Internet-Wide Scanning."  And the synopsis of this paper shows us how we move from Internet-wide scanner to an Internet-wide search engine.  And just so that I'm  sure I have everyone's attention, this is free.  So search.censys.io, you can put some search terms in and pull from what I'm about to be reading.  And you don't have to be a university researcher.  You just have to be researching for a good purpose, and they will give you credentials.  There is a commercial side where like all the big security companies, they pay to have API backend access to this database.  But people listening to this podcast could say, hey, I want to - I've got an idea.  I want to do some research, and you can get signed up.



So here's the deal.  They said:  "Fast Internet-wide scanning has opened new avenues for security research, ranging from uncovered widespread vulnerabilities in random number generators to tracking the evolving impact of Heartbleed.  However, this technique still requires significant effort.  Even simple questions, such as 'What models of embedded devices prefer CBC ciphers?' require developing an application scanner, manually identifying and tagging devices, negotiating with network administrators, and responding to abuse complaints.



"In this paper, we introduce Censys, a public search engine and data processing facility backed by data collected from ongoing Internet-wide scans.  Designed to help researchers answer security-related questions, Censys supports full-text searches on protocol banners and querying a wide range of derived fields.  It can identify specific vulnerable devices and networks and generate statistical reports on broad usage patterns and trends.  Censys returns these results in sub-second time, dramatically reducing the effort of understanding the hosts that comprise the Internet.  We present the search engine architecture and experimentally evaluate its performance in this paper.  We also explore Censys's applications and show how recent questions become simple to answer."



And this brings us to the second part of this, which is censys.io.  The Censys mission statement reads:  "At Censys we believe that cybersecurity is critical to the future of our global economy.  And in order to evolve cybersecurity defenses, both the public and private sector need access to best-in-class intelligence data.  By arming our customers with the visibility and insights that they need to protect against critical threats, Censys provides the intelligence needed to bolster cybersecurity capabilities worldwide."  So basically we now with Censys have access to near-real-time, deep intelligence, Internet-wide, port-wide scanning of everything that is public.



What does Censys tell us about their Internet scanning?  Their timeline notes that ZMap was invented in 2013, which we know.  Zakir did that.  Thus Z of ZMap.  That Censys was founded four years later in Ann Arbor, Michigan, probably when he got his bachelor's, which is where Zakir had gone to University.  And it also shows that two years later, in 2019, the original ZMap scanner was replaced by their proprietary scanning technology.  So there are two projects.  There's the ZMap project which has continued moving forward, advancing ZMap scanner-based applications, which is still open source and available.  And then there's Censys, which has moved to a far more sophisticated scanning technology.  We don't need that because we have access to its results.  And of course it makes sense that there would be a change over six years.  As I said, the world is constantly changing.  And six years since Zakir first created ZMap, you can now do a much better job.  And so that's what they have.



Under the topic of "Host Scanning Introduction" they explain:  "Censys continually scans the entire public IPv4 address space on 3,592-plus ports using automatic protocol detection to present the most accurate representation of the Internet's current state.  Censys also leverages redirects and the Domain Name System to discover and scan around 79 million in-use IPv6 addresses."  Now, that's interesting, since it's entirely possible to scan all IPv4 addresses which occupy a 32-bit address space.  However, there's no possibility of scanning the entire IPv6 address space, which is 128 bits.  So it's necessary instead to discover, hold, and build up a sparsely-populated map over time of active IPv6 addresses, which these guys have.  And it's also interesting that this number is still apparently as few as around 79 million.  Okay.  79 million is a lot, but it's not close to IPv4's essentially fully occupied 4.3 billion IPs.



They continue to explain:  "Censys scans only obtain information.  Censys never attempts to log into any service, read any database, or otherwise gain authenticated access to any system."  They ask themselves the question:  "How often does Censys scan for new services?  Discovery means finding a service on an IP and port that was not there last time we looked.  Censys has several schedules for discovery based on our experience scanning the Internet.  First, Global Scan of Popular Ports:  We scan the whole IPv4 space on 137 ports with IANA-assigned services every day."  Okay?  So the entire IPv4 space on all 137 IANA-assigned ports every day.



"Cloud Provider Scans:  Since many cloud hosts are ephemeral, we scan the 1,440 most popular ports on Amazon, Google, and Azure hosts every day.  Global Scan of Less Popular Ports:  We scan the whole IPv4 space on 3,455 additional ports on a regular basis, completing a walk every 10 days.  And finally, a Global Scan on Every Other Port Number.  We scan the entire IPv4 address space across ALL ports (65535) at a low background rate."



Okay.  So stop for a second.  Internet background radiation?  Yeah.  That means any IP address, any IPv4 address.  You put a port monitor on it, and these guys are going to wind up sending a packet to you to every single one of your IP, your one IPs, 65535 ports, over some period of time.  All 137 of the IANA-assigned ports every day, on every single IP out there.  So this is some radiation.



"How Often Does Censys Refresh Data for Known Services?  Once a service has been discovered, Censys prioritizes refreshing the information about that service to ensure it is accurate and up to date.  Once a day, the age of each of the approximately 2.1 billion services in our data set is checked.  Any unnamed service with an observation timestamp older than 24 hours is rescanned.  With this process, the average age of high-value service data is about 16 hours."  Okay.  So that means they've got 2.1 billion specific services, meaning a port, an IP, and they know what's there, 2.1 billion of them, and they revisit it and reverify it about every 16 hours.  None of the data in that data set is older than 16 hours.  And it's available to be searched.



"How Does Censys Scan?"  Okay, now, they don't really answer their own question, which I thought was interesting, even though they asked it of themselves.  They say:  "Censys has invested time and technology into setting up multiple global perspectives and developing sophisticated scanning techniques to produce the richest, most useful data set for the security community."  Okay, well, that didn't really answer the question.  They could have just said "We scan good."



Anyway, they said:  "Censys peers with and scans from five Tier-1 ISPs (NTT, Tata, Hurricane Electric, Telia, and Orange) to produce nearly 99% coverage of listening hosts across the globe with enhanced protection against packet drop.  The ISP that Censys scanned any given service from is recorded in the services.perspective field.



"Deep Protocol Scans:  On ports with IANA-assigned protocols, Censys attempts to complete a handshake with the assigned protocol," you know, expecting Telnet to be on port 23, for example.  They said:  "If that fails, we try additional handshakes according to our experience with protocol and port pairings."  Okay, so, for example, if a Telnet handshake were to fail on port 23, they might try an SSH handshake since its foreseeable that someone might have put SSH over on Telnet's port.  They said:  "On ports without an assigned service, we start by sending an HTTP request" - because, as we've seen, HTTP is kind of everywhere - "and attempt to automatically detect the protocol based on the response.



"Automatic Protocol Detection," they said.  "The Censys scanner analyzes every server response to identify its service, even if it's non-standard for the port, which allows us to uncover the vast majority of services in unexpected places.  For example, if an HTTP request results in an SSH banner, Censys will close the HTTP connection and reattempt an SSH handshake.  Censys can detect 25 protocols on any port.  Some protocols do not have a lot of data to parse and index."  Meaning, if an initial TCP connection handshake succeeds on some random unassigned port, now what?  So they said:  "Censys identifies 47 lightweight services and collects a banner.



"What Protocols and Services Does Censys Detect?  Censys can detect and complete scans for over 100 Layer 7" - meaning application layer - "protocols.  The default Layer 4 [IP layer] protocol used by our scanners is TCP, although some protocols, such as DNS, are scanned with UDP, and HTTP can be detected over QUIC.  Service names represent the most specific service information we have.  For example, a generic HTTP service has a service name of HTTP, while an HTTP service that's actually an Elasticsearch server has a service name of ELASTICSEARCH."  Meaning that they will get as specific as they can.



"There's also an UNKNOWN fallback, which means that Censys could not identify the protocol in use by an open service, either because the service is not adhering to a protocol" - and they say there are a lot of HTTP-like services out there - "or because Censys does not have a protocol-specific scanner written for that.  So once a day around 2.1 billion individual Internet services which they maintain in their data set is checked, and they maintain a searchable index of everything that they have found."



And what if someone like Russia doesn't want to be scanned and indexed?  About this Censys says:  "Censys strives to be a good citizen of the security industry.  We never attempt to log in to any service, never read any database, or otherwise gain authenticated access to any system.



"Can I opt out of Censys data collection?"  They said:  "Censys scans help" - like they don't want you to.  So they say:  "Censys scans help the scientific community accurately study the Internet.  The data Censys gathers is sometimes used to detect security problems and to inform operators of vulnerable systems so that they can be fixed.  If you opt out of data collection, you might not receive these important security notifications.  However, if you wish to opt out, you can configure your firewall to drop traffic from the subnets we use for scanning."



And here in the show notes and on their site they proudly display them.  There are five /24 networks, so that's 256 IPs, or 253 or 4.  So there are five of those networks.  And then two IPv6 networks which are /80s.  So, what, 48 bits on the machine name.  And so again, you're not going to scan all those.  But so they do show a total of seven networks which, if you didn't want them, if you didn't want your network to be in their database, in their data set, which could be searched for things, add those to your firewall.



Okay, then what happens?  They did say:  "Additionally our HTTP-based scans," because they do make HTTP queries to see what's there, like on port 80, "use a Censys-specific user agent, which can be used to filter requests from our scanners."  So again, if you arrange to have your server reply with, it's got CensysInspect/1.1 - I'm sorry.  If you see this coming as the user-agent into your server, you could have your server do something different, just drop the request, you know, just hang up.



They said:  "Configuring your services to drop connections from Censys's subnets will prevent our scanners from indexing your services.  Historical data is not removed from Censys data sets as part of this change.  Host services are typically pruned from Censys Search within 24-48 hours of their last observation timestamp."  So they're not just accruing everything forever.  If you go off the 'Net, after a couple days of you not being there, they will remove you because they want to be a current snapshot, not an archival snapshot.  They said:  "Host services are typically pruned from Censys Search within 24-48 hours of their last observation timestamp, while Virtual Host services can remain in the data set for up to 30 days."



And finally they explain:  "Censys started as a research project at the University of Michigan, and we continue to provide free Internet data to the research community.  We provide verified researchers" - and the verify, as I said, the verify threshold, the verify bar is very low.  Our listeners could qualify - "the same access to our data as our highest-tiered commercial customers."  I've got a link to the research access to Censys data, here on the last page of the show notes.



So as I said, while they do sell access to their databases and data sets to commercial entities, which they make available through an API, they also make this access available to pretty much anyone who has a justifiable use for such access.  And they say, you know, if your use is I want to make my company's hosts more secure, no.  But if you want to make the world a better place, welcome, and we'd be happy to have you.



So I've been noticing that we've been running across this group, the name Censys, more and more, with their name being cited by other security researchers who are clearly their commercial customers.  So I've been wanting to do a bit of a deep dive into who they are and where they came from.  Now we know.  So the next time I refer to them, it won't be, "Who?"  It'll be, "Oh yeah, those guys.  They're good."  I believe they are.



LEO:  This is fascinating.  And well done to Censys, I guess; yes?



STEVE:  Yeah.  I mean, to be giving the world a near, I mean, as real-time as it could be, maintained, searchable data set of what is the Internet.  I mean, that is.  That is the Internet.



LEO:  Could it be misused in the way that Shodan is kind of misused?



STEVE:  Absolutely.



LEO:  Okay.  Just checking.



STEVE:  Absolutely.



LEO:  It's like Shodan, basically.



STEVE:  Yeah.



LEO:  Yeah.  Okay.



STEVE:  Yeah.  I mean, you know, they used that access to find and enumerate the Heartbleed-vulnerable servers immediately.



LEO:  Right, right.  So there you go.



STEVE:  So, yeah.



LEO:  Wow, fascinating.  And another great show, jam-packed episode.  This is a two-hour, 21-minute extravaganza.  Well done, Mr. Gibson.  Steve's at GRC.com.  That's where SpinRite lives, the world's best mass storage maintenance and recovery utility.  6.0 is going to be 6.1 any day now.  Buy 6.0 today, you get to use it.  And as soon as 6.1 comes out, you'll get that for free, a free upgrade.  GRC.com.  While you're there, pick up SpinRite.



Copyright (c) 2023 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#928

DATE:		June 20, 2023

TITLE:		The Massive MOVEit Maelstrom

HOSTS:	Steve Gibson & Jason Howell

SOURCE:	https://media.grc.com/sn/sn-928.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week, two big stories dominate our podcast.  We start by taking a quick look back at last week's Microsoft Patch Tuesday.  Then we examine the latest surprising research to emerge from the Ben-Gurion University of the Negev.  What these guys have found this time is startling.  Then, after sharing some feedback from our listeners and a long-awaited big SpinRite milestone announcement, we're going to spend the rest of our available time examining the story behind this month's massive cyber-extortion attack which is making all of the recent headlines and causing our listeners to tweet:  "I'll bet I can guess what you're going to be talking about this week."  Yes, indeed.



SHOW TEASE:  Coming up next on Security Now!, Leo's off to Disneyland.  So I, Jason Howell, am filling in for Leo today, joining Steve Gibson who has all the words.  Actually, a few very key security stories.  One takes a look at what kind of security information can be gleaned from a tiny little LED power light.  You might think not a lot.  I'm here to tell you you're wrong.  So that's coming up.  Also this month's major cyber extortion attack, the MOVEit maelstrom.  Steve goes into some intricate detail on that.  Plus last week's Patch Tuesday.  And Steve has a major milestone to announce with SpinRite.  All that and more coming up next on Security Now!.



JASON HOWELL:  This is Security Now! with Steve Gibson, Episode 928, recorded Tuesday, June 20th, 2023:  The Massive MOVEit maelstrom.



It's time for Security Now!.  I'm Jason Howell, filling in for Leo Laporte.  I promise never to sing the theme song again.  I'm sorry I did that.  Right off the top I have regrets.  Here joining of course the man that you checked out this podcast for, Steve Gibson, to talk all about security today.  It's great to be here with you, Steve.



STEVE GIBSON:  Great to have you.  This was a surprise to me, but apparently not to anybody else, so...



JASON:  You know, Leo just sometimes on a spur of the moment decides, I'm going to skip town and go to Disneyland.  And that might sound like a joke, but that's exactly what's happening.



STEVE:  I don't think he's actually ever done that before.  No, I think this is a first in 18-plus years.  But, you know, what the heck?



JASON:  Hey, when you own a company for 18 years, you might as well have the ability to take off.



STEVE:  That's a good point.  I've done my own share of skipping out of my own company.  So this week, it was funny because you got the show notes, and you thought, uh, did I get incomplete show notes?  Is this, like, everything?  It's like, well, yes.  Two big stories dominate our podcast.  We start by taking a quick look back at just - this is not one of the big stories - at last week's - fortunately it's not one of the big stories - last week's Microsoft Patch Tuesday.  But then we're going to examine the latest surprising research to emerge from the Ben-Gurion University of the Negev.  What these guys have found this time is startling.  And I'll give a little more back story when we get to that.  But then, after sharing some feedback from our listeners and a long-awaited big SpinRite milestone announcement, we're going to spend the rest of our available time examining the story behind this month's massive cyber extortion attack, or I'm not sure if you'd call it an attack or attacks, plural, because it's so big that it's been making all of the recent headlines, and it's been causing our listeners to tweet to me things like "I'll bet I can guess what you're going to be talking about this week."



JASON:  You probably get that all the time.



STEVE:  Yes.  It's often, pretty much, pretty clear.



JASON:  Yeah.



STEVE:  So anyway, I titled Episode 928 for this June 20th, titled it "The Massive MOVEit Maelstrom" because this is really bad.  And so we'll spend some time looking at what happened, why it happened, how it happened, and what does it mean about the future.



JASON:  All right.  Well, we've got a lot to look forward to.  I'm super curious about that.  The two big stories... 



STEVE:  Oh, and of course we have a Picture of the Week.



JASON:  Which if I sounded distracted is because I was looking at it.  I was trying not to laugh in advance because it's pretty awesome.



STEVE:  Isn't it wonderful?  Oh.



JASON:  It's pretty awesome.  And we can finally talk about the Picture of the Week, which had me kind of stop in my tracks a few minutes ago, trying to figure this out.  But tell the audience about it first.



STEVE:  Okay.  So there's a little bit of back story since you're not always the host of the podcast.



JASON:  Sure.



STEVE:  There was an earlier famous bicycle lockup Picture of the Week which really made you wonder because they had locked their bicycle with a big, heavy security band to a post that had nothing on the top of it.  I mean, it was just a post.



JASON:  I love that.



STEVE:  So if anyone had wanted the bicycle, they could have just lifted it off of the post, you know, I mean, lifted the lock off.  Anyway, so this one is great, and I'm sure one of our listeners did send it to me knowing that we have an affection for wacky pictures.  I gave this the caption, "If it's not tied down..." because, if we believe this, I mean, this could obviously be set up, but still it's great.  Somebody really, really, really wanted to lock up their bicycle.



So they had this seriously strong metal steel gate that has a bar running along the bottom, and one running along the top.  And so they used those U-shaped strong bicycle locks through both the front tire and the back tire, one each, looped around the steel rail that runs along the bottom.  And just, I mean, you would imagine that would be enough.  But they figured, oh, no, I want to also lock something else.  And since there's a bar running across the top, they thought, well, I'll run another security cable underneath the seat through the metal mounting of the seat to further lock down the top of the bike.



What we see in this picture is exactly those three things, and only those three things.  Someone stole the bike out from, like the chassis, out from the three things that were locked down.



JASON:  The frame's gone.  No more frame.



STEVE:  The frame is completely, that's true, frame is gone.  There's just a front tire and a back tire, right where they always were, locked rigidly to the lower rail, and a seat hanging from the upper security cable on the top rail, and nothing in between.  So those three pieces are disconnected from each other because, yes, the frame of the bicycle, apparently someone thought, okay, I'll show you, and it's gone.



JASON:  I guess things could have been worse.  You could have lost the whole bike.  Instead you just lost the frame.  I'm looking at it.  And so what confused me earlier was when I was looking at the seat, it kind of looked to me like the seat was just hanging on the loop.  But what I realized as I looked further, there is kind of like a metal thing underneath the seat that it's going through.  So that seat isn't going anywhere.



STEVE:  Correct.



JASON:  I'm curious to know, like, what's the most expensive - maybe I'm thinking too deeply into this.  What's the most expensive part of the bike?  Is it the frame?  Is it the tires and the seat?  Because if it's the frame, I mean, you know, whoever took that bike obviously knew what they were doing.  They're like, well, still, I mean, I realize they don't me to take this, but I can.  And that's still worth something.



STEVE:  Now, I can offer some guidance, Jason, about our Pictures of the Week.



JASON:  Yeah, don't think too deeply?



STEVE:  It is not possible to think too deeply about them.  Leo and I have sometimes spent a good half hour on why is there a gate in the middle of this field?



JASON:  Yeah, that's good.  So you don't have to worry about overthinking this or thinking, you know, like going in too deep.  No.  I don't think it's possible to go too deep into any of this podcast's Pictures of the Week.  Fortunately, it's not possible to go too deep into last week's Microsoft Patch Tuesday.  This being the third Tuesday of the month, we're able to look back on last week's Patch Tuesday.



It's still, to me, somewhat astonishing that just last week Microsoft patched 26 remote code execution vulnerabilities.  You know, when Leo proposed this podcast to me, I didn't have any idea that we'd still be going strong after 18-plus years.  There's no sign, similarly, that Microsoft is ever going to run out of serious vulnerabilities.  The only thing, the only way to explain this is that they're, you know, everything they do is creating new ones, and so they're fixing some of the old ones, but at the same time creating new ones to be found in the future.  So similarly, I don't think we're in any danger of Microsoft running out of serious vulnerabilities.



Four of these 26 RCEs, remote code execution problems, were critical, and three of those four were spotted and fixed in a single Windows component known as PGM.  That's the Pragmatic General Multicast queue.  Because, you know, the thing you really want from your general multicast queues is pragmatism.  What you don't want from them is remote code execution flaws, especially when they're bearing a CVSS score of 9.8.  So after applying last Tuesday's patches, for the time being there will be three fewer RCEs, which probably makes the decision to use the queuing API in the first place somewhat more pragmatic.  So that's good.



There were also 17 elevation-of-privilege problems fixed, only one of which made it to the critical list.  It had a CVSS score, however, of a whopping 9.8, which, you know, I think maybe we've seen a 10 once or something.  It's got to be, you know, the really bad ones are 9.8, and they save the 9.9s and the 10.0s for, like, the end of the world.  So we've only had the end of the world happen I think once before.  Anyway, if exploited, an attacker could gain administrative privileges as a consequence of one of these 17 elevation-of-privilege problems.



Microsoft wrote:  "An attacker who has gained access to spoofed JSON" - that's not you, Jason, that's JSON - "Web Token authentication tokens could use them to execute a network attack which bypasses authentication and allows them to gain access to the privileges of an authenticated user.  The attacker needs no privileges, nor does the user need to perform any action."  Now, since we're talking about, and I didn't mention this yet, the SharePoint server, which is the same sort of database sharing that has landed the MOVEit Transfer software which we'll be talking about in the second half of this podcast, into such hot water earlier this month, getting that one patched is something that should probably not be put off, especially given that it's a 9.8.  Microsoft is saying, yes, we're glad we fixed this, and we're going to try not to create any more.



Overall, of the 73 flaws fixed, six are rated critical, 63 are important, two are moderate severity, and one is low.  So included within that group are three issues which were fixed in the Edge browser.  It's worth noting that during the intervening month between last month's patches and this month's, Microsoft did also eliminate 26 other flaws in Edge, included among those a zero-day which Google last week disclosed which was being actively exploited in the wild.  So that's fixed.  But this month is the first time in several months when none of the known and patched problems were either publicly known or under attack at the time of their fixes.  So that's good.



And we mentioned both publicly known or under attack because Microsoft has their own definition of zero-day.  Everybody else considers a zero-day to be one where it's first learned about because it's spotted being used in an attack.  Microsoft broadens that to say, if they're informed of something they didn't already know about, then it's a zero-day, which, I don't know, seems overly pessimistic to me.  But anyway, we didn't have any.



The final note is that two of the remaining remote code execution vulnerabilities were found and fixed in Exchange Server.  Being that Exchange Server is all about exchanging content with the outside world, it's generally a significant portion of an enterprise's attack surface.  So absolutely want to get that fixed.



Okay.  So I got that little bit of housekeeping out of the way.  Next is one of two big pieces of amazing stuff.  I titled this "Does EVERYTHING [in all caps] leak?"  This next bit of jarring news leads me to pose that question; you know?  Does everything leak, as in leaking information?



A couple of days ago, I received a  Twitter DM from Ben Nassi.  Ben tweeted:  "Hi, Steve.  My name is Ben Nassi, a postdoctoral researcher at Cornell Tech and a long-time listener of Security Now!.  I just published a new research that I think you should see.  We recovered cryptographic keys from devices by obtaining video footage of their power LED.  The devices were not compromised," meaning ahead of time.  He says:  "The research will be presented at Black Hat and Def Con this year."  And then he sent me a link to their research PDF.



Okay.  So the summer is approaching, with Black Hat and Def Con being held back-to-back every summer, as we know, in Las Vegas, Nevada.  Ben also sent a link, as I said, to the research paper which is titled "Video-Based Cryptanalysis:  Extracting Cryptographic Keys from Video Footage of a Device's Power LED."



JASON:  That is crazy.



STEVE:  And I can still hardly believe this even as I'm reading it.



JASON:  I can't believe that.  That's pretty insane.  I'm super curious.



STEVE:  You're going to love the details.  So first of all, again, because you're not always the host of the podcast, my mentioning the Ben-Gurion University of the Negev may not, like, ring any bells for you.  It does for our listeners.  Ben is one of the prolific researchers there who have for years brought us an unending stream of often entertaining, but also often sobering, extremely clever examples of data exfiltration.  Our longtime listeners will recall, for example, their work with extracting audible conversations from a room at a distance by visually detecting the sympathetic vibrations induced in a birthday party balloon, the leaf of a plant, or a light bulb, all of which allowed them to recover the audio filling the rooms occupied by those objects.  I mean, and they've done all kinds of things.



JASON:  Wow.



STEVE:  I think, yes, it's just incredible.  We could safely label these guys the masters of the detection and recovery of side-channel information leakage.  And so now we have another.  And whereas some of their schemes have required malware to first be installed in the victim device, which is why he was careful in his tweet to me to point out not this time, you know, like for example they've deliberately changed the sound being emitted by a machine's power supply and used that for remote signaling.  Or changing the system's fan speed in order to signal an air-gapped microphone.



You know, and for example, when they do that, it's easy to say, oh, change the system's fan speed in order to signal an air-gapped microphone.  When they do this, though, they look at how long it takes for the speed to change when they change it, and how many discrete levels of speed they can make distinguishable by a microphone at what distance, because the number of levels and the speed it takes to get to among each level determines the bit rate at which they're able to send, I mean, these guys wrestle this stuff to the ground all the way, every time.



So, okay.  In order to put myself in the same place as our listeners as I share this, I have not yet even glanced at Ben's paper.  I have now because I wrote the show notes.  But as I was writing the show notes, as I was first putting this down, I had not opened the paper.  So we're going to do that together in a moment when I share the paper's abstract, which as I said, I have not yet seen as I'm writing this.  But what we immediately and intriguingly ascertain from Ben's tweet is that, astonishingly, variations - and again, we're deriving this from everything we've learned over 18 years of the podcast; right?  Variations in the work being done inside devices, where that work is dependent upon cryptographic secrets, must be sufficient to produce tiny variations in the power being supplied to such a device's LED power indicator.



Now, that's surprising enough, that any such fluctuations would be so tiny as to be detectable.  And that they are theoretically detectable at a distance by a standard video system or, you know, despite the depth of digitizing resolution, the effects of video compression, or the frame rate of the video, right, which is going to be limited.  But to do what they've done, Ben's group apparently overcame all of these practical barriers, and they've done this before.  And they're presenting this at Black Hat and Def Con.  So, yeah.



And as I've observed in the past with their work, what really distinguishes their accomplishments is that they solve the whole problem.  They just, you know, the headline on the paper tells you where they're headed, but where they go is typically amazing.  Okay.  So now let's discover together what Ben's group explains at the start of their paper.



They write:  "In this paper, we present video-based cryptanalysis, a new method used to recover secret keys from a device by analyzing video footage of a device's power LED.  We show that cryptographic computations performed by the CPU change the power consumption of the device, which affects the brightness of the device's power LED.  Based on this observation, we show how attackers can exploit commercial video cameras."  And they said:  "For example, an iPhone 13's camera or Internet-connected security camera, to recover secret keys from devices.  This is done by obtaining video footage of a device's power LED, in which the frame is filled with the power LED, and exploiting the video camera's rolling shutter to increase the sampling rate by three orders of magnitude from the frames-per-second rate of 60 measurements per second, to the rolling shutter speed of 60K measurements per second, in the iPhone 13 Pro Max."



Okay.  So I'm going to pause here for a minute and say that's brilliant.  With an LED, even if the illumination across the surface of the LED is not perfectly uniform, as in fact they aren't, being a solid-state illuminator, any change in illumination will be uniform and effectively instantaneous.  So this is the key that allows them to obtain a sufficiently high effective sampling rate from an otherwise grossly insufficient 60 frames per second video recording.



So continuing, they said:  "The frames of the video footage of the device's power LED are analyzed in RGB space, and the associated RGB values are used to recover the secret key by inducing" - and I'm sure that's a typo, they meant deducing - "the power consumption of the device from the RGB values."



Okay, now, pausing again for a moment, we long ago talked about variations in power consumption during cryptographic operations being a well-understood side-channel that could theoretically be used to reverse engineer the work being done by a device when that work is a function of secret data.  But the presumption has been that that theory runs smack up against reality when there's no practical way to obtain instantaneous power consumption measurements without hooking deeply into a target device's electronics.  These guys have quite cleverly solved the problem of doing that.  They realized that minute variations in the device's power draw would induce tiny changes in the system's supply voltage.



The instantaneous brightness of an LED is determined by the instantaneous current flowing through it.  LEDs will invariably have a resistor in series with them to set their operating current at the given supply voltage.  But that current is not otherwise regulated.  And that means that any variation in the system's total supply voltage will create a variation in the LED's current and therefore in its illumination.  I'm still surprised this works, but apparently it does.  And you have to imagine that the world's intelligence services have just perked up in response to this news.



So they finish their abstract, saying:  "We demonstrate the application of video-based cryptanalysis by performing two side-channel crypt-analytic timing attacks and recover, first, a 256-bit elliptic curve DSA key from a smart card by analyzing video footage of the power LED of a smart card reader via a hijacked Internet-connected security camera located 16 meters away from the smart card reader; and, two, a 378-bit SIKE key from a Samsung Galaxy S8 by analyzing" - get this - "video footage of the power LED of Logitech Z120 USB speakers that were connected to the same USB hub that was used to charge the Galaxy S8 via an iPhone 13 Pro Max.  Finally, we discuss countermeasures, limitations, and the future of video-based cryptanalysis in light of the expected improvements in video camera specifications."  Oh, my lord.



Okay.  So before I go any further, I'll just mention, like in terms of extending this, now that we know that lights are flickering or just barely changing their illumination, the Light Pen which I developed back in 1983 for the Apple II had a response time of 140 nanoseconds because that was the pixel clock rate of the Apple II's video.  140 nanoseconds is a rate of 7.14 MHz.  Now, that happens to be twice the NTSC color burst frequency of good old NTSC video back then, and that was part of Woz's design brilliance for the Apple II.



Okay.  But my point is, since monitoring the power LED of devices that are performing secret computations has now been proven by these guys to work, it would be trivial to take the technology of a high performance light pen, place its photodiode at the user end of a telescope, and aim that scope at any power LED of any device containing secrets to begin collecting data.  The data gathering and secret gathering power of such a system with 7 MHz bandwidth is somewhat terrifying.



Okay.  So while it would be possible to create a highly sophisticated spying scope, the brilliance of what these guys have discovered and invented is the ability to use existing camera technologies, off-the-shelf tech, thanks to their observation that the cameras in our devices do not actually snap an entire scene at once.  Instead, they actually scan the image from top to bottom or left to right in much the way that the images of our original cathode ray tubes did.  This brilliantly allows them to sample the illumination of a device's LED with far greater temporal resolution than the camera's overall frame rate.  Here's how Ben's paper describes it under the title "Increasing a Video Camera's Sampling Rate Using a Rolling Shutter."



They wrote:  "We note that the FPS [frames per second] rate supported by the vast majority of commercial smartphones and security/IP video cameras is limited to 60-120 frames per second, which is sufficient for performing cryptanalysis.  In order to increase the number of measurements per second sampling rate to a level sufficient" - oh, I'm sorry.  I hope I said 60-120 frames per second which is insufficient for performing cryptanalysis.  Of course.  That's just not enough, I mean, the computations happening inside are way faster than that.  So that's not going to cut it.



So they said:  "In order to increase the number of measurements per second, the sampling rate, to a level sufficient for cryptanalysis, the attacker can exploit the video camera's rolling shutter.  The rolling shutter is an image-capturing method in which a frame of a video in video footage is captured by scanning the scene vertically or horizontally.  When this method is used, a frame per picture is not actually composed of a single snapshot of a scene taken at a specific point in time, but rather is composed of multiple snapshots taken of vertical or horizontal slices of the scene at different times.



"With a vertical rolling shutter, a sensor's pixels are exposed and read out row-by-row sequentially at different times from top to bottom, or left to right, according to a configurable shutter speed which determines the amount of time that the sensor is exposed to light.  Because each row, or a group of adjacent rows, in a sensor with a rolling shutter is captured at a different time, attackers can increase the effective sampling rate from the camera's frame-per-second rate of 60 or 120 frames per second to the rate at which rows are recorded, a rate which is based on the shutter speed."



Okay.  So, wow.  The biggest limitation is that for this clever rolling shutter rate up-sampling to work, the LED's image, as I briefly touched on at the start, the LED's image must fill the entire camera frame, right, because you want all of the individual row or line samples to be of the LED.  So it can't be just a tiny spot in the middle of the camera.  You've got to zoom the camera in so that the LED's light fills its camera sensor.  But there are tiny external lenses that can be added to a smartphone to make that easier, and there are doubtless many applications where the installation of a device, such as a smart card reader, is made with the inherent assumption that its internal secret cryptographic computations are not being broadcast outside of the device.  Ben's latest work has, amazingly enough, shown the world that this longstanding assumption has always been wrong.



You know, we've often commented about the fact that the Internet "activity LEDs" which are ubiquitous on all networking equipment, routers and switches and everything, do not reveal anything about the data that's passing through their interfaces.  Well, it turns out we've been looking at the wrong LED.



JASON:  I'm thinking of the phones that I've had, and of which I've had a lot over the years, and how many of them have some sort of an LED light.  Like that used to be a really common thing.  It used to be used for like notifications and whatever.  And if you plugged it in to charge, that little LED would light up.  But I don't feel like I see that as much anymore.  So at least there's that obscurity element on phones, let alone other devices.



STEVE:  Correct.  Correct.  I mean, yeah, on phones.  And in fact, as I mentioned, in one of their documented attacks, they put a phone virtually on the LED of a smart card reader while it is scanning the smart card.  And they don't have to process it at the time; right?  So all they have to do is they just turn the recorder on, and they're just recording the video while the smart card gets scanned, and they're able to get the private key of that smart card.



JASON:  Yeah.  Wow.



STEVE:  Oh, goodness.



JASON:  How smart are people that they can be like, well, I wonder, you know, I have a hunch, and they look into it, and they're able to figure this out.  I'm in awe of the ability to do that.



STEVE:  Yeah.  This was a beautiful piece of work.  So Ben, I know you're listening.  Congratulations.



JASON:  Wow.



STEVE:  Wow.  Just a fabulous piece of work.



JASON:  Back to you, Steve, with Closing the Loop.  Lot of feedback today.



STEVE:  Yeah.  So Kevin, tweeting from @sharpestmarble, he said:  "Listening to Security Now! 927, and you're praising Apple for Live Voicemail, which they're going to be developing.  But this is something Google phones have had for over a year now."  And Jason, as I said before we began, I was glad to have you here since you know Android, and I don't.  So is it all Android phones, or Google Pixel devices?



JASON:  Well, I mean...



STEVE:  Like Samsung?



JASON:  Yeah, yeah, definitely Samsung has the ability to do it, too.  I'm not sure if they're tapping into the same thing that Google can do.  But the visual voicemail, transcribing voicemail that comes in, definitely Google, I mean, Google's had Google Voice technology for more than a decade.  I don't even know when Google Voice was bought by Google.  It was Grand Central prior to that.  And somewhere along the line they started to automatically transcribe the voicemails.  I think that was kind of the beginning, if I'm not mistaken, the beginning of Google's kind of experience in doing this live transcription of these voicemails, and they definitely got better with it over time.



And then that feature has expanded outside of just voicemails into, you know, there's a feature that I was telling you about prior to the show that's embedded into the volume rocker on Pixel devices and some other Android phones.  And it's basically a live transcribe of any audio that's passing through the phone.  So that could be a video you're watching.  That could be the music, you know, if there's any speaking in music, it would transcribe that.  The phone call, all of this stuff is transcribed in a little closed caption box that you can drag anywhere on the screen.  Google's gotten really, really good at it, and it's one of my favorite kind of innovative features that Google's integrated in recent years.  But, yeah, Google's solid on this stuff.



STEVE:  Thank you, Kevin, for letting me know so we could also give credit where it's due.  Not a great innovation, Apple, but you're catching up.



JASON:  They're all borrowing from each other.  You know, one side will get a great idea, and they'll implement it, and then suddenly that becomes table stakes, you know, when they all have...



STEVE:  That's right, wasn't Apple the first fingerprint reader?  I think Apple had the first fingerprint reader. 



JASON:  Could be.  I'll say off the top of my head I can't remember.  But, you know, they all borrow from each other.



STEVE:  Yeah.  So Vincent Stacey, he said:  "Think Apple could use the same on iPhone technology protecting children from unwanted images to catch and analyze the iMessage vulnerability?"



So a couple weeks ago we talked about an iMessage vulnerability which was probably impossible to catch because the end-to-end encryption protected the contents of the message, which, once it launched malware, deleted the attachment which iMessage brought along with it, thus removing all of its fingerprints of the attack.  And so I made the point back then, the observation that the same really strong encryption which is protecting the content of Apple's iOS and iMessage user messaging is also protecting the bad guys who have an exploit of something in the system.  If it deletes itself, how is anyone ever going to be able to analyze it cryptographically or forensically?



Anyway, in answer to Vincent's question, I suspect that the same technology won't be useful because one aspect, which is scanning for unwanted material, is within the bounds of normal operation; while the operation of this malware is explicitly out of bounds.  The trouble with catching it is that whatever it is that the iMessage vulnerability is doing, it's breaking what are supposedly unbreakable rules.  It is somehow escaping from within the rigid controls and sandboxing that Apple has explicitly and deliberately erected around it to contain and neuter exactly such exploits.  We know only that the malicious iMessage is bringing along an attachment.



The fact that this is a zero-click exploit means that for the benefit of its user, iOS must be automatically attempting to render whatever it is that iMessage has brought along with it, and that it is during this auto rendering that iOS loses control.  So it's breaking the rules that iOS is using, so I can't see how iOS attempting to analyze it within the bounds of the rules would succeed.



Okay, but having said that, if iOS were somehow able to record all iMessage attachments before performing any processing of the attachment, and that was in an immutable audit log on the target device which could not be, being immutable, that log could not have anything deleted.  Even if the attachment were deleted, the record of it would be preserved.  Then, if a target device was to be compromised, and if that compromise was recognized, then it would seem possible for a forensic analysis after the fact to be made.



You know, this theoretical immutable log would need to be immutable even to Apple's own software; right?  Because, if not, then malware running as Apple software could delete it.  But if there were something which, once written, no software running in an iOS device could erase it, then it's certainly feasible, given all the other preconditions.  One of the problems is that the target needs to know that they've been a victim.  Probably often they don't.  And if they knew it, then they would have to know that they could take their phone to someone and say, hey, I've been victimized.  I need the immutable log, the immutable forensics log, which we would then all know that iOS devices had, to be analyzed.  And that's a problem, too; right?  Because that would say that every attachment was being recorded by the iPhone.  And a lot of people don't want that.  So probably no good way to solve this problem, unfortunately.



Jason Egan said:  "Hey, Steve.  After hearing about the grc.sc situation, and please forgive me if I'm late to giving this option," he said, "I wondered if you couldn't just solve the problem with a RewriteRule on your host?  I've had to do this in the past with some of my projects."



Okay, now, so just to remind everyone what Jason's talking about, on Episode 926, two weeks ago, I ended the podcast with a shortcut of the week, grc.sc/926, you know, .sc as in shortcut; right?  And then the next week I commented, Mrlinux11 wrote saying:  "Hey, I tried to use that, but I'm getting a page not found."  Well, it turned out that he was using www.grc.sc.  And so something had added "www." in front of the unencumbered, simple "grc.sc" and broken it.



So, now, Jason is saying you could do a RewriteRule.  Okay.  As I've mentioned in the past - well, okay.  So first, so he's saying that I could solve the problem with a RewriteRule if my DNS included *.grc.sc in addition to grc.sc.  And that's true.  It never occurred to me that anyone would stick an arbitrary www. in front of grc.sc.  As I've mentioned before, I do have a *.grc.com in my DNS since that's able to handle all of the many prefixes that I use, like "www.", "forums.", "dev.", "sqrl." and whatever else .grc.com.



And I do redirect any other wayward queries over to www.grc.com.  Many years ago, GRC's web servers accepted connections on either grc.com or www.grc.com.  I figured, why not?  But then we noticed that Google's search results were coming up with a mixture of either grc.com and then whatever URL, or www.grc.com and whatever URL.  Google was seeing these as separate distinct websites and indexing them as separate websites.  And people were linking to GRC somewhat arbitrarily as either grc.com or www.grc.com.  And that was having the unintended side effect of reducing our Google page ranking by diluting the number of incoming links among both sites, what Google thought was two different sites.



So I changed GRC's web server to redirect by returning an HTTP/301 redirect from any plain grc.com over to www.grc.com.  And Google's spiders quickly learned that this was a single site which could, yes, be accessed by two different domains, but they all brought you to www.grc.com.  And then that consolidated all the links around a single domain, and that worked.  At the time I had to choose one or the other, and I'm really not sure I did the right thing by choosing the longer "www.grc.com" as the enforced default.  Yes, it's technically more accurate of the two.  But whenever I or anyone talks about GRC.com we say "grc.com," not "www.grc.com."  And I'm sure everyone just enters "grc.com" into their URL bar whenever they want to visit.



So anyway, in the next tweet, the answer to how this www.grc.sc came about was provided.  A guy tweeting as CPUGuru, he said:  "I found the source of the errant 'www' that you discussed in the podcast.  The hyperlink in the 926 PDF actually includes it."  So first of all, thank you, CPUGuru.  Good observation.  And it was Google Docs that did this.  I typed in grc.sc and /926 and hit ENTER.  All I saw was grc.sc/926.  But sure enough, if you hover your mouse over that link, there's a www. in front of it, which Google Docs silently added, which I didn't ask for.  So anybody who clicked the link in the Docs as I invited them to, I mean, I made it bold and increased the font size, saying, "Here, click me."  And that didn't work, thanks to Google Docs.



Jos Javier Vegas tweeted:  "Hi, Steve.  Regarding SN-927" - so that was last week.  He said:  "There is no official Let's Encrypt client today.  They transferred their implementation to the EFF, and it's now called 'Certbot,' which is the only one they recommended."



So thank you, Jose.  That's good to know.  I am still issuing certs for my own servers the old-fashioned way, through my favorite Certificate Authority, DigiCert.  But assuming that the world is going to switch, as has been proposed, to 90-day maximum life certificates, I'm glad that DigiCert also supports the ACME protocol.  And I will certainly then need to find automation for this, as will pretty much everyone because nobody wants to be generating new certs every three months.  That's way burdensome.



And finally, Michael Horowitz, tweeting from his Defensive Computing Twitter account.  He said:  "Steve, an FYI about HP+ printers."  He said:  "They must be online all the time, even if connected to a PC via USB.  And you must have an HP account."  He says:  "Perfect for spying.  Details on this page."  And he sent a link.  Michael has a bunch of really great pages.  This one is at the domain defensivecomputingchecklist.com.  And so it's defensivecomputingchecklist.com/printers.php.  And it's interesting that it's .php because we'll be talking about URLs ending in scriptable languages not long from now.



And so there was a perfect example of printers.php, where that's actually running a PHP script which is then pulling the contents of the page from some sort of CMS, you know, Content Management System.  Anyway, the first line of Michael's printers page reads:  "I hate printers.  So, too, does Leo Laporte, who is known as the Tech Guy on the radio.  He will not take phone calls about printers."  So that's Michael's lead-in to his page about printers.



I read through some of what Michael wrote on that page, and I recommend it to any of our listeners who may be curious to know more, and who might be in the market for a printer.  Michael has collected many reviews and anecdotes, and he tells a horror story of spending half a day trying to install an HP printer/scanner for a friend.  Mind you, Michael knows his way around PCs as well as any of us, yet he repeatedly hit wall after wall just trying to, like, play by the rules and install HP software.  At one point he mentioned that he downloaded the first package, which was 310MB, which, again, .3GB for printer software?



Anyway, his experience further supported my earlier statements when we were talking about this debacle with the HP printers, that I have long found HP software to be unconscionably atrocious.  It is just the worst.  And short version, apparently Brother makes the printer that most people recommend and have no problems with.  I think it's Brother.



JASON:  Yeah, it is Brother.  It's funny that you even mention this because I've been going through, it's not an HP printer, but it's an Epson printer at home that we've had, this Epson Inkjet forever.  And this is the third time that I've had to take it apart and try and repair it and get it back to working and everything.  And it didn't go back together the way I expected it to.  I was like, you know what, I'm done.  Like I actually just talked to Leo when he came in today.  I was like, what - send me the make and model.  He's like, "It's Brother.  It's a LaserJet printer."  He's like, "Get it.  You won't need another printer."



STEVE:  That's exactly the same conclusion that Michael came to.



JASON:  Yeah.  So I'm getting that.  I don't know which one it is.  I have to look up the model number and get it from him.  But I'm not going to look back.  I'm just going to do that.



STEVE:  Good.



JASON:  Yeah, and all printer software sucks.  It's just so bad.  It's, ugh.



STEVE:  What happened?



JASON:  I don't know.



STEVE:  It's such a simple thing to do.



JASON:  Totally.



STEVE:  You would think.  But...



JASON:  It's just always horrible.



STEVE:  Wow.



JASON:  That's been my experience, anyway.



STEVE:  Okay.  So finally, it is with no small amount of pride, a feeling of accomplishment, and some pent up relief that I can finally assert that, as far as I know, the work on the business end of SpinRite 6.1 - which is its DOS executable side - is finished.  There are presently no remaining known bugs, great or small.  Last Sunday afternoon, so two days ago, I announced the availability of the 29th alpha release to GRC's 696, but I noted we just got one more, so now we're at 697, registered SpinRite 6 owners who have been testing 6.1 since we began the alphas in November of last year.  And I suggested for the first time to the group that the code we all now have would likely and certainly could, with probably very few remaining changes, be moved into beta status to soon become SpinRite 6.1's shipping code.



6.1 contains a great many new features, one of them being an integrated FAQ which explains the choice of SpinRite's five redefined operating levels, its many command-line options, and a bunch of other useful tidbits.  Sort of, you know, power user tips and things that we've discovered and things to try and so forth.  So while the dust settles on this latest alpha, I'm currently writing that FAQ.  And that'll give some time for any testers who may have become bored with the seemingly endless interim development releases to give this proposed final code one last check.



Once we have the finished DOS code, that code, along with the very first work I did on the project, which was to create GRC's InitDisk USB drive prep utility, all that will be integrated into an updated SpinRite Windows app, and then we'll have 6.1.  Any new purchasers of SpinRite will automatically then begin receiving 6.1.  But since I want to let all of this new code breathe a bit - like any nice red wine that you open, give it a little time to breathe, it'll mellow - I plan to hold off on announcing it to all of SpinRite's past purchasers until it's had a bit more time among a wider audience.



So as soon as the official upgrade path is established, I'll be inviting all of this podcast's listeners, many of whom have previously purchased SpinRite, to update to v6.1.  Then, once it appears that it's going to be clear sailing, I'll begin the process of informing everyone who owns 6.0 that they get a free upgrade.  Even though it's been 19 years, it's free for them.  So no new action needs to be taken by anyone who hasn't already jumped onto testing the pre-release code.  I just wanted to note that the project had achieved a significant milestone along the way.



JASON:  Big-time.  Congrats.



STEVE:  Thanks to everybody for their support along the way, all this time.  And thank you, Jason.  Yes.



JASON:  Yeah, good work.



STEVE:  It's like, ahhh.



JASON:  Yeah, I bet.  Sigh of relief.  You only get a week, though, to rest on your laurels.  And then you've got to start working on the next version.  I'm sorry, Steve.



STEVE:  That's it.  That actually is the plan.  I'll be starting on SpinRite 7 immediately.



JASON:  I was going to say, just give yourself a week, I mean, you deserve to take some time, maybe go to Disneyland is an idea.



STEVE:  So inevitably some of the testers have been asking for this or that new feature.  I mean, like extensions beyond what it currently is.  And I've said, look, I said, understand this.  My goal has been to update 6 to 6.1, to resolve the problems with speed, the problems with drive size.  And inevitably I ended up doing way more than that.  But I said, "Understand this.  My goal is to obsolete 6.1 as quickly as I can with 7.0 because there were some things we could not do.  We could not do UEFI.  DOS will not run on UEFI.  Period.  So in order to move to UEFI, we have to leave DOS."



Well, that's a big change.  So that's not like an incremental - no one would call that an upgrade.  And also I wasn't able to do native support for USB.  So USB still runs through the BIOS, as all USB always has for all of SpinRite.  But I want to get to 7.0 so I can resolve those things also.  But it was just necessary to say, no, we're stopping at 6.1 and after that point my goal is to obsolete it as quickly as possible.  And once everyone has 6.1, that'll be what they want, too.  So it's a win-win.



JASON:  Absolutely.



STEVE:  And Jason, let's do our last sponsor insert, and then we're going to talk about the Massive MOVEit Maelstrom.



JASON:  And now it's time to move it.  It's time to get into the  Massive MOVEit - is it Maelstrom?  I always see that word, and I'm like, is it Maelstrom?  Maelstrom?



STEVE:  Maelstrom.



JASON:  Maelstrom.  I think it's Maelstrom.



STEVE:  Think so.



JASON:  Maelstrom, okay.



STEVE:  So our main topic today arrives about three weeks after the first signs of this significant problem arose.  I've been aware of and watching what's been happening.  But it wasn't until this past week that the scope and scale of the problem became fully apparent.  So this week we need to do a bit of catching up with what's been going on, and then we'll look at where we are today.



The trouble surrounds a globally popular file transfer facility named MOVEit.  MOVEit has both a local version called MOVEit Transfer, and also a cloud-based solution, MOVEit Cloud,  Basically it's a file-sharing and management solution from its parent company, Progress, www.progress.com/moveit, M-O-V-E-I-T.  And the way they've spelled it is MOVE is all caps, and "it" is small.  So MOVEit describes itself as "Managed File Transfer Software."



They said:  "Secure File Transfer and Automation Software for the Enterprise.  Guarantee the reliability of core business processes and transfer sensitive data between partners, customers, and systems the secure and compliant way with MOVEit.  Secure, Auditable, Automated, and Compliant File Transfer - On-Premise and in the Cloud.  MOVEit provides secure collaboration and automated file transfers of sensitive data and advanced workflow automation capabilities without the need for scripting.  Encryption and activity tracking enable compliance with regulations such as PCI, HIPAA, and GDPR."



Okay.  We've just read that it's compliant with PCI, HIPAA, and GDPR.  Unfortunately, another abbreviation its web frontend is fully compliant with is SQL, and not in the way they intended.  That's right.  The industry has been hit with another very powerful and significant SQL Injection attack.  I was tempted to title today's podcast "Little Bobby Drop Tables."  And our longtime listeners will understand the history of that name, or cartoon, or joke. 



The last time we were on this subject, I railed against the fundamentally broken design of the SQL command model, which exposes a fully capable command language to a web server that typically only needed to issue queries against the data.  Yet the unrestrained nature of this powerful command-line interface meant that the web server could do anything it wished.  And if someone could arrange to get the web server to pipe their own user-supplied text through to the backend SQL server, such a remote user could do pretty much anything they wished.



My denigration of SQL generated some pushback from some of our listeners who quite correctly noted that there were several other much safer and much more proper ways to do this with modern SQL servers.  And those listeners were of course 100% correct.  But I wasn't saying that safer and more proper ways had not since been developed to do this, but that the original unsafe ways also continued to be present in the interest of endless backward compatibility and not breaking legacy systems.  Consequently, nothing prevents the original horrible and fundamentally insecure approach from continuing to be used.



And so here we are today in 2023, we have the latest example of this bad architecture striking once again.  Even if someone was being as responsible as they could be with this hot potato, it would be the programmer's responsibility to try to think of all possible ways bad guys might attempt to sneak commands under the cover of data by encoding them strangely, using an unusual language locale, or who knows what?



As is always the case with security, the battle is asymmetric.  The programmer must block every possible avenue of conquest, whereas the bad guys only need to find one way in.  To Progress's credit - the publisher of MOVEit - from the start they have not tried to hide any of this in any way.  Right there on their main product promo page they write, they have a big  yellow, bright yellow warning, says:  "PRODUCT ADVISORY:  MOVEit Transfer and MOVEit Cloud Vulnerability.  Click for mitigation measures and patch information."



Okay.  So here's what we knew three weeks ago as May was coming to an end.  As we already know, MOVEit's solution includes a web-based frontend to manage the sharing, uploading, and downloading of files.  This makes sense in an era of JavaScript which is able to accept drag-and-drop uploads and manage local downloads.  When we mix in web authentication, which is, as we know, an entire discipline of its own, it's quite possible to create a fully functional web-based file management and distribution system.  And when the alternative was email, you know, using email for that, there's no competition.  MOVEit also supports FTP, but I assume that's for legacy purposes since FTP support has, as we know, finally been deprecated and removed from web browsers, and it's unclear what advantage FTP has any longer over any modern web browser solution.  Once upon a time, sure.  Today, eh, not so much.



As for this vulnerability, the bad news is that it was a true zero-day.  Progress learned of it only after and because the bad guys were already exploiting it.  And boy, were they.  Being responsible, Progress quickly patched all of their supported MOVEit versions and their cloud-based, you know, the MOVEit Transfer local, self-hosted versions, as well as their cloud-based service.  



There are essentially four things that can be done with this vulnerability:  the deletion of existing data, the exfiltration of existing data, the modification of existing data, and the implantation of new files and malware.  As it turned out, two of those four have been seen happening.



Mandiant, which is now owned by Google Cloud, has been tracking the MOVEit breach activity under their uncategorized moniker UNC4857 and posted that the opportunistic attacks have singled out a wide range of industries based in Canada, India, the U.S., Italy, Pakistan, and Germany.  And Mandiant also wrote that it was "aware of multiple cases where large volumes of files have been stolen from victims' MOVEit transfer systems" and adding that the web shell left behind, which they call LEMURLOOT, is also capable of stealing Azure Storage Blob information.



So we have, to start with, massive data exfiltration.  And Mandiant's mention of a web shell brings us to the second of the two things that is being done, the implantation of new files and malware, because the bad guys have also been found to be dropping web shell malware before they leave.  We've previously talked about web shells, but here's a bit of history.  In the early days of the web, web servers only delivered static HTML web pages. You gave them a URL which was basically the location of the page's text on the server, and it returned that page to the browser.



After several years of that, browser-side scripting, which was embodied by and enabled by JavaScript, introduced the concept of running code that you received from a web server on your browser, so-called client-side scripting.  This brought the user's client alive, giving it some of the capabilities of a local application.  A perfect and simple example is that a user's entered password could be hashed locally by scripting running in the browser so that a web server that wanted a password never received anything but the hash.  That was a big benefit in security for the user.



These days, of course, we're seeing the logical evolution of scripting on the client with amazingly complete and complex web apps.  But even browser-side scripted pages could be delivered by static files.  You know, lots of JavaScript is just a .js, it's just a JavaScript file that is being delivered statically to drop a library of complex JavaScript like no.js onto the user's browser.  That's just a static download, and typically it's cached, so it's very quick.



The big change on the server side occurred when web servers started running code to respond to queries being made by their clients.  The earliest implementation of this was known as CGI, which stands for Common Gateway Interface.  The idea behind CGI was simplicity.  A web client would make a query, and the web server would essentially serve as an intermediary between the user's web browser and some code which the web server would execute on the server.  To do this, the web server would launch and run a separate CGI program in the background, often with the extension .cgi.



The web server would provide the CGI program with what the user had queried, and whatever the CGI program returned through its Standard Output would be piped back by the web server to the user's web browser to be seen.  So with CGI, rather than delivering static textual web pages, the output of a pre-compiled program was returned to the user's web browser.  This was a big application for Larry Wall's PERL, which was often used in early CGI applications.



This model was clean and simple, and it remains in heavy use today where, as I noted earlier from that printers.php page, where PHP is the back-end recipient of a client's CGI queries. GRC's web forums and that link shortener I mentioned are all PHP, as is WordPress, which as we know runs a huge portion of the web, WordPress written in PHP.  Modern web servers provide for many ways to generate dynamic content, as it's called.  GRC's ShieldsUP!, Perfect Passwords, Perfect Paper Passwords, GRC's DNS Spoofability test, and GRC's support for SQRL are all implemented using DLLs that I wrote of course in assembly language, using the Microsoft ISAPI API to obtain and return data to and from users' web pages.  So this is common.



Microsoft also promotes their own server-side interpreter which implements their scripting language known as Active Server Pages (ASP).  When a web server which has Active Server Pages enabled encounters a URL referring to a file ending in .asp or .aspx, the web server will look for that file on the URL's provided path and will run the code contained in that file, the Active Server Pages code.



And that brings us back to the - I can't say it.  Lemur, L-E-M-UR.



JASON:  LEMURLOOT?



STEVE:  Lemur, thank you, lemur.



JASON:  I don't know, I'm guessing.



STEVE:  If I start - I think you're right, it's LEMURLOOT.



JASON:  LEMURLOOT.



STEVE:  LEMURLOOT web shell, a tongue twister.



JASON:  Yeah.



STEVE:  Thank you, Jason.  That's being left behind by these attackers.  In machines that have been attacked, the attackers leave behind a file named "human2.aspx."  They chose that name since a human.aspx file is already present in the system as part of the authentic file set.  So obviously they're making it look like it belongs there.



They also sometimes leave additional files with the file extension ".cmdline."  This human2.aspx file functions as a web shell.  It's a sophisticated script that will provide future access to any ASP-script capable web server that's unlucky enough to host it.  The bad guys know its name, and they know its location on the server.  So they're able to invoke it at any time in the future remotely simply by querying the server for a URL and path ending in "human2.aspx."  The web server will immediately run that code, which typically gives remote attackers control of the system and probably of the network.  The web shell is also engineered to add new admin user account sessions with the name "Health Check Service" in an effort to appear benign and avoid detection.



This means that just patching against the attack after the fact will not be sufficient protection, since a previously vulnerable server may have already been quietly infected with the human2 web shell.  After patching, it will be necessary to search for any "IOCs," as we call them today, Indications of Compromise.



CISA quickly issued a nationwide alert to let everyone know and to demand that all government agencies using the MOVEit Transfer system update and check for evidence of past incursions.  Our friends at the web scanning search engine Censys, who were the subject of last week's podcast, have identified more than 3,000 vulnerable instances of MOVEit, the majority of which are located in the U.S.



Huntress Labs was all over this at the start of the month. Excerpting and editing a bit what they wrote, they said:  "On June 1st, 2023, Huntress was made aware of active exploitation attempts against the MOVEit file transfer application.  Previously, on May 31st, the vendor Progress had just released a security advisory expressing there is a critical vulnerability that could lead to unauthorized access.  On June 2nd, the industry dubbed this vulnerability CVE-2023-34362.  Progress brought down MOVEit Cloud as part of their response and investigation.



"Huntress has fully recreated the attack chain exploiting MOVEit Transfer software.  We have uncovered that the initial phase of the attack, SQL injection, opens the door for even further compromise - specifically, arbitrary code execution.  We use our exploit to receive shell access with Meterpreter, escalate to NT AUTHORITY\SYSTEM" - meaning full system-level privileges - "and detonate a Cl0p ransomware payload."  And we'll be talking about Cl0p in a moment.



They said:  "This means that any unauthenticated adversary could trigger an exploit that instantly deploys ransomware or performs any other malicious action.  Malicious code would run under the MOVEit service account user 'moveitsvc,' which is in the local administrators group."  Meaning, again, full admin privileges.  "The attacker could disable antivirus protections, or achieve any other arbitrary code execution.  Another demonstration showcased compromising the MOVEit Transfer API and application itself.  With that alone, we upload, download, and potentially exfiltrate files as a threat actor would.



"The behavior that the industry observed, adding a human2.aspx web shell, is not necessary for attackers to compromise the MOVEit Transfer software.  It's an option that this specific threat actor chose to deploy for persistence, but the attack vector offers the ability to detonate ransomware right away.  Some have already publicly reported attackers pivoting to other file names," that is, using other than human2 because that's now become known.  They end:  "The recommended guidance is still to patch and enable logging.  From our own testing, the patch does effectively thwart our recreated exploit."  Okay.  So beginning of June, patch released.  Unfortunately, as they say, the horses had left the barn.  Actually, they had left the barn, and they had traveled quite a distance from the barn.



Microsoft attributed the MOVEit Transfer zero-day attacks to Lace Tempest, a threat actor previously linked to Cl0p ransomware, data theft, and extortion attacks.  On June 6th, the Cl0p gang posted a communication to their leak site demanding that their victims contact them before June 14th to negotiate extortion fees for deleting the stolen data.  I'm going to read without correcting the grammar what the ransom note said that was posted on Cl0p's site for their victims to respond to.



Cl0p wrote:  "Dear Companies.  CL0P is one of top organization offer penetration testing service after the fact."  Which, huh, offer penetration testing service after the fact.  That's one way to put it.  They said:  "This is an announcement to educate companies who use Progress MOVEit product that chance is that we download a lot of your data as part of exceptional exploit.  We are the only one who perform such attack, and relax because your data is safe."  Yeah, right, in Russia.  They said:  "We are to proceed as follow, and you should pay attention to avoid extraordinary measures to impact your company.  IMPORTANT!"  In all caps with an exclamation point.  "We do not wish to speak to media or researcher.  Leave."



Then we have three steps.  "Step 1:  If you had MOVEit software, continue to Step 2, else leave.  Step 2:  Email our team at unlock@rsv-box.com or unlock@support-multi.com.  Step 3:  Our team will email you with dedicated chat URL over TOR.  We have information on hundreds of companies, so our discussion will work very simple."



Now we have seven steps.  "Step 1:  If we do not hear from you until June 14th, we will post your name on this page.  Step 2:  If you receive chat URL, go there and introduce you.  Step 3:  Our test will provide 10% proof of data we have and price to delete.  Step 4:  You may ask for two to three files random as proof we are not lying.  Step 5:  You have three day to discuss price; and if no agreement, you custom page will be created.  Step 6:  After seven days all your data will start to be publication.  Step 7:  You chat will close after 10 not productive day, and data will be publish."



Next section is, all caps, "WHAT WARRANTY?  Our team has been around for many years.  We have not even one time not do as we promise.  When we say data is delete it is cause we show video proof."  Okay.  I guess they're going to show someone typing the command into a console.  Wow.  That's proof, all right.  It's definitely gone now.  Then they say:  "We have no use for few measle dollars to deceive you."  They don't want any measle dollars, apparently.  "Call today before your company name is publish here."



Friendly cop.  "Friendly Cl0p," it says.



JASON:  Cl0p, Cl0p, Cl0p.



STEVE:  "FRIENDLY CL0P.  P.S.  If you are a government, city, or police station, do not worry.  We erased all your data.  You do not need to contact us.  We have no interest to expose such information."



Okay.  Then, presumably after the initial June 14th contact deadline had passed, they added updates.  So under updates they have "Bissell.com 50TB company data.  Get ready for something interesting.  Fmeraldc.com 100TB company data.  Get ready for something interesting."  And then a bit later, "360EquipmentFinance.com files, Part 1 Published.  PrecisionMedicalBilling.net files, Part 1 Published.  HCI.edu files, Part 1 Published."



So the point of this story is, over the past three weeks, is that we have the internal private data of thousands of U.S. companies who have been using Progress's MOVEit Transfer software, first exfiltrated from their servers, and in some cases as much as 100TB worth, apparently.  This was not good, but we're not done yet.  After a week had passed, believe it or not, Progress announced the news of additional discovered vulnerabilities.



June 9th they said:  "In addition to the ongoing investigation into vulnerability CVE-2023-34362, we have partnered with cybersecurity experts to conduct further detailed code reviews" - oh, what a nice idea, let's do it now - "as an added layer of protection for our customers.  As part of these code reviews, cybersecurity firm Huntress has helped us to uncover additional vulnerabilities that could potentially be used by a bad actor to stage an exploit.  These newly discovered vulnerabilities are distinct from the previous reported vulnerability shared on May 31st, 2023.  All MOVEit Transfer customers must apply the new patch, released on June 9th, 2023.  All MOVEit Cloud customers, please see the MOVEit Cloud Knowledge Base Article for more information.  The investigation is ongoing, but currently we have not seen indications that these newly discovered vulnerabilities have been exploited."



Then, exactly one week after that, last Friday, June 16th, we have more.  "June 16th, 2023:  Yesterday we reported the public posting of a new SQL injection vulnerability that required us to take down HTTPS traffic for MOVEit Cloud and to ask MOVEit Transfer customers to take down their HTTP and HTTPS traffic to safeguard their environments.  We have now tested and deployed another patch to MOVEit Cloud, returning it to full service across all cloud clusters.



"We have also shared this patch and the necessary deployment steps with all MOVEit Transfer customers.  All MOVEit Transfer customers must apply the new patch, released on June 16th, 2023.  Details on steps to take can be found in the following Knowledge Base Article.  All MOVEit Cloud customers, please see the MOVEit Cloud Status page for more information.  The investigation is ongoing, but currently we have not seen indications that this newly discovered vulnerability has been exploited."  So what happened in the third case is that some random independent person looked at the software and said, "Oh, how about this one?" and posted it publicly.  Whoops.



Okay.  So what do we know about the victims so far?  CNN Business had some reporting on this.  Excerpting from what CNN reported:  "A growing number of businesses, universities, and government agencies have been targeted in a global cyberattack by Russian cybercriminals that are now working to understand how much data was compromised.  CISA said Thursday that several federal agencies have experienced intrusions.  The U.S. Department of Energy said it took immediate steps to mitigate the impact of the hack after learning that reports from two department entities had been compromised.  It's also impacted state governments in Minnesota and Illinois.  And on Thursday, state agencies said 3.5 million Oregonians with driver's licenses or state ID cards had been impacted by a breach, as well as anyone with that documentation in Louisiana."  In other words, all of Louisiana.



"British Airways confirmed that its staffers' names, addresses, national insurance numbers, and banking details were exposed because its payroll provider Zellis used MOVEit.  The BBC said its staff had also been afflicted because Zellis was its payroll provider.  The UK's beauty and health company Boots said some of its team members' information was also stolen.  Brett Callow, threat analyst at cybersecurity firm Emsisoft, said the hackers have also listed Aon and The Boston Globe as victims.



"'By my count,' he said, 'there are now 63 known/confirmed victims plus an unspecified number of USG agencies.'  The hacking campaign has also spread to academia.  Johns Hopkins University in Baltimore and the university's renowned health system said in a statement that 'sensitive personal and financial information,' including names, contact information, and health billing records, may have been stolen in the hack.  Meanwhile, Georgia's statewide university system, which spans 40,000-student University of Georgia along with over a dozen other state colleges and universities, confirmed it was investigating the scope and severity of the hack."



TechCrunch added:  "Cl0p, the ransomware gang responsible for exploiting a critical security vulnerability in a popular corporate file transfer tool, has begun listing victims of the mass hacks, including a number of U.S. banks and universities.  The victim list, which was posted to Cl0p's dark web leak site, includes U.S.-based financial services organizations 1st Source and First National Bankers Bank, Boston-based investment management firm Putnam Investments, the Netherlands-based Landal GreenParks, and the U.K.-based energy giant Shell.  GreenShield Canada, a non-profit benefits carrier that provides health and dental benefits, was listed on the leak site but has since been removed.



"Other victims listed include financial software provider Datasite, educational non-profit National Student Clearinghouse, student health insurance provider United Healthcare Student Resources, American manufacturer Leggett & Platt, Swiss insurance company OKK, and the University System of Georgia.  A spokesperson for German mechanical engineering company Heidelberg, which Cl0p listed as a victim, told TechCrunch in a statement that the company is 'well aware of its mentioning on the Tor website of Cl0p and the incident connected to a supplier software.'



"Cl0p, which like other ransomware gangs typically contacts its victims to demand a ransom payment to decrypt or delete their stolen files, took the unusual step of not contacting the organizations it had hacked.  Instead, a blackmail message posted on its dark web leak site told victims to contact the gang prior to its June 14th deadline.  Multiple organizations have previously disclosed they were compromised as a result of the attacks, including the BBC, Aer Lingus and British Airways.  These organizations were all affected because they rely on HR and payroll software supplier Zellis, which confirmed that its MOVEit system was compromised.



"The Government of Nova Scotia, which uses MOVEit to share files across departments, also confirmed it was affected, and said in a statement that some citizens' personal information may have been compromised.  However, as we know, Clop's leak site said, 'If you are a government, city or police service, we erased all your data.'  Ofcom, the U.K.'s communications regulator, also said some confidential information had been compromised in the MOVEit mass hack.  In a statement, the regulator confirmed that hackers accessed some data about the companies it regulates, along with the personal information of 412 Ofcom employees.  Transport for London (TfL), the government body responsible for running London's transport services, and global consultancy firm Ernst & Young are also impacted, according to BBC News.  Neither organization responded to TechCrunch's questions."



Boy.  You know, it's one thing to say, oh, thousands.  It's another thing to actually hear them listed and understand what it means that that many actual physical entities were affected and afflicted by this.  Since we haven't yet enumerated the literal thousands of individual companies, government and educational agencies, and other organizations that that have been compromised in this mass attack, many more victims are expected still to be revealed in the coming days and weeks.



And now that they know what to look for, security researchers looking back through their logs have determined that someone had been experimenting with the exploitation of these MOVEit vulnerabilities for the past two years, since 2021.  And Cl0p was also responsible for previous mass attacks exploiting flaws in Fortra's GoAnywhere file transfer tool - remember that one recently? - and Accellion's file transfer application.



So, welcome to our new normal.  A serious flaw is silently discovered in a popular, highly used, web-connected application.  Its discoverer remains quiet for years while patiently working out exactly how to set up the attack for maximum effect in the greatest number of cases.  There's a bit of a risk/reward tradeoff here, since it's always possible that by not jumping on and exploiting a vulnerability immediately, it'll give it some time to be discovered and remediated before an attack can occur.  We've often seen this effect when, for example, a well-planned Pwn2Own exploit fails because patches were coincidentally released on the eve of the competition to foreclose the exploit.  Then, all vulnerable service instances are located ahead of time, and the attack is staged and readied.



And finally, effectively all at once, all of that vulnerable service's users have their data silently exfiltrated and stored.  In the case of Cl0p, all of that sensitive data probably lands in Russia, and a web shell is also installed to allow future access.  Finally, victim companies are notified, threatened en masse, publicly shamed, and eventually, if they don't accede to the extortionist's demands, have their potentially sensitive internal and client data released to the world.



The industry has observed that, in this instance, the traditional "don't call us, we'll call you" model has been reversed, with the attackers asking their victims to initiate the contact.  It's been suggested, as I noted, by Tech Crunch that this is due to the fact that there are just too many victims for the attackers to manage proactively.  So they have chosen to be more passive and wait to be contacted.  This is probably an optimal strategy, since what the attackers want is maximum extortion payments.  And the likelihood of being paid, and being paid a larger amount, is far higher if they are proactively contacted by a concerned victim than if they reach out to cajole all of their targets.



Now, at this point we don't know how much money the campaign will net for Cl0p.  We're still in the early days, unfortunately.  But the numbers are distressing for ransomware and cyber extortion gangs in general.  Ryuk is known to have netted $150 million.  REvil, $123 million in 2020.  LockBit, $91 million.  DarkSide, $90 million.  Maze/Egregor, $75 million.  Cuba, $44 million.  Conti, $25.5 million.  NetWalker, $25 million.  Dharma, $24 million.  All told, that comes to just shy of $650 million that has been raked up by these guys.  And of course everybody knows this.



As this podcast has observed, malicious hacking is no longer being done for sport.  It's now all about money.  And unfortunately, money creates incentive.  And as we've also seen frequently, there are a sufficient number of undiscovered vulnerabilities lurking within much of today's software to incentivize the bad guys into finding and exploiting them for their profit.



And it's not as if the bad guys are smarter.  As soon as equally talented security researchers began taking a closer look at Progress's MOVEit Transfer software, additional previously unsuspected vulnerabilities started falling out of the thing weekly.  That widely used software turned out to be a mess.  Yet good guys were never given sufficient prior reason to examine it because the economics of doing so did not make sense.  No security researcher was going to earn millions of dollars by discovering those problems and turning them in for a bounty.  But the economics for the bad guys did and does make sense, since they will likely manage to extort millions of dollars overall from their newly acquired victim base.



Something needs to change.  Academics in their ivory towers are busily inventing and developing new computer technologies that have none of these problems.  But what we know is that down here on the ground, nothing changes unless it is forced to.  When I was previously complaining about the utterly and obviously broken traditional model of SQL database access by web servers, I was scolded by our listeners and told, "Oh, Steve, don't you know that was the old way of using SQL?"  Right?  Old.  Tell that to the thousands of victims of this latest catastrophe of SQL database usage.



And speaking of old, does everyone know just how old this attack is?  Just how old is the exploit that created the "Little Bobby Drop Tables" joke?  The operation of the SQL injection exploit was first documented in 1998 by cybersecurity researcher and hacker Jeff Forristal.  His findings were published in the hacker magazine Phrack.  Writing under the moniker Rain Forest Puppy, Jeff explained how someone with basic coding skills could piggyback unauthorized SQL commands into legitimate SQL commands to pull sensitive information out of a website's database.  Gee,  doesn't that sound a lot like what just happened last week?  And that warning came 23 years ago.  23 years ago.  This is a fundamental database architecture that was horribly bad then, and nothing has changed since.  It happened again three weeks ago.



In '98, when Jeff Forristal notified Microsoft about how the vulnerability impacted their very popular SQL Server product, Microsoft - no one's going to believe this.  Microsoft didn't see it as a problem.  As Forristal put it at the time in his article for Phrack:  "According to Microsoft, what you're about to read is not a problem, so don't worry about doing anything to stop it."  So how did that advice work out?



In 2007, the biggest convenience store chain in the United States at the time, 7-Eleven, fell victim to a SQL injection attack.  The Russian hackers used SQL injections to hack into the 7-Eleven website and used that as a stepping stone into the convenience store's customer credit card database.  This allowed the hackers to then withdraw cash back home in Russia.  Wired magazine reported that the culprits absconded with $2 million.  That same year, cybercriminals used SQL injection to gain administrative control over two U.S. Army-related websites and redirect visitors to websites with anti-American and anti-Israeli propaganda.



The next year, in 2008, MySpace data breach ranks as one of the largest attacks on a consumer website in history.  Cybercriminals stole emails, names, and partial passwords of almost 360 million MySpace user accounts.  Thanks to that attack, we learned that it wasn't a good idea to reuse passwords across sites.



The award for the most egregious lack of security probably goes to Equifax.  The 2017 Equifax data breach which yielded extremely personal information - names, social security numbers, birthdays, and addresses - for 143 million consumers was, you guessed it, a SQL injection attack.  And what's worse, prior to the data breach a cybersecurity researcher had warned Equifax that they were susceptible to a SQL injection attack.  Whoops.



Every three years the OWASP Open Web Application Security Project ranks the Top 10 Most Critical Web Application Security Risks.  Guess where SQL injection ranks?  Yep, number one.  Our listeners, who kindly took the time to educate me about there being much better and more secure ways to use SQL, were absolutely correct.  Which only serves to underscore the tragedy of the fact that SQL will still happily operate today the same way it did 23 years ago, in 1998.



Using SQL the horribly and fundamentally insecure way is, unfortunately, also the obvious and easy way.  This thing should have been strangled in its crib the moment it was born in 1998.  But instead, Microsoft and others blessed it and said:  "Oh, it's so wonderfully easy and powerful.  Those worrywarts are just trying to get some ink.  It's fine.  Don't worry about it.  Just be careful."  Right.  Be careful walking on ice while carrying that dynamite.



JASON:  Just don't fall.  Just, you know.



STEVE:  Just don't fall.  And what was interesting was that notice that Huntress who found that second round of multiple exploits missed the third one.  So even they, staring at the code, didn't see it.  And that's the problem.  This approach is insecure by default.  And so it's up to someone attempting to filter out any possible misuse.  And obviously that's not working out very well.



JASON:  Mm-hmm.  Mm-hmm.  Wow.  That is a lot to take in, and to know that there is no resolve, there is no resolution on anything, what you're talking about right now.  It's worse.  It just continues to snowball.



STEVE:  You know, and normally we're able to put some of the blame on the people who get attacked; right?  It's like, well, you shouldn't have been doing this or that.  You know, someone clicked a link, and that brought the malware in, and the ransomware got you.  No.  This was a widely used, you know, basically it's a bit of a competitor to Microsoft's file sharing solution, you know, SharePoint.  It does the same sort of thing for enterprises.  And so this thing was widely used.  There was no reason to imagine that it would wipe out every one of its users.



JASON:  Trust no one or nothing.  Period.  End of story.  Because it all could go down.  



STEVE:  I've got databases here, and none of them run SQL.



JASON:  There you go.



STEVE:  My ecommerce system and SQRL need a user ID database on the backend.  Nope, you won't find me using that piece of never mind.



JASON:  Piece of never mind.  I like that, actually.  Good stuff.  I love the deep dive.  But, I mean, you always do fantastic deep dives.  But I often find myself at the end of it like, oh, dear god.  Like, what are you going to do?



STEVE:  Well, Jason, now we've got the power LEDs on all of our security equipment.



JASON:  I know.  If it's got a little LED power on it, then it's over.  We're screwed.  It's like, even that isn't safe.  If power LEDs aren't safe, what is?  That's what I ask you.  Steve, great stuff.  Anyone who wants to get into everything that Steve's up to, all you have to do is go to GRC.com.  You'll find all sorts of Steve Gibson goodness there, SpinRite of course.  Again, congratulations, major milestone.



STEVE:  Thank you.



JASON:  And if you aren't onboard, you can get it there.  The best mass storage recovery and maintenance tool can be found at GRC.com.  Audio and video of this show, of course.  Also transcripts of this show can be found there, as well.  If you are on the TWiT site, you want to find Security Now!, it's easy to do:  TWiT.tv/sn.  Of course we have our audio and video there, as well.  But we also have ways to subscribe to the show, which is really at the end of the day one of the most important things.  If you love what we're doing here at TWiT, if you love podcasting in general, subscribe to the shows.  That's sending the best signal to us and to the people who help pay us to do what we do.  So that can be found at TWiT.tv/sn.



We do record this show every Tuesday.  Usually it's Leo sitting here in the studio.  But if not, I'm happy to fill in for Leo any time he's out at Disneyland.  But we do it every Tuesday, 4:30 p.m. Eastern, 1:30 p.m. Pacific, 20:30 UTC.  Thank you so much to John for engineering here in the studio.  Thank you to Steve.  You do a fantastic show, and I'm always honored to sit in the Leo hot seat while he's out.  So thank you for welcoming me, Steve.



STEVE:  Jason, absolute pleasure, and I'll see you next time you're standing in.  



JASON:  Sounds good.  We'll see you next time on Security Now!.  Bye, everybody.



STEVE:  Bye.



Copyright (c) 2023 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.






GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#929

DATE:		June 27, 2023

TITLE:		Operation Triangle

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-929.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  Today's podcast is chock full of news.  What has DuckDuckGo just announced?  What about the Tor Project?  Has Opera just made a big mistake?  What is the KasperskyOS?  What's happening to non-Russian web hosting for Russians?  Are SolarWinds executives finally going to be held to account?  We now have the U.S. Space Force, what's next?  What's the latest large site to support Passkeys?  Who would like permission to spy on their own citizens?  Which facial recognition smartphone unlocking can you trust and which should not be?  And what was the inevitable shoe to drop following last week's coverage of the Massive MOVEit Transfer mess?  Then, after sharing a bit of listener feedback, we're going to take a much closer look into Kaspersky's discovery of a pervasive 4-year iPhone spyware campaign.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  We've got new browsers from DuckDuckGo and Opera, now with AI.  We'll also talk about Kaspersky's discovery of a severe bug on iPhones - that's why there was an Apple update, emergency update last week - and the cost of doing business in the Russian federation.  That and a whole lot more coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 929, recorded Tuesday, June 27th, 2023:  Operation Triangulation.



It's time for Security Now!, the show where we cover the latest security news, of which there was a lot, and I missed.  But fortunately, Steve's going to fill me in.  Hi, Steve Gibson.



STEVE GIBSON:  Yes, Leo, welcome back.  We missed you last week.  We had a - it's funny, too, because throughout the podcast I recognized how much continuity there is in this podcast, and so I was...



LEO:  We build on, each episode builds on the episode before.	



STEVE:  Yes.  And so I kept saying to Jason, well, you wouldn't know this, but blah blah blah blah, and like trying to quickly bring him into what we were talking about.  But then I thought, well, wait, doesn't he produce this podcast?  So maybe he's being forced to listen to it every week whether he wants to or not.



LEO:  I think he may be forced to listen to it.  



STEVE:  Yeah, yeah.



LEO:  Anyway, thank you, Jason Howell, for filling in.  I really appreciate it.  And we had a good time in L.A.



STEVE:  So then I thought, maybe this is dumb that I keep saying, well, you wouldn't know this, if he's thinking, yes, Gibson, unfortunately I do.



LEO:  I know it.  You know, it's a fair thing to say about anything you say on this show, "You wouldn't know this," because it's advanced.  These are advanced topics.



STEVE:  But I actually created the second page of our show notes for you, Leo, because I just - I couldn't have you miss the things that we talked about, mostly because there were only three.



LEO:  Yes, I did note that, yes.



STEVE:  Okay.  But so this week we're going to come back to a topic that we opened three weeks ago, talking about Operation Triangulation.  And at the very end Eugene Kaspersky tells us why he named it that, which I hadn't seen anywhere else.



So today's podcast is chock full of news.  We're going to answer a bunch of questions.  What has DuckDuckGo just announced?  What about the Tor Project?  Has Opera just made a big mistake?  What is the KasperskyOS?  What's happening to non-Russian web hosting for Russians?  Are SolarWinds executives finally going to be held to account?  We now have the U.S. Space Force.  What's coming next?  What's the latest large site to support Passkeys?  Who would like permission to spy on their own citizens?  And that's a little disturbing.



Which facial recognition smartphone unlocking can you trust, and which should not be trusted?  And what was the inevitable shoe to drop following last week's coverage of this Massive MOVEit Transfer mess?  Then, after sharing a bit of listener feedback, we're going to take a much closer look at Kaspersky's discovery of a pervasive four-year-long iPhone spyware campaign.  And Leo, we've got another great Picture of the Week and a recap of last week's because I couldn't have you miss it.



LEO:  Steve, I am touched and honored that you care so much about me that you did that.  That is very sweet.  I appreciate that.  I have not looked.  I have not peeked at any of our Pictures of the Week, either of our Pictures of the Week.



STEVE:  Good, because this one, the concept behind the picture was not unique, but I'm happy with the caption that I gave it.  And it ends up creating a wonderful effect for people who are in the know, as of course you and our listeners all are.



LEO:  Yes.  You know, you raise a really interesting point.  I hadn't really thought about it, but it is true.  You probably don't want to miss an episode of this show, and I hope none of our listeners are.  Subscribe and download, and that way you'll always have a copy because we do build, each show does build, assume a certain amount of knowledge.



STEVE:  Yeah.



LEO:  You've got mail, Steve.  I hear it.  I hear it.



STEVE:  I forgot to silence my phone next to me.



LEO:  Bamm-Bamm.  So is it Picture of the Week time?



STEVE:  Picture of the Week time.  And it's fun.



LEO:  All right.  Should I show the picture?



STEVE:  Sure.



LEO:  I'll pull it up.  I'm going to see it first.  And then I will switch over to it so that you can - okay.  Very good.  Very funny.  The caption you wrote; right?



STEVE:  I wrote the caption, yeah.



LEO:  All right.  Let me show it here.  Unfortunately it keeps shrinking down.  I want to show it full screen, if I can.



STEVE:  But I think it was Simon Zerafa, our longtime podcast listener and follower, who had the concept of it.  And I thought it was very clever.



LEO:  When Apple creates a walled garden, they don't mess around.  And of course what's the picture of?



STEVE:  Yeah, Apple headquarters, which is enclosing a beautiful garden, and that's one hell of a wall.



LEO:  It is a literal walled garden, yeah. 



STEVE:  It actually is, yes.



LEO:  Yeah.



STEVE:  I thought that was very cool.



LEO:  A lot of people pointed out that the Apple campus, the brand new headquarters of Apple, a few years old, faces inward, not outward.  That's another point to mention.



STEVE:  Yeah, yeah.  And like we don't, like normal people don't ever get to see it, do they.



LEO:  No.  No.



STEVE:  I mean, do they give - it's just completely off limits to everyone.



LEO:  We get to go to the Visitors Center out here.  Sometimes people get to go inside the ring.  But, yeah, you really have to be somebody special to be invited inside, inside the special spaceship.



STEVE:  We see little bits of it on the WWDC, and it's just spectacular.



LEO:  Yeah.  It really does look beautiful.  I mean, when you've got the money.  And Jony Ive designed it with the help of Steve Jobs.  You kind of get the best.



STEVE:  Yeah.  Okay.  So now the next page of the show notes is the security catch-up for you, Leo.



LEO:  Okay.  Starting with the Picture of the Week.



STEVE:  Starting with last week's Picture of the Week.



LEO:  If it's not tied down, of course, all that's left of this bicycle - which has been triple-locked, two U-locks on the wheels, and even a lock around the seat.  And of course all that's left is the wheels and the seat.  The frame is long gone.  Yeah, that's the problem.  Bicycle thieves, I tell you.



STEVE:  I just love it.  So, okay.  So the few subjects that we talked about last week, believe this or not, and you just had to know this, Leo, which is why I didn't want you to miss it.



LEO:  Okay.



STEVE:  It turns out that those brilliant researchers at Israel's Ben-Gurion University of the Negev, who are always coming up with wacky ways of exfiltrating data, they listen to Security Now!.



LEO:  Whoa.



STEVE:  And the lead researcher sent me a DM saying, hey, Steve, we've just finished some other research.  You know, love the podcast.



LEO:  Wow.  We talk about them all the time.



STEVE:  We do.  And they're listeners.  So get this.  You know how much fun we've had joking about the fact that the flashing LEDs on our routers don't actually convey any data.  All they're doing is showing that there's something going on on the wire.  Well, that's true.  It turns out, however, that they have discovered that the power LEDs of equipment doing, like, secret computation is affected enough by the work required to process the secret information in crypto algorithms that they have been able to recover secret keys by recording the LED fluctuations.  And they're even able to do it using an Apple iPhone 13.



LEO:  Oh, man.



STEVE:  Because even though the frame rate of the iPhone is 60Hz, it turns out that the imaging arrays are scanning at 60Hz.  So if you fill the frame with the image of an LED, like by zooming in or holding the camera really close to it, you end up getting two orders of magnitude.  So it goes from 60Hz to 60KHz of effective scanning rate.  And that gives them enough of a high sample rate to be able to capture minor intensity fluctuations which they've been able to then reverse engineer the secrets that are being processed by the equipment.



LEO:  Wow.  Wow.



STEVE:  So you just had to know that.  While you were off riding the Dumbo ride...



LEO:  That's amazing, yeah.



STEVE:  ...we were here doing real work.



LEO:  I did ride the Dumbo ride.  You must have seen our Instagram post.



STEVE:  I'm sorry.  Well, I heard you did, please, what about it's a Small Small World?



LEO:  I did not go on it.  I did not.  But I did go on the Dumbo ride.



STEVE:  Okay.  That's okay.



LEO:  I'll show you.  That'll be my Picture of the Week a little later.



STEVE:  The last thing is that, believe it or not, we were hit, actually the entire industry was hit with the 25-year-old persistent SQL injection vulnerability.  



LEO:  Oh, my gosh.



STEVE:  I know.  I was going to say hundreds, but it's actually a couple thousand companies were all using some software from a company called Progress Software called MOVEit Transfer.



LEO:  Oh, yeah, yeah, yeah.



STEVE:  Which is a managed file sharing facility.  Turns out horrible SQL injection vulnerabilities.  All of them got their data exfiltrated and are now being extorted.  And I mean the list of companies that this happened to was astonishing.  And so of course that sent me off on a tirade last week about how is it that in 1998 this was observed as being a problem, Apple said don't worry about it, it's not a problem, and here we are now.  You know, it's also been OWASP's number one security threat, like, constantly on their list.  And it happened again.



Oh, and the final announcement of last week was that SpinRite is at, like, completion.  I proposed, I released Alpha 29, and then Alpha 30.  We're, like, right at the edge of this thing being done.  It looks like I may actually have just broken something in the last couple days.  But, you know, a so-called regression.



LEO:  Hands off, Steve.  Hands off.



STEVE:  I'll get that fixed.



LEO:  Stop touching it.  And here I am, while you were talking about that, literally on Dumbo.



STEVE:  And there's Lisa in her own - is Lisa behind you there?



LEO:  Yeah, she had her own Dumbo.  We didn't want to share.



STEVE:  She had her own Dumbo car, okay.



LEO:  Yeah, yeah, well, yeah.



STEVE:  It looks like to me you're having too much fun on the Dumbo ride, Leo.



LEO:  It was a lot of fun, I have to admit.



STEVE:  What?



LEO:  That's more my speed than the roller coasters.  She went on the Matterhorn.  I did not.



STEVE:  Wow.  Of course, none of the roller coasters at Disneyland are worth anything.



LEO:  Yeah, they're not that scary.



STEVE:  You need Knott's Berry Farm kind of stuff.



LEO:  Oh, you're a purist, are you?  Okay.



STEVE:  Oh, yeah.



LEO:  You know that the - they used to call it California Screamin'.  It's the Incredicoaster at the California Adventure part of Disneyland in L.A.  Just down the road from you, I might add.  Has a rollercoaster that is a railgun.  Did you know that?



STEVE:  I loved seeing the pictures of it.  You were talking about it somewhere.  And I think that's so cool.



LEO:  It's magnetic conduction.  They have these plates, acceleration.  They have these plates in the car so you get positioned there, instead of, you know, most roller coasters clickety-clack up a steep hill, and you know there's a lot of anticipation.  When we get to the top of the hill, we're going down.  This one's...



STEVE:  I grew up at Santa Cruz, so I know all about...



LEO:  Oh, the worst boardwalk - that thing, that was the thing, that's why I don't go on roller coasters.  I was in high school.



STEVE:  It's rickety wood.  And boy, you just...



LEO:  And the beams are going by you 80 miles an hour, inches from your head.  And that's the first and last roller coaster I ever went on, in high school.



STEVE:  The world has changed a lot.  You used to have Easy-Bake ovens where you could burn your fingers.



LEO:  Yes.



STEVE:  And you used to have chemistry sets where you could actually create chlorine gas.



LEO:  Yes.



STEVE:  I actually did, so I know that.



LEO:  Oh, geez.



STEVE:  But not anymore.



LEO:  Oops.



STEVE:  Now, no, children, you cannot have anything that's actually fun to play with.



LEO:  No.



STEVE:  No actual rockets that work.



LEO:  Yeah, the roller coasters are safer now.  That's the upside, yeah.



STEVE:  So DuckDuckBrowse.  Joining the macOS browser they launched last year, DuckDuckGo now has their Windows browser in public beta.



LEO:  Ah, good.



STEVE:  As we would expect from the privacy-first search folks, the DuckDuckBrowser  and I sure do hope that's not what they're going to actually name the thing  is privacy first.  It sports, and I kid you not, the Duck Player, which is a YouTube player that allows viewing YouTube videos without privacy-invading tracking ads.



LEO:  What?



STEVE:  Yes, and prevents videos viewed from impacting future recommendations.  So they're not letting it track you and profile you.



LEO:  Enjoy it while you can, kids, because that's not going to last.



STEVE:  No.



LEO:  YouTube has already blocked most adblockers on YouTube.  I think it's just a matter of time.



STEVE:  So the Duck Browser may not be long for this world.  They claim that the browser's tracker blocking, which is built in, goes above and beyond what's available from Chrome and other browsers.  They wrote:  "Our third-party Tracker Loading Protection, for example, blocks the hidden trackers from companies like Google and Facebook lurking on other websites before they get a chance to load."  And it's unclear what this means.  They wrote:  "Smarter Encryption to ensure that more of the websites you visit and the links you click are encrypted, relative to other browsers."



LEO:  Okay.  So this was written not by the engineers, but by the marketing department, I'm certain.  That makes no sense.



STEVE:  Yeah, yeah.  I guess what they're saying is they're being more clever about choosing HTTPS alternatives when those are available.  But really that problem's kind of been solved already, so I'm not sure that that's that useful.  Now, okay.  Here's something that might be worth the price of admission,  which being zero admittedly sets the bar rather low.  But they said:  "Cookie Pop-up Management, a tool that automatically selects the most private options available and hides cookie consent pop-ups."



LEO:  Yes.  Yes.



STEVE:  So I would like to have that.  That would be good.  I don't know how they'd do it because how are you going to, like, automatically respond to arbitrary pop-ups and choose the most private one?  I don't know.



Okay.  Now, here's a problem.  They called it the Fire Button, as in lighting a fire.  And they said:  "Burns recent browsing data in one click."



LEO:  Oh, please.



STEVE:  I know.  And there's also the Fireproof option, that's what it's called, for any sites you want to stay logged into.  Now, I suppose if you name your privacy-centric search service DuckDuckGo, then you've already lowered expectations.



LEO:  We know you hate that name.



STEVE:  Oh, my god, about the name you're going to use for other things.  But somehow the idea of a web browser having a "burn bag" into which websites are tossed by pressing the "Fire" button, to light them on fire and reduce them to ashes unless you have "Fireproofed" them ahead of time - I don't know.  Maybe it wasn't the marketing people after all, Leo, because this really seems like it should not have gotten out in the public view.  The browser also offers built-in email protection to hide user email addresses behind uniquely generated @duck.com, because that's what everyone wants to be known as, addresses when signing up online.  Now, while that sounds handy, it also would create some quite powerful lock-in effects, if like all of your logins are some email address @duck.com.  So I'm not sure about that.



Anyway, the beta of the browser, which apparently goes by the catchy name "DuckDuckGo for Windows," is available from, not surprisingly, duckduckgo.com/windows.  And they note that switching is easy since of course like all current browsers it's able to import bookmarks and passwords from other browsers and password managers.  Their announcement had a couple of additional interesting things to say.



They wrote:  "The browser doesn't have extension support yet, but we plan to add it in the future."  And I would say, well, okay, if it survives.  Anyway, they said:  "In the meantime, we've built the browser to include features that meet the same needs as the most popular extensions, ad blocking and secure password management."



So they said, of secure password management:  "Our browser includes our own secure and easy-to-use password manager that can automatically remember and fill in login credentials.  DuckDuckGo for Windows can now also suggest secure passwords for new logins."  Which of course everybody else has already had for a decade.  "This will get even more convenient soon when we roll out private syncing across devices," which, you know, you really can't use this until it has that.  "So you'll be able to sync your bookmarks and saved passwords between different devices, whether you're using a DuckDuckGo browser on Windows, iOS, Android, or Mac."



Okay.  Ad blocking:  "DuckDuckGo for Windows is equipped with our privacy-protecting alternative to ad blockers.  The browser blocks invasive trackers before they load, effectively eliminating ads that rely on creepy tracking."  You know, because, they said, "So many ads work that way, you'll see way fewer ads, if any at all.  We also remove the whitespace left behind by those ads..."



LEO:  Well, that's good.



STEVE:  Yeah, "...for a clean, distraction-free look without the need for an outside ad blocker."  So, yeah, that sounds good.  And finally:  "Duck Player, our browser's more private way to watch YouTube."  They said:  "This built-in video player protects you from tracking cookies and personalized ads with a distraction-free interface that incorporates YouTube's strictest privacy settings for embedded video."



They said:  "In our testing, by blocking the trackers behind personalized ads, Duck Player prevented ads from loading on most videos altogether."  Which again, Leo, I agree with you, like let's see how long this lasts.  "YouTube still logs video views, so it's not completely anonymous.  But none of the videos you watch in Duck Player contribute to your personalized recommendations or your YouTube advertising profile.  You can leave the feature always on, or opt in on individual videos."



And I thought that what was most interesting was that this recently created browser was apparently not simply window dressing surrounding Chromium, which are pretty much everyone else's web browser, including Microsoft's own Edge.  So they explained:  "DuckDuckGo for Windows was built with your privacy, security, and ease of use in mind.  It's not a 'fork' of any other browser code."  All the code...



LEO:  Oh, that's interesting.  Their own engine.



STEVE:  Yes, yes.  Well, kind of.  "All the code, from tab and bookmark management to our new tab page to our password manager, is written by our own engineers.  For web page rendering, the browser uses the underlying operating system rendering API."



LEO:  Oh.



STEVE:  "In this case, it's a Windows WebView2 call that utilizes the Blink rendering engine underneath."  So that's interesting.  On the other hand, what this means is this is all virgin code.  And, like, you know, don't trust it very far, right, because Microsoft abandoned Blink in order to switch to Chromium for Edge.  So DuckDuckGo has come along and said, okay, we're going to use Blink.



LEO:  Is Blink the Internet Explorer engine?  I guess it is.



STEVE:  Well, no, it was the, yeah, the IE11 engine.



LEO:  Oh, my god.



STEVE:  I know.



LEO:  Okay.  What could possibly go wrong?



STEVE:  Exactly.



LEO:  On Apple I presume it uses WebKit, which is a pretty up-to-date standard.



STEVE:  Yes, yes, exactly.  So they finish by saying:  "Our default privacy protections are stronger than what Chrome and most other browsers offer, and our engineers have spent lots of time addressing any privacy issues specific to WebView2, such as ensuring that crash reports are not sent to Microsoft."  Because of course the crash report would also tell, like Microsoft, which URL you had pulled which caused their pressure browser rendering engine to crash, and then so they can go fix it.



So, okay, Leo.  Since Paul Thurrott appears to have an interest in exploring the experiences and features offered by various web browsers, perhaps when the subject of web browsing next comes up, as it probably will tomorrow...



LEO:  Tomorrow, yeah, yeah.



STEVE:  Yeah, just mention DuckDuckGo for Windows.



LEO:  I'll ask him about it, yeah.



STEVE:  And, yeah, see if he wants to go poke at it.



LEO:  He has certainly tried DuckDuckGo on his iPhone and his Macs.  So it won't be unfamiliar to him.



STEVE:  No.



LEO:  And then I think many of us use their search engine.  So, yeah.



STEVE:  Yeah, yeah.



LEO:  All right.



STEVE:  While we're on the subject of browsers, I'll note for the benefit of any of our Tor browser users that version 12.5 has just been released.  It supports a bunch of UI improvements, including a redesigned visualization of the Tor circuit which shows the Tor onion router hops between you and whatever site you're visiting.  Basically, you used to have to go to a separate place in the browser.  Now, in the same way that you can click on the URL bar to like show certificates and things, now in the redesigned UI for the Tor browser you're able to click just ahead of the URL, and it drops down a little window showing you a cute little circuit diagram of you at this IP, and then the first router at this IP, the second router in the chain at this IP, the third router at this IP, and then the site where you're visiting.  So anyway, it's kind of cool.



And finally, one more browser update.  Not long ago, everything was blockchain this and blockchain that.  You know?  Blockchain was the magic pixie dust that was being sprinkled on everything to make it more better.  Today, that role has been taken up by the phrase, which we were talking about at the top of the show...



LEO:  AI.



STEVE:  AI.



LEO:  It really is the blockchain of - this is blockchain, it really is.



STEVE:  Exactly.  Exactly.  I suppose it shouldn't surprise anyone that every other word in Opera's announcement of their "totally rebuilt from the ground up" all new web browser is "AI."  So last Tuesday they posted this:  "Hey, Opera fans!  Today we're excited to drop the big news that Opera One," which is what they're calling it, "the latest incarnation of the Opera Browser, is here and ready for you to download.



"Here's the scoop," they wrote.  "Opera One is your familiar Opera Browser," except as we'll see in a minute it's not.  "But it's been given a major makeover.  And we're not just talking about a new coat of paint.  We've reimagined and rebuilt Opera from the ground up, paving the way for a new era in which AI is not just an add-on, but a core part of your browsing experience.  So what's actually new?  Well, for starters, Opera One is introducing Aria, the first-ever native browser AI.  There's also a totally fresh Modular Design and a bunch of game-changing features like Tab Islands, ingrained within the browser.



Okay, now, I'm not going to spend any more time on this.  And from the comments in the announcement's posting - which was the posting was long, the comments were at least as long.  From what  could tell, this totally new look, feel, and AI were not going over very well with existing Opera users.  And in fairness, big changes always have that risk, right, like this is a completely changed look.  It doesn't even look like a browser.  It's got super roundness, and things are floating around, whatever these Tab Islands are, you know, unfortunately it may be Gilligan's three-hour tour.  It just doesn't look like this thing is going to go.  But anyway, for what it's worth, there are Opera fans out there.  Wanted to let everyone know, Opera One, take it or leave it, it's got AI in it.  I don't know what that means.  But if you're curious, you can find out.



Okay.  As we've reported, the Kremlin in Russia is now moving away as quickly as possible from Western-made smartphones.  And this is like one of those, why did it take them so long?  Because, yeah.  So it only makes sense that they would turn to their own well-regarded Kaspersky for a solution.  To that end, Kaspersky has previewed the first version of their KasperskyOS, a "hack-resistant," we don't know what that means exactly, but good, mobile-targeted operating system that they've been developing for the past several years.



It was demonstrated at a business conference recently held in Saint Petersburg just earlier this month, with the initial version equipped with a bare bones set of basic applications for phone calling, SMS messaging, an address book, and a settings panel.  So again, bare bones.  Kaspersky says it's currently working on adding a Chromium-based web browser and support for a camera, WiFi, and NFC features.  They are looking for a partnership with a hardware smartphone vendor to produce a finished product which will eventually be made available on Russia's internal market.



And I don't have to tell them this because these guys know what they're doing.  But if you want security, you need to hold back on features; right?  I mean, you're not going to be competing with iOS or Android unless you want to just give up on security.  I don't know what hack-resistant means, but keeping this thing to a bare minimum of features is the way to keep it secure.  So it'll be interesting to see how this evolves.  And this would, of course, provide an answer to Russia's need for something more secure than go buy an Android device from a Chinese vendor, which is what they've been saying up to this point.



And while we're on the subject of Russia, the cost of doing web hosting business in Russia just increased.  So I suppose that means that the cost of web hosting to Russian citizens located within Russia will also be increasing as those costs are passed along.  Last Thursday, our favorite Russian Internet watchdog, Roskomnadzor, named the 12 largest and most popular Internet hosting companies who must participate in some new legislation.  I had Google translate Roskomnadzor's announcement from Russian.



According to the legislation, foreign hosting providers whose users are located, among other locations, on the territory of the Russian Federation, are subject to Federal Law No. 236-FZ, which is titled "On the activities of foreign persons on the Internet in the territory of the Russian Federation."  Inclusion in this list of entities imposes obligations on foreign hosting providers to open a branch, a representative office, or some legal Russian entity in Russia, post an electronic feedback form for Russian users on their website, and register an account on the Roskomnadzor website for interaction with local Russian authorities.  Failure to comply with the legislation risks the imposition of fines and even access being blocked to their infrastructure.  And the list is pretty much the Who's Who of Internet hosting:  AWS, DigitalOcean, GoDaddy, HostGator, DreamHost, Bluehost, Hetzner, WP Engine, Network Solutions, IONOS, FastComet, and Kamatera.  So...



LEO:  Everyone.  Is it?



STEVE:  Yes, basically everyone.  Now, I did notice that Azure is not there.  Does Azure do web hosting, or are they just like cloud service stuff?



LEO:  Oh, that's a good question.  Yeah, you could probably run IIS on Azure and serve it.  That's a good question.  I don't know. 



STEVE:  Yeah, they were sort of conspicuously missing.  Anyway...  



LEO:  Nor is Google on there, either.  Oh, you know why?  Because Microsoft and Google both already have offices with humans in them.  And by the way, this is the whole point is so that there is somebody they can arrest... 



STEVE:  Yes, exactly.



LEO:  ...if they don't like what you're doing, and there's actual collateral damage.



STEVE:  Yes, yes.  Some skin in the game.



LEO:  Some skin in the game, literally, yes.



STEVE:  Yeah.  And so these guys are offering their services to Russians inside the Russian Federation without themselves being there.  So, yeah.



LEO:  No, not going to happen.



STEVE:  So slowly turn the wheels of justice. 



LEO:  Oh, yes.



STEVE:  SolarWinds - remember SolarWinds?  Of course we do.



LEO:  We do, yes.



STEVE:  From three years ago.  They've said that some of its current and former executives have received what's known as a "Wells notice" from the U.S. Securities and Exchange Commission for their role of overseer, you know, the SEC is in the role of overseer of publicly traded companies.  The notice in this case is in connection with the company's devastating 2020 security incident, which is of course why we all, and the only reason we all, know the name "SolarWinds."  A Wells notice is a letter that the SEC sends to companies when the agency is planning to bring an enforcement action against them.  SolarWinds says the SEC may fine or bar some executives from serving as officers or directors of public companies.  So, you know, you can't completely hide behind the corporate shield, especially when something this bad happens.



Last Friday, the Senate Armed Services Committee announced that it will be formally exploring the idea of creating a new dedicated Cyber Force branch of the U.S. military.  So, I mean, it'll be standing alongside the Army, the Navy, the Air Force, the Marine Corps, Coast Guard, National Guard.  And of course we have the Space Force.  Now looks like we're on our way to having, like, an official Cyber Force as a branch of the armed services.  To further this, a provision has been added to the 2024 National Defense Authorization Act calling for an assessment of creating such a dedicated Cyber Force branch.



And now, Leo, I have a picture in the show notes here at the bottom of page five which shows this apparently in action.  And what I want to know is why do these photos of U.S. Cyber Defense always show guys with shaved heads - that part I understand.



LEO:  And camo.



STEVE:  They're sitting, yes, exactly, they're sitting in front of their screens and keyboards, dressed up in full camo.  You know?  And is this an attempt to avoid being seen by the webcam?



LEO:  It's actually the opposite of camouflage, if you think about it.  They would be much harder to spot if they were wearing business suits and ties.



STEVE:  Yes.



LEO:  It's pretty obvious that there is something going on here.



STEVE:  And I don't think those outfits are comfortable, are they?  I mean, I don't know.



LEO:  I have to say I've never served our country, and I've never worn them.  I don't know, maybe somebody who is in the service knows.  Can you wear, I mean, do you have to wear the camo in all, every - is this your uniform that you wear everywhere?



STEVE:  And what I don't see is a Post-it note.  If they just used a yellow Post-it note over their webcam...



LEO:  That keyboard looks pretty good.  That's like a good keyboard.



STEVE:  It does look like a nice keyboard.



LEO:  That's a fancy - those are fancy switches.



STEVE:  Although look at the wire, like, is it stuck up in front of the display?  I think the whole photo was a setup, Leo.



LEO:  Staged.  Hey, you've got to get rid of that Logitech Bluetooth keyboard.  Here, use this.



STEVE:  But definitely dress up in camo because we want to show that you went through boot camp in order to boot your computer.



LEO:  Yeah.



STEVE:  I don't know.



LEO:  Yeah.



STEVE:  Doesn't make any sense to me.



LEO:  Yeah.  ReverbMike says they're comfortable, these BDUs, and khaki's comfortable.  He wore them everywhere.  So there you go.  There you go.



STEVE:  Good to know.  And I wonder if they actually do wear them, like in these cyber...



LEO:  I don't - I feel like they wear black T-shirts that say you know, like death metal bands on them.



STEVE:  Yes, exactly.



LEO:  But I might be wrong.



STEVE:  Like "Boot You" or whatever. 



LEO:  Boot me.



STEVE:  Okay.  Just a quick note that Apple has added Passkeys support for logging into Apple.com.  You will need to wait for the formal release of iOS 17, iPadOS 17, or macOS Sonoma to be able to do that, or be using a beta.  But for what it's worth that support is there now.  I suppose that other Passkeys clients should also work now, as well.  So if you're looking for somewhere to log in, you can do that at Apple.



Okay, now, here's a bit of sadness that actually we'll be coming back to at the end of the podcast.  Several European governments, specifically the French, German, and Dutch officials, are pushing the EU to add an exemption in its upcoming European Media Freedom Act (EMFA) which would explicitly - I can hardly say, I can hardly believe this - explicitly allow EU member states to continue spying on the electronic communications of journalists under the guise of "national security."  The push follows the results of the EU's own PEGA commission, which advised the EU to head in the opposite direction by adding additional safeguards to protect democracy and the rule of law in the EU against the abuse of spyware tools.



In PEGA's report published last year, the commission said several EU countries were abusing surveillance technologies to illegally spy on their own citizens, including journalists, under murky and vague "national security" justifications.  More than 60 journalistic organizations and civil society groups have signed a joint letter to the EU Council advising against weakening the upcoming law and giving governments an explicit spying carte blanche.  So, yeah.  Apparently everyone else gets constrained by the GDPR and all that it brings, but the governments themselves which are behind the GDPR are seeking to legislate a loophole to allow themselves to use spyware, which of course is in itself, let's not forget, illegal malicious software.  Unbelievable.



Okay, now, it may be obvious to everyone, but I think it's still worth reminding everyone that just because Apple did a beautiful job and got the whole facial recognition challenge correct, that fact should in no way confer any presumption that anyone else did the same.  A recent study updated an earlier study from four years ago.  Both concluded that with the sole exceptions of Apple and Samsung, the phrase "smartphone facial recognition security" is an oxymoron.



The updated research conducted by a Dutch consumer protection association found that facial recognition systems on most of today's mid- to upper-tier smartphones, which is to say the only smartphones that have any, can be bypassed using a simple two-dimensional photograph.  The researchers bypassed facial recognition on 26 different smartphone models by showing photos of the owner to their phones.  Only Apple and Samsung devices were found to be secure.  Researchers were unable to bypass facial recognition on any of Apple's iPhones, and only one out of 12 Samsung models failed the same test.  Fourteen of the 26 smartphones that failed the test were Xiaomi models.  Among the failures were Motorola Motos, Nokias, a OnePlus, two OPPOs, and one Samsung Galaxy.  That one Samsung was a Samsung Galaxy A04s.  And then all the rest were just like all of these Xiaomi phones.



Now, of course we'll all remember, because we were all here on the podcast, when Apple first unveiled their facial recognition.  The first thing that naturally occurred to all of us was to wonder how easily their technology could be spoofed.  What we learned was that the phone projects a scanning dotted grid, an IR grid, which is viewed by offset cameras to determine whether what's being presented to it matches the model of the 3D face that was created and mapped when the phone's user was first presented to it and deliberately moved around to register themselves and create that map.  While that system, which is quite sophisticated, can be spoofed by creating 3D replicas of the user's face, no simple-to-create flat photo will do the job.



So I just wanted to remind everyone that, again, just because Apple went to the extreme measures to create a highly spoof-resistant facial recognition and unlocking technology, no one should assume that anyone else who offers facial recognition unlocking also took the time to get it right.  Apparently, no one but Apple and Samsung did.  Since getting it wrong is so much easier to do, that's what's typically done. And it seemed to me that the danger is that facial unlocking would have started off with a great reputation of being secure, and that other manufacturers would just be riding Apple's coattails by saying, yeah, we've got it, too.  Look, you can look at your phone and unlock it.  Well, yes.  And apparently you can show it a photo from the Internet and unlock the phone just as well.  So just a caution that maybe you want to actually do that if you have a non-Apple or Samsung phone to see how secure that unlocking really is.



LEO:  I wonder if that includes Google?  Or does Google not have face ID?  I guess Google doesn't.



STEVE:  I think they're big on thumbprints.



LEO:  Yeah, yeah, yeah, yeah.



STEVE:  Okay.  Google, speaking of Google, has committed more than $20 million to the creation of cybersecurity clinics at 20 higher education institutions across the U.S.  The clinics will provide free cybersecurity training and hands-on experience for thousands of students.  Some Google employees will serve as mentors and trainers at some of the clinics.  Google will also provide free scholarships to allow some students to attend its Cybersecurity Certificate program.  In part of this announcement, Google said:  "These clinics provide free security services in the same way law or medical schools offer free clinics in their communities.  They give students the opportunity to learn and improve their skills, while helping to protect critical infrastructure such as hospitals, schools, and energy grids."



Now, this sounds like a great idea, though I'll admit that the cynic in me wonders whether this might not also be a terrific means for recruiting talent from those institutions; you know?  Not that there's anything at all wrong with doing so.  After all, the reason those students are there is to acquire the knowledge and skills necessary to find gainful employment.  So getting a head start with Google might be a way to do that.



Okay.  So now, finally, I suppose it was inevitable that the subject of last week's "Massive MOVEit Maelstrom," which was last week's title, I suppose it was inevitable that Progress Software would soon be facing lawsuits because the damage that occurred was astonishing.  And, sure enough, at least two federal class action lawsuits have been filed so far in connection with this devastating SQL injection vulnerability which was discovered and widely exploited in their software, which of course we covered in detail last week.  The lawsuits allege that it was the company's negligence which led to the breach, thus putting their personal financial data, that is, all of the individuals who are bringing these lawsuits, at risk.



The first suit, which was filed on June 15th in U.S. District Court for the Eastern District of Louisiana, alleges that the vulnerability led to the breach of the state Office of Motor Vehicles, which as far as we know it did.  Louisiana said that their Office of Motor Vehicles statewide was completely, all of the personal data was exposed.  They announced the breach the same day, warning all, that is, Louisiana state, warning all Louisiana motor vehicle drivers that their names, addresses, dates of birth, driver's license numbers, Social Security numbers, and vehicle registrations, and any other information that they had was likely stolen.  You know, pretty much the whole enchilada.  About six million records were exposed and likely stolen.



The plaintiff in the first case, Orleans Parish resident Jason Berry, alleges that his personal data was put at risk by the breach.  He alleges that the company also failed to promptly notify potential victims of the risk of exposing their personal information.  The suit seeks class action status for others impacted by the breach.  Now, I'll just note that that's nonsense because he brought the suit the same day that Louisiana announced the problem.  So how could Progress Software have known that this was the case until Louisiana said, yup, we were hit by this.  So I don't think this stands much chance of going anywhere.  And you and I, Leo, are both not big fans of class actions because mostly that just seems like a way to enrich attorneys.



As we were recording last week's podcast on this topic, the second case was being filed in the U.S. District Court for the District of Massachusetts on behalf of also Louisiana.  Three Louisiana residents, Shavonne Diggs and Brady and Christina Bradberry, brought that class.  The class exceeds 100 people, and the plaintiffs are seeking upwards of $5 million.  Now, is that for the whole class, or individually?  That wasn't clear.  But this is according to the complaint.  The second Massachusetts case alleges that Progress Software failed to adhere to Federal Trade Commission guidelines for data security, failed to protect customer data, and failed to properly monitor its own internal systems.



Okay, except that's not the nature of the breach that occurred.  And, you know, I don't have any opinion more or less about this one way or the other, that is, in terms of like from the legal standpoint.  One issue may be that the plaintiffs need to be more than just upset over the news of this happening.  At this point they may just be chasing ambulances.  I suspect that they need to demonstrate that they have been individually and collectively damaged by the breach, and that may not be easy.  Remember as we talked about last week, the Cl0p gang, who are Russian extortionists, did say that they wanted nothing to do with government, educational, or police agencies, and that any data obtained from any of them would be immediately deleted.  So Louisiana is certainly a government agency, as opposed to a private enterprise.  So I hope that Progress Software's attorneys are up to speed on that and maybe saying, look, as far as we know, there's no danger here.



Everyone knows quite well that I have no sympathy whatsoever for anyone who designs web server software in such a way that it feeds any user-provided text to a backend SQL database which stupidly mixes commands and query text into the same text stream.  Anyone who is still doing that 25 years after it was first observed to be a really bad idea, and with it being consistently the top vulnerability on OWASP's top 10 list of really bad ideas, is probably going to get what they deserve.



But we don't know in sufficient detail how this happened.  Remember that back in November of 2015, when Marriott International acquired Starwood Hotels & Resorts, the Marriott execs didn't know that Starwood's network was hosting some serious security vulnerabilities.  And three years later, in September of 2018, that oversight came back to bite them hard.  Should Marriott have done an in-depth security verification?  Yes.  And perhaps they did.  We don't know.  If vulnerabilities were not extremely difficult to find, they would all be eliminated before software was ever shipped, and the entire bug bounty industry and Pwn2Own competitions would not exist.  But the fact that bug bounty hunting can be a profession these days, and Pwn2Own is full of previously unknown vulnerability discoveries, it just demonstrates that these things are hard to find.



So in this case of MOVEit and Progress Software, I don't feel any sense of schadenfreude.  This is a tragedy all around where everyone has lost.  Our listeners know that I always completely separate mistakes from policies.  So my only argument here is that the use of SQL in this way, in any way that opens the door for injection, is a policy decision.  It was a mistake that this policy was not implemented perfectly.  But if this database architecture policy had not been used at all in the first place, then there would have been no reliance upon the filtering code needing to be perfect.  And apparently some imperfections were found and exploited.



So it'll be interesting to see over time what happens with this.  You know, lawsuits are unfortunate.  We're in an industry where - and Leo, I think it was on one of the other podcasts I heard somebody lamenting the bizarre fact of the hold harmless clauses in software licensing.



LEO:  Oh.  Oh, yeah.  Yeah, yeah.  We've talked about that a lot with Cathy Gellis and others on TWiG, yeah.  Yeah, yeah.  It's gone, I think, yeah.



STEVE:  It is an anomaly for this industry.



LEO:  Basically and we've all read it, if you ever read the EULAs.  We warrant no representation that this software will do anything it's supposed to do.  We are not responsible for anything it does wrong.  It's your problem if it does it wrong.  We're not liable.  And it's actually coming up because of self-driving vehicles.  That's the latest iteration of this is who's responsible if a self-driving vehicle kills you?  Isn't it the maker of the software?  And so I think this is going to end up getting rid of the hold harmless clause.  President Biden put out a, I don't know, it doesn't have the force of law, but put out a kind of future of technology thing in which they say we don't want these clauses that prevent liability.  We want to override them.  So I think it's an agenda of the White House, at least, yeah.



STEVE:  I mean, so it's a problem because it would be difficult to publish software if anyone could sue you if they were not happy with what the software did.  And, I mean, there are clauses in there that say our entire liability is to refund the purchase price.  Except that, you know, giving you your money back for the car that, you know...



LEO:  Yeah, it's not going to do it.  Yeah, yeah, that's not going to do it.



STEVE:  ...that plowed into a crowded group of people, that's not going to work, is it.



LEO:  I mean, this is how the court system works now.  You can sue anybody for anything.  Suing just means I'm going to court. 



STEVE:  Right.



LEO:  And the good news is, I would hope in most cases, that judges will throw out frivolous and stupid suits, but maintain suits that have merit.



STEVE:  And because the attorneys know that, they won't even take up a case when they know that the judge - it was not going to get past square one.



LEO:  And there are some states that have SLAPP laws, which I think are probably a good idea, which if it is a frivolous lawsuit and found to be, then the person who brought the lawsuit is liable for costs.  And those are effective as a deterrent, as well.  But I do agree, and we were talking about this on Sunday with Alex Lindsay, maybe this is what you're remembering, his dad is a trial lawyer.  I do agree that that's one of the important ways people can hold these big tech companies accountable is suing them.



STEVE:  Yeah.  So David Scholten, he sent me a tweet.  He said:  "@SGgrc I have loved listening to Security Now! over the last 10-plus years, and I believe it has helped me greatly in my IT career, from technician to IT admin.  Now, I have a non-IT question.  Is it just me, or have I been hearing a fire alarm low battery beep in the background in several podcasts?"



LEO:  Uh-oh.  I haven't heard that.



STEVE:  David, thank you.  I'm glad you haven't, Leo.  Many of our listeners have.



LEO:  Last week?



STEVE:  David, I wish it was your imagination.  No, it's been going for several months.



LEO:  You're kidding.  You can't find it?



STEVE:  I'm not kidding.  I cannot find it.  Something in my environment started beeping occasionally many weeks ago, and I have no idea what it is or where it is.



LEO:  So frustrating.



STEVE:  It's not any of my smoke detectors.  And the room it's in is full of equipment, so there are a great many places it might be.  Since it began, I've embarked on several missions to locate and find it.



LEO:  Oh, my god.



STEVE:  But the chirp is so short that I don't get enough of a sample to obtain a bearing.



LEO:  And plus it's a high frequency, so it's hard to figure out.  You know what?  You wrote, years ago as a youth, you did the Portable Dog Killer.  I think you need to write something, make something called Chirp Finder.



STEVE:  A high-frequency sound locator.



LEO:  Yeah.  Yeah.  Yeah.  Because it has to record it and do it instantaneously.  You could do this, Steve.  It's not too late.



STEVE:  Fortunately, SpinRite is almost finished.



LEO:  This can be your next project.



STEVE:  Yeah, it's...



LEO:  Chirp Finder.



STEVE:  So I tend to tune it out.  I really don't hear it that much.  But, and I was self-conscious about the podcast, but since the Heil microphone is pointing away from the room where it's happening, I thought, okay, it's probably not going to be very much.



LEO:  I remember hearing it several months ago.  But I haven't heard it lately.



STEVE:  It's been going on nonstop.  And I'm waiting for the battery to die.



LEO:  Oh, my god.



STEVE:  So far that hasn't happened.



LEO:  Oh, my goodness.



STEVE:  Anyway.  And it's funny, too, because since I'm unable to determine the bearing, I'll wait for it to happen.  Then I'll go stand over near where I think it is.



LEO:  Yes.  Everybody knows this look.



STEVE:  And wait again.



LEO:  We're all waiting for that.  Oh, I hate that.  I hate it.  And of course it always happens in the middle of the night; right?  There is a great - there's a TV show you probably haven't seen, I think it's on HBO, with the guy who was in "House."  I can't remember his name.  But he's the captain of a...



STEVE:  Hugh Laurie.



LEO:  Hugh Laurie is the captain of a - it's a science-fiction comedy of a cruise ship, space cruise ship.  It's called "Avenue 5."  And there's one whole episode devoted to a beep.  And it beeps.  It doesn't beep consistently.  It beeps at random intervals.  No one can sleep.  Some people laugh every time it beeps.  Some people hunch over.  It's actually a very funny episode, if you get a chance to see it.  I'll find the episode number.  Actually, I watched the whole thing.  I thought it was pretty good.



STEVE:  "Avenue 5."



LEO:  "Avenue 5."



STEVE:  I like Hugh Laurie a lot.  It's freaky that he has an English accent because none of that shows when he's playing Dr. House.



LEO:  Oh, it's a big part of the show.  There's a lot of tongue-in-cheek.  It's actually very funny.  It's the guy who did "Veep."  He's very talented.  Armando Iannucci.  But it just missed slightly, and I guess it was canceled.  But it was good.  Anyway, look for that.



STEVE:  I'll track it down.



LEO:  You need a Chirp Finder of some kind.



STEVE:  I know.  And I did find myself thinking, how can - somebody must have done this already.  Like there must be, like,  if you had two phones that were in communication...



LEO:  Oh, so you could triangulate it, you mean.



STEVE:  Yes.  You could use time of arrival in order to determine where it was.  Anyway.  Don't know.  I'll eventually find it.



Fabian Santiago said:  "I'm still sore for and with you about SQRL vs. Passkeys, et cetera.  It does warm my heart to see the SQRL iOS Testflight Client App still receiving updates, though."  He said:  "Just today for me."



So I wanted to take this opportunity to give Jeff Arthur, SQRL's iOS client author, a shout-out and a thanks for his continuing work on SQRL.  I know that it's been a labor of love for him, and it would be terrific if something were to ever come of it.  If FIDO2/WebAuthn and Passkeys evolves to require elliptic curve crypto as one of its available crypto suite options, that would immediately enable the use of SQRL-style deterministic, rather than random, private keys.  And that would in turn mean that all of the other work that has been done on SQRL to solve all of the other problems that today's Passkeys clients still have, would be immediately available, too.  So we'll see how this evolves. All may not be lost.  But I'll be on to SpinRite 7 and beyond by that point.



LEO:  And of course Chirp Finder.  Very important.  You've got to get to work on that, Steve.  There's a lot of people out there who would appreciate it.



STEVE:  It's got to be, to have an opportunity to track that down.  Okay.  So David R. Bunting, he's tweeted me:  "Jungle Disk.  Do you still recommend it?  Thanks, Steve."



Okay.  So no.  Jungle Disk, for those listeners who haven't been around since the beginning, was one of our very early and very good TNO, as in Trust No One, client-side encrypted cloud storage solutions.  They were purchased some time ago by something called "CyberFortress," and those guys probably wear camo, too.  And it appears that they've completely gone corporate.  So it's unfortunate they got...



LEO:  It's not free anymore, and it's - yeah.



STEVE:  Yeah.  They got gobbled up, essentially.



LEO:  Such a cool product.



STEVE:  Yeah, it was.  It was great back then.  So today my number one favorite choice and recommendation is the Canadian firm and service SYNC.com.  You can get 5GB for free to see how you like it, or you can use my referral code to start off with 6GB, so you get an extra gigabyte for free.  And that's just grc.sc/sync (S-Y-N-C).  So grc.sc/sync.  All you have to do is create a username and password, no credit card required or anything else.  So it's really absolutely free.  Now, I've been using them since, I checked, August 7th of 2019, so we're approaching four years.  And in my opinion they are a total win.  The only downside is that they don't support Linux.  And although they know that there's a demand for it, especially from our listeners, unfortunately the demand for Linux is dwarfed by the interest in Windows and Mac, both which they do support.  So there's no sign that Linux is coming.



What I like most about SYNC is that it's probably the right solution for most people because it just works.  When it's installed, it creates a SYNC folder in the system's directory tree, and anything that's placed under there is, whether it's folders or files or a complex tree, anything, is kept fully and immediately backed up to the cloud.  And it is TNO.  It's locally encrypted with all the bells and whistles you would like.  You can ask for a link if you want to share a file, and it will be locally decrypted on the recipient's browser.  I mean, they really did this correctly.



If you've got multiple machines, all of their SYNC directories are kept fully cross-synchronized through the cloud.  And all this is done with deep versioning so that you're able to go back to previous versions from between six months and a year.  Using their web interface, you're able to browse back in time to retrieve something, even files you have previously deleted.  It's also zero configuration about how often you want to sync.  It just syncs everything all the time.  Using the Windows tray utility, it's possible to select things you may not want to sync for some reason which are within the tree underneath your SYNC folder, so there's some optional flexibility there.



When I was deep into SpinRite work, all of my code and management scripts assumed that the ASM directory that I use as the root of all my assembly code was at the C: drive's root.  But to have it all backed up to the cloud and synchronized between machines, which is what I really wanted because I have two locations, it had to be underneath the SYNC directory.  So what I did was I moved the \ASM directory under the SYNC directory.  Then I created a Windows NTFS junction link so that an apparent \ASM directory on the root would be aliased to the ASM directory under the SYNC directory.



So nothing needed to change.  Everything that's in my code and scripts and everything still referenced \ASM so that everything worked, even though it was actually over in the SYNC directory.  And it all worked perfectly.  And I have to say there have been several times when the fully automatic detention of previous file versions has come in very handy.



So once again, if you're interested in their free trial, you can use grc.sc/sync, which will bounce you over to them with my affiliate code appended to start you off with an extra gig for a total of six.  And then, you know, if you like it, and I just checked, their basic personal plan is $8 a month, which buys you 2TB of storage, and you're able to increase that as needed.



LEO:  Did you stop using Syncthing?



STEVE:  No.



LEO:  Okay.  Because that would - wouldn't that do what you want to do with your ASM folder?



STEVE:  It would, although it wouldn't give me the additional level of cloud backup.



LEO:  Cloud storage, yeah, yeah.



STEVE:  Because Syncthing is purely peer to peer.



LEO:  Right.



STEVE:  And in fact I have mentioned that in my show notes.  I wanted to mention one other thing.  So anyway, so SYNC.com is my current and well-proven, I've been using it for four years, recommendation for a simple-to-use, foolproof cloud storage solution if you don't need Linux clients.  Now, I should note, and Leo we've talked about this a little bit, that while I am still using SYNC for many things, I have since switched to using a pair of cross-synchronized Synology NAS boxes.  And I am so impressed by Synology.  Boy, you know, every contact I have with it I just think, well, these guys got it right.



So I'm using a very nice free Windows utility called @MAX SyncUp, @MAX SyncUp.  That synchronizes my various directories on my Windows machine to the local Synology NAS.  And then the Synology NASes are - they use whatever they have that is built in in order to mirror each other at my two locations.  So basically I've kind of created my own little personal cloud system using two Synology NASes.  But I did want to mention to people this @MAX SyncUp because it is a beautiful Windows solution for producing synchronization to local shares.  It will also sync to Google Drive.  And it's been independently reviewed by a bunch of stuff.  You can find them online.  And it gets, you know, all the stars.  



And before we leave the discussion I wanted to mention Syncthing.  I'm glad you made sure I would not forget it, Leo.  It is a terrific peer-to-peer cross-platform solution that is quite happy with Linux.  I still have Syncthing running on a surviving Drobo, which is Linux-based.  And that instance of Syncthing is keeping my wife's fleet of remote Windows laptops synchronized out in the field.  And it really is a terrific peer-to-peer solution.  And I know, Leo, that you have gotten up to speed on it and like it a lot.



LEO:  Well, I've been using it for years.  That's really my only backup solution.  I put Syncthing on every computer.



STEVE:  Yup.



LEO:  But, and maybe you didn't know this, there is a third-party Syncthing app for Synology.  So I make Synology my master Syncthing, and I make sure it's for read-only.  It doesn't delete.  It only reads.  It receives only.  So that way you don't have any accidental deletions.  It records everything I've ever had.  And then I do the same thing as you do with Synology.  I have a dual Synology setup, one at home and one here.  So that gives me off-site.  That gives me my cloud.



STEVE:  Redundancy.



LEO:  Yeah, none of my stuff is ever in a cloud, but it is redundant, and it's offsite, and it's all done with Syncthing.  And Syncthing is Windows, Mac, Linux.  And that, honestly, I think SYNC.com is great.  I just, without a Linux client, it's not going to help me, yeah.



STEVE:  Right.  Do you know if the Syncthing for Synology requires a container?  I thought that it was only...



LEO:  No.  There's two.  There are several.



STEVE:  I didn't think it was native.



LEO:  There is a docker Syncthing.  But there's a native Syncthing.  It's a third-party app, so you have to enable a  third-party app store.  I think it's from the Syncthing folks.  And it runs, not in a - you do obviously have to have some additional software.  I think Note has to be running or something.  But then it runs, and it does - and so the key with Syncthing, which it took me a while to figure out, is...



STEVE:  I know.  It is funky.



LEO:  Well, you have to only let one - start from one place.  And that will be the default folder name because it uses a long obscure GUID for each folder name.  Let the master, whatever that is, could be your Synology if everything's there, be the name of that, that GUID of that folder; and then make that be the one that introduces to everybody else, and let everybody else say, yeah, I'll take that, I'll take that, I'll take that.  My mistake sometimes was creating a documents folder on two different machines.  Then you have two documents folders because the GUIDs are different.  And so it's better to say this is the canonical documents folder.  Let that replicate to everything.



And once I figured that out, it's been working flawlessly ever since.  There are a few little things, you know.  If it can't copy something, it looks like there's been an error, and it's just, you know, it's just being cautious, just letting you know.  And I have seen some people say it's accidentally deleted everything, which it could in theory because any time you synchronize that could happen.  That's why it's good to have a read-only copy somewhere.  Synology is probably the best one for that.  And Syncthing's free, open source.  I love Syncthing.  It's just amazing, really.



STEVE:  Agreed.  Okay.  I thought I'd said I thought I'd said all I had to say about SpinRite.



LEO:  It's done.  Steve, it's done.



STEVE:  Well, I caught up with my Twitter feed yesterday and found a very heartwarming pair of tweets from someone whose name is crzy8ers.  So crzy8ers tweeted:  "Had a catastrophic hard drive failure.  All my finished photos were ready for print.  Thought my memories were lost forever until someone on Twitter recommended this software, SpinRite by Mr. Steve Gibson @SGgrc."  He says:  "Here is his website to find SpinRite Data Recovery."



And that tweet included apparently one of his photos, which is beautiful.  And then in a follow-up he said:  "Don't know how else I can thank you for your amazing Hard Drive Data Recover Software.  When I print my next photo book, I'm going to send you a copy.  Thank you."



And so he's not a Security Now! follower.  He didn't know me from Adam.  But someone said, oops, get a copy of SpinRite.  And he did, and he ran it, and he got all of his photos back.  And so there's that one, and then there's also one of a bear cub up in a tree.  And I was thinking, boy, it's dangerous to take pictures of bear cubs, but maybe he had a long lens, and so he was actually in a different state.



Anyway, I'm hoping that SpinRite 6.1's ability to once again run on drives of truly any size, with any format file system, and in a reasonable amount of time, will help to dispel the lingering misperception that SpinRite's day has come and gone.  You know, here's fresh proof that SpinRite is still alive and well.  And his recovery was done with 6.0.  So there's more goodness coming soon.  During all of the testing that we've been doing, many of us are watching SpinRite recovering sectors of data just as well today as it ever has, if not perhaps a bit more so, since modern drives have pushed the data storage envelope even further.  And  I'll just say that I have a few surprises up my sleeve for v7, which is why I'm already committing to that'll be the next thing I work on.



Okay.  So three weeks ago, while covering the week's news for Episode 926, which was our "Windows Platform Binary Table" topic, we touched on Kaspersky's discovery earlier in the week of something unknown, which was apparently generating unexpected network traffic, which they had just found crawling around in their network.  And the unknown traffic appeared to be originating from some of their iPhones.  At the time I quoted them saying:  "The malicious toolset does not support persistence, most likely due to the limitations of the OS.  The timelines of multiple devices indicate that they may be reinfected after rebooting.  The oldest traces of infection that we discovered happened in 2019."  Thus four years.  "As of the time of writing in June 2023, the attack is ongoing, and the most recent version of the devices successfully targeted is iOS 15.7."  Now, that 15.7 turns out to be a clue that we'll get back to at the end.



Okay.  So recall that they were examining iPhone backups to detect traces of this infection.  And they had named this still-unknown malware campaign "Operation Triangulation."  That being the title of today's podcast, you might expect that we're returning to this because they now know a lot more than they did then.  And their knowing a lot more coincides with the need all iOS, iPadOS, macOS, and watchOS users had to restart their devices last Wednesday when Apple pushed out a raft of emergency updates in response to what Kaspersky discovered.



Okay.  So what did Kaspersky discover?  They used mobile device backups to look at partial snapshots of those devices' file systems.  And from what they determined, there's this sequence of events:  The target iOS device receives a message via the iMessage service with an attachment containing an exploit.  Without any user interaction, thus zero-click, the message triggers a vulnerability that leads to code execution.  The code within the exploit downloads several subsequent stages from the command-and-control server, and that includes additional exploits for privilege escalation.  After successful exploitation, a final payload is downloaded from the command-and-control server that's a fully-featured APT, Advanced Persistent Threat, platform.  The initial message and the exploit in the attachment are deleted.



So they explained that at the network level, a successful exploitation attempt can be identified by a sequence of several HTTPS connection events.  They said:  "Legitimate network interaction with the iMessage service, usually using the domain names *.ess.apple.com.  Then download of the iMessage attachment, using the domain names .icloud-content.com and content.icloud.com."  So they're able to see those interactions.  Regular non-malware iMessage attachments will do the same thing.



Then:  "Multiple connections to the command-and-control domains, usually two different domains."  And I'll share a list here in a second.  "Typically netflow data for the command-and-control sessions will show network sessions with significant amount of outgoing traffic."  So a lot of data flowing out from the phone.  And that makes it a little unusual.  "The iMessage attachment is encrypted and downloaded over HTTPS.  The only implicit indicator that can be used is the amount of downloaded data is about 242Kb."  So basically they're reduced, as we can see, to relying upon metadata since they have no visibility into the phone.



Then they said:  "Using the forensic artifacts, it was possible to identify the set of domain names used by the exploits and further malicious stages.  They can be used to check the DNS logs for historical information, and to identify the devices currently running the malware."  That is, you know, so if you look at DNS logs, depending upon how far back you have them, you will spot DNS lookups to these domains which you now are able to associate with this active malware today.  And based on which devices you have which are today generating DNS queries to those domains, you now can determine which of your iOS devices are currently infected.  So those domains are addatamarket[.]net, backuprabbit[.]com, businessvideonews[.]com, cloudsponcer[.]com, datamarketplace[.]net, mobilegamerstats[.]com, snoweeanalytics[.]com, tagclick-cdn[.]com, topographyupdates[.]com, unlimitedteacup[.]com, virtuallaughing[.]com, web-trackers[.]com, growthtransport[.]com, and then anstv[.]net and ans7tv[.]net.



So they're obviously meant to appear kind of like benign, generic, like if you saw that happening you'd go, okay, you know, web-trackers.com, of course.  Tagclick-cdn.com, yeah.  And like, okay.  Virtuallaughing.com?  Well, who knows.  But okay.  So again, wouldn't raise any red flags necessarily, and especially when you consider all the other, like, you know, remember that a website, when you load it now, has hundreds of other DNS lookups that are occurring.  So this would just get lost in the noise.  



So essentially they are unable to see into their iOS devices, which they're sitting here like they're holding them, and they know they're infected with malware because they've been able to see what's going on.  All they're able to see is the metadata traces of what these devices are doing.  And they're able to get additional metadata from examining iPhone backups and from the these DNS lookups that they're able to intercept.  So as I noted before, this whole process of iPhone security serves as a double-edged sword.  It attempts to prevent malware from gaining a foothold into the device.  But it just as strongly prevents legitimate researchers from gaining a foothold to understanding any malware that does manage to get into a device.



And one of the distressing and growing trends we're witnessing is that these incursions are not arising from some black hat bad guys wanting to sneak into our devices.  The driving forces here appear to be legitimate democracies - well, and in some cases autocracies, but even democracies such as those in France, Germany, and the Netherlands.  And those are the only ones who have raised their hands to ask whether this could please be made less illegal and unofficially sanctioned.  We know that more traditionally repressive regimes are also doing the same without asking for anyone's permission.



So my point is, the more we learn about the increasing pressure to subvert the privacy of our personal communications devices, predominantly coming from the world's governing bodies, the more happy I'm becoming that Apple has been steadfastly working in this direction from the beginning; you know?  There was a time, maybe 10 years ago, when all this effort that Apple was putting into this seemed a bit like overkill.  Well, I no longer think that.



Unfortunately, we're still talking about this today because they haven't yet succeeded in getting it 100% buttoned down.  And it's not even clear that it's going to be possible.  While we're still using our current hardware architectures and our current software models, all the evidence suggests that new critical bugs are being introduced at about the same pace as old bugs are being found and eliminated.  Windows is certainly showing no signs of running out of bugs to patch; and nor, unfortunately, is iOS.  While it's true that iOS may have many fewer of them per month, it only ever takes one.



Okay.  So back to Kaspersky.  In their pursuit of this malware over the past three weeks, they've posted a series of updates, their most recent one being last Wednesday, coinciding with Apple's release of patches for the zero-day, zero-click problems Kaspersky has uncovered.



So Kaspersky wrote:  "Over the years, there have been multiple cases when iOS devices were infected with targeted spyware such as Pegasus, Predator, Reign and others.  Often, the process of infecting a device involves launching a chain of different exploits, for example, for escaping the iMessage sandbox while processing a malicious attachment, and for then getting root privileges through a vulnerability in the kernel.  Due to this granularity, discovering one exploit in the chain often does not result in retrieving the rest of the chain and obtaining the final spyware payload.



"For example, in 2021, analysis of iTunes backups helped to discover an attachment containing the FORCEDENTRY exploit.  However, during post-exploitation, the malicious code downloaded a payload from a remote server that was not accessible at the time of analysis.  Consequently, the analysts lost the ability to follow the exploit.



"In researching Operation Triangulation, we set ourselves the goal to retrieve as many parts of the exploitation chain as possible.  It took about half a year to accomplish this goal.  And after the collection of the chain had been completed, we started an in-depth analysis of the discovered stages.  As of now, we have finished analyzing the spyware implant and are ready to share the details."



Their comment about this taking them half a year took me by surprise.  I had assumed that when said they had caught this malware in their network, they meant a week or two before.  But they apparently meant half a year ago, and that they've only recently been making the results of this ongoing research public.  And now in retrospect that does make more sense, since what they were revealing is far more than a week's worth of effort at reverse engineering.



So they said:  "The implant, which we dubbed TriangleDB, is deployed after the attackers obtain root privileges on the target iOS device by exploiting a kernel vulnerability."  Okay.  So what I believe is that they found that kernel vulnerability, told Apple about it, and that's what got fixed.  That corresponds with the CSV that I'll be wrapping up with here in a second.  It doesn't look like they have yet found the iMessage sandbox escape, nor the transient attachment which is what gets in there, talks to the command-and-control server, and then downloads the final Advanced Persistent Threat.



What they finally got, probably by intercepting the TLS communications, setting up a TLS interception proxy, and then using that in order to decrypt the HTTPS transaction when one of their infected devices reached out to the command-and-control server to download this final piece, they were able to obtain the final piece and then reverse engineer it.  And that's what they're now talking about today.



Being able to foreclose the kernel vulnerability might stall this, but it means that we still have the other parts of the attack chain that, as far as we know, they're saying they have not yet been able to obtain.



So they said:  "It's deployed in memory, meaning that all traces of the implant are lost when the device gets rebooted."  That's what we have known from what they said before.  "Therefore, if the victim reboots their device, the attackers have to reinfect it by sending an iMessage with a malicious attachment, thus launching the whole exploitation chain again.  In case no reboot occurs, the implant uninstalls itself after 30 days, unless this period is extended by the attackers.  The TriangleDB implant is coded using Objective-C, a programming language that preserves names of members and methods assigned by the developer.  In the implant's binary, method names are" - yes, yes.



LEO:  You don't need a symbol table.  You've got them.  They're built in.



STEVE:  Yes.  "Method names are not obfuscated; however, names of class members are uninformative acronyms, which makes it difficult to guess their meaning."  Okay.  So in other words, exactly as you said, Leo, a huge aid to anyone wishing to reverse-engineer Objective-C code.  The names, and thus the purpose and intentions, of the code routines remain visible.  But in this case the names of the variable parameters they are exchanging are not useful.  Examples of method names which they found are populateWithFieldsMacOSOnly, populateWithSysInfo, getCInfoForDump, unmungeHexString, and getBuildArchitecture.



So having those names is far more useful than unnamed hexadecimal address offsets which is all that's generally available from any language that compiles all the way down to native machine code after any space-wasting symbols have been removed.  Although the variable names that were contained in the exploit code are far less useful, they noted that in many cases it's possible to guess what their acronym names mean from context.  For example, osV is the iOS version, and iME contains the device's IMEI. 



Anyway, they continue to explain:  "Once the implant launches, it starts communicating with the command-and-control server, using the Protobuf library for exchanging data.  The configuration of the implant contains two servers, the primary and the fallback.  Normally, the implant uses the primary server, and in case of an error it switches to the fallback server by invoking 'swapLpServerType' method."  And again, you're able to see the name of that in the code.



"Additionally, the sent and received messages are encrypted with symmetric (3DES) and asymmetric (RSA) crypto.  All messages are exchanged via the HTTPS protocol in POST requests, with the cookie having the key g, and a value that is a digit string from the public KI configuration parameter."  So the cookie has some of the public key parameters used for doing the RSA crypto.  Basically they've been able to completely reverse-engineer the thing that runs in RAM after this exploit finally is finished getting itself installed into the system.



They said:  "The implant periodically sends heartbeat beacons that contain system information, including the implant version, device identifiers (the IMEI, the MEID, the serial number and so forth), and the configuration of the update daemon, whether automatic downloads and installations of the updates are enabled."



So my first thought upon hearing that was that it was interesting that heartbeat data was being periodically sent, since that makes this thing more noisy and thus more prone to discovery.  But then it occurred to me that an iPhone is probably already extremely noisy with all of the legitimate traffic that it has going back and forth.  So any heartbeat data, which is relatively infrequent and not that much, you know, not high-bandwidth, is likely able to hide in plain sight without fear of discovery.



They said:  "The command-and-control server responds to heartbeat messages with commands.  Commands are transferred as" - oh, and I should also mention that one reason you need a heartbeat to be outgoing from the phone is that holds open any NAT that you've got between the outside public Internet and wherever your phone is behind that.  So if you didn't have an occasional heartbeat going out, there would be no way for command-and-control to access the phone behind NAT because there would be no mapping.  The NAT would look like the one-way valve it is, like a firewall.  So having a heartbeat creates an opportunity for commands to be sent back to this implant.



They said:  "Commands are transferred as Protobuf messages that have type names starting with CRX.  The meaning of these names is obscure.  For example, the command listing directories is called CRXShowTables, and changing C2 server addresses is handled by the command CRXConfigDBServer.  In total, the implant we analyzed," they said, "has 24 commands designed for" - and they're shortened them down into five categories.  "Interacting with the filesystem (creation, modification, exfiltration and removal of files).  Second, interacting with processes, listing and terminating them.  Third, dumping the victim's keychain items, which can be useful for harvesting victim credentials.  Fourth, monitoring the victim's geolocation.  And, finally, running additional modules, which are Mach-O executables loaded by the implant.  These executables are reflectively loaded, and their binaries stored only in memory."



So their documentation lists each of the individual commands, each of those 24, in details, and explains each one's purpose.  I won't enumerate them here, but it should be abundantly clear that essentially this represents a full and deep remote takeover of any exploited iPhone.



Okay.  And get a load of this.  They said:  "One of the interesting commands we discovered is called CRXPollRecords.  It monitors changes in folders, looking for modified files that have names matching specified regular expressions.  Change monitoring is handled by obtaining a Unix file descriptor of the directory and assigning a vnode event handler to it.  Then, whenever the implant gets notified" - it's proactively notified by the file system - "of a change, the event handler searches for modified files which match the regex provided by the attacker."  Think for a minute about how sophisticated this thing is.  Then such files are scheduled for uploading to the command-and-control server.



So in other words, it's possible for the command-and-control server to prime the Advanced Persistent Threat implant in a device, to autonomously notify the server when something in that device happens of specific interest to it.  When a change in the contents of a directory occurs, a check is done for relevancy.  And if that comes back affirmative, the files in question are queued for transmission.  In a very real sense, it is no longer your iPhone in your pocket.  It is theirs.  Talk about being pwned.



They said:  "While analyzing TriangleDB, we found that the class CRConfig, used to store the implant's configuration, has a method named populateWithFieldsMacOSOnly.  This method is not called anywhere in the iOS implant; however, its existence means that macOS devices can also be targeted with a similar implant.  The implant requests multiple entitlements, permissions, from the operating system.  Some of them are not used in the code, such as access to camera, microphone, and address book, or interaction with devices via Bluetooth.  Thus, functionalities granted by these entitlements may be implemented in modules."  Which they hadn't seen.



Then, at the end of the work of assembling all of this, I found an earlier note written by Eugene Kaspersky himself.  And this was written at the beginning of this month.  He said:  "We believe that the main reason for this incident is the proprietary nature of iOS.  This operating system is a 'black box' in which spyware like Triangulation can hide for years.  Detecting and analyzing such threats is made all the more difficult by Apple's monopoly of research tools, making it a perfect haven for spyware.  In other words," he said, "as I have often said, users are given the illusion of security associated with the complete opacity of the system.  What actually happens in iOS is unknown to cybersecurity experts, and the absence of news about attacks in no way indicates their being impossible, as we've just seen."



Okay, now, I thought that was very interesting.  He's clearly annoyed, and that's a bit of sour grapes, by their inability as security researchers to obtain any visibility into what's going on inside an iPhone.  At the same time, you know, they are Russian security researchers, and I've never seen any reason to mistrust them, but there are people who are unhappy that Kaspersky is in Russia.  As we've seen, and as he has said, they are limited to monitoring encrypted traffic for metadata, and making iPhone backups, and sifting through that detritus for clues.  I can understand his frustration when they are also targets of these attacks.



And what he just said echoes that thought that occurred to me a few weeks ago when I realized that Apple's high level of security has the unintended effect of protecting malware from discovery.  He has just said exactly that.



So this is everything that Kaspersky has publicly shared so far.  And the glaring piece of information that is lacking, perhaps because it's unknown, is any commentary about how this thing crawls into iPhones by escaping from Apple's security controls.  We have one clue about what I think is probably the late stage of this, which are thanks to the CVE which is associated with one of Apple's updates last week.  This is CVE-2023-32434 titled "Integer overflow in kernel."  Apple wrote:  "An app may be able to execute arbitrary code with kernel privileges.  Apple is aware of a report that this issue may have been actively exploited against versions of iOS released before iOS 15.7."  And then credit is given to three Russians who all work for Kaspersky.



So it appears that Kaspersky knows a little bit more than what they were saying because they didn't talk about that aspect of it for the time being.  And given that this vulnerability apparently enables the later stage of a powerful zero-click iPhone takeover, hopefully we'll never learn more because we really don't have to.  It'll be patched.  But there'll be phones  that will never get patched, so it's better that it's just left unsaid.



Oh, and one last piece of information that came from Eugene Kaspersky was an explanation for their choice of the name "Triangulation," which I had been wondering about.  He wrote:  "P.S.:  Why the name Triangulation?  To recognize the software and hardware specifications of the attacked system, Triangulation uses Canvas Fingerprinting technology, drawing a yellow triangle in the device's memory."



So what he means there is that it's possible, and often used, to ask graphic rendering software to draw into an offscreen buffer.  And it turns out that the precise details of one graphic renderer compared to another may differ ever so slightly.  The difference might be invisible to the naked eye.  But, for example, when a diagonal line is drawn, as when rendering a triangle, the exact values chosen by the line-smoothing, anti-aliasing algorithm might differ from one generation or model of a device to another.  The  practice known as "Canvas Fingerprinting" uses those invisible yet significant details to tell devices apart.



So thanks to Kaspersky's intrepid work, with their forensic analysis being actively impeded every step of the way by the very security they were trying to strengthen, last Wednesday's Apple updates foreclosed upon a kernel vulnerability that had apparently been in active use for at least four years.  We'll never know who or why or what or where.  But at least now we know how.



Do the bad guys have another way in?  Unfortunately, that seems more than likely.  What's most annoying and a bit galling, though, is the idea that our own governments may be the customers for whatever comes next.



LEO:  Yeah.  It's almost certainly nation-states; right?



STEVE:  Yes, yes.



LEO:  Sure these are very expensive.



STEVE:  Yup.  Oh, boy.  To be able to purchase that kind of capability, to send anybody you want who has an Phone an iMessage and then have that level of access to their device, you know, to be able to - in fact, there was some mention that, like, any previous video or audio recordings are immediately exfiltrated and sent back.



LEO:  Right.  Would it be, I mean, I've heard other researchers complain that Apple's security makes it hard for them to, for instance, take a look at any given iPhone and know whether it's compromised.  Right?



STEVE:  Right.  Exactly.  In fact, here the only way they knew was by looking at the communications traffic from the device.



LEO:  Yeah, yeah.



STEVE:  Because you can't see inside it.  It's a black box.



LEO:  You know, and I'm thinking about Google's Chromebook, which is also quite secure.  And Google keeps that secure by having, you know, a hash of some kind describing the system files.  You know, I would wonder, if Apple really wanted to, if there'd be some way that they could show system integrity without revealing the contents of the device.



STEVE:  I see what you mean.  So like, well...



LEO:  I mean, Secure Boot works that way with certificates.  But it also validates that the firmware is official firmware; right?



STEVE:  Right.  Although the problem is...



LEO:  I guess Apple already does that.  That's...



STEVE:  Yeah, I was just going to say, the problem is there's a bug.  If there were no bugs, the system would be perfect.



LEO:  Right.



STEVE:  And so it's the imperfection in the security that is the problem.



LEO:  Yeah.  Okay.  That makes sense.  Just feel like Apple could make some sort of effort to...



STEVE:  Be more transparent?



LEO:  No, because, I mean, honestly, look, there's security by obscurity, and I don't think that's a good plan.  But there is also security by locking the son of a gun down, encrypting everything, and not making it visible to anybody.  You know, I think that's fine.  It seems like Apple could have some sort of canary or something that would let you know if there had been tampering.  Maybe not.  



STEVE:  I think the problem is that the canary can be put to sleep.



LEO:  Right.  It's a bug, as you said.



STEVE:  Right.



LEO:  So bypasses all security.



STEVE:  Yeah.  You just gas the canary, and then you...



LEO:  He's alive.



STEVE:  Okay.



LEO:  He's alive.  What are you talking about?  He's alive.  He's just sleeping.



STEVE:  Canary only had one foot.



LEO:  He's just sleeping, it's okay.



STEVE:  That's right.



LEO:  Do we trust Kaspersky now?  I guess in this regard we do.



STEVE:  I really do.  Our listeners know that I hate the broad brush of saying, oh, all Chinese software is bad, and all Russian software is bad.  Like all Chinese people are bad.  That's just ridiculous.



LEO:  Right.



STEVE:  You know, I think Kaspersky, I mean, they're giving Apple fixes for zero-days.



LEO:  Yeah.  It's such an interesting conundrum.



STEVE:  It is, yeah.



LEO:  I mean, I probably wouldn't use Kaspersky antivirus.  But I think this kind of research is verifiable, so it's not like the Russian - it's not like Putin told him to say this.  So I wouldn't put their software on my system.  That's a bridge too far, perhaps.



STEVE:  I have to say I feel the same.  Although, well, only because I don't put anyone's software on my system.



LEO:  Well, that's right.  Can't trust anybody.  Steve, you've done it again, a great episode of Security Now!.  Thank you for catching me up.  I appreciate that.



STEVE:  Bye.



Copyright (c) 2023 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.






GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#930

DATE:		July 11, 2023

TITLE:		Rowhammer Indelible Fingerprinting

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-930.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  Could it be that yet another SQL injection flaw was found in the MOVEit Transfer system, and what more has been learned about last month's widespread attacks?  What's a "Rug Pull"?  What horrible conduct was the popular Avast AV found to be engaging in?  Did China actually create their own OS?  Version 1 is out!  How many times can we say "TootRoot" while covering one story?  What's the controversy surrounding the recent release of Firefox 115?  Did Russia just successfully disconnect itself from the Internet?  What are modern Internet honeypots discovering?  How much of your life savings should you transfer into online cryptocurrency exchanges?  (Okay, that's an easy one.)  What did EU agencies just rule against Meta and Google?  What happened to Apple's quickly withdrawn Rapid Security Response update?  And after a bit of miscellany and listener feedback, we're going to look at the return of Rowhammering for the purpose of creating indelible fingerprints.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  There's a whole bunch of stuff to talk about.  If you thought cookies were bad, wait'll you hear about Indelible Rowhammer Fingerprinting.  Steve will also talk about his usage of Syncthing.  He's got a pretty good workflow for backing up all of his assembly language.  And then, yes, another critical SQL flaw in MOVEit file transfer.  Wow.  All of that and a lot more coming up next with Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 930, recorded Tuesday, July 11th, 2023:  Rowhammer Indelible Fingerprinting.



It's time for Security Now!, the show where we get Steve Gibson in here to tell us what's going down.  Steve's the host of the show for how many years now, Steve?



STEVE GIBSON:  We're closing in on the end of year 18.



LEO:  18.



STEVE:  I think, you know, whenever I say that I get corrected by Elaine.  She says, no, you started in - we started in '05, so when we finish...



LEO:  18 years, yeah.



STEVE:  Yeah.  So when we finish 2023, and I think that's like next month, maybe, August?  So that's good.  We've almost reached maturity.  We're almost adults.



LEO:  Sorry, the Alexa was making noise.  Okay.  Yeah, you're going to be able to vote, but not drink.



STEVE:  Go figure.  And defend our country's honor.



LEO:  There you go.  Why don't you stay where you are and do your job.  You're doing a good job right where you are.



STEVE:  Yeah, I was, you know, a little nervous when we were - remember we had the lottery back when we were in high school.



LEO:  I know.



STEVE:  It was like, uh...



LEO:  I got a high number.  What was your number, do you remember?



STEVE:  I was also a high number.  I was in a lot of danger.



LEO:  So a breath of relief; right.



STEVE:  And I figured, you know, Leo, they weren't going to put a rifle in our hands.  They were going to say, oh, look.



LEO:  Look, a geek.



STEVE:  Yeah.  But we're going to make you wear a uniform, even if you're staring at a CRT.  It's like, what?  Why do I have this?  What?  Anyway...



LEO:  I got a number of emails from people.



STEVE:  Yeah.



LEO:  Who said Battle Dress Uniforms, or BDUs, are very comfortable.



STEVE:  That's good because, you know.



LEO:  We were talking last week.



STEVE:  You know what, yeah.



LEO:  About the guys in the Cyber Command wearing camo.  You know, it would be better camo if it looked like the Matrix or something, you know, if it was green on black.



STEVE:  Ooh, yeah, yeah, that'd be good, yeah.



LEO:  Yeah.



STEVE:  So for Security Now! Episode 930 - and by the way, Leo, we did confuse the world who are not in the U.S. about what happened to last Tuesday's podcast.



LEO:  Oh, yeah, I apologize.



STEVE:  I also got a bunch of, like, where did it go?  What what what what?  But I have to tell you, I had a very nice week off, so.



LEO:  Steve used to hate days off.  Then he got married.



STEVE:  Yeah.



LEO:  To a very attractive woman.  And now he says "Give me some time off, I'll take it."



STEVE:  I'd be happy to stay home.  So this is sort of an interesting evolution.  Today's podcast is titled "Rowhammer Indelible Fingerprinting."  And the title sort of says it all, but we're going to get into the details because that's where it really gets interesting.  But before we get into that, we're going to ask the question, could it be that yet another SQL injection flaw was found in MOVEit's transfer system?  And what more has been learned about last month's widespread attacks on all the users of that software?



What is a "rug pull"?  What horrible conduct was the popular Avast AV found to be engaging in?  Did China actually create their own OS?  Version 1 is out.  How many times can we say "TootRoot" while covering one story?  What's the controversy surrounding the latest release of Firefox 115?  Did Russia just successfully disconnect itself from the Internet?  What are modern Internet honeypots discovering?  How much of your life's savings should you transfer into online cryptocurrency exchanges?  Okay, yeah.



LEO:  I'm thinking a number - is there a number lower than zero?



STEVE:  Yeah, that is an easy one.  What did EU agencies just rule against Meta and Google?  What happened to Apple's quickly withdrawn Rapid Security Response update yesterday?  And after a bit of miscellany and some listener feedback, we're going to look at the return of Rowhammering, this time not for a security breach, but for the purpose of creating indelible fingerprints.



LEO:  Oh.



STEVE:  In other words, they can recognize your computer, and you can't escape.



LEO:  Oh.  That's not good.



STEVE:  Yeah, not good.



LEO:  Never mind.  I was all excited.  Never mind.



STEVE:  And we do have a bizarre Picture of the Week.



LEO:  Okay.



STEVE:  Which we will get to in a minute.  Which I gave the caption "A Different Definition of Insanity."



LEO:  I'm not looking until everybody gets to see it.  I want to see it at the same time.  All right, I'm ready, Steve.  I'm going to take off the blindfold for the Picture of the Week.  Oh, wait a minute, I have to push some buttons here before I can show that.



STEVE:  It'll take a little bit of visual parsing.



LEO:  It's one of those things I have to look at for a little bit before I understand what I'm seeing?



STEVE:  Unh-unh, mostly because you won't believe your eyes.



LEO:  Oh, come on.  My little computer sometimes - there we go.  Here we go.  Okay.  I am ready.  I'm going to roll and scroll up.  It's titled "A Different Definition of Insanity."  What?  Okay.  Why don't you describe - I get it.  It's so silly.  I'm glad they made nice thick cables for this junction box.



STEVE:  So it's hard to - I think I understand what's going on because there's a little bit of a giveaway with this Post-it note down below the junction box.  But so what we have is two massive conductor cables.  I mean, like they look like they're an inch in diameter.  They're huge.  And they are running through, like, coming in from the left of the picture, both of them in parallel, running through a junction box and then going out of the picture to the right.  Then, and they're separated by maybe a couple inches.  Then, okay.  So like these things carry some ungodly amount of current.



LEO:  These are big, yeah.  There's one brown, one blue.  Clearly, you know, this is 240 at least.  It's a lot of voltage.



STEVE:  This is serious.  So someone has removed the insulation from each of these, about two inches' worth of insulation, so we can see this massive copper cable that is being protected by the insulation.  And then, apparently needing some light, they've wrapped some wire around, like one turn around each of them, and run the wires out the bottom of the junction box.



LEO:  Oh, I didn't see the bottom.  There's a little light bulb.



STEVE:  Where it's being used to power a light bulb.



LEO:  A little one.



STEVE:  Yeah, like a regular, you know, Edison screw-in base light bulb.  And so it's like, as if like this is the way you want to light the room or something, is like by opening up Godzilla's power supply.



LEO:  By the way, the little wires aren't even soldered onto the copper.  They're just wrapped around it and twisted.



STEVE:  It's just like, as I said, a different definition of insanity.



LEO:  Now, what's this Post-it?  Greater than underscore.  What does that mean?



STEVE:  Yeah.  I think that's meant to say greater than ground, as if to say, by the way, these wires are live.



LEO:  Oh, they're hot.  That's what the lamp is.  It says these are hot.



STEVE:  Yes.  I think this was all set up as some insane way of determining whether these are energized.



LEO:  So don't touch them, kids.  We've stripped off the insulation and opened the junction box, just to let you know, don't touch them.



STEVE:  Yes.  Now, I'm sure all of us old school geeks learned a long time ago the trick of sticking your tongue on a nine-volt battery to see whether it was good or not.  Do not do that here.



LEO:  No, no.



STEVE:  Because you will have no tongue left.



LEO:  Wow.  Wow.



STEVE:  Anyway, this was just like crazy that this was done so informally.  But the only way I can explain this is that this was some, like, poor man's way of determining whether the power was on on these Godzilla's revenge cables.



LEO:  And presumably that box, because it isn't really a junction box, it's just got two wires going through it, it's built, I mean, it's for this purpose.  Presumably they closed it up after they showed you their clever little hack.  I hope they did.  I hope they did because I don't know about you, but I'd be really tempted to lick it.  I don't know why.  I just want to; you know?



STEVE:  Oh, yeah.  Anyway, thank you.  We have such great listeners who are now on the constant prowl for gates in the middle of nowhere, bridges that go nowhere, and strange wiring.  And, well, I have a few more, and I'm not going to give them away.  But we've got some more fun things coming.



Okay.  So anyone who understands the inherent danger of exposing the unnecessarily powerful SQL command stream to web servers, especially web server visitors, will not be surprised to learn that yet another instance of this extremely common and potentially devastating source of vulnerabilities has been uncovered in Progress Software's MOVEit Transfer system. Fortunately, the previous problems drew the attention of security researchers, in this case HackerOne and Trend Micro's Zero Day Initiative researchers.  They decided to have their own look at MOVEit's code.



The good news is they responsibly disclosed their discoveries - still more discoveries - so that patches could be prepared and made available to Progress Software's beleaguered users.  At the same time, Progress also patched another pair of high security vulnerabilities which were not SQL injection.  There was one that allowed you to crash the MOVEit server.  It's like, okay, fine.



So after literally an update per week - there have been three required updates about a week part, each which patched critical vulnerabilities - it's time to do it for the fourth time in four weeks.  And I seriously don't know what I would advise MOVEit's users to do at this point.  I guess I would feel extremely uncomfortable if I were to understand the inherent danger that this system presents to its users.  As we covered three weeks ago in Episode 928, literally, I mean, truly thousands of Progress's customers were penetrated and did have their internal private data exfiltrated to Russia.  Then they were extorted by Russia's Clop cybercriminal organization.



And the problem going forward is that due to the extremely poor fundamental security design of this MOVEit system, there's no reason to believe that the last serious data exfiltration problem has now finally been found.  Sure, the system is doubtless far more secure than it was four weeks ago, but the problem is this class of bugs, these SQL injection bugs, are slippery.  And unfortunately bad guys are highly motivated to find a way in.



And as for MOVEit itself, we're back here on the topic of MOVEit hacks.  The most current forensics conducted by Huntress Labs suggests that the attacks on those thousands of MOVEit-using companies appear to have been limited to just data theft and extortion.  Which is a good thing because these were really powerful intrusions.  The Clop cybercrime group has not yet been observed deploying ransomware into any of the incidents linked to its exploitation of MOVEit.  Huntress also said that it has not observed the Clop gang expanding access into full network compromises, which, again, they could have done.  The gang has apparently deliberately limited itself to infecting and infiltrating only the hacked MOVEit appliance itself.  So as I said, things could have been far worse if a much more aggressive attacker had been found behind this.



And the other thing, too, that sort of gave us some pause is that the Clop gang from the beginning said, "We're not going to do anything with the data of governments or law enforcement."  Almost as if they were a little put off by the aggressive response which some of the earlier ransomware attacks engendered.  And they just kind of wanted to say, uh, well, we're bad, but we're not that bad.  And so governments and law enforcement, if we get your data by mistake, we're just going to delete it, so don't come after us.  No, and also educational institutions because there was a - it wasn't Carnegie-Mellon, it was some major university got exfiltrated.  Anyway.  So hopefully nothing came of all that.



I was scanning a news blurb about another crypto exchange mess.  And it's like, yeah, yet another one.  In fact, we have some numbers a little bit later.  And everyone should understand that the bar has been raised fairly high for me by this point so that it's not all of these that I bother to bring up because it's just like, why monitor the flood?  You know, you're wet, so you know that there's a water problem.  But, you know, there's just so much of this going on.  And it's safe to say that the entire crypto world is in pandemonium.



But then I ran across a term that's taken hold in the industry which left me shaking my head.  And really there is some value to having everyone in our community of podcast listeners really appreciating the degree to what a mess things are.  So here's the news that I encountered, which is interesting in itself, which concludes with this new bit of terminology.



Last Friday, July 7th, approximately $126 million worth of crypto-assets were mysteriously transferred from the accounts of cryptocurrency platform Multichain in what is believed to be a  hack.  This is according to blockchain security firms PeckShield, SlowMist - and we'll come back to SlowMist a little bit later - Lookonchain, and CertiK.  And maybe you know you have a problem when there's four firms, like when monitoring blockchain security is immediately a business.  That, you know, might be a bit of a problem.



So Multichain is sort of a meta platform which, as its name sounds, interconnects different blockchain platforms allowing users to exchange tokens across them.  And as a result of this incident it was shut down, Multichain was shut down to investigate what happened.  At the moment, neither Multichain nor any blockchain experts know exactly what it is that happened.  CertiK believes some of the platform's private keys were probably compromised, although this theory has not yet been confirmed by anyone else.



Fortunately, due to quick intervention by platforms like Circle and Tether, $65 million of those $126 million which were moved from Multichain wallets were frozen pending the determination of what happened.  So there was like, you know, if nothing else, as a consequence of there having been so many problems, the industry is getting a bit better at just saying, okay, stop.  You know, freeze everything.  Let's figure out if this is good or not.



This is also the second time this year that Multichain has suspended its operations.  It also halted trading at the end of May.  At the time, the company said it couldn't perform server maintenance because its CEO had gone missing, and they didn't have access, as a consequence, to the entire platform.  The rumor was that its CEO had been detained by Chinese authorities.  So Multichain never indicated whether its CEO had rejoined the company.  If the recent incident is confirmed, this would be the company's third hack in the past two years.  It lost $3 million in January of 2022 and another $8 million in July of 2021.



Okay.  So that's a bit of the background.  Here's the way this piece of news concluded which made me just shake my head.  It said:  "Blockchain experts aren't ruling out a rug-pull either."  And I thought, a what?  A rug-pull?  Yes.  A rug-pull is when a cryptocurrency's platform developers run away with the money themselves.  They "pull the rug" out from under their own operation.  So what we have here is an industry that is so unstable that the term "rug-pull" has been coined as a shorthand to mean "the bank's vault got all filled up with value, and then the bankers just decided to take it."  Because, you know, why not?  Crazy.



And you know, Leo, just the other day we were both observing that neither one of us is generally a big fan of attorney-enriching class-action lawsuits.  But I stumbled over some news that really makes you shake your head and maybe think, well, maybe there's a place for them.



LEO:  I think, I mean, nobody makes any money; right?  Except the attorneys.  But it does call these companies to account, which can have some value; right?



STEVE:  Yes.  And that is why I'm hoping that a big one lands on this firm.  The Netherlands has a foundation known as the CUIC, which is for "Consumers United in Court."  This CUIC Foundation has filed a class-action lawsuit against the quite well-known security firm Avast.  CUIC alleges that, from its special position in its users' PCs - and not a few of them, either - Avast collected data on its users' activities, which it then sold to online advertisers without its users' knowledge or consent.  Everyone who used Avast's AV products or browser extensions between May of 2015 and January of 2020 had their data collected and sold.



The list of products includes Avast Online Security, AVG Online Security, the Avast Secure Browser, the AVG Secure Browser, and the AVG Online Security extension.  The collected user data was sold through a U.S.-based subsidiary named Jumpshot.  Avast shut down Jumpshot back in January of 2020 after there was some hint of what was going on, and that was exposed.



So much as I generally despise class action lawsuits, in this case, as I said, I hope that the proverbial book gets thrown at them.  This is a deep betrayal of trust.  And it's sad.  Avast has been around for 28 years, since 1995, near the beginning of the PC era.  And it's currently in use by 435 million users.  The only thing you can figure is that it must have been that the proceeds from collecting and selling data on the habits of that many individuals was just too much for them to resist.  I wouldn't mind seeing them put out of business over this, frankly.  Essentially, this was commercial spyware running in the PCs of those 435 million users.



LEO:  It's kind of ironic.  You get a secure browser.  Oh, yeah, it's secure from everybody but us.



STEVE:  That's right.



LEO:  We're going to keep an eye on you.



STEVE:  We're going to let you know if any other spyware comes in.  But we're giving ourselves permission.



LEO:  Avast acquired AVG.  These are the two free antiviruses  that a lot of people use because they were free.



STEVE:  Not quite so free, it turns out.



LEO:  Well, it just reminds you, if you're not paying for it...



STEVE:  Uh-huh.  Yeah.



LEO:  There's some way they're monetizing this.



STEVE:  That's exactly right.  Okay.  So we've been observing that the era where it was feasible to actually create a new operating system from scratch has long since passed.  The BeOS, you know, B-E OS, it occurs to me that that was probably the last entry that reached some level of maturity and had some adoption and critical mass.  But of course then even it couldn't make it.



LEO:  And that was the late '90s.  I think people looked at that and said, oh, yeah, you can't write a new operating system now.  It's just a Mac and Windows and Linux world, and that's that.



STEVE:  Right.  And there have been some various small hobby efforts, but they never get very far, either.



LEO:  Actually, BeOS is still around as Haiku.  There's an open source project.



STEVE:  Yes, yes.



LEO:  And of course Google wrote Fuchsia, but they're putting it on their IoT devices.  It's not...



STEVE:  Well, and did they write it from scratch?



LEO:  Yeah.  Yeah, they said they did.  No, no, they started from scratch.



STEVE:  Really.



LEO:  It took them years, yeah.



STEVE:  Wow.



LEO:  But it's not a general purpose OS.



STEVE:  Right.  Well, and with Linux being open source, and with an incredible amount of effort already having gone into it, that's where any sane project would start today.



LEO:  Start with a Linux kernel, absolutely.



STEVE:  And, sure enough, that's exactly the conclusion that China came to.



LEO:  Yeah.



STEVE:  Recall that China announced that they were going to be replacing the West's Windows with "their own" operating system.  It's actually Linux under its quite attractive covers.  I have a link in the show notes, Leo, if you want to scroll through the pictures of the screen shots.  I mean, it is, frankly, it's gorgeous-looking.



LEO:  Yeah, a lot of people been saying nice things about this, you know.



STEVE:  It's called OpenKylin, K-Y-L-I-N.



LEO:  It's Ubuntu; right?



STEVE:  Well, it's based on the Linux 6.1 kernel.



LEO:  Okay.



STEVE:  They've been working on it for many years.  It's available for x86, ARM, and RISC-V architectures.



LEO:  They're clearly trying to make it look like Windows.  It's even got a recycle bin here.



STEVE:  And rounded corners on the...



LEO:  Yeah, yeah.



STEVE:  Yeah, I mean, it is really pretty.  And so x86, ARM, and RISC-V.  More than 200 companies, 74 special interest groups,. and 3,000 developers contributed to the effort.  And the good news is, for any of our English speakers, it does have an English language option.  So, you know, it looks very pretty.  So anyway, so that answers the question about what China is doing, and it's their intention to say goodbye to Windows and hello to OpenKylin.



LEO:  I'm all for that as long as they treat it as open.  Russia did the same thing; right?  Wasn't there a Red OS or something like that?



STEVE:  I don't know what they're doing.  I did note that Firefox is the browser for this, so that's great, too.



LEO:  Yeah.



STEVE:  We want to keep Firefox alive.



LEO:  Yeah, that's right.  Well, now you've got a billion new users.



STEVE:  Yeah.  Okay.  So I might not normally mention an extremely severe CVSS 9.9 vulnerability, except that it's in Mastodon.  And when you learn that the vulnerability has been named "TootRoot"...



LEO:  Oh, boy.



STEVE:  ...you really - you have no choice but to talk about it.  So the Mastodon project has fixed the critical rootin' tootin' "TootRoot" vulnerability which  and get this, it was bad  allowed bad guys to commandeer any Mastodon server simply by posting a Mastodon toot containing a malicious multimedia file extension.



LEO:  Oh, wow.



STEVE:  Uh-huh.



LEO:  Oh, boy.



STEVE:  And as we know, that's not an easy thing to defend against, right, because the extension is, I mean, it's very difficult to get multimedia correct because multimedia codecs are massively complex interpreters.  And interpreters are tricky.  So the good news is that once the trouble had been discovered, everything proceeded properly from there.  Patches were released two days before the details were published.  And in fact there was an announcement that there was going to be a release two days before that.  So the whole system proceeded.  And here's what's interesting, too, is TootRoot was discovered by security researchers at Cure53 because they were conducting a security audit at the behest of Mozilla.



LEO:  Oh.



STEVE:  I don't know why Mozilla said...



LEO:  Oh, Mozilla's launching in Mastodon.  They're doing a Fediverse.



STEVE:  Nice.



LEO:  That's why.  That's why.



STEVE:  Nice.  Then that would be it.  Before we dive, before we jump in, we want you guys to give it a security audit.  And sure enough, the audit came up with a potentially very serious vulnerability.  And as we know, these guys were just looking at the Mastodon code.  Bad guys could have, too.  And so what the good guys can find, the bad guys can find.



LEO:  Right.



STEVE:  Better than the good guys found it first.



LEO:  And we're patched, if you're curious, twit.social.



STEVE:  Yeah.



LEO:  And have been since the patch came out.  We have a very good administrator.



STEVE:  Yeah.  In total, Cure53 discovered four security flaws.  And again, props to somebody for naming this TootRoot because, you know...



LEO:  That's a good name.



STEVE:  That's a good name.  Okay.  So those of us who are using Firefox will find that we're now using Firefox 115.  Since one of my two primary workstation machines, in fact it's the one that I'm sitting in front of right now, is still running Windows 7, and it's running perfectly.  I also received the notice that this would be the final release of Firefox for this machine, other than security updates.  And that's fine since security updates is all I really need anyway.  Edge and Chrome had both given up on me several months ago.



The somewhat controversial new feature in Firefox 115 is Mozilla's declared ability to remotely prevent arbitrary extensions to run on arbitrary websites.



LEO:  Hmm.



STEVE:  Uh-huh.  For some reason this has upset people, which makes no sense to me.  I cannot imagine that Mozilla's motives would ever be anything less than pure.  So if they know something I don't about some extension I'm running, and how some evil website I might mistakenly venture into is able to abuse that extension that I'm running, then by all means shut it down, and thank you very much.



LEO:  That's what happens in the Chrome extension store.  Everywhere; right?



STEVE:  Well, so there's blacklisting extensions.



LEO:  Oh, it's not just in the store.



STEVE:  Correct.



LEO:  Oh.



STEVE:  So this is the browser itself, and it's per website.  So it's more fine-grained blacklisting.  So in their explanation of this, Mozilla said:  "Mozilla maintains an open ecosystem for add-ons, which gives developers many choices in how they create, use, and deploy their work.  The same openness also provides malicious actors more opportunities, as well."  And I think that's the key is they have some evidence that their openness is being abused by bad guys who are using the access to the code to find previously unseen problems, just like we were talking about with Mastodon.



Mozilla said:  "While Mozilla can identify and block malicious add-ons discovered through tooling, reviews, and user reports, this is not always enough.  Firefox version 115 introduced Quarantined Domains" - with capital Q, capital D, so that's their formal name for it, Quarantined Domains - " to protect user privacy and security when we discover significant security issues presented by malicious actors.  This feature allows us to prevent attacks by malicious actors targeting specific domains where we have reason to believe there may be malicious add-ons we have not yet discovered.  Users can also control this behavior for each add-on in the Add-on Manager (about:addons) starting with Firefox version 116."  So the next major release.  And they finished, saying, "We will be further improving the UI for users in future releases."



Okay.  So 115 lacks some UI for this, but that's coming in 116, which I won't get on Windows 7, but I don't care because I'm happy to leave this in Mozilla's far more focused hands.  Now, an example of an extension developer who drew some attention over like his upset with this is a guy named Jeff Johnson whose posting carried the headline "Firefox 115 can silently remotely disable my extension on any site."  And it's like, yes, that's by design.  And why would they if it wasn't for the protection of the users of these extensions?  Anyway, Jeff begins his grumbling by writing:



"Firefox version 115.0 was released on July 4th, but I'm not celebrating.  I'm concerned about a new 'feature'" - he has that in air quotes - "in the release notes."  And then he quotes them saying:  "Certain Firefox users may come across a message in the extensions panel indicating that their add-ons are not allowed on the site currently open.  We have introduced a new backend feature to only allow some extensions monitored by Mozilla to run on specific websites for various reasons, including security concerns."  So that's what he quotes Microsoft saying.  And he says:  "For various reasons.  That's quite uninformative and mysterious."



And he says:  "I'm all in favor of giving users control over which extensions are allowed to load on which sites.  Safari already has this feature on both macOS and iOS.  My concern," he says, "is not about user control - little of which even exists in Firefox 115, as I'll show later - but rather about the remote control that Mozilla has now given itself."  Again, to which I say "So what?"  I mean, you know, that's good.  That's what we want.



So anyway, Mozilla's in the driver's seat here.  They must review and examine and digitally sign any extension before Firefox will even consider running it.  And they also maintain, and Firefox enforces, a formal black list to disallow any previously signed extensions that are later found to be malicious.  So all they're doing is adding an additional level of granularity and control by supporting the intersection of specific extensions and specific web domains.  Sure.  We may not understand exactly why.  But they must have seen a need and decided that they're going to fill it for the safety of their users.  So maybe the guy just needed to draw some attention to his site by posting this.  I don't really understand.



LEO:  Oh, no one ever does that.  That's a...



STEVE:  That couldn't be a motivation.



LEO:  Couldn't be the case, no.



STEVE:  How selfish.  Okay.  So as we've previously observed, Russia has been making noise about their own RuNet, which is what they call the portion of the Internet located within their territory.  It appears that Russia now has a law which requires, and I get to say it, Roskomnadzor to perform an annual test disconnection of the Internet for the purpose of verifying RuNet's stability when it's functioning as a freestanding network.  And they're claiming that happened in the very early morning last Wednesday, July 5th.  However, people in Russia are doubtful.



Someone named Natalia Krapiva, she tweeted:  "Last night, Russia tested disconnecting itself from the global Internet.  On June 5th, around 2:00 to 4:00 a.m. Moscow time, authorities tested the Sovereign Internet system, which led to disruptions of various websites and government infrastructure services.  Russian railroad services and food safety systems were reportedly disrupted after the Sovereign Internet testing on the morning of July 5th."



She then links to a Moscow Times article about this.  However, someone named Oleg Shakirov quoted her tweet and replied:  "Please don't take Russian or anyone's claims about testing disconnection from the global Internet at face value.  Always remember about incentives within the system to exaggerate one's work.  There are plenty of reports that there were no universal break in connectivity.  There is no real evidence that the drills caused disruption of Russian Railroads and the agriculture regulator.  I mean this in a literal sense.  There is no evidence cited in this Moscow Times piece.  As I tweeted earlier, RZD" - and that's the Russian Railroad - "RZD problems started before the exercise and continue today, most likely caused by pro-Ukrainian DDoS-attacks."



And in fact earlier Oleg had tweeted:  "The website and app of Russian Railroads have been disrupted for four days.  Customers are told to purchase tickets offline.  According to RZD, this is due to hacker attacks.  The problem persists today despite yesterday's statement."  He finishes:  "The IT Army of UA implied its role."



And lastly, somebody else whose handle is @ug_sig tweeted:  "Yes, a quick look at the sites that monitor Internet plumbing did not reveal any great disruptions during the time period identified in the reporting.  One would have expected to see massive routing drops and other disconnections, but I didn't see any of that."



So yes, disconnecting from the Internet in any meaningful way for the purpose of isolating all of Russia from the West - have you seen how big Russia is?  That would indeed be visible to anyone monitoring the Internet's operation.  It seems likely that if anything was done at all, it was some token gesture, you know, like they pulled the plug on the Kremlin or something.  You know, it's like, oh, look, no Russia.  No Western connection.  No Facebook.  No evil Google; you know?  And then they plugged it back in again, and that was their annual test.  So, and in fact, there were reports of pro-Ukrainian forces taking credit for shutting down Russia's rail system using DDoS attacks.  So again, I don't think that actually happened.



LEO:  All right.  Fully hydrated and ready to go with Act 2.



STEVE:  So Trustwave's SpiderLabs group carried out a six-month experiment creating a globally diverse honeypot network.  They wrote:  "To obtain a better perspective of attacks worldwide, Trustwave has implemented a network of honeypots located in multiple countries around the globe.  By distributing honeypots in such a manner, we can gather a reliable set of information on the methods and techniques used by attackers and their botnets.  In our pursuit to explore the current threat landscape, we established a honeypot sensor network across six countries:  Russia, Ukraine, Poland, UK, China, and the United States."



Okay.  So basically they established a widely geographically distributed set of listening posts across the Internet, collecting all of the incoming arriving packet traffic for a period of half a year.  What did their six months of listening reveal?  They found that fully 19% of traffic that probed their test honeypot network was malicious.  So one in five packets incoming malicious.  And of that malicious traffic, 95% came from IoT botnets which were out scouring the Internet trying to locate and exploit new devices and enlist them into their botnets.



So we have a world now where 19% of the traffic arriving at arbitrary IPs will be malicious.  One in five packets is trying to do something bad.  And 95% of those are from IoT botnets looking for a vulnerability in something that has been discovered trying to take it over.  That's the reality of today's world.



LEO:  Wow.



STEVE:  Yeah.  It's not just the occasional packet; right?  I mean, there's a lot of traffic coming to an active IP that is valid.  And for 20% of that to be bogus, that's just astonishing.



So I know I've already talked about, just earlier in this podcast, the crazy losses being visited upon various cryptocurrency exchanges and services.  But in preparing today's news I ran across a more comprehensive summary, sort of an aggregation of this, that I wanted to share.  And remember that I had already mentioned SlowMist.  A week ago, on Monday, July 3rd, three days after the first half of 2023 ended, the blockchain monitoring group SlowMist published their 2023 Mid-Year Blockchain Security and Anti-Money Laundering Report.



They started off by explaining:  "This report delves into blockchain ecosystem security, summarizing key security incidents and funds recovery status in the first half of 2023.  It aims to help readers identify suspicious transaction patterns and behaviors by analyzing typical cases, and explore the anti-money laundering landscape within the blockchain ecosystem."



Their report is lengthy and detailed, and there's no need to go into all of that.  But here are some of the high points that will catch anybody's attention.  Get this:  More than $922 million worth of cryptocurrency assets, $922 million, so just shy of a billion dollars worth of cryptocurrency assets were stolen in the first half of this year, 2023, across a total of, and that is to say occurring during, 185 security incidents.  185 incidents.  What is that?  Is that one a day?  That's one a day in half a year.



That dollar figure is, interestingly, less than half of what was lost during the first half of 2022.  During the first half of last year, 2022, hackers stole $2 billion worth of crypto assets across, interestingly, 187 incidents.  So the same number of incidents in both the first halves of this year and last year.



In total during 2022, that is, last year, Chainalysis is the name of the reporting firm, reported the loss of more than - so this is in 2022 - loss of more than $3.8 billion worth of assets.  It's unclear why the first half of this year saw fewer funds, like half the funds, which were stolen compared to the first half of last year.  But no one believes it's because cryptocurrency platforms have become more secure.  As we saw, the total incident count was 185 versus 187.  So essentially the same.



Nearly half the funds stolen this year, so far in the first half of this year, were taken from NFT, DeFi, and cross-chain bridge platforms, like that Multichain that we talked about before, that's like a new thing.  And they lost a total of $487 million in 131 incidents.  So about half of the total amount was just in that.  And the year's largest hack so far was the Euler Finance incident, where the platform lost $197 million.  That hacker eventually returned most of the stolen funds in one of the 10 incidents where attackers returned any stolen crypto.  Usually that's in return for amnesty and a "bounty," which is typically a hefty percentage of their total take.  So just saying.



It should be so abundantly clear that the world is still a long way away from figuring out how to do any of this securely.  So it is really difficult to see how participating in any of this is worth the risk.  It's just - it's just crazy.  And I know none of our listeners are that nuts; you know?  They're not going to stick their tongue across those two high-tension cables, either.  But we all know somebody.  Actually, we have some neighbors who were like telling us, you know, expounding on the benefits of cryptocurrency investment.  It's like, and I just, you know, I bit my tongue, and not because it was sore from having stuck it on a 9V battery.



LEO:  Did they ever try to sell you Amway or anything like that?



STEVE:  Oh...



LEO:  That's the problem with the pyramid scheme.  You've got to get others to join.  Otherwise your initial investment is quickly worthless.



STEVE:  Yeah.  So the world is continuing to struggle, well, actually I would argue the struggles are just beginning, over issues of cross-border Internet consumer privacy and the monetization of consumer data.  Two examples.  The European Union has just ruled that what Meta is doing is illegal under the EU's GDPR.  The European Court of Justice has ruled in a case between Meta and Germany's Federal Cartel Office, concluding that Meta's interpretation of the EU's GDPR regulation is illegal.



The court sided with the German agency, which ruled in 2019 - as we know, these things always take a long time.  Here we are, what, four years later - that Meta was bypassing GDPR privacy protections by taking data collected by various of its services without proper user consent and merging it behind the scenes.  The aggregated data then allowed Meta to continue tracking German and EU users to feed its advertising business. The court's ruling bars Meta from engaging in such behavior in the future.  So that's a first.



LEO:  It also keeps Threads out of the EU, I think.



STEVE:  Uh-huh.  Yes.  Meanwhile, Sweden has just joined Austria, Denmark, France, and Italy in their growing crackdown on Google Analytics use.  The Swedish data protection agency has fined two local companies - and these are hefty fines.  I can't imagine they're going to get paid.  The Swedish data protection agency just fined two local companies for their use of the Google Analytics service and has recommended against future use of the tool.  A one million euro fine was handed out to Swedish telco Tele2, and a 25,000 euro fine to a local online retailer CDON.  The fines were handed out because companies allowed Google Analytics to collect data on Swedish citizens which was then transferred to the U.S.  As I noted, Sweden thus becomes the fifth EU member state to fine or recommend against the use of Google Analytics, which we talked about, you know, a couple months ago.



Now, assuming that the EU's legislation holds up, and they have every right to create and enforce whatever legislation they choose, it looks like it's going to become truly necessary for our large multinational Internet service providers to establish and run fully independent facilities which are able to demonstrate complete autonomy within each national region.  And of course everyone's fighting against that, the Internet service providers, Facebook, Google, Apple and so forth, because it won't be cheap or easy to do so.  But they're going to go kicking and screaming, but it does looks like the future.  It doesn't seem like there's going to be a way for them to get around this because they really are aggregating user data and using it for their own benefit.  The problem is right now it's crossing national lines.



Yesterday, and I didn't have a chance to listen to MacBreak, Leo, so I don't know if you guys talked about this, but Apple issued...



LEO:  Oh, yeah.



STEVE:  ...and then almost immediately retracted an emergency update.



LEO:  We did talk about it.  One hour later they took it back.



STEVE:  Yeah, just after posting, and began pushing one of their new emergency, you know, it's the RSR, the Rapid Security Response updates.  This one was iOS, iPadOS, macOS Ventura, and Safari.  An hour later it was canceled and withdrawn.  So what happened?  According to reports posted to MacRumors forums, Facebook, Instagram, WhatsApp, Zoom, and other websites started producing warnings to their visitors about their updated Apple Safari browsers not being supported.



Okay.  So the update repaired a WebKit zero-day vulnerability that had been discovered while it was under active exploitation.  And that brings Apple's patched zero-days count to 10 for the year so far.  Okay.  But get a load of what happened.  The previous versions of the various iOSes were 16.5.1 for the iOSes, and in the case of macOS Ventura, 13.4.1.  What Apple did, since this was meant to just be a quickie RSR WebKit patch, was to leave the primary version number unchanged at 16.5.1 and to simply append a lowercase "a" enclosed in parentheses to the end of the version number.



So, as a consequence of this change, Safari dutifully began appending that same parenthetic "a" to the end of its User-Agent version string.  And that was quickly revealed for some reason to deeply upset and confuse the User-Agent string parsers being used by a number of quite popular web servers.



So in the User-Agent string it said Version/16.5.2 is what I've got in my notes, with a "(a)" separated by a space.  And apparently that was all it took.  So Apple quickly retracted this update to minimize the damage that it was doing and advised people if they were seeing this problem to immediately roll back to the previous version.  And they have said that they would be releasing a "(b)" update.  And presumably, obviously it'll have the same zero-day fix; but they will, I guess, not put this into the User-Agent version string.



LEO:  Not change the user-agent, yeah.



STEVE:  Yeah.



LEO:  I kind of blame, I don't know, there's blame to go around because why are they being so picky about the user-agent?



STEVE:  I agree.  I think that those web servers were using some common software that is common to them, which failed in parsing because...



LEO:  There's an (a).  What version is that?  I don't know, no, no.  Stay away.



STEVE:  Yeah.  And, for example, I have an actual picture of the user-agent string which was captured by the updated software.  And there's a bunch of stuff in parentheses.



LEO:  Right.



STEVE:  AppleWebKit/605.1.15 (KHTML, like Gecko).  So you would think that they're just going to ignore what's in the parentheses.



LEO:  Yeah.



STEVE:  But no, apparently not.



LEO:  Probably some, you know, complex grep that just fails; right?



STEVE:  Yup, it just stumbled; right.  



LEO:  Yeah, yeah.



STEVE:  So a couple of weeks ago we were talking about synchronized Synology NAS boxes and Syncthing and Sync.com and that free Windows app, @MAX SyncUp.  Since I've changed what I'm doing, I wanted to update everyone, correct the record, and share a cool little bit of tech.



I soon recognized that the problem with using that @MAX SyncUp Windows solution was that, while it did provide bidirectional sync, it did not have an active agent running in the local Synology NAS.  So it could not be proactively notified of any changes made there due to the inter-Synology synchronization, which I have between my sites, just as you have between your sites, Leo.  And that turned out to be a problem.



Now, you mentioned, Leo, when we were talking about this that there was a native means for running Syncthing without needing some messy Docker or container encapsulation.  So that sent me looking.  And sure enough, I found it.  As you said, by trusting an additional and very clearly trustworthy source of Synology add-ons, I found a native Synology build of Syncthing.  So Syncthing is now running natively in each of my NAS locations.



LEO:  Woohoo.  Isn't that a great feeling?  Oh, I love it.



STEVE:  Yeah, it is just right.



LEO:  And you set it for, I hope, Receive Only.  You can set a Syncthing folder to Receive Only, or Send and Receive, or Send Only.  And I think that what you want to do on Synology is make it Receive Only, which means it will never delete anything.  So it will only aggregate new versions.  It will never delete old or, if you delete files on one machine, it won't delete them from the backup.  That makes it a backup, a true backup instead of a sync.



STEVE:  Right.  So that's sort of a different purpose than I have.



LEO:  Oh, okay.  You want it to sync sync.



STEVE:  Yes, yes.



LEO:  Yeah, okay.



STEVE:  So I have an ASM tree on my machine in front of me, and an ASM tree on the machine in my other location.



LEO:  Yeah.



STEVE:  And I want those two assembly language code trees to be kept synchronized.



LEO:  Got it.  You want to synchronize deletions as well as new files.  Yeah, yeah, yeah.



STEVE:  Right, right, right.  And so I've got Synology, I mean, I have Syncthing running on my various Windows machines.  It's performing local-only LAN sync.  And then I use the built-in, the inter-Synology NAS sync in order to keep those two Synology NASes the same.



LEO:  Right.  That's a really good solution.  You don't need a cloud now at all, really.



STEVE:  No, no.  And I, you know, I don't trust a cloud.  Okay.  So anyway, so I wanted to share how I'm handling the Internet exposure.  Synology offers lots of "logon to your Synology NAS remotely" functions, and to which I say "thanks, but no thanks."  I get it that that might be the right thing for some people, but it presents too great a vulnerability that can go far too wrong if the bits hit the fan.  As we know, "featuritis is what bites us."  So I have all of that "join your NAS to the happy Synology land community" stuff turned off.  You know, my Synology NASes are little independent islands.  What I want for my NASes is zero third-party and zero public Internet exposure, and it's possible to have that.



At each location, a pfSense firewall is what's facing the Internet.  As I've noted before, pfSense is incredibly handy for performing static port mapping to bypass Cox's consumer port filtering.  But in this case I'm using another nifty feature of pfSense, which is DNS-driven firewall rule updating.  In other words, pfSense's firewall rules can track IP changes through DNS.



Each of the pfSense firewalls opens a single port which can only be connected to by the other endpoint, which is to say I have  explicit IP to explicit IP openings through the pfSense firewalls.  But that means that each endpoint needs to know the other endpoint's IP.  pfSense also has a very capable and mature DynDNS client module.  So I'm using a free DynDNS service to track the IPs of each endpoint for the other endpoint's use.  As I noted, pfSense's firewall filters can be slaved to DNS lookups.  So if either endpoint's IP should change, though that actually doesn't happen very often with a cable modem, that local pfSense instance will note that the IP it's received from the ISP, Cox, has been changed.  So it will use its DynDNS client to update the DynDNS service with the news of its new IP.  The other endpoint's firewall will then automatically update to allow a connection only from that new IP.



And yes, because I'm depending upon DNS, which is not fully secured, this solution is just ever so slightly less secure than the absolutely secure solution of manually configuring the IPs of each endpoint's firewalls.  I could do that if it were absolutely necessary.  But what this protects me from is the primary threat, which is having my use of Synology known in advance from an Internet-wide scan and then being vulnerable to a targeted attack due to any suddenly discovered vulnerability in Synology's software.



So anyway, I just wanted - it's cool that the pfSense can use DynDNS to publish its IP, and that it's also able to use DNS to automatically edit the filter rules in its firewall in order to track the IPs of anything that might be changing that needs to have access in.  And that would also be very cool if you were traveling and wanted to have access into your network only from the IP where you currently were.  So I wanted to share that little tip.  And Leo, thank you for letting me know that I could run Syncthing on my Synologies.  It is, you know, it is absolutely...



LEO:  Yeah, I just think it's such a great program.  And then, you know, I have it on every machine I use.  So it's great.



STEVE:  Yup.  Yup.  And it keeps them all synchronized.



LEO:  And I just, for people who want to use it for backup on a Synology NAS, when you set it up, you get all the folders.  I basically said, you know, my Mac is the introducer, so just whatever it says it wants to share, accept.  But then I go into settings in the folders, and I make them Receive Only.  And according to Syncthing, that means it will still, if there's a change on the Mac, it will go to the Synology, and it will still synchronize that change to other computers.  But if I have a deletion on any computer, it will not delete it on the Synology, which is what I want because it's backup; right?  I want it to have everything ever, because the Synology is huge, and never - because my real fear with any synchronization tool is that you can synchronize deletes.



STEVE:  Right.



LEO:  And if you delete a folder on one machine, and then it deletes it on all the other machines...



STEVE:  It propagates through the whole network.



LEO:  Yeah.  Then it's not a backup.  So it's very important if you want to use the Syncthing as backup.  And I do believe it works fine.  It has for a long time for me.  But if you set it to Receive Only on those folders on the Synology, then they'll be backup folders.



STEVE:  And I don't know if you've looked at it, but Syncthing also has a very nice and mature version tracking system.



LEO:  I use the staggered file system, which is fantastic.



STEVE:  Yup.



LEO:  Yeah, it's really good, yeah.



STEVE:  And, everybody, it's open source and free, so...



LEO:  Free.  It's the best.



STEVE:  And also widely multiplatform.



LEO:  Yeah, yeah.  Yeah, everywhere.  I have it everywhere, yeah.



STEVE:  Okay, so a couple of little Closing the Loop pieces.  George Balogi, he said:  "Hi, Steve.  Long-time listener of the show, and I owe a lot to you for all the IT knowledge I've gained.  Thank you.  Had a question and was wondering if you might know.  Is it possible to read odd file system disks like those found in enterprise-level Xerox machines?"



LEO:  Oh, boy.



STEVE:  He said:  "Trying to prove or disprove a theory of what's left on these drives."  And so, yes.  It is definitely possible to look at and examine raw disk contents independent of any file system.  Under Windows, my favorite piece of freeware is called HxD (capital H, lowercase x, capital D).  The full name is HxD Freeware Hex Editor and Disk Editor.  If you Google that, you'll find it.  I have a link to it in the show notes.  Made by a neat German guy.



If you can arrange to attach that drive to any Windows machine, HxD will allow you to "open" the drive without mounting any file system.  And you then have all of your familiar Windows browsing tools, you know, a scroll bar so you can very rapidly scrub through the drive, a page at a time, search for strings and so on.  And you would immediately be able to determine what was there, depending upon whether it's encrypted or not and so forth.  But anyway, yes, George, absolutely possible.  I would recommend HxD for Windows.  And I think all of our various OSes have some means of allowing you to look at raw disk contents now.



Fairlane, posting as Skynet, he said:  "I'm late watching last week's podcast, but thanks for sharing about the chirping of the mystery smoke detectors."  He said:  "Hilarious but not uncommon that you can't find it.  I've had that happen at home.  You said you didn't have any smoke detectors.  What about carbon monoxide detectors?  We have both, and when I can't find the one chirping, I will take out all the batteries of all of them and then start putting them back in one at a time until I find the one that starts beeping."



LEO:  Oh, that's fun.



STEVE:  So I mention this because it generated a huge thread over in the newsgroup.  It turns out that this is a very common problem.



LEO:  Oh, yeah.  And by the way, we don't know if it's a smoke detector.  It could be a lot of things in your house.



STEVE:  It could.  Actually, I know what it is.



LEO:  Oh, you found it.  Oh, good.



STEVE:  Yeah.  It is a - for a while, well, actually several years ago the drain line on my air conditioning system clogged.



LEO:  Oh.



STEVE:  And that caused the condensate from the inside coils to fill up, back up, and begin running water down into the people below me, and the people below them, because I am at the top of a three-stack.



LEO:  The good news is, when you hear those screams from downstairs, you know who that is.  There's no mystery about it.



STEVE:  So I decided, when this was all fixed, I added my own water leakage detector.  And it's got a 9V battery in it.  And were I to find it and stick my tongue on it, my tongue would  tell me that it is no longer at 9V any longer.  And so that's the thing that is chirping because it's trying to let me know that it's no longer reliably detecting water.  However, the news, I don't need that information because I have since replaced the whole air conditioning system that has a built-in water monitoring and automatic shutoff system so that the water detector was superfluous.  I took it out, and had lost it, and it is busy chirping forever.



LEO:  Oh no.  Oh no.  Oh, jeez.  So it's worse.



STEVE:  So I know what it is.  I still can't find it.



LEO:  It's under something.



STEVE:  Uh-huh.



LEO:  But that's the problem is that frequency for some reason, it's hard to echolocate.



STEVE:  And if it only would like do it for a second, instead of like, what?



LEO:  Yeah, yeah.  And so you can get closer and closer.  But I think because it's such, you know, normally high-pitched frequencies you can echolocate.  But I think because it's so high-pitched it bounces off everything.



STEVE:  Yes.  Wherever I go, it seems like it's somewhere else.



LEO:  It's there.  Yeah.  Yeah, yeah, yeah.  Oh, yeah, yeah, totally awful.  Totally horrible.  By the way, if you ever figure out, I replaced all my carbon monoxide detectors in the house the other day, they have americium in them, which is a radioactive material.  And I've been talking with my friends.  No one can figure out what do you do with this stuff?  You don't want to put it in the landfill.  Nobody takes americium.  The best I could come up with is somebody said, if you have a friend who works at a hospital, they have hazardous radioactive disposal bins.  Get them to put it in that.



STEVE:  Wow.



LEO:  So get your feedback folks to help with that, too.



STEVE:  Okay.  Oh, and we did have another person, Stephen McCalley, said:  "Long-time SN listener.  You probably already got the answer since I'm usually listening a day late, but the military requires wearing the 'Uniform of the Day' while on duty."  He says:  "For most branches that is the working Battle Dress Uniform (BDU)," he says, "which is camo."  So yes, indeed.



LEO:  Yeah.  But again,  the camo is for a jungle.  They have desert camo if you're in the desert.  They should have cyber camo.



STEVE:  It would be really good.



LEO:  I want submissions for cyber camo.



STEVE:  We need cyber camo.  And if we told Alex Lindsay, he'd have a photo of cyber camo by the time we were done.



LEO:  Oh, yeah, he would.  I'm sure.  I'm going to work on it right now, come to think of it.  I have Midjourney, too.



STEVE:  Good.  So Simon Zerafa brings us our "Cringe of the Week."  He said:  "@SGgrc Attended a security conference that offers attendees the option to line-up for a help-yourself lunch.  However, if that queue gets above a certain length, remaining delegates are provided an a la carte dining instead.  This is to prevent Buffet Overflows."



LEO:  Ohhh.  Booo.  That is a dad joke.  Booo.



STEVE:  Yes, the old buffet overflow, when the line to get lunch gets too long.



LEO:  That's terrible.



STEVE:  TWS said:  "Hi, Steve.  During a podcast from a couple of months ago, you mentioned a favorite Authenticator app that you use.  Could you remind me what it was?  Had been using Google Authenticator for many years, but I do not like these new changes."  He didn't tell me what they were, but okay.



LEO:  I don't like them either.



STEVE:  "Thanks for making a great podcast, and of course SpinRite.  Looking forward to the new version."  So it's OTP Auth.  O-T-P space A-U-T-H.  Since Apple's search is crappy, put something in, and what you're looking for is just a simple gray padlock.  It's called OTP Auth.  And it's just a gray padlock.  And I love it.  And again, it's also written by a neat German guy.  I trust those Germans.



LEO:  I will throw in my favorite, which is also open source and free:  2FAS.  And it's iOS and Android.  And as long as we're throwing things in, here are Steve Gibson and Leo Laporte in cyber camo BDUs, according to Midjourney.



STEVE:  My god, this thing works, doesn't it.



LEO:  That was just one of several choices.  Let's see what else we've got here.  Here's a - I kind of like this one.  It doesn't look like us.  Which one do you like?  The red?  The green?  It's definitely...



STEVE:  [Crosstalk] green.



LEO:  Yeah.



STEVE:  I think that's...



LEO:  Yeah, that's the one I upscaled.  I thought that was very attractive.  I don't know what the hood is, but that's your tempest-proof hood.



STEVE:  That's right, it's cold down here in Southern California.



LEO:  Okay, sorry.



STEVE:  Wow.



LEO:  Yeah.



STEVE:  Okay.  So speaking of SpinRite.



LEO:  Yes.



STEVE:  Sunday evening I posted the 33rd Alpha release and noted that it was the first release candidate.



LEO:  Oh, baby.



STEVE:  Yeah.



LEO:  We're getting there.



STEVE:  So this is the first release candidate for the DOS component of SpinRite 6.1.  It appears to be finished.  That release contained SpinRite's new embedded FAQ, and the people who read through it found three hyphen characters that were not displaying correctly.  It showed as a lowercase "u" with an accent over it.  But so far, everything appears to be holding.



This evening, while the newsgroup gang continues to pound on what we now have, I will begin the work of updating SpinRite's Windows app with the new USB formatting InitDisk technology that I started out creating three years ago, basically putting all the pieces together, and that will then move us to actual SpinRite 6.1, which once we know that it's working and tested, will just be replaced on the website, and that's what we'll begin offering.  I'll let everybody here know that at that point that they're able to update their 6.0 with the easily downloadable and installable version.  And Leo. 



LEO:  Yes?



STEVE:  Last Wednesday during Windows Weekly, something, I don't recall what now, caused you to note that for SpinRite's future I had chosen to...



LEO:  Oh, yes.  I was going to ask you about this.



STEVE:  ...finally leave DOS in favor of another operating system.



LEO:  You had been using FreeDOS; right?



STEVE:  Yes, yes.  Okay, so the first exposure many of us had to Intel processors was IBM's PC.  As we know, it contained Intel's 8088 and then 8086 and so on.  But as we also know, Motorola was bidding to have their 68000 processor chosen, and in fact that's what Apple used in their first Macintosh PC.  To this day I still wish that's what IBM had chosen since it was a truly lovely processor.  I mean, it was just a gorgeous architecture, and I wanted to program it, you know, in assembly language.  But that's not what we got.  My point is that Intel didn't create their processors for the PC.  The IBM PC project chose the already existing Intel processor family as the basis for their PC product line.



So that begs the question, what operating system did other early Intel x86 customers use who were not building PCs?  You know, they were building elevators; trains, planes and automobiles; launching telecommunications satellites; and installing commercial HVAC systems.  These things had no screens or keyboard.  They were known as embedded processors, and they did their work mostly without any recognition of any kind.



Well, it turns out that in many cases those companies chose to use a real time operating system which had been created by - I don't know what it is with me and Germany these days, but a German named Peter Petersen.  To give everyone an idea of who has always been using Peter's RTOS-32, just the beginning of his alphabetical customer list reads:  "3M Company, Adaptec, Agilent Technologies, Airbus, Air Force Research Laboratory, Alcatel, AT&T Bell Lab, Audi, Bayer, Blaupunkt, BMW, Boeing, Bosch Telecom, Carl Zeiss, Carnegie Mellon, CERN, Daimler, Deutsche Telekom, Digital Research, Dow Chemicals, DuPont, Ericsson Mobile, ETH Zrich, Ford, Fuji Photo, Goodyear, Hewlett Packard, Honeywell Aerospace, IBM's Research Division, JPL, Lawrence Livermore National Laboratory, Leica," and on.



LEO:  And that's just through the L's, kids.



STEVE: And I skipped a whole bunch of like lesser known, yeah.  Anyway, so you get the idea.  And to that we can now add to that list Gibson Research Corporation.



LEO:  So it's a real-time operating system.



STEVE:  Yes.



LEO:  Okay.



STEVE:  Yes.



LEO:  As opposed to DOS.



STEVE:  Exactly.  This OS predates everything.  Peter has been licensing it to the Who's Who of industrial Intel processor users around the world since the early '90s.  And as Peter explained in his "going out of business" letter at the end of last year...



LEO:  This is what cracks me up.  I was accurate in that; right?  Okay.



STEVE:  Right.  The trouble was it is a finished product.



LEO:  Right, like SpinRite.  It doesn't need to get heavy.



STEVE:  Exactly.  It perfectly interfaced his customers' provided code to their Intel-based hardware.  And what's moreover, over the years he ran out of bugs because it was done.  It was complete.



LEO:  Wow.  That's nice.  Wow.



STEVE:  It was perfect.  There was nothing left to fix.  So his customers, faithful though they were through the decades, they stopped paying for annual maintenance because they never had any problems that needed fixing.  So I purchased it lock, stock, and source.  I own the result of those 30 years of embedded operating system software refinement.



LEO:  Have you looked at the source code just to kind of...



STEVE:  Oh, it's just gorgeous.



LEO:  It's in assembly, obviously; right?



STEVE:  No, it's all in C.



LEO:  Oh, it's all in C, okay.



STEVE:  It's written in C.  Which is fine because all of my code will still be in assembler.  And it is, I mean, it is lean.  16K is the OS overhead.  



LEO:  Wow, wow, wow.  That's amazing.



STEVE:  Yeah.  So, I mean, it's exactly the right thing.  It's able to boot on UEFI or BIOS.  So that's really what I needed.  And it runs, it is its own 32-bit protected mode operating system, and it has cloned a large subset of the original Win32 API, which Paul now pooh-poohs, but it's what I'm still writing to because it's like, you know, the real API.  Anyway, so I cannot wait to get started on SpinRite 7.



LEO:  Wow.  And you don't care that it's out of support because you've got the source.



STEVE:  Yes, it's done.



LEO:  It's done.



STEVE:  It's done.  I got the source code.  And, you know, eventually there will be some mass storage technology that follows NVME.



LEO:  Right.



STEVE:  Because SpinRite 7 will be supporting NVME.



LEO:  Okay.



STEVE:  And so, you know, for SpinRite 9 or 10 I'm going to want to be able to support that.  So I have the source which will allow me to continue to evolve this operating system going forward.



LEO:  Is it still - you still using 18, or is that available on it?  Or is that - that's BIOS, isn't it, the interrupts.



STEVE:  Oh, yeah, yeah.  BIOS is all gone.



LEO:  So you don't have any BIOS dependencies anymore.



STEVE:  Correct.



LEO:  And does it use RTOS to do the disk reads and writes?  Or are you doing that all internally, low level?



STEVE:  I'm not exactly sure.  It does provide a disk interface.  And what's really nice is that it's got file system support.



LEO:  Oh.



STEVE:  For FAT32, for all FATs and for NTFS.  I'll need to add, you know, EXT and so forth for the others.



LEO:  So normally SpinRite doesn't care about file systems; right?



STEVE:  Well, it doesn't today, but that's where it's headed.



LEO:  Yeah, yeah.



STEVE:  You'll be able to tell it I want to recover this file that I can't read.



LEO:  Ah.  That's nice.



STEVE:  Oh, yeah.



LEO:  Oh, I'm looking forward to that.



STEVE:  And why SpinRite the whole drive if only a quarter of it has files on it.  Only SpinRite the files.



LEO:  Yeah.  Interesting.



STEVE:  So, you know, I've got lots of plans for 7 and beyond.



LEO:  What do you use the OS for?  I guess running the program; right?  Loading and running?



STEVE:  Well, yes, exactly.  It sets up the processor.  It handles multiple cores.  Basically it does everything that I don't want to bother doing.  And like, you know, and you know me, I may end up replacing a bunch of it with assembler or fixing it or there.  But it just sort of gives me a start.  Also it's got vast support for network adapters, and it allows cross-network debugging.  So I'll be able to run it on another PC, and it understands Visual Studio.  So I get to run in my very nice Windows debugger mode while I'm actually operating code in the other machine.  And because it runs over the network, if a customer has a problem, I'll be able to debug it on their machine no matter where they are.  So there are some really very cool advantages going forward.



LEO:  Nice.  Nice.



STEVE:  We should take our final break.



LEO:  Oh, yes.



STEVE:  And then we're going to talk about fingerprinting that we cannot get away from.



LEO:  Kind of wish RTOS was our advertiser at this point.  Love to do an ad for RTOS.  So is Peter - what's he going to do?  He's retiring? 



STEVE:  He must be about my age because he's been at this as long as I've been at SpinRite.  And, yeah, I mean, he wrote sort of a sad note.  And he said, "Well, you know..."



LEO:  That's it.



STEVE:  I think what happened is that COVID finally tipped over.  I think companies looked at their bottom line and thought, you know, where can we trim the sails?



LEO:  Right.



STEVE:  And it was like, well, let's just let our license expire because we don't need it anymore.



LEO:  That's why I've lately been begging people to join Club TWiT.  We don't want our license to expire.



STEVE:  Well, and the other thing, too, is that if you were starting today to build an HVAC or a CAT scanner or an IV drip, you would not put an Intel processor in it.  You would put a little ARM chip, and that's what you'd use.  So the problem is no new application is going to be using the old PC architecture.  But I am.  It's perfect for me.



LEO:  There's a few legacies like you still around.



STEVE:  That's right.



LEO:  That's nice.



STEVE:  That's right.



LEO:  Well, I don't want you to retire either, Steve.  So I'm...



STEVE:  No.



LEO:  Now you've got me all excited about SpinRite 7.



STEVE:  It's going to be good.  So at the end of February next year, during NDSS, which is the Network and Distributed System Security Symposium which will be held in San Diego, the recently completed work of a team of six UC Davis researchers will be presented.  Their paper is titled "Centauri:  Practical Rowhammer Fingerprinting."  Another title might have been "The Creation of a DRAM Supercookie."



Here's how they described their findings in their paper's Abstract.  They wrote:  "Fingerprinters leverage the heterogeneity in hardware and software configurations to extract a device fingerprint.  Fingerprinting countermeasures attempt to normalize these attributes such that they present a uniform fingerprint across different devices or present different fingerprints for the same device each time.  We present Centauri, a Rowhammer fingerprinting approach that can build unique and stable fingerprints even across devices with homogeneous or normalized and obfuscated hardware and software configurations.



"To this end, Centauri leverages the process variation in the underlying manufacturing process that gives rise to unique distributions of Rowhammer-induced bit flips across different DRAM modules.  Centauri's design and implementation is able to overcome memory allocation constraints without requiring root privileges.  Our evaluation on a test bed of about 100 DRAM modules shows that Centauri achieves 99.91% fingerprinting accuracy.  Centauri's fingerprints are also stable with daily experiments over a period of 10 days revealing no loss in fingerprinting accuracy.  We show that Centauri is efficient, taking as little as 9.92 seconds, so less than 10 seconds, to extract a fingerprint.  Centauri is the first practical Rowhammer fingerprinting approach that's able to extract unique and stable fingerprints efficiently and at scale."



Okay.  So this is some brilliant work.  And it's one of those discoveries that's immediately obvious in retrospect.  Rowhammering is a subject that's come up over and over through the years for us.  What we know about it is that, in order to obtain maximum storage densities from our system's dynamic RAM, the storage cells have been shrunk, and the number per unit area has grown.  And effectively any margin for error has been deemed too costly and has thus been eliminated.  What we wind up with is main system memory that can be pushed over the edge through deliberate abuse, where in this case "abuse" amounts to just hammering on one memory address which can, with distressing success, cause adjacent memory bits to spontaneously flip from a zero to a one or a one to a zero.



This breaks all of the rules since there's nothing inherently wrong with reading one memory address over and over.  So through the years we've seen this unwelcome consequence of too much data being stored in too little space being very cleverly leveraged in many different ways to breach the protective hardware-enforced barriers isolating virtual machines from each other or imbuing unprivileged processes with full root kernel privilege.



And now today we have another consequence of this aggressive hardware engineering, the observation that tiny variations in the manufacturing of today's DRAM chips allow them to be uniquely identified in the field, thus enabling indelible fingerprinting.  Where previous Rowhammer researchers needed to search physical DRAM to find a location whose bits could be flipped, these UC Davis researchers realized that identifying the exact location of such flipping, and of which bits were flipped in which direction, produced what could be used to uniquely and indelibly identify one specific piece of DRAM out of the multitude, and that would never change throughout the service life of that storage device.



So here's how they frame their accomplishment in somewhat greater detail.  They said:  "In this work, we investigate a stronger threat model where a fingerprinter aims to extract unique and stable fingerprints for devices with identical hardware and software configurations over extended periods of time.  To this end, we aim to capture fundamental differences in the physical properties of the device's hardware as unique fingerprints.



"Our key insight is that a fingerprinter may be able to extract fingerprints from inherent differences that arise as a result of minute process variations in the hardware CMOS manufacturing process.  As users seldom modify their device hardware, these fingerprints remain stable, as long as they account for differences resulting from process variation in the same hardware.  While prior research has explored variations in internal clocks, GPUs and CPUs, we're the first to successfully leverage memory (DRAM) for fingerprinting."



You know, I was thinking about this.  Remember back in the early days of hard drives where like the MFM drives came with a printout of where the defects had been located in the factory, and you were supposed to enter those defects into the low-level format so that those sectors would be marked bad from the start.  Well, back then no one was thinking about fingerprinting anything.  But those defects were a fingerprint.



LEO:  Sure.  They're unique.



STEVE:  For the drive.



LEO:  Yeah.



STEVE:  Yes.  Because you were never going to have the same map on two different drives because they were physical surface blemishes that the manufacturing process was unable to, like, not to have any of.  So this is the same thing.  These are defects that are not bad enough to disqualify the DRAM from use, but they are bad enough to uniquely identify a specific DRAM chip from all others.



So they said:  "We leverage Rowhammer to extract fingerprints by capturing the side-effects of process variation in memory modules.  At a high level, 'hammering' a memory row - in other words, repeated read or write operations in a short time interval - results in bit flips in adjacent memory rows.  In this paper, we show that the pattern of bit flips due to Rowhammer can be leveraged to build a fingerprint.  We also show that the pattern of Rowhammer bit flips is sufficiently unique and stable to build a reliable fingerprint for the population of computing devices, billions of devices.



"To build intuition, we visualized the distribution of bit flips produced by executing Rowhammer at the same locations on two identical DRAM modules at two different points in time.  And the results looked promising.  The distribution of bit flips was reasonably similar on the same DRAM modules at different points in time while being noticeably different across the pair of modules.  So that was our starting point.



"Centauri is a practical Rowhammer-based fingerprinting approach that exploits bit flip distributions to extract highly unique and stable fingerprints even among homogeneous devices with identical software and hardware configurations," meaning nothing else would allow them to be fingerprinted.  They are essentially identical, but they're not using all the exact same DRAM chips.  They're using their own DRAM chips over an extended period of time.



"Centauri overcomes three main challenges that make it then practical for fingerprinting:  First, the bit flips triggered by Rowhammer are non-deterministic; in other words, hammering the same location does not flip the same set of bits.  Thus a fingerprinter has to account for this non-determinism to extract stable fingerprints.  We identify certain practical scenarios that exacerbate this non-determinism where comparing set similarity to match fingerprints falls short.



"With Centauri, we hammer the same locations multiple times to extract a probability distribution of bit flips as fingerprints.  We then compare the divergence of these distributions that leads to better re-identification of devices, even where there is a drastic difference in the specific set of bits flipped."  In other words, they deal with the fact that they're actually dealing with frequency distributions as opposed to specific bits flipped.



They said:  "Second, fingerprinters are constrained by the abstractions provided by the operating system to allocate memory."  Right.  Like virtual memory, the memory that an app sees is not the actual underlying physical memory due to the page tables that exist between.  So they say:  "These abstractions provide limited access to contiguous physical memory, and hide information about their allocation on the DRAM.  Without root privileges, these constraints prevent fingerprinters from trivially tracking the location of bit flips to fingerprint devices.



"We use the insight from our measurement study that the distribution of bit flips in contiguous 2MB chunks of memory is unique and persistent to overcome this challenge.  Armed with this insight, we sample enough 2MB chunks to guarantee access to the same chunk for fingerprinting."  And that is, I think, quite clever, and a bit chilling since it means that this can actually be accomplished without privileges, like by a web browser.



"Third, memory modules implement mitigations against Rowhammer, such as Target Row Refresh," which we've talked about before.  They said:  "While prior research has demonstrated ways to craft hammering patterns to bypass TRR, they provide limited insights towards operationalizing them to trigger bit flips at scale.  Centauri systematically identifies effective patterns for at-scale fingerprinting using Rowhammer.



"We then evaluate Centauri on a set of 98 DIMMs across six sets of identical DRAM modules across two major DRAM manufacturers. Centauri produces high entropy," meaning many significant bits of fingerprinting precision, which you want if you're going to identify a single device among billions.  And, you know, if you've got 32 bits of entropy, well, we know that's 4.3 billion devices.  And they said:  "...high entropy, with a highest fingerprint accuracy of 99.91% corresponding to a precision across that population of 98 DIMMs of 100% precision, and recall of 97.06%.  Centauri also demonstrates high stability with daily experiments to extract fingerprints from the same devices over a period of 10 days without any degradation in fingerprint accuracy.  Our experiments show that Centauri only suffers a minor loss in accuracy of 0.9% in presence of external factors that are not under the control of fingerprinters" - it might be something like temperature variation, for example - "but affect the distribution of bit flips such as the CPU frequency," they said.



"We also investigate the trade-off between the accuracy of Centauri's fingerprints against the efficiency of Centauri's approach in terms of the time taken to extract fingerprints.  Centauri is able to extract a fingerprint in as little as 9.92 seconds" - so just shy of 10 seconds, as I mentioned before - "reducing the overhead by more than 95.01% while degrading accuracy by just 0.64%."



And they finished:  "Therefore, our key contributions include" - there are four things.  "Practically extracting highly unique and stable fingerprints using Rowhammer.  We practically demonstrate Centauri on the largest scale of DRAM modules in current literature.  Second, handling non-deterministic bit flips.  We handle non-deterministic bit flips by hammering the same memory chunks multiple times and using the divergence between probability distributions of bit flips to re-identify the same devices when they're seen a second time.



"Third, overcoming memory allocation constraints.  We overcome memory allocation constraints by devising a novel sampling strategy" - that is, the 2MB chunks - "that guarantees access to the same chunk of memory for fingerprinting.  And finally, operationalizing bypass techniques for Rowhammer mitigations.  We bypass Rowhammer mitigations, like target row refresh, by identifying effective hammering patterns that can trigger bit flips at scale."



So their paper is 16 pages long, and I just shared an edited-down summary from about the first page and a half.  Through the balance of their document they proceed to explain in absolutely complete detail how they pulled this off and how anyone else could, as well.  There's no mystery left.  Anyone who is sufficiently skilled and motivated who wanted to implement an indelible tracker for anything that uses DRAM, and I'll just note that's everything, now has all the information required to duplicate this technology.  So Rowhammer has struck again.



LEO:  You know, in a way it's ironic because the original Rowhammer was kind of non-trivial to implement; right?



STEVE:  Right.



LEO:  You had to be pretty sophisticated.  This is easier to do.



STEVE:  Yeah, you're right, because Rowhammer depended on exact bit flips in order to get what you wanted to change.



LEO:  Right.



STEVE:  Here, they just look at, like, a big spread of what got flipped and go, okay, just remember that.



LEO:  So could you do this in JavaScript in a browser?



STEVE:  Yes.



LEO:  Oy.  Thanks for publishing that article.



STEVE:  Yeah.



LEO:  Now everybody knows.  Because they've been looking for indelible ways to fingerprint individuals for a long time.  I mean, obviously cookies are no longer the way to do it.  And this is why the cookie banner just pisses the hell out of me, because it's identifying something that's not a threat, when real threats like this exist.



STEVE:  Yes.



LEO:  I mean, this is very straightforward, it sounds like.  Oh, well.



STEVE:  Yeah.  It's going to end up, you know.  It'll be implemented in WebAssem because there you can do it at full speed, and it'll be a WebAssem module that websites load into your browser and run in order to see where you are.



LEO:  Who are you?  Oh, hi, Steve.  Good to see you again.



STEVE:  Uh-huh.  Uh-huh.



LEO:  Yeah.  You can block all the cookies you want, buddy.  We don't care.



STEVE:  Yeah.  And in fact when you think about it, we've been thinking in terms of fingerprinting browsers.  But if you ran someone's app in your machine, like Instagram Desktop or some other app, the app could also - would obtain the same fingerprint.



LEO:  The machine is unique, yeah.



STEVE:  Yes.



LEO:  Not the browser.



STEVE:  The machine.



LEO:  The machine is unique, yeah.



STEVE:  Exactly.  So it's cross-browser and cross-app identification.



LEO:  Nice.



STEVE:  Yeah.



LEO:  Well, it's not that surprising.  It's great work.  I mean, it's amazing that they thought of this.  And I love the analogy you used because now I completely understand what they're doing.  You know, that makes sense.



Mr. Gibson, as always, chef's kiss.  This show is a regular listen for everybody who wants to know what's happening in the world of computing, especially in the world of security.



STEVE:  Bye.



Copyright (c) 2023 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.






GIBSON RESEARCH CORPORATION



SERIES:		SECURITY NOW!

EPISODE:	#931

DATE:		JULY 18, 2023	

TITLE:		SATELLITE INSECURITY, PART 1

HOSTS:	STEVE GIBSON & LEO LAPORTE

SOURCE:	SN-931.MP3

LENGTH:	100 MINUTES



DESCRIPTION:  What did Kaspersky have to say about last Tuesday's Microsoft patch event, and what security consequences does it have for all non-subscribing Microsoft Office users?  What was inevitably going to happen once the power of Large Language Model generative AI became widely appreciated and available?  What does it mean that Microsoft just revoked more than 100 malicious Windows drivers?  What two new well-known companies have been added to Clop's MOVEit file transfer victim list?  What does Dun & Bradstreet have to do with Android Apps?  Where in the world can you use Meta's new Threads service, and where not?  And what's a side effect of bitcoin addresses looking like gibberish?  And after we examine those questions, cover some miscellany and user feedback, we're going to turn our attention to the heavens in recollection of those famous words of Henny Penny.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here with a stunning flaw in Microsoft.  They say it's a feature, not a bug, and it's been around since 2011.  You'll be interested in what Kaspersky has to say about all that.  Microsoft also just revoked 100 malicious Window drivers.  Wow.  Why can't you use Threads in Europe?  And then a look, Part 1 of our look at satellite security, or should we say insecurity?  It's next with Steve Gibson on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 931, recorded Tuesday, July 18th, 2023:  Satellite Insecurity, Part 1.



It's time for Security Now!.  I know you've been waiting all week long.  Finally, Tuesday's here.  Security Now!'s on the air.  And there he is, Steve Gibson, the star of our show.  Hi, Steve.



STEVE GIBSON:  Yo, Leo.  Great to be with you again.



LEO:  You know your Wikipedia calls you Steve "Tiberius" Gibson.



STEVE:  I think it knows that's not the case.



LEO:  Hope so.  Every once in a while I like to call you that, since you are a Star Trek fan.



STEVE:  Indeed.



LEO:  What's up?	



STEVE:  So an interesting bit of news about a paper that was submitted and accepted and presented at a recent IEEE security symposium about two months ago in May caught my eye because it's something in our 18-plus years of the podcast we've never talked about.  We've always been talking about it on the security, you know, fiber optic cables and stuff on the ground.  We've never looked up.  And today's topic, it started off just to be today.  It ended up being today and next week.  So this is Satellite Insecurity Part 1 for today.



LEO:  Hmm, interesting.



STEVE:  Really interesting.  And not surprisingly, things are not good up there in the friendly skies.  But anyway, we've got a lot to talk about.  We're going to look at what Kaspersky had to say about last Tuesday's Microsoft patch event, and what security consequences it has for all nonsubscribing Microsoft Office users.  Also, what was inevitably going to happen once the power of Large Language Model generative AI became widely appreciated and available?  What does it mean that Microsoft just revoked more than 100 malicious Windows drivers?  100.  More than.  What two well-known companies have been added to Clop's MOVEit file transfer victim list?



LEO:  Uh-oh.



STEVE:  What does Dun & Bradstreet have to do with Android Apps?  Where in the world can you use Meta's new Threads service, and where not?  And what's a side effect of bitcoin addresses looking like gibberish?  And after we examine those questions, cover some miscellany and user feedback, we're going to turn our attention, as I said, to the heavens, in recollection of those famous words of Henny Penny.



LEO:  The sky is falling?



STEVE:  Ah, yes.



LEO:  Good old Henny Penny.



STEVE:  Henny Penny.



LEO:  She wasn't wrong.  She wasn't wrong.



STEVE:  No.  And we do have a great Picture of the Week which has already generated some laughter among the Twitter people who saw it earlier.



LEO:  I have not looked.  My new policy is to only...



STEVE:  That's good.



LEO:  Yeah, I increase the size of the show notes to such a point that the picture's below the fold.



STEVE:  Nice.



LEO:  So I will scroll it up and share it with you.  I shall scroll up to see the Picture of the Week.  I don't know what it means, but I like it.  Will you explain this to me, Steve?



STEVE:  So the caption I gave this is "Insecure Parking Spaces - Lock Your Car."  And what we're looking at is something that one of our listeners, bless their hearts, you know, being a listener to the podcast, saw and thought, oh, my god, I've got to take a picture of this and send it to Steve.



LEO:  Wow.



STEVE:  So you know how parking slots are often labeled with, like, who's allowed to park in there, like Denny's Parking Only kind of thing, if there's some movie theater next to it, and Denny's is upset because people who are going to the movies are parking in their slots.



LEO:  I hate it when that happens, yeah.



STEVE:  So these...



LEO:  Worse, I'm at Denny's all the time, and I want to park and have my fried egg and get on out of there.  So yes.  Yes.



STEVE:  That's right.  You don't want those...



LEO:  Yes.  Those movie people.  Get them out of there.



STEVE:  ...movie people taking up your spot.  That's right.  So here we have stenciled on parking spaces who is allowed to park in them.  And I kid you not, I don't know what the initials stand for, but it says "HTTP PARKING ONLY."  So of course...



LEO:  So no insecure...



STEVE:  That's right, those are insecure parking places.



LEO:  Is there an HTTPS next to it?  That's the question.  One I can use.



STEVE:  You really want to lock your car.



LEO:  Yeah, that's hysterical.  I love it.  So obviously that's a company of some kind.



STEVE:  Yeah, exactly.  It's some company's initials.  So that, you know, everybody who would be wanting to park there would realize, oh, wait, that's - I can't park in this spot.  That's reserved for HTTP.  Which, you know, means something to us.



LEO:  Yes.



STEVE:  Okay.  So Kaspersky being Kaspersky, a very technologically savvy security firm, had an interesting take on last Tuesday's monthly Microsoft patch event.  It was the heading on their posting that first drew me in.  They titled their posting, that is, Kaspersky did, "Band-Aid on a... corpse," which, you know, is not the way you want to start describing Patch Tuesday, right, "Band-Aid on a corpse."



They said:  "Microsoft patches IE again," and their subhead was "July Microsoft Patch Tuesday:  A collection of exploited vulnerabilities."  So this is all definitely worth sharing as we look back at the past week.  We often do a retrospective on Patch Tuesday.  Today we're going to start with what Kaspersky had to say, and then I'm going to, you know, flesh it out a little bit.



So Kaspersky wrote:  "The Microsoft July patch collection has turned out to be a quite surprising event.  First, they're once again fixing apparently dead Internet Explorer.  Second, as many as six of the vulnerabilities are already being actively exploited by attackers."  In other words, six zero-days in last  Tuesday's patch batch.  They said:  "Third, two of those six actively exploited vulnerabilities were closed, not with patches, but with recommendations."  And that's what we're going to end up talking about because this is a little distressing.



So they said:  "Here are the total statistics:  132 flaws were closed," making it one of the larger ones.  And this is, you know, every month this happens.  "Nine of which are considered critical.  Exploitation of 37 of those vulnerabilities can lead to arbitrary code execution, 33 to privilege elevation, 13 to security feature bypasses, and 22 to possible denial of service."



They said:  "Not so long ago we wrote that Internet Explorer had kicked the bucket, but not quite.  In particular, we talked about Microsoft's advice to continue installing security updates related to IE, since some of its components are still in the system.  And now it becomes clear why they gave this advice.  The July patch closes as many as three vulnerabilities in MSHTML, the engine inside the legendary browser.  In the CVE descriptions, Microsoft states the following."



So Microsoft said:  "While Microsoft has announced retirement of the Internet Explorer 11 application on certain platforms" - and I'm thinking, wait, are there any platforms where it hasn't been retired?  I don't think so.  And, they said, Microsoft said:  "The Microsoft Edge Legacy application is deprecated."  Right, remember Edge started using MSHTML and then switched to Chrome, or the Chromium engine.



So they said:  "The Microsoft legacy application is deprecated.  The underlying MSHTML, EdgeHTML, and scripting platforms are still all supported.  The MSHTML platform is used by Internet Explorer mode in Microsoft Edge, as well as other applications through WebBrowser control.  The EdgeHTML platform is used by WebView and some UWP applications.  The scripting platforms are used by MSHTML and EdgeHTML, but can also be used by other legacy applications."  In other words, they wish it were dead, but it's just too deeply wired into Windows to actually go away.  And it's got some problems.



So they said:  "Updates to address vulnerabilities in the MSHTML platform and scripting engine are included in the IE Cumulative Updates; EdgeHTML and Chakra changes are not applicable to those platforms.  To stay fully protected, we recommend that customers who install Security Only updates install the IE Cumulative updates."  And yes, I second that advice.



Okay.  So back to Kaspersky, who says:  "The most dangerous of the freshly discovered IE vulnerabilities is CVE-2023-32046."  And of course all the CVEs I'll be talking about are 2023, so I'm just not going to be saying that every time.  Anyway, 32046.  They said:  "And it's already being used in real attacks.  Its successful exploitation allows cybercriminals to elevate their privileges to those of the victim.  Attack scenarios involve the creation of a malicious file that's sent to the victim by email or hosted on a compromised website."  So, you know, opening a file, bang.  "All attackers need then is to convince the user to follow the link and open the file.



"The remaining two vulnerabilities, 35308 and 35336, can be used to bypass security features.  The first allows a cybercriminal to create a file bypassing the Mark-of-the-Web mechanism so that the file can be opened by Microsoft Office applications without Protected View mode.  And both holes can be used to trick a victim into accessing a URL in a less restrictive Internet Security Zone than was intended.



"The next two vulnerabilities are also being actively exploited; but instead of full-fledged patches, they've only received security recommendations."  And this is the, woo, we're going to be spending some time on this because this is a little surprising.  They wrote:  "The first one, 36884, with CVSS rating 8.3, is being exploited in the Storm-0978/RomCom remote code execution attacks on both Office and Windows.  To stay safe, Microsoft advises adding all Office executables to the FEATURE_BLOCK_CROSS_PROTOCOL_FILE_NAVIGATION list."  Okay, that's actually the name of a registry key.  So Feature Block Cross Protocol File Navigation.  And we'll be coming back to that, as I said, and have a lot more to say about that in a minute.



Kaspersky continues:  "The second unresolved issue" - and again, what I just talked about is like a problem that Microsoft has chosen not to fix, and we'll explain why.  They said:  "The second unresolved issue relates to the signing of kernel-level drivers.  This one doesn't have a CVE index," Kaspersky says, "but only a guide with recommendations.  Microsoft revoked a bunch of developer certificates used in Advanced Persistent Threat attacks and blocked several malicious drivers, but the root of the problem remained.  Hackers still manage to sign drivers with Microsoft certificates, or sign them backdated to make them work as one of the exceptions and not require the MS developer portal signature."  And this is something we've been talking about, right, how Microsoft really has a problem with driver signing.



They said:  "As a countermeasure, Microsoft recommends keeping both Windows and EDR" - that's the endpoint security - "up to date.  The only small consolation is that in order to exploit such drivers, the attacker must have admin privileges."  On the other hand, if you've got privilege elevation exploits wandering around, like in IE, that may not be difficult. 



"Besides the above-mentioned vulnerabilities there are three more holes that are already being exploited by cybercriminals.  We've got 32049, a SmartScreen security feature bypass vulnerability.  Its exploitation allows attackers to create a file that opens without displaying the Windows warning 'downloaded from the Internet.'



"We've got 36874, a privilege escalation vulnerability in the Windows Error reporting service.  That allows attackers to elevate privileges if they already have normal permissions to create folders and technical performance monitoring files."



Finally, "35311.  It's a security feature bypass vulnerability in Outlook.  Its exploitation helps cybercriminals avoid showing warnings when using preview."  And of course as we know, clever attacks actually do use these things in order to slip past users even when they're trained up and are wary.



Okay.  So on balance, we got a bumper crop of 132 total patches this month, nine being critical, 37 allowing for arbitrary code execution, six being actively exploited in the wild as true zero-days.  One of those zero-days being actively exploited in the wild right now was that 36884 - that's the one carrying the CVSS, which is pretty high, of 8.3 - being exploited in a phishing campaign being conducted by a group designated as Storm-0978.  What's got people stirred up is that, despite this being actively exploited in the wild, and having been identified as a zero-day, Microsoft has not patched it, and they appear unlikely to do so.  The reason is that this phishing campaign is using a feature, not a bug.  Were it to be disabled for security, Microsoft is afraid that might break too many existing things.  And so they're afraid to turn it off.



Now, this is one of those things, and we've encountered them before, which Microsoft should have turned off a long time ago, in which case this would have never been a problem.  Or better yet, should have never made possible in the first place.  In which cases developers would have found, like legitimate developers, would have found some other safer way to do the same thing.  But no.  It's like scripting in email.  What could possibly go wrong?



Okay, so what does Microsoft have to say about all this?  Microsoft's posting of July 11th, right, so that was Patch Tuesday, last week, Microsoft posted about this in a separate posting titled "Storm-0978 attacks reveal financial and espionage motives."  So I thought, okay, as I was digging into this, what's going on here?



Microsoft said:  "Microsoft has identified a phishing campaign conducted by the threat actor tracked as Storm-0978 targeting defense and government entities in Europe and North America.  The campaign involved the abuse of CVE-2023-36884, which included a remote code execution vulnerability exploited before disclosure to Microsoft" - in other words, a zero-day; right?  It was "exploited before disclosure to Microsoft via Word documents, using lures related to the Ukrainian World Congress."



"Storm-0978 - also they have DEV-0978, also referred to as RomCom, the name of their backdoor, by other vendors - is a cybercriminal group based out of Russia, known to conduct opportunistic ransomware and extortion-only operations, as well as targeted credential-gathering campaigns likely in support of intelligence operations.  Storm-0978 operates, develops, and distributes the RomCom backdoor.  The actor also deploys the 'Underground' ransomware, which is closely related to the Industrial Spy ransomware first observed in the wild in May of 2022.  The actor's latest campaign detected in June of 2023" - so just last month - "involved abuse of this exploit, 36884, to deliver a backdoor with similarities to RomCom.



"Storm-0978 is known to target organizations with trojanized versions of popular legitimate software, leading to the installation of RomCom.  Storm-0978's targeted operations have impacted government and military organizations primarily in Ukraine, as well as organizations in Europe and North America potentially involved in Ukrainian affairs.  Identified ransomware attacks have impacted the telecommunications and finance industries, among others."



Okay.  So now we get to the good part of this, after that background.  Microsoft 365 Defender detects multiple stages of Storm-0978 activity.  Customers who use Microsoft Defender for Office 365 are protected from attachments that attempt to exploit 36884.  In addition, customers who use Microsoft 365 Apps versions 2302 and later are protected from exploitation of the vulnerability via Office.  Organizations who cannot take advantage of these protections can set the FEATURE_BLOCK_CROSS_PROTOCOL_FILE_NAVIGATION registry key to avoid exploitation.



In Microsoft's posting from last week, that Registry key was highlighted and underlined like a link.  And sure enough it was a link, so I clicked it.  Where did it take me?  It jumped me to a page, and the link used the pound sign suffix to preposition me a ways down the page to a specific section which they wanted to refer to.  And that prevented me from initially seeing the title of the page.  The section of the page I was jumped to was titled "New restrictions on use of the file:// Protocol."  So, of course, I thought, whoa.



LEO:  Oh, I can't believe this.  That's still around?



STEVE:  I know.  I know, Leo.  I thought, so, like whoa, that's what we're talking about here?  And it's being exploited in a zero-day today?



LEO:  Oh, lord.



STEVE:  We're talking about bad guys leveraging the file:// scheme to arrange to run programs on the user's machine from Office documents.  And that thought was followed by, wait.  Exactly as you said, Leo.  That's still possible?  So then I started to read what Microsoft wrote on this page that had been linked to by their posting from last Tuesday.  And there they wrote:  "Prior to this update, Internet Explorer would allow non-file-protocol (i.e., HTTP and HTTPS) delivered pages to frame" - in other words, using an Iframe - "or navigate to pages that were delivered using the file:// protocol scheme.  IE would only block loading of resources from the local computer, for example, file:///C:/temp/test.gif, for example.  But resources from non-local paths would be allowed."  And then they said:  "Here's an example page displayed in IE 9.0.1."



And I thought, IE 9?  So, I finally scrolled up to the top of the page to see what in the world I was reading, and it was from Microsoft, posted on August 12th of 2011.  Yes, 12 years ago, titled "Internet Explorer 9.0.2 Update."



LEO:  Oh my god.  Oh my god.



STEVE:  And sure enough, they showed where IE 9.0.1 - apparently just like Office apps today - will load an Iframe with text content provided by the file:// scheme from, for example, live.sysinternals.com was the example they gave from 12 years ago.  And then they show the same thing done under the new and improved IE 9.0.2 and, what do you know, by golly, that Iframe, is empty.  Then they note:  "Other browsers have blocked cross-protocol interactions for quite some time.  Here are screenshots of Firefox 5, you know, that browser from times past; Chrome 14; and Opera 11.5 developer consoles in this same scenario."



Okay.  So just to make sure that everyone is on the same page here, this Russia-located Storm-0978 phishing campaign has been successfully installing trojan code into unsuspecting Office users' machines, using a technique that IE 9.0.2 celebrated ending in August of 2011.



LEO:  2011.



STEVE:  Noting at the time that everyone else had already done that.  Yet, just last week Microsoft wrote, and I'm quoting them again:  "The campaign involved the abuse of 36884, which included a remote code execution vulnerability exploited before disclosure to Microsoft via Word documents."  Although they had disclosed it to themselves in August of 2011.  So they've known about it since IE 9, finally decided to fix it, and even then it was the last of the bunch to do so.



So it came back.  They turned it back on for some reason in Office.  Who knows when?  But now Microsoft is afraid to turn it off again, despite the fact that it's being abused in a trojan-installing remote code execution vulnerability in their own Office documents.  They can't turn it off because they have no way to predict what doing so might cause to break.  So they're not going to make it their problem.  Unless you're using their online subscription Office stuff, in which case they'll protect you from it.  But if not, it's up to you.  So there's a registry key which will allow anyone and everyone to turn off this behavior which is currently under active abuse, apparently by Russians, to install malware into the computers of unsuspecting link clickers.



I've got a picture from a registry snapshot showing this.  The key is HKEY_LOCAL_MACHINE\SOFTWARE\Policies\Microsoft\Internet Explorer\Main\FeatureControl\FEATURE_BLOCK_CROSS_PROTOCOL_NAVIGATION.  And under that key it's necessary to enumerate each of the various Microsoft apps whose behavior, in this case, you would like to restore to Internet Explorer 9.0.2 where this was originally fixed 12 years ago.  I also have a screenshot of the registry showing the enumeration of the REG_DWORD values under that key.  And Leo, thank you, it's on the screen right now.



LEO:  So what are you supposed to change it to?



STEVE:  Well, you have to add all of that.  None of that there now.



LEO:  Oh, no.  Okay.  All right.



STEVE:  Yeah, you've got to put all that in.  And, now, this...



LEO:  Isn't it possible to make a regedit file that you just double-click, and it does it?



STEVE:  Absolutely.  Regedit file, and it would just be a simple one-click, and it would do that.  Now, as I said, this has stirred up a bunch on the 'Net because Microsoft is literally not fixing something that is in active abuse right now as a remote code execution vulnerability.  They can't fix it because it will break things that developers are depending upon.



LEO:  Yeah.  You should break it.



STEVE:  Yes, you should break it.



LEO:  Frankly, it's a stupid thing to have had in the first place.



STEVE:  It is incredibly, as I said, like scripting in email, what could possibly go wrong?  The problem is all over the 'Net you will now find scripts that are doing this.  Unfortunately, they spell "PowerPoint" all the way out, and that's incorrect.  It needs to be Powerpnt.exe.  So just a heads-up.  If you do use these scripts, you'll see that they have PowerPoint.exe.  It needs to be Powerpnt.exe, the actual name of the EXE, that component of Office.  So anyway, it's crazy that they've done this.  Who knows when this came back into use.  The bad guys found it and said, oh, that's nice, just like Internet Explorer 9.0.1.



LEO:  It's 2011 all over again.



STEVE:  We can go back to our old...



LEO:  Oh, my god.



STEVE:  Unbelievable.  Unbelievable.  Okay.  So I suppose it was inevitable, though it happened sooner than I would have expected.  The underground now has a ChatGPT-style generative AI all their own, without any of the abuse prevention built into the front end that is in ChatGPT.  It is known, and I kid you not, as WormGPT and it exists.  The news of this comes from a reformed black hat computer hacker named Daniel Kelley who collaborated with the team at the business email and messaging protection security firm SlashNext.  Daniel begins his posting by providing a background about the use of legitimate generative AI like ChatGPT and discusses, as we have here, the fact that such AI can be hugely useful to bad guys when they're able to coerce it or seduce it into giving them what they want, meaning ChatGPT, which is trying not to.  But now it appears this will no longer be necessary.



Daniel explains in his posting, he said:  "We recently gained access to a tool known as 'WormGPT' through a prominent online forum that's often associated with cybercrime.  This tool presents itself as a black hat alternative to GPT models, designed specifically for malicious activities.  WormGPT is an AI based on the GPT-J language model, which was developed in 2021.  It boasts a range of features, including unlimited character support, chat memory retention, and code formatting capabilities.  WormGPT was allegedly trained on a diverse array of data sources, particularly concentrating on malware-related data.  However, the specific datasets utilized during the training process remain confidential, known only to the tool's author and publisher.



"We conducted tests focused on Business Email Compromise, you know, BEC attacks, to comprehensively assess the potential dangers associated once WormGPT, or similar tools, become more widely available and well known. In one experiment, we instructed WormGPT to generate an email intended to pressure an unsuspecting account manager into paying a fraudulent invoice.  The results were unsettling.  WormGPT produced an email that was not only remarkably persuasive, but also strategically cunning, showcasing its potential for sophisticated phishing and BEC attacks.



"While appearing largely similar to ChatGPT, WormGPT is deliberately unbounded by any ethical boundaries or limitations.  It will answer any question asked, will generate any form of document required, and will author any type of malware requested.  This experiment underscores the significant threat posed by generative AI technologies like WormGPT, even in the hands of novice cybercriminals.  It renders them immediately far less novice in their presentation and skills.



"Generative AI can produce emails with impeccable grammar, making them appear significantly more legitimate and reducing the likelihood of being flagged as suspicious.  And the use of generative AI enables the execution of much more sophisticated BEC attacks than could have been launched before.  Even attackers with limited skills and inability to use the target's language can now use this technology, making it an accessible tool for a broader spectrum of cybercriminals."  And Leo, as I said, this happened sooner than I expected; but in retrospect, of course.



LEO:  Do you know what the quality of the code is?  I mean, so far the code we've seen generated by other LLMs has not been superb.



STEVE:  Well, it's not been bug-free in the same way that you ask it to generate...



LEO:  Well, worse than that, it's been kind of trivial.  So it's not, I mean, in other words, there are plenty of people with the skills to write this code themselves.  It just enables people who don't even have those skills to create some...



STEVE:  Right.  And so we would argue that this code was trained on code that was written by skilled people, and it is just regurgitating it.  On the other hand, it is often producing credible code.  And I think what we can expect to see is this will only get better going forward.  So anyway, I guess the point is we've often joked at like ransom notices' poor grammar.  And you see, you know, if you bother to read spam, it's often obviously spammy.



LEO:  Oh, yeah.  Oh, yeah.



STEVE:  Well, we can expect that to go away now because it will be easy to dump this through a large language model trained up I the target language, and it will clean up the misspellings and the bad grammar and make spam now become indistinguishable from legitimate email.  So on our radar.



Microsoft revoked more than 100 malicious drivers.  And when you first encounter the headline "Microsoft revokes more than 100 malicious drivers," you know, that seems like great news; right?  Whew, 100 fewer malicious drivers now.  But then you stop and think, wait a minute.  Before they did that, there were 100 additional malicious drivers floating around?  And if there were that many more, then isn't this going to be just like bugs, where we're never going to run out of them?



And of course malicious drivers could do anything they want with the system.  And that's not good.  And then we recall that, historically, Microsoft's track record of keeping these malicious driver lists up to date has been, shall we say, a bit less than stellar?  Like didn't we catch them for two years like not bothering to update the list, and then going, oh, yeah, it's like, and then saying that they were going to, but even then they didn't, as I recall from a prior podcast. 



The problem is that all of the evidence suggests that there are far too many ways to get around Microsoft's driver signing.  Bad guys apparently have no trouble doing it.  Kernel driver signing apparently poses a much greater inconvenience for the good guys than it does for the bad guys, who simply arrange somehow to run a bypass.  And in fairness, this isn't really Microsoft's fault, at least not today.  They're still stuck with the original design from Windows NT.



Now, consider that Windows NT was first released, and the architecture was in place, in late July of 1993.  So July of '93, almost exactly 30 years ago, when the world, as I've often said, was a very different place.  Consider that Netscape didn't invent SSL until two years after that in 1995.  So, yeah, a very different world 30 years ago.  So NT's architecture, which considers peripheral drivers to be trusted peers running alongside it in ring 0, that architecture did not foresee, and could not really have foreseen, the degree to which unknown and untrusted third parties would be creating what amount to kernel extensions.  It should not be necessary to fully trust some random printer driver to the same degree as Microsoft's own kernel code.



But the architecture of Windows NT, which is what we're still living with today, makes what has turned out to be a very poor assumption about the trustworthiness of drivers.  Drivers are sacred.  They were designed that way.  They're meant to be.  But now everybody just includes them in random things that you install.  And, you know, they're down in the kernel, along with everything else that Microsoft created, and with full ring 0 privileges.



So here's how Microsoft couches the current mess while, at the same time, taking more than 100 existing "previously certified good and safe" Windows drivers out of circulation.  Microsoft said:  "The Microsoft Windows Hardware Compatibility Program (WHCP) certifies that drivers, and other products, run reliably on Windows and on Windows certified hardware.  First reported by Sophos, and later Trend Micro and Cisco, Microsoft has investigated and confirmed a list of third-party WHCP-certified drivers used in cyberthreat campaigns.  Because of the drivers' intent and functionality, Microsoft has added them to the Windows Driver.STL revocation list."  Woohoo.



"The Windows Driver.STL list is part of the Windows Code Integrity feature.  The file contains digital signatures and lists of drivers that Microsoft has revoked.  This stops malware from running in the Windows boot and Windows kernel processes.  Driver.STL ships along with Windows, but is not part of Windows.  It cannot be turned off, tampered with, or removed from the system.  Microsoft updates the contents of the revocation file.  The updates are sent to Windows systems and users from Windows Update."  Right, like every six months.



"The Windows Code Integrity feature validates the source and authenticity of the drivers that run in Windows.  The feature uses digital signatures to verify the integrity of Windows files and drivers.  It prevents the loading of unsigned or tampered files.  Windows Code Integrity and the Driver.STL revocation list have existed alongside Windows since Windows Vista."



Okay.  So what this all means is that, as Microsoft themselves say, WHCP certified signed drivers are being used in cyberthreat campaigns because driver signing is no longer workable.  I mean, it's not useful.  They're having to do blacklists of drivers, digital signatures, listing them in this file.  And they just added more than 100.  I checked.  Their previous update was December of last year.  So we're getting these fixes in large batches less than twice per year.  And unfortunately, this really isn't adequate.  But it's what we've got.



And I don't see anything that they can do now.  They can't change the way NT's architecture is.  We're stuck with it.  They're no more able to change NT than Intel could decide to give up on its x86 family and do something else.  You know, this is old legacy architecture dating back three decades, and all Windows is based on it.  They keep changing the API layers, moving that all forward from Win32 and .NET, and then a whole series of evolutions on top of this fundamental architecture.  And unfortunately, the way it's been designed, they're allowing people to write whatever they want to, get it signed, and until it's found to be bad, it's allowed to run in the kernel.  The world we've got.



So following the massive MOVEit massacre...



LEO:  Oh, that's a good name, I like it.



STEVE:  Yeah, yeah.  And unfortunately it's too accurate.  Boy.  Russia's Clop leak site has been steadily adding to the list of companies whose data it successfully exfiltrated and is now threatening and holding for ransom under threat of full disclosure, which will occur when their proprietary data are sold to the highest bidder on the dark web.  Two recent additions to the list, which now numbers more than 200 companies, are noteworthy.  The well-known stock photography portal Shutterfly and the Discovery Channel are the latest victims to be listed.  Yikes.  That was a bad hack.  And of course that's the SQL injection vulnerability that I've bored everybody by yammering on endlessly about because it just drives me nuts that this is still being done today.



Here's one that caught me by surprise.  And I'm not sure how many of our listeners will recognize the name, Leo.  But last Wednesday Google posted to the Android Developers Blog the news of a new policy to begin this August.  It had the headline "New policy update to boost trust and transparency on Google Play."  Google wrote:  "One of the many ways we keep Google Play a safe and trusted platform" - yeah, they wish - "is by verifying the identity of developers and their payment information.  This helps prevent the spread of malware, reduces fraud, and helps users understand who's behind the apps they're installing.



"For example, we require developers to verify their email address and phone number to make sure that every account is created by a real person, with real contact details."  That doesn't seem like much.  Anyway, they said:  "Today, we're announcing expanded developer verification requirements in our Play Console Requirements policy.  As part of this update, we'll also share more developer details on your app's store listing page to help users make more confident, informed choices about what to download."



Okay.  So it's interesting, first of all, that this is happening now.  It seems like certainly an overall good thing to do.  But it's also interesting that it comes after we reported that news of U.S. legislators threatening to have app stores proactively warning U.S. users when an app they wanted to load had ties to China.  Anyway, Google then explained the specifics of their new plan.  And get this, Leo:  Requiring organizations to provide a D-U-N-S number.



They said:  "When you create a new Play Console developer account for an organization, you'll now need to provide a D-U-N-S number. Assigned by Dun & Bradstreet, D-U-N-S numbers are unique nine-digit identifiers that are widely used to verify businesses.  Because we'll use D-U-N-S" - that's D-U-N-S - "numbers to verify your business information during the account creation process, it's important to make sure the information that Dun & Bradstreet has about your business is up to date before creating a developer account.  You may also be required to submit official organization documents to help us verify your information.  If you're not sure if your organization has a D-U-N-S number, you can check with Dun & Bradstreet or request one for free.  The process can take up to 30 days, so we encourage you to plan ahead."



Now, okay.  Anyone who's been in business for long will have encountered Dun & Bradstreet.  I googled "Gibson Research Corporation Dun & Bradstreet" and was taken right to our page at D&B.  Dun & Bradstreet was founded by Robert Graham Dun & John M. Bradstreet in, okay, 1841, 182 years ago.



LEO:  It's how business is done.



STEVE:  That's exactly right, Leo.  Basically, they just keep records on all businesses, and they serve as a clearinghouse for corporate data.  I just renewed GRC's server and code signing certificates with DigiCert.  And since the certificates are Organization Validation (OV), which is one level up from DV (Domain Validation) and EV (Extended Validation), because I want EV code signing certs, which are slightly more trusted, it was necessary for us to have someone present to answer our corporate phone line at the number that's listed for GRC at Dun & Bradstreet.



LEO:  Right.



STEVE:  There's no way around that.



LEO:  Right.



STEVE:  That you have to do that.



LEO:  It's kind of like an EV cert, an extended cert; right?



STEVE:  Yeah.



LEO:  Yeah.



STEVE:  Yeah.  So anyway, I thought it was very interesting that Google is adding this layer and level of corporate authentication.



LEO:  It's not unusual.  I've had to do that in the past, with others, as well.



STEVE:  For what?



LEO:  You know, like if you want a business account at Facebook and things like that.



STEVE:  Oh, yeah, yeah, yeah.



LEO:  You just have to prove that you are the business and, you know, that you are the [crosstalk].



STEVE:  Right.  And again, anyone who's in business for long, D&B should have discovered you by themselves.



LEO:  Oh, yeah.  We're in D&B, yeah, yeah.



STEVE:  Yeah.  So they said:  "On August 31st we'll start rolling out these requirements for anyone creating new Play Console developer accounts."  And they said:  "Your 'About the developer' section will be visible to users as soon as you publish a new app.  Over the first couple of months, we'll listen to feedback and refine the experience before expanding to existing developers.  Then, in October," they wrote, "we'll share more information with existing developers about how to update and verify their existing accounts."



So initially only for new accounts.  They'll work out that process, get any kinks and wrinkles out of it.  But then they're going to retroactively go back and tell all existing corporate organization accounts, you need to get yourself validated through D&B, or we're going to have to talk to you about that.  So anyway, I just thought that was interesting.  And, you know, it's a good thing that there'll be more accountability for where apps are coming from, especially on the Android store.



I titled this "No Threads for you.  Or EU."  The European Union's GDPR is of course a frequent topic on this podcast because it's being wielded to complain about U.S. companies' cross-border transit of EU citizen data.  When Meta recently released their smash hit "Threads," intended to be an alternative to Twitter, they deliberately did not release it in the European Union because it was pretty clear by now that various EU countries would jump up and down and file lawsuits against the privacy invasion they felt were being created by this American juggernaut we have over here.  However, did I mention that Threads has been a smash hit?



LEO:  Oh, yeah.



STEVE:  Uh-huh.  Where are we now?  I've not kept...



LEO:  Last I saw 150 million users.  It's been a little more than - it'll be two weeks tomorrow.



STEVE:  Wow.



LEO:  So it's growing fast.



STEVE:  And that those same European Union citizens who are being protected by their GDPR, whether they want its protection or not...



LEO:  They're pissed.



STEVE:  Uh-huh.  They immediately began clamoring for access to Threads, and they discovered that they could country-hop by using a VPN.  Well, that worked up until last Thursday when people began complaining that they could no longer access Threads over their VPNs because Meta decided that they'd better close that loophole, too.  So, yes, once again, no Threads for you in the EU.



Okay.  And finally, this little bit of news is just too fun not to share.  It seems that a Brit has been sentenced to three years in prison for blackmail and unauthorized access to a computer network after he tried to hijack a ransomware payment which was being made by his employer to a ransomware gang.



So this all began five years ago, in February of 2018, when an Oxford-based company where this British citizen Ashley Liles was working as an IT security analyst, but apparently not the sharpest IT security analyst around, his firm was hit by ransomware.  Officials in the UK say that after Ashley's company was hit by a ransomware gang, Ashley abused his position in the company's IT staff to secretly - but it turns out not that secretly - log into his manager's email account and replace the attacker's bitcoin address with his own.  After all, they all look alike.  Ashley also created an email account that was nearly identical to the attackers' address, obviously so that the change would not be noticed, and then pressured his employer to pay the ransom.



But Ashley apparently wasn't very accomplished with IT, as he had not covered his tracks.  His whole scheme collapsed when the company's security team noticed the unauthorized access to the executive's email.  An investigation into what happened tied the intrusions to Ashley's home IP address.  Whoops.  And then the entire plan fell apart.  It took five years for the wheels of justice to grind slowly, but Ashley will now be behind bars for the next three years because his little scheme didn't work.  And who knows how much money he would have "made."  But you have to imagine that when the bad guys said we didn't get the payment, and the company said, you know, we sent it to you, they would have checked the address and realized, whoops, it went to the wrong address, and then, you know, how did that happen would have occurred.  So anyway, don't do that.



Last Thursday the 13th - this is just a bit of miscellany that will be of interest to our Twitter-using followers, and to me, actually.  Last Thursday the 13th TechCrunch wrote:  "As Twitter fends off new competition from Instagram's Threads, the company today announced a change designed to cut down on spam in users' inboxes.  Starting 'as soon as,'" TechCrunch wrote, "July 14th, Twitter will introduce a new messages setting aimed at reducing spam in Direct Messages by moving messages from Verified users you don't follow back to your Message Request inbox instead of your main inbox.  Only messages from people you follow will arrive in your primary inbox going forward.  Notably, these changes will also now apply to everyone who has their inboxes open to allow messages from everyone."



LEO:  Which is you.



STEVE:  And of course, yeah, exactly.  The reason I'm bringing this up as pertinent is that I very much enjoy and even depend upon the ability of this podcast's listeners who are also Twitter users, if only occasionally using Twitter like me, to be able to send DMs.  As Leo always reminds our listeners at the end of every podcast, my DMs are open.  But this just closed them, at least to people with whom I've never corresponded in the past.



So TechCrunch continues.  They said:  "Previously, people would only be able to message you via Twitter DMs if you had opted into an option, as I had, to receive messages from anyone through Twitter's Settings, or if the senders were Verified users, meaning they pay for a Twitter subscription, and you had specifically opted into receiving Direct Messages from Verified users.  Additionally, people could Direct Message you if you had first sent them a Direct Message at some point in the past.



"The change to move messages from Verified users back to the Message Request inbox instead of the primary inbox unless you follow them signals another failure of Twitter's new verification system, where users can pay for the blue badge that gives them elevated status on the platform.  Before becoming pay-to-play, verification indicated a person was a public or notable figure of some sort  a politician, celebrity, athlete, journalist, or some other well-known individual.  By making the Verified checkmark accessible to anyone who had a credit card to buy it, Twitter diluted the value of verification," writes TechCrunch.



"That apparently escalated to the point that people have become bothered by Verified users spamming their main inbox, when they had set it open to receive DMs from the blue-badged crowd.  In other words, it's a tacit admission that Twitter has a Verified user spam problem.  Twitter notes that if users still want to receive DMs from Verified users in their main inbox, they can manually switch back to that setting at any time after these changes are put into place.



"The update will also make it more difficult for journalists to contact sources for more information or permission to use a tweet, as they not only lost their verification badges under Musk, but now, even if they now pay to be Verified, will have their DMs dropped into the Message Requests folder, where they may remain unseen."  And finally:  "As some users pointed out in the replies to Twitter's announcement, the update doesn't actually cut down on spam, from Verified users or otherwise.  It simply relocates those messages to a different folder."



So after encountering this news yesterday, I went over to check on my settings and, sure enough.  I have a screenshot of what I found.  There are three settings:  Allow messages only from people you follow, allow message requests only from Verified users, and allow message requests from everyone.  I was set to the middle one from Verified users.  I set myself back to "from everyone," as I had been before.  So we are again open for business.



Anyway, so regular DMs will be able to flow in.  I was noticing that there was something called "Message Requests" in my DM.  My favorite Twitter interface is TweetDeck, and so I had, like, 10 things.  And I thought, well, I don't know what those are, and I hadn't bothered to look because I was getting plenty of regular tweets from people.  Then I realized these were people that I had responded to in the past.  So that makes sense.



You know, whenever I can, I will make the time to send a thanks or an acknowledgment or a comment back to someone who has sent something to me, or asked a question, or provided a really great Picture of the Week, as is often the case.  So I supposed that it was because I had previously interacted with these people that, even though I was set to that middle setting, Twitter knew that I had a dialogue in the past, so it allowed those to come through.  Anyway, we're back to everybody again.  So I'm glad for that.



A couple of Closing the Loop tidbits.  Steve Fintel tweeted:  "Hi, Steve.  I've been listening to Security Now! since Episode 1.  You were recently talking about your favorite TOTP apps.  Since you're already a Bitwarden user, why not use its TOTP?  After filling in your credentials, it places the current one-time password in the clipboard automatically.  So when you get to the next dialog that's asking for the one-time password, you just need to paste it."



This question is far more important than it might seem at first glance because doing this significantly increases the user's risk.  This has nothing to do with Bitwarden which is, as Steve notes, the solution I chose after leaving LastPass.  And at the time I made that decision I explained the rationale for my choice for choosing Bitwarden in that episode titled "Leaving LastPass."



LEO:  Well, and furthermore, LastPass offered its own TOTP authenticator, which we recommended against using for the same reason you're about to describe now.



STEVE:  Right.  Exactly.  From a strictly theoretical security standpoint, having the same system, no matter how secure it might be, containing both the secrets for providing your username and password login, and the secrets for also providing the one-time password code, creates a single point of failure.  I use and rely upon an external disconnected standalone authenticator specifically because it is all of those things.  It would make me very nervous to have my password manager not only able to autonomously provide my username and password, but to then also provide what is intended to be a separate and robustly independent additional form of identification, additional factor.



It is absolutely less convenient to have to manually transcribe those six digits.  For me, it's a very small price to pay for the huge increase in security that that affords.  And it serves as a classic example of the tradeoff between convenience and security.  I'm not saying that no one should have their password manager handle everything.  But Steve asked why I'm not doing it, and I doubt that I ever would.



When we all received that initially frightening news of the LastPass breach, and not the first one, I remember commenting on this podcast that one of the first things I did was to look over through the accounts I have registered for one-time passwords.  And I was immediately relieved to see that all of my most important accounts were protected by those entirely independent secrets that were stored outside of the browser.  But imagine if LastPass had also offered TOTP fill-in, and if my account also contained all of those TOTP secrets, as well.  So anyway, Steve, thank you for the terrific question and the opportunity just to say, eh, you know, there is such a thing as too much convenience, and I think that's crossing a line.



A different Steve, Steve M, he said:  "In Episode 930 you talked about using dynamic DNS-based port forwarding for connecting your Synology NAS devices.  I have two Synology devices at separate locations, as well.  I use Tailscale, which has a native Synology app, to connect them over VPN.  Then they can talk to each other with no problem.  I also have it installed on my Mac, so I can use the Synology Drive client to access the shares on my NAS from anywhere in the world."



And Steve, I just wanted to say that's another great solution.  We visited the topic of so-called overlay networks many times. The very first one was Hamachi.  Back then, Hamachi cleverly reused the entire five-dot IPv4 space, that is, all IPs beginning with five-dot and then something dot something dot something, for its own virtual IP nodes since, at the time, none of the five-dot IP space had ever been used.  That meant that any machine's reference to an IP beginning with five-dot could be assumed to be referring to a Hamachi node for routing.



The fact that there's a native Tailscale implementation for the Synology NAS is just more one reason to love Synology.  I haven't yet had any need to access my NASes while roaming, but I'm sure that need will eventually arise.  And I'm delighted to know that I'll be able to use Tailscale to securely and transparently connect to those NASes as if they were still sitting right next to me.  So that's really cool.



And lastly, Timbr, T-I-M-B-R.  He said:  "Hi, Steve.  When possible, please teach us about Windows pagefile and swap.  Regarding our recent SSDs and lifetime, is it recommended?"



Okay.  So the first thing I do when I'm setting up a new machine is to make absolutely certain that the Windows pagefile is either moved to a spinning magnetic drive or turned off entirely.  Of course, it's only feasible to turn it off entirely, or at least it's only practical, to completely disable the pagefile when a system has sufficient main memory.  But all of mine do.  The first thing I do is load up a system with as much memory as it can handle or makes sense.  That's just part of my standard operating procedures.  And then I disable paging completely.  It works just great.  And having lots of RAM is something that just keeps paying dividends over the lifetime of the machine.  So anyway, absolutely you want your - you do not want to swap on an SSD.  There's just no good reason to.  Especially with RAM, you know, main system RAM being so cheap these days.



And this question of writing to solid-state mass storage leads me into a note from a SpinRite tester.  Last week I talked about SpinRite's first Release Candidate, and I explained about its future switch to the embedded RTOS-32 OS.  At that time I had what I'm about to share, but I didn't want to further burden that podcast.  I'll just share it now.



A SpinRite pre-release tester named Jim McHale posted to GRC's SpinRite development newsgroup.  He wrote:  "I have an old Lenovo with a Samsung 840 SSD."  He said:  "Loaded up Alpha-32" - meaning SpinRite Alpha-32 - "and get these rates:  front of the drive, 138 MB/s; middle of the drive, 445 MB/s; end of the drive, 56 MB/s."  So again, SpinRite has a built-in benchmark that benchmarks the front, the middle, and the end of the drive.  He was getting on his Samsung 840 SSD 138, 445, and 56.



Then he wrote:  "I seem to recall Steve saying you can run a SpinRite scan to regain the lost speeds.  I tried Level 1, and it did not improve.  What should I do for SSDs?  I noted the warning in the instructions about SSDs, so I didn't want to go beyond Level 1 without guidance."



So first of all, SpinRite now notices if you are running at any level that writes to the drive; and if it is an SSD or a shingled magnetic drive, you get an extra notice that writing is something you need to consider carefully with that particular device.



So anyway, I wrote back to Jim to explain that what's needed for SSD maintenance is a rewrite of the SSD's data because over time, and especially with repeated reading in the area, the disturbance caused by the reading of adjacent SSD media has been found to disturb the integrity of the SSD's stored data.  Anyone who does an Internet search for the term "read disturb" will get an eyeful.



SpinRite's Level 1 is a read-only pass.  So what Jim needed to do was to run Level 2, which performs a read, followed by a write, of the same data, right back to the SSD.  And you could optionally use Level 3 which follows that up with a final reread, if you just wanted to be extra safe, although I don't think it's necessary.  But I also explained that while it made sense to do this in what appeared to be an extreme case such as his, it should be done sparingly since writing very slightly fatigues SSDs.



So Jim replied the next day in the developer newsgroup with his update.  He wrote:  "Thank you, Steve, and everyone else who chimed in.  What a great group.  The numbers after Level 3 are now 564 across the board."  He said:  "Wow.  Hubba hubba hubba."  So he went from 138, 445, and 56 MB/s to 564 MB/sec by running a Level 2 pass of SpinRite over his SSD.  And what he experienced is what everyone has been seeing.  His SSD was restored to brand new performance.



With SpinRite 6.1, for now, rewriting the entire drive is the best I can offer.  But this is one of the reasons I'm still willing to invest in developing what will be an entirely new SpinRite 7, written from scratch under a new OS.  SpinRite 7 will add what I call "targeted rewriting" to selectively rewrite only those spots on the SSD that require it.  And this is not just for speed.  Speed is what you get.  But it's every bit as much about storage reliability, since the reason those regions are being read back more slowly is because their stored bits have been softened and have become less certain.



So the SSD's media controller is having to work much harder to determine what was originally stored there.  When you rewrite it freshly, it no longer has to work as hard.  The data is restored much more safely and securely, and you get to read it back much quicker.  So anyway, all of this means that, much to my amazement, SpinRite has every bit as much of a story to tell for solid-state storage as it always has had for spinning magnetic storage.



LEO:  Take a break before you get to the thing; right?  You want to get to the thing?  Satellite Insecurity?  I did want to - regarding this use of swap file on Windows 11.  



STEVE:  Yeah?



LEO:  For a long time, historically, even if you had, you know, 10GB of RAM, you would still want a swap file because Windows used it for other things besides just swapping out RAM when you ran out of memory.



STEVE:  Yes.



LEO:  And I think with Windows 11 that is still the case.  It uses it to...



STEVE:  It uses it to store the system RAM dump during a kernel fault.  If you crash, it will store that.



LEO:  Well, there is a swapfile.sys that it also uses to sleep UWP apps.  It uses it for other things than what we think of a swap file as, as a little extra storage on the hard drive in case you run out of RAM.  And for a long time I've recommended, even if you have ample RAM, not to have some usually fixed size, can be small, swapfile.  So I'm not sure not having a swapfile is necessarily optimal.  I'll have to ask Paul and see if he knows on Windows.



STEVE:  See what he thinks.  I'm running Windows 10 without one.  And, I mean, the fact that I'm running Windows 7 without one is less germane.  But Windows 10 works great without it.



LEO:  Yeah, it's not that it won't work great, but it may not be working as well as you want it to.  The other issue that I would say is you don't have to worry about an SSD.  I've been using swapfiles on SSDs for ages.  Mark Thompson did that test where in the earliest days of swapfiles he put it on an SSD, and it burned it out quickly.  But now I think the firmware on modern SSDs is good enough that I don't think you have to worry about burning out the SSD.  I've been running swapfiles on SSDs for Linux and Windows for years without any issues.



So I'll do a little more research into it.  But I think it might actually be better for performance if you have a fixed-size SSD, even if it's fairly small, not because of running out of RAM, but for other uses that Windows puts it to.  They've changed it, by the way.  There's a new swapfile.sys replacing the pagefile.sys.  It's all different.  So I'm going to check into that on Windows 11.  I'll find out.  And I don't think it's - I think it's harmless to do it on a swapfile these days.  Most people, that's an SSD [crosstalk].



STEVE:  I will respectfully disagree with you. 



LEO:  Really?



STEVE:  Yup, absolutely.  I would never write to SSD if I didn't have to.  It is really - it fatigues it.



LEO:  I think because of the wear leveling they do these days, that, I mean, you're writing to it all the time anyway.



STEVE:  I don't like that either.



LEO:  I know you don't like it, but I think it's not a problem.  That's one for [crosstalk]. 



STEVE:  It just seems completely unnecessary to have a swapfile if you've got lots of RAM, unless I'm wrong.



LEO:  I'll check.  I would just - look.  I defer to you in every respect on hard drives.  But it does sound counter to stuff I have been told before.  So I will look into it, just so people have that potential caveat.  Let's talk about satellites.



STEVE:  Yes.  So we spend, as I said at the top of the show, we spend a lot of time looking at ground-based systems.  I mean, like, virtually all of our time.  In the 18-plus years of this podcast we've never looked to the sky.  Well, unless it was to talk about aliens, of course.  But just as our dependence upon ground-based fiber optic communications has crept forward kind of slowly, you know, almost being unappreciated until we suddenly realized that we were unable to live without it, the same has been happening, largely unseen, far above our heads in orbit.



On March 1st of this year, Bloomberg posted a piece titled "How Do You Hack a Satellite?"  It had the subtitle "Inside the frighteningly easy form of cyberwarfare."  And Bloomberg wrote:  "It's morning, on February 24th, 2022.  Ukraine has just been invaded, but you live halfway around the world.  Your neighbor comes over to complain that their Internet is out.  Suddenly, you lose connectivity.  Could it be the Russians?



"Unlikely as it might seem, for a number of satellite Internet customers of Viasat Inc., that's exactly what happened.  In a story in this week's Businessweek, Bloomberg reporter Katrina Manson digs into the hack that disabled thousands of broadband users all over Europe.  She writes:  "Across Europe and North Africa, tens of thousands of Internet connections in at least 13 countries were going dead.  Some of the biggest service disruptions affected providers Bigblu Broadband PLC in the UK and Nordnet AB in France, as well as utility systems that monitor thousands of wind turbines in Germany.



"The most critical affected Ukraine.  Several thousand satellite systems that President Volodymyr Zelenskyy's government depended on were all down, making it much tougher for the military and intelligence services to coordinate troop and drone movements in the hours after the invasion."  So that's the end of Bloomberg's quote.



Bloomberg continues:  "It turns out that satellite hacking is one of the bigger and less understood threats of cyberwarfare.  For many years no one worried about someone hacking a satellite because, well, it was so hard to even launch a satellite.  But in 1986, a man going by 'Captain Midnight' jammed HBO's feeds because he was mad about paying a higher fee.  There are number of touch points that could be vulnerable to interference.  You've got the orbiting satellite itself, its transmitted data, and the network of dishes on the ground, sending and receiving information."  So anyway, Bloomberg continues, but that gives us a little bit of a sense.  So that's the commercial side.



But what about GPS and about our deep dependence upon space-borne communications and surveillance technology for our national security?  And not just our national security, but everyone's national security?  What caught my eye and first put this topic on my radar was a security research paper that was accepted for and recently presented during the 44th IEEE Symposium on Security and Privacy in May.  It was titled "Space Odyssey:  An Experimental Software Security Analysis of Satellites."  And as you might expect since we're talking about it here, the news was not good.  In fact, as you really might expect, it's downright horrifying.  And we're talking down at the firmware level that probably cannot be fixed from the ground.



But seeing this reminded me of another recent news blurb that I had recalled.  I found some coverage of that event in Newsweek with the headline "Five Teams of Hackers Will Compete to Breach U.S. Satellite in Space," and the subhead "Protecting satellites from hacks is becoming more important as industries from agriculture to banking to insurance rely on space-based capabilities."



Newsweek wrote:  "This August, at the famed Def Con hacker convention, the U.S. military will stage a contest in which competing teams of white hat hackers will, for the first time ever, try to penetrate and take over computer systems on a satellite actually in orbit.  Steve Colenzo, Technology Transfer Lead for the Air Force Research Laboratory's Information Directorate in Rome, New York, and one of the contest's organizers, said:  'It took four years, but this year we are in space for real.'



"The Hack-A-Sat 4 capture-the-flag contest comes in the wake of the notorious cyberattack on the Viasat KA-SAT European satellite network last year," the one we were just talking about.  "Russian military hackers sought to decapitate Ukrainian command and control of its armed forces by shutting down the network, just as Russian invaders rolled across the border.  Although there are conflicting reports about its impact on the fighting, the attack was completely effective from a technical perspective.  Every one of the KA-SAT's ground user terminals that was turned on at the time shut itself down and could not be powered back up.



"That, plus the collateral damage the attack caused, such as the wind farms in Germany knocked offline, underlined both the integral role in the world economy of space-based global communications networks and their vulnerability to hackers.  It also demonstrated the value of the annual Hack-A-Sat contest, which aims to highlight the cyberthreat created by space-based capabilities.  Steve Colenzo said:  'We've turned a corner.  A lot more people now understand those threats.'"



So today's podcast is Part 1 of this important topic because I wanted to lay a bit more groundwork for the discussion of what this group of six serious German cybersecurity researchers discovered and reported in their IEEE paper.



It's one thing to be unable to watch Seinfeld reruns, but entirely another for a country to be deliberately blinded by its adversaries when it's most in need of surveillance intelligence.  It's very clear that the security of what's in orbit above is crucial to the physical security of our lives we're leading down here on the ground.  So I want to conclude Part 1 of this examination today by sharing some background from the U.S. Defense Department about the history and present status of the U.S.'s military satellite-based presence.  There's a lot more going on up above us than most of us know.



So from the U.S. Department of Defense:  "One tool the U.S. military has used to gather intelligence on its adversaries is the reconnaissance satellite.  Starting with the CIA's Corona program in the 1950s, the United States has employed orbiting satellites and high-altitude aircraft to photograph points of interest in enemy territory.  These tools allow for an immediate area to be surveyed from a safe distance, improving the efficiency of missions.



"Throughout the Cold War, overhead reconnaissance satellites and spy planes brought attention to the USSR's nuclear buildup in Cuba, helping the United States dispel Nikita Khrushchev's missile gap ploy.  In the 1990s, the stealth plane F-117 Nighthawk aided U.S. missions in the Persian Gulf and Yugoslavia.  More recently, overhead reconnaissance provided critical images of Osama bin Laden's Abbottabad compound.  Much of the United States' other overhead reconnaissance capabilities and missions are still classified, and the portfolio will remain a critical aspect of the military's C4ISR apparatus.  The C4 stands for Command, Control, Communications, Computers; and the ISR is short for Intelligence, Surveillance and Reconnaissance.



"In addition to simply taking photographs, the military's newest reconnaissance satellites use artificial intelligence to analyze and sort captured images.  Once this process has gone through the satellite's system, the sorted images are transmitted to ground stations on Earth.  Here, machine learning allows the stations to compare new images to a plethora of others in the station's database.  The compiled images in the database act as a control group, and differences found in the new images, such as a new structure being built or a plane following an unusual flight pattern, are brought to the attention of decision makers.



"At the same time, new technology like the European Space Agency's PhiSat artificial intelligence chip allows satellites to quickly filter through images and discard the ones that are not useful.  This capability is helpful when dealing with natural disruptions to captured images; cloud cover, for example, renders many images useless.  With AI, satellites can be programmed to recognize clouds and transmit only the cloud-free images to Earth, saving military analysts valuable time.



"Timely and reliable communication is a vital aspect of all U.S. military missions.  Over the past few decades, the United States has relied on four different satellite systems to fulfill this role.  Efforts to create a military communications satellite first began in 1960.  The first satellites were launched in June of 1966; and by July of 1967, 19 satellites made up the system then called the Initial Defense Satellite Communication System (IDSCS).  Data and photographs transmitted by the IDSCS system were first used in military operations during the Vietnam War.



"During this time, satellite technology improved.  In 1971, the first of 16 new satellites were launched under a new system called the Defense Satellite Communications System II (DSCS II).  Advantages over the IDSCS system included increased communications privacy and compatibility with ground-portable units.  The military's third system, DSCS III, came under development in 1975.  Between 1982 and 2003, 14 satellites were launched as part of this network.



"Today, the U.S. military relies on the Wideband Global SATCOM (WGS) network.  The Department of Defense ordered WGS's first two communication satellites in 2002, launching the first satellite in '07 and providing communications coverage over the Pacific Ocean.  Two years later, the second satellite was put into orbit, expanding the communicative reach over the Middle East and Central Asia.



"Each WGS satellite is digitally channelized and transponded.  These characteristics provide a quantum leap in communications capability, connectivity, and flexibility for U.S. military forces and international partners.  Just one WGS satellite provides more SATCOM capability than the entire legacy Defense Satellite Communications System constellation.  WGS is an international system, with Australia, Canada, Denmark, Luxembourg, the Netherlands, and New Zealand also investing in the satellite constellation.  The system's 10th satellite was launched on March 15th, 2019, and an 11th is set to be completed by 2023.



"Looking forward, the Pentagon is already planning the next communication satellite system.  Spearheaded by the recently created Space Development Agency, the system will include development of deterrent capability, space situational awareness, a resilient common ground-based space support infrastructure, command and control systems, and artificial intelligence-enabled global surveillance.  Additionally, the system is expected to be comprised of seven mission-enhancing layers, including deterrence, navigation, and battle management.  Another goal of this next program is to develop a network that has lower financial and security risks than its predecessors.  In order to achieve this, the SDA is exploring the use of small smart satellites.



"While both the physical size and cost of satellites have decreased over the years, these smaller satellites are not yet equipped with features at the same level as those employed by larger satellites.  This shortfall, however, can be negated if a group of hundreds or thousands of small satellites" - we're talking swarm technology - "are launched as one network.  Under this system, if one small satellite is damaged or knocked off course, the cost is minimal, and the system as a whole will not suffer.



"The same cannot be said of the older, larger satellites.  A damaged WGS satellite is costly both in terms of financials - the 11th WGS satellite will cost the U.S. government $605 million, so .6 billion - and functionality of the current satellite network."  That is, if one of these big guys is knocked out, it hurts the functionality of the whole network. 



"In order to make the small satellite plan a reality, Defense Advanced Research Projects Agency" - of course DARPA - "created Blackjack, a program designed to loft a network of 20 prototype small spy satellites to low Earth orbit in 2021.  If adopted into the SDA's future satellite network, the Blackjack prototype would first focus on surveillance and communication missions.  However, there have been talks about broadening the scope to more complex assignments such as space-based battle management.



"Big satellites are big targets that, if damaged, have big and inimical consequences.  While a future system will likely make use of small and smart satellites, the current WGS network is comprised of 10, soon to be 11, large, unprotected satellites, meaning adversaries need only damage one or two of them in order to dramatically disrupt the system.  The biggest threats to WGS come from China and Russia.  Both nations have ground-based anti-satellite weapons capable of destroying satellites in low earth orbit.



"Beyond that, Beijing and Moscow are currently developing what they call 'peaceful spacecraft.'  These machines are purportedly being made in order to 'reduce the growing amount of orbiting debris and to refuel, repair, and refresh China's and Russia's existing fleet of satellites.'  Designed with robotic arms, these machines can easily be utilized to remove parts from U.S. satellites, empty fuel, and break antennae and solar panels.



"Someone in the know was quoted:  'Unlike ground-based missiles designed to knock out orbiting satellites, which give hours of warning before they can hit key targets in geosynchronous orbits, the spacecraft satellites China and Russia are developing can destroy an intolerable number of our critical satellites with little or no warning.'



"DARPA is currently building the United States' own satellite repair machines.  Once launched, these or similar machines could also serve as 'bodyguards' for U.S. satellites.  With this defense, the WGS would be protected and able to serve the needs of the U.S. military until the future SDA satellite network is completed."  So we're talking bodyguard satellite robots up there to protect our low number of large big satellites until we can bring high numbers of small swarm technology satellites into service.



This continues, and we're nearing the end:  "Another risk to current and future satellites is hacking.  Carried out by foreign governments, non-state entities, or even individual actors, cyberattacks are relatively inexpensive endeavors.  On top of that, tracing a cyberattack back to its source often proves difficult, if not impossible.  Dark Reading's Robert Lemos was quoted:  'The importance of satellites make them a critical part of any nation's infrastructure and make attacking those satellites a strategy that most nations need to consider.'



"Over the past decade, both China and Russia have launched cyberattacks against U.S. and NATO-affiliated satellites.  Because both nations are rapidly incorporating cyberattacks into their military arsenal, the threat of similar instances will only increase.  The information collected and transmitted by satellites is vital to the success of U.S. military operations;  68% of U.S. munitions, for example, were guided using space-based means during the U.S. invasion of Iraq in 2003.  68%.  On top of that, the U.S. military relies heavily on GPS systems to move troops and supplies.  In short, an effective cyberattack on a critical U.S. satellite could have detrimental repercussions on the battlefield.



"In order to protect the satellites from hacking, the Pentagon should focus on risk-reduction frameworks through communication networks and supply chains.  Moreover, the United States needs to explore protective technology, such as the Chinese development of communications protected by quantum cryptography.  As cyberthreats and capabilities continue to proliferate and evolve, so should the United States' ability to deflect and counterattack, and this means shifting satellite protection of a central priority of U.S. C4ISR."



So this concludes the first part of our two-part examination of satellite insecurity.  Next week we'll look at exactly what that team of German cybersecurity researchers found when they took a close look at the state of actually deployed satellites orbiting above us.  And again, what was that that Henny Penny said?



LEO:  There's only 300 shopping days left till Christmas?  No, that wasn't it.



STEVE:  And Leo, apparently they can actually be knocked out of orbit.



LEO:  Wow, yeah.



STEVE:  I mean, you can actually...



LEO:  And we have lasers.



STEVE:  Oh, no.  I mean, by hacking, by cyberhacking...



LEO:  Oh, by hacking them, oh, yeah.



STEVE:  You can drop a satellite out of, I mean, back to ground.



LEO:  Yeah.  Well, I look forward to Part 2.



Copyright (c) 2023 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




