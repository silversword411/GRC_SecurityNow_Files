GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#721

DATE:		July 2, 2019

TITLE:		Exposed Cloud Databases

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-721.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we track further occurrences of ransomware in Florida and elsewhere.  We check in on the state of the "going dark" anti-encryption debate.  We look at a stunning new BlueKeep proof-of-concept demo produced by the guys at SophosLabs.  We update some miscellany and present some closing-the-loop feedback from our terrific listeners.  Then we examine the nature of the continuing problem of massive publicly exposed databases.  In the third example of this just this week, we discover a prolific Chinese IoT manufacturer who is logging more than a million of their customers' devices into an exposed database of two-billion-plus records - which returns us to the dilemma we have with the utter lack of oversight and control over our own IoT devices, and the need to soberly reconsider what "IoT" stands for.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  Yes, yet another ransomware attack in Florida and now Georgia, as well, and what your city should be doing to protect your data.  Also coming up we'll take a look at cloud breaches.  There's a couple of new ones, including one that you just won't believe.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 721, recorded Tuesday, July 2nd, 2019:  Exposed Cloud Databases.



It's time for Security Now!.  We're going to save your bacon today with...



STEVE GIBSON:  First, yes.  



LEO:  The king of bacon saving, Mr. Steve Gibson is here.  Hello, Steve.



STEVE:  Hey, Leo.  Great to be with you again for Episode 721.



LEO:  Holy moly.



STEVE:  Counting down to 999 and the end of existence as...



LEO:  Can you stop saying that?  You're making me sad.  I know it's years away, but it just makes me sad.



STEVE:  Ah, well.  We're going to have resolved all of these problems.



LEO:  Oh, that's right.  Oh, never mind.  No problem.



STEVE:  So, yeah, there'll be like nothing left to talk about.



LEO:  Yeah.  It's all fixed now.



STEVE:  I have mentioned before that I deliberately skip over stories about yet another exposed database in the cloud.  But three different things happened in this past week, and one of them is a little bit breathtaking, so much so that we're going to redefine what the initials IoT stand for.



LEO:  Oh, boy.



STEVE:  But first...



LEO:  Initialization of terror?



STEVE:  Oh, that's not bad.  But first we're going to track further occurrences, believe it or not, further occurrences of ransomware in Florida and elsewhere.  We check in on the state of this "going dark" anti-encryption debate.  And I stumbled into something that's more than I could cover with everything else we have to do today, so it's probably our topic for next week, which is the Ghost Protocol.  I kid you not, that's its official name, which GCHQ has proposed.  We're also going to look at a new stunning BlueKeep proof-of-concept demo which was produced by the guys at SophosLabs.  We update a bunch of miscellany, some closing-the-loop feedback from our terrific listeners, and then spend much more time than we normally do on this nature of the continuing problem of massive publicly exposed databases on the Internet.



And as I mentioned, in this third example of the three that occurred just this week, we discover a prolific Chinese IoT manufacturer who is logging more than a million of their customers' devices' activities into a massive, more than two billion record, publicly exposed database.  They have not responded to calls to shut it down, to protect it from the public.



Which returns us to the dilemma that we were talking about, I think very usefully, last week - although it's a dilemma without a solution, so that's annoying - which is the utter lack of oversight and control that we have now over our own IoT devices and what it means.  Which brings us to the need to soberly rename what IoT is an abbreviation for.  So I think an interesting podcast for our deserving listeners, and a very apropos Picture of the Week, as it happens.



LEO:  We have a number of guesses, speculative guesses in the chatroom for what IoT might now stand for.  I think one of them is accurate, so I won't say a word.



STEVE:  If somebody cheated and looked at the end of the show notes...



LEO:  Oh, yeah, it's in the show notes, isn't it, yeah.



STEVE:  Yeah, yeah.



LEO:  What are you laughing at?



STEVE:  Well, our Picture of the Week, which is not genius, but it's just fun.  We have the boss sitting behind his desk, and he has an employee standing in front of the desk who announces to his boss, "Our devices are now 100% secure."  And the boss is very impressed.  He says, "How did you do that?"  All caps.  And the employee says, "I turned them all off."



LEO:  The only way, baby.



STEVE:  So, yeah.  So speaking of turning them all off, more ransomware news from Florida.  We talked last week...



LEO:  It's so amazing.  It's so amazing.



STEVE:  I know.  I know.  We talked last week about a Riviera...



LEO:  Riviera Beach was...



STEVE:  Riviera Beach, yes, in Florida, right.  And that was a relatively small community of 30,000.  On June 10th, the computers of Lake City, Florida, population 12,000, ceased to function as malware encrypted their data after they were hit by a powerful ransomware attack that was named "Triple Threat" because it mixes three different methods of compromise.  As we have always seen, and this happened again, an employee opened a document they received in email which infected the city's network with the Emotet trojan, which later downloaded the TrickBot trojan.  And then later that downloaded the Ryuk, R-Y-U-K, ransomware.  And Ryuk is something that we're going to be hearing - or maybe it's Ruk?  Anyway...



LEO:  I'm think, I'm guessing, it's like the Three Stooges "Nyuk nyuk nyuk."  It's ryuk, ryuk, ryuk.  R-Y-U-K; right?



STEVE:  Ryuk, ryuk, ryuk.  So the Ryuk ransomware, but it's really nothing to ryuk, ryuk, ryuk about.  Now, in this case these guys quickly detected the attack, disconnected the affected systems within minutes of the discovery that something was very wrong.  But that, of course, all this stuff moves at the speed of light.  So the malware was still able to infect systems controlling most of the city's landline phones and email systems, and of course lots of other ancillary systems.  The landline phones and email, being communications, was very much in everyone's face, which forced employees to revert to pen and paper in an attempt to continue working.  Fortunately, in this case, the police and fire and other emergency services remained unaffected since they were on a separate network.



They didn't receive a demand for ransom immediately, and they were wondering whether it might have been because they quickly took the systems offline, thinking that maybe the bad guys didn't know they had been infected.  But it turns out that wasn't the case.  The request was received for ransom a week later by the Florida League of Cities, which is the insurance provider for the city of Lake City, which is the city that was infected.



LEO:  Wait a minute.  You're saying that the ransomware guys bypassed the middleman and went straight to the insurance company?



STEVE:  Yes.



LEO:  Oh, my god.



STEVE:  I know.



LEO:  Oh, my.



STEVE:  Which is a little chilling.



LEO:  Well, it's a spear phishing attack; right?  These are targeted attacks.



STEVE:  Yes, yes.  So this league, the Florida League of Cities, the insurance carrier, began negotiating with the attacker and agreed to pay 42 bitcoin.  At the time - bitcoin has been...



LEO:  That's a lot.



STEVE:  ...jumping around a lot recently.  It hit a peak at $13,000, but it just today dropped below 10.  But at the time it was 400,000.  And I was interested to hear that the attacker sent the ransom demand to the city's insurance career.  This suggests more sophistication than I had expected, but I think we need to recalibrate ourselves.  Now, get this.  The city's information technology director, whose name is Brian Hawkins, said:  "Our systems are shut down, but there is no evidence to indicate any sensitive data has been compromised."



LEO:  Nyuk, nyuk, nyuk.



STEVE:  Uh-huh, which would be painting a happy face on the disaster.



LEO:  There's no evidence.



STEVE:  Of course, again, he's the city's information technology director, Brian Hawkins.



LEO:  Oh.  He's the guy responsible for this, then.



STEVE:  Yes.  We'll be coming back to him in a moment.  He said:  "All customer service payment data, such as credit card data, is stored offsite by third-party vendors and would not have been accessed by an attack like this on our network."  However, whoops.  With no proper backup to restore the data, Lake City was left with no option but to pay the ransom.  Thus the city's leadership approved the ransom payment last Monday, 42 bitcoins were paid last Tuesday, and the city's IT staff began decrypting files later the same day.  So pretty quick action, negotiated with the insurance carrier.



The insurance carrier - in fact, I'm getting ahead of myself because I have here written down in the show notes, Mayor Stephen Witt said that the administration made the decision to pay up after talking with the FBI.  And the FBI probably said there's nothing we can do to help you.  And so Stephen Witt said okay, thank you, next?  And they switched over...



LEO:  The FBI's standard recommendation is do not pay.  But, you know, in this case I think you have to change your tune if you have no recourse because normally they say don't pay for two reasons.  It incents the bad guys.  Well, believe me, we don't need to incent them anymore.



STEVE:  Right.



LEO:  And it often doesn't work because they may not have or give you the key.  But I've been thinking about this.  Something so highly publicized?



STEVE:  Yes.



LEO:  You're going to get the key because the ransomware authors want you to think paying works.



STEVE:  Yes, exactly.



LEO:  It may not if you're just a private individual.  But if you're a city, I think you have a better shot at getting it back.  Don't you?



STEVE:  Well, yes.  And as we talked about last week, when we looked at two previous instances, the Riviera Beach and one in, I think it was in Atlanta, the cost, the actual cost of not paying - wait.  It wasn't Riviera because they were the people who paid $600,000.  So it was two previous ones...



LEO:  Baltimore didn't pay, and they were down for weeks and weeks.



STEVE:  And the remediation cost was millions, many millions of dollars.  So although no one wants to pay, unfortunately, it actually ends up being the least expensive proposition.  So in this case, for this city, their deductible was $10,000.  So they paid $10,000.  The insurance carrier paid the balance.  And the city's IT director, Brian Hawkins, was fired.  



LEO:  Thank you.  Thank you, you nitwit.



STEVE:  Yes, exactly.  So we have 42 bitcoins this week, 65 the week before.  So that's in the neighborhood of $1.3 million for these two cities.  So this creates a dilemma.



LEO:  There's a third Florida city, by the way, Biscayne Beach, I think, yeah.



STEVE:  Yup.  I'm just getting to it.



LEO:  Oh, good, all right.



STEVE:  So here's the problem.  Attackers have apparently found an interesting niche to attack.



LEO:  It's a good niche.  It's a good niche.



STEVE:  Like individuals, smaller government municipalities apparently lack sufficient IT infrastructure to allow them to both protect from and recover from targeted attacks.  But unlike individuals, they have much more important data at stake, as well as insurance to take the short-term sting out of paying up.  And they've got, arguably, much deeper pockets, as well.  So, okay.  Here's a test.  I want everyone currently listening to this podcast to raise your hands if you think - oh, unless you're driving - if you think that the bad...



LEO:  Then raise a finger.



STEVE:  Okay, a finger, good.  Yes, take a finger off the wheel if you think that the bad guys have all the money they want and will now stop pestering deep-pocketed, well-insured, small municipalities with targeted attacks.



LEO:  They're going to retire now, yeah.



STEVE:  Okay.  Everybody got your hands up?  Anyone?  No?  That's right.  We're going to be seeing more of this.  And, oh, guess what?  That's already happened.  Since then - I was trying to, like, wrap up this story.



LEO:  I'm sorry, I ruined it for you.



STEVE:  And news kept coming in.  An even smaller village of 3,000 residents in Florida, Key Biscayne, was just yesterday hit.  Officials reported, or I guess it was earlier because we have yet another one, officials reported a Ryuk ransomware infection last week, but they haven't decided yet if they want to pay the ransom demand.  A special session of the Village Council of Key Biscayne was held last Friday night, with a second session on Saturday, to determine how to respond to the Ryuk ransomware attack on the village's systems.  No answer yet.  What's happened is, as a consequence of this now sort of ongoing series of high-profile attacks, is that the public and the media are beginning to turn negative on city officials who fail to secure networks and then decide to pay hackers.



LEO:  Yeah.  In fact, there's a term now, "Riviera Beached."  You're been Riviera Beached.  So that'll give you some idea.



STEVE:  Yeah.  So now paying ransom is viewed as a sign of a city administration's failure and weakness, rather than being a quick fix to get access back for the citizens' data and so forth.  And as we would expect in following all of this up, forensics has always determined that it was a city employee who ushered the malware into the network by opening a malicious email and clicking on something.



LEO:  But you can't blame the employee so much.  Why is that malware getting through their intrusion detection systems?



STEVE:  Right.



LEO:  Why is it passing - you know, have they been trained?  You can't completely blame the employee.  This stuff's hard to spot.  By the way, there's an update on this from Ars Technica.



STEVE:  Okay.



LEO:  According to Ars, Riviera Beach, in addition to the $600,000 they spent, will spend a million dollars in remediation.  So this is - they're not done.



STEVE:  Wow.



LEO:  And this morning the Georgia Administrative Office of the Courts confirmed they've been hit by Ryuk.



STEVE:  Yes.  And in the show notes, oh...



LEO:  It was in the notes?  Okay.



STEVE:  And just as I was trying to wrap up the story...



LEO:  Just as you were doing it.  Unbelievable.



STEVE:  ...the state of Georgia's Judicial Council and Administrative Office of the Courts fell victim to a Ryuk ransomware attack.



LEO:  So it makes sense because these smaller cities, they don't have budget.



STEVE:  Yes.



LEO:  They have valuable information.



STEVE:  Yes.



LEO:  Plus I thought about this.  It's probably easier to spear phish them because they all have public websites, so does the Georgia court, where you can go and find the name of the city manager, his email.  You can find the name of the comptroller, her email.



STEVE:  Ah, very good point.



LEO:  So you don't have to do a lot of snooping around to get everything you need.



STEVE:  And you could probably generate email, like for example spoof the sender data of some employee to be their boss.



LEO:  Right.  Right.



STEVE:  Because the managerial structure is also public knowledge.  I mean, it's all public record.  So you do some research, and you generate very convincing-looking email which you then send to an unwitting employee and say, you know, "Hi, Betty.  Following up on our conversation recently, here's the data that I told you I would provide."



LEO:  Yeah.  You're the city manager, and you send a note to the comptroller saying this spreadsheet looks like the numbers are funny.  Can you check it?



STEVE:  Uh-huh.



LEO:  That's all you have to say.



STEVE:  Yup.



LEO:  And unfortunately, you still read news reports saying watch out for email from people you don't know.  Which is exactly the wrong thing to watch out for.  It's always from people you know.



STEVE:  Right.  So their infrastructure is publicly available.  They are insured, so they've got, I mean, and one wonders how long insurance is going to be making these payments before either the rates start to skyrocket or the insurance companies say hey, you know, we're not going to insure you against ransomware.  You need to do training, and you need to put systems in place, and also seriously be able to back up and recover yourself from this kind of problem.



LEO:  Yeah.



STEVE:  I mean, it really is, I mean, so their deep pockets...



LEO:  My guess, though, is this is not - the insurance company, in this case, what did you say it was, the Florida state governments, Florida city governments?



STEVE:  Yeah, so maybe - they called themselves the...



LEO:  Association of Florida City Governments, something like that.



STEVE:  Yeah.



LEO:  It sounds like it's the governments themselves.



STEVE:  Florida League of Cities, you're right.  So it sounds like it's a private insurance pool that they all...



LEO:  Right, exactly.



STEVE:  Yes.



LEO:  Nevertheless, you don't really - how much is left?



STEVE:  Yeah.



LEO:  Wow.  Wow.  This is - of course.  And you know what, if  you're the Petaluma City Council or the Irvine City Council, your fiduciary duty means you get to go ask your IT director, what's the situation?  Now, Steve, there's something I wanted to ask you about this.  It seems like a simple backup would solve this problem.  Is that not the case?



STEVE:  Yes, except, well...



LEO:  It has to be offline because you can't have the virus infect it.



STEVE:  Correct.  And the viruses also look for all connected drives and network drives.  So, I mean, so if you could do a full nightly image of everything, then...



LEO:  And take it offline.



STEVE:  Exactly.



LEO:  You take it offline, or put it somewhere like Glacier, where it's immutable.  If you did that - and I think that that's not so hard or expensive.



STEVE:  And I think the problem is it's just a matter of doing it.  Like, for example, we still have Flash in browsers.  Like against all reason.  Why?  Well, because we've had it before.  How long has it taken us to stop using old hashes and old ciphers?  And so the problem I think also is none of these municipalities have as much budget.  I wouldn't be at all surprised if the IT people are saying to their boards...



LEO:  Please, buy us a hard drive.  Please, I beg of you.



STEVE:  Yes.  Like we need this much money.  And I'm sure the city council says, we don't have it.  I mean, yeah, we'd love, oh, we'd love to repave our roads, and we'd like to update our parking meters and blah blah blah blah, you know.  I mean, so I'm sure it's just a matter of, like, the stuff is working.  And so, like...



LEO:  Yeah.  That's really what it is.  By the way, a user in our chatroom just sent me a link to the Florida League of Cities Municipal Insurance Trust.  On the front page:  "Take our webinar on remediating and avoiding ransomware."  On the front page.  And they're doing a webinar, but it's not till next week.  "Preventing Ransomware:  Tools, Tips, Techniques, and Technologies."  Lots of T's in there.  I hope people...



STEVE:  Again, it's like - so certainly preventing the infection is what you would like to do.  That's probably the least expensive thing you could do would be to somehow keep this from happening.  But in a large organization, I mean, a large municipality, how do you get that message out to everyone?



LEO:  No, it's hard.



STEVE:  Especially when they've got bad habits from home and from their phone and from everywhere else on the Internet.  And it only takes one.  That's the other problem is, you know, back when we were talking about Sony and the advanced persistent threat there, I said on the podcast, I don't want that job.



LEO:  No.



STEVE:  You cannot secure something that big.  You just can't.



LEO:  On Saturday we had, in studio, the man in charge of IT at West Point Military Academy.  And I asked him about this because we were talking about this very story.  And he said, "Well, we have the U.S. Army Cyber Defense Command on campus, and they're very good."  He said, "But the problem is you only have to make one mistake.  We have to be perfect.  The hackers don't have to be perfect.  We have to be perfect.  No mistakes."  And that's hard to do, no matter what, even for West Point.



STEVE:  Well, and consider, okay, in the case of the last two attacks, average of half a million dollars ransom. 



LEO:  For crying out loud.  Buy a technology.  Put it in the closet.



STEVE:  That is a serious ransom windfall for the bad guys.  And how many cities are there in the United States?  I mean...



LEO:  Oh, yeah.  And they're all this bad, I'm sure.



STEVE:  Yeah.



LEO:  Every one of them.



STEVE:  Well, because, again, it's aging infrastructure.  It's systems that, I mean, they're probably running - some of them are probably running XP still because they just - it's like, oh, this works.  Or our city infrastructure was written for XP, and it won't run.  It's 16-bit Windows code, and it won't run on Windows 7 or on Windows 10.  So they're stuck.



LEO:  So, IT guy.  Run down to Costco right now, buy a 4TB hard drive, USB.  If you have to walk to every machine and copy the data off of that machine and then put that hard drive in the closet, do it now, and do it every week from now on; right?



STEVE:  At least.



LEO:  Seems like.



STEVE:  Yeah.



LEO:  Criminy.



STEVE:  Well, and I've mentioned that, because it is such a clear issue, and because ransomware will jump to and encrypt anything connected...



LEO:  Has to be offline.



STEVE:  Yes.  The system I have built dynamically mounts and images and then unmounts drives which are not visible except during the brief window of time when I'm doing the imaging because...



LEO:  And you have to rotate the images in case the most current image copies the encrypted data.  You have to be able to go one back, yeah.



STEVE:  Exactly.  



LEO:  I don't think it's that hard.  I think you've got to do it.  It's 88 bucks for a 4TB drive now, $88.



STEVE:  Well, and it's just - and it's intent.  I mean, it's like, time.  Like the IT guy is probably busy keeping his system up, rather than - you know.  And then City Council says, okay, you need to tell us that we're completely immune to a ransomware attack.



LEO:  Nope.  Nope.



STEVE:  And the IT guy says, "We're not.  I've been telling you for two years."  You know?  And so, I mean, so maybe some money gets freed up.  But again, as you said, it takes one mistake, and it takes down a municipality that has insurance and deep pockets.  They're going to pay the ransom because it's going to be far more expensive.  I mean, they're basically gone until they, you know, like limping along until they pay the ransom.  And exactly as you said, in this case the decryption keys were provided same day.  Here's your bitcoin.  Here's your decryption key.



LEO:  The guy's probably down the street; right?  I mean, he's probably in the area.  Maybe not, but it might as well be.  Something else to mention is a good email system like Office 365, Microsoft Exchange, or Gmail will, in most cases, you're not going to get Ryuk through your email.  You're just not.  But what they can't filter out against is links, infected websites.  I mean, they can, but they're not going to be up, you know, I can make today a website they wouldn't know about till tomorrow.



STEVE:  There is an attack right now.  I didn't get it into the show notes.  But it's leveraging JavaScript obfuscated in images which are being served by ad servers to unwitting websites.



LEO:  How are you going to avoid that?  Boy.



STEVE:  Also just don't click on an ad.  Who ever clicked on an ad, anyway?  I don't know who clicks on ads.  Only bots click on ads.



LEO:  Well, couldn't these infect you without your clicking?  I mean, they're just running code.  Couldn't it just, if you had the flaw - you'd have to have a flaw in your browser; right?



STEVE:  Yes.



LEO:  So remember that Firefox zero-day.



STEVE:  That's right.  You do not want to - because browsers are sandboxed, and so they're trying not to let anything out without some user action.  And so, I mean, there's a big difference between viewing something and clicking something.  Clicking lets everything loose because you don't know what's behind that button.



LEO:  Yeah, yeah.  Wow.  It's a scary world.



STEVE:  Meanwhile, the "going dark" problem that we touch on from time to time is not going away.  And I don't think it shows any signs of going away.  A headline in Forbes caught my eye.  The headline read, and just I'll say first that it's not true, it read U.S. - well, okay, no.  Technically, it is true.



LEO:  It's not - it's a rumor.



STEVE:  Yes.  "U.S. May Outlaw Messaging Encryption Used by WhatsApp, iMessage, and Others."



LEO:  See, I've been watching this story with interest, too.  And I haven't talked about it on any of our shows because it's just a leak.  It's a rumor.



STEVE:  Yeah.  Right.



LEO:  But the minute, and I wouldn't put it past President Trump, the minute that they propose this, we've got to raise holy hell.



STEVE:  So, okay.



LEO:  But they haven't yet.



STEVE:  So Forbes saying, you know, normally pretty upstanding, although it wasn't an author who I know...



LEO:  Forbes is not upstanding.  That's the problem is half their content's good; half their content's not.



STEVE:  Well, so this is "U.S. May Outlaw."  Well, okay, yeah.  It may.



LEO:  Yeah, may, yeah.  



STEVE:  Anyway, so I thought, what?  How did I miss that?  And so of course the headline was more speculative than anything.  Anyway, I tracked down the source material for this article, which was a story in Politico, which actually had some useful reporting.  The story was titled "Trump Officials Weigh Encryption Crackdown."  I think even that is a little inflammatory.  But of course, you know, click bait.



So I've trimmed down what Politico wrote, removing a lot of the superfluous stuff and fluff, like of them explaining what end-to-end encryption is that our audience of course all knows.  But there were some interesting bits left which are useful for getting a sense of the environment.  So paraphrasing heavily:



"Senior Trump administration officials met last Wednesday to discuss whether to seek legislation prohibiting tech companies from using forms of encryption that law enforcement can't break."  The encryption challenge - which the government, as we know, calls "going dark" - was the focus of a National Security Council - that's NSC - meeting Wednesday morning, last Wednesday, that included the number two officials, the so-called "deputies," from several key agencies, according to three people who are familiar with the matter.



So Politico says:  "Senior officials debated whether to ask Congress to effectively outlaw end-to-end encryption."  One of the people interviewed said the two paths were to either put out a statement, a general position on encryption, and say that they would continue to work on a solution; or, second path, to ask Congress for legislation.



The unreported meeting of the NSC's so-called Deputies Committee did not produce a decision.  "A decision to press for legislation would have" - this is Politico writing - "far-reaching consequences for the privacy and security, effectively forcing companies such as Apple and Google to water down the security features on their smartphones and other devices.  A ban on end-to-end encryption" - which is not really what anyone is proposing - "would make it easier for law enforcement and intelligence agents to access suspects' data; but such a measure would also make it easier for hackers and spies to steal Americans' private data by creating loopholes in encryption" - which is more correct - "that are designed for the government, but accessible to anyone who reverse engineers them."  Okay, maybe.  "Watering down encryption would also endanger people who rely on scrambled communications to hide from stalkers and abusive ex-spouses," blah blah blah, right.  So, you know, this is just...



So Politico says:  "Politico was unable to determine what participating agency leaders said during the meeting, but there is a well-known fault line on encryption within the executive branch.  The DOJ and the FBI argue that catching criminals and terrorists should be the top priority, even if watering down encryption creates hacking risks.  The Commerce and State departments disagree, pointing to the economic, security, and diplomatic consequences of mandating encryption backdoors.



"DHS [the Department of Homeland Security] is internally divided.  The Cybersecurity and Infrastructure Security Agency" - the CISA we've talked about - "knows the importance of encrypting sensitive data, especially in critical infrastructure operations, but ICE and the Secret Service regularly run into encryption roadblocks during their investigations.  An NSC spokesperson declined to comment on the meeting."



So the high-level NSC discussion highlights "how policymakers have continued grappling with encryption even as it has receded from the headlines."  And there's a lobbyist familiar with the discussions was quoted:  "There is a significant administration-wide effort underway on what to do about the 'going dark' issue."



And then, finally, they said tech companies such as Apple and Google - okay, we know about this.  San Bernardino, we know about that.  Oh.  "The transition between the Obama and Trump administrations saw a hand-off of sorts between two high-profile advocates of the need to access encrypted data.  After President Donald Trump fired Comey, Deputy Attorney General Rod Rosenstein succeeded him as the government's top 'going dark' warrior.  Rosenstein, who dealt with the issue as a U.S. attorney, warned vaguely that cooperation with Silicon Valley was unlikely to work, implying that legislation might be needed.  But now Rosenstein is gone, and experts generally agree that Congress is unlikely to pass a bill requiring warrant-compatible encryption."



And that's the first time I've seen that term anywhere, and that's interesting because that's sort of what we've been talking about, warrant-compatible encryption, meaning you still have encryption, but it's some means for, under warrant, allowing decryption.  Which the reason it's interesting, it's an interesting term, is that we've talked about the Constitution's protections against unwarranted search and seizure that we have in the U.S.  But under a warrant you're able to - if you're able to get a judge to issue a warrant, then law enforcement is able to do, with probable cause, to do what they feel they need to.  And as we know, senators Richard Burr and Dianne Feinstein floated a draft measure back in 2016, after the San Bernardino issue, but it was met with an intense backlash which quickly killed any prospects for it going forward.



So the presumption is that this was sort of a sentiment-gauging meeting which might be preliminary to more public and open talks.  And in any event, as I followed up other links from this story, I discovered the Lawfare Blog, with its back-and-forth discussions about a proposal by the U.K.'s GCHQ.



LEO:  Oh, yeah.



STEVE:  Which I referred to at the top of the show.



LEO:  The Ghost Protocol.



STEVE:  Yes, the Ghost Protocol.  We don't have time to talk about it this week.  But I'm preliminarily titling next week's podcast "The Ghost Protocol" to discuss it at some length because it's the first instance of a serious government-sourced quasi-compromise beyond where we are now, which is sort of an all or nothing.  And there's been immediate pushback from the tech community.  I mean, basically the tech community is going to push back against any what they perceive as weakening of encryption.  But there's no way around the fact that warrant-compatible encryption is a weakening of encryption.



And so, I mean, it's difficult to see how we move past this.  But I think we're going to have to.  I mean, either we decide that we need to get shims installed into the devices before the encryption, as we've talked about, so that we're not weakening the encryption, but the problem is what law enforcement wants is everything to be recorded and then to be able to go back into previously recorded encrypted conversations and retrospectively decrypt them.  And so the idea being, well, we don't, you know we don't know ahead of time what the conversations were in, for example, the case of the San Bernardino attack.  They wanted to get the guy's smartphone and then look at what conversations had been had prior to the attack.  So the fact that those are tantalizing available is part of the problem.



So I don't know.  I mean, this is going to create a tension, and I think this is one of the most interesting things happening today in the security and privacy arena is this problem, this tension between the fact that we now have math that cannot be cracked, and governments are not happy with it.



So our friends at SophosLabs advanced the state of the art in BlueKeep exploits in a stunning proof-of-concept demo video.  It's relatively short because there's no audio.  It's all video.  I don't think it makes sense for us to play it into the podcast.  But I have a link to it.  And in fact I was going to, a little bit later in the podcast, introduce our listeners to a new service.  Okay, good.  I'm glad...



LEO:  I'll play it while you talk, and that way people watching can watch it; right?



STEVE:  Well, yes.  It is amazing.  What this demonstrates is they have two machines side by side in the same system.  One is in a virtual machine.  And they first demonstrate using Remote Desktop Protocol to connect to the second machine.  And it requires a login.  You get prompted for a username and password.  You can't do anything else.  You're just stopped there.  So then they close that session down, and then they launch their tool, which is their proof-of-concept which they've created, which does some massaging of memory in order to prepare the exploit and the target machine.  Once that's finished, they then close that.  Basically that changes the target machine over RDP.  They then relaunch an RDP session.



Oh, and the other thing it does is, when you bring up the Remote Desktop Protocol desktop, down in the lower left is an accessibility button for those who need accessibility help.  Their exploit changes what that is linked to, to bring up an admin system-level console through remote desktop that gives you full access to the machine.  So it's just a neat proof of concept.  They are not making it public.  And again, this is sort of a mixed blessing because this also further demonstrates what can be done by people who know what they're doing.  And we know that, unfortunately, there are probably as many people with mal agendas who know what they're doing as with good agendas.



But I mentioned a link.  I've been wanting to bring up a GRC-owned link shortener for some time.  Our listeners all know that I've been a long-term user of bit.ly, B-I-T dot L-Y.  I've had .sc, I've had grc.sc, as in shortcut, for years.  And the problem with using bit.ly is that for some time the pattern of the links I was generating were obvious.  It was bit.ly/sn-627 or something.  And some rascals went ahead and pre-allocated a bunch of future Security Now! episode numbers and related links, just to be mischievous.  And so I thought, okay, I need my own shortener.



Anyway, so this is the debut of GRC's link shortener, grc.sc/bluekeep.  And that will take anyone who's interested to this demo video, which is worth taking a look at if you didn't just see it in the video on the podcast, grc.sc/bluekeep.  And that just redirects to a link on Vimeo, where the guys at SophosLabs have put this.  Still no evidence of a worm.  And again, to me it doesn't make any sense.



I think we've seen an Exim worm immediately appeared because the exploit was so time-consuming.  I imagine this is being exploited quietly, probably, based on everything we're seeing.  We're seeing that what the bad guys want is money.  They want bitcoin.  And so either you extort people by encrypting their networks in municipalities, or you put bitcoin miners on people's servers that are apparently unattended, and no one is paying any attention to, and you mine bitcoin that way as part of a pool.  So I would imagine a lot of these systems that have un-updated - we know that it's more than several million have un-updated RDP servers.  They're probably now increasingly running coin mining for someone.



I had a tweet from someone about SQRL.  Joshua asked, he said:  "I have a question regarding a unique use case of SQRL.  I'm active duty military and am frequently using controlled systems that don't allow installation of third-party software, including browser extensions.  I also work either on underway ships or in areas where cell phones and other electronic devices are prohibited.  Is there any way for SQRL to work for me?"  He says:  "I tried to sign up for the forum, but my account is in moderation."



And so real quickly, if SQRL succeeds, then part of its success, part of what I will consider success is that it becomes adopted by browsers.  All we have to have is Chromium, the Chromium Project, to decide, hey, this makes sense.  Immediately then - well, and Mozilla, too.  Immediately Chrome and Edge get it, and then Mozilla follows, and we've got browsers covered.  So the point being that eventually there's no way that this won't end up being built into browsers.  It is a natural thing for that to happen.  And I've said in GRC's newsgroups that, having finished a client for Windows, if I knew then what I know now, I would have done this as an extension.



On the other hand, when I began this six years ago, we didn't have a common ecosystem for browser extensions.  Everybody was disparate, and nobody was synchronized, and the browsers were competing with each other to a much greater degree.  So there wasn't a single solution.  And there is something to be said for a single client running in a multi-browser system.  That is, if you have a Windows client for SQRL, then you can use it with Chrome and Firefox and IE.  And so one installation of your client runs across all the browsers, if it's in the OS.  So maybe that'll be the other thing that'll happen is it might be moved into the operating system.  That's another place for it to be.



So eventually, I mean, so I can get it where, in a locked-down environment, it's not possible to install things to make this work.  And if you can't even put it in an outboard phone, then you're going to have to wait until it's, you know, because it does require some computation.  I mean, at the same time, this environment wouldn't allow the use of a password manager.  So I don't know what these people do.  I guess they have a big book of passwords and somehow enter hopefully hard to memorize passwords manually because, if you can't install a password manager, how do you have a different password for every website?



LEO:  I don't know if they go to websites, to be honest with you.



STEVE:  Yeah.  And so in that case...



LEO:  Do they surf the 'Net?



STEVE:  That's a very good point.



LEO:  That's why you need SQRL, right, yeah.



STEVE:  So why do you need SQRL anyway?  Right.  And just so everyone knows, the SQRL forums are moderated.  We've got a big team of moderators who signed up in the beginning days.  And I'm so grateful to them all because essentially anybody who joins, just in order to - because we don't want this to be a free-for-all and a spam fest and a mess, which unmoderated forums tend to devolve into.  So people have to demonstrate that their intent is sincere.  And if anyone starts to become a problem, we'll just remove them because the point is for these forums to be useful for SQRL users.  They are not intended to be a place for people to spray their graffiti.  So everybody is moderated when they start.  And then they come off of moderation after they've proven themselves a little bit.



Oh, and GMT also said:  "Searched for SQRL app on iTunes.  Found this."  And then he said, "sqrl.chat."  And I have no idea what that is.  But the only way at the moment, because this has not yet been released to the iTunes store, Jeff is still working on it, is to join the forums, and you'll find links there to get on the beta team for the iOS, SQRL for iOS, and then you're able to use it.  And that's what I used as part of my SQRL presentation.



LEO:  Somebody in the chatroom has said that there is now a plug-in for SQRL for WordPress.  Were you aware of that?



STEVE:  Yes, there is.



LEO:  That's great.



STEVE:  It's still in its early stages.  The guy who did the SQRL for Android...



LEO:  Daniel Persson.



STEVE:  Yes.  He saw all of the conversation.  And so he said he spent between eight and 10 hours.  He knows PHP, and so he created a - it's not feature complete.  I haven't looked at it closely.  But it is working.  And I've also heard that our good friend of the podcast Rasmus Vind, who did the beautiful implementation of SQRL for the forums, apparently another listener of ours has asked him to consider consulting to create a formal WordPress plug-in.  So that would be great.  I would love to have Rasmus do a commercial-grade version of plug-in for WordPress, which would be great.  So, yes, lot's happening.  There's work on a native SQRL for Linux.  Several people are working on macOS versions.  So there's a lot going on.



LEO:  Nice.



STEVE:  And, Leo, "I Am Mother."



LEO:  I watched it.



STEVE:  Yes.  And I think we talked about it at the end of the podcast.  I don't remember if it was after we stopped recording.  But you enjoyed it a lot.



LEO:  Yeah.



STEVE:  Yeah.  I went looking for a review when someone posted:  "In the most recent Security Now!, Steve mentions a link he was going to send to Leo of a review of 'I Am Mother,' written by a guy who created an account on IMDB solely to post about the film."  And this person says:  "Did he ever share that URL during the podcast or in the show notes?  I would love to read that review, but I haven't been able to spot it by idly browsing through all the reviews of that film on IMDB, nor by browsing or searching transcripts or skimming through the episodes."



So when I went looking for that review in order to respond, I ran across another review that read:  "Such an underrated piece of sci-fi."  And I have a link to that review in the show notes.  And he said:  "The reviews I read only gave me the impression that this would be a decent thriller, not ... this."  He said:  "After watching one generic blockbuster after another, it's always so nice to see small, creative films like 'I Am Mother' are still getting made.  I'm really curious about how much this film cost because it looks really impressive, but I also have the feeling that it was a very small budget.  This film has solid acting and interesting characters.  It touches on complicated ideas with a tightly paced narrative from start to finish, and managed to keep me on the edge of my seat till the very end.  If you love to treat yourself to some great science fiction, I highly recommend 'I Am Mother.'"  And of course our listeners know I recommend it, too.



Anyway, with the warning that this is a complete spoiler, I mean, and the reason you and I were talking about this particular review, not the one I just read, the one I have a link to, and I cannot read, is that this guy who created an IMDB account just to post this, it's because he was annoyed by people not understanding the movie.  I mean, and you really do need to pay attention, especially at the end, where some things are said that are, like, aha.  I mean, sort of like the whole thing's been building, and a few last pieces of a puzzle click into place.



Anyway, this was going to be my first use of GRC's link shortener, and this is a good use for it.  But again, warning, total spoiler.  So for those who have seen it, or you know you will never see it, then grc.sc/iam.  Grc.sc/iam.  And you and I, Leo, both liked it.



LEO:  Yeah.  And I'm reading this, and now I realize I half understood what happened at the end.



STEVE:  Yeah, it was well conceived.



LEO:  Yeah, yeah, very - but you have to do some math to really understand what it all meant.



STEVE:  Right.



LEO:  I kind of suspected something, but now I understand completely.



STEVE:  Yup.  So anyway, but again, don't go there if you...



LEO:  No, no.  Watch it.  And don't even watch the trailer.  Just watch the movie.



STEVE:  Yes, yes.  The trailer, as I said originally, gave away way too much.



LEO:  Yeah.



STEVE:  I mean, again, not the whole thing.  But it's like, just better to see the movie.



LEO:  Yeah.  You'll enjoy it much more, I think.



STEVE:  And we've had a lot of tweets from our listeners who have said thank you, thank you, thank you for the recommendation.  I might have missed it, if not for you guys talking about it.  So it's been popular among our listeners.



LEO:  Just to address that first review, which is I wonder how much it cost, I think one of the reasons you see a lot of sci-fi take place inside of space ships or in this case inside of a facility is that's cheap.  That's really cheap.



STEVE:  Yes.



LEO:  Because they build one set.  They shoot the whole thing.  Like "Aliens."  Cheap.  They shoot it in a set, and they don't have to go anywhere.  They don't have to do anything.  And I'm going to guess that there was a lot of CGI, a lot of computer graphics in there for - because there's - I can't say anything without giving away anything.



STEVE:  Yeah.



LEO:  But there's enough detail that you would think, wow, this must have taken a lot of work.  So a lot of times when you make a movie you can either do it with practical effects, physical things, or you do it with computer graphics.  And of course sometimes it's cheaper to do it with practicals.  But the kinds of graphics they had I would suspect it was CGI.



STEVE:  Yeah.



LEO:  So CGI and a set and two actors?  Come on.  Cheap.



STEVE:  Yeah, yeah.  And I think there was a lot of running down the same corridor over and over and over.



LEO:  Yeah.  You noticed that corridor didn't change much.



STEVE:  Yeah.



LEO:  Yeah.  You noticed that, did you?  Yeah.



STEVE:  Yeah.



LEO:  But that's fine.  So what?  It was a great movie.  A great story.



STEVE:  It worked.  



LEO:  Put the money...



STEVE:  And again, isn't that what a movie is supposed to be?  We've sort of gone into this whole computer graphics overload, which is where it's more like a videogame than it is a story with content.



LEO:  Yeah.  This doesn't appear to be that, at all.



STEVE:  No.



LEO:  No.  It's quite well done, I thought.  And provocative.  As with all great science fiction, the ideas are what make it interesting.



STEVE:  Yeah.  I got a note from, I guess it must have been email:  "Howdy Steve and team."  He says:  "My name is Bob Johnson."



LEO:  My name is Team.  As Team, I appreciate that.



STEVE:  I should have made it capital T.  He says:  "I've been using SpinRite since v5.  During that time I was in the United States Air Force and was my fighter squadron's SCM," and he says, parens, "(small computer manager)."  He says:  "Basically, I was THE IT department.  We had a copy of SpinRite v5," he says, parens, "(I cannot vouch for its license), but we used it to recover many a crashed drive."



He says:  "So, when SpinRite 6 came out in 2004, I immediately bought one for myself.  It was the best money I have ever spent, hands down.  So now I'm fervently awaiting v6.1.  I have caught you from time to time mentioning your group of testers.  I would like to be one of them.  Am I correct in my understanding that only those that find their way to your forum/usefeed/hideaway of testers get to beta test?  Okay, then.  Challenge accepted.  When I have time to poke around, I'll find you guys.  In the meantime, reserve me a copy of whatever the latest version is.  Take care, see you around."  He says:  "Bob Johnson, avid SpinRite user, Ketonian, and defeater of diabetes, the latter thanks to you and your selfless research with your own n=1" - meaning a sample size of one - "ketogenic experiment."



LEO:  Nice.



STEVE:  So, very, very cool.  And just for Bob and everybody else, I have two places now, for example, where there's SQRL discussion.  There's the public web forums, and then there's our old-school, text-only, boring, dry, Usenet newsgroups.  I am so much happier there.  With a good news reader, which I have, Gravity for Windows, I can mark threads unread.  I can leave posts marked.  It's sort of built-in project management and dialogue and interaction and feedback in a community.  That's where the work of SpinRite 6.1 will be happening.  I just can't - I don't have the same sense of presence or management ability in a web forum setting.  It just isn't there.  And it's not what it was meant for.



So the way this will work is that I'll explain, for people who are interested, how to go over, how to get into the newsgroups.  The good news is most people won't want to.  And frankly, we have a good set of people ready there already.  But anybody with an existing SpinRite license will be able to use that to download whatever it is that is the current version under test, which I'm sure won't hurt anybody's hard drive.  We never have.  So I don't think that'll happen.  But the point is you don't have to participate in feedback in order to be able to play with what I've got.  And it is my intention, as I have said, to be immediately producing useful code.  So I'll be talking about it, obviously, on the podcast, since there's so much interest in that here.  And that's how we'll manage that going forward.  But not in the web forums.  That just does not work for me.  It'll be in the boring, dry, textual newsgroups because it's sort of a built-in project management environment.



Someone calling himself Alan Turing, obviously not, tweeting as @CyberWarrior010, said:  "Steve, not sure you have heard about this yet, so I thought I would drop it in your ear.  I have found an Exim worm that is, get this, patching the Exim vulnerability.  Fun stuff.  I have screenshots if you're interested."  I didn't follow up but I just thought I would share the fact that, I mean, that's very cool.  I mean, it's illegal to do any kind of a worm; but thank you, whoever you are, for breaking the law and patching people's email servers for them.  That seems like a good thing, even though don't do that.



James Alseth said - oh.  He said, quote, "'There are no bugs.'  That's a hell of a claim.  What test suites do you run?  What are your testing processes?"  And I got a kick out of that.  Of course he's referring to my bold claim last week that SpinRite has no bugs because I did release 6 in '04, as Bob mentioned, 15 years ago, and I haven't touched it because there's nothing to fix.  And I liked his question because it sort of reflects old school versus new school.  My testing processes are 15 years' worth of users using it, and tech support standing by to tell me that I need anything to fix.  And so it's actual use for 15 years on countless systems and hard drives that have never revealed that there's anything I can do except to advance it to its next version, which of course is my intention.



And lastly, Spencer Dailey.  He said:  "Hello again, Steve.  This is a neat thing about uBlock origin, probably worth mentioning.  I remember back when you were deciding to switch from NoScript to uBlock Origin, and at the time it was presented as a monolithic 'Do I run JavaScript or not?' dilemma.  Since then, uBlock Origin has added the ability to disable JavaScript on a per-site basis."  Which of course is what NoScript used to have.  He says:  "It's the icon that looks like a script in the bottom right of the popup.  It would probably be good," he writes, "to let listeners know that they can selectively disable JavaScript on a site-by-site basis for a pretty big security and performance gain.  Cheers, and a shout-out would be cool.  Spencer Dailey from Austin."



So Spencer, there's your shout-out and a thanks for making sure our listeners knew that just scripting can be enabled or disabled per site for anybody who is interested in doing that.  It doesn't default normally because, again, we've sort of said, okay, you just can't run the Internet today without scripting.



So as we know, I recently commented that I often don't bother to mention the more or less continuous flow of reports of publicly and inadvertently exposed cloud-based databases.  These reports are not super interesting.  I mean, it's like, yeah, millions of this and millions of that.  But there's no real takeaway from each of those.  If we had access to the internal details of each breach, so that we could see and appreciate exactly how each of those happened, then that would make for some fascinating and probably teachable moments for the podcast.  But normally there is no access to that.  All we hear is somebody discovered this big database that was exposed.



But one of the things we do know is that every one of these breaches must come down to two pivotal mistakes.  First, probably without exception, the data that, well, first of all, the data was never intended to be made public.  Given the nature of the data which is disclosed, it's clearly not the intention of the people who are curating this data that it be publicly exposed.  So that says that the database server should never have been bound to the public-facing server interface.



So the developers of the database systems apparently wrongly assume that network-savvy engineers will be setting these systems up and will know better.  So they'll bind the server to the local interface, not globally, not 0.0.0.0, which typically means all networks connected to an interface.  Rather they'll bind it to the internal LAN side so that it just doesn't answer any calls from the public side.



And then, second, also without exception, we know that all access to the data, even internally, should be protected by some form of strong authentication mechanism, and preferably several forms.  Only if both of these absolutely must-do measures have been ignored could we have the problems that we keep seeing occurring constantly out on the public Internet.  The server is on a public-facing interface.  It's bound to a public-facing interface.  And there's no authentication.



So given that being the case, it's really not accurate to call these data breaches, although the tech press often does, I guess because that sounds sexier.  It's not a data breach when a web browser is used to access a website because a web server is publishing the website's contents.  We don't call that a data breach.  So neither is it a data breach when a database has been published by a database server onto the public Internet so that anyone who queries the server will obtain the database's data.  It might have been an inadvertent publication of that data, but no breaching of anything was required to obtain access.  It was being openly served and publicly exposed.



So this week, once again, several recent data exposures.  And it was them all hitting, especially this third one, that I thought, okay, let's just stop for a minute and talk about this problem.  So in the first of these three instances, we have more than five million records - five million - containing personal information and medical insurance data exposed in a database belonging to the insurance marketing website MedicareSupplement.com.  MedicareSupplement.com is a U.S.-based marketing site which allows visitors to find supplemental medical insurance available in their area.



Last Thursday, researchers said that they found this publicly available.  This happened to be a MongoDB database that had been online for several days and was unprotected by any password or other authentication.  So again, like this five million-plus-record database of Medicare supplement insurance interested people.  Bob Diachenko, who discovered the database along with researchers at Comparitech, told Threatpost, who did some - a lot of people covered this, but Threatpost said, or Bob said to Threatpost:  "While I have not seen any ransom notes or suspicious activity, the IP in question has first been indexed by a public search engine BinaryEdge on May 10.  With a MongoDB exposed in the wild for such a long time," he said, "there is a very high likelihood that it has been accessed by others."



Those five million records contained personal and personally identifiable data, including first and last names, physical addresses, IP addresses, email addresses, dates of birth, and gender.  So if nothing else, here's five million records of strongly deanonymizing data.  And you can imagine, I mean, if we're in a world now where there are parties interested in exactly this, in deanonymizing people for getting where is this email address physically located?  What is the first and last name of the person with this email address, their dates of birth and their gender?  Well, here's five million instances out on the 'Net waiting.



Also, within that subset, a 239,000-record subset of that more than five million also indicated the areas of interest for the customers, I guess beyond just medical insurance.  For example, specifically, insurance for cancer, as well as life, auto, and supplemental insurance.  So there was additional data.  Sounds like it was a big marketing database maybe that the MedicareSupplement.com was using to say, hey, folks, come over and take a look at our stuff.  Anyway, the researchers disclosed the presence of this vulnerable database to MedicareSupplement.com, after which the database access was disabled and then proper security was put in place.  So there's, again, this is an example of the stories that I'm constantly stumbling over and thinking, okay, we don't have - this is only a two-hour podcast.  We don't have time.



LEO:  Only.



STEVE:  To talk about, like, this constant problem with databases in the public.  Second instance.  Also in the news this past week were three, well, and this generated a lot of headlines because Netflix, TD Bank, and another one I have in the show notes I'll get to.  But some big names were in this.  But this is huge.  Okay.  So three publicly accessible cloud storage AWS buckets, apparently under the control of an Israeli-based data management company, Attunity, A-T-T-U-N-I-T-Y, leaked, get this, more than, and we don't know how much more than, but more than a terabyte, yes, that's more than a thousand gigabytes of data belonging to half of the Fortune 100.  The Fortune 100 are the hundred largest companies in the U.S.



So more than half a terabyte, I mean, more than a terabyte - and it's just that a terabyte was sampled - with half of the Fortune 100 and apparently about several thousand other less than the top 100 companies, publicly leaked to the Internet, including internal business documents, system passwords, and sensitive employee information.  UpGuard was the group who discovered this open data, wrote up their discovery under the title "Data Warehouse:  How a Vendor for Half the Fortune 100 Exposed a Terabyte of Backups."



They wrote, and I've paraphrased this a bit and cut out a bunch of the stuff that didn't make sense for the podcast:  "The UpGuard Data Breach Research team" - and again, not really a data breach when you don't have to breach anything in order to get the data - "can now disclose that a set of cloud storage buckets utilized by data management company Attunity have been secured from any future malicious action."  Okay.  We don't really know that, any current malicious action.  Maybe they'll go open again, who knows.  "Attunity, recently acquired by business intelligence platform Qlik" - and I guess that's how you'd say it, it's spelled Q-L-I-K, so probably they're trying to be, you know, Qlik, Q-L-I-K - "provides solutions for data integration."  Yeah.  "An UpGuard researcher discovered three publicly accessible Amazon S3 buckets related to Attunity."



And I'll just stop here because - to editorialize for a minute about Amazon S3 buckets.  You have to work to publicly expose them.  I use Amazon.  I use Amazon S3.  And, for example, I'm hosting videos, SQRL how-to videos, on the SQRL Forums through Amazon AWS.  And it's work to make these things public.  They're not public by default.  So I'm always curious when I see, oh, yeah, there was an Amazon bucket exposed.  Well, someone did that on purpose.  Maybe they pushed the wrong button?  I don't know.  But, I mean, you have to try.  I just wanted to say that.  It's not like it happens by chance.



Anyway, so they said:  "Of those three, one contained a large collection of internal business documents."  And oh, my god, Leo, I thought I had the link in the show notes, and I don't, because you would be scrolling through, and your head would be spinning if you saw what was there.  I mean, like, oh.  Anyway, they said:  "The total size is uncertain, but the researcher downloaded a sample of about a terabyte in size, including 750 gigabytes of compressed email backups."  Think about that, 750GB of compressed email.  Email compresses down to nothing in sizes because it's typically text.  So 750GB of email, that's like everything from the dawn of time for, like, a huge number of people, 750GB.



They wrote:  "Backups of employees' OneDrive accounts were also present and spanned the wide range of information that employees need to perform their jobs:  email correspondence, system passwords, sales and marketing contact information, project specifications, and more."  In other words, this was an archive of what we were just talking about, Leo, for being protected against ransomware.  These were the encrypted, well, I'm sorry, not encrypted.  They were the backups of people's systems that were exposed in this Attunity bucket.



"On May 13th, 2019," they write, "an UpGuard researcher discovered publicly accessible Amazon S3 buckets named 'attunity-it,' 'attunity-patch,' and 'attunity-support.'  The oldest files in 'attunity-it,' where the bulk of the sensitive data was stored, were uploaded in September of 2014" - so nearly five years ago - "though that does not necessarily mean that they were publicly available at that time.  The most recent files had been uploaded just days prior to the researcher discovering the bucket."  So the bucket was in active use at the time of its discovery and contained at least five years of archived customer data, meaning the data was current and historical going five years back.



"The researcher," they write, "notified Attunity on May 16, 2019.  As a result of time zone complications," they said, "and due to Attunity having been recently acquired by Qlik, the researcher ultimately wound up speaking on the phone with Qlik support.  By the next day, public access to the buckets had been removed.



"In previous cases," they wrote, "we have discussed the complexity" - I love this.  "We've discussed the complexity of fully analyzing and describing data stores that contain copies of users' workspaces and mailboxes.  Work lives are spread across countless files, each one with some importance, but difficult to reduce to any single metric of significance.  While various tools can help search across large data stores, gaining those quantitative measures comes at the expense of analyzing the qualitative nature of the data."  In other words, there was too much data.  Sorry, but 750GB, like we didn't know what we had because there was too much to look at.



They said:  "There may be a large number of email addresses, for example, but whether they are enterprise customers or merely marketing targets changes the significance of their inclusion.  A few key examples from the Attunity data set can illustrate the kinds of data that users have access to and which can be exposed by misconfigured storage of those users' file collections, as in this exposure of Attunity data."  And then they said:  "According to Attunity's website, more than 2,000 enterprises and half the Fortune 100 use Attunity."



And, sure enough, a file with a client list found in the repository included a client list with a number of companies commensurate to that description.  So they also found, apparently, Attunity's customer list and, oh, look, there's half the Fortune 100 and a couple thousand others.  They said:  "Every business must have customers, and having commercial relationships entails some exchange of information, leading many data exposures to involve third-party data for customers or other involved entities.



In this case, Attunity's business in cloud migration and data integration also involves supplying and managing the software that processes customer data.  Exhaustively documenting the files associated with each of thousands of companies is not feasible or necessary for the research team's purpose of raising awareness of the risks of data leaks.  As in past reports, a few examples can highlight the kinds of information that can be exposed through misconfigured storage, and were exposed in the case of Attunity."



Okay.  So their report then shows page after page of horrifyingly exposed data, even after having been heavily redacted for their own responsible disclosure, things like password reset emails, mail saying "This is your VPN PIN and password," and much, much more.  The idea of all of this data having been publicly exposed for who knows how long is sobering.  This is the internal data of half the Fortune 100 and thousands of other large companies who were using this service.  And it was inadvertently public.



And of course, depending upon whose hands this fell into - and we don't know because it was there, and UpGuard found it; and then, after they reported it, whoops, it disappeared.  But we don't know how long it was open.  We don't know who and how many people sucked out these terabytes of internal company data exposing thousands of companies' archived emails, OneDrive backups, and workstation contents.  We just don't know.  The tech press noted that among these companies were Netflix, TD Bank, and Ford.



Anyway, in conclusion, UpGuard's write-up said, as they're concluding, they said:  "Attunity's business is to replicate and migrate data into data lakes for centralized analytics.  The risks to Attunity posed by exposed credentials, information, and communications then are risks to the security of the data they process.  While many of the files are years old, the bucket was still in use at the time detected and reported by UpGuard, with the most recent files having been modified within days of discovery.



"The chain of events leading to the exposure of the data provides a useful lesson in the ecology of a data leak scenario.  Users' workstations may be secured against attackers' break-in, but other IT processes can copy and expose the same data valued by attackers.  When such backups are exposed, they contain a variety of data from system credentials to personally identifiable information.  Data is not safe if misconfigurations and process errors expose that data to the public Internet."



And I think that was a great point.  The point here is that attackers needn't try to seduce their way into an enterprise's workstations to look around when their full backups are available for the taking due to the misconfiguration of an affiliated third-party cloud services provider.  I mean, this was horrible.  And we have no idea how long it was there before UpGuard found it and said, "Guys, your backdoor is way open."  Yikes.  And we're not done yet.



LEO:  There's more?  But wait.



STEVE:  Yes, but wait.  vpnMentor reports "Orvibo Smart Home Devices Leak Billions of User Records."  They wrote:  "Our expert cybersecurity research team, led by Noam Rotem and Ran Locar, discovered an open database" - once again, on the Internet, open database - "linked to Orvibo Smart Home products."  And Leo, go to Amazon and put in "Orvibo Smart Home" into the Amazon search.  O-R-V-I-B-O.  These are real people.  The database, get this...



LEO:  Despite the name.



STEVE:  Yes, despite the name, they're apparently well known.  So this open database includes two billion logs that record everything from usernames, email addresses, and passwords, to precise geolocations.



LEO:  Oh, man.



STEVE:  As long as the database remains open, the amount of data available continues to increase each day.  "Orvibo claims to have around a million users.  These include private individuals who connected their homes, as well as hotels and other businesses with Orvibo Smart Home devices.  This constitutes a massive breach of privacy with far-reaching implications," these guys write.  "The data breach affects users from around the world."  They said:  "We found logs for users in China, Japan, Thailand, the U.S., the U.K., Mexico, France, Australia, and Brazil."  They wrote:  "We expect that there are more users represented in the two billion-plus logs.



"We first contacted Orvibo via email on June 16.  When we did  not receive a response after several days, we also tweeted the company to alert them to the breach.  They still have not responded, nor has the breach been closed."  So in this case we have a company producing IoT devices, smart home devices, with more than a million active users, and a log which is daily being posted to by these devices numbering more than two billion  records at this point.



They said:  "The amount of data available from Orvibo's servers is enormous.  It's also highly specific, which shows just how much data smart home devices can collect about their users.  According to the company, there are over a million users who have installed Orvibo products in their homes and businesses.  The Chinese company, based in Shenzhen, manufactures 100 different smart home or smart automation products.  Data included in the breach" - and Leo, in the show notes I have some redacted screenshots from the logs - "email addresses, passwords, account reset codes, precise location, IP address, username, userID, family name, family ID, smart device, device that accessed the account, scheduling information."  And so I have a picture that's been redacted showing this information.  And without the redaction, there's the actual stuff.



And they say:  "In this first example, we can see that Orvibo is collecting a large amount of data about its users.  In this case, not all of the data points are recorded; however, we have other examples that include very specific geo-data, chosen family names, usernames, passwords, and the reset codes that would allow for account takeover."  And they show another screenshot of that, and then a screenshot that follows.



They say:  "These data logs are for the same account, which we can verify with the matching email address and user ID number.  In the first, we only have the email address, IP address, and a reset code.  With this code accessible in the data, you could easily lock a user out of their account, since you don't need access to their email to reset the password.  The code is available for those who want to reset either their email address or password.  This means a bad actor could permanently lock out  a user from their account by changing first the password, then the email address.  Orvibo does make some effort to concealing the passwords, which are hashed using MD5 without salt.



"The above example is a small sample" - okay, remember, more than two billion of these records - "a small sample of the kind of geolocation data we have.  Orvibo keeps logs of precise longitude and latitude coordinates, spelled L-A-T-O-T-I-D-E in the data.  The precision of the coordinates can lead us to a user's exact address.  This also demonstrates that their products track location in their own right, rather than determining location based on an IP address.



"In this entry from a user in Mexico, it shows exactly which device the user was connected to when the data was logged. According to the Orvibo website, HomeMate is a full smart home system that employs a full range of their products to connect your entire home.  This amount of data shows just how vulnerable a user can be should a hacker take advantage of this breach."  Data which, again, is right now publicly available.



"One of the products Orvibo offers is a smart mirror.  This includes technology to show the weather and display the user's schedule.  Here, we have a log for the schedule the user has set with a customized name, 'Winter Week Morning,' giving us precise information about the user's calendar.



"This is a data log that includes a large number of devices connected to a single account.  We can see a clear record of the user having one of Orvibo's smart cameras.  Another device is named 'massage room.'  Though not all of the device names tell us which device is where, it could help someone to pinpoint a device to hack if they wanted to do so.  The 'massage room' label also points towards this data likely belonging to a business.



"Another Smart Camera log included a message that was recorded word for word.  That opens the possibility of a user revealing even more personal information through their account.  It's important to note that not every single data log included every type of personal information.  However, even with over two billion records to search through, there was enough information to put together several threads and create a full picture of a user's identity."



Okay.  Now, just for a minute, stop to consider that this is a Chinese company, based in Communist Shenzhen China.  The concern here is with a massive and incredibly irresponsible disclosure of extremely sensitive customer data.  But even if this massive multibillion-record database were not exposed, it is still being gathered and retained.  But to what end and business purpose?  Why log all of this?



"A breach of this size," they say, "has massive implications.  Each device in Orvibo's product catalog can have a different negative impact on its users.  This is on top of having an abundance of identifying information about users.  Much of the data can be pieced together both to disrupt a person's home while possibly leading to further hacks.



"Though Orvibo does hash its passwords, we tested the security ourselves to see how easy it was to discover the real password.  In some cases, we uncovered our own password.  In order to test this, we created our own account, then searched for our email address to see what account information was accessible.  Though our chosen password was hashed, it was easy to crack."  So they set up an Orvibo device, created an account, set up a password.  Then they went and pulled the transactions from the publicly exposed transaction database on the Internet, found it, found their hashed password, and reversed the simple MD5 hash in order to verify that that's what was going on.



They said:  "If Orvibo had added salt to their hashed passwords, it would have created a more complex string that is far more difficult to crack.  Salt" - and blah blah blah.  We know about salt.  But these are unsalted MD5 passwords. 



They say:  "There are Orvibo devices whose poor security could have severe consequences.  A number of the devices offered by Orvibo fall under the umbrella of 'home security.'  They include smart locks, home security cameras, full smart home kits.  With the information that has leaked, it's clear that there is nothing secure about these devices.  Even having one of these devices installed could undermine, rather than enhance, your physical security.



"There's enough information leaked from the database that it makes taking over a user's account a simple task.  A malicious actor could easily access the video feed from one of Orvibo's smart cameras by entering into another user's account with the credentials found in the database.  At the same time, it would be easy to unlock a door from the same account.  With precise geolocation, this simplifies home break-ins, an event smart homes are supposed to help protect against."



And Leo, of course this perfectly dovetails on what we were talking about last week, how utterly blind we are to everything having to do with today's IoT technology.  You know, this is a big deal.  A huge number of people are installing interconnected, networked, essentially black boxes all over their homes, pressing a few buttons, and presto, look.  You know?  I can check the dog while I'm away from home, see what the dog is doing.  And of course so can someone who may not have your best interests at heart.



The industry has recently been talking about a worm, as we know, this notion of a BlueKeep worm, with great concern, so much so that, as we've talked about it, Microsoft has issued multiple warnings.  The U.S.'s NSA and DHS have done the same.  But all of those two-plus billion Orvibo transaction log records were created by machine so they could be read by machine and acted on instantaneously, en masse, by machine.  So if someone in Communist China wished, or somebody who grabbed that database off the public Internet wished to, an attack could be launched against a million of Orvibo's customers instantly.  It's horrifying.



Anyway, so that brings us to my somewhat serious proposal for a renaming of what IoT stands for.  I don't think it should be Internet of Things.  I think IoT should stand for Installation of Trojan.



LEO:  Yup.



STEVE:  And again, why are they logging billions of transaction records of their million-plus customers?  Why?  What is the business purpose of keeping that kind of log?  As an IoT vendor, we're trusting them.  We press some buttons and, oh, look, all of our stuff is connected to the cloud, to something.  Orvibo in Shenzhen.  We have no control over anything that happens.  I mean, and that's important.  We have no control.  We don't know what is going on.  Certainly these million Orvibo users did not know that all of the information that they put into their apps and management, the schedule which they have given to their Magic Mirror that shows their schedule in the morning, is publicly accessible to anybody on the Internet.  It is really the Wild West.  And what people are doing is installing trojans into their homes. 



LEO:  And I wonder how much intentionality there is from Orvibo.  Like what are they up to?  You know?  It doesn't seem like it's an accident.



STEVE:  Well, the logging cannot be an accident.  Somewhere, I mean, they know, they're providing storage space.



LEO:  They know they're doing this, yeah.



STEVE:  Yes.  Certainly it being public, one imagines, one has to imagine...



LEO:  [Crosstalk] intention.



STEVE:  But they're not even responding to email or tweets.  Like they don't care.  Incredible.



LEO:  Yeah.  And even if your stuff isn't from a Chinese company, it's often made in China.  So, you know.  Who knows what's going on?  We've seen [crosstalk].



STEVE:  Well, we have all of this whole Huawei going on right now with do we trust, I mean, as I've said, I'm surprised other countries are using Windows.



LEO:  Right.



STEVE:  You know?  That's just - that's bizarre.



LEO:  Right.



STEVE:  And it does make sense after that scare we had of the Supermicro servers where the question was raised, is there some weird hardware being embedded in the motherboards of these servers?  It looks like that was a bogus report, but it could happen.



LEO:  Yeah.  Okay, well, if it happens, you'll hear it here first.  You can listen to us do this show live, if you want.  We do it Wednesdays, I'm sorry, Tuesdays at 1:30 Pacific, 4:30 Eastern, 20:30 UTC.  There's an audio and video stream always running of what we're doing live or rebroadcasts, if we're not live at TWiT.tv/live.  You could choose your favorite stream there.  If you are watching, you should probably be in the chatroom or listening at irc.twit.tv, where all the other folks are watching and listening live.



Steve has on-demand versions of the show, both audio and really nicely written transcripts.  They take a few days to post after the show is over.  Elaine's going to be typing this evening and working on that.  You'll find those at GRC.com.  While you're there, check out SpinRite, the world's finest hard drive maintenance and recovery utility; progress on SQRL, the SQRL Forums are there.  And there's lots of free stuff, useful stuff like a password generator that is second to none, all at GRC.com, the Gibson Research Corporation.



You'll find Steve on Twitter at @SGgrc.  And that's one of the ways to reach him.  You can direct message him at @SGgrc.  You can also go to GRC.com/feedback and fill out the feedback form there.  He, I know, likes to hear from you.  You can also get audio and video of this show from our site, TWiT.tv/sn for Security Now!, TWiT.tv/sn.



But as always, with all of our shows, we encourage you to subscribe.  That's the best way to not only assure that you'll have a copy ready for your Wednesday morning commute, but that it helps us because we know you're going to download it.  You won't skip it by accident.  So subscribe in your favorite podcast application.



Steve, thank you so much.  We'll see you next time on Security Now!.



STEVE:  Thanks, buddy.



Copyright (c) 2019 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.










GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#722

DATE:		July 9, 2019

TITLE:		Gem Hack & Ghost Protocol

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-722.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we stumble over a number of instances where technology appears to be colliding with the status quo.  In any complex social system, individual and group interests are often complex and may be in opposition.  So when new technology comes along to offer new capabilities, not everyone is going to be pleased.  So this week we discuss some of the mounting tensions being created by connectivity, storage, and computation which are being combined to create many new capabilities.  We look at the surprising backlash to Mozilla's privacy-enhancing DNS-over-HTTPS support, concerns over the use of facial recognition and automobile license plate scanners, and the future of satellite-based Internet services.  We present some SQRL news and share a bunch of closing-the-loop feedback from our listeners.  We then examine how a Ruby code repository was hacked and look at the U.K. GCHQ's proposal for adding "ghost" participants into private conversations.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  We're going to talk about DoH.  It's not what you think.  It's DNS-over-HTTPS.  And it's why you might want to look at your router and see if you could do it.  We'll also talk about satellites, satellites all over the place, maybe too many satellites.  And a Gem hack that any Ruby user is going to want to know about.  This is all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 722, recorded Tuesday, July 9th, 2019:  The Gem Hack and Ghost Protocol.



It's time for Security Now!, the show where we cover the security practices of a wide-ranging spectrum of people including city governments and people like you and me with this guy right here, Mr. Steve Gibson, our security in chief enforcer.  Hi, Steve.



STEVE GIBSON:  Leo.  Great to be with you again.  As a matter of fact, we have a listener who is in one of the townships that we have been discussing.  And so we have some interesting insider feedback from that person, who was a little chagrined, he wrote, to hear his town named in Security Now!.



I said last week that I wanted to talk about this GCHQ Ghost Protocol.  But I didn't feel like that was going to be enough to talk about.  Now I'm kind of thinking, well, maybe, because it keeps bringing me back to my annoyance with the fact that the arguments being made are semi-specious.  And we'll get to that because I don't - our listeners get confused when they hear me suggest that maybe encryption is not always going to be absolute.  They think that I'm promoting the idea that it's not always going to be binary.  And that's not it at all.  I'm annoyed at the argument that it can't be done.  It can be done.  But the proper argument is, is that what we want?  I mean, so that's the proper place to frame the argument.  Anyway, we're going to get to that.



Also there was an interesting attack on a Ruby Gem repository.  And what was so fascinating was that the person who discovered it explains the process that he went through.  So I thought that would be sort of a cool anatomy of like how repositories get infected and exploited.  But overall, there were like sort of a  number of things sort of, you know, sometimes the podcast acquires a theme sort of by itself.  And in this case there were a number of instances where our technology appears to be colliding with the status quo.  So I want to discuss a number of instances where there are mounting tensions between the way things have been and the new capabilities that are being created by connectivity, huge storage, amazing computation, and sort of the problems that those are creating.



For example, we're going to look at a surprising backlash to Mozilla's privacy-enhancing DNS-over-HTTPS support.  We've got concerns over the use of facial recognition and automobile license plate scanners, and also the future of satellite-based Internet services that will create censorship problems potentially for repressive governments.  We also have some SQRL news, a bunch of closing-the-loop feedback from our listeners - I teased one of them before, but there's more - and then we'll finish, taking a look at this Ruby code repository attack and also sort of, again, taking a look at this Ghost Protocol, the idea that the GCHQ has proposed, and what I think is the unsupportable argument against it and sort of wishing that we just used the argument of what do we want?  Technology will give us anything we want, so we just have to ask.



LEO:  As long as you consider the unintended consequences, I guess, because there's always that, yeah.



STEVE:  So xkcd has done it again.  This cartoon, our Picture of the Week, made the rounds a few weeks ago, and I just had to push it out on our stack in favor of things that were a little more apropos.  But I wanted to get to it.  So it's xkcd.com/2166.  And we've talked about a number of different places where software architecture forms a stack.  There's the famous networking stack where you have the physical layer, then the logical layer, then the transport layer, application layer, and so forth, the idea being that there's a stack of layers where each layer has its functions constrained to do just one specific well-described and well-defined job.  It relies on services provided from the layer below, and it offers services up to the layer above.



And so this notion of a software stack takes many different forms in our industry.  You can also have like a development stack, or you have a server on the top and then a language that the server calls in order to render pages for the website, and it then relies on underlying libraries.  So again, there are different types of stacks.



Anyway, this one is great.  It's titled "The Modern Tech Stack."  And at the very bottom of the stack we have "Massive Undiscovered Hardware Vulnerability."  Then on top of that we have "Compromised by a Foreign Government" or "Compromised by Our Own Government," you know, so basically that's the next layer.  And then up above that is "Compromised by Unknown Hackers," "Compromised by Bitcoin Miners," or "Compromised by a Current Employee."  And then above that is "Compromised by a Former Employee" or, on the very top, "Compromised by a Customer."  In other words, pretty  much every possibility.  We've got customer, former employee, current employee, bitcoin miner, unknown hackers, our government, some other government, or the hardware doesn't work.  So, yes, welcome to the 21st Century of security software.  Anyway, just another great cartoon by xkcd.



So you would never expect something as apparently clean, simple, and beneficial as enabling DNS lookups over HTTPS to stir up any controversy.  Seems like a good thing.  We all know what a problem it is that, despite every website we visit now being HTTPS, with few exceptions, where its identity is authenticated because we trust the entity that signed its certificate asserting its identity, and all of the communications back and forth are encrypted, relying on this TLS technology.



But unless special measures are taken, even though we've got all the sites we talk to are HTTPS, unless we do something special, all of our browsers' DNS queries to those websites and all other domains, each of the pages we receive from those websites, and as we know sometimes that could be hundreds of subsidiary domain queries because of all of the junk that is coming in from all corners, every one of those pages spews forth a storm of unencrypted UDP packets carrying a DNS query formatted payload in order to - so outbound is the domain name that our browser needs to get the IP address for, and the response is an IP or a collection of IPs where we can access that resource.  They're all unencrypted.  They're all in the clear.



So the concern has been that anybody passively monitoring our traffic, like our ISP, who's like first step off of our connection as our data goes out to the Internet, or, well, anyone passively monitoring our connection is able to see everything that we're doing, not into our connection, but they know where we're going based on the queries that we're making.  But then also it means that, because it's unencrypted and unauthenticated by default, any active interception can manipulate the DNS replies to send our browser to some other server.  And if that other server has arranged to obtain a certificate that we trust, and unfortunately with so many certificate authorities globally now and our browsers trusting all of them that's not a stretch, especially if the entity doing the interception is highly placed, then we don't really have any security.



So we've talked in the past about how Mozilla and Firefox and Google with Chrome are moving toward this idea of doing DNS in a different way, of tunneling DNS queries over TLS to a remote DNS server that establishes a single connection.  So in this mode, which is described in RFC 8484, the browser no longer hands the job of DNS lookups over to its host operating system.  That's what all of our browsers do now is the OSes do that job for the browser.  Our OS has a DNS cache.  It has a DNS resolver.  So the browser just makes a query through an OS API saying, hey, look this IP up for me.  That goes to the operating system, and it's our network adapters then that know the IP to query for DNS.



They typically get that by making a DHCP, Dynamic Host Configuration Protocol query, to typically our router that we have for our local environment.  It may be a DNS server itself.  But more often it's just passing through the IPs that it in turn received when it made a DHCP query to our ISP, using our ISP's DNS, which makes sense because we want the answer to come back quickly, and you want a DNS server to be close to you so that you're not - because all of your connections wait on getting the IP to connect to after sending out DNS queries.  So DNS response time is very important.



So about a year ago, more, about a year and a half ago actually, Mozilla began experimenting with this.  We talked about it at the time.  And it was, by every measure, a complete success.  That is, the idea of establishing a connection, an HTTPS connection to what's known as a DoH, DNS-over-HTTP, a DoH provider, and then tunneling the same lookups through that connection.  So what that means is the browser no longer asks the OS.  It itself establishes the connection.  So what this provides, of course, is complete protection from DNS UDP passive monitoring or active interception.  Cloudflare offers a service.  Google offers a service.  Quad9 offers a service.  So there are multiple places now, providers that are well connected, global, and offering DNS-over-HTTPS.



And it turns out, for example, Firefox.  You go to, in Firefox, go to Options in the browser's menu, scroll to the bottom of the first general page that is displayed, click on Settings there under Network Settings, and at the bottom of the dialog that pops up you'll find a checkbox:  Enable DNS Over HTTPS.  So you put a check in the box.  And unless you have some reason for choosing some other DNS provider, you leave it set to Cloudflare, which Mozilla has established a relationship with for this purpose.



And now all of your DNS queries are - so at that point, when you turn that checkbox on, Firefox opens up a single persistent connection to a local Cloudflare endpoint using their global geolocation-aware technology so that you're connecting to someone relatively close to you.  And all DNS goes through this HTTPS tunnel and is replied to.  So from someone passively monitoring your bandwidth, whether they're on the Intranet or out close to you like your ISP on the Internet, suddenly, like, oh, what happened to DNS?  This person's not making DNS queries like they used to before.



Okay.  So that seems all good.  What's the problem?  Believe it or not, entities in the U.K. are all up in arms.



LEO:  They were pissed off.



STEVE:  Yes.



LEO:  They called them the "Internet enemy."



STEVE:  I know, Leo.



LEO:  By the way, as soon as I read that article, I started using Firefox.  I said, that's it.  If they're the enemy of the Internet, I'm using them.  Holy cow.



STEVE:  I know.  The ISPA, the Internet Service Providers Association, has named Mozilla one of the three finalists for their Internet Villain of the Year.



LEO:  Yeah, they're villains.



STEVE:  For, quote, I'm quoting them, "their proposed approach," that is, "their" as in Mozilla's, "proposed approach to introduce DNS-over-HTTPS in such a way as to bypass U.K. filtering obligations and parental controls, undermining Internet safety standards in the U.K."



LEO:  Shocking.  Shocking.



STEVE:  And Leo, okay, so I'm at this - I go to this ISPA page, the Internet Service Providers Association, as I'm pulling the pieces of this thing together.  And I just had to shake my head when, at the bottom right-hand corner, I was presented with this popup that I have in the show notes here.  It says, down in the lower right-hand corner for me - oh, and you've got it, too.



LEO:  Yeah, because I'm using Firefox.



STEVE:  Uh-huh.  It says - so this is the ISPA; right?  Who is upset that Mozilla is going to be tunneling DNS-over-HTTPS for all the benefits that we just outlined.  The popup reads:  "It looks like your cookies are switched off."



LEO:  Oh, yeah.



STEVE:  "To ensure the best experience whilst visiting our website..."



LEO:  So British.



STEVE:  "...please consider allowing cookies."



LEO:  Yes.



STEVE:  "You can find out how to change your settings or more about cookies we use at the bottom of this page."  Unfortunately, I think it was covered up by the notice.



LEO:  Yeah, I don't see it, yeah.  I'm looking.



STEVE:  Yes, the ISPA has our best interests at heart.  That's right.



LEO:  Well, part of this is, remember, that the U.K. wants to do this licensing system that, if you don't get a license, you can't see porn on the Internet, because they're trying to keep porn out of kids' hands.  So the people who are going to - one of the groups, you can go into a pub to get a license.



STEVE:  To monetize?  Ah.



LEO:  No, no.  You can go to a pub to get a license.



STEVE:  Oh, you go in.



LEO:  You have to prove you're - so you have to go with your driver's license or some age proof, and then you get a license that allows you to surf the Internet freely.  One of the people who is doing this is a company called MindGeek, which runs YouPorn and pretty much every porn site you ever heard of is run by MindGeek.  So you have to - what could possibly go wrong? - go to a MindGeek site, give them your driver's license, your passport, proof of age, and they will give you a license to use the Internet freely.  This is such a bad idea.



STEVE:  I do think I remember seeing somewhere, as I was just scanning this, that they want your phone number, as well.



LEO:  Yeah, oh, yeah, all sorts of stuff.  The funny thing is they passed this bill and were about to implement it when they realized they hadn't told the EU because they thought, well, by now we'll be out of the EU.  So they had to stop.  They're not.  So among all the other Brexit problems, this is another one.  They actually had to put this change off until either they tell the EU about it or they're not in the EU anymore.  And I don't think the EU will allow it.  So it's just a mess.  It's so - it's such a mess.  Oh, my god.



STEVE:  Well, yeah.  And so here we have a problem.  We're advancing privacy and security for web browser users, yet there's, I mean, the other side of this is that - so from the research I did, in the U.K., ISPs are legally forced to block certain types of websites.



LEO:  Right, right.



STEVE:  Such as those hosting copyright infringing or trademark content.  Some ISPs also block other sites at their discretion such as those that show extremist content, adult images, child pornography, and so forth.  These latter blocks are voluntary and are not the same across the U.K.  But most ISPs usually tend to block child abuse content, which seems like a good thing.  Unfortunately, of course, we all know that this isn't a strong protection anyway.  I mean, so the idea being that there are filters that the ISP manages that match on known domain names and do not return an IP address or return some placeholder IP, you know, redirection page, if you try to get that.



In mid-May, Baroness Thornton, MP for the Labour Party, brought up the DoH protocol and its impending support from browser makers in a session of the House of Commons, calling it a "threat to the U.K.'s online safety."  And, similarly, GCHQ, Britain's intelligence service that we'll be talking about later in the podcast, has also criticized both Google and Mozilla, claiming the new protocol would impede police investigations.  And of course I cued on that because it's like, wait a minute.  How would this impede police investigations unless there was passive eavesdropping going on over on the wires that would cause matches?  And then GCHQ continues, saying it could undermine its existing government protections against malicious websites.  Okay, but how tunneling your DNS queries impede police investigations is really unclear.



The Internet Watch Foundation (IWF), a British watchdog group, also with a declared mission to minimize the availability of online child sexual abuse content, also criticized both Google and Mozilla, claiming the browser makers were ruining years of work in protecting the British public from abusive content by providing a new method for accessing illegal content.  Now, of course, remember that, like, any VPN does this; right?  I mean, this is kind of a VPN for DNS.  Whereas the way it's always been is that we've just been spraying unencrypted UDP out onto the Internet, now it's in a browser tunnel.  I mean, they're not able to see into HTTPS content.  Now we're simply HTTPS tunneling DNS queries.  So yeah, that's, you know.  But, oh, it's a new method for accessing illegal content.



In their coverage of this, ZDNet noted that essentially Google and Mozilla support for DoH effectively narrows down to the same moral dilemma that surrounds the Tor Project and the Tor network.  Which, yes, it upsets people because it allows people to be anonymous.  Browser makers, ZDNet wrote, "Browser makers must now decide if it's worth supporting a tool that brings privacy improvements to millions at the expense of a few that may have to suffer."



When ZDNet asked Mozilla for a comment on its nomination, right, the Villain of the Year nomination, Mozilla replied:  "We're surprised and disappointed that an industry association for ISPs decided to misrepresent an improvement to decades-old Internet infrastructure.  Despite claims to the contrary, a more private DNS would not prevent the use of content filtering or parental controls in the U.K.  DNS-over-HTTPS (DoH) would offer real security benefits to U.K. citizens.  Our goal is to build a more secure Internet, and we continue to have a serious, constructive conversation with credible stakeholders in the U.K. about how to do that," Mozilla said.  "We have no current plans to enable DoH by default in the U.K.  However, we are currently exploring potential DoH partners in Europe to bring this important security feature to other Europeans more broadly."



I was unable to see anything that definitively talked about Mozilla's plans for default enablement.  We do know, you know, one of my favorite coined terms is the "tyranny of the default."  We know that most users will never dig down into any of their browser settings and flip any switches.  So our listeners, and you, Leo, and I, you know, we have DNS-over-HTTPS.  I just explained how to do it if you are a Firefox user.  And it's coming soon to a Chrome browser, and probably a Chromium-based browser near you, which expands that field even more broadly.  So it's a matter of turning that switch on.



What really matters, then, is whether our browser vendors decide to make it the default.  And so what Mozilla is saying is no, no, no.  We have no plans to make it the default in the U.K.  I don't know whether that will change over time.  Who knows what?  But again, I thought this was a perfect example of where, yes, just improving things, making things more secure and private, does generate some backlash.  And unfortunately, Mozilla - there are two other contenders for Villain of the Year that this association has named.  We'll see who ends up getting the...



LEO:  Strike Mozilla from the list because, as of today, the ISPA has withdrawn Mozilla.



STEVE:  Oh, yay.



LEO:  I think they're a little stung:  "In the 21 years the event has been running, it is probably fair to say that no other nomination has generated such strong opinion."  Then they go on about to show all the great things.  "The villain category is intended to draw attention to an important issue in a lighthearted manner.  But this year we clearly sent the wrong message, one that doesn't reflect our genuine desire to engage in constructive dialogue.  We are therefore withdrawing the Mozilla nomination."



STEVE:  Yay.



LEO:  They still think it's important to "scrutinize the plans."  And they say, oh, yeah, data protection, security, online safety, user choice, user consent.  You've got to pay attention, Mozilla.  So they got their attention.  The award ceremony is day after tomorrow, in case you're curious.



STEVE:  What this certainly does is, if they want to impose these sorts of filters, they'll have to do it in a different fashion.  But it is certainly, I mean, it's not robust to use DNS anyway.  So this probably means they will end up with better filtering of the stuff they want to filter.  And the vast majority of people will end up with way more privacy.



I did want to segue from this to mention DoH in SOHO routers.  This is a feature that our listeners might want to be on the watch for when they're next shopping for a small office or home router or consider upgrading.  The advantage of doing this, of course, is that then all of the network's DNS queries within your local LAN would be forwarded through the router to its own tunneling to Cloudflare or Quad9 or whomever.  My current favorite router that I've been talking about, Netgate's SG - no relation - 1100, which runs FreeBSD and the wonderful pfSense firewall router, uses the unbound DNS server which has supported DoH from the start.



So anyone with pfSense, or even if you didn't use that router, if you're running pfSense on an old PC because, I mean, it's very lightweight.  FreeBSD and pfSense, they have a bootable package, easy to install.  So it's simple to set up a router.  Anyone with pfSense can follow a link that I have in the show notes to the Netgate blog posting titled "DNS Over TLS With pfSense," which shows how to quickly enable this slick feature.  And when you do that, your little router sitting there will open - it'll establish a TLS, you know, HTTPS tunnel to whichever provider, free provider you choose.  I'm blanking on the DNS provider that we've talked about for years.  I think they offer...



LEO:  OpenDNS?



STEVE:  OpenDNS, yes.  My bet is that they also offer a free DoH service.



LEO:  I bet, too.



STEVE:  I haven't mentioned them, so I just wanted to touch on them.  Also anyone using an OpenWRT-based router, that is, a router where you have reflashed the firmware to run OpenWRT, it also can do DoH for you.  So if anyone is interested in pursuing this, and of course this is - not only does this then encapsulate your browser's DNS queries, but all of those made by your OS, all of your iOS devices running on your local WiFi connections and so forth.



LEO:  OpenDNS uses DNSCrypt, which has the same effect, right, as DoH?



STEVE:  Yes, it does.  There you install a local client in your machine as sort of a proxy, and then your OS's queries go through that client and over to OpenDNS. 



LEO:  They don't - "We're not using SSL.  While we make the analogy DNSCrypt is like SSL in that it wraps all DNS with encryption, it's not the crypto library being used.  We're using elliptic curve cryptography."  Maybe they prefer it to SSL.



STEVE:  That's all good.  Try...



LEO:  It's a lot easier.  You don't have to install anything.



STEVE:  Well, try OpenDNS space DoH and see if that...



LEO:  I did.  This is what I found, yeah.



STEVE:  Oh, okay.  So if they're not, they're probably going to because they're going to want to not lose a bunch of people to Quad9 and Google and Cloudflare.  And it's looking like DoH, I mean, DNSCrypt was - I would regard that as an early pre-standard proof of concept.  But it's like it didn't win.  And DoH is what we have an RFC 8484 for, and it's what all the browsers are going to be supporting.  It's what you get, as I was mentioning, in our SOHO routers.  So I think that's going to be the standard that ends up taking over.  So I would think OpenDNS will probably bring up a service to offer that.



LEO:  Yeah, unless they're all in on DNSCrypt.  But you'd have to install DNSCrypt on your side to use it, so that's not so good.



STEVE:  Yeah, yeah.



LEO:  It also breaks DNSSEC, which is another issue.  Does DNSSEC still work with DoH?  I'm sure it does.



STEVE:  Yeah, yeah.



LEO:  Doesn't matter.



STEVE:  Yeah.  Because DNSSEC are just internally signed replies where your local resolver says, oh, yeah, the signature works.



LEO:  Right.



STEVE:  So remember all those crime shows 20 years ago where a computer's facial recognition system would bring up a photo of some suspect.  And that big photo is always over on the left-hand side of the screen.  I don't know why it's always on the left, but that's where it always is.  And then a bunch of lines would be drawn over it by a computer to form a facial recognition map.



And then over to the right, and it also is always over on the right, the upper right of the screen, the computer would then scan through a gazillion faces, like just flicker through them super fast, presumably pretending to be matching them, and suddenly it would - sometimes it would slow down if it was a cheesy show, or it would just stop, which is what you would hope because it's not a roulette wheel.  And you would be looking at the matching face.  And then, after a dramatic pause to make sure the audience had all caught up, the personal details of the individual's criminal record would paint down the screen below the matching photo.  So we've been seeing that for decades.  And it was never true.



It turns out it's true now.  This is one of those things which has sort of quietly happened.  And it's being done at a level that surprises people who haven't been really paying attention.  We've got, as we know, this explosion of mass storage that allows everything to be stored.  Just that.  It's like, not what is being stored.  The answer is everything is being stored.  And we've got crazy computational power.  We've got a jump in AI capabilities recently, enabled by the crazy jump in computational power.  And so this is all - what used to be fiction is now absolutely happening.



ZDNet picked up on some coverage in the Washington Post which did some investigative reporting based on work conducted by Georgetown Law's Center on Privacy and Technology.  Over the past five years researchers at Georgetown issued public records  requests which have revealed that federal investigators, specifically the FBI and ICE, have turned state departments of motor vehicles databases into what they described as a "bedrock of unprecedented surveillance infrastructure."  And to be clear, this doesn't appear to be a direct violation of any privacy laws.  And this is sort of one of the points of this podcast is that technology is often enabling things for which society has not caught up.



LEO:  Yeah.  We gave them our pictures for those drivers licenses, never thinking what the consequences might be once face recognition became widespread.



STEVE:  Exactly.



LEO:  And there's nothing you can do now; right?  I mean, that's it.  It's done.



STEVE:  Yes, our face is out.



LEO:  Grow a beard.



STEVE:  Yeah.  Anyway, in recent congressional testimony...



LEO:  Although I did read, and you've got to do this, that...



STEVE:  Okay.



LEO:  You don't know what Juggalo makeup is.  But you can look it up.  Juggalo makeup effectively obscures face recognition.  So I think, get ready, there's going to be an invasion of Juggalos.



STEVE:  Uh-oh.



LEO:  Well, yeah.  "Uh-oh" is right.  I'll pull up a Juggalo while you continue, and you'll see what I'm talking about.



STEVE:  Okay.  In recent congressional testimony, members were not happy.  And having watched more than my share of congressional testimony, no one wants to have Jim Jordan, who is a very, shall we say, excitable and interactively aggressive Republican congressman...



LEO:  No, he's scary.



STEVE:  ...from Ohio.



LEO:  This is really scary.  This guy is wearing Juggalo makeup, just so you know now.



STEVE:  Well, Leo, if my mother wore that, I wouldn't know who she was.



LEO:  Right.  It works.  And I suspect we're going to live in a sci-fi future where people are walking around with this weird makeup.  It comes from a band called the Insane Clown Posse.  I'll leave it at that.



STEVE:  People don't like clowns, Leo.



LEO:  No.



STEVE:  This is really - this is bad.



LEO:  Juggalos are scary.



STEVE:  So as the Washington Post put it:  "Rep. Jim Jordan (Ohio), the House Oversight Committee's ranking Republican, seemed particularly incensed during a hearing into the technology last month at the use of driver's license photos..."



LEO:  Good.



STEVE:  "...in federal facial" - yes, good - "in federal facial recognition searches without the approval of state legislators or individual license holders.  Jordan said:  'They've just given access to that to the FBI.  No individual signed off on that when they renewed their driver's license, or got their driver's licenses.  They didn't sign any waiver saying, "Oh, it's okay to turn my information, my photo, over to the FBI."  No elected officials voted for that to happen.'"



LEO:  Didn't need to, though; right?  I mean...



STEVE:  That's my point is that we've just jumped ahead of legislation.  The Washington Post said:  "Despite those doubts, federal investigators have turned facial recognition into a routine investigative tool.  Since 2011, the FBI has logged more than 390,000" - okay, 390,000 - "facial recognition searches of federal and local databases, including state DMV databases, the Government Accountability Office said last month, and the records show that federal investigators have forged daily working relationships with DMV officials.  In Utah alone, FBI and ICE agents logged more than 1,000 facial recognition searches between 2015 and 2017, the records show.  Names and other details are hidden, though dozens of the searches are marked as having returned a 'possible match.'"



The Washington Post said, back in May:  "Both Democrats and Republicans blast facial recognition technology in a rare bipartisan moment.  A committee hearing becomes the venue for worries about privacy and civil rights."  They wrote:  "Facial recognition technology endured fierce resistance in Washington on Wednesday" - that was May 22nd - "as both Republican and Democratic lawmakers criticized the artificial intelligence software" - and this, by the way, comes from Amazon - "as a danger to Americans' privacy and civil rights.



"At a time when most issues in Washington generate a starkly partisan divide, members of the House Oversight and Reform Committee were startlingly bipartisan in their condemnation of the technology, which federal and local law enforcement agencies are already using across the country to identify suspects caught on camera.  Members blasted the largely unregulated technology as inaccurate, invasive, and having potentially chilling effects on Americans' privacy and free expression rights.  Several voiced support for passing federal laws to restrain the technology's use before, as Rep. Mark Meadows (R-NC) said, 'it gets out of control.'



"Others voiced worries about the technology being used in the United States as it is in China, where it is critical to the government's systems of public monitoring and social control.  Committee chairman Elijah Cummings said 'there's a lot of agreement' among lawmakers that the technology should be regulated.  The question, he said, is whether the systems should face a moratorium while the technology is assessed or refined, or whether it should banned outright.  The committee's ranking Republican" - that's Jim Jordan, who I mentioned before - "compared the technology to Big Brother in the dystopian George Orwell novel '1984' and said it threatened Americans' First and Fourth Amendment rights covering free speech and protections against unreasonable searches.  'Seems to me it's time for a timeout,' he said. 'Doesn't matter what side of the political spectrum you're on.  This should concern us all.'"



So there was that.  And then, in a related story that I just happened to catch, a CNN special report on the increasing prevalence of automobile license plate scanners, which I guess have sort of very quietly been installed everywhere, based on the story.  In this particular story, a license plate was entered, and the entire past of the vehicle, everywhere its plate had been seen and scanned, back through time, was shown in this huge connectivity graph.  And frankly, it was chilling.  And not because someone did something wrong.  We want to catch the bad guys.  But it was just the idea of that sort of power in anyone's hands is a little bit chilling.



And of course it brought to mind our recent discussion of Google's Android phone tracking database that, remember, went back more than 10 years, the Sensorvault repository, which was accepting queries from law enforcement of the form "Give us identifier tokens for every phone that was in such-and-such a location at such-and-such a time."  Anyway, so I guess my point is that none of this is science fiction anymore.  Just a very short time ago it would have been impossible because we didn't have, you know, storage was too expensive to record everything.  Cameras were expensive.  So they weren't readily deployable everywhere as they are now.  We didn't have the connectivity that we do now that glues all of this together, allows everything to be seen, everything to be scanned by AI, features to be extracted, those all to be stored forever, and then all to be glued together into massive databases.



So what we have is an ongoing debate as we struggle, and we've been focusing on this from time to time where this intersects the technology that we discuss on the podcast, is over the question of an individual's right to have absolute privacy.  And of course we've talked about this notion of the expectation of privacy.  That's one of the legal arguments is saying, like, well, when you're walking around out in public, you don't have an expectation of privacy, which you arguably do when you do things within the confines of your home, for example.  But anyway, I think that, over time, this is going to be one of the interesting things where we watch our expectations and legislation catch up with the fact that technology has outstripped us at this point.  



LEO:  You've been talking, I mean, I remember when you talked about not giving your fingerprint to Disneyland.



STEVE:  Yes, yes.



LEO:  And I think people just weren't aware of it.  And so you've been talking about this for a long time.  But who doesn't have a picture in the database at DMV?  Who doesn't?  I think it's up to the states to say no, you can't have that information.



STEVE:  So a rather liberal-leaning city, San Francisco, has banned the use of facial recognition.



LEO:  We did a Triangulation with the guy who advanced that.  There's another city in the country that's done it, too.  I hope to see more cities doing that.



STEVE:  Yeah.  Well, and I think that maybe pending legislation from Washington, local municipalities will take it upon themselves to just say no.  And then, you know, until we figure out what kind of controls we're going to have on this.  Because right now, I mean, for example, there's no search and warrant process.  You know, we have a Constitution in the United States that says that privacy is not an absolute.  If law enforcement can demonstrate probable cause to obtain a warrant to search something, then absolute privacy is conditional.



But in this instance, the FBI is not obtaining search warrants in order to search the DMV photo database.  They're saying, hey.  In fact, in this reporting, I didn't put it in the show notes, but there was a note that a letter was sent to some DMV office from the FBI saying, "We want to search your database."  And the DMV said, "Yeah, okay.  You're the FBI."



LEO:  You remember the President's been telegraphing that they're going to do a big ICE sweep throughout the nation, and this is one of the most immediate uses of this because in many states, even if you're undocumented, you can get a driver's license.  Oh, it is going to be a boon.  And I don't know.  This is a very scary prospect.



STEVE:  Well, and then you match the driver's license with figuring out the person's name.  That connects them to the plate on the car.  And now you have the plate scanners.



LEO:  You know where they are, yup.



STEVE:  In the CNN story the police cars had roof-mounted cameras pointing in every direction.



LEO:  Oh, man.



STEVE:  And as the cruiser drove down the street, this thing was scanning the plates of every parked car on both sides and doing a deep dive, looking for any problems.



LEO:  City of Petaluma decided to upgrade their meter maids with, instead of having to do chalk marks, license plate recognition.  There's a little side effect of that.  Now you have a database of license plates and locations and times.



STEVE:  Yeah.  Because a database of chalk marks really doesn't go very far.



LEO:  Wow.  It's a scary world we live in.  I'm going to get Juggalo makeup for my Tesla.  Steve, back to you, my friend.



STEVE:  Well, speaking of enhanced connectivity, we were talking recently about Russia and China's ongoing work to proactively, well, to attempt to proactively manage and control their citizens' access to unfiltered global Internet content.  Both governments clearly see it as in their respective nation's best interest to restrict the flow of uncensored communications.  As we've noted, this is at least theoretically possible for land-based wired communications.



We talked about how Russia is going to arrange with Roskomnadzor, whatever their technology group is, to force all Internet traffic through some pinch points that allows them to control it.  And that means they'll have to have their own internal DNS, so they're setting that up.  And our listeners reminded me after that discussion about wired solutions that the problem is much bigger for radio.  And then I was reminded of this when I saw a story yesterday about Amazon's recent application to the United States FCC, our Federal Communications Commission, asking for regulatory approval for their Project Kuiper.



Wikipedia has this to say about Amazon's Project Kuiper:  "In April 2019, Amazon announced that they would fund and deploy a large broadband satellite Internet constellation called 'Project Kuiper.'  It is expected" - is it Kuiper?  I think that's how you pronounce it.  The Kuiper Belt?  Yeah.



LEO:  Yeah, the Kuiper Belt, Vancouver Belt, yeah, yeah.



STEVE:  Yeah.  That "u" in front of the "i" always kind of throws me off a little bit.  It's like, wait a minute.



"It is expected to take up to a decade to fully deploy" - get this - "all 3,236 satellites planned for the full constellation in order to provide Internet to 'tens of millions of people who lack basic access to broadband Internet.'  The satellites will use an orbit with a height between 590 and 630 kilometers.  Kuiper will work in concert with Amazon's previously announced large network of 12 satellite ground station facilities."  That's the AWS Ground Station Unit announced back in November of 2018.



"Amazon filed communications license documents with U.S. authorities in July of 2019" - that's this, so I was impressed that Wikipedia was right up to date - "which included information that the wholly owned Amazon subsidiary that intended to deploy the satellite constellation was Kuiper Systems LLC, based in Seattle, Washington.  'The Kuiper System will consist of 3,236 satellites operating in 98 orbital planes at altitudes of,' in this case I'll do it in miles, 370 miles, 380 miles, and 390 miles."  So I guess three different shells.



"The Kuiper System includes high-performance satellites, terrestrial gateways, internetworking technologies, and a range of customer terminals," says Wikipedia.  ZDNet wrapped up their coverage noting that Elon Musk's SpaceX Starlink broadband satellite system was deployed about a month ago.  It uses 60, six zero, satellites, and all but three of those are functioning as intended.  Additionally, SpaceX has been given permission by the FCC to deploy up to 7,000 satellites in the future.  And both OneWeb and Facebook have outlined their own plans to "monetize space."  Love that term.



LEO:  There's going to be 100,000 satellites up there.  It's crazy.



STEVE:  Right?  It is nuts.  And who's going to keep them from bumping into each other is what I want to know.  You'll have to - how do you launch a rocket without running through the flight path?  So, yeah, literally tens of thousands of satellites.  



LEO:  Yeah, Elon wants to do 12,000 by himself.  



STEVE:  Whoa.



LEO:  So just add Amazon, Facebook, and everybody else.  I love the idea of Internet, high-speed Internet in low Earth orbit, low Earth Internet satellites, to everywhere in the world.  That would be amazing.  In fact, it'd transform my life because I could do this show anywhere.



STEVE:  So it appears...



LEO:  But I also want to see the sun once in a while.



STEVE:  Why is the sky gray?  Well, once upon a time it was blue.  But then we gave ourselves a cloak of satellites.



LEO:  Google the Kessler Syndrome because that's the real fear of all of this, that if they start to collide, you'll have a chain reaction.



STEVE:  Oh, I know.



LEO:  When you have so much debris around the Earth, we won't be able to save it.



STEVE:  So of course this makes you wonder what happens with China and Russia because we've got all these satellites zipping around all over the place.  I guess they could be programmed, and presumably they would be, if they're passing over nations that don't want their citizens to have access.  They would just go dark during their overfly in order to not be present.  But, you know, it'll be interesting to see how this all evolves because, boy, as you said, Leo, the sky is going to be full of Internet.



Ashley Cawley, oh, he posted in the newsgroup, and I wanted to just highlight this for our listeners.  He says:  "A couple of us at work today installed Daniel's great WordPress plugin for SQRL and gave it a whirl.  It worked beautifully for us and made a great demo."  He wrote:  "I'll be demoing it to more of my work colleagues tomorrow.  I also have a number of ideas for further promoting SQRL.  Keep up the great work, all."  And you mentioned this last week, Leo, and I created one of my new GRC shortcuts:  grc.sc/sfw, grc.sc/sfw.  SQRL, well, not only Safe for Work, but also SQRL for WordPress.



So there it is, an official, on the WordPress.org site, an official SQRL plugin that allows SQRL to be used to log into WordPress sites.  And it's been 100%.  Someone was confused that it hadn't gone into the control panel to turn it on, that kind of thing.  But basically it's there, and it's working.



LEO:  Five-star reviews.  And Daniel just updated it 13 minutes ago, so it's in active development, so that's good.



STEVE:  Is it zero - did he updated it to the .7?  I think that's where he was, 0.0.7?



LEO:  1.0.2.



STEVE:  Oh, my goodness.



LEO:  He's been busy.



STEVE:  He took it to 1.0.  So, yeah.  RayG posted, he posted in the - oh, as a consequence of this, I created another forum for - oh, I'm sorry.  I'm getting my postings confused because I jumped to someone named SilverSword posted:  "Installed from WordPress on a clean system.  Giving the error:  WordPress site running without SSL.  Wondering if that's the reason."  And so I wrote back to him, and I thought I would share with our listeners, that we did think years ago long and hard about whether to allow SQRL to be used with non-secured sites.



And what's interesting is that the protocol, SQRL's protocol is so robust, it doesn't need security.  Unlike with usernames and passwords and one-time tokens, et cetera, the core SQRL technology itself can provide secure authentication even without the authentication and encryption that SSL/TLS provides.  And in fact, my early implementations of SQRL worked over either HTTP or HTTPS.



We finally decided that since it really makes no sense to sign in to a non-secured site, since the browser's session cookies are readily sniffed to allow passive impersonation and session hijacking a la Firesheep - we all remember that mess - we didn't want to have SQRL associated with those sorts of nonsecure sessions.  I mean, like, yes, it's nice to have that authentication.  But you can imagine people saying, well, look, I logged in with SQRL, but then my session was hijacked.  It's like, yes, but that's not SQRL's fault.



Anyway, so even though it could have provided secure authentication reliably, we ultimately decided to have it require HTTPS.  And so it would have probably the side effect of helping people who want to use SQRL to move their sites to HTTPS because of course now it doesn't cost anything to do that, thanks to the ACME protocol that's able to issue certificates on the fly.  Oh, and I also saw this morning someone, RayG, posted in SQRL's Web Browser Extensions Forum.  He said:  "I have it running in MS Edge Chromium, and it is all working okay.  Imported my ID and set it up, and I can log into the forums and the GRC demo sites."



So we also have, SQRL has a web browser extension which is running - it was developed under Firefox.  But because it uses the common browser extensions architecture, the API that we finally now have across browsers, it runs under Chromium, and that means it's good for Chrome, Google's Chrome, and also, as Ray has just posted, Microsoft Edge, the Chromium version of Microsoft Edge.  So it's getting around.



Some feedback from our listeners, closing the loop.  And here was a person who, again - and I'm going to step in this here in another few minutes.  David D. tweeted:  "@SGgrc I can't believe you're discussing retroactive decryption of people's conversations for the sake of law enforcement.  Why stop there?  Let's record everyone's private conversations for easy access at any time we figure they may have done something.  Sure, the FBI is giving mass murder as justification.  But everyone knows once they have that tech in their hands it'll be a free-for-all.  At the first indication of any wrongdoing, your digital life would be fair game."



And so I'm going to be clearer about this when I talk about this Ghost Protocol.  As I mentioned at the top, I'm not advocating this.  I'm just saying that arguments that this is about technology are absolutely wrong, and that an argument founded on a weak premise, on a fallacious premise, isn't going to hold.  So what we need to do is decide what it is that we want, and then have the technology implement that.  Again, as I said, I'll be clearer about that than I have been.  But these things get, the different arguments get sort of mixed together.  And lawmakers can sense that there's something not right about that.  And so it weakens the argument overall.



Oh, and we have a - this was sent from "a relieved IT professional" who has made himself deliberately anonymous.  He said "Location Somewhere."  The subject was "The Florida Ransomware Attacks."  He said:  "Hey, Steve.  I've listened to your podcast for a few years now.  I thought, and hoped, my town would never make the podcast.  But now you've probably seen the ransom of $460,000 that was paid here in Lake City, as I heard Leo mention on TWiT.  But it was almost the same time that Riviera Beach paid theirs, so I don't recall it making Security Now!."



And actually he followed up later, after hearing that podcast where we were talking the next week about Lake City.  And he said:  "Thankfully, that was not the agency I work at, but I thought you'd be interested that this seems to be the same attack attempted on many government agencies in Florida, including the one I work at.  We had a mass email come to us from a hijacked account at a certain state-level agency which contained an MS Word document with a malicious macro.  The document was made to mimic an error dialog, and tried to coerce the user into clicking 'Enable editing,' which of course allows the macro to run."



He says:  "I de-obfuscated the macro to find the link it was downloading from, in order to verify that if those that it was mailed to opened it on our network, that it was properly blocked by our firewall."



LEO:  Smart.  Good man.



STEVE:  Yes.  "Thankfully," he says, "the link was blocked as it was hosted in Russia, which we geoblock."  Which again, super smart.  Why allow outbound connections of a local municipality to Russia?  Just seems like that's only going to be a bad thing.  So very nice IT work.  He says:  "Everything I've been told about the City Hall attack lines up perfectly with this attempted attack that went to many local agencies around Florida, so I suspect they didn't have particularly restrictive network firewall rules."



And that's probably why Brian Hawkins got his butt booted is that other, you know, they probably said, City Hall probably said, "Hey, why didn't other people get hit?"  And the answer was, well, we have firewalls that block access from Russia.  And then City Hall says to Brian, "Why didn't you do that for us?"  Anyway, I also should say I got a lot of email from people who were very sympathetic to - and so on behalf of everyone else who wrote to me saying, "Hey, Gibson, we have no budget, we have no time, we have no equipment.  You know, we're screaming that we want this stuff.  Nobody will give it to us.  We're doing the best we can."



And oh, by the way, and this was not these individuals themselves, but people explained that typically third-tier techs are the people who are put in charge of these things in local municipalities, you know, not top-end people.  So again, it's a sense of that, you know, lack of budget, lack of time, doing the best they can, maybe not being like listeners to this podcast, and so not knowing everything that is possible.



Anyway, he said:  "Sadly, the attacks seem to be going around here, and just this week" - and then he refers to Georgia's court system being hit.  He says:  "So the lesson here:  Tighten up your network firewalls; and, more importantly, keep cold backups."  And I think he made a - oh, it may have been in a follow-on message that he talked about keeping backups offline.  So anyway, thank you very much, a Security Now! listener filling us in on some details.  Those are interesting.  And the idea of geoblocking by country absolutely makes sense.



LEO:  At .cn and .ru, for sure.



STEVE:  Uh-huh, yeah.  Also, I'm sure you know about this, Leo, because there's been dialogue in various groups.  Charlie sent with the subject "Help Spamgourmet."  And he said:  "The fellow who runs Spamgourmet is seriously ill.  He's transferred some of the duties to family members and friends.  Now would be a good time to support this valuable resource with donations or technical support.  Currently they have stopped accepting new users.  Please consider making a mention of this on Security Now!.  Charlie from Natick."  And he said:  "Only one Natick in the U.S.A.," so we have to Google it.



LEO:  It's in Massachusetts.  Natick, Mass.



STEVE:  Ah.



LEO:  Natick.



STEVE:  And he said:  "PS:  Even older than you, my first flip-flop was made with a dual triode."  Oh, that's cool, a dual triode, because you could get two triodes in one tube.  And so if you cross-connect them, a triode could be set up as an inverter, and so you cross connect them, and you get a flip-flop.  Anyway, he said:  "My first flip-flop was made in a dual triode.  This convinced me that digital electronics would go nowhere.  Maybe I was wrong, but we'll see."  So Charlie has a sense of humor.



Frank Pielhau in Kenosha, Wisconsin.  Subject is "Mailman and Security Now! Listener."  And I got a kick out of it.  He said:  "I'm a mailman in Wisconsin; and as I deliver the mail, I love listening to your show."



LEO:  Perfect.



STEVE:  Yeah.  "Fifteen years ago I was downsized from my IT job and, with a new baby at home, applied for all jobs I found in the paper.  The post office called first.  As time went on I started to get comfortable with the steady work hours as a mailman, not being on call, and the crazy late nights," meaning his life as an IT person.  "A couple of years ago decided it was time to start studying and preparing to get back into IT now that the kids were older.  Security Now! was one of the podcasts I started listening to.  The more and more I listen to your show, the less and less I want to get back into IT."  Yeah.  "This is nothing against you."



LEO:  No, I think being a mailman's nice.  It's a nice walk.  You get to enjoy the people.  Life is good.



STEVE:  ITProTV.



LEO:  Yeah.



STEVE:  And he says:  "This is nothing against you, and I love your show.  I ask myself every day, do I want to take on the risk and responsibility of managing the security of a network?"



LEO:  Crazy.



STEVE:  "The digital world can be a scary place.  By Episode 999, my studies in underwater basket weaving should be complete, so we can retire together..."



LEO:  Excellent major. 



STEVE:  "...and move on with our lives.  My one question is this:  When you retire from Security Now!, will you still dabble in IT?  Or will you move on to a new hobby and leave the IT industry for the birds?"



LEO:  Good question.



STEVE:  I have no answer to that question.  Depends upon what else is going on around then.



LEO:  What he's talked about doing is writing his own operating system for his PDP-8.



STEVE:  Yeah, I've decided against that.



LEO:  Good.  I want you to learn - I want you to, first of all, get rid of all the Windows computers in your house.  The only reason you have those is because of this show and SpinRite.  Once you retire, no more Windows.  Okay?  Can we agree to that?



STEVE:  I think that would be good.  Yeah.



LEO:  Get yourself a nice BSD box.



STEVE:  That's what I want to do.  FreeBSD.



LEO:  Although, you know, I've been playing with OpenBSD.  I've always had trouble getting BSD installed on hardware.  But older ThinkPads and OpenBSD, it's practically a no-brainer.  And I know FreeBSD is reputed to be more secure, but I think all the BSDs are pretty the same.



STEVE:  Oh, they are.  They are very, very strong.



LEO:  Yeah, yeah.  Open BSD is the one I've got on my - I've got it right here, actually, on a USB key.  I'm going around installing it.



STEVE:  So I got email from a Bill Rakosnik near Athens, Georgia.  The subject was "SpinRite 6.1 Level 4 Speed and Empty Sectors."  He said:  "Steve, on a recent Security Now! episode you said that SpinRite 6.1 will be able to complete a 2TB drive in just over three hours."  That's right.  He says:  "I'm sure that assumes that the drive is healthy and doesn't need data recovery."  Okay, that's true, except for what it's worth, if there were problems, they would be small relative to that.  So, and you don't mind having it slow down and get to work if it's actually doing data recovery somewhere.



What you want is you want a maintenance pass that finds problems and causes the drive to spare them out before they become in need of deeper data recovery.  You want that to be practical.  And 2TB in three hours moves this into practicality for maintenance.  He says:  "What about the speed of SpinRite 6.1 on Level 4, assuming a healthy drive?"  Now, of course that's going to be a lot more time, well, somewhat more time consuming because SpinRite reads the track, inverts the data, writes it, reads it back, reinverts it, writes it again, and reads it again.  So it's going to be, what is that, like five times slower?  And that's, again, so that's going to slow things down.



I would argue that a Level 2 is where a lot of people are going to be spending much more time productively because error correction has advanced to such a degree that, if maintenance, if a Level 2 scan is done periodically, that's going to catch problems.  But, for example, maybe you run a Level 4 scan before you deploy the drive.  So you just set it up and let it go, and now it's going to take 10 hours for 2TB, which, again, sounds like a long time.



But has anyone recently tried to format a drive without using the quick format option?  That is, to actually go out and, you know, what formatting a drive does, if you don't do the quick format, is it goes out and reads the whole drive.  And it takes a lot longer than three hours to do that for a 2TB drive because it's not super optimized the way SpinRite 6.1 will be with its 32MB read buffer that never misses a revolution.  You know, that's how we're able to get it to go so fast.  So anyway, 4, because it's doing so much more work, will definitely take longer.



I will do everything I can to make it as fast as it can be.  And, well, in fact, what I can guarantee is it is not possible to do it faster than 6.1 will do it because it will never miss a rev.  So it'll be constrained by the raw data transfer rate of the data off the drive, which is to say, how fast is that drive spinning?  Drives that spin faster will be able to go faster with SpinRite.  Anyway, he ends up saying...



LEO:  That's a good slogan.  I like that.  Drives that spin faster will be able to go faster with SpinRite.



STEVE:  I'll hold onto that one, yeah.  And of course I don't know what that means for SSD.  They ought to really scream.



LEO:  They don't spin at all, yeah.



STEVE:  I think SpinRite will just really move through an SSD.  Okay.  So the Gem Hack.  I wanted to talk about this because this was a nice anatomy of, I mean, we get to take a look into exactly what happened and how it happened.  The headlines covering this that I first encountered read "Ruby Library Strong Password" - that's the gem - "Contains a Dangerous Backdoor."  And then it went on to summarize:  "An attentive developer located a bogus version of a new Ruby library that he used for his software."



And I'll just note that, of course, if you're going to have a language called Ruby, then its libraries have to be called Gems; right?  So they said:  "The uploaded version carried a dangerous backdoor that enabled the attacker to execute code remotely.  Admins are urged to downgrade to the previous version, as they are running a risk with the latest one."  Yeah, no kidding.



So then, dipping in sort of to the next level of detail, developer Tute, T-U-T-E, Costa has recently discovered a serious problem with the "strong_password" v0.0.7 Ruby library that injects a middleware into the code when deployed on production systems.  The library was hijacked by hackers to enable them to silently and remotely execute arbitrary code on the compromised machine.  The backdoor would send information about the infected URL to its command-and-control server via HTTP, with the instructions arriving as cookie files that were then executed through the eval function.  If the deployment occurred in a production machine, the gem would download its payload from Pastebin.com, the popular text storing and sharing website.



Tute Costa has published the details of his discovery, which makes for a terrific anatomy of a repository breach.  I have the link in the show notes.  And Leo, I've got a couple code snippets which are simple enough that it'll be kind of fun to show them.  So Tute wrote:  "I recently updated minor and patch versions of the gems our Rails app uses.  We want to keep dependencies fresh, bugs fixed, security vulnerabilities addressed, while maintaining a high chance of backward compatibility with our codebase.  In all, it was 25 gems we'd upgrade.



"I went line by line linking to each library's changeset."  And he says:  "This due diligence never reported significant surprises to me until this time.  Most gems have a changelog.md file that describes the changes in each version.  Some do not, and I had to compare by git tags or commits.  The jquery-rails upgrade contains a jQuery.js upgrade, so the related log was in another project."



He says:  "And I could not find the changes for 'strong_password.'  It appeared to have gone from 0.0.6 to 0.0.7, yet the last change in any branch in GitHub was from six months ago, and we were up to date with those.  If there was new code, it existed only in RubyGems.org."  He says:  "I downloaded the gem from RubyGems and compared its contents with the latest copy in GitHub.  At the end of lib/strong_password/strength_checker.rb version 0.0.7 there was the following."  And he shows a little four-line snippet.



He says:  "I checked who published it, and it was an almost empty account, with a different name than the maintainer's, with access only to this gem.  I checked the maintainer's email in GitHub and wrote to him with the prettified version of the diff."  So he took the difference between the 0.0.6 and the 0.0.7.  That identified this four-code change that it made to 0.0.7.  When you run it through a prettifier, basically it makes it more legible because in this case the indentation of the loops and the flow of control was obscured by the fact that it had been condensed down into just four lines.



The prettified version is very readable, and it shows that, at the beginning of that strength verifier, a new function is defined with the name _!.  And that is started and set up to ignore any exceptions that are raised.  So it'll skip over any errors.  The function, when executed, creates a new thread, which loops.  The first thing it does is obtain a random value from zero to one, which it multiplies by 3333.  Well, there are 3,600 seconds in an hour.  So this is, basically, it's obtaining a random - it's that thread after being created sleeps for a random amount of time up to just short of an hour, up to 3,333 seconds.



Then it calls the eval function, which will execute anything that it is given, that is, the argument that is given.  In the eval function is an HTTP GET query to a Pastebin URL.  So in other words, this dynamic - so after waiting a random amount of time, somewhere between zero and just shy of an hour, this will query this particular Pastebin URL, get its contents, stick that into the eval, which the system will then run.  So it's a very clever, simple backdoor.  It's four lines of code, of Ruby code, added to the end of this function.



So 15 minutes after the person who discovered this wrote to the maintainer, Brian McManus, who is the maintainer, wrote back saying:  "The gem seems to have been pulled out from under me.  When I log into RubyGems.org, I don't seem to have ownership now."  He said:  "Bogus 0.0.7 release was created 6/25/2019."  And then our author, who's writing this, says:  "In case the Pastebin got deleted or changed, I emailed the Pastebin that was up on June 28th."  Basically he extracted what that was that was being sent down.  And so he then shows us the code which would be executed on the system.



And he says:  "While waiting for their answers, I tried to understand the code.  If it didn't run before, that is, checking for the existence of a dummy Z1 constant, it injects a middleware that evals cookies named with a double underscore id suffix, only in a production environment" - that is, not a debug environment - "all surrounded by the empty exception handler function that's defined in the hijacked gem.  This opens a door to silently execute remote code in production at the attacker's will."



And so, anyway, they basically reverse engineered this.  I'm confused here by my notes.  Rafael Franca replied 25 minutes after he had been contacted.  Someone at RubyGems yanked that off of the repository, and it was cleaned up.  They got a CVE issued, CVE-2019-13354, which was used to announce the potential issue into production installations since it had been there for a while, and anybody who had updated to it would have this backdoor installed.  So you would want to roll back and find out, also verify that your system had not been compromised.  Maybe if you have any cold backup from before the update, just go back to that because this gave attackers remote command injection capability into a person's system.



And then, finally, on July 8th, the author explained how he believes his account was taken over.  And this is our final takeaway.  He said he had his RubyGems account for long enough that two-factor authentication wasn't even an option.  Back  then he did not use unique passwords for different websites, and since then many services have been breached.  Attackers may have guessed his credentials.



So the takeaway is use password managers, rotate weak passwords, activate two-factor authentication wherever possible.  And I would suggest that that's another good reason to revisit any passwords you have not changed for a long time.  Certainly new passwords are being created uniquely.  You want to make sure that old ones have not still been allowed to languish somewhere where they are sensitive.



Okay.  So the Ghost Protocol.  As I mentioned last week, I ran across some interesting back-and-forth in the Lawfare blog, an "Open Letter to GCHQ" on what they called the "Threats Posed by the Ghost Protocol."  The beginning of it is where most of the meat is, and it's short, so I'll just share this.  They said:  "Last fall, Lawfare published a piece by Ian Levy and Crispin Robinson of GCHQ entitled 'Principles for a More Informed Exceptional Access Debate.'"



They said:  "Our organization, the Open Technology Institute, has worked alongside other people and organizations to coordinate a response from an international coalition of 47 signatories, including 23 civil society organizations that work to protect civil liberties, human rights, and innovation online; several tech companies and trade associations, including providers that offer leading encrypted messaging services; and 17 individual experts in digital security and policy."  So you can imagine the position they have.  You know, "No."



Anyway:  "Our coalition letter outlines our concerns that the GCHQ proposal poses serious threats to cybersecurity and fundamental human rights."  Okay, now, and here's where I say, wait a minute.  Cybersecurity and fundamental human rights are two different things.  So mixing them together is a mistake.  Anyway, "...including privacy and free expression.  We shared our letter with GCHQ officials on May 22nd and are now releasing it to the public as an Open Letter to GCHQ."  I have a link in the show notes, if anyone's interested, to a PDF that's hosted over on Amazon.



They continue:  "In their Lawfare piece, Levy and Robinson set forth their proposal for 'silently adding a law enforcement participant to a group chat or call.'  This proposal to add a 'ghost' user into encrypted chats would require providers to suppress normal notifications to users, so that they would be unaware that a law enforcement participant had been added and could see the plain text of the encrypted conversation.  Levy and Robinson state that they offer their proposal in an effort to have an 'open and honest conversation' about how law enforcement can gain access to encrypted communications.  We appreciate this call for a discussion and have organized our coalition in response.  Lawfare has already published other pieces addressing the GCHQ proposal."  And then they have two links to things.



"Our letter explains how the ghost proposal would work in practice, the ways in which tech companies that offer encrypted messaging services would need to change their systems, and the dangers this would present.  In particular, the letter outlines how the ghost proposal, if implemented, would 'undermine the authentication process that enables users to verify that they are communicating with the right people, introduce potential unintentional vulnerabilities, and increase risks that communications systems could be abused or misused.'  If users cannot trust that they know who is on the other end of their communications, it will not matter that their communications are protected by strong encryption while in transit.'"  Okay.  Again, that's a different issue.  So my argument here is that we get these very different things, technology and policy, intermingled.  And that doesn't help.



"These communications will not be secure," they say, "threatening users' rights to privacy and free expression."  Again, we're mixing things.  "Our letter concludes by urging GCHQ to abandon the ghost proposal and any other approach that would pose similar risks to digital security and human rights."  And I'll just step back and say, okay, what they're saying is there is no way that we will ever be happy with this being done.  Which is their right.  That's fine.  But when they try to claim technology reasons, that's where they fall short, in my opinion.  And they say:  "And by noting that we would welcome further dialogue on these important issues."  Uh-huh.  "The Open Letter to GCHQ is available here."  And then they had another link.



So for me, this response is disappointing because it is not factually accurate, and thus as an argument it is weakened.  Some of our listeners from whom I often hear misunderstand me when I discuss the notion of any weakening of our presumed perfect, but far from it in practice, which is what this podcast is all about for the last 12.5 years, coming up on 13 - or, wait, maybe we're in 12 now, I can never keep it straight.  Elaine always reminds me.  [Year 15 begins August 20, 2019.]



So let me be clear.  I am not advocating for government intervention in encryption technology.  I am not.  It would be fine with me if, as a society, we were to finally decide that some things need to remain totally private.  Period.  End of discussion.  And if that's what we want, then that's fine.  We really can't seem to pull it off in practice, but certainly it's a worthy goal.  And I would agree with people that anything that deliberately weakens it is weakening of it in practice.  So my only complaint is that the designed-to-frighten nonsense being spewed, that there is no possible way for the technology to be adapted so that this could be safely accomplished is simply not true.  And basing the argument on an untruth, as I said, weakens it.



So take the example I've been using for years, which remains true today, which is that, unless users manually manage the encryption keys for the conversations themselves, like for example Threema encourages - which is why, if I had a conversation where I absolutely have to have encryption or privacy, first of all, I wouldn't do it on a smartphone because it's just not safe.  I mean, it just - it isn't.  The idea that we have encrypted end-to-end communications with a smartphone, that's an illusion which, I mean, for all the reasons we've talked about on the podcast.  But it makes people comfortable, so okay, fine.  Okay.



But unless you're managing the key yourself, then you are inherently trusting someone else because keys have to be managed.  So if you're not doing it yourself, you're inherently trusting someone else to manage them for you.  And if that other party is not worthy of your trust, then you have no actual security.  Yes, you have the illusion of security, but not true security.  And that's fine for most people.  Apple manages our iMessage keys transparently for us.  Behind our backs.  Which is exactly what most people want.  They don't want to mess with all that.  And Apple has made it very clear that they go to great lengths to protect us, and we believe them.  We have every reason to believe them.



LEO:  We have no option but to believe them because they don't disclose anything.



STEVE:  Right.



LEO:  Which is why I don't use Threema, either.  It's not open source.



STEVE:  Right.  But iMessage supports multiparty group communications.  And we assume that Apple is not allowing law enforcement to request the silent and hidden insertion of themselves and their encryption keys into any of those communications.  So what if Apple were to add a facility to allow the silent and hidden insertion of an additional party into specific selected iMessage conversations under court-ordered search warrant?  Like obtaining a warrant to bring up a phone tap in the old days.  This actually does nothing to weaken the encryption.  Okay, yes, theoretically, the more people you have in a conversation, from that standpoint is it weaker?  Yes. As has always been said.  If you want to keep a secret, don't tell anyone.  The moment you tell someone, it's no longer a secret.



But from a technology standpoint, it isn't any weaker.  The system is already designed to allow multiple participants to securely converse.  This just silently adds another party to the encrypted conversation.  Okay.  Therefore, any decision about whether to allow law enforcement to eavesdrop on encrypted communications must fall entirely within the realm of is this what we want, which is where it properly belongs, rather than in it's not possible without collapsing the entire system.  That part is not true.  And lawmakers do correctly smell that that's not true.  They don't believe it when people from Silicon Valley go testify.  They just - they've seen what technology can do, and they're right.  Any argument that's based, as I've said, upon a fallacy will eventually collapse.



So we are far better off if we argue based on the principles of what we want, what we agree we want the technology to do.  Then we design the technology to implement that intention, which is what technology always does.  And so I just wanted to - I wanted to clarify because this keeps coming up.  And, I mean, this is a big issue, a big question for our society now.  And it's going to probably, maybe, it's going to vary from government to government.  As our listeners know, those who've been with us for a long time, the reason I bailed on creating CryptoLink, my version of an encrypted VPN, was that this felt like this was happening.  It just felt like governments were going to outlaw encryption that they could not see into through some means.  And, you know, this is very slow rolling.



And I'm glad this is rolling slowly because that allows these sorts of debates to be had over the course of months.  And I think GCHQ is doing the right thing by saying, okay, how about this?  I mean, that's the way you advance the dialogue.  And anyway, so I wanted to just sort of weigh in that we have existing encryption systems that can allow this to be done without weakening them.  The golden key analogy was a bad one.  Backdoors are bad.  None of that...



LEO:  Yeah, but isn't this a backdoor?



STEVE:  No.  It's not a backdoor.  Right now iMessage allows group encryption, which Apple asserts is secure.  In order for that to work, everybody in the group's phone has to be encrypting the conversation for every other participant.  Yet that's all being hidden from us.



LEO:  That's why, if you're intelligent, you don't use it.  Here's my big fear with this is that people who understand this will, of course, including most terrorists and bad guys, will use some other system.



STEVE:  Yes.



LEO:  No terrorist is using iMessages anyway.  So who's compromised by this?



STEVE:  Yes.  Yes.



LEO:  It creates this huge have and have-nots between people who understand crypto and can use it protecting themselves, and those who can't.  Those who can't will be the ones who will be surveilled.



STEVE:  Yes.



LEO:  And the true bad guys will not because there always will exist a way to use true encryption that isn't government breakable.  Period.



STEVE:  Yes.  As we said years ago, that is out of the bag.



LEO:  Yeah.  So if I want - and that's why I don't use Threema or WhatsApp or even, frankly, Signal, although Signal is convenient, but it's tied to your phone number.  PGP, as far as I'm concerned, or Gnu Privacy Guard, as far as I'm concerned, is a sensible way to do it.  I would love for you sometime to look at a company called Keybase.io.  They have a PGP-based encryption system.  You generate your own key.  You can create group chats through them with keys that you generate, not them, so they're not shared with anybody except your group.  But you're right.  You have to manage it.  But Keybase makes it somewhat simpler by putting your public keys on a server.  I would love to hear what you think about Keybase because I know a lot of people, including sophisticated security people, who believe that that is a secure way to group chat, and certainly a secure way to message.  Keybase.io.



STEVE:  Cool.  I will take, yeah, I have had people pointing me to it, and I just, you know, haven't made the time.



LEO:  I feel - this is my biggest fear.  Not that - I actually don't worry about the backdoor and stuff.  I just feel like sophisticated criminals are going to know how to keep themselves private.  Is that right?  You can stipulate that?



STEVE:  Yes.  I completely agree.  Although, I mean, it's weird, too, because you do find evidence on smartphones.



LEO:  Oh, there are stupid crooks.



STEVE:  Yes.



LEO:  And this is what law enforcement's always told me privately is, well, those are the ones we can catch.  We can catch the people who aren't sophisticated.  And maybe the biggest threats aren't necessarily all the sophisticated ones.  You're not going to get Ernst Stavro Blofeld, but you're going to get the average doofus who saw a video about extreme Muslim terrorism and decided to blow himself up and a bunch of other people.  That guy you might catch.  So maybe that's their thought.  And actually I'm all for that.  I just fear that the people who want and deserve real privacy aren't going to have the skills or information or knowledge to get it.



STEVE:  Yeah.  Well, and I think that people who really need it, I mean, and the classic example was Glenn Greenwald and Edward Snowden.



LEO:  Edward Snowden, yeah.



STEVE:  Where Snowden kept trying to get him to get PGP working, and he's like, oh, I didn't get around to it yet, blah blah blah.   Because Edward knew that, you know, if you really need to have email encryption working right, you have to use PGP in order to do it, you know, not to just send text messages back and forth.



LEO:  Yeah.  And I guess it is true that you only have to make one mistake.  Somebody's pointing out that the Internet Research Agency got caught because they forgot to use a VPN once.  And so people make mistakes, and so maybe that's the other way you can...



STEVE:  Well, and we've talked about the idea that, sure, it's great if you've got indestructible, let's say military grade encryption over the wire, but you're typing on a keyboard that you didn't write.  



LEO:  Well, there's all sorts of - yeah, right.



STEVE:  Yeah, I mean, so there's the whole pre- and post-encryption issue, too.  So, I mean, the idea - and this is why part of my annoyance is that, exactly as we were saying, this security that everyone says they want, they don't actually want.  I mean, sure, if you check the box, would you like security or not, yeah, oh, yeah, check that box.  How about pay $10 for it?  What?  No, I don't want to pay anything for security.  It's supposed to be free.



LEO:  So it really is a great debate.  And you're right, this GCHQ paper was not, like, let's make this a law.  It wasn't lawmakers.  It was a proposal to discuss.  And, you know, I don't care because I'm never going to use - I'm never going to plan my plot to take over the world on iMessages.



STEVE:  Well, and I fully recognize I'm not a good case because I'm saying to Lorrie I'll be a few minutes late for dinner.



LEO:  Yeah, right.



STEVE:  I mean, that's what I use iMessage for.



LEO:  I don't mind, exactly, I don't mind using SMS, which is also horribly insecure, for that kind of message.  Who cares?



STEVE:  Yeah.  And, I mean, the truth is, if I actually had something that absolutely had to be proof against interception, I wouldn't use any of this stuff.



LEO:  No.



STEVE:  I mean, it's just, I mean, not really.



LEO:  No.



STEVE:  We're 12 years into how badly all this works, unfortunately.  In fact, the podcast is really misnamed.  Should have been Security Never!.  



LEO:  Security What?  Are you nuts?  Steve Gibson is the man in charge at GRC.com, the Gibson Research Corporation.  Go there.  Hie thee to GRC and get SpinRite, the world's best hard drive maintenance and recovery utility.  That's his bread and butter, but there's lots of free stuff there, including the latest on SQRL.  Oh, so much stuff.  GRC.com.



He also keeps the podcast there, 64Kb audio, 16Kb audio for the bandwidth impaired.  And the most efficient form, plain text, ASCII text, created by the fine Elaine Farris out of each and every nugget that Steve puts out.  She types it in, and you can read along.



STEVE:  Highly compressible.  Highly compressible text.  Because there's so much redundancy in what I have to say.



LEO:  The thing that Elaine does so well, because honestly people don't realize this, but no one speaks as well as the written word.  There's ums, and there's pauses.  You drop a thought.  You begin another thought.  And a transcript can really highlight how weird the spoken word is.



STEVE:  Well, and when you and I say "Threema," she goes and finds it and makes sure that it's exactly spelled correctly.



LEO:  Right.  Good for her, yeah.  She does a great job of making us sound halfway literate.  GRC.com.  We have our own audio and video available at the TWiT website, TWiT.tv/sn.  And so of course if you subscribe there you will get a copy whenever you want it, as soon as it's available, hot off the presses for your Wednesday morning commute, if that's how you listen.  Or if you're a mail carrier, for your Wednesday morning deliveries.  Just go to TWiT.tv/sn.



We also stream it live as we do it, 1:30 p.m. Pacific, 4:30 Eastern, 20:30 UTC every Tuesday afternoon or evening.  Just go to TWiT.tv/live for a host of live audio and video streams.  If you're doing that, please join us in the chatroom.  There's so much fun in there.  It's a great bunch of people.  Vetman says, "Let's call it Insecurity Now!."  How about that?  Insecurity Now!.  Irc.twit.tv.



Steve, thanks so much.  We'll see you next Tuesday.



STEVE:  Thanks, buddy.  



Copyright (c) 2019 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.


GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#723

DATE:		July 16, 2019

TITLE:		Encrypting DNS

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-723.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we cover a few bullet points from last Tuesday's monthly Windows patches, as well as some annoyance that the patches caused for Windows 7 users.  We track some interesting ongoing ransomware news and look at the mixed blessing of fining companies for self-reporting breaches.  We check out a survey of enterprise malware headaches, update some Mozilla/Firefox news, and examine yet another (and kind of obvious) way of exfiltrating information from a PC.  We address a bit of errata, some miscellany, and closing-the-loop feedback with our listeners.  We then conclude with a closer look at all the progress that's been occurring quietly with DNS encryption.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  He has an update on Microsoft's Patch Tuesday update.  He also talks about LaPorte County.  It was struck by Ryuk.  And are IT professionals prepared for ransomware?  Plus we'll talk about, in more detail, DNS over HTTPS.  It's all coming up next on Security Now!. 



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 723, recorded Tuesday, July 16th, 2019:  Encrypting DNS.



It's time for Security Now!.  Ladies and gentlemen, here he is, the star of our show, Steven Gibson of the Gibson Research Corporation, king of the hill when it comes to security NOW.  Hi, Steve.



STEVE GIBSON:  Hey, Leo.  It's great to be with you again.  Episode 723.  And in my errata I have a note from Elaine because she heard me stumbling over what year this was.  And so she just said, "By the way, you will be beginning year 15 on August 20th."  So we're closing in on the end of our 14th year.  And last week's mention, you know, we talked about Mozilla's adoption of encrypting DNS for privacy and the U.K.'s pushback, the ISPA, whatever that was, the association of Internet service providers...



LEO:  They called them the "Villain of the Year."



STEVE:  The villain of the year, yes, one of three nominated as Villain of the Year.  And my conversation about that generated as much interest as we've seen in a long time.  So I decided, okay, let's sort of take a look at where we are because we haven't talked about this since the early days of OpenDNS and DNSCrypt, which, I mean, we've touched on it a little bit here and there, but not given it any time.  So today's topic is "Encrypting DNS," which we will get to.



But first we're going to talk about a few bullet points from last Tuesday's Patch Tuesday, which, interestingly, Adobe chose not to synchronize themselves with.  Normally they're doing their patches on the same Tuesday as Microsoft, but not this time.  Also there was a little bit of upset caused for some Windows 7 users.  I just wanted to mention it in passing because it was interesting that Microsoft has probably deliberately done something they said they would not do.  We'll track some interesting ongoing ransomware news.  And there's even a county with your name, Leo, that has been attacked, LaPorte County.



LEO:  Oh, those Laportes get around, I tell you.



STEVE:  Ah, they do.  We're going to look at the mixed blessing of fining companies for self-reporting breaches, why I'm not that sanguine about the idea of major fines being levied against, well, in this case it's Marriott, and it's big, and it's the GDPR regulation being used against them.  Which, I don't know, it feels wrong, but we'll see.



Oh, there was an interesting survey that Sophos commissioned, an independent survey of 3,100 IT execs about the problems they have, which produces some interesting statistics and graphs that we'll take a look at.  Also some update on additional Mozilla Firefox news.



A paper being released in two days at the, I don't know, 84th - could it be 84th?  Maybe not.  Anyway, I have it in the notes, IEEE conference on something or other about yet another way of exfiltrating data from a PC.  And it's annoyingly obvious.  But to these guys' credit, they really wrestled this thing to the ground.  I mean, so there's, like, no stone unturned in dealing with something obvious.  And I'm thinking that next we want to have them research the optimal Dixie cup size for connecting two paper cups together by string.  But we'll see.  And also what should the tensile strength be and maybe a less elastic medium than cloth string and so forth because I'm sure they could really do that justice for setting up a simple telephone system.



Anyway, we also have bit of errata, some miscellany, closing-the-loop feedback with our listeners.  And then we, as I said, will take a look at where the world stands with encrypting DNS.  So I think another great podcast for our listeners.



LEO:  Lots to talk about.



STEVE:  Our Picture of the Week pretty much sums up what you were just saying, Leo.



LEO:  Yeah, it does.  We've always said security's a binary issue.



STEVE:  That's right.  We have someone giving a presentation to a group with - he's got a front-projecting screen and a little pointer, and we have the caption:  "We've narrowed our security risks down to these two groups."  And we have the first group:  Everyone who works here.  The second group:  Everyone who doesn't work here.



LEO:  That's it.  They've narrowed it down.  That's the threat.  That's the threat.



STEVE:  So that pretty much covers the territory.  So last Tuesday was Patch Tuesday.  And there were your typical bunch of things.  There were two zero-days which were being exploited by Russian hackers at the time, in addition to 15 critical flaws that were fixed.  A total of 77 vulnerabilities which affected Windows, a range of, well, IE, DirectX, and the graphical subsystem.  Of those 77 vulnerabilities, as I mentioned, 16 of those were critical, 60 were acclaimed to be important, and one was given moderate severity.



Most of the critical vulnerabilities allow the attackers to execute remote code, so those were RCEs, remote code execution vulnerabilities on the user system.  And 19 of the only important vulnerabilities could be used for local privilege elevation.  However, as we've seen, although sort of privilege elevation seems like less of a big deal than remote code execution, they are very valuable.  And in fact these two zero-days were privilege elevation exploits that were important enough to be in active use.  That one moderate problem which resolved was an authentication bypass for applications using Windows Communication Foundation and the Identity Foundation API.  So of the two that were zero-days, one was an elevation of privilege, as I mentioned, in the Win32k component, a null pointer dereference.  And the other was in the printer spooler, of all things.



So anyway, what was interesting was the attacker gets elevated, or they were able to get in for the purpose of elevating their privilege with any of six browser memory corruption vulnerabilities or five Chakra engine vulnerabilities.  So again, the privilege of elevation sort of allowed them to gain a foothold after these basically 11 different browser vulnerabilities which Microsoft fixed allowed them to get in in the first place.  So I'm sorry I sound a little bit fragmented.  I got distracted by something else going on in my environment.



Anyway, if an attacker were to cause a victim to visit a malicious website, they could execute remote code in the context of the user's browser, then gain full control over the machine using either of these two zero-days.  So anyway, those patches have been applied.  And as I said, Adobe for whatever reason did not synchronize their updates.  We saw some significant updates to Adobe that we talked about a couple weeks ago, so I guess they just weren't ready for another round.



And I mentioned that there was one thing that happened that upset some stalwart Windows 7 users who, based on the reporting, got pretty worked up and annoyed to receive a non-security update, after specifically asking Microsoft only to deliver security updates, that is, Microsoft gave them a Windows telemetry update on Windows 7 machines, even though it was labeled as a security-only monthly patch.  Recall back in 2016 when Microsoft simplified its patching of Windows versions by offering Windows 7 and 8.1 users two types of updates.  You could either get the monthly rollup, which is what I do because it's, you know, why not, which is both security and non-security patches.  So for IE, like for bugs and for reliability.  But the second option was to say I don't want any feature changes.  I don't want anything other than security patches for my system.  So you could ask for security-only updates and receive a minimal package.



Well, it turns out that last week, on July 9th, Patch Tuesday, there was a security-only update, KB4507456, which actually contained something called the Compatibility Appraiser tool which was slipped in.  And our friend Woody Leonard, writing in his "Woody on Windows" column for Computer World, posted under the title "New Windows 7 'security only' update installs telemetry/snooping feature."  And the subhead of his piece says:  "Three years ago, Microsoft promised to keep Windows 7 and 8.1 updated with two tracks of patches, monthly rollups that include everything, and security-only patches that are supposed to be limited to security fixes."  And he says:  "Guess what happened?"



Anyway, for anyone who's interested, Woody's article has a ton of good information for people who want to know more.  And he cites a security expert who tweets as @VessOnSecurity, Dr. Vess Bontchev, who tweeted:  "I have officially stopped updating my Win7 machine."  This guy tweets:  "I no longer trust Microsoft's updating process.  I'll protect it from any existing and future vulnerabilities with my other defenses as well as I can."  And he signed off with:  "Eff you" - and he didn't say "eff" - "Microsoft."  And Woody politely left that ending out of his copying of this guy's tweet.



My feeling is that all we can be is informed; right?  I mean, that's what we do on this podcast.  That's why we're here.  I choose to use Windows 7, which I do with my eyes wide open.  The job Microsoft is doing, frankly, I think is impossible.  I don't want that job.  No one wants it.  And given the messy legacy of Windows code, the fact that it is using a barely Windows-literate user base, I mean, I know we techies who listen to this podcast, when we're trying to help our other Windows friends, we're like, okay, do you understand what this button does?  No.  Just, you know, they don't want to know.  And as we talk about every week, there is an incredibly and increasingly hostile environment for which Windows attempts to protect its users.



So I give Microsoft a lot of credit for doing, all things considered, I think an amazing job.  And clearly whatever this compatibility appraisal tool thing was is something that they felt they needed to put into, for whatever reason, some telemetry in preparation for the fact that, as we know, Windows 7 will stop receiving any of these things in six months, in February of 2020.



So I feel like I should take a moment to talk about Windows 7 and Windows 10 and me because security updates will stop flowing to Windows 7 six months from now, in February.  That is, unless Microsoft changes their mind again and pushes that deadline further back, which could still happen.  We've seen them do it before.  It was going to be cut off earlier, but Windows 7 was still the majority operating system, despite all their efforts to push people to Windows 10.  So we know that, at the beginning of this year, only just between the snapshot in December of 2018 and January 2019, Windows 10 finally outpaced Windows 7.  There were finally, based on a snapshot of installs in the world, 7 and 10 traded places.



But even today, six months later, seven months later, they're still neck and neck.  Windows 10 sits at 40.61%, with Windows 7 at 38.06% of market share.  So they're still near parity.  Over time we're going to see Windows 7 systems disappear, but probably only because you can't buy any new hardware that runs Windows 7.  Windows 7, you have to jump through real hoops to install Windows 7 on a machine that supports USB3.  And all, I mean, every machine for a long time has.  So it's very difficult to get Windows 7 running on contemporary hardware, and the newer chipsets don't support it at all.  So as hardware dies or gets recycled or replaced just because of its age, even though Windows 7 is just fine, it's going to have to be running Windows 10.  It won't have any choice.



And as our long-term Security Now! listeners know, I won't be moving my main workstations.  I have two at each of my main residential locations.  I won't be moving them to Windows 10, even after Windows 7 stops being supported.  I do have Windows 10 laptops for testing.  When I go out to present SQRL to a group, that laptop is running Windows 10.  You and I, Leo, are Skyping over a Windows 10 machine.  But I know from long, loving, and trouble-free experience with XP that this Windows 7 machine I'm sitting in front of with its five high-resolution screens, and everything is installed in...



LEO:  Five?



STEVE:  Yeah, yeah.  I'm looking around, I've got, like, everything is all - I love this environment.  It will continue to happily purr away for many years, even without constant nursing from Microsoft.  Windows Defender has never found anything on any of my machines other than false-positive annoyances from my own code that I've written that it doesn't know about that it protects me from. or old well-marked viruses in email archives.  Sometimes it fires off, and I think, oh crap, it found something.  And I look and say, oh, no, for some reason it went and sniffed some old directory somewhere where I have some virus repositories that I'm keeping, and they're well marked and known.  And besides, I'm not even sure they infect anything like Windows 7 or 10 any longer.  I mean, they're really old.



But it will stop being updated in six months, and I'll miss it.  It's nice to have Defender sort of watching my back, even though it's never found anything.  But I think I'll be okay.  And we just talked about all of the ways bad stuff was getting in, which Microsoft just patched last week, were browser vulnerabilities in IE and Edge.  And I will continue not using them in the future.  So I'll stick with Firefox, and I'll probably be okay.  And as I've said before, my backups have backups.  And I also keep a rolling off-machine incremental file change backup of all the projects I'm working on, as well as monthly static offline deep-freeze snapshot images.  So I'm well protected.  On the other hand, I'm not your average user.  And for what it's worth, neither are the listeners of this podcast.



Also, I love so many of the apps that I have running on Windows.  I'm just like, I'm extraordinarily happy with them.  There are apps that I've been moving forward through the years from machine to machine.  The move away from XP and the loss of native 16-bit support was traumatic for me, but it finally had to be done because even Firefox and Chrome finally were refusing to update themselves on XP.  So I thought, okay, fine.  I'll do that.  But I'll continue using Windows 7, and Firefox and Chrome will continue to keep me being safe.



But the one thing I want to say - because I get the sense that maybe people believe I'm suggesting this is okay for everyone, and so I'm not saying that.  I know that there are listeners within our audience that feel the way I do.  I mean, look, half the world is, I mean, literally half of the Windows systems even today are running Windows 7 as opposed to 10, despite all the pressure that there is and there has been to move to 10.  And of course that's going to ratchet up through the rest of this year until, I imagine, there will be people who don't feel as I do that it is, for whatever reason, safe to continue using Windows 7 without this constant drip-drip-drip of fixes to things that Microsoft finds that are wrong.



So I am not suggesting that anyone else follow my example.  And one of the reasons is, and I've said this before, is my use of Windows is boring compared to most others.  I don't use my machine for entertainment or gaming.  I don't watch YouTube videos or just follow random link trails to see what's out there on the Internet.  I'm really not very interested in most of what is out there.  So my main Win7 workstations are, while they're not technically air-gapped from the Internet, they are Steve-gapped because I just don't do much with them.  You know, I assemble my own code and design PC circuit boards and, I mean, I use it as a workstation rather than as a toy.  And so my exposure to danger is, I think, reduced from the typical Windows machine.



But anyway, I just wanted to say that here we are, six months away from end of updates for Windows 7.  I think it is remarkable that Windows 7 versus 10 is at 38% versus not quite 41%.  I mean, people just don't want Windows 10.  So it'll be fun to track this as we move forward, and we certainly will.



And Leo?  LaPorte County, Indiana.  No relation.



LEO:  You know there's a city of LaPorte in every state of the union.



STEVE:  Is that true?



LEO:  Yeah.  We were fur traders, and we got around.  That's all I'm going to say about that.



STEVE:  Anyway, LaPorte County.  The Michigan City News Dispatch reported last Tuesday, the 9th, their headline was "Malware attack on county computers.  LaPorte County website, government email servers out of operation."



LEO:  They're besmirching the family name.



STEVE:  Oh, my goodness.  Well, and it'll be interesting to see how this goes.  Paraphrasing and trimmed down from this article, the report was all LaPorte County government email and the county website remained out of commission late Tuesday - that is, last Tuesday - following a malware virus attack that affected the system on Saturday morning.  The LaPorte County Board of Commissioners President, someone by the name of...



LEO:  You're just doing this to irk me, aren't you.



STEVE:  No, no, no, no.



LEO:  Just wait till Gibson Township goes down.  I'm just telling you.



STEVE:  That's right.  Dr. Vidya Kora said Saturday evening:  "The system will be inoperable as authorities respond to 'a malicious malware attack that has disabled our computer and email systems.'"  Then, a few days later, last Tuesday, County Attorney Shaw Friedman confirmed that county government computers were "impacted by a sophisticated ransomware virus early Saturday morning."



LEO:  Must be sophisticated or we wouldn't have fallen prey.



STEVE:  That's right.  It was a baddy.  He said:  "Fortunately, our IT team reacted quickly."



LEO:  Unplug them.  Unplug them, quick.



STEVE:  Although after the fact, of course, and shut down much of the system.



LEO:  They did, they unplugged it.



STEVE:  I know.  They did.  They unplugged them.  Even though it was a weekend.  So yes, our IT team is on the job.



LEO:  On the job.



STEVE:  Even on the weekend.  He said:  "Less than 7% of our laptops have been infected; however, it did hit our two domain controllers, which means no server can access network services."  Whoops.



LEO:  OMG.



STEVE:  And actually, Leo, it also got their backups. 



LEO:  Uh-huh.



STEVE:  Yeah, uh-huh.  An insurance policy taken out last year, Kora said, will help the county recover.  He said:  "Fortunately, our county liability agent of record, John Jones, last year recommended a cybersecurity insurance policy" - I bet there's a lot of those recommendations going around right now.



LEO:  Yeah.  You ought to get an insurance policy.



STEVE:  "Which the county commissioners authorized from Travelers Insurance."  He said:  "We informed Travelers Insurance late Saturday" - while we were still busily unplugging  machines, no, he didn't say that - "of the malware attack, and they immediately referred us to the Wayne, Pennsylvania incident-response law firm of Mullen Coughlin LLC that specializes in responses to such cyberattacks and coordinates system repairs and protection of our computers from such virus infections."



Friedman said:  "The forensic investigation firm has been retained to determine the nature and scope of the incident, including how the county could have been infected."  Actually, they never did find out.  But, he says:  "We're developing a game plan to respond to the attack..."



LEO:  Oh, that's reassuring.



STEVE:  I know, got to have a game plan, "and come up with an approach to repair our systems and protect them from further damage."  Right.



LEO:  A little late for that.



STEVE:  After we've plugged them back in.  After we plug them back in.  "The county's IT department has been working long hours" - oh, we're pumping that up - "long hours to try and get things operational."



LEO:  So please don't fire us.  Please.



STEVE:  Oh, get this.  Exactly.  Please, please.



LEO:  Please don't fire us.



STEVE:  We don't want to follow, what was his name, Brian somebody, Brian...



LEO:  Yeah, he's gone, but he's looking for work.



STEVE:  Yeah, he's looking.  Yeah, don't hire him.



LEO:  I hope he goes to Gibson County.  I just pray, I pray that he goes to Gibson County.



STEVE:  It says they've been working long hours to try and get things operational, including, Leo, spending Sunday...



LEO:  Oh, no.



STEVE:  Even on Sunday.  They never get to rest, those IT people - to ensure that the courts and prosecutor's office remain functional.  Because we've got to prosecute somebody.



LEO:  Well, yeah.



STEVE:  After we figure out who did this to us.  So "This particular ransomware variant, known as Ryuk" - no kidding.



LEO:  Oh, yeah?  Oh, where have we heard that name before?



STEVE:  Yeah, R-Y-U-K, "is especially insidious as it seeks to delete or encrypt system backups."  Whoops.



LEO:  How dare they?



STEVE:  But Leo, he said:  "We are exhausting all possibilities."  We're going to be exhausted, as are our IT people.



LEO:  Maybe there's a hard drive in the closet somebody forgot to connect.



STEVE:  He said:  "We're even tapping the FBI cybersecurity unit and reviewing all workarounds" - we're going to review those workarounds - "in order to determine how to restore the county to a full operational status."  So, you know, we're glad we voted for this guy because, you know, even on Sunday.  So staff from this firm, the law firm Mullen Coughlin, arrived in LaPorte - at LaPorte?  In LaPorte?  I don't know - on Sunday night.  Night, even, Leo.



LEO:  Oh, traveled on a Sunday to get there.



STEVE:  No sleeping, to assist.



LEO:  No, took a Greyhound.



STEVE:  They will help prepare documentation to report the attack to the FBI and other appropriate law enforcement agencies.  Kora and Friedman both praised the efforts of the IT department.  Kora said:  "I commend our Director Darlene Hale" - while she still has her job - "and her team..."



LEO:  Don't fire me, please.



STEVE:  "...for shutting down our systems Saturday afternoon" - she came right in - "as soon as the malware virus was detected.  Unfortunately..."



LEO:  It was way too late.



STEVE:  "...at least half our servers have been infected."  Because you know that malware is quick.



LEO:  Speed of light, my friend, speed of light.



STEVE:  That's so unfortunate, "and it will take some time to fully restore service.  I ask for patience from the public as we seek to become fully operational again."  They like that phrase.  Friedman echoed that sentiment, saying:  "Darlene Hale and her team have been working 15-hour days," Leo, 15 hours, "since this virus hit to try to restore portions" - okay, we're getting a little more modest now - "portions of our system that can be restored."  Because of course you cannot restore those portions that can't be restored because they can't be restored.  "We ask for patience from all concerned."



Okay.  So that was the incident reporting.  Then, a week later, BleepingComputer reports:  "A forensic investigation firm and the FBI were involved, but attempts to recover the data encrypted by the malware without paying the ransom were fruitless.  The cybercriminals got about $130,000 in Bitcoin..."



LEO:  Oh, they paid them.  Oh, boy.



STEVE:  "...from this attack, with $100,000 being covered by insurance.  So the impact may not be immediate," they write, "but it does create some ripples in the long run.  The decision to pay the cybercriminals came after seeing that the decryption keys from the FBI" - I guess they must have had some from previous...



LEO:  Ryuk cyber is one of those malwares that sometimes can be reversed.



STEVE:  I don't think so.  I don't think Ryuk can.



LEO:  No?



STEVE:  No.



LEO:  Somebody sent us an email saying, yeah, we do this, and sometimes you can reverse it.



STEVE:  Ah.  Well, actually I do think there were some versions.  I may be confusing it with a different one.  But anyway, according to a local report from WSBT, a local station, the county had backup servers, but the malware encrypted them.  So you don't want your backup servers to be on your network all the time.  So we now know that insurance companies are bearing the brunt of the payouts for these attacks.



So I'll bet that we're not far from the time when the conditions of continued insurance require regular training and reviews, periodic security audits, and more reliable backup solutions.  I'll bet that we're, I mean, in other words, we're going to be hearing from insurance companies, quote, something like, "We'll insure your municipality; but unless you want the insurance premiums to be really sky high, you need to get much more proactive about protecting yourself from these threats.  And when you come calling for a payout, the first thing we will do is audit to figure out why none of the multiple safeguards you promised to put in place and to maintain were effective in this instance.  And only if we find that you were not at fault, given the terms of this insurance, are we going to pay."  So I think we're going to see something happen.



And then I got a kick out of this.  Also in the news, U.S. mayors adopted a resolution not to pay any more ransoms to hackers.  Whoo.  They have adopted a resolution, Leo.  It turns out that just happened.  The 2019 Adopted Resolutions of the 87th Annual Meeting - oh, that's the 87 I was probably thinking of, not the IEEE because that'd be a long time to have IEEE meetings.  But the 87th Annual Meeting of the United States Conference of Mayors, of the Committee for Criminal and Social Justice, included the resolution to "oppose payment to ransomware attack perpetrators."



And actually the proposal adopted resolutions stuff is pretty humorous, so I've put a link in the show notes.  And I had to scroll down through, like, endless adopted things.  Finally got down to opposing payment to ransomware attack perpetrators.



And so there are seven points.  They said:  "One, whereas targeted ransomware attacks on local U.S. government entities are on the rise; and, two, whereas at least 170 county, city, or state government systems have experienced a ransomware attack since 2013; and, three, whereas 22 of those attacks have occurred in 2019 alone, including the cities of Baltimore and Albany and the counties of Fisher, Texas and Genesee, Michigan; and, four, whereas ransomware attacks can cost localities millions of dollars and lead to months of work to repair disrupted technology systems and files; and, five, whereas paying ransomware attackers encourages continued attacks on other government systems, as perpetrators financially benefit; and, six, whereas the United States Conference of Mayors has a vested interest in de-incentivizing these attacks to prevent further harm" - yeah - "seven, now therefore be it resolved that the United States Conference of Mayors stands united" - yes, united - "against paying ransoms in the event of an IT security breach."  In other words, we're saying don't do it anymore.



LEO:  We stand agin it.



STEVE:  We stand united against paying what we're going to be paying.  We're not happy.  So, uh-huh.  Anyway.



LEO:  Ransomware, I'm agin it.



STEVE:  That's right.



LEO:  I don't like it.



STEVE:  So basically they're all definitely, Leo...



LEO:  We are all in against it.



STEVE:  ...unhappy.  We're really, really not happy.



LEO:  Somebody's got to get these guys some help.  So we got an email from a guy, and I can't vet it, so maybe, I don't know, you can or something, named Brett Callow.  He works for a company, a New Zealand company called Emsisoft.  His point was that Ryuk uses hard-coded keys that sometimes are reused.



STEVE:  Ah.



LEO:  And so those are the keys probably referred to by the FBI.  It was unclear.  He wanted to get the word out that, you know, they offer a downloader that will check it against the keys that are known.  This is the website.  I don't know anything about it.  They say it's free of charge.  But the point being that he said, "I just want to get the word out that sometimes you can get a key to decrypt it that's been reused."



STEVE:  And it may work.



LEO:  And it may work.  And certainly should do that before you pay anybody any money.



STEVE:  Yeah, especially, you know, lots of bitcoin.



LEO:  Oh, man, that's a lot of money.  Wow.  You know, how long is insurance going to be offered?  I mean, it's going to either...



STEVE:  That's exactly right.  I mean, the premiums are going to start going up.  And the fact that the insurance company paid the round number $100,000 makes it sound like that was the cap on their payout for this particular county.  So, in fact, the county may have decided, well, boy, you know, to get full coverage it's going to cost - the premiums are going to be too high.  So we'll accept a cap of $100,000 because, you know, whatever.  Anyway, believe it or not, Leo, this problem, not surprisingly, actually, has created, well, we already saw they've created a law firm that specializes now.



LEO:  Isn't that interesting?  Yeah.



STEVE:  Yes.  And now we have Coveware.com, ransomware remediation.  It's C-O-V-E-W-A-R-E dot com.  "We are the first responders to your ransomware recovery.  Coveware aggregates global ransomware data to minimize your ransomware-related costs and downtime.  Let our IT security professionals manage your ransomware incident response."



They say:  "How do we restore your encrypted data?"  Well, "One, explore free remediation options.  Identify ransomware type.  Find free decryptor tools," like what you were just talking about.  "Free initial assessment risk.  Identify the threat actor group."  Then second main point:  "Threat actor negotiations.  Secure and safe negotiations.  Complete and transparent communications.  Determine risks and outcomes."



So basically we now have an industry which is establishing itself as professional ransomware remediation and negotiation, I mean, they have experience with this.  So I'm sure they have the threat actors' number and know how to contact them and say, okay, look, let's see what we can do here.



Then number three:  "Ransomware settlement.  100% transparency, reimbursed costs, transparent documentation, compliance checks."  I presume that means that they get paid out of what they, like, out of insurance or settlement.  And then, four:  "Restore data and end downtime.  Professional IT support.  Insurance documentation."  So they're able to have their costs paid by the municipality's insurance and roll experts in who are able to apply the decryption tools and bring the systems back up.  So if your local IT staff are not up to it, now there's Coveware that you can contact.  Then they sign off on their web page saying:  "Minimize your ransomware downtime.  Let us manage your ransomware recovery."  Unbelievable.



LEO:  Wow.  It's really a business.



STEVE:  It really is a business.



LEO:  I want to ask this one more time.  I feel like I've asked this many times.  Is it not the case that you could probably prevent this with good IT?  I know you might get infected; right?  I mean, sometimes they'll sneak through.  But if you have good cold backups, I mean, it seems to me this would be avoidable.  But maybe not.



STEVE:  I've had a lot of feedback from our listeners while we've been talking about this.  I mean, from our listeners who are on the IT front line and who say, you know, you guys need to stop saying that this is as easy as backing up all the systems.  There are real logistical problems to doing that.  For example, there are servers that, I mean, so I don't have those jobs.  I can't definitively say.  But what I'm hearing from our listeners is that there are servers that can't be taken down.  There are workstations that for whatever reason can't be logged off of.  The backups cannot be done on the fly.  There are, like, open files that prevent themselves from being backed up.



And we know that that can happen where you just - you can't take a snapshot of a system that's in use.  You have to stop it in order to snapshot it in some instances.  And there are systems that can't be taken down.  For what it's worth, Leo, I'm absolutely sure that it is not an impossible problem to solve.  But it probably takes a lot more than is practical given the resources that these people have.  And in fact this takes us perfectly into the next topic, which is this survey that Sophos commissioned from a U.K. research-based firm.  After our second break we will talk about it.



LEO:  Yeah, I don't mean to diminish the efforts and the difficult of this.  It seems like it would be worth doing it, somehow preventing it; right?



STEVE:  I really do think that it's a tradeoff.  You know, how much time and effort and money and staff do you commit to mitigating... 



LEO:  But you're looking down the barrel of a gun that's going to hit you.



STEVE:  I know, I know.  But I'm sure the IT people are saying at every meeting, the CIO, we need more money.  We need more money.  And the boss says, okay, yeah, but, you know, you've got to do what you can with what you've got because we don't have any more to give you.  And I'm sure they're saying, look,  everything was good yesterday.  Everything is good today.  We're going to hope that everything's good tomorrow.  And of course...



LEO:  It ain't gonna be, though.



STEVE:  I know.



LEO:  It ain't gonna be.



STEVE:  That's true.



LEO:  You're going to be the next one.  So we've not been hit by ransomware, knock on wood.



STEVE:  Knock on wood, I know.



LEO:  You've not been hit by ransomware, knock on wood.  We're in a worse - you have one person opening your email.  We have 20 employees opening emails.  We probably have been targeted.  I would imagine we have.



STEVE:  Well, Leo, I live in fear.  I would love to have servers statically mapped, and I'm disconnecting from them all the time because, I mean, this is the problem that we face today.  And so, I mean, it is really, I mean, it is the problem is that something gets in and encrypts the data.



LEO:  And also ransomware's more sophisticated than it used to be.  We've got two people in the chatroom who are saying - Web108 says, "We had two ransomware attacks in 2017.  We contained them.  We restored.  No loss of time or data."  Beta4a says, "My company's been infected by cryptoware twice.  We have wiped, rebuilt, and restored with a loss of a maximum of one day's work."  So, but it may be the case also, ransomware, thanks to Blue, what is it, Blue Heaven?  Blue, you know, and various tools that are now out there that make it easy...



STEVE:  Oh, BlueKeep, BlueKeep, yeah.



LEO:  BlueKeep, to worm its way through your network.  Maybe it's more virulent than it used to be.  It feels like there's things you should do.  Maybe you can't prevent it 100%.  But it feels like it's well understood what you need to do.



STEVE:  Well, for example, as I've said, I mean, my computer could explode, and I'd be up.  I have an entirely separate physical redundant machine just sitting here waiting to be commissioned. 



LEO:  But, yeah, you don't, you're not - you're not running an active server that's doing 100 transactions a second or anything like that, either.



STEVE:  No.  And I said a long time ago I don't want the job of keeping Sony safe.  Nobody wants that job.



LEO:  I told you we had the guy who protects the cybersecurity for West Point, the military academy at West Point.  And he said it's tough because "we only have to make one mistake."  Right?  They're attacking all the time.  It only takes one mistake.  Now, he's lucky because the army's Cyber Defense Command is also there, so they help out a little bit.  But still, you're right.  I wouldn't want that job.  We're not saying you guys are dopes.



STEVE:  No.  No, no.  I mean, and I know IT people who their lives are, you know, it's like that mailman we talked about last week.  He's happy.  He's delivering the mail.



LEO:  It's easy.  Life is easy.  All you have to worry about is occasional dogs, yeah.  Well, and I won't talk about what we do.  But we have a fairly, I mean, we have a number of barriers to the outside world.  We use Gmail, which says that, you know, Google says, "We filter against known malware attacks."  I don't know.  I feel like - I don't know.  Watch, because tomorrow I'll be saying, Steve, we can't do the show.  All our servers are encrypted.  Do you know any good malware authors?



STEVE:  Get one of those Dixie cups with string so we can talk to each other.



LEO:  Yeah.



STEVE:  So Sophos commissioned an independent survey of 3,100 IT managers.  They used the U.K.-based research house Vanson Bourne.  And this survey was conducted at the end of last year to the beginning of this year, so December 2018 to January 2019.  To provide a representative size split, they chose the same number of organizations between 100 and 1,000 people, and 1,000 and 5,000 people, so an even mix of smaller and larger organizations.  What they found was - none of it was really very surprising, but we have some nice numbers.



Respondents who had been victims of a cyberattack in the last year were asked how the most significant cyberattack got into their environment.  The results revealed that where respondents knew how the attack got in - and they didn't always know - not surprisingly email was the number one most common attack vector, which was used in one third, 33% of the attacks.  And of course we know that that's conducted with phishing, where email is sent that is designed for someone to think that it's authentic.  Typically in targeted attacks somebody clicks the link.  And in some cases, like somebody else's email account could get compromised, so the email is actually coming from someone you trust, but it's malicious.  And the rest is what we talk about all the time.



The web is also a major vector, which was used in three out of 10 attacks, so 30%, just slightly less than email.  So again, as we've often said, the browser is today's attack surface.  It's why I made the comment when I talked about continuing to use Windows 7 in the future, I'll be using Firefox or Chrome - well, Firefox probably, which is being kept constantly updated - even after Windows 7 stops being updated because, well, and for that matter Thunderbird for email, both that are being constantly maintained, even if the underlying OS isn't.



IT managers, however, cannot just focus on email and the web.  23% of attacks got in via a software vulnerability of some kind, and 14% through a USB stick or external attached device.  So those things, we don't really talk about those very much, but those are still happening.  Back at the beginning of the podcast, Windows was infamous for running a program when you stuck a USB device onto the machine.  So it was very easy back then to do drive-by attacks.  Anyway, so 33% email, 30% through the web, 23% through some software vulnerability, and 14% through USB or some other device.



And in one out of five instances, no one knew.  They did not know how something got in.  They were unable to identify the way something happened.  And, you know, in a sufficiently large organization, I can understand where something happens, and you just say, well, you know, we looked everywhere, and we were never able to determine how something happened.  I mean, even "I don't know" is, you know, you'd like to know, but it's hard to know in every case.



Also what was interesting is that these cyberattacks that we're seeing, as you said, Leo, they are becoming increasingly sophisticated, which says they may not just use one thing.  They may be multistage and coordinated and blended.  Respondents whose organizations had been victim of a cyberattack revealed that they had suffered a range of attacks.  So, for example, the second graphic that I have shows 53% phishing, 41% data breach, 35% malicious code, 35% software exploit, 30% ransomware, and 21% credential theft.



Well, 53, 41, 35, 35, 30, 21, that adds up to way more than 100%, meaning that what they were seeing was that many of these attacks used multiple means of obtaining their goals, not just one type of vulnerability.  It could be phishing email that then leveraged a software exploit.  And of course we see that, for example, where phishing email leverages scripting in Word, where there's a vulnerability in Word, where if you coax the user to taking it out of protected mode, it will run the Word macro and then leverage one or two other vulnerabilities that exist somewhere.  So it's a complex sort of multiprong attack because no one thing anymore is sufficient because our systems overall, the various ways that things can happen are increasing in their security; but, by combining multiple vulnerabilities, people are still able to get in.



Of the 2,109 - okay.  So 3,100 organizations were surveyed.  2,109 of those were hit by a cyberattack in 2018.  Over half, 53%, were victims of phishing.  So that is still the most lucrative, the most high return attack across all of the survey.  And there was some variation based on country on the nature of software exploits.  Over a third, 35%, suffered from an exploit taking advantage of a vulnerability in software they were using.  Interestingly, in Mexico, over half the organizations that fell victim to a cyberattack experienced a software exploit which was double the number of those in Brazil, at 22, and South Africa and Japan, both at 23.  So there is, for whatever reason, there was like a statistically significant difference by country.



And the survey asked the question, as I mentioned, about technology, talent, and time, and concluded that they were in short supply.  In this report they said:  "As we've seen, organizations face a wide range of attacks and need to secure multiple threat vectors."  They revealed that, on average, IT teams spend 26%, so just 1% over a quarter, 26% of their time managing cybersecurity.  So think about that:  26% of the IT team time is cybersecurity related.  And they concluded that, for the majority of respondents, this is not the correct ratio, meaning that it should be higher.



And then again, there was some variation by country.  Organizations in India spent the most time, at 32%, and Japanese teams the least at 19%.  Organizations that have been hit by a cyberattack, I guess not surprisingly, spent a little more time now on IT security, 28%, over those who had never experienced an attack, yet were still spending substantial time, 23%.  So maybe that accounts for the fact that they had not yet been hit.



And the report said:  "Given the variety and complexity of threats, it's not surprising that 86% of respondents said they need greater cybersecurity skills within their organization.  Those organizations that had experienced an attack have even greater need for cybersecurity experience than those that hadn't, 89% versus 79%."  But still, even those who had not been hit, 79% of those organizations said we need to be doing more than we are able to.



Anyway, so they said that bringing the expertise to fill these gaps is a major challenge.  Eight in 10 organizations say they struggle to recruit the right skills.  So they're struggling to find people who have the skill set.  They said when it comes to recruitment, India faces the greatest challenge at 89% of the organizations saying they cannot find people who have the skills they need, and Germany the least.  But still, two out of three German IT managers, so 66%, say they struggle to bring in the right skills.



So anyway, I just thought that was interesting, to get some sense for the fact that, I mean, given all the stuff that we cover and the way we cover it, this fits everything that we  believe in terms of the major threats that we're seeing, the way these threats get in, and how difficult it is in practice to counteract them.  And the fact that IT organizations, it may just be that there's a bit of a brain drain, too.  I know that a lot of our listeners sometimes ask, you know, are there jobs in security?  I think it's very clear that somebody who focuses on security can increasingly find work there in the future.



And we're seeing that fines are beginning to happen, where mistakes are starting to cost organizations more than just reputation damage.  And I'm of two minds about fines.  We really do want major organizations to act responsibly with the personal and abusable data that they collect about us through their normal course of justifiable business operations.  But we also want and need them to self-report when, despite their best efforts, they fail to live up to their and our hopes for their ability to keep our data safe.  And given that responsible self-reporting is inherently voluntary, unless a breach is discovered externally, which is much less common than internal discovery, levying burdensome and abusive fines on those organizations may not actually improve end-user security and privacy.



Which, you know, the reason I'm talking about this is that, as I mentioned at the top of the show, the U.K.'s Information Commissioner's Office, the ICO, has announced that it intends to impose a hefty fine.  It's 99,200,396 euros - or, no, I'm sorry, pounds, which is in this case $123,705,870.  Nearly $124 million fine on Marriott, the hotel chain, over last year's data breach.



As we know and reported at the time, last November 2018, Marriott self-reported that hackers had had access to the Starwood guess reservation database over a period of four years, since 2014.  Starwood was a different chain of hotels which Marriott had acquired in 2016.  So the breach occurred two years before Marriott acquired it.  Marriott initially reported that hackers had stolen the details of, and it was a rough estimate, half a billion, so a big breach, 500 million hotel guests, which they subsequently reduced to 383 million after a more thorough investigation.



And remember that there are also passports involved.  There were 383 million guest records, 18.5 million encrypted passport numbers, 5.25 million unencrypted passport numbers, 9.1 million encrypted payment card numbers, and 385,000 card numbers that were still valid at the time of the breach and had not been encrypted.



So unfortunately in this day and age, class-action lawsuits began piling up within hours of Marriott's announced security breach.  And I suppose not surprisingly now, with the GDPR, the U.K.'s Information Commissioner's Office, which is in charge of such things, has stated that Marriott's security practices are in violation of the EU's GDPR.  And it'll be interesting to follow this to see whether that's actually the case.  I have no opinion one way or the other.  We don't, without much more information.  The good news is that Marriott has stated that they are going to oppose this fine.  They filed a note with the U.S. Securities Exchange Commission that they're going to formally oppose it.



Marriott International's President and CEO, Arne Sorenson, said:  "We are disappointed with this notice of intent from the ICO, which we will contest.  We deeply regret this incident happened.  We take the privacy and security of guest information very seriously and continue to work hard to meet the standard of excellence that our guests expect from Marriott."  And he did say that Marriott had retired, and we mentioned it at the time, the Starwood guest reservation system earlier this year.  So it's no longer in use.



So I don't know how - I guess I don't know how to feel about the EU stomping on Marriott for a violation of GDPR which occurred over a period of time, involved an organization that they didn't own at the time, that they're now slapping them with a big fine over.  And again, we want organizations to responsibly disclose breaches, rather than to fix them quietly and not acknowledge that there was a leak that could affect their customers.  Yet having the GDPR used in this way really seems to put cold water on that.  So it'll be interesting to see.



Oh, the day before that also, by the way, the ICO in the U.K. also announced plans to hit British Airways with a $230 million fine after they failed, British Airways failed to protect their website, which was infected with a web-based card skimmer which was collecting payment card details from British Airways customers for, let's see, April, May, and June, for three months back in 2018.



LEO:  Oh, I didn't know there was such a thing as a web-based card skimmer.  That's awesome.



STEVE:  Yeah.  It was infected JavaScript which got in there and was capturing all of their credit card information while they were putting it in.  So I don't know.  It seems...



LEO:  You feel like they're being scapegoated because they're big names?



STEVE:  Yeah.  And they've got deep pockets.



LEO:  I liked OutofSync's idea.  Instead of fining them and collecting it and lining your coffers, make them spend that money on security.  Say, like, good, now you're going to spend $99 million to make your system more secure, and we want to see the receipts.



STEVE:  Yes.  I think that makes a lot of sense.



LEO:  I think that would be better.



STEVE:  Yeah.



LEO:  I guess if they're not sitting up and paying attention to the hacks, maybe the fine would get companies to pay attention.  But it doesn't feel like that.



STEVE:  No.  And this Information Commissioner Elizabeth Denham in the U.K., she said:  "The GDPR makes it clear that organizations must be accountable for the personal data they hold.  This can include carrying out proper due diligence when making a corporate acquisition and putting in place proper accountability measures to assess, not only what personal data has been acquired, but also how it is protected.  Personal data," she says, "has a real value; so organizations have a legal duty to ensure its security, just like they do with any other asset.  If that doesn't happen, we will not hesitate to take strong action when necessary to protect the rights of the public."



So I guess, I hope, that they can't simply levy a fine.  I hope that, for example in this case, Marriott says, no, prove that we were negligent.  Then there will have to be an investigation that the ICO has to undertake in order to demonstrate Marriott's negligence post-acquisition.  Because she's saying that they have an obligation, even for organizations that they acquire.  So you imagine Marriott did something.  I mean, we talked about it at the time, that there was some looking at what it is that they're getting.  They missed it, clearly.  But everyone makes mistakes.  Anyway, it'll be interesting to see how this plays out.  But I agree, Leo, just telling them we're going to force you to spend this money to make yourself stronger.  It's like, well, okay.  We didn't want to spend it that way, but it's better than you guys having it.  As you said, Leo, lining the coffers.  That doesn't seem right.



And speaking of fines, although in this case it's a different nature because it was a policy decision that they're being hit with, remember a few months ago when we talked about Mark Zuckerberg addressing his shareholders and stating that they had "set aside," I think those were his words, some billions with a "b" of dollars for an expected Federal Trade Commission fine in a settlement in the infamous Cambridge Analytica-tied privacy violations.  Well, the Wall Street Journal just reported that FTC commissioners have voted and approved a $5 billion settlement with Facebook.  So there's a slap.  And certainly in this case no one would argue that these guys - this wasn't a mistake.  This was Facebook selling their information.  So they're paying the price.



Mozilla, actually it was you mentioned it while I was talking about the ISPA because it was just happening as we were recording the podcast, you mentioned last week that the ISPA had reversed their position on Mozilla.



LEO:  Yeah.



STEVE:  Paul Ducklin, who is a writer for Sophos "Naked Security," he followed up his earlier column about that nutty ISPA nomination of Mozilla as Internet Villain of the Year with a column titled:  "Mozilla aren't villains after all."  And in his piece he nicely summarized, which is why I'm quoting it, why unprotected DNS over UDP is a problem in the first place.



He wrote:  "If I unlawfully sniff your DNS traffic so I know where you went, I'm violating your privacy.  Merely by knowing where you surfed, without getting any details of what you actually surfed, I can infer an awful lot about you.  I can probably piece together your daily routine, both at work and at home; figure out your likes and fears; learn which companies you do business with, which bank you use, the shops you frequent, the clubs you belong to, the hobbies you enjoy, the medical surgery you're registered with, the sports teams you support, and much more."



So anyway, I liked that brief summary.  As we all know, there are many other means for blocking access to unwanted sites.  I just sort of wanted to follow up on this before, and we'll be talking about encrypting DNS in a minute.  But what we know is that the U.K. is unhappy with Mozilla for making it so easy, is the only way I can read this, making it so easy to blind their ISPs to the DNS queries that Mozilla's customers are making.



And so I thought I'd just say for a minute, since DNS to IP mapping sometimes changes, an ISP's content blocking device, rather than doing a match on DNS queries, could periodically make the same DNS queries their customers make, retrieve the DNS lookup IP, and dynamically manage an IP filter blocking list in order to keep those connections from being completed after the user's browser tries to make them.  Or redirect them to a "prohibited content" page or whatever.  Or some concerned organization could perform the lookups and communicate IP address additions and removals to concerned ISPs.  Or ISPs could subscribe to a published block list in the same way as spam has been thwarted since way back in 1997 with RBLs, real-time blacklists of the IPs of known spammers.



So my point is there are a great many ways to solve this problem that are just as robust as filtering on DNS.  And certainly those organizations being filtered, that is, the ones that are being blocked, know, already know that by changing their domain names, they can sidestep the filtering until it again catches up with them.  So, you know, yeah.  Enhancing the privacy of all web browsing users by encrypting DNS at the expense of asking ISPs to change the details of the way they selectively block access so that some domains which haven't yet changed their names to avoid the blocking get blocked, to me makes a great deal of sense.  And I'm glad the ISPA came to their senses on this.



And speaking of Mozilla, recall that we previously covered that shady organization who chose to name themselves DarkMatter, which was petitioning Mozilla to include their root CA certificate in Firefox's Trusted Certs store.



LEO:  What could possibly go wrong?



STEVE:  What could possibly go wrong?  At the time, cybersecurity experts and privacy advocates were strongly cautioning and urging Mozilla against doing so, stating that  DarkMatter could abuse its position - yeah - to help its surveillance operations.  Remember, it is a manufacturer of those middleboxes, which are used to intercept HTTPS connections.  And right now, if its middlebox certificate is not trusted, then users would get a warning and/or have to trust its certificate.  But if they are able to get into the root store, then their middleboxes could be issued certificates which would raise no alarm, which we don't want.



Some of these operations, that is, of DarkMatter, these surveillance operations, have been previously reported.  So it's not just it could happen.  This is what DarkMatter has done in the past.  Reports from Reuters, The New York Times, the Intercept, and other sources have detailed alleged DarkMatter-orchestrated hacking operations against human rights activists, journalists, and foreign governments, which DarkMatter carried out at the behest of the UAE, United Arab Emirate government. So these guys don't sound like anybody you want to have in your root store.  I mean, Hong Kong Post Office, that's benign compared to these guys.



So get this, Leo.  Just recently, in a last-ditch effort to find a way to get its certificates trusted inside Firefox, DarkMatter attempted to create a spinoff certificate authority business called DigitalTrust.



LEO:  Oh, much better.  I like that.



STEVE:  Much better.  Much better.  They're digital, and they're trustworthy.  That's right.



LEO:  Yeah.



STEVE:  Unfortunately, both DarkMatter and DigitalTrust were run by the same CEO.  These guys seem kind of clueless.  If you're going to set up, try to have a different organization, it's really not your name.  We don't like your name DarkMatter, but that's not why we said no.  So creating a spinoff run by the same guy called DigitalTrust, oh, yeah, trust us.  No.



So taking everything into consideration, having given plenty of time for contemplation, and because they really don't want to deny anybody who should have this privilege out of hand, Mozilla has finally announced its decision last week in a Google Groups discussion.  Wayne Thayer, Certificate Authority Program Manager at Mozilla, said:  "Our foremost responsibility is to protect individuals who rely on Mozilla products."  He said, "I believe this framing strongly supports a decision to revoke trust in DarkMatter's existing intermediate certificates."



He says:  "While there are solid arguments on both sides of this decision" - I guess the argument on the pro side is, well, maybe they aren't bad.  He says:  "It is reasonable to conclude that continuing to place trust in DarkMatter is a significant risk to our users."  He says:  "I will be opening a bug requesting the distrust of DarkMatter's subordinate CAs" - that is, the intermediate certificates - "and will also recommend denial of the pending inclusion request and any new requests from DigitalTrust."



So anyway, the distrust of the subordinate CAs that Wayne was referring to was something we also talked about before.  DarkMatter had been issuing certificates, which would be trusted by Firefox, using an intermediate CA certificate which had been signed by QuoVadis, which is trusted.  So those certificates, that intermediate CA, is going to be killed, as well.  Once Mozilla removes the QuoVadis intermediate certificates from Firefox in a future update, all websites that use TLS certificates acquired from DarkMatter will show the standard illegal certificate warnings in Firefox, warning and blocking users from accessing their content.



So what I'm wondering now, because we don't know, I haven't seen, is what Windows and other root stores are going to do.  Recall that in order to prevent problems with third-party AV, Mozilla stated that in some conditions they will be importing the Windows CA root and trusting certs signed by those roots.  So the specific conditions are, if you're on Windows 8 or Windows 10 with a recent Firefox, and who isn't recent, anything since 66, and I think we're at 68 now, and you have a non-Windows Defender AV registered with the system, in those cases, Windows 8 or 10, and you're using a non-Windows Defender AV, then Firefox may be, and I think is, but I can't test it here because I don't have a non-Windows Defender AV, Firefox may be turning on the option to trust the Windows root store.



If Windows is trusting DarkMatter certs, and I don't know whether it is either way, or their previously issued QuoVadis intermediate cert, then your Firefox would, too.  There is the switch, the option switch that we talked about before.  Maybe you can inspect it and see if it is set, if your situation matches that, that is, you're probably using Windows 10, and if you're using a non-Windows Defender AV.  Go to about:config in your address bar and put in security.enterprise.  That will bring up one entry, security.enterprise_roots.enabled.  I checked it, and for me, it was set to the default of "false."



But as I understand it, Firefox in those circumstances, because they don't want to be causing problems with not recognizing certs that are installed by these AVs, which are wanting to filter HTTPS TLS connections, they will be bringing the roots which are registered with Windows into the Firefox store.  And that switch will be set to "true."  So it'll be interesting to find out whether other roots follow Mozilla's lead and do not trust DarkMatter certs.



Oh, and one other nice forthcoming Mozilla feature, the next release of Firefox, 69, will add a tracker blocking report.  When we get 69, and I'm not sure when that's scheduled, putting about:protections into the URL will bring up a graphical display showing how many and of what type of trackers Firefox has autoblocked during the previous seven days.  We didn't talk about this when it happened since so much was happening that particular week.  But it was last month.



Mozilla has decided, I guess, pretty clearly, to differentiate itself from the Chromium-based browsers by focusing upon privacy through proactive anti-tracking.  They released the full version of their enhanced tracking protection they call ETP, in Firefox 67 last month.  It added default blocking for cross-site tracking, which are, as we know, small bits of JavaScript embedded in websites by advertisers.  Those bits of code send back our location to monitor what we're doing across the web for the purposes of generating profiles.



At the same time, Firefox released an updated version of its Facebook container, which stops Facebook from tracking people in the same way.  So all of those Share and Like buttons which appear ubiquitously across the web, which report back to Facebook even if they are never clicked, are now also completely blocked by the updated Firefox container, along with all the other connections to Facebook's servers that might happen.  So in May, oh, and also in May Firefox began blocking cryptomining for us and also is now blocking fingerprinting.



So all of those things are now being handled by default by Firefox.  And in Firefox 69 we'll get a very nice graphic.  I have a picture, a snapshot from the proposed graphic in the show notes for anyone who's interested.  And it breaks out all of the different types of blocking and how many of these trackers have been blocked over the course of the last seven days.



So Chrome is a great browser, and it now, as we know, has the majority of the Internet.  But we also know how Google makes its money.  I love their search engine.  And this show notes document was created using their very slick online tools.  But I am more closely aligned with Firefox's philosophy.  I love having its tiny tabs down along the sidebar.  And Firefox works perfectly for me.  So I expect to be sticking with Firefox for the foreseeable future.



And now, Leo, in this week's installment of Wrestling a Simple Idea to the Ground, we have the paper which will be delivered this Thursday during - ah, it was the 43rd.  I knew it couldn't be 87th.  It was the IEEE 43rd Annual Computer Software and Applications Conference, which is COMPSAC, C-O-M-P-S-A-C, titled "CTRL-ALT-LED:  Leaking Data from Air-Gapped Computers via Keyboard LEDs."



My first thought about hearing this was that it should have been named, rather than CTRL-ALT-LED, CTRL-ALT-DUH because, okay, it's obvious to all of us that, if software can blink a keyboard's LEDs, you know how keyboards have, what, Caps-Lock, Scroll-Lock, and Num-Lock LEDs.  So if you can put those under software control, and you can, and you can install malicious software in a computer, and we know that happens, and you can arrange to have something watching the LEDs over time, obviously at a time when there's no one sitting there because otherwise they'll think, what the heck?  What the hell?  My computer's just gone berserk.  My lights are blinking on my keyboard like crazy.  If all of those preconditions can be set up, then, yeah, you could send data, you could exfiltrate data from the computer.



Now, to their credit - and Leo, you're scrolling this on the screen right now.  I have it in the show notes.  They really did solve the problem.  And as I said, I want to put them on the task of figuring out what the best communications medium to use is between the bottoms of two Dixie cups which are stretched so that, when we talk, we get the most clear communication.  And for that matter, since it's really not a really clear communication, maybe which language would be best used for increasing intelligibility of a Dixie cup telephone because these are the guys you want to put on that project.  They really wrestled this thing to the ground.



They have looked at the nature of the radiation pattern as a function of angle from dead-on, the Lambertian radiation pattern, the transmitted power as a function of how far off of the axis you are from the LED.  If you have a camera lens which is imaging multiple LEDs, to what degree are you able to differentiate between the LEDs spatially when the LED illumination falls onto the camera sensor.  They have a wide range of camera types that they have experimented with.  And what about if you have a non-imaging receiver?  If you have something that can only detect the illumination, but doesn't have a focusing lens, so all you can detect is a subtle change in brightness in the room, for example.



And they wrestled it all the way down to which keyboards produced the highest bit-rate, believe it or not, looking at Dell with a single LED versus multiple LED, Lenovo the same conditions, Logitech keyboards, or Silverline.  And for anyone who is worried about this, the Silverline keyboards allowed them, at a relatively low bit error rate of 3.10%, they were able to very cleverly manage an exfiltration of a little more than 5 kbps.  So that's, you know, that's not bad.  That's way over Morse code.  Anyway, for anyone who's interested or worried, I guess you could, I mean, I never really use those lights on my keyboard.  You could not only stick a Post-it note over your...



LEO:  One more thing to tape up.



STEVE:  ...webcam, exactly.  And these guys are the blinking light experts.  They're the people we've talked about before who worried that hard drive activity lights could be used for exfiltrating data.  And remember we talked about them saying that the lights on routers could be used.  And I said, "What?"  You know, maybe if the data bits were exposed on the LED.  But routers don't put the data bits.  They, like, sort of aggregate them together and just blink the light slowly if anything is happening at all.  So I don't know.  They do tend to push the limit.



LEO:  Ilya says:  "What about the flash on your camera?"  That could probably be used to...



STEVE:  Yeah.



LEO:  On your camera phone; right?



STEVE:  Yeah.  Wow.  Anyway, so they really wrestled the problem to the ground.



LEO:  These guys are the kings.



STEVE:  They are.  So I have the note in Errata that I already mentioned was that Elaine, hearing us talk about how long we've been doing the show, corrected me, saying, no, Steve.  I think maybe I said we were in year 13 or something.  She said no, year 15 will begin next month, on August 20th.  So we'll be right here.



LEO:  You can listen to Security Now! #1 right at TWiT.tv/sn1. 



STEVE:  That's every one from then on is available.



LEO:  Yup, they're all there.  And I try to keep numeric numbering, so sn1, sn2, sn3, sn4, all the way up to sn, what is it, 723.



STEVE:  723.  Yes, my friend.  And I did want to just make a mention that Lorrie and I managed to slug our way through "Stranger Things 3."  Ugh.



LEO:  Oh, that's too bad.



STEVE:  Yeah, yeah.  It was a disappointment.



LEO:  It was so good, the first two.  It would be hard, you know, it's hard to keep up that quality.



STEVE:  Yes.  And the other thing that was really annoying was it seemed like such a commercial play.  You know, there were clear product placements throughout the entire thing.  I mean, the fact that it was held in a mall where you were seeing the various stores advertised.



LEO:  A lot of those stores were '80s vintage stores that no longer exist.



STEVE:  Some of them, yes.



LEO:  They rebuilt that whole mall.  They created it, yeah.



STEVE:  Yeah, yeah.  So anyway, a couple pieces of closing the loop.  Oh, and this is relevant to our topic today.  Chuck posted to GRC's Security Now! newsgroup.  He said:  "I enjoyed show 722 yesterday.  Last night I checked the Firefox setting to 'Enable DNS over HTTPS' (DOH)."



LEO:  Doh.



STEVE:  Doh.  He said:  "I chose a VPN location in Europe and started browsing."  He says:  "There was a dramatic improvement in speed with which websites started loading on the screen."  He says:  "I mean really fast."  He says:  "Tonight I'm going to do some unchecking and rechecking to confirm that DOH is responsible.  Is DOH improving the efficiency of a VPN connection?"  And then he followed up exactly 24 hours later with:  "I disabled DNS over HTTPS (DOH) in Firefox last night. Web page loading performance slowed considerably."  He says:  "Naturally I turned it back on, and the joy of quickly loading web pages returned."



LEO:  Well, that company has a slow DNS server, I guess; right?



STEVE:  Yes, exactly.  And so it may well be that by turning the tunneling on, you are avoiding the slow DNS service you were using by default, and you got a big acceleration.



LEO:  Yeah.  A lot of ISPs have crap DNS servers.



STEVE:  Yes.  It's sort of an afterthought; you know?  It's not like, you know, we've got the best DNS.  It's like, yeah, we had to plug one in, so it's over there in the corner.  I mean, it's not very glamorous.  So it is often the case that even though the ISP's DNS service is almost by definition the closest server to you in terms of the connection, it may not be the fastest one to deliver a response.



LEO:  Well, that's why you wrote your DNS Benchmark.



STEVE:  The Benchmark, yup.



LEO:  You can easily test it.



STEVE:  So our topic, "Encrypting DNS."  There's four things we have.  We have DNSSEC; DNSCrypt; technically we have DNS Curve, but that never, that's sort of been replaced by Crypt; DNS over HTTPS; and DNS over TLS.  So let's talk about each of those to sort of clarify where they stand, what's going on, what they do.  DNSSEC first because it's not encryption.  It provides cryptographically signed DNS records, which allows DNSSEC-aware OSes to verify that the DNS response which was received, which may have been cached and forwarded from its originating authoritative DNS server, has not been tampered with or altered in any way.  So it's just a signature.  That's all it is.  Since it's signed with a private key, which no forger can have, this essentially means that we're assured that the received DNS reply is authentic.  It hasn't been tampered with.  So that's all good.



But what DNSSEC does not do is encrypt.  It was never intended to provide privacy, only authenticity.  So the records are signed; and as I said, they cannot be tampered with.  But anyone watching the traffic will still see the DNS client's queries and their replies just as if DNSSEC was not in use because all it does is it adds a signature record to the existing DNS reply, which allows a DNSSEC-aware client to check the signature.  Oh.  But before I go on, I'll note that all three of the full encryption options, that is, the other three things - DNSCrypt, DNS over HTTPS, and DNS over TLS - all three of those are now compatible with DNSSEC.  The earliest versions of DNSCrypt were not compatible with DNSSEC, which is what you remembered from our original coverage of this, Leo, back in the old days.



But an update to DNSCrypt allowed essentially a full encapsulation of DNS so it became DNSSEC-compatible.  So that while that's not been true all the time, it is true now.  So DNSSEC can be used with all of the three encryption solutions.  So we first discussed DNSCrypt back in the context of OpenDNS, which was subsequently purchased by Cisco.  DNSCrypt uses the same fast, lean, and secure crypto that I chose to use with SQRL.  That's Dan Bernstein's Elliptic Curve 25519.  It successfully provides encryption for privacy, but it is not nearly as attack and hack resistant as we would wish a contemporary protocol to be, since it does not use any of the existing public certificate infrastructure.  The server's public key is published over DNS, and it's implicitly trusted, though it can be verified with DNSSEC.



So when DNSCrypt added DNSSEC, then it does allow for privacy and protection, and you can verify the server's public key in order to provide protection.  But what it really means is that DNSCrypt was simple and lightweight.  It was the progenitor of these later full tunneling protocols.  It could ride atop either UDP or TCP, which was a benefit.  Unlike the connection-oriented protocols, it required much lighter server resources.  So it was very easy to implement, did not require a full TLS stack and the security troubles that we have, as we know, that implementing full TLS can bring with it.  But it never made it to the IETF standards, does not have an RFC, and was never taken up by the IETF for standardization.



So it's there.  There are providers for it.  It was a pioneer in encrypting DNS.  But my sense is that it sort of wasn't the right solution.  There is a tool known as DNSCrypt-Proxy which is written by Frank Denis.  He wrote it in GoLang.  It supports both DNSCrypt and DNS over HTTPS.  And we're going to be referring to that in a minute because it ends up being probably the right solution.  It provides client services for Linux, BSD, Windows, macOS, Android, and others.  And there are a whole bunch of binary distributions ready to run.  I've got the link to it, the GitHub link, in the show notes.  Again, it's DNSCrypt-Proxy, so I'm sure if you just google "dnscrypt-proxy" you'll be able to find it.  And there is, if you are a Windows user, there is a simple configuration tool for it called Simple DNSCrypt, which provides a very nice-looking front end.



Okay.  So the second to the last is DNS over TLS.  And as we know, HTTPS runs over TLS, which runs on top of TCP.  So DNS over TLS is, as it sounds, a protocol for encrypting and wrapping DNS queries and their replies in a TLS tunnel.  So that means that we get both privacy via TLS's encryption and authentication via TLS's support for the entire public key infrastructure, all the root certs and certificates and all.  So this prevents eavesdropping, thanks to encryption, and any manipulation of DNS via man-in-the-middle attacks which, as we know, simple DNS over UDP is extremely prone to.



Cloudflare, IBM's Quad9, Google.  There's a company, Quadrant Information Security, and CleanBrowsing are providing public DNS resolver services via DNS over TLS.  So it's broadly available from some big, well-connected services - Cloudflare, Quad9, and Google.  Back in April of 2018, Google announced that Android Pie would include support for DNS over TLS, and it does.  I'll get to that in a second.  There's DNSDist from PowerDNS also announced support for DNS over TLS in its latest version.  Users of the older BIND DNS server can get DNS over TLS by proxying it through stunnel.  So that is to say, it's just DNS running through a TLS tunnel.  That's all it is.  And the newer unbound DNS server, which is in the various BSDs now, it has supported DNS over TLS natively since early last year.



So that's definitely something to consider.  DNS over TLS is a nice option, especially if your client platform, like Android Pie, supports it natively.  There's a link in the show notes here to a Cloudflare post about doing exactly that.  They say first, in Android Pie and probably subsequent, go to Settings.  Under Network and Security, Advanced; and you'll find under Advanced, Private DNS.  Select the Private DNS provider hostname option.  Enter one.one.one.one or 1dot1dot1dot1.cloudflare-dns.com and hit Save.  Then visit 1.1.1.1/help to verify DNS over TLS is enabled.  And it's just that simple.  So my goodness.  If you're an Android user, why would you not turn this on and immediately have all of your Android smartphone or other Android device DNS tunneled through DNS over TLS to Cloudflare?



Oh, and last week I misspoke about the pfSense firewall support.  I said it was DNS over HTTPS.  That's what we were talking about last week.  It wasn't.  It's DNS over TLS.  But again, there's plenty of support for it.  And it essentially provides all of the same services as DNS over HTTPS, as long as you have a provider on the other end, and there are plenty of providers.



And so finally this brings us to DNS over HTTPS.  It is a proposed IETF standard, as I mentioned last week, specified under RFC 8484.  It uses HTTP/2 or HTTPS, and supports the on-the-wire format of DNS responses.  So exactly as are returned by existing UDP responses, meaning that you just take exactly what a standard DNS server would send back in a UDP packet, you stick that - you reply over HTTPS the same thing.  That means that it is extremely simple to bring up on a web server, to allow a web server to host DNS over its existing protocols.  It defines a new HTTPS payload type with a MIME type of application/dns-message.



So you know how MIME types typically are like plain/text or application/something, Excel or Word or something, this is application/dns-message to identify it as DNS content.  When HTTP/2 is used, because of the features of HTTP/2, the server can even push DNS answers that haven't been queried yet because remember that HTTP allows you to do send ahead.  So it's able to push values that it anticipates the client may find useful in advance.



So it feels to me like either DNS over TLS or DNS over HTTPS, depending upon your platform, is the one you want to use.  And the client I mentioned before, even though it still has the name DNSCrypt, DNSCrypt supports DoH, DNS over HTTPS.  So for users, like for Windows, well, actually it's widely supported - Linux, BSD, Windows, Mac, Android, and more.  You can install DNSCrypt-Proxy, which you can get a binary from, making it easy, you don't have to build it yourself, from GitHub.  You install that and configure it on your OS, and you will get - because it also not only supports DNSCrypt but full DNS over HTTPS.



And there are plenty of all of those other providers, all of the big DNS providers also support, like Cloudflare and Quad9 and Google, support DNS over HTTPS.  And if you are a Windows user, you can use SimpleDNSCrypt.org, https://simplednscrypt.org, a front end, very nice configuration front end for the DNSCrypt-Proxy on Windows.  And so what that essentially means, and the reason I wanted to discuss this, is that not only with a Firefox browser can you now flip a switch and have your Firefox DNS protected from snooping.  But it is entirely practical to install DNSCrypt on any OS platform, configure it to use the big DNS provider of your choice, and you go dark to anyone, your ISP or anybody else who may be sniffing your traffic.  And it sure looks like you suffer nothing in terms of performance loss.



LEO:  So, well, not anybody else, but anybody else in between you and the DNS server at that point.



STEVE:  Correct.  Exactly.  All of your queries are emerging and are known to, for example, Google.  Or Goog.



LEO:  Right, the Goog.  The Goog knows all.



STEVE:  The Goog.  The Goog knows all.  Especially if you choose them as your DNS over HTTPS endpoint.



LEO:  Right, right.  Well, yeah, then there's Verizon.  You could choose them.  They're known for their privacy policies.



STEVE:  Ah, yes.



LEO:  I would use - Quad9, we still don't know.  Who is running Quad9?



STEVE:  IBM.  



LEO:  Oh, it's IBM.  Well, I trust them.



STEVE:  They offered it as the service.  Yeah, I do, too.  They have...



LEO:  Cloudflare is good.  Quad1, yeah.



STEVE:  Yup, yup, for sure.



LEO:  Yeah.  And you know, I was just again talking to Ilya, who's been a font of information in here.  Google's Pixel phones come with a similar, like 1.1.1.1 built into the phone, so you can turn that on to make DNS queries.



STEVE:  Nice.



LEO:  But it goes back to Quad8.  It goes back to Google.  It's basically Quad8.



STEVE:  Right.



ILYA:  But you can change it.



LEO:  You can change it?  Oh, I'll have to look in the settings.  You can change it to something else, if you want Quad1 instead.



STEVE:  Away from the Goog.



LEO:  The Goog.  The Goog knows way too much.  Why give them more?  That's my philosophy.  Steve, great episode.  I'm going to run home and watch the mall episode of "Stranger Things."



STEVE:  Please don't.



LEO:  You said one aisle you can see one brand of cereal.  You could see it all clearly.  And across the aisle everything's blurred out.



STEVE:  Yes, yes.  I mean, it was obnoxious.



LEO:  That's pretty bad, yeah.



STEVE:  And it was all cereal, all by one manufacturer.



LEO:  Well, Netflix is running low on money.  You've got to help them out a little bit here.



STEVE:  Yeah.



LEO:  Yeah.  We do this show every Tuesday.  We try to get in here about 1:30 Pacific.  We're usually pretty good.  But if we're a little late, well, you'll understand.  That's 4:30 Eastern time, 20:30 UTC Tuesdays.  You can watch it live at TWiT.tv/live, or listen.  We've got live audio and video streams there.  If you're doing that, chat with us at irc.twit.tv.  Always a good bunch of people in the chatroom during Security Now!, smart people.



Also, often, really great people in the studio audience.  It was nice to have Ilya here.  If you want to be in our studio audience - he's waving at you, Steve - all you have to do is email tickets@twit.tv.  Steve's waving back.  And we'll be glad to put a chair out for you.  This is a pretty small studio.  So any of the shows like Windows Weekly, Security Now!, The Tech Guy show that I do in here, it's a very good idea to email ahead because sometimes it can fill up, and we don't really have an overflow studio for you.  The big studio, well, we can always get one more person in there.



If you want to get versions of this show after the fact, there are several places you can go.  Start with Steve's site, GRC.com.  In fact, while you're there, pick up - you've got transcripts.  You've got audio.  And you can also pick up a copy of SpinRite, the world's best hard drive recovery and maintenance utility, and Steve's bread and butter.  That's the best way to support Steve.  And by the way, you get some value out of that, some real value out of that.  SQRL's there, too, a lot of other great stuff:  GRC.com.  Steve is @SGgrc on Twitter.  You can DM him there.  He's open to DMs, if you have a suggestion or a question you'd like him to answer.  You can also go to GRC.com/feedback for the feedback form.



We have audio and video at our site, TWiT.tv/sn.  And of course, as always, you can subscribe.  Probably the best way to do it.  Find your favorite podcast application.  Search for Security Now! - 15 years, we should be in there by now - and subscribe.  That way you'll get it every time it's available, the minute it's available.  Stay on the line, Steve, because we've got to do some future planning.  But thanks for being here, and I'll see you next time.



STEVE:  Yes, good.



LEO:  On Security Now!.



STEVE:  Perfect.  



Copyright (c) 2019 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.






GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#724

DATE:		July 23, 2019

TITLE:		Hide Your RDP Now!

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-724.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we start off with something bad that we unfortunately saw coming.  We then look at the changing security indication feedback in browsers; the challenge of keeping browsers compatible with important but non-standards-compliant websites; the failure and repair of incognito browsing mode; the possibility of a forthcoming "super incognito mode" for Firefox; a new super-fast TLS stack written in the Rust programming language; Microsoft's promised open source release of their voting machine election software; and yet another widely deployed, exposed, and exploitable Internet server.  We have a quick bit of miscellany and some terrific SQRL news.  Then we look at a recent and quite sobering report from Sophos about attacks on exposed RDP servers.



SHOW TEASE:  It's time for Security Now! with Steve Gibson.  I'm Jason Howell, filling in for Leo this week.  Steve is going to talk about how Kazakhstan might actually be writing the man-in-the-middle playbook for authoritarian regimes, the Torification of Firefox, Microsoft's ElectionGuard, how incognito mode isn't very incognito when it comes to paid content sites, and Steve's RDP honeypot whitepaper.  Fascinating stuff, coming up next on Security Now!.



JASON HOWELL:  This is Security Now! with Steve Gibson, Episode 724, recorded Tuesday, July 23rd, 2019:  Hide Your RDP Now!



It's time for Security Now!.  I'm Jason Howell, filling in for Leo Laporte.  That's right.  He's gallivanting somewhere in the world.  Joining me today is of course the man who makes this show entirely possible, Steve Gibson.  Steve, how are you doing today?



STEVE GIBSON:  Great.  And great to be with you.  I wasn't expecting you to be co-hosting, so I'm delighted.  This is a treat.  Is Leo off on business, or do we know?  Someone must know what he's up to.



JASON:  I should really know whether he's off on business, but I think it's pleasure.



STEVE:  Well, none of us know.  But I was hearing about a rollercoaster.  Maybe this is the - was this the rollercoaster trip?



ENGINEER:  Yeah, yeah.  It's Michael's 16th birthday, so they went to Venice with Michael and a bunch of his friends.



JASON:  There we go.  See, this is what happens when I take vacation last week, and they take vacation this week.  I'm just completely lost and in the dark.  I'm also in the dark because my computer is not turning on.  So this will be really interesting today, Steve.



STEVE:  So your face will not be bottom lit from your screen, lighting you up.



JASON:  No.  Yes, exactly.



STEVE:  That's all right.



JASON:  I won't be casting a shadow behind me, either.  So it's all right.



STEVE:  So we have Episode 724 this week.



JASON:  That's right.



STEVE:  And up until the last minute, I didn't have this title for the show, as you know, because you and I were talking for the last few hours as we were getting synchronized here.  But a study that I read, a result of Sophos's setting up some RDP honeypots to get a sense for bad guys trying to log into Microsoft's Remote Desktop Protocol, the numbers were so compelling, and not in a good way, that I thought, okay, that has to be the topic for this week's podcast.  And their conclusion was the same thing that I have recommended to our listeners while we've been talking about BlueKeep exploits and the problem of the fact that it's now possible to get into RDP servers without authenticating, the fact that there's really no safe way to have a remote desktop protocol exposed.



And so, for example, I use it like crazy.  But there's no open port anywhere.  And it's just not secure enough.  It has had a troubled past.  And so I titled this podcast "Hide Your RDP Now!" because one way or another you really need to have it hidden.  But we'll get to that.



We're going to start off looking at something bad that has happened, which unfortunately we saw coming.  I got a lot of tweets from our listeners over the last week saying, oh, my god, this is what you were worried was going to happen.  And yes, looks like it is.  Then we're going to look at the changing security indication feedback provided by browsers, in this case some changes coming to Firefox to follow what Chrome has done.  The ongoing challenge of keeping browsers compatible when important but non-standards-compliant websites apparently are designed for one particular browser, but no one bothers to test them on other browsers.  That turns out to be a problem.



Also we have the failure and then the subsequent repair of incognito browsing mode.  The possibility, really interesting, of a forthcoming super-incognito mode for Firefox.  A very fast new TLS stack written in the Rust programming language.  Microsoft's promised open source release of their voting machine election software, which I'm very excited about.  Yet another widely deployed, exposed, and exploitable Internet server, more than a million of them just waiting for people to hook up.  And then we've got a bit of miscellany.  Some terrific SQRL news, including my announcement of a trip into Europe to present SQRL to two OWASP groups.



And we're going to look at, as I said, we'll finish up looking at this very sobering report as a result of Sophos putting some honeypots up on the 'Net and just monitoring the bad guys.  Oh, and actually this is relevant also to ransomware.  It turns out this may be the way that they have been getting into these municipalities.  So I think another great podcast for our listeners.



JASON:  No, not "think."  I know another great podcast for our listeners.  And I was just actually checking in on your episode last week.  Fifteen years, that's crazy.  That's craziness.



STEVE:  Yeah.



JASON:  You're like podcast king, you and Leo.



STEVE:  We're closing in.  We're closing in.  Next month on the 20th we will be beginning Year 15.  So we're wrapping up Year 14 now.



JASON:  And security has gotten not any less interesting or complex.



STEVE:  Yes.  I remember when Leo suggested this to me, I remember thinking we're going to run out of stuff to talk about.



JASON:  Right.  Yeah, right.  I think your job is secure.  I think you're 100% secure in that.  All right.  Starting with the news that we have here, and it seems to be all about Kazakhstan.  What's going on here?



STEVE:  Well, yes.  So as I said, a whole bunch of our listeners tweeted this, and I got some private messages, too.  Unfortunately, we sort of foresaw this.  It was more of a - I guess it was more of a worry than a prediction.  It's something that I've been hoping would not happen, but this may be just like the beginning of the break in the dike.  We'll see.



If we look at the Picture of the Week on the first page of the show notes, if we put that on the screen, what we see on the left is the authentic Facebook certificate with the common name *.facebook.com, and then a long list of affiliated domains listed in its subject alternative name, the SAN field.  So these are all of the different domains that that certificate is valid for.  And that authentic certificate was signed, was issued by and signed by my favorite certificate authority, DigiCert.  So that's the real McCoy.



Sadly, on the right we see a certificate that is in every way a clone copy of Facebook's authentic certificate containing the same *.facebook.com common name and the identical list of affiliated domains.  But the common name of the signer of this fraudulent certificate reads "Security Certificate," which is not a valid CA.



JASON:  Sounds legit to me.



STEVE:  That inspires confidence, doesn't it?  And what we now know is that the government of Kazakhstan has begun officially requiring that its own CA root certificate be installed into the web browsers of its citizens.  And of course we know why.  It's so that it can do just what we saw.  That certificate, that fraudulent Facebook.com certificate, was synthesized by Kazakhstan and signed by their root certificate.  It of course would never be trusted.  It's bogus.  It's fraudulent.  So it would never be trusted by anyone's browser anywhere, except if you had installed their root cert in your machine, which is what they are now beginning to require their citizens to do.



They say that this will allow them to increase the security and provide protection for the citizens of Kazakhstan.  What we understand, of course, is that it allows essentially a man in the middle to intercept the traffic.  We've talked about how enterprises often will do this.  What was that - it's not BlueKeep, it's Blue something, I can't remember the name of it.  There's a very often-deployed, you know, it's called a "middlebox."  It's on the border of an enterprise.  The employees in the enterprise install this certificate into their PCs.  And actually it's done for them.  It's typically done over the network.  Anybody who connects a machine up onto the network gets this certificate added to their machine.  And what this allows the enterprise to do is to scan incoming and outgoing traffic for malware, for viruses, perhaps to do content control, whatever.



Well, it's one thing to do that in an enterprise, and an entirely different thing for a nation to do that to its citizens.  The first indication of this, which is what some astute people saw and sent to me, was a Bugzilla report #1567114, which was titled "MITM [man in the middle] on all HTTPS traffic in Kazakhstan."  This Bugzilla report reads:  "Since today, all Internet providers in Kazakhstan started MITM on all encrypted HTTPS traffic.  They asked end users to install government-issued certificate authority on all devices, in every browser."  And unfortunately the links here are in either Kazakh or Russian, neither of which I read.  So they're not very informational to English-only people.



But Matthew Hardeman quoted in the thread.  So what this started was a dialogue that was held over on a thread, a discussion thread on Google.  And his was, I think, one of the more insightful, balanced, you know, what are we going to do about this?  And a bunch of Mozilla.com people were involved and copied on this.  He wrote:  "If the government of Kazakhstan requires interception of TLS as a condition of access, the real question being asked is whether or not Mozilla products" - because at this point this is over on the Firefox side - "Mozilla products will tolerate being used in these circumstances."



He says:  "Your options" - he's writing to the Mozilla people.  "Your options are to block the certificate, in which case Mozilla products simply become unusable to those subject to this interception, or not block the certificate."  He says:  "I certainly think that Mozilla should not distribute the MITM root nor do anything special to aid in its installation.  I believe policy already makes clear that no included root, commercial or government, is allowed for use in MITM scenarios; and I believe that policy should be held firm.  I do believe that, as it is manually installed rather than distributed as a default, that it should continue to override pinning policy.  This is an accepted corporate use case today in certain managed environments," as we were just saying.



"The dynamic is quite different for an entire people under an entire government, but the result is the same.  One has to choose whether to continue serving the user, despite the adverse and anti-privacy scenario, or if one simply won't have their products be any part of that.  Much has been said," he writes, "about the TLS 1.3 design hoping to discourage use cases like this, but the reality is," he says, "what I always suspected:  some enterprises or governments actually will spend the money to do full active MITM interception.  Let's posit what might happen if Mozilla made their products intentionally break for this use case.  Further, let's stipulate that every other major browser follows course, and they all blacklist this or any other nation-state interception certificate, even if manually installed."



And of course we know that's possible, just to take an aside here, because when the certificate comes to the browser, it chains up to a root.  And Mozilla could check to see whether it's the "security certificate" from Kazakhstan, rather than a valid certificate like DigiCert's root.  So it's possible from a technological standpoint to do this.  It's trivial.  And of course we've covered in years past where, when CAs have been caught doing things wrong, the browsers like Chrome and Firefox have blacklisted their roots.  So, again, I mean, there are already blacklists in our browsers to prevent known bad roots from being used for any purpose at all.  Kazakhstan could easily be added, like in an instant.



So he says:  "Isn't the logical outcome" - that is, if all browsers blacklisted.  "Isn't the logical outcome that nation-state forks one of the open-source browser projects, patches in their MITM certificate to undo the blacklisting?"  He says:  "I think that's exactly what would happen.  The trouble is, there's no reason to expect that the fork will be maintained or updated as security issues are discovered and upstream patches are issued.  We wind up with an infrequent release cycle browser being used by all these users, who in turn get no privacy and get their machines rooted disproportionate to the global population."



He finishes:  "I do definitely support a persistent UI indicator for MITM scenarios that emphasizes onscreen at all times that the session is being protected by a non-standard certificate and some sort of link to explain MITM and the risks."  So he finishes, I mean, that's the end of his posting that I thought took a very good position on this.



So some background here.  The Kazakh government first tried to have all its citizens install a root certificate three and a half years ago in December of 2015.  And that sounded familiar, so I imagine it caught our attention, and we talked about it at the time.  Back then, the government ruled that all Kazakh Internet users had to install the government's root certificate by January 1, 2016.  However, that ruling was never implemented because the government was sued by several organizations, including ISPs, banks, and even some foreign governments who feared that this would result in weakened security of all Internet traffic, and weakened security for the businesses originating from the country.  And of course they were, and still are, correct in that regard.  At the time, in December 2015, the Kazakh government also applied to Mozilla to have its root included in Firefox by default, but Mozilla at the time declined.



So here we are now.  And starting last Wednesday, July 17th, the Kazakhstan government has started intercepting all HTTPS traffic inside its borders.  Local Internet service providers have been instructed by the local government to force their respective users into installing a government-issued certificate on all devices and in every browser.  And as we know, once that's been installed, the certificate will allow local government agencies, well, pretty much anybody who has the matching root which would allow them to sign an on-the-fly-created certificate like that one we saw for Facebook.  That was actually caught - that was caught on the wire.  That was caught in use.  So we know it's just not a theoretical question.  I mean, there is a fraudulent Facebook certificate.  And you have to imagine Facebook's not happy that their cert has been copied.  Oh, well.



So in a statement posted on its website, the Kazakh Ministry of Digital Development, Innovation, and Aerospace said:  "Only Internet users in Kazakhstan's capital of Nur-Sultan will have to install the certificate."  However, users from all across the country reported being blocked from accessing the Internet until they installed the government's certificate.  Some users also received SMS messages on their smartphones about having to install the certs, according to local media.  Ministry officials said that the measure was "aimed at enhancing the protection of citizens, government bodies, and private companies from hacker attacks, Internet fraudsters, and other types of cyber threats."  Yes, we're looking out for you by intercepting all of your otherwise secure traffic, which wouldn't have been subject to any of those problems before.



Now, it's true, as we know, this sort of interception could be used to benignly filter traffic for malware, just as corporations do.  But unfortunately it's equally possible to scan and surveil traffic for whatever other content the government or other MITM agency might choose.  So as a consequence of this, at the moment, Google, Microsoft, and Mozilla are gathered to discuss a plan of action about dealing with websites that have been reencrypted by the Kazakh government's root certificate.  It's going to be very interesting to see how this develops.  And what seems most likely is that, as the person who I quoted suggested, that the browsers and other devices will adopt some form of persistent visible warning, you know, like all the chrome around the edges of the browser might turn scarlet or something.



JASON:  Or flash at you.



STEVE:  Yeah, I mean, you know...



JASON:  For the entire time that you're using it.



STEVE:  That's right.  And it's true.  Since the industry's best browsers are now all open source, private labeling a Kazakhstani browser containing only Kazakhstan's root certificate would not be an insurmountable task - although, as he also noted, maintaining it and keeping it current would be a big job.  Which means it probably wouldn't happen.  And people would have a going-out-of-date browser that is the only one that worked in Kazakhstan, which really does not really seem to be serving the best interests of its users.



So, you know, most users will have no idea what this is about.  They're told to do something in order to, you know, their government, oh, the government needs me to install something.  Okay.  And so what choice do they have?  I'm sure the media will give them a sense for what's happening.  Anyway, so it's either lose your privacy or lose the Internet.  You choose.  And of course it's also likely that other repressive and authoritarian governments are probably watching this to see what happens because they'd all like to do it, too.



Now, in some later follow-up reporting, the water was a bit muddied, although based on the fact that people outside of the capital were reporting they could not access the Internet at all until they added the certificate, it really does sound like what this official cited.  Caleb Chen, writing for the PrivateInternetAccess blog, noted, he said:  "A Kazakh official clarified on the 19th that the installation of the certificate was voluntary and not a prerequisite to accessing the Internet."



Now, as I read that again, I'm thinking, well, maybe they meant, like, email or something that is not using HTTPS and browser certs.  So maybe this guy is sort of trying to say, oh, yeah, you could still use the Internet; but, okay, yeah, but not your browser, which is for many people the Internet, just as "the Google" is the way you find the Internet.  Anyway, Caleb provided a link to the page, and I have it in the show notes for what good it does anyone.  Maybe if you read Kazakh or Russian, and I'm not even sure what language it was written in.



JASON:  Google will translate it for you, by the way.



STEVE:  It's Greek to me.  And that translation's getting better, too, isn't it.



JASON:  It's getting really good.  I'm really impressed.



STEVE:  Yeah, I've not used it for a while.  So anyway, so what do tech-savvy Kazakhs think about this?  The Mozilla bug thread contains a posting from one.  He says:  "I am a citizen of Kazakhstan.  If Mozilla/Google Chrome developers see this message, I kindly ask you to consider blocking the above-mentioned certificate and any access to your browsers for the certificate holders.  If this certificate didn't pass web trust audit, it can be the same as presented in 2016," meaning same outcome.  "So blocking it from the major world browsers is the only chance for Kazakhs to avoid MITM attacks and keep at least some privacy rights, meaning that, if blocked/blacklisted, the government will have to call back the certificate, as was done in 2016."



He finishes:  "If the certificate is not blacklisted, but only the visual message will pop up warning users about untrusted certificate, it will not help, since majority of citizens, especially elderly ones," he writes, "simply will not pay enough attention to such a message."



So of course this has caused an industry-wide kerfuffle.  Our illustrious crypto professor Matthew Green weighed in via Twitter.  He tweeted, Matthew tweeted:  "This is the future all those cypherpunks warned us about.  Let's hope the West sees the danger and doesn't look on with admiration."  So, yeah, that has happened.  And we'll see how long it lasts.  There was a big pushback.  And as our listeners know, I have worried that at some point we might see, we in the U.S. might see our ISPs that are the perfect place to intercept and filter our HTTP traffic "on our behalf."



JASON:  To protect us.



STEVE:  Yes.  Your ISP is the entity from whom you purchase your bandwidth.  So it would be logical for them to say, okay, we're upgrading your service.  You need to install this certificate in your browser in order to continue accessing websites on the Internet through us.  Boy.



JASON:  Not a bug.  It's a feature.



STEVE:  Hasn't happened yet.  And boy, I'll be surprised if it does here.  But we'll see.



JASON:  Yeah, I mean, that last comment from Matthew Green really feels like a 50/50 chance it could go either side of what he's saying there, either seeing the danger or looking on with admiration.  You just really don't know.  That's kind of where we are right now.



STEVE:  Well, and as we've said, our government is unhappy that it's unable to see into our browser sessions.  And so I don't know.



JASON:  Yeah.



STEVE:  Speaking of indicators and browser security and privacy, starting in October of this year with Firefox 70 - so that's two from now, we're currently on 68 - Mozilla will be prominently marking all HTTP, that is, non-HTTPS pages as not secure, the way Chrome does now.  What Mozilla has been doing with Firefox so far was only choosing to show not secure indicators on HTTP pages, where it probably mattered more, meaning pages which accepted some user-provided content containing forms or login fields.  However, today the percentage of sites serving HTTPS has now surpassed 80%.  I mean, it's just what you do.  So their thinking is that users don't need good news for what is rapidly becoming the default, that is, being secure, but rather a warning when, for some reason, a site is not secured by HTTPS because now that's the exception.



Firefox developer Johann Hofmann wrote:  "In desktop Firefox 70, we intend to show an icon in the identity block" - that's the left-hand side of the URL bar which is used to display security/privacy information, and that's actually getting kind of crowded over there.  I had to use it this morning.  I wanted to check out - I was monitoring MacBreak Weekly, and they were talking about the new "Picard" trailer for the "Picard" series coming out next year on CBS All Access.  Leo and I had spoken of it.  I'd seen the very first teaser, and I knew that at Comic-Con they had released the first long trailer.



Anyway, so I wanted a high-resolution one, and I figured that CBS All Access would be the place to get it.  So I went there, and I am a subscriber, and so I logged in.  But it wouldn't show anything, saying that I was filtering the traffic, and their sponsors were trying to show me something that I wasn't letting them show.  So I thought, huh.  And so I disabled uBlock Origin and then refreshed the page, and it still wasn't happy.  And I thought, okay.  Then I saw, okay, maybe something's still filtering stuff.  So I switched to the in-private browsing, still wouldn't display.



And then I remembered that I had turned Firefox's anti-tracking all the way up to 12.  So sure enough, one of the little icons over there, which is what made me think of it, was there.  I clicked on it, and I made an exception to CBS.com, and then I was able to happily play the 2:08 long "Picard" trailer, which I do recommend to all of our sci-fi interested listeners.  It does, I mean it's annoying that it's not part of a broader subscription like Netflix or something.  And CBS All Access, I think it's $10 a month, so it's not cheap either.  But the upcoming series does look wonderful.  And how fun to have Jean-Luc back.



Anyway, the point is, Firefox will be adding yet another thingy over in the URL bar to show people when they need to do something.  Now, it has had configurable behavior flags which were viewable and could be set, accessible through its about:config page, since the end of 2017.  Those flags are still present in the current stable version of Firefox.  And anybody who is interested can preview the forthcoming indication, if you go to about:config, and then in the search bar - because as we always say there's just a bazillion things in that about:config section.  If you search for "security.insecure_connection," that's probably enough.



That'll bring up four different options.  One is to show a broken lock on HTTP sites.  One is to show "not secure" text on HTTP sites.  The other is show a broken lock on HTTP sites in private browsing.  And the fourth, show the "not secure" text on HTTP sites in private browsing.  And you just did it.  And it looks like there's more than four.  So there must have been a couple other things that are within that same search term.  Or maybe underscore connection.  If you add underscore connection, that might weed it down further.



JASON:  Right, yeah, looks like that would cut it down to four.



STEVE:  Okay.  And as I was writing this, I thought, okay, but it can now be difficult to find a non-HTTPS site for testing.  Well, it happens I have one.  And I used it the other day because I was curious about this.  Although my new GRC link shortening service of course supports HTTPS, and it always redirects users to secure pages, I didn't see when I was setting it up any reason not to allow it to accept an incoming link over HTTP.  Browsers are still trying that first.  At some point browsers, if you don't give it an explicit https://, someday  they'll just assume you mean "s" and try that first.  Today they're still just going to HTTP.  So that means you have to have the - they try to bring up the page.  You need to be bound to that port.  You need to then redirect them over to the "s."  And so blah blah blah.  I didn't see any reason to do that.



So for our listeners, anyone who's interested about how, if you want to check any browser to see how it looks on an HTTP site, though they are becoming increasingly rare, you can go to http://grc.sc and just bring up the root page, you know, .sc/ to bring up the root.  I have just a little placeholder text there.  But it will leave you.  It won't redirect you to HTTPS.  It'll leave you there.  And you sure enough can see how Chrome and Firefox and what other browser you might be using and with configuration settings and things, how that looks in that mode.  So anyway, just a little tip since it's becoming hard to find non-HTTPS sites these days.



And speaking of Firefox's about:config page, Mozilla's Dan Callahan, writing about the changes in the latest, that is, our current Firefox 68, he was explaining about some of the subtleties of browser behavior which were required to allow Firefox to operate on websites that were apparently written to expect some specific quirks of some non-Firefox browsers.  He wrote:  "Keeping the web open is hard work.  Sometimes browsers disagree on how to interpret web standards.  Other times, browsers implement and ship their own ideas without going through the standards process."



He says:  "Even worse, some developers intentionally block certain browsers from their sites, regardless of whether" - I can't imagine doing that.  What is wrong with these people?  Anyway, "regardless of whether or not those browsers would have worked."  That's got to be - but he just wrote it, so I guess, I mean, it sounds like the dark ages.



Anyway, "At Mozilla," he says, "we call these 'web compatibility' problems, or 'webcompat' for short.  Each release of Firefox contains fixes for webcompat issues.  For example, Firefox 68 implements," and then he says, "Internet Explorer's addRule and removeRule CSS methods."  Which, okay, so I guess IE just - IE?  Wow.  IE has the ability to have JavaScript add or remove CSS methods.  Okay.  And then also Safari has a CSS property, -webkit-line-clamp.



And he says:  "In the latter case, even with a standard line-clamp property in the works, we have to support the -webkit version to ensure that existing sites work in Firefox."  And he says:  "Unfortunately, not all webcompat issues are as simple as implementing non-standard APIs from other browsers.  Some problems can only be fixed by modifying how Firefox works on a specific site, or even telling Firefox to pretend to be something else," you know, via a lie being told in the user-agent header, "in order to evade browser sniffing.  We deliver these targeted fixes as part of the webcompat system add-on that's bundled with Firefox."



And the reason I'm telling everybody about this is I never knew this was in there.  And now they've just surfaced it on the UI, and it's very cool.  He says:  "This makes it easier to update our webcompat interventions as sites change, without needing to bake those fixes directly into Firefox itself.  As of Firefox 68" - that is, the one that we all have now - "you can view and disable" - although why would you - "these interventions by visiting about:compat."  So that's my big tip, which is what this is about.  We all know about about:config.  We're talking about it all the time.  About:compat, C-O-M-P-A-T.  And sure enough, I never knew about that.  It's only just recent been created.  So I put that into my Firefox, and up comes kind of an interesting list of sites you've kind of heard of.  Some are important, some less so.  But they are sites where Firefox checks its compatibility, this webcompat feature, and alters its behavior per site in order to work right.  So kind of cool.



He says:  "Our first preference is always to help developers ensure their sites work on all modern browsers, but we can only address the problems that we're aware of.  If you run into a web compatibility issue, please report it to webcompat.com."  So they also grabbed that domain for their own efforts.  So anyway, nice piece of work to keep Firefox compatible.  And, wow, you know, it's unfortunate even today we still don't have a single standard.  We still have disparate things that random browsers just do because they want to.  Crazy.



JASON:  And they all seem to at some point, hopefully, lift each other up onto the same plane.  But, yeah.



STEVE:  Well, one of the nice things with Edge, the fact that Microsoft gave up their own Edge engine, and they're now going to be adopting the Chromium engine in Edge for Windows, automatically one fewer browser to be incompatible because, you know, it'll be an Edge browser with a Microsoft wrapper.



JASON:  Right, right.  This next story, it speaks to me a little more than some of the other stuff that we've talked about today, only because in the preparation that I do for the shows that we do on the network, we've got subscriptions to a lot of the sites that we pull news from.  But we don't, I mean, obviously we can't buy a subscription to every single site.  And every once in a while you run into that free article limit, and dang it.  I want to read this.  I want to invite that person on to talk about the article, and I can do nothing.  Well, I guess I could sign on, but anyways.



STEVE:  So what you're talking about is that we've probably all encountered teaser paywall websites that allow a limited number of articles to be viewed per month by non-subscribers.  And this sort of feels like a workable compromise between fully open and fully closed sites.  They're clearly hoping that you'll, you know, they're wanting you to see all their goodness rather than never be able to see any.  But obviously they're hoping to upsell you into establishing a subscription.  So they're hoping that you'll find value there, and then be annoyed when that one article you really want to read fades out into that notice that the site requires a monthly fee for you to keep reading.  I get hit by that all the time, just like you, Jason, when you're putting the shows together.



And this, of course, begs the question:  How are they counting the number of pages you've visited?  The answer could be some form of fingerprinting.  But we know that browser fingerprints, while containing some entropy, are not unique.  Many other browser users will coincidentally have a browser with the same fingerprint since it's just based on a bunch of characteristics that are visible from script on your browser.  And these sites are not attempting to block rocket scientists, after all, from having access.  They're only trying to put up a barrier to casual users, which is to say it's not like it's impossible to get around this.



So the way they're clearly doing it is by the universally available first-party domain cookies, which everybody has enabled in order to effectively use the Internet.  So it wasn't long before people who want to read that one additional article, but didn't want to join up, discovered that switching to their favorite browser's incognito or private browsing mode, whatever it's called on your browser, made that possible, since private browsing deliberately does not record first-party domain cookies or, for that matter, anything else.  A user of an incognito browser is suddenly unknown, and always starts out with a zeroed prior articles counter, since they appear as an unknown, uncookified visitor.



And of course in this cat-and-mouse world, just as it didn't take long for the users of private browsing for this trick to become widespread, either to be discovered independently or spread by word of mouth, neither did it take the web developers then long to figure out a way, at least in the case of the Internet's number one web browser, Chrome, how to detect when somebody was visiting their paywalled site incognito.  And then, for example, if you visit any article on the Washington Post's website using Chrome's incognito mode, you'll be greeted with the message:  "We noticed you're browsing in private mode.  Private browsing is permitted exclusively for our subscribers."



Wait.  What?  If you're a subscriber, then they know who you are, and that's not very private.  But they said, and their note continues:  "Turn off private browsing to keep reading this story, or subscribe to use this feature, plus get unlimited digital access."  But wait.  What?  The idea of being identified, even as someone who is visiting incognito, sort of puts the lie to the whole incognito thing; right?



JASON:  Indeed.



STEVE:  It's like, wait a second, you're not supposed to know anything about me, including, and perhaps even importantly, that I have deliberately chosen not to have you know anything about me, including that.  So this occurred to the engineers at, as Leo used the term last week, "the Goog," which is now...



JASON:  I love it.  It's an endearing term, "the Goog."



STEVE:  The Goog, yes.  This occurred to the engineers at the Goog.  So they decided to do something about it.  It turns out that Chrome's incognito mode disables its file system API, as you would want it to, as part of Chrome's effort to prevent the user's activities from leaving any lasting traces in the system.  Websites figured out that this could be easily checked with a bit of JavaScript running on the page.  So that's what generates that "Please disable your incognito mode to receive a limited amount of free stuff" message.  Consequently, at the end of this month, actually it'll be July 30th, one week from today, we're going to be getting Chrome 76, with a file system API that no longer gives away the fact that the browser is in incognito mode, or the browser's incognition.



And just in case publishers then go searching for some other method to detect private browsing, since Google really does wish to avoid having its incognito visitors flagged, the Goog has stated that "Chrome will likewise work to remedy any other current or future means of incognito mode detection."  So be forewarned, you web developers out there.  The Goog is going to work to protect its users' privacy.  And now we all know, I didn't realize it before, but that's going to come in handy now, that I'll be able to use in-private mode or incognito mode or whatever it's called to avoid those notices.  So thank you, Goog.



JASON:  Thank you, the Goog.



STEVE:  And taking it up another notch, and this is very exciting, Firefox may be - and it's premature, but worth discussing - may be getting super-private browsing in the not-too-distant future a la Tor.  Due to the fact that the Tor browser is a descendant of Firefox, there have been various projects and grants awarded over time to look into the possibility of adding a Tor mode to Firefox.  This is exciting because setting up a Tor session takes some doing.  And also because wrapping all outbound traffic in a multilayered, successively encrypted onion, and then having that traffic bounce three or four times around Tor nodes while each successive layer of the onion is stripped and decrypted and then forwarded on to the next Tor node, means that surfing the web through Tor is not for the impatient.



But the idea here is that Firefox users would be able to just flip a switch, the way we currently do when we turn on private browsing, in order to get the benefit of Tor on an as-needed basis.  It would allow Firefox users to just drop the cone of silence over their online activities when they need some real, I mean like some robust anonymity, and then just as easily lift the cone to their regular high-speed browsing.



So it turns out that during a meeting of the core team, developers, volunteers, and invited guests in Stockholm, they gathered to discuss plans, milestones, deadlines, and other important matters.  The idea of a Tor mode add-on for Firefox was proposed and is being considered.  I have a link to it, for anyone who's interested in more details, in the show notes.  What was said on that page is there is an idea to, in the future, have Firefox use Tor in private browsing mode or in a new extra-private mode.  They said that will take a lot of engineering work and buy-in, that is to say, meaning to innately, natively embed Tor into Firefox.  So they're kind of wanting not to do that.



So they said, to help smooth the path, there is a proposal for a Tor mode add-on.  This would not be packaged with the browser by default, but would be something that users could download from addons.mozilla.org to give them a Tor mode button or similar.  It would allow users to experience what an eventual full integration with Tor would look like.  It could also help gauge interest by counting downloads and usage and so forth.



Someone whose handle is "acat," his name is Alex Catarineu, has demonstrated how to compile Tor to WASM.  So that's been done, you know, web assem.  This would allow packaging all the necessary Tor code inside the add-on itself, without a dependency on an external binary.  The add-on would still need to be privileged.  A privileged add-on is one with elevated privileges compared to a standard web extension.  It can call XPCOM functions, for example, which are otherwise privileged.  A privileged add-on needs to be signed by Mozilla, but the idea for this proposal is to have it produced and distributed by Mozilla anyway, so that's not a problem.



The add-on would configure the browser to use Tor as a proxy, as well as setting various prefs to prevent proxy bypasses and resist fingerprint and so forth, much like those that are currently being set by the Tor browser, which is based on Firefox.  Discussion of visual options for UI, under that topic they said:  "Clicking the Tor mode button would probably open a new window that uses a dedicated profile.  This is because some of the prefs that the add-on has to set are global to a profile, not to a window or tab."



So that is to say, it could not be a Tor mode tab on an otherwise non-Tor mode browser window.  It would just have to give you a new window.  And then they asked what to do about HTTP.  The feeling is that it's dangerous to pass unauthenticated HTTP through exit nodes.  Packaging NoScript does not provide the best experience, either.  The easiest solution is to enforce and require HTTPS when in Tor mode.



So this is well in advance of any timeline or any kind of a release number target.  But it's neat that the Tor gurus are thinking along these lines.  As for myself, beyond experimenting with Tor just for the experience, like to play with it in order to talk about it for the podcast, I've never used it seriously because I don't have a big need for persistent anonymity when I'm on the 'Net.  But I for one would add a Tor add-on to my browser if it made it just as simple as clicking a button for those few and brief times when being truly stealthy would be useful, and when I could put up with a much slower browsing experience because it is a lot slower.  But still, very cool that that kind of idea would be forthcoming.



Rust has been in the news recently, the language, because Microsoft recently announced that they were going to be seriously considering Rust as the implementation, the systems implementation language for their projects moving forward, which is to say moving away from C, C++, and even C#, which is their own C.  And along those lines, Rust - and it's hard to pronounce this because we need two T's.  It's Rust-TLS, but it's Rustls, so Rustls, I guess.



JASON:  That seems about right for me, Rustls.



STEVE:  Yeah, Rustls.



JASON:  Or Rustls.



STEVE:  That's, yes, Rustls have ridges.



JASON:  Right.



STEVE:  So it turns out that somebody wrote a TLS stack in Rustls and it outperforms SSL in nearly every way.  It is a tiny, currently, well, it's tiny and a currently relatively unknown TLS library written in Rust.  Benchmarks were performed by its author, Joseph Pirr-Bixton - or, I'm sorry, Birr.  I got the "P" and the "B" backwards.  Joseph Birr-Pixton, who is the author of the Rustls library.  Joseph's finding showed that Rustls - I love saying that, Rustls - was 10% faster than OpenSSL when setting up and negotiating a new server connection, and between 20 and 40% faster when on the other end receiving or initiating setting up a client connection.



And of course these days most TLS traffic relies on resuming previously negotiated handshakes, that is, if the endpoints agree on the material that they had previously established, then they're able to more quickly - they're able to cut out some of the handshake traffic back and forth and just say let's continue using the existing cryptographic material that we had previously exchanged.  So that is resuming existing connections.  And there it's between 30 and 70% quicker to resume a client connection.  Moreover, Rustls fared better in bulk data transfer, as well.  Joseph's measurements show that Rustls could send data 15% faster than OpenSSL and receive it 5% faster.  And just to put a cherry on top of all that good news, Rustls uses only half the memory consumed by OpenSSL.



Now, we've spoken of OpenSSL frequently because it's been a core of the Internet.  It is old, and it's becoming creaky.  And we're beginning to see more and more alternatives appearing.  But it has been the workhorse.  It's where all new SSL-related ideas are first developed and proven, and then over time they evolve.  So it's evolved SSL all along the way, acquiring all of SSL's additional features and being the test bed for them.  And then of course it was used to make the straddle from SSL to TLS, and then to evolve TLS.  And we all know that anyone using any brand new TLS stack should initially do so with an extra supply of suspenders because TLS is a complex protocol, and you don't want to find your pants down around your ankles.  Which is to say it's neat that there is a brand new fast TLS stack written in Rust.  But I would be cautious because, you know, it's just it's hard to do this right the first time.



That said, Rust is probably well suited to this sort of application.  Unlike C and C++, Rust has security designed into the language itself to avoid memory-related security bugs.  Microsoft did a study, and they showed that roughly 70% of all the problems they find in their own code or, embarrassingly, other people find in their own code, 70% are memory allocation, memory management, buffer overrun, buffer underrun type bugs.  And now it's looking like Rust is generally able to outperform code written in C and C++.



Rust was initially developed by Mozilla.  Firefox since 2016 has had Rust in it.  It's been Rusty, and getting more so.  And I remember that it was ridiculed at first, wasn't really taken very seriously.  But as it's proven itself, it's really becoming a serious language.  I think, when I was doing some background research, I think all of the Stack Overflow surveys since 2016, so 2016, '17, '18, and even '19 have all put it as number one most favorite language.  It has a clear, clean syntax.  It's easy to learn.  I mean, people are loving it.  So I think it's a strongly typed language.  So it's got a lot of the features that programmers are liking to help prevent them from making errors.



Firefox and Brave browsers are relying on Rust components, and large companies are adopting Rust.  Cloudflare, Dropbox, Yelp, and NPM have adopted it for their production systems.  The Tor Project is experimenting with Rust.  And even Facebook's recently launched Libra cryptocurrency will be using Rust.  And as I mentioned, last but not least, Microsoft just recently announced their plan to use it.  So, oh, yeah, here I had in my notes, it's come out on top as the most popular programming language in Stack Overflow's developer survey the past five years, although there's only four years - 2016, 2017, 2018, 2019.  So maybe it's going to be the same way in 2020, who knows.



JASON:  Yeah.  Yeah, look at the positive.  



STEVE:  Positive outlook.  For anyone who's interested, it's Rust-lang.org, R-U-S-T hyphen L-A-N-G dot org.  And on that site, which is of course pro-Rust, they say of performance:  "Rust is blazingly fast and memory efficient.  With no runtime or garbage collector, it can power performance-critical services, run on embedded devices, and easily integrate with other languages."  Of reliability:  "Rust's rich type system and ownership model guarantee memory safety and thread safety, and enable you to eliminate many classes of bugs at compile time."  And of productivity:  "Rust has great documentation, a friendly compiler with useful error messages, and topnotch tooling:  an integrated package manager and build tool, smart multi-editor support with auto-completion and type inspections, an auto-formatter, and more."  So anyway, it does look like it's something that's worth looking at.



So I wanted to, first of all, put Joseph's Rustls library on everyone's radar in case there might be some interest and need, and also because computer languages are fun.  As I've mentioned before, I plan to accelerate my own development of SpinRite 7, not 6, not the 6-dot releases, but the 7, because I plan to add a large array of features.  Whereas 6.1, .2, .3 and so forth will be performance-oriented, support-broadening work.  What I plan to do is to develop the low-level driver code for the SpinRite 6.x series, but then to implement the next UI, and the technology underlying the UI, in something that's faster for me to prototype and experiment and work with.  I had been thinking Python for that, but I'll definitely take a look at Rust.  And I know that Leo would be saying, "Steve, look at LISP.  Look at LISP."  But no.  I'm not writing SpinRite 7 in LISP.  But maybe Rust.



I mentioned Microsoft and election security.  I'm excited about this.  Last week during the Aspen Security Forum Microsoft announced ElectionGuard.  Well, they announced and demonstrated.  They had some kiosks running.  It's a new open source technology which can be used to secure modern electronic voting machines.  And thank god it is not the beginning of Microsoft balloting machines based on Windows 10.  No, we're not going to have that.  Microsoft stated that it had no plans to release commercial voting machines.  Actually, this came out of Microsoft Research, which is a different group.  Instead, they said they plan to release the ElectionGuard software on GitHub under an open source license later this year.



They described the technology behind ElectionGuard as being relatively simple and centering around a few core principles, which is exactly the right approach.  And what's interesting is that somehow we've covered this before.  So maybe we talked about it in a very pre-release mode.  But now it's demonstrable.  So here's what we know.  Voters who vote with the machine receive a tracking code from the act of voting.  The tracking code can then be used on an election website to verify that their vote has been counted, and that that vote was not altered.



But the tracking code reveals nothing about the voting, so it will not allow third parties to see who voted for whom.  It uses a form of homomorphic encryption which was developed in-house at Microsoft by their cryptography team.  Homomorphic encryption, difficult to pronounce, is a very cool technology that allows the counting of votes while keeping the votes themselves encrypted.  The ElectionGuard SDK also allows third-party verifier apps to independently check if encrypted votes have been counted properly and not altered.  So there is no single point of authority.  It's inherently a distributable and distributed technology.



The verifier apps were created for voting officials, the media, or any third party interested in the voting process.  So basically it kind of makes it open and auditable, inherently.  ElectionGuard Machines can also produce paper ballots as a printed record of their vote, which voters can place inside voting boxes like old-fashioned votes.  And it supports voting through accessibility hardware such as Microsoft's Surface, or they give the example of the Xbox Adaptive Controller.



So it's already attracted interest, as it should have because it's the right solution.  Microsoft hopes voting machine makers will use its new ElectionGuard software for their own products.  And according to Microsoft, the SDK has been warmly welcomed by some voting machine vendors already.  Tom Burt, who is Microsoft's corporate vice president in charge of customer security and trust, said:  "We previously announced that we have partnerships with suppliers that build and sell more than half of the voting machines used in the United States.  Today we're excited to announce that we're also now partnering with Smartmatic and Clear Ballot, two of the leading voting technology vendors; and Dominion Voting Systems is actively exploring the inclusion of ElectionGuard in their offerings."



And from my standpoint this is exactly the correct direction for our voting systems.  As I said before, it's fine for the likes of Diebold to manufacture and sell the hardware.  But the way their machines work and what they do cannot be allowed to remain proprietary secrets.  That's just ridiculous, the idea that voting machines are proprietary, and for there to be a "just trust us" model.  As we know, they are famously riddled with vulnerabilities.  And they don't begin to provide this next-generation style of really leverageable crypto that the ElectionGuard technology offers.



So the entire notion that some random hardware manufacturer has some kind of secret sauce to protect our votes is utter nonsense.  And it will take someone with a solid reputation, just like Microsoft Research, to offer an answer at no cost to the hardware vendors.  And it being open inherently means all the academicians can attack it and see if they can find any problems.  So I just say a big yay to this.  It's really good news. 



JASON:  Yeah, makes a lot of sense.



STEVE:  And of course now what we need, once the new machines exist, we need the old ones to be taken out of service because I've seen this massive...



JASON:  Right.  And that'll happen really fast.



STEVE:  Yeah.  Well, it'll happen, especially if governments are required by law to have machines that support ElectionGuard technology.  That would be perfect.



JASON:  Absolutely.



STEVE:  And we always, you know, what could almost be a weekly installment of Security Now!, we have yet another remotely exploitable vulnerability of a widely used, high-profile, and easily discoverable Internet server.  Yes, more than one million currently online instances of the popular open source ProFTPD server, which is present in distributions of Debian, SUSE, Ubuntu, and many other Linux and Unix-like systems, is open to remote compromise.  And that is to say has always been so, including remote code execution as a result of an arbitrary file copy vulnerability.



What's on the screen right now is the result of a Shodan search for ProFTPD daemons.  This shows nearly 203,000 of them in the U.S., 182,000 in Germany, almost 90,000 in France, 55,000 in the Russian federation, 48,000 in the Netherlands, 47,000 in Japan, 41,000 in the U.K., 35,000 in Spain, almost 28,000 in Hong Kong, and 24,000 in Italy, totaling more than a million.  All ProFTPD versions up to and including the latest, 1.3.6, which was released back in 2017, are currently vulnerable unless their default configuration has been modified, and we know what that means.  They're all currently vulnerable.



The problem is ProFTPD has a mod_copy module which implements additional commands which any anonymous FTP client can issue.  The ProFTPD site explains:  "The mod_copy module implements SITE CPFR and SITE CPTO" - that's copy from, copy to - "commands which can be used to copy files/directories from one place to another on the server without having to transfer the data to the client and back."  Okay.  So the ProFTPD developers, they said, hey, let's enhance FTP.  Rather than requiring you to download a file and then upload it somewhere else, let's just allow you to give some commands to cause the FTP server to perform essentially an intra-site file or directory copy.



So they are anonymous.  That is, the SITE CPFR and SITE CPTO can be used by an anonymous, non-authenticated FTP client which connects to any ProFTPD server and can simply arrange to copy any file that's there to somewhere else.  Okay.  And of course anybody who's used FTP can immediately begin to see the problems.  There is a bug report on the ProFTPD.org site which says a ProFTPD user reported the following via email:  "The mod_copy module's custom SITE CPFR and SITE CPTO commands do not honor the <Limit READ> and <Limit WRITE> configurations as expected.  To reproduce, just enable the anonymous user example that is preconfigured in the Debian default proftpd.conf."  And then they show us a sort of a standard Linux-style, Unix-style configuration where you have an anonymous FTP user block opened, and then you define the user, the group, the user alias, max clients, the login message to display and so forth.



And then within that anonymous definition you have <Directory *>, meaning for all directories, <Limit WRITE>.  And then in there is DenyAll.  Then you close the limit, then you close the directory, and you close the anonymous specification.  So that's your standard.  So that's saying do not allow writing to any directories.  Then in this ongoing bug report they say issue the command ftp space and then whatever, you know, proftpdtest.domain.org as an example, meaning just connect to the server from any client.  Login as anonymous, and then you issue the site space cpfr.  And then in the example they gave, space welcome.msg, the welcome message.  Then you say site cpto malicious.php, which has the effect of copying whatever was in the welcome.msg file into malicious.php.



And of course that example is not going to be particularly useful, but they're not going to want to put an actively useful malicious example in their bug report.  This just demonstrates the problem.  But as many FTP systems are set up, and many people know, it's often the case that FTP servers will designate, for example, an uploads directory where anonymous people can submit stuff.  They can upload things to the uploads directory.  They can't put them anywhere else, and they can't do anything with them.  But it's like, you know, upload the file to the uploads directory.  Then you contact the site admin and say, hey, I just sent you a sample of something, might be malware, might be a virus, might be whatever.  Go do with it whatever you want to.



The point is now the bad guys upload something to the uploads directory.  Then they issue these two commands to copy the thing they uploaded that they should have absolutely no other access to except uploading it to that directory.  They now copy it wherever they want to on the server.  So it's very clear that this is a very bad problem.  And if any of our listeners do have a ProFTPD server up and running, you want to immediately, and that's the remediation, immediately disable that mod_copy module.  That mod_copy module was the add-on which adds these two commands that no one's probably using anyway.



So it's another example of this should never have been turned on by default and left on by default.  One wonders why it was even written as an extension, but there it is.  So make sure you're not using it.  That would be - you're going to be much happier not to have people basically replacing files on your server, anywhere on your server at their whim, based on FTP.  Yikes.



I have a note from Patrick Delahanty, who is a web engineer.



JASON:  Who?  Who?



STEVE:  People have probably heard his name.



JASON:  Dela-what-hanty?



STEVE:  Uh-huh.



JASON:  Oh, okay, TWiT.  Oh, so he works here.  He's one of the hundreds of people walking past me in the hall on the way to the studio.



STEVE:  One of those many people who keeps the lights on, yes. 



JASON:  And he's about to come in here and glare.  Okay, so he's glaring at me right now.  I knew that was going to - literally, his office is right on the other side of the studio door.  So hi.



STEVE:  I got email from Patrick this morning.  He said:  "Hi, Steve.  I've been making some simple Apple TV apps for some TWiT shows that have evergreen content people like to watch even after the content has been rotated out of our podcast feeds.  I've released a Security Now! app for Apple TV that uses the TWiT API to access content and lets people watch or listen to every episode" - yes, all 724 - "going all the way back to the beginning."



JASON:  Nice.



STEVE:  So for everyone who's listening:  http://twit.to/sntv for Security Now! television.  And Patrick, thank you very much for doing that and for letting me know so I can let all of our listeners know.



JASON:  Looks really good.  Yeah, looks awesome.



STEVE:  Very cool.  So I have some SQRL news.



JASON:  Nice.



STEVE:  Yeah.  Scott White has sqrl-ssp working, which is a very nice, pluggable implementation of the SQRL service provider API, which he has written in Go.  And I know from interacting with him over in the SQRL forums that he developed it specifically to be very horizontally scalable.  My own implementation is a sort of a single-server approach.  And so we've been going back and forth.  And what he's got written in Go up on GitHub is highly scalable for much bigger installations.



Paul Farrer has a reference implementation of the SSP API in C, which he closely translated from mine, which I provided to him.  I gave him the source, which is in assembly language, because Paul's also comfortable with assembler.  And so it's a feature-complete implementation and supports several different TLS stacks.



Daniel Persson, who created the Android client and the WordPress plugin, both which then evolved into very effective team efforts, is now working on a sqrl-libpam module, PAM of course being a Pluggable Authentication Module for Linux.  So that's on the way.



Adam Lenda, working with PHP and SQRL, is working to add SQRL support to the PHP Symfony application development framework.  So that's in the works.  Jurgen Haas has SQRL for Drupal 8 coming.  And, meanwhile, the Android, iOS, and web browser extensions are rapidly nearing completion.  Those are all clients.



I finished the second of the four SQRL documents to fully specify SQRL.  As we know, the first one was "SQRL Explained," and the next one is titled "SQRL Operating Details."  Before making it widely available, and it's not quite yet, I notified those in GRC's newsgroup, the NNTP newsgroup, who have passed a very fine-tooth comb through it, finding many typos, and also noting grammatical mistakes.  Thank you very much, all.  So I will be revisiting the document to incorporate all of their suggestions as soon as I'm through with the podcast.  And I'll post it widely as soon as that's ready, so I think in a day or two.



And I'm now at work on the third document, "SQRL Cryptography," which will detail all of the crypto glue that holds SQRL together.  And then the final document will be "SQRL on the Wire," which will document the communications protocol down at the character level.  When these are finished, we'll have both a top-down and a bottom-up view of SQRL.  So I am very excited to get the documentation wrapped up.



And last, but certainly not least, I'm very excited to have been invited to present SQRL in late September to two OWASP groups which will be meeting in Dublin, Ireland and Gothenburg, Sweden.  Both have graciously offered to cover the direct travel and lodging costs for Lorrie and me.  Lorrie would never forgive me if I went without her.



JASON:  Of course.



STEVE:  So I'm very much looking forward, yeah, very much looking forward to that two months from now.  I will be at the OWASP meeting in Dublin, Ireland on September 24th, and then two days later at the OWASP meeting in Gothenburg, Sweden on September 26th.  They, of course, have Security Now! listeners.  And if they have room, I'd love to meet more of our listeners at those meetings.  So anyone who's interested should contact the organizers of those OWASP chapters and inquire about attending.  It would be fun to meet more people and shake hands and so forth.



JASON:  That sounds like a blast.  



STEVE:  Lots happening, yeah.



JASON:  Have some fun.  That's great.



STEVE:  Very happy with the way things are developing and rolling out.



JASON:  Absolutely.



STEVE:  So Sophos titled their research "RDP Exposed:  The Wolves Already at Your Door."  And for anyone who is interested, you haven't heard this before, Jason, but I announced GRC's link shortener a week or two ago, and I just talked about it in fact in the context of http://, and I saw that you brought up the nonsecure page.  I used it to shorten the very long Sophos link to the PDF of their report.  So for anyone who's interested, it's got just this podcast's number.  So, well, actually sn724.  So grc.sc/sn724.  That will bounce you to the Sophos PDF titled "RDP Exposed:  The Wolves Already at Your Door."  And it's much longer.  I'm summarizing it.



So they wrote in their summary:  "For the last two months, the infosec world has been waiting to see if and when criminals will successfully exploit CVE-2019-0708."  And of course we know that well.  That's the BlueKeep vulnerability.  They say:  "The remote, wormable vulnerability in Microsoft's RDP (Remote Desktop Protocol), better known as BlueKeep."  They write:  "The expectation is that sooner or later a BlueKeep exploit will be used to power some self-replicating malware that spreads around the world and through the networks it penetrates in a flash using vulnerable RDP servers."



Now, we've talked about this a lot.  I'll break from reading their summary for a second just to note that we haven't seen a worm.  We've seen lots of exploits and proof of concepts.  It's now been a couple months.  And I've always thought that where it made sense for a worm to be created, like with the Exim email server vulnerability, and sure enough we got a worm pretty quickly because it took a week of connection to one of those servers in order to exploit that vulnerability, whereas with BlueKeep you can get right in with no authentication.  And since Shodan will find them all for anybody, it's just a matter of, I mean, it's like easy pickings.  You just choose one, you connect to it, and you set up cryptomining, cryptocurrency mining malware, and maybe close the door behind you so nobody else can kick your miner out.  And now you start at mining Monero.  So it just didn't seem wormable.  Maybe mass exploitable, but not via a worm.  And so far we haven't seen that.  But anyway, RDP is the issue.



They write:  "In other words, everyone is expecting something spectacular in the worst possible way.  But while companies race to ensure they're patched" - and of course we know how that's gone, not - "criminals around the world are already abusing RDP successfully every day in a different, no less devastating, but much less spectacular way.  Many" - now, here's the key.  "Many of the millions of RDP servers connected to the Internet are protected by no more than a username and password, and many of those passwords are bad enough to be guessed, with a little - a sometimes very little - persistence.  Correctly guess a password on one of those millions of computers and you're into somebody's network.  It isn't a new technique, and it sounds almost too simple to work, yet it's popular enough to support criminal markets selling both stolen RDP credentials and compromised computers.  The technique is so successful that the criminals crippling city administrations, hospitals, utilities, and enterprises with targeted ransomware attacks and demanding five- or six-figure ransoms seem to like nothing more."



They have a chart which I copied into the show notes here showing the SamSam ransomware from 2015, no longer active, but its infection vector was RDP.  Dharma, which we'll remember talking about in 2016, still active, using RDP as its infection vector.  The Matrix ransomware, also 2016, still active, using RDP.  BitPaymer, 2017, still active, using RDP.  And we've been talking about it a lot, Ryuk, which has recently been the ransomware that infected all of the various municipalities, a bunch of them in Florida and the Georgia court system, using RDP.  So we've wondered how they're getting in.  Is it phishing?  We know in one instance that it was tracked down to a phishing link.  But in some cases the IT people are unable to say how.  Well, if those networks had RDP exposed, and they had not patched - so remember, either dumb username and password, easy to get in, or you did not patch it after Microsoft said you must, then that's apparently one of the ways this ransomware is getting into people's networks.



So they continue:  "All of which might make you think there must be a lot of RDP password guessing going on."  Now, that would apply to patched RDP servers, remember.  No need to guess with any of those millions that have not yet been patched.  However, many more are patched.



They said:  "Noting the popularity of RDP password guessing in targeted ransomware attacks, Sophos's Matt Boddy and Ben Jones and I set out to quickly measure" - "I" meaning the author of this - "set out to quickly measure how quickly an RDP-enabled computer would be discovered, and how many password guessing attacks it would have to deal with every day.  We set up 10 geographically dispersed RDP honeypots and sat back to observe.  One month and over four million password guesses later we switched off the honeypots, just as CVE-2019-0708 was announced."



So that was four million guesses against 10 honeypot RDP servers in one month before the announcement that there was a problem with them.  On the other hand, as I said, once you know there's a problem, and once you know what the problem is, you don't need to guess anymore.



They said:  "The low interaction honeypots were Windows machines in a default configuration, hosted on Amazon's AWS cloud infrastructure.  They were set up to log login attempts while ensuring attackers could never get in, giving us an unhindered view of how many attackers came knocking, and for how long, and how their tactics evolved over the 30-day research period."  The first honeypot to be discovered was found one minute and 24 seconds after it went live.



JASON:  Wow.  Wasting no time.



STEVE:  Yikes.  One minute and 24 seconds after it went live.  The last was found in just a little over 15 hours.  So they have a chart here, and I copied it into the show notes, the time to first login attempt by honeypot.  And so it happened that the one that was found took a minute and 24 seconds was in Paris.  It took seven minutes and five seconds in Sao Paolo; 21 minutes and 29 seconds in Frankfurt; a little over half an hour, 35 minutes and 25 seconds, in Ohio.  California took two hours and 38 minutes to be found.  And then the chart goes up.  Singapore was the last one, about 15.5 hours for the first login attempt at a honeypot in Singapore.  So again, that's unscientific.  It doesn't represent a big sample size, but still it's interesting.



Then between them the honeypots received 4.3 million login attempts at a rate steadily increasing through the 30-day research period as new attackers joined the melee.  So this is login attempts per day.  We have a chart.  The vertical scale is zero to, looks like it comes in at about 210,000 attacks per day.  And it is clearly on the rise.  So it looks like, once discovered, then the IP address of the server begins to spread or is being discovered by an increasing number of attackers.  So there's a feeding frenzy out there on RDP servers, and this is what their data shows, to the point where they're clearly getting about 205,000 login attempts per day at the peak.



They said:  "While the majority of attacks were quick and simple attempts to dig out an administrator password with a very short password list, some attackers employed more sophisticated tactics.  The researchers classified three different password guessing techniques used by some of the more persistent attackers."  And you could read more about them - they called them the Ram, the Swarm, and the Hedgehog - in the whitepaper.



So, they conclude, what to do?  They say:  "RDP password guessing should not be a problem."  And of course I agree because you should never, as I have said, have an RDP server exposed.  But they said:  "It isn't new, and it isn't particularly sophisticated, and yet it underpins an entire criminal ecosystem."  Once again, RDP password guessing underpins an entire criminal ecosystem.



They said:  "In theory, all it takes to solve the RDP problem is for all users to avoid really bad passwords.  But the evidence is they won't, and it isn't reasonable to expect they will.  The number of RDP servers vulnerable to brute force attacks isn't going to be reduced by a sudden and dramatic improvement in users' password choices, so it's up to sysadmins to fix the problem."



And I suppose some RDP servers are not for sysadmin remote access.  They're for random enterprise users to log in when you're on the road.  And so they've probably deliberately dumbed-down the password, mistakenly thinking that it's okay to do so in order for their less security-conscious, more casual users to be able to log in.  That's the wrong strategy.



They finish:  "While there are a number of things that administrators can do to harden RDP servers, most notably two-factor authentication, the best protection against the dual threat of password guessing and vulnerabilities like BlueKeep is simply to take RDP off the Internet.  Switch off RDP where it isn't absolutely necessary, or make it accessible only via a VPN, a Virtual Private Network, if it is."



And of course that's what I've been saying now from the beginning is, I mean, it is so simple to put your RDP server behind a VPN.  It will not be visible.  It will not be seen.  VPNs can be secured in a way that doesn't inconvenience users, where you use certificates, and the VPN client in your user's laptop has the certificate that the RDP server must see and vice versa.  That then establishes a connection to the RDP server, at which point the user logs in.



It's just, you know, RDP has been a troubled protocol.  BlueKeep is just the latest example of a catastrophe with it.  Anybody who understood that it was never safe to have RDP publicly exposed had nothing to worry about from BlueKeep.  They could take their time patching it because it didn't matter.  Really, just it's so simple these days to bring up an OpenVPN server.  I mean, our routers all have them built into them now.  So really give that some thought.  And that's our show.



JASON:  That's awesome.  I just want to remind viewers and listeners, where can they go to read up on that?



STEVE:  Ah, right, grc.sc/sn724.



JASON:  That's going to be in the show notes, and also have a direct link to the whitepaper that you were talking about.  They have all that data listed inside that.  That is really interesting.  Doesn't take very long, does it.  Like a swarm of bees. 



STEVE:  Oh, my goodness.  A minute and a half, wow.



JASON:  Yeah, that is impressive.  This is awesome, Steve.  I want to, real quick before we end the show, call out that we have some guests who have joined us in the studio to watch the show live.  Shelly and Cody are sitting right there on the left, and then we have Andrew...



STEVE:  Hey, guys.



JASON:  ...on the right.  And they've been sitting and enjoying the show throughout.  So it's always fun when we have people visiting in the studio, and it's really great to have you guys here.  Thank you for coming in.  Anyone can, by the way.  Tickets@twit.tv, if you email there, we can get you set up and then put out a chair for you in the studio while we record Security Now! and any other show that we do on the TWiT network.



But as for Steve, go to GRC.com.  Everything you need to know that Steve's involved with, is doing, busy with every single day can be found there, GRC.com.  SpinRite, of course, the best hard drive recovery and maintenance tool.  You can get your copy there.  Information, you talked about SQRL today, and that's super exciting.  It feels like there's momentum right there.  You can find more information about that at GRC.com.



Audio of the show, of course, can be found on there also.  Transcripts of the show can be found there, as well.  And again, that's GRC.com.  If you want to go to our website, it's TWiT.tv/sn.  There you can find all the links to our audio and video podcasts.  The times of the show, which we actually record Security Now! live every Tuesday starting at 1:30 p.m. Pacific, 4:30 p.m. Eastern.  I know it's probably out of date, but 20:30 UTC, does that sound about right, right now?



STEVE:  Sounds familiar, yeah.



JASON:  Okay, all right.  I forgot to check it before the show to be sure that there hasn't been some sort of a time switch from the last time I wrote that down.  But anyway, somewhere around there.  I guess do a little homework, and you won't miss the show.  Steve, thanks a lot for allowing me to tag along.  I really appreciate it.



STEVE:  Oh, allowing you?  It's been a pleasure, Jason.  Thanks so much for helping out.



JASON:  All right.  We'll see you all next week on another episode of Security Now!.  Bye, everybody.



STEVE:  Bye.



Copyright (c) 2019 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#725

DATE:		July 30, 2019

TITLE:		Urgent/11

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-725.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we close the chapter on the Marcus Hutchins saga.  The U.S. Attorney General weighs in on "warrant-proof" data encryption.  We look at what's popular with the underground, give an update on the latest four new ransomware attacks, examine three different attacks on exposed network attached storage (NAS) servers, cover a bit of miscellany, then take a close look at the news of the just-released-yesterday vulnerabilities in the two billion-strong VxWorks embedded OS. 



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  Marcus Hutchins gets his sentence, a terrible show on Netflix, and how software running on over two billion devices is vulnerable to hacking.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 725, recorded Tuesday, July 30th, 2019:  Urgent/11.



It's time for Security Now! the show where we cover your security - I'm saying it like a baseball announcer.  And in left field, with his giant mug o' Joe, Steve Gibson.  Hello, Steve.



STEVE GIBSON:  Yo, Leo.  It's great to be with you.  We are, as I have said, we are closing in on the end of our 14th year.



LEO:  Holy moly.



STEVE:  Next month by this time we'll be into year 15.  And boy, are we not running out of things to talk about.



LEO:  Things aren't getting any better, are they.



STEVE:  Oh, my god.  So this is Episode 725.  I was going to call this, I was all set, everything was like - the show notes were written.  The outlines were put together.  The title of this was going to be "Your NAS Is Grass."



LEO:  I like it.



STEVE:  And I did, too.  Until yesterday's shocking announcement of 11 serious flaws in an embedded OS that no one has even probably ever heard of before called VxWorks.  And VxWorks has been sort of like the secret weapon of embedded designers for the last 32 years.  There was something, there was Charles River Systems, and then Wind River Systems.  Oh, and Ready Systems was the original source.



LEO:  Yeah, I remember all those.  Yeah, yeah.



STEVE:  Way back, more than three decades ago.  Unfortunately, it's closed source, so it's never been subject to anyone scrutinizing it.  And it turns out that it is everywhere.  I mean, it's almost laughably everywhere.  It's what the Mars Rover runs on.



LEO:  Wow.



STEVE:  So, yeah.  And it turns out that, so, like, there's two billion installs.  It's what IoT devices are all using.  Those that aren't using a little trimmed down Linux kernel are pretty much using VxWorks.



LEO:  It's funny because I just don't know that name.  Wind River I know absolutely.



STEVE:  Right.



LEO:  So they must have gone by that for a long time.



STEVE:  Yes, exactly.



LEO:  Yeah.



STEVE:  So anyway, unfortunately, "Your NAS Is Grass" ended up becoming a subtopic of the show rather than our title, which is "URGENT/11," which is the formal title given by Armis, Inc., who reverse engineered, they got old, like, obsolete source to kind of give them a start.  And then they had to reverse engineer the binary of installed in-place VxWorks in order to take a look at it.  And what they found was horrifying.  It turns out that all of these devices that are connected to the Internet - the good news is the Rover is not on the 'Net.  But, you know, surprising things are, like SCADA systems that are controlling all of our nuclear reactors.



LEO:  Great.



STEVE:  I mean, that's all VxWorks.  And everybody's car has VxWorks in it.  So anyway, that took over the podcast, and we will be talking about that in detail.  We're going to close the chapter, happily, I mean, a happy ending on Marcus Hutchins, that whole two-year saga we've been following since he was nabbed when leaving Las Vegas.  We have our U.S. Attorney General weighing in on warrant-proof data encryption and how horrible that is.  We look at what's popular with the underground, in the underground of the Internet.



We'll update on the latest four new ransomware attacks which have occurred since last week.  We'll examine three different attacks on exposed NAS servers that, as I said, were going to be the main topic, but that got supplanted by VxWorks.  We'll discuss a little bit of miscellany and then take a close look at this news of these just-released-yesterday vulnerabilities in two billion installs of devices using VxWorks, for example, all - well, not all, but most SonicWall firewalls are using VxWorks.  And so, I mean, it's designed to be on the Internet, which unfortunately is not where you want it to be right now.  So I think another great podcast for our listeners.



LEO:  Sounds fantastic, as always.



STEVE:  So our Picture of the Week I got a kick out of.  Someone tweeted it to me a couple weeks ago, and now we have a slot open for it.  It's a cartoon with four frames.  First frame shows a guy saying, "Thanks to Windows Subsystem for Linux," second frame, "I can have the workflow of Linux in Windows."  Then the third frame shows the blue screen, "Working on updates.  11% complete.  Don't turn off your computer."  And of course, you know, with the rollercoaster dots spinning around.  And the fourth frame has this creature looking at us again saying, "Marvelous."  So, yeah.  Not quite the entire Linux experience.  Thank you, Windows.



LEO:  It does make you wonder, why bother?  Just put Linux on your machine.



STEVE:  Oh, my goodness, yes.  Why?



LEO:  Why do you want to wrap it with Windows?  Yeah.



STEVE:  Yeah.  So Marcus Hutchins is free.  Free at last.  It was two years ago this coming DEF CON.  After the annual 2017 DEF CON security conference, as Marcus was - I guess he described himself as still sort of suffering, he was hung over and intoxicated from partying the night before.  He was walking through the Las Vegas airport... 



LEO:  That's when they get you.



STEVE:  Yeah.  You have your guard down.



LEO:  Yeah.



STEVE:  Preparing to return home to the U.K.  He was nabbed by law enforcement.  And of course he had been for many years a reformed gray or black hat, I guess.  I don't think he ever actually participated in malicious hacking, but he did write some, in his youth, as an adolescent, he wrote some malware.  Anyway, he was nabbed and held on charges of computer hacking, fraud, and abuse.  It wasn't long before he was released on bail from jail, but I think he had an ankle tracker.



So he was working with attorneys, and we've talked about him several times over the last couple years as his case has moved through the legal system.  And he's been working with his defense team and appearing at various legal hearings from time to time.  We know that he had outgrown those earlier, you know, his sort of sketchy adolescence where he was doing some things that were not on the up-and-up.  And let us not forget that what initially brought Steve Jobs and Steve Wozniak together was their interest in what was known as "blue boxing," which were these little boxes which generated different sets of tones than the normal touchtone tones, which were used to sidestep the long-distance telephone billing that the U.S. telephone system used for making free long-distance phone calls.  So, you know, even our contemporary tech heroes were not always on the up-and up.



Anyway, shortly before his arrest, he was studying, that is, Marcus was studying the alarming emergence and rapid propagation of the incredibly prolific and dangerous WannaCry Internet worm.  And in his research he serendipitously halted its progress, and we've talked about this many times, by looking inside the code, seeing that there was kind of an odd DNS query that the worm was making.  When he looked up what the DNS domain was that it was querying, he found it was unregistered.  He thought, okay, that's weird, I wonder what it's doing with that DNS address?  So he registered it.  And in the process of it no longer being unregistered, just that act halted the worm's propagation.  And we never really understood in detail, like we never had a clear understanding of why.  It's believed that that was a kill switch, which Marcus, as I said, serendipitously set in order to cause the worm to stop propagating.



And it turns out that this was taken into account.  U.S. District Judge Joseph Peter Stadtmueller said that the malware Hutchins helped stop was much more damaging than the two programs he created, and thus sentenced him only to time served, with a year of supervised release.  So Marcus is free to return to the U.K.  He tweeted on July 26, which was last Friday, he tweeted first:  "Heading into court now.  No matter what happens, I love y'all."  And then a short time later he tweeted:  "Sentenced to time served!  Incredibly thankful for the understanding and leniency of the judge, the wonderful character letter you all sent, and everyone who helped me through the past two years, both financially and emotionally."  That was at 11:25 a.m. last Friday.



So he's free to return to the U.K.  U.S. authorities still need to decide whether he's now barred from returning to the U.S. due to his "criminal record."  And of course in my opinion barring him would be really dumb.  But on the other hand, we did say "U.S. authorities."  So, you know, we'll see what happens.  So that's good news, a nice outcome.  And it would be nice if he were free to return.  His presentation at Black Hat or DEF CON was about what he had just done with WannaCry, and so after demonstrating, I mean, he had been doing other white hat hacking things.  As he know, his Twitter handle was MalwareTechBlog.  And so, you know, he was blogging about how to stop this stuff now.  And anyway, so this was the right outcome.  And it would be nice if we let him come back to future Black Hat and DEF CON presentations.



And Leo, I have a two-minute YouTube video of U.S. Attorney General William Barr on warrant-proof data encryption.



LEO:  Oh, is this the speech he was giving?  Oh.



STEVE:  Yes.  This was two minutes.  So I thought it would be worthwhile to play this into the podcast since this is the...



LEO:  Go ahead.  I'm sorry.



STEVE:  Yeah, no.  Let's just go ahead and play it.



LEO:  All right.



[YouTube clip]



WILLIAM BARR:  The deployment of warrant-proof encryption is already imposing huge costs on society.  It seriously degrades the ability of law enforcement to detect and prevent crime before it occurs.  And after crimes are committed, it is thwarting law enforcement's ability to identify those responsible or to successfully prosecute the guilty parties.  These costs will grow exponentially as deployment of warrant-proof encryption accelerates, and criminals are emboldened by their ability to evade detection.  Converting the Internet and communications platforms into a law-free zone, and thus giving criminals the means to operate free of lawful scrutiny, will inevitably propel an expansion of criminal activity.  If you remove any possibility that the cops are going to be watching a neighborhood, the criminals already in the neighborhood are going to commit a lot more crimes.



But there has been enough dogmatic pronouncements that lawful access simply cannot be done.  It can be, and it must be.  We are confident that there are technical solutions that will allow lawful access to encrypted data and communications by law enforcement without materially weakening the security provided by encryption.  But I am suggesting that it is well past time for some in the tech community to abandon the posture that a technical solution is not worth exploring and instead turn their considerable talent to developing products that will reconcile good cybersecurity to the imperative of public safety and national security.



[End clip]



LEO:  That seems somewhat similar to your point of view.  Yes?



STEVE:  Well, the devil is in the details, of course.  And my feeling is that, in some of these ecosystems, it's possible for us to do some of it.  For example, as I was just saying recently, right now Apple is managing the encryption keys for iMessage.  So it would be possible to have a hidden participant in iMessage conversations that in no way does it weaken the system except that, yes, you are making a conversation available to a third party.  But it's still entirely under Apple's control, as it is now.



But of course, if you try to broaden this too far, for example, if the government were to say we need the ability to decrypt any TLS connection, now that's a horse of a different color.  That's a whole different problem where they're saying, you know, we need, I mean, that really would then be some form of a backdoor if you said, you know, we need to be able to get into any TLS connection between a web browser and a server.  That's a big different problem.



So, I mean, it really is the case, if I were to play devil's advocate to myself, it really is the case that the technology we have developed is secure point-to-point encryption that we've developed this technology, we the industry, over decades, without the assumption that a backdoor is going to be required, and the technology doesn't allow it.  So the reason this is consistent with my position on iMessage is that iMessage is a layer above.  It's still supporting encrypted point-to-point, but the points are aggregated, and messages are being reencrypted by Apple in order to send them to other devices.



So it's not at all clear how you would implement the ability - which is what Bill Barr is saying.  He's saying that, essentially, they want to be able to decrypt everything; that they don't want the Internet to be a, quote, "law-free zone."  So I'm 100% onboard with the position that none of our fundamental encryption algorithms support this today.  They don't.  I mean, they don't.  So if it would be acceptable to encrypt, to somehow make the data available outside of the encrypted connection, which is what iMessage does, and which is what I've been talking about, then we know how to do that.  But we don't know how to make all of our current point-to-point encryption accessible to a third party.  It isn't currently.  It just isn't, fundamentally.



So anyway, I thought this was interesting because, if a compromise can be reached, then I think a compromise is technically feasible.  But if something like the government demands to be able to decrypt everything is what they're asking for, that really can't be done.  I mean, that really requires going back to the drawing board.  I mean, there are really advanced, much different schemes for encryption, sort of like at the next level, things like homomorphic encryption where, I mean, it's a whole 'nother generation of encryption which isn't currently in use and deployed.  And, I mean, it really does send us back to the drawing board from where we are today.



LEO:  I feel like the real issue, and the thing that they're going to end up going for, look, it's too late to prevent VeraCrypt and other strong encryption technologies from existing.  You can't stop that.



STEVE:  It's out of the bag; right.



LEO:  It's out of the bag.



STEVE:  It's math.



LEO:  So they know that.  So what this really is, is to get Samsung and Apple and all the other phone manufacturers who ship with encryption to provide backdoors.  And they'll be happy with that.  And any criminal who wants to use Signal can continue to use Signal because you can't stop Signal.  It's done.  So you're not going to be able to go to Moxie Marlinspike and say could you please put in a backdoor.  He's going to laugh at you.



STEVE:  Although the API that Signal uses is the OS's API.  And so...



LEO:  Ah, yes.  So that's why I'm saying, if you can get the phone, you're going to get 90% of what you want anyway. 



STEVE:  Right.  And you can grab, you can tap the keyboard and the screen before and after Signal does its encryption and its decryption.



LEO:  So if you want privacy, you're going to use open source software.  You're going to use an open source operating system.  You're going to use existing crypto tools, open source.  And you can...



STEVE:  Actually, if you want privacy, you're not going to use a smartphone.  



LEO:  Just don't use a smartphone.  There are coming FOS smartphones.  There's a Firefox phone.  Librem is making a phone.  And I think that's really what they're aiming at is that day in the not-too-distant future when no commercial device will ship with strong encryption, or foolproof encryption.



STEVE:  Or without the ability to be served a warrant and have its contents available.



LEO:  But despite what AG Barr said, we know that, if you put a backdoor in devices like that, you can't - I don't think you can keep it to law enforcement only.  A backdoor, any extant backdoor, even if requires a warrant, well, you've talked about ways maybe a company like Apple could have secret keys that they keep in a vault kind of thing.



STEVE:  Yeah, yeah, I mean, again, it's easy to use Apple as an example because they've built such a sophisticated, controlled environment.  So right now Apple goes to great lengths to explain how they do not have the key to the secure enclave, how the device generates it and never sends it.  Well, that's their choice.  They've chosen that.  But if the device, over a secure connection, after generating its secure enclave key, I mean, and again, Apple could do this if they wanted to.  If it shared that...



LEO:  Or are forced to by law.



STEVE:  Yes, or are forced to.



LEO:  Because I don't think they'll ever want to.  But if they're forced to.



STEVE:  Right, right, right.  So, and I meant "want" from a technical standpoint.  They could receive that secure enclave key.  So the counter argument is that having Apple, some Apple storage of all of the enclave keys for all of their devices is a weakness.  And there's no argument to that.  Right now they don't have, you know, you can't lose what you don't have.  No one can hack what you don't have.  So they don't want to have the secure enclave keys.  They really, really, really don't.  And notice that their security model, their privacy model, their sales model is we don't have those keys.



LEO:  Yeah.  But can I point out they do have the keys to iCloud, and they do encourage everybody to back up everything to iCloud?



STEVE:  Yeah, right. 



LEO:  And they even told the FBI, well, if you'd only let the phone back up to iCloud, we could have given you everything.



STEVE:  Yup, yup.



LEO:  So honestly, Apple, well, I don't know what Apple really thinks.  But it seems to me more a marketing strategy than anything else.



STEVE:  Yeah.



LEO:  And you're right.  If you wanted to stay private from the government for whatever reason, and by the way I think increasingly it becomes clear it isn't merely because you're unlawful or a bad person, you merely could be a dissident, and you want to keep your stuff private, you won't use a commercial  device of any kind, computer or phone.  Period.  You can't because those companies will be compelled by law, by the law of the country they're in, whether it's the U.S. or China, to provide backdoor access to law enforcement.



STEVE:  As we've said, if you really, really, really want private communication, you meet with someone naked in the middle of Central Park, throw a blanket over you so that your lips cannot be read, and you very - oh, and you use - I don't know if paper and pencil would be safe either.  Maybe just whisper.



LEO:  Don't write anything down, that's for sure.



STEVE:  Right, right.



LEO:  And by the way, they now have satellites that can read that pencil and paper over your shoulder.  So maybe cover your mouth, too, while you're talking.



STEVE:  Yeah.  I mean, it's just - yeah.



LEO:  It's pretty hard to defeat somebody with the resources of a government.



STEVE:  Yes.  And the problem is that this technology is so convenient that it's what gets used.  It's like, if you really, really, really want to be secure, you have to be physically in the same location and whisper into somebody else's ear underneath a blanket.  Otherwise, as soon as you start wanting convenience, you sacrifice true security.  You know, it's easy, easy to have the appearance of security, for everyone to feel happy and, like, oh, yeah, we've got encryption.  But it's like, okay, great.  You also have malware on your phone that's sending everything that is encrypted, before it gets encrypted, somewhere else.  So, yeah.



LEO:  I think it's prudent to start preparing ourselves and thinking about what it means to be private, how we would achieve that privacy, how we would know if it wasn't private, what things to look for, all of that.



STEVE:  Well, and really, as we also said, Leo, does it really matter?  Does anyone care?



LEO:  It doesn't matter now.  I'm not convinced it's not going to matter in the next five years.



STEVE:  Yeah.



LEO:  I'm not completely sanguine about the future of liberty in this world.



STEVE:  And there are places where we really do want strength, like our voting systems.  We would like our voting systems to be immune from the fact that we just learned that Russia was actively trying to not hack just one or two locations, but all 50 states during the 2016 elections.



LEO:  Which is why I really question the motivation of our government when they say, "And we want a backdoor."  Because they've got to understand that that makes us vulnerable to everyone.



STEVE:  Yeah.  It will be, I mean, again, I can't think of a more interesting issue like for right now where we are.  And, boy, I hope this gets resolved by podcast 999 because, you know, otherwise...



LEO:  You're going to have to stick around, Steve, I'm sorry.  You need to keep going until we get the job done.  That's all I'm saying, Steve.



STEVE:  So what is popular on the Internet underground?  The firm Recorded Future did a bunch of really interesting sleuthing.  They assembled a beautiful report titled "Bestsellers in the Underground Economy:  Measuring Malware Popularity by Forum."  And for anyone who wants the details, I'm just going to cover the top layer.  But I've got the link both to their posting and also to the detailed PDF that has a whole bunch of interesting graphs.



In summary, they said:  "By analyzing over 3.9 million posts from May 2018 to May 2019" - now, first of all, hold on.  In one year there were 3.9 million posts?  Think about that.  That's a lot of posts in a year.  They said:  "...across all underground forums indexed by the Recorded Future platform.  Recorded Future's Insikt [I-N-S-I-K-T] Group identified the top malware variants being referenced on underground forums.  The Insikt Group also attempted to find real-world events that correlated with a higher number of malware references on these forums, as well as differences in tools advertised in forums of different languages, to see if any differences existed.



"The Insikt Group discovered that a majority of the top 10 mentions of malware in multiple languages included openly available dual-use tools, open source malware, or cracked malware.  Some of these malware families were also over three years old or could be mitigated with basic security precautions.  Activity in underground forums that correlated to growth in malware references included sale of malware in a larger bundle, advertising updates to the malware, advertisements of the malware on a new forum in which the malware was not previously sold, news articles related to malware shared on forums, and community engagement.



"Insikt Group also discovered that underground communities in different languages did indeed focus on different malware, malware categories, and attack vectors."  So there was language-based bias.  "English- and Chinese-speaking underground communities, for example, focused more on Android malware than other communities.  By separating forum advertisements by language, Insikt Group found that forum members occasionally used online translation services to attract business partners and buyers from different language communities."



Then they broke this all down into four key judgments, which I'm going to share.  But it's interesting.  Reading between the lines is really interesting because it sort of shows what is and what is not happening there.  So there are four key judgments.  The first was "The top 10 mentions of malware across Recorded Future underground forum collections suggest that underground forum members are discussing and using tools readily available to them more often than paying for or inventing new tools."



Second:  "Based on the prevalence and longevity of the malware, Insikt Group assesses with medium confidence that there likely exist enough victims who do not comply with basic security precautions for forum members to successfully infect."  Okay, so in other words, the stuff being transacted on these forums is still effective.



Third:  "Approximately half of all activity concerning ransomware on underground forums are either requests for any generic ransomware or sales posts for generic ransomware from lower level vendors."  They wrote:  "We believe this reflects a growing number of low-level actors developing and sharing generic ransomware on underground forums."



And, fourth:  "The Insikt Group assesses with medium confidence that, due to the number of underground forum members sharing, deploying, and providing reviews about malware and its functionality, the 10 most popular malware on underground forums hit host computers with higher frequency, but are low to moderate threats compared to other malware due to their age, ineffectiveness without a delivery vehicle or crypter, and existing AV detections."



So what was interesting, and I did read the whole report, so I'm also pulling from there, but this suggests a few things.  For example, I scanned the charts and the graphs and the detailed description for, for example, any mention of Ryuk, which is, as we know, the ransomware that is actually doing damage to municipalities.  It is nowhere.  Why?  Because it's high-end, proprietary, upper-crust ransomware.



LEO:  Oh, we only use the best ransomware.



STEVE:  The ransomware, well, it's not been let go of.  It's not been released.  The ransomware that's for sale in these forums is sort of the equivalent...



LEO:  The old crap.  WannaCry.  Yeah.



STEVE:  Yes, yes.  The second-hand, non-state-of-the-art, hand-me-down ransomware.



LEO:  I might make a different distinction, that Ryuk is spearphished, targeted attacks.



STEVE:  Yeah.



LEO:  Whereas WannaCry and others are where they just email a million pieces out and just see what happens.



STEVE:  They're just spraying it, yes.



LEO:  Just spraying it.



STEVE:  And also those who are capable of writing state-of-the-art ransomware are not selling it in these forums.



LEO:  Yeah.  They keep it themselves.



STEVE:  They're not selling it at all, exactly.  They are, rather, they are deploying it.  And so perhaps in five years Ryuk will be seen changing hands within the underground community, but not today.  Today it's earning a reputation that will evolve into mythology.  And then that mythology will eventually be driving its aftermarket release.  But that will be long after Ryuk's authors have moved on to the next whatever it is software to achieve their ends.



And so that brings us to this week in ransomware.  Lawrence Abrams headlined his weekly snapshot ransomware coverage for his BleepingComputer posting, he called it "State of Emergency."  And he started off with a declaration that I think is all too true.  He wrote:  "Now that ransomware developers know that they can earn monstrous payouts from local cities and insurance policies, we see a new government agency, school district, or large company getting hit with a ransomware attack every day."  He said:  "For example, this week the Governor of Louisiana declared a state of emergency for the wave of attacks targeting school districts in the state."



Anyway, so that's all I'll share from that.  But what happened in this past week?  Louisiana's Governor John Bel Edwards last Wednesday declared a formal state of emergency after three public school districts were taken over by ransomware.  The reason for them making it a declaration of a state of emergency is interesting, and we'll get to that in a second.  One of the three school districts, which was the Sabine Parish in Northern Louisiana released the following statement.



They said:  "The Sabine Parish School System was hit with an electronic virus Sunday morning.  This virus has disabled some of our technology systems and our central office phone system.  The district staff reported this electronic viral attack to local law enforcement, state officials, and the FBI.  All available resources are being utilized to get the district systems back online.  An investigation involving local, state, federal law enforcement is ongoing at this time.  The school phone systems were not affected by this attack."  But the district systems were, as opposed to the individual schools.



"The central office phone system is being repaired, and service will be restored as soon as possible.  According to the Louisiana Department of Education, several other school districts were attacked by the same virus this week."  And apparently that is the case.  We don't have any details yet, but does anyone want to bet that it wasn't Ryuk, since that does seem to be what's being used in all of these recent attacks.  State officials have not yet released a full list of the affected systems.



Eddie Jones, who is a principal of the Florien High School within the Sabine Parish, told the local news station KSLA that his technology supervisor received an alert on his phone around 4:00 a.m. Sunday morning about a surge in bandwidth usage.  So that was probably an automated report.  And he said it was particularly unusual given first the time of day, and also the fact that schools are all on summer break.  When technical staff investigated, Eddie Jones said they found ransomware on the servers.  The principal said that he doesn't believe that any sensitive information was lost.  But what was lost was "anything and everything," quoting him, stored on the school district's servers, including 17 years' worth of Jones' personal documents - his speeches, test schedules, master schedules, and more.



What's interesting is that the declaration of a state of emergency means not only that state resources will be made available, and that assistance will be coming from cybersecurity experts from the Louisiana National Guard, Louisiana State Police, and Office of Technology Services and others to assist local governments in responding to the crisis and preventing further data loss, but also the declaration of an emergency includes protection from being price gouged for the cost of extra help and resources, which apparently has been a problem previously.



LEO:  Oh, interesting.  Ah.



STEVE:  Yes, uh-huh.  The language concerning that protection from a Louisiana proclamation about states of emergency reads:  "During a declared state of emergency, the prices charged or value received for goods and services received within the designated emergency area may not exceed the prices ordinarily charged for comparable goods and services in the same market area at or immediately before the time of the state of emergency..."



LEO:  C'mon.  That's just surge pricing.  C'mon.



STEVE:  "...unless the price by the seller is attributable to fluctuations in applicable commodity markets."



LEO:  It's a fluctuation.  That's what's going on.



STEVE:  Yeah, uh-huh.  It's a fluctuation, "...fluctuations in applicable regional or national market trends, or reasonable expenses and charges and attendant business risk incurred in procuring or selling the goods or services during the state of emergency."



So what's interesting is this is the first time that Louisiana has activated its emergency cybersecurity powers, which were created for just this type of cyberattack.  There response is being handled by the states' newly formed Cyber Security Commission, which was established two years ago, in 2017.  It brings together the state's key stakeholders, subject matter experts, and cybersecurity professionals from Louisiana's public sector, private industry, academia, and law enforcement.  So anyway, three districts in Louisiana, Northern Louisiana, were all hit.  So it's interesting that the timing was coincident for those three.  It's probably the case that there was some network connectivity among them that allowed a successful penetration to get into those districts.  So they were likely linked in some fashion.



LEO:  Well, but remember the school calendar, too, Steve.  Because these districts are closed for the summer.  Administrators come back right about now.  Teachers come back in, getting ready.  They open their email.  They click those links.



STEVE:  Yes, yes.  And I was thinking that it might...  



LEO:  Get ready, because there's going to be a whole bunch of them soon; right?



STEVE:  Yes.  I was thinking it was likely that there would be less pressure on them to get their systems up.  But following up on all of this, it turns out that they're, like, a week away from needing to get themselves up and going.  So you're exactly right.



LEO:  Yeah, there's no coincidence, yeah.



STEVE:  You're exactly right, Leo.  Probably it was email that was, like, waiting for them.  And then just yesterday we learned of a fourth Louisiana school district.  And I can't even begin to pronounce this:  T-A-N-G-I-P-A-H-O-A, Tangipahoa.



LEO:  Laissez les bons temps rouler.  Tangipahoa.  I think you're right, Tangipahoa, yeah.



STEVE:  Tangipahoa.  Anyway, they're down, too.  The Advocate in Baton Rouge had an article that stated just yesterday a fourth Louisiana school district was attacked.  In local reporting of that attack, the comment was made that the timing was especially troublesome since school was scheduled to resume in about a week.  So I guess they are all under significant pressure to get themselves back up.  And I guess we will find out.



LEO:  I guess we're going to see a lot more of this in the next month, is what I guess.



STEVE: Oh, boy.  Yeah.



LEO:  Yeah.  Wow.



STEVE:  Deep pockets and insurance companies and very effective ransomware.



LEO:  Well, and also administrators and teachers coming back a couple weeks before the kids do.



STEVE:  Yup, and saying, gee, well, and kind of being in a hurry to catch up on all of the mail that they got in their inbox and maybe being a little less cautious.  So yikes.



As I said, until the Urgent/11 announcement, this podcast was going to be titled "Your NAS Is Grass."  One by one, as we have been seeing, service by service, we have been seeing attackers targeting anything that's exposed and is either deliberately or inadvertently offering services to the Internet.  Last week's podcast was titled "Hide Your RDP Now" because, as we saw, even non-BlueKeep-vulnerable RDP services and servers were under attack to a sobering degree with password guessing.  So now we're looking at NAS, Network Attached Storage, because it turns out those are often, by nature, I mean, certainly some NAS is inside the organization.  It's providing Intranet services.  But it's often the case, unfortunately, that these things have a public exposure, as well.



Earlier this month a campaign targeting the NAS devices produced by the Taiwanese company QNAP Systems, Inc. was uncovered by researchers at Anomali Labs.  They found that QNAP's devices were being compromised, both by brute-forcing weak credentials and exploiting known vulnerabilities.  And, once infected, a malicious payload was then encrypting specific file extensions on the NAS - thus we're talking ransomware - using AES encryption and then appending the .encrypt extension to the encrypted files.



After that was done, the following ransom note was left behind.  It reads:  "All your data has been locked (crypted).  How to unclock," actually it says in a typo "(decrypt) instruction located in this Tor website."  So then it pointed people to a .onion Tor site with a bitcoin address and with the instructions, "Use Tor browser for access .onion websites."  And then it gives you a DuckDuckGo URL with the link to search for "tor browser how to" in order to instruct the person how to use Tor in order to go to the onion address.



LEO:  I like that they're using DuckDuckGo, though.  I think that's good.  They really care about our privacy, yeah.



STEVE:  Wasn't that a nice touch, Leo?  Yes, exactly.  Then they say:  "Do not remove this file and not remove last line in this file," which contains the cryptographic key, Base64 encoded, which is necessary to be provided to them in order to decrypt the NAS.  So basically we have brute forcing of either, well, not only QNAP, but probably any.  But in the case of QNAP they're able to get in, install malware, encrypt all of your files.  And presumably, if it's a NAS, it's got a lot of storage and a lot of stuff that you care about.



LEO:  A lot of stuff, yeah.  You're backing everything up there, yeah.



STEVE:  And so it's ransomware.  They reverse engineered the ransomware, discovered that it had been written in the Go programming language; that it was very straightforward, not very complex; consisted of fewer than 400 lines of code.  Upon execution, the malware first reaches out to its command-and-control server, listening on IP 192.99.206.61 on some high-numbered port, I don't remember which.  That notifies it that the encryption process has started.  The malware then walks the file system hierarchy, looking for files that have not yet been encrypted, meaning don't have the .encrypt extension already, and encrypts them, then appends .encrypt to the end.  Once it's done, it again contacts the command-and-control server, notifies it of completion, and produces the ransom note, which it leaves behind.



And in the coverage of this I thought that they made a good point, which is that NAS devices are not normally running any form of AV software since they tend to be turnkey embedded systems, often without a screen or keyboard.  They're just, you know, mount them in the rack or put them somewhere, plug them in, and they go.  Even so, at the time of its discovery, VirusTotal reported that only two or three out of, what, like 70-some AV systems that VirusTotal queries were detecting this malware as being malicious.  So it had not been seen widely before and had not been submitted for analysis before such that it was largely unknown by VirusTotal.



And in their conclusion the researchers recommended that external access to QNAP NAS devices be restricted.  Ensure that the device's firmware is current, up to date.  Use strong credentials.  And, I mean, that's super important for any devices that are exposed to the Internet.  As we saw last week, there is a real upsurge in Internet-connected things.  We talked about RDP brute forcing.  Now we're looking at NAS device brute forcing.



LEO:  I think a lot of these NASes are unsophisticated people [crosstalk].



STEVE:  Yes.



LEO:  And they really are vulnerable.  And they're using monkey123.  I mean, on my Synology I have two-factor enabled.  I've turned off all services.  I don't have FTP turned on.  If you turn on SSH, don't use passwords, use SSH keys.  I mean, there are just ways to secure any server that are sensible.  And I do see, every time I look at my log, I see Chinese IP addresses and others trying to brute force the password.  It's every day.  And Synology has - I'm sure QNAP does, too, because it's very good - a feature where you say, if somebody tries to log in five times from a single IP address and fails, block that IP address forever.  And I do that, too.



STEVE:  Yes.  Good.



LEO:  So, I mean, honestly, I think it's not terribly difficult to secure your NAS.  But I think a lot of these are just people plugging them in.



STEVE:  Yeah.  Well, and speaking of Synology, that's story number two here.  Last week Synology issued an urgent warning titled "Synology Urges All Users to Take Immediate Action to Protect Data from Ransomware Attack."  And this was July 23rd.  They said:  "Synology recently found that several users were under a ransomware attack, where admins' credentials were stolen by brute-force login attacks, and their data was encrypted as a result.  We investigated and found that the causes of these attacks were due to dictionary attacks instead of specific system vulnerabilities."



LEO:  Yeah.  Yeah.



STEVE:  So props to Synology, exactly as you were saying, Leo.



LEO:  And they offer two-factor, which would also be a good thing to do in this case.



STEVE:  Yes, yes.  And I'm wondering if here, blah blah blah blah blah.  Ken Lee, Manager of Security Incident Response Team at Synology said: "We believe this is an organized attack. After an intensive investigation into this matter" - oh, because several of their users did have ransomware attacks - "we found that the attacker used botnet addresses to hide the real source IP after collecting admin account passwords with brute-force attacks.  Since this attack was not related to system security vulnerabilities, it's recommended that Synology users utilize built-in network and account management settings to enhance system security level, preventing malicious attacks from the Internet."



So this is just like a general, you know, please protect yourself because if these boxes are publicly exposed, they are going to be brute-force attacked.  And so the end of their announcement said, exactly as you said, LEO:  Use a complex and strong password and apply password strength rules to all users.  Create a new account in the administrator group and disable the default "admin" account.  Enable - and here's the nice feature -  Auto Block in the control panel to block IP addresses with too many failed login attempts.  And then run the Security Advisor to make sure there are no weak passwords in the system.  So, you know, Synology is a beautiful piece of equipment.  It doesn't have a problem.  But if you expose it publicly, and you use a weak password, you're going to get yourself taken over.



LEO:  Put your server on the Internet.  



STEVE:  Yes, yes, yes.  Deliberately.



LEO:  Yeah.



STEVE:  And so third and last, and this is unfortunately not a strong use case, as Synology is, Lenovo-EMC and Iomega have a significant flaw.  And when I saw Iomega, it's like, what?  Iomega?  They're still around?  Well, it turns out that 11 years ago, in 2008, EMC bought up the assets of Iomega.  Then five years ago a Lenovo-EMC joint venture relaunched Iomega as LenovoEMC.  So today we have an Iomega/LenovoEMC vulnerability announcement.



Our story begins with an intrepid employee with the infosecurity firm Vertical Structure, based in Northern Ireland.  He first discovered files apparently being publicly exposed on the Internet via Shodan.io last year.  After a bit of additional sleuthing, Vertical Structure confirmed that documents, a great many documents actually, were being publicly exposed to the Internet, without any password or authentication checks, through an unauthenticated API.



So here we have a situation where people were putting these NAS devices on the Internet.  And unfortunately, they not only offered the standard file access API, but a different API elsewhere, that is, running on other ports with no authentication.  Clearly, I mean, assuming that the designers of this weren't completely out of their mind, they assumed that this NAS would not be publicly exposed, or that it would be behind a firewall of some kind so that all of the ports of the NAS's IP were not public.  But that didn't happen.



The exposed API was eventually tracked down to belonging to an older set of Iomega NAS boxes that were - thanks to this widely exposed and unauthenticated API - now, today, leaving many millions of files, more than three million as it turns out, totaling 36TB of information, exposed on the Internet.  Simon Whittaker, a director at Vertical Structure, the group that found this, said there were a significant number of files containing sensitive financial information, including credit card numbers and financial records.



Vertical Structure was able to track down the common source, a legacy Iomega storage product acquired by EMC and co-branded LenovoEMC as a result of their joint venture.  Within those three million files, Whittaker said there were 405,000 images, 20,055 documents, 13,677 spreadsheets, 13,972 text documents, and then lots of other stuff to bring the total above three million.  After realizing the extent of the exposure, Vertical Structure called in the firm WhiteHat in Silicon Valley, who ran their own independent investigation of the leak and confirmed that public-facing Iomega-LenovoEMC devices were indeed spewing all of their data onto the Internet via an unauthenticated API.



The two companies alerted Lenovo to the problem, and the vendor responded, Lenovo responded by bringing the software essentially out of retirement to address the bug.  There is now a LenovoEMC NAS vulnerability disclosure.  There's a patch available.  I've got a link to it in the show notes.  So if anybody listening has an older Iomega NAS, you want to make sure that it is not among those that are vulnerable.  I can't remember the name.  There was a bunch of them that were just a bunch of cryptic, weird-looking names.  But something "Stor" something, S-T-O-R - I ought to just click on this link, if I've got it right here in front of me, and bring up the announcement.  StorCenter.  So the bulk of them, one, two, three, four, five different StorCenter NAS Cloud Servers and others.



So anyway, if you have an older NAS device, you want to make sure that you bring this thing current.  So as it would you, Leo, it makes me shake my head that even consumer NAT routers provide more security to us than we're seeing in a commercial NAS like this.  The idea that this thing could apparently, by default, be publishing an unauthenticated API on its public-facing interface is just crazy.  As I said, it must be that the designers intended this to be behind a firewall so that somebody would only be mapping the NAS normal interface through from the public, rather than putting it out publicly.



And that's just, as a word of advice for all of our listeners, don't put a NAS on - don't give it its own public IP.  Put it behind a NAT router.  Map the ports that you need through to it.  Put it behind a firewall.  Only map through to it what you need.  There's just no safe way to make a device like that, to give it its own IP and say to the Internet, here, scan this device and attack anything that says hello when you connect to it because that's what Shodan did.  And Shodan found all of these things, incredibly.



Okay.  Time for a little fun, Leo.



LEO:  Well, or not.



STEVE:  Or not.  I did mention the show on the podcast.  I don't remember if it was with you two weeks ago.



LEO:  Yeah, two weeks ago.  You liked the publicity stills of Katee Sackhoff in her skivvies.



STEVE:  Ah, right.  I did, indeed.



LEO:  That should have told us something, by the way.



STEVE:  Yeah.  She's better known to us as Starbuck...



LEO:  Starbuck, yeah.



STEVE:  ...from "Battlestar Galactica," which is a much-appreciated sci-fi series.



LEO:  Great show, yeah.



STEVE:  So I did have - I had high hopes for the Netflix series "Another Life," which dropped in its entirety last Friday.  I tweeted immediately after:  "The best thing about Netflix's much-anticipated, incredibly, unbelievably awful and disappointing science fiction series 'Another Life' are the reviews on IMDB."



LEO:  Which are universally terrible.



STEVE:  Oh, my god, Leo.



LEO:  But I figured it out after watching 10 minutes of this.  I knew exactly.  This is very Hollywood.  Somebody said, "We want to give Katee a show."  "Well, she's got to be in her underwear," somebody else said.  Then somebody said, "You know what's hot?  You know what the kids like these days?  They like those reality shows like 'Big Brother' or 'Real Life,' where you take a bunch of young people, you put them in a house, and you let them fight it out.  What if we did 'Big Brother in Space'?"



STEVE:  That's exactly what happened.



LEO:  And that's what it is.  It's awful.



STEVE:  Yes.  Yes.  It is un- well, I mean, it just - it isn't for us.  It isn't for our listeners.



LEO:  I don't know who it's for.



STEVE:  It isn't actually science fiction.  I mean, it's just unbelievably bad. 



LEO:  The first 10 minutes are all exposition.  I mean, I hate it when you put words in people's mouths like, "Well, and then the alien artifact arrived on Earth, and everybody wanted to know where it was going," I mean, they were explaining the whole thing.  But the best line, my favorite line was once they get on the ship, and it's completely out of nowhere, it's not in response to anything, Katee says, "Well, 10 years ago they said we didn't have to wear uniforms in space."  And it's clear that they did that so that people could be dressed in high heels and leather pants and skivvies.



STEVE:  Yes, I mean, there was somebody in high heels, Leo.



LEO:  Which is of course very practical for space.



STEVE:  That's right, yes.  And, I mean, oh, I won't go any further.  It was just...



LEO:  It was awful.



STEVE:  It was unbelievably bad.



LEO:  Yeah.  But not so bad - not like "good" bad.  It's bad bad.



STEVE:  Well, and I have to say, if you are a Netflix subscriber...



LEO:  It's pretty.



STEVE:  It's really almost worth 30 minutes.  Lorrie and I got 30 minutes in.



LEO:  You got further than I did.  I couldn't.



STEVE:  I know.  Because I just didn't - I had hope.  I wanted to think - and, I mean, you know, we know how to do really good special effects.  Although, Leo, when you're slingshotting around a star, why are you subject to sudden jolts that is causing the lighting panels above you to fall loose and swing from their wires?



LEO:  The science wasn't exactly well thought out in this, I don't think.



STEVE:  It's really, really, really, really, really, really...



LEO:  It's kind of - I don't understand how it got produced.  I really don't.



STEVE:  I don't.  I agree.  Somebody spent some money on this thing.  Anyway, it's, yeah, it's really bad.



LEO:  So bad.



STEVE:  So I have two pieces of better news.



LEO:  Okay.



STEVE:  The first is, also released Friday, "The Great Hack."  It's a just-released Netflix original documentary, primarily about Cambridge Analytica and, somewhat secondarily, about its role in the 2016 U.S. Presidential Election.  It is not very technical.  It's got some beautiful graphics.  It's predominantly kind of a human interest piece, and it obtains a look inside Cambridge Analytica and several related organizations through the eyes of a young woman who got swept up a bit in the power and, as a consequence, was very much on the inside.  I mean, she was there and later saw what this thing had become.



The sense I got was that we have her story because she's still the only one talking.  The main guy, whose name I forgot, isn't talking.  There's doubtless more to be told, and presumably more will come out over time.  But it was a two-hour documentary.  Anyone watching it, I think - Lorrie and I really enjoyed it - will very much come away with a deeper appreciation of the truly influential power that big data mining and its use to drive targeted advertising has over us.  So it was compelling from that sense.  And I thought it was a worthwhile two hours on Netflix.



You know, when you think about it, in today's world we are direct eyewitnesses to only a tiny portion of all that we believe that we know to be an accurate depiction of reality.  Everything else we receive is relayed through a channel that has some purpose for doing so.  Anyway, it's called "The Great Hack," available since last Friday.  And so I would, if you think you would be interested in learning more...



LEO:  I will watch that, yeah.



STEVE:  ...about the effect of big data, yeah.  And I'll ask you what you thought of it next week, Leo, because I think it was worthwhile.



LEO:  We did have Alexander Nix, who was the initial whistleblower for that, on Tech News Weekly this past week.



STEVE:  Oh, cool.



LEO:  Or I think it was Tech News Weekly.  So worth watching that, as well.



STEVE:  And for those who like Tarantino.



LEO:  Christopher Wylie, that's his name.  



STEVE:  Ah, Christopher Wylie.  For those who like Quentin Tarantino's work - are you a Tarantino fan, Leo?



LEO:  Love him.  Love him.  And I'm dying to see this new movie.  Did you like it?



STEVE:  It's fantastic.



LEO:  Oh, I can't wait.



STEVE:  In fact, my biggest problem was that Lorrie and I were, I guess, screaming out loud, laughing.  I was so self-conscious of the rest of the - the fact that we weren't alone in the theater.  I was trying to mute myself, but it was - okay.  So first of all, I read a whole bunch of the reviews on IMDB.  It's at nine point plus.  It's north of nine.  So, I mean, it is - they're all tens.  There were a couple people who gave it a seven or a six.  And I have to say, as I was watching it, DiCaprio and Brad Pitt are two major names, although Michael Madsen is there, Bruce Dern is there.  Again, anybody who, you know, Quentin says, "Hey, would you want to be in my new movie," they're going to say yeah, you know, yeah, please.



So it was - I want to say it was plotless.  And it was almost plotless.  And about maybe two hours in - it was two hours and 45 minutes.  So it's a long - it's one of Quentin's typical long movies.  It was enjoyable.  Great performances.  But you're just sort of - I remember thinking, well, maybe I'm going to be kind of disappointed by this.  Maybe, like, this is going to be like a movie about nothing, where it was nice, but nothing really happened.  Leo, it's all a setup.  It is all just Quentin playing with us for almost probably two and a half hours, until the whole thing comes crashing into a crescendo that is just unbelievably wonderful.  So, I mean, I will...



LEO:  Good.  I will go see it.



STEVE:  ...absolutely see it again.  And, I mean, I can't quite sit through the entire two hours and 45 minutes just to experience the [crosstalk]...



LEO:  A long setup.  That is a long setup.



STEVE:  It was a long setup, but OMG.



LEO:  I can't wait.



STEVE:  Oh, my god, it was fun.  And I completely get it that some people are like, who want a plot, people who want [crosstalk] movies...



LEO:  I like this kind of movie.  I like this kind of movie, the Hollywood insider movie.  I like "The Player," and I like that kind of movie.



STEVE:  Yes.



LEO:  Yeah, yeah.



STEVE:  Yes.  It is absolutely that.  And DiCaprio does some of his best acting of his career.  Brad Pitt is fabulous.  It's just, I mean, it just is, it was a great piece of work.



LEO:  I'm surprised, though.  You did not have the one story of the week I really thought you would be talking about.  "The Expanse" is coming back for Season 4.  And I know you love this show.



STEVE:  I am glad, yes.  I read the books.



LEO:  And they could go on for years, really, couldn't they.



STEVE:  It was a fabulously rendered series.  I mean, that was it.  I've talked about "The Expanse" on the show.  It had some of the best realistic combat I have ever seen put on film.



LEO:  Yeah, yeah.  Well, they're bringing it back.



STEVE:  Yeah, I'm going to have to get Lorrie to sit through it.  It was a little, I mean, there was a lot of, like, the Belter-Mars politics...



LEO:  Oh, at the beginning, the first episode there's a lot to get into, yeah.



STEVE:  Yeah.



LEO:  I've actually watched the first episode four times and not got past it.  So I will now.  And how many "Expanse" books are there?  There are quite a few; right?



STEVE:  Yeah.  They kept going.  So there were, I think, three in the original, and then there was a fourth, and maybe a fifth.  So, but, yeah.



LEO:  There's a lot of material.



STEVE:  Yeah.  Really, really hard, hard sci-fi.  It is a little, I mean, it's a little challenging because time is spent with the politics of Earth, Mars, and the Belt, and the fact that they have different needs that drive different politics.  And that creates tension.  But, boy, it was good.



LEO:  Yeah, yeah.  There's nine total.  I guess eight of them have been published.  So there's...



STEVE:  Wow.



LEO:  There'll be many, many years if Amazon Prime wants to continue producing the show.



STEVE:  And if they continue it, if they maintain the production quality, then it would be worth...



LEO:  You've got all the CGI templates, you might as well; right?



STEVE:  Yeah.  



LEO:  All the work's been done.



STEVE:  Okay.  So the news that has taken the industry's breath away is that, as I said at the top of the show, the number one most used embedded operating system on the planet, which is closed and has never been scrutinized, was painstakingly reverse engineered.  We were just recently talking in the context of the idea of China rolling their own Internet operating system, how ridiculously improbable that really is.  And remember I used the example of the huge amount of trouble our entire industry had just getting the TCP/IP stacks in our operating systems running solidly and correctly due to the extreme complexity and all edge cases of the evolving TCP/IP spec over time.  So that's where the problems are.



The entire industry has now been bitten by a set of 11 vulnerabilities, six of them critical remote code execution vulnerabilities, found to affect VxWorks' TCP/IP stack.  That's where the problems are.  Which has been quietly embedded into more than two billion devices.



LEO:  Whoa.  Wow.



STEVE:  Billion with a "b."  Two billion.  And Leo, if you jump ahead and scroll through the show notes, I couldn't resist listing them by class.  I'll get to that in a second.  But okay.  Since VxWorks is not a common household name, we need to step back for a moment to understand what VxWorks is and why it matters.



I've talked about so-called "embedded operating systems" in the past.  A microwave oven will have an embedded operating system.  The good news is it probably won't have an Internet connection.  All modern automobiles have at least one, and probably several.  The Amazon Kindle is based upon an embedded Linux kernel that's been stripped down for embedding.  All of our consumer NAT routers have one, as will commercial firewall devices.  Certainly all modern printers are written on top of an embedded OS.  And somewhat more worrisomely, virtually all sophisticated IoT devices, you know, Internet of Things, the "I" is the Internet, which means connectivity.



LEO:  Wait a minute.  Even Grandma is running, even Grandma is running on this.



STEVE:  I know, I saw that.  That was a kick.  So IoT devices will have an embedded OS at their heart.  The number one most popular, most widely used OS with a 20, sorry, with a 32-year history is Wind River Systems' VxWorks.  Wikipedia has the following to say about VxWorks.  They say:  "VxWorks is a real-time operating system (RTOS) developed as proprietary software by Wind River Systems, a wholly owned subsidiary of TPG Capital.  First released in '87, VxWorks is designed for use in embedded systems requiring real-time, deterministic performance and, in many cases, safety and security certification, for industries such as aerospace and defense, medical devices, industrial equipment, robotics, energy, transportation, network infrastructure, automotive, and consumer electronics."



They say:  "VxWorks supports Intel architecture, PowerPC architecture, and ARM.  The RTOS can be used in multicore asymmetric multiprocessing, symmetric multiprocessing, and mixed modes and multi-OS, for example, Type 1 hypervisor designs, on 32- and 64-bit processors."  So it's very capable.  "VxWorks comes with the kernel, middleware, board support packages, Wind River Workbench development suite, and complementary third-party software and hardware technologies.  In its latest release, VxWorks 7, the RTOS has been re-engineered for modularity and upgradeability so the OS kernel is separate from middleware, applications, and other packages.  Scalability, security, safety, connectivity, and graphics have been improved to address Internet of Things (IoT) needs."



So then, under Notable Uses, Wikipedia enumerates a few, which is worth sharing to get a better sense for the popularity and broad sweep of this OS.  Yes, VxWorks will be in the Mars 2020 rover scheduled for launch next year.  It is currently in the Mars Reconnaissance Orbiter.  It's in the Mars Science Laboratory, also known as the Curiosity rover.  It's in also the rovers Spirit, Opportunity, and Sojourner.  It's in the Deep Space Program Science Experiment, the Phoenix Mars lander, the Deep Impact space probe, the Mars Pathfinder mission, the SpaceX Dragon, and NASA's Juno space probe sent to Jupiter.  In other words, it's what NASA uses, and JPL uses, as the core OS for all their stuff.



Fortunately, those things don't have IP addresses, so we're probably okay.  Although we did actually recently hear about an exploit that was getting into the communications link to something going on with NASA.  I forgot what it was.  I talked about it just recently.  So that may be a concern.  We also have it in the AgustaWestland Project Zero aircraft, whatever that is.  Northrop Grumman's X-47B Unmanned Combat Air System uses VxWorks, as does the Airbus A400M Airlifter.  BAE Systems Tornado Advanced Radar Display Information System, that's TARDIS, used in the Tornado GR4 aircraft for the U.K.'s Royal Air Force, and Lockheed Martin's RQ-170 Sentinel UAV.  So it's also in drones.



And this list goes on and on and on.  Toshiba uses it in automotive for their image recognition advanced driver assistance systems.  Bosch Motor has it in their race car telemetry.  Hyundai Mobis's IVI system.  It's inside BMW's iDrive system since 2008.  Siemens uses it in their automotive navigation system.  Renault trucks.  The Volkswagen RNS 510 navigation systems.  It is the OS in the Apple Airport Extreme.  The Drobo uses it.  I really thought Drobo was Linux-based, but apparently it's VxWorks.  Even the cute Honda Asimo robot is VxWorks based.  It's also in Linksys's WRT54G wireless router, unless you apparently reflash it with the Linux-based firmware.  I mean, ReplayTV uses it as its embedded OS in their DVR.



And, I mean, there's industrial robotics, test and measurement, transportation, all kinds of SCADA controllers, external RAID controllers.  As you said, GrandMA full-scale - GrandMA is Grand M-A, full-size and light console by MA Lighting, whatever that is.  Medical systems, Varian, and on and on and on and on.  And then, unfortunately, the longest list is networking and communications infrastructure, which are probably on the Internet.  Certainly, and there were some notable ones, Cisco's CSS platform and their ONS platforms.  Dell PowerConnect switches are VxWorks based, as are SonicWall firewalls.  And that may be the most obvious, biggest problem because there's lots of SonicWall firewalls deployed in enterprise environments all over the Internet.



And unfortunately this problem, this is an exploit in the IP networking for VxWorks that these devices are going to have.  So it's almost easier to ask what doesn't use VxWorks.  And probably in the embedded world, the only thing that doesn't are things that do use embedded versions of Linux, you know, stripped-down Linux for their applications.  So today, or actually yesterday, after a 32-year, relatively blemish-free track record, we now have 11 very serious vulnerabilities, six of which are remote code execution capable, that we know of in VxWorks.



Armis, Inc. found and responsibly reported and disclosed these findings to VxWorks, which has of course fixed them all and released patches.  The problem is, well, okay.  We can't patch - maybe NASA could patch the Mars rover if it were concerned about it, but they probably aren't.  But unfortunately, many of these devices are hooked up to the Internet.  And those that are in IoT may not be firmware rewriteable.  They just may be fixed.  So they could be in trouble.



Armis explains what they found.  Urgent/11, as they called it, is a set of 11 vulnerabilities found to affect VxWorks' TCP/IP stack, which is a module which you can include in VxWorks known as IPnet, that is, the IP networking module.  It's used by versions, many versions of VxWorks.  Six of the vulnerabilities, six of the 11, are classified as critical and enable remote code execution.  The remaining vulnerabilities are classified as denial of service, information leaks, or logical flaws.  As each vulnerability affects a different part of the network stack, it impacts a different set of VxWorks versions.



However, as a group, Urgent/11 affects the VxWorks versions described with at least one remote code execution vulnerability for each version.  A wide range of affected versions spanning over the last 13 years, they write, is a rare occurrence in the cyber arena and is the result of VxWorks' relative obscurity in the research community.  This time span might be even longer, as according to Wind River three of the vulnerabilities were already existent in IPnet when it acquired the stack from Interpeak back in 2006.



Urgent/11 are the most severe vulnerabilities found in VxWorks to date, which, they write, has suffered from only 13 public CVEs in its entire 32-year history.  Urgent/11 is a unique group of vulnerabilities that allow hackers to circumvent NAT and firewalls.  And this is important.  These pass through NAT and firewalls and, they write, take control of devices remotely via the TCP/IP stack undetected, with no user interaction required.  This is due to the vulnerabilities' low-level position inside the TCP/IP stack, which enables attacks to be viewed as legitimate network activity.



Such vulnerabilities do not require any adaptations for the various devices using the network stack, making them exceptionally easy to spread.  In most operating systems, such fundamental vulnerabilities in the crucial networking stacks have become extinct after years of scrutiny unraveled and mitigated such flaws, just as I've been saying of our public OS stacks.



They said:  "Urgent/11 is comprised of 11 vulnerabilities, separated into two classes of severity.  There are six critical vulnerabilities allowing remote code execution, and five vulnerabilities leading to denial of service."  So to give our listeners a sense for what these are, there's a stack overflow in the parsing of the IPv4 options.  They said:  "This vulnerability can be triggered by a specially crafted IP packet sent to the target device, even as a broadcast or multicast packet.  It does not require any specific application or configuration to be running on the device," again, because it hits too low, down in the low level stack.  They said:  "...and it affects any device running VxWorks v6.9.4 or above with a network connection."  And that's, because VxWorks evolves very slowly, this is very widespread the past 13 years.



"The vulnerability causes a stack overflow in the handling of IP options in the IPv4 header, making it easy to reach remote code execution through it."  They said:  "There are four memory corruption vulnerabilities stemming from erroneous handling of TCP's Urgent Pointer field.  The four vulnerabilities all stem from erroneous handling of TCP/IP's Urgent Pointer.  The Urgent Pointer is an esoteric TCP field that is rarely used in modern applications.  An attacker can trigger the erroneous handling of this field by either directly connecting to an open TCP port on the target device, or by hijacking an outbound TCP connection originating from the target device.



"Once triggered, these vulnerabilities will cause the application on the target device to receive more bytes than expected from the system's receive function, leading to a memory corruption of either the stack, the heap, or a global data section, depending on which buffer was passed to the receive function.  This means an attacker can probe the various TCP connections of the target device and attack the application that is the easiest to exploit."



And then they go on through additional details, breaking down each of these four different Urgent Pointer attacks.  Then they also discuss a heap buffer overflow in this.  And the disclosure goes on.  They have complete details, and the fact of this was  made public yesterday because VxWorks fixed this a month ago, and they will be detailing this in greater depth during their Black Hat presentation next week on August 8th in Las Vegas.



Trying to sound like this wasn't really their fault, a Wind River spokesperson told ZDNet:  "These vulnerabilities are not unique to Wind River software.  The IPnet stack was acquired by Wind River through its acquisition of Interpeak in 2006.  Prior to the acquisition, the stack was broadly licensed to and deployed by a number of other RTOS vendors."  Uh-huh.  So if you have a SonicWall, Dell PowerConnect switches, a Linksys WRT54G wireless router v5 or later, or a Drobo exposed to the Internet, you really need to pay attention.  Make absolutely sure that you have patches, not only current, but that they are within the last month or so, so that you know that the patch vendor will have received the RTOS update from their source and will have updated because this is bad.



The technical press expects essentially that this problem is going to be with us, that is, VxWorks-related problems, hacks, and exploits are probably going to be with us, essentially forever.  It will be until the vulnerable devices are retired.  We know that some of them are going to get updated.  But, you know, look at Windows that has a fully mature patching system in place, where you have to just not prevent it from updating, and it does.  And even there, months after fixes are available, they're not fixed.  Well, we know that light bulbs that have embedded OSes in them, and our household thermostats, who knows, you know, what's in the higher end IoT devices is very likely a copy of VxWorks is the embedded OS in these things.



So anyway, this is, I mean, unfortunately on a per-device basis there's nothing end users can do except follow the advice of keeping your stuff patched.  And I know that, once this podcast is done, I've got a bunch of stuff I'm going to be checking up on.



LEO:  Wow.



STEVE:  Wow.  Two billion devices.  And really, you know, the takeaway moral of this is that the reason this happened is they were using an unaudited networking software for 13 years.



LEO:  Yeah.



STEVE:  Yes, this guy says, "Hey, it's not our fault.  We bought this from somebody else."  Uh-huh, 13 years ago in '06 you bought it.  And you never let anybody see the source.  You kept it closed.  And as a consequence, this happened.



LEO:  Yup.  Well, let's hope the people fix it and that the Mars rover isn't hacked.  Although that'd be an epic hack.



STEVE:  Oh.  Yup.



LEO:  We do this show every Tuesday about 1:30 Pacific, 4:30 Eastern, 20:30 UTC.  It's a fun one to tune in and listen to live, if you can, at TWiT.tv/live.  On the other hand, if you wait until the transcript comes out and download the audio, you can read along, and that might make it a little more comprehensible.  Sometimes you need those aids to understand everything that goes on in this show.  This was a pretty straightforward show, I think.  You'll find all of those at Steve's site, GRC.com, the Gibson Research Corporation, GRC.com.



He also has, of course, SpinRite, the world's best hard drive maintenance and recovery utility, available for purchase there.  Lots of other stuff, all of it free, including the latest about SQRL, and his Perfect Paper Passwords.  ShieldsUP!, I think the single most-used network utility in the history of mankind.  It's got to be up there, anyway.  All of that at GRC.com.  Steve's on the Twitter at @SGgrc.  You can DM him there if you've got a question, or go to GRC.com/feedback.



We also have audio and video of the show, if for some reason you want to watch it, at our website, TWiT.tv/sn for Security Now!.  You could subscribe, too, in your favorite podcast application.  Every single one will have Security Now!.  And if you subscribe, the advantage of that is you'll get it automatically.  Even if you forget, oh, it's Tuesday, it'll automatically appear on your phone or device.  And that way you can listen on Wednesday.  Steve, thank you.  And we'll see you next time on Security Now!.



STEVE:  Thanks, buddy.  Bye.



Copyright (c) 2019 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#726

DATE:		August 6, 2019

TITLE:		Steve's File Sync Journey

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-726.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we look at a widespread false alarm about Facebook's planned subversion of end-to-end encryption, still more municipality ransomware attacks, more anti-encryption saber-rattling among the Five Eyes nations, Microsoft's discovery of Russian-backed IoT compromise for enterprise intrusion, Chrome 76's changes, this week's Black Hat and DEF CON conferences, a bit of miscellany, and closing the loop with our listeners.  Then I want to share my recent experiences and findings about the challenge of synchronizing a working set of files between two locations, and the tools I settled on.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  We've got lots to talk about, including a rare attraction from Bruce Schneier.  We'll talk about the Five Eyes.  Are they going to get what they want?  Steve and I think probably so.  And Georgia, once again on my mind.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 726, recorded Tuesday, August 6th, 2019:  Steve's File Sync Journey.



It's time for Security Now!, the show where we cover the latest security news, we teach you a little bit about how this stuff works, we talk a little bit about sci-fi, whatever Steve Gibson's in the mood for because he's the man in charge at Security Now!.  Hi, Steve.



STEVE GIBSON:  Leo, it's great to be with you again, as always, for our 726th episode.



LEO:  Yikes, shmikes.



STEVE:  This is our annual pre-Black Hat and DEF CON episode because those happen starting tomorrow is Black Hat and DEF CON.  And there's no question that next week's podcast will be, well, it's going to be the second Tuesday of the month, so maybe there'll some news, but we're all kind of getting inured to, okay, update because otherwise the sky will fall.  But Black Hat and DEF CON always bring us just sometimes weeks of stories to catch up and talk about.



I've provisionally finished a multi-month quest.  Because I ended up with a solution, with several false starts, that I'm really happy with, I had it on my mind.  I thought, you know, I've got to stick this in miscellany.  I want to tell our listeners about this journey because I think people would find it interesting.  Some people might find it useful.  And it ended up, when I started to put it all together, there was enough there that I just made it the title of our podcast.  So today's podcast is titled "Steve's File Sync Journey."



And briefly, the background is that I'm now working a substantial amount of time every day in two locations, here where I've always been for we're closing in at the end of 14 years of the podcast, and I bought my home here in 1984, so 35 years.  But I have another residence now with Lorrie where I spend the evenings.  And she's a therapist who often is seeing clients in the evenings, so we both work through the evening.  We'll take a break for dinner, and then I'm there, and I have another four or five hours' worth of time, which is great for me because I love what I do.  I love to work.



But I was having this problem of needing to keep files in sync in two locations.  Which you'd think would be pretty simple.  And in fact I've come up with a solution that other people had already found, but I wanted to make sure there wasn't anything else.  So I want to explain the things I tried and the solution I wound up with, and also one that may be even more interesting for people who can do a completely roll-your-own solution.  So we'll talk about that.



But we also have - I wanted to address a widespread false alarm about Facebook's planned subversion of end-to-end encryption that many of our listeners tweeted me about, and then the person who was being retweeted said, whoops, my bad, to his credit.  We also have, believe it or not, still more municipality ransomware attacks.  We have some increasing anti-encryption saber rattling, this time among the Five Eyes nations that got together.  And I think that may have been what the video we played of Bill Barr last week, that might have been where his speech occurred was during this Five Eyes nations meeting because he was there.



We also have Microsoft's discovery, which they will be further describing tomorrow, I think it is, at Black Hat, of a Russian-backed, state-sponsored IoT compromise which is being used for enterprise intrusions.  So in other words, the IoT devices are the point of entry.  And so we're going to talk some more about network segmentation and Virtual LANs as a means for dealing with that in the real world.  Also Chrome 76 happened last week with a couple new welcome features.  We've got the Black Hat and DEF CON conferences, some miscellany, a little bit of closing the loop with our listeners.  And then, as I said, I want to talk a little bit about, you know, we've not talked about cloud security stuff since, well, actually we touch on it from time to time.  But remember Jungle Disk, Leo?



LEO:  Oh, yeah, which was S3, yeah.



STEVE:  Yeah, exactly, good...



LEO:  Then they got acquired, which kind of...



STEVE:  Yeah, yeah.  Very good memory, Leo.  And of course now we've got bandwidth galore.  We've got super cheap mass storage.  So anyway, I think another interesting podcast for our listeners.



LEO:  And I probably have a few things to add to the file sync security because...



STEVE:  I had a feeling you would. 



LEO:  You know, you have been in two locations, but I've been in many locations for many, many years.  And I have literally, I think, tried all of them.



STEVE:  Cool.



LEO:  And I have a roll-my-own solution, too.  And I've tried, well, we'll talk about it.



STEVE:  Yeah, cool.



LEO:  So, yeah, I'm very curious to see what your conclusions were.



STEVE:  So our Picture of the Week, I just, you know, these are just too fun.  I would really love to know what the story is behind these because we've seen them before.  I had this one in my collection of pictures to pull out for fun when there's nothing more topical.  So here we have the security door with the wired glass, breakproof glass, and the combination lock with five buttons - 1, 2, 3, 4, 5 - and a big Enter button down below, above a big twisty handle.  And of course on the glass it says:  "New Door Lock."  Then it says:  "Push the #2 and #4 at the same time.  Then push the #3 and press Enter."



LEO:  Clearly not a secure facility.



STEVE:  And okay, you know.



LEO:  That solves that.



STEVE:  I just don't - and in fact, if you look, Leo, you can sort of see...



LEO:  I can see the person taking the picture.



STEVE:  Well, yes.  You can see their reflection in the glass.  But also, if you look around the perimeter of the lock itself, there was a different shape lock when the door was last painted.  And so it does look like the lock was changed.



LEO:  It's new, brand new, shiny new lock.



STEVE:  Yeah.  And so, but okay, what's the story?  You know, I mean, is it to slow people down?  Is it a fire door?  Is it like, I mean, it's not secure because despite the fact that...



LEO:  Why even put a lock on it if you're going to do that?



STEVE:  Exactly.  I just don't get that.



LEO:  You could put "New door lock.  Call me if you don't know the combination."



STEVE:  Okay.  Or maybe it's a reading comprehension test?



LEO:  Most of our burglars are illiterate, so yeah, yeah.



STEVE:  I don't know what the deal is.



LEO:  Yeah, yeah.  Geez, Louise.



STEVE:  So Bruce Schneier's blog, which generated a tweet storm, was titled "Facebook Plans on Backdooring WhatsApp."  And that's such big news that it was to be the title of this week's podcast.  I had it, I mean, I immediately grabbed it, grabbed the link, read the story, thought oh, my god.  Thanked a few people who tweeted the news to me when I saw it.  And I had provisionally titled today's podcast.  But Bruce subsequently learned, from sources who are far more reliable - and Leo, you'll get a kick.  You'll be smiling when you learn who it was that gave him the bad information because I similarly was fooled by this same organization a few months ago, and you said no, Steve, they're not very reliable.



LEO:  Bruce fell for it, too, huh?  Oh.



STEVE:  Anyway, so he found out from sources more reliable, though not from Facebook directly, that the story, which was sourced from Forbes...



LEO:  Oh, there you go.



STEVE:  Yup.  And in fact, when I followed the links, I got a big kick out of that even more so; but I'll get to that in a second.  Which brings new meaning to the term "extrapolation" because that's what this Forbes guy did.  To Bruce's credit, he immediately corrected the record, once he was confident of what was actually going on.  And of course many people know that I often quote Bruce.  He's credited with one of my favorite bits of pithy wisdom, which is, and our listeners have heard me say this, "Attacks never get weaker.  They only ever get stronger."



LEO:  Yeah.



STEVE:  Which just keeps, unfortunately, being true all the time.  So anyway, for those who tweeted and may not have seen his follow-up, I wanted to make sure that the record was corrected.  So the bogus Forbes article was titled "The Encryption Debate Is Over, Dead at the Hands of Facebook."  And again, just to make sure everyone knows, that's not true.  The four relevant paragraphs of what is now the quite thoroughly debunked Forbes column, within which this author, the Forbes column author twice cites his own previous, also bogus Forbes articles as supporting references, and which quite reasonably set Bruce off last week, those four paragraphs read:



"To solve this problem, Facebook announced earlier this year preliminary results from its efforts to move a global mass surveillance infrastructure directly onto users' devices where it can bypass the protections of end-to-end encryption.  In Facebook's vision, the actual end-to-end encryption client itself, such as WhatsApp, will include embedded content moderation and blacklist filtering algorithms.  These algorithms will be continually updated from a central cloud service, but will run locally on the user's device, scanning each cleartext message before it is sent and each encrypted message after it is decrypted.



"The company even noted that, when it detects violations, it will need to quietly stream a copy of the formerly encrypted content back to its central servers for further analysis, even if the user objects, acting as a true wiretapping service.  Facebook's model entirely bypasses," he's writing, "the encryption debate by globalizing the current practice of compromising devices by building those encryption bypasses directly into the communications clients themselves and deploying what amounts to machine-based wiretaps to billions of users at once."



Now, if you read that, I mean, it sounds reasonable on its face.  You would think, oh, my god.  And that's what Bruce read.  And so he wanted to give this broader attention, which he did.  And within the links to that text is a link to the same author's previous story in May, on May 28th, which was titled "Facebook Is Already Working Towards Germany's End-to-End Encryption Backdoor Vision."  And to support that, he links to two other of his even more previous articles.  One was "Facebook's Edge AI Content Scanning Brings NSA-Style Surveillance and Censorship to the Planet," and second article was "Deep Learning Will Be the End of End-to-End Encryption."



So I don't know if this guy's got some bug up his you know what about Facebook, or if he's starving for things to write about.  I don't know what the back story is.  But, I mean, it ends up that, I mean, and I was curious.  So I read all of that.  And, for example, in one of those supporting stories he writes:  "Even more worryingly, Facebook's presentation alluded to the company's need to covertly harvest unencrypted illicit messages from users' devices without their knowledge and before the content has been encrypted or after it has been decrypted, using the client application itself to access encrypted-in-transit content."



And then he says:  "While it stopped short" - "it" meaning Facebook - "stopped short of saying it was actively building such a backdoor, the company noted that when edge content moderation flagged a post in an end-to-end encrypted conversation as a violation, the company needed to be able to access unencrypted contents to further train its algorithms, which would likely require transmitting an unencrypted copy of the user's device directly to Facebook without their approval."  So this is sort of a mishmash of supposition and the extension of allusions that are being made.



Anyway, so one thing that occurred to me as I was reading all of this is to ask the question, is it clear that point-to-point, user-to-user, one-to-one, end-to-end encrypted conversations should be moderated in the first place?  I mean, the idea of an encrypted conversation being moderated in any way does sort of seem to fly in the face of the whole concept of a totally private two-party dialogue.  You know, I'm having a conversation with one other person over an encrypted channel.  So the idea of that being moderated seems, like, odd to me.  It's not, I mean, maybe you don't want to use WhatsApp?  I mean, maybe it's clear that it's moderated, and there's going to be some sort of a banner?  I mean, I don't know, the whole idea of a two-party end-to-end encrypted conversation having moderation by Facebook seems antithetical.



But whatever the case, Bruce, I think, can certainly be forgiven for believing the extrapolations of this Forbes author.  And I guess also Facebook does share some responsibility in this because, based on their past and all of the privacy abuses we know that they have engaged in, it's believable to imagine that they might do this.  But just for the record, it is wrong.  Bruce updated his original blog posting, adding at the bottom, he said:  "Edited to add" on August 2nd.  He said:  "This story is wrong.  Read my correction."  And then he explained what happened.



So anyway, we know that something is going to be happening before long because there is this tension now between encryption and government that isn't going to be going away.  What we don't know is how it's going to end up being resolved.  And again, Facebook, the CEO or CTO, one of the top guys at WhatsApp, did officially respond and say no, we don't have any - the Forbes stories are completely incorrect.  We have absolutely no intention of doing anything like that.



So anyway, I can understand that Bruce would do that.  If I followed a story, and it seemed factual, I would report it.  And then, you know, he fixed the record.  And we've done the same things in our errata section a few times in the history of this podcast.



LEO:  Yeah.  And, you know, the guy, I'm looking at his bio, has - he's got good credentials.  So I think that the author of the original Forbes article just misinterpreted, misunderstood.  I'm not going to - I wouldn't want to impute bad faith on his part because, I mean, he was a Google developer expert for the cloud platform, senior fellow at GWU's Center for Cyber and Homeland Security.  I mean, he's done a lot of good stuff.  He's not a hype master.  I don't - I don't know, but I don't think.



STEVE:  It may well be that his bringing this to light...



LEO:  It's a good thing, yes.



STEVE:  Yes, yes, that it may well have headed Facebook and WhatsApp away from considering doing something like this by immediately flooding it with light and then allowing them the opportunity to say, whoa, no, that's not what we meant.  We never intended to do that.  Which would be good.



The Five Eyes alliance we've spoken of from time to time.  It's an alliance of national intelligence partners whose members are Australia, Canada, New Zealand, the U.K., and the U.S.  And the rhetoric surrounding this issue of encrypted messaging is heating up.  There was a story last Tuesday in the Telegraph which was headlined, "Facebook is threatening to hinder police by increasing encryption."  And the U.K.'s new Home Secretary is - I guess I would pronounce her name Priti, P-R-I-T-I, Patel.  The Telegraph reports that, in the first intervention by a minister, the new Home Secretary says the tech giant, meaning Facebook, tech giant's plans to introduce end-to-end encryption on its messaging platform would benefit, and here she's saying "child abusers, drug traffickers, and terrorists plotting attacks."



Writing for the Telegraph, she says it would prevent law enforcement agencies investigating and tracking down lawbreakers by enabling criminals to hide their messages.  And of course, as we know, and what she was responding to, was that Mark Zuckerberg in March announced what he framed as a major change to Facebook.  And we were joking about it at the time, Leo, saying they ought to just scrap everything that they have, rather than trying to fix what they have, and just start over because, you know, they're discovering logs that code had made of unencrypted passwords and all this craziness.  But the point was that Mark had said that they were going to be essentially further encrypting, like, everything else, not just having an encrypted WhatsApp app, but adding the same sort of protections to Facebook Messenger and Instagram, as well.  And apparently this is the move that has apparently set everybody off.



And of course this prospect is unanimously seen as bad news by the Five Eyes nations.  And this warning, if that's what it was, by Patel came two days after a meeting that she hosted.  She hosted a Five Eyes meeting in London with Geoffrey Cox, who's the U.K.'s Attorney General.  And that's where I think that probably our own Attorney General spoke because in attendance were security and law enforcement officials from all of the Five Eyes nations, who unanimously agreed that they were worried about high-tech companies moving to - and what they said was in quote from their meeting - deliberately designing their systems in a way that precludes any form of access to content, even in cases of the most serious crimes.



So the meeting published a report, or what they called a "communique," coming from the meeting, calling for backdoors.  And of course this is a problem we have is that, you know, "backdoor" is a heavily weighted or freighted term.  They said:  "Tech companies should include mechanisms in the design of their encrypted products and services whereby governments, acting with appropriate legal authority, can obtain access to data in a readable and usable format."



In September of 2018, so just about almost a year ago, the Five Eyes governments had called on their governments to demand that tech giants build some sort of technology in.  And they were saying that they would be insisting upon it by force, if necessary.  From a memo that the Australian government issued on behalf of the pact a year ago, they said:  "Should governments continue to encounter impediments to lawful access to information necessary to aid the protection of the citizens of our countries, we may pursue technological, enforcement, legislative, or other measures to achieve lawful access solutions."



So anyway, Reuters covered this and also spoke with a former senior European security official who said that the Five Eyes is using very general language at this point, at best to demand for government access in telecom systems and wanting some sort of a way of drilling a hole through encryption.



LEO:  This battle is coming.  We know it's coming.



STEVE:  Yes.  It absolutely is.  And as I have said, and I'll just reiterate one last time, that the counter argument I think is sort of equally wrong or overblown on the other side, which is the so-called experts on the security side are all arguing that this cannot be done.  There's no way to do this safely.  And again, I agree that, if we define what the government wants in the way that a backdoor has always been, I mean, that term has been used, no one would disagree that that's bad, the idea that the encryption itself would be compromised so that there was the golden key or the master key or something that allowed unilateral decryption of the content.



And it may be as a result of the pushback which has already come from the tech community that we're seeing an improvement and a tightening of the language, where governments are no longer talking about golden keys.  Now they're saying that, you know, even Bill Barr last week, he was arguing against warrant-proof encryption.  Meaning that, if a warrant were served on some entity, there would be some means of obtaining some content.  Well, and as I last said when we talked about this, I think it's crucial to separate policy from technology because I am sure that the technology can provide whatever we decide we want from a policy standpoint.  So rather than arguing - and that's been one of the problems is that the politicians hear the Silicon Valley geniuses saying it cannot be done; there's no way it could be done.



Well, the politicians know that's not true.  And they're right that it's not true, that there isn't a means to do this if we want to.  So again, what we have to do is just decide what it is we want, reach an agreement, and then implement that from a technology standpoint.  The technology can do anything we want it to do.  We have all the crypto bits we ever need in order to pull this off.  It's just a matter of, as we've said, doesn't have to be a backdoor.  Doesn't have to be, you know, someone said, I have it in my show notes - Wizner.  Who's Wizner?  Oh, Ben Wizner, an expert in national security law with the ACLU, the American Civil Liberties Union.



LEO:  Yeah, I've interviewed him.  He's great.



STEVE:  Yeah.  But he said that, if the U.S. and other nations get access to private messages, Wizner told Reuters in an interview, that means that adversarial nations such as Russia could demand that they get the same access.  Well, no, that doesn't mean that.  I mean, we could decide if we want to give that to them.  But again, it's entirely possible to provide access with whatever degree of control we want.  It's not easy.  And I won't argue that it's not as secure because, yes, if you're going to add a mechanism to selectively decrypt, by definition that's less absolutely secure.  But that may be the cost, you know, the price if we're going to be operating in an environment where we need to abide by the laws of a given government in order to have the technology that we want the rest of the time.



LEO:  You've talked about this before, but I think it's important that we at some point talk about how it would be done in a way that doesn't compromise security for non-governmental actors, in other words, allow hackers in. 



STEVE:  Right.



LEO:  I know you're convinced that that's possible, but I think you have some convincing to do because you seem to be one of the few that thinks this.  So at some point we should delve into that.  You've talked about it before, and I understand your point of view.  But I think you're an outlier in this.



STEVE:  Yeah.  And you know me.  I don't mind being an outlier.



LEO:  No, not at all.  And I think you're probably right, to be honest.  But I just think it would be worth talking about it.



STEVE:  Yeah, yeah.  I mean, you know, the example I've used in the past is - and this is "an" example.  This is not universally applicable because not everybody has the control that Apple does.  But when we talk about iMessage, the idea that Apple is managing the keys, and in fact this is what the U.K. has talked about with this Ghost Protocol, where there would be the ability to add a silent additional party to the conversation.  Right now, iMessage supports many-to-many multiway messaging, not just one to one.  And many of these protocols are able to operate that way.



So Apple is managing the keys for us.  Apple could create an additional entity that would be joined to an iMessage group that wouldn't appear.  And that newly created entity could be created under warrant from the government, and the government given access to that entity as a silent partner, as an invisible participant in an iMessage conversation.  In which case they would receive all of the transactions back and forth, and it would be - it doesn't compromise other communications that that user has.



LEO:  As long as Apple keeps the keys close to its vest, obviously.



STEVE:  Exactly.  Exactly.



LEO:  So I think everybody would...



STEVE:  And note that Apple is doing that now.



LEO:  Well, that's the point.



STEVE:  Apple has all of those keys now.



LEO:  That's the point.  I think anybody would stipulate, yeah, okay, but that's not going to be the end of the government's requests because what they really don't like is end-to-end messaging.  As long as a corporate entity that they can subpoena, that they can serve a warrant to, has the keys, they're fine with that.  What they don't like is end-to-end encryption, which means the only person who has the keys are the parties in the conversation.  Is there a way to make that breakable without compromising security?



STEVE:  No.



LEO:  No.



STEVE:  No.  And this is to your point, Leo, that you brought up correctly last time, which is it's math.  We already have the ability to do unbreakable encryption.  And so...



LEO:  I'd be happy to give the government, since they already have it, the right to do whatever they want to do with non-end-to-end encryption - Telegram and Apple's Messages and, well, WhatsApp is end to end; right?  WhatsApp, nobody holds those keys; right?  It's using Open Whisper's protocols; is that right?



STEVE:  It's been a while since I looked.



LEO:  I think they use the Signal messaging protocol.  Signal, let's give the Signal example.  Signal, no one has the keys.



STEVE:  I do know that WhatsApp is using Signal, yes.



LEO:  In that case there is no key escrow.  There's no one holds that key.  You are the key, you and the - the only parties who could access that would be the parties involved in the conversation.



STEVE:  Well, because that's the design of it.



LEO:  So what would you do then - see, the government wants WhatsApp.  That's what they want.  I don't think they really care about Apple Message because they can already serve a subpoena to Apple.  They can warrant Apple and get that message. They know that.



STEVE:  Yeah, yeah.  They want...



LEO:  They want WhatsApp.



STEVE:  Exactly.  And so I don't know that Moxie would ever be moved to...



LEO:  No, I know he wouldn't.



STEVE:  ...to add that technology.  But what we're talking about here, I mean, it's serious.  It would be the government outlawing, formally outlawing encryption that it cannot serve a warrant on in order to obtain access.  It would be against the law.  And, I mean, the government's made mistakes before.  Remember when - I remember you couldn't use more than 40-bit keys once upon a time.



LEO:  Right.  They called it munition.



STEVE:  Yes.  And so 128-bit keys existed, but you couldn't go outside of the U.S. with more than 40 bits of encryption, presumably because...



LEO:  Which led to people wearing T-shirts with the code for strong encryption and exporting it that way.



STEVE:  Yes.



LEO:  It's pretty hard to tie that down.



STEVE:  So imagine a world where a law is passed by the U.S. government that says, at some point, I mean, and there'll be some sunsetting period, you know, like by 2025 it is against the law to use encryption that there is not a means for the government to obtain a warrant for access to.  Now, at that point Facebook needs to decide, are they going to give up encryption, or are they going to soften their encryption?  I mean, Signal's open source.  So again, and there are other smart people.  So it might be that we have no choice but for Apple and for Facebook and for Google, I mean, major commercial organizations to use warrant-compatible encryption...



LEO:  I think that's what's going to happen, yeah.



STEVE:  ...as the only choice.  And of course the counterargument is good.  And that is that, well, bad guys will use illegal encryption.



LEO:  And they'll use Signal, yeah.



STEVE:  Well, yes, yes.  And yes, they will.  But still, the government will think, well, this is the best we could get; you know?  At least...



LEO:  This is my fear.  That's not what the government will think.  That my government will say, well, that's not enough, we still can't see every communication.



STEVE:  Well, in that case the only thing they could do would be to put something in our ISPs which block anything that the ISP cannot decrypt.



LEO:  Oh, that's interesting.  We're done.



STEVE:  In which case, then the end user gets handcuffed.



LEO:  We're done.



STEVE:  Like then illegal encryption won't work because you won't be able to transit...



LEO:  You can't transit.



STEVE:  ...illegal encryption over the wire.



LEO:  Okay, well, you've just drawn a picture of our future.



STEVE:  I'm afraid.  I mean, I'm afraid, Leo, because this isn't going away.



LEO:  Yeah, no, they're not giving up on this.



STEVE:  The government shows no sign.  It's too easy for them to make a law.  It's like, oh, good, you know, now we have a law.  Everybody's got to follow the law now.  And it's like, okay.



So I'm starting to wonder, Leo, whether at some point I'll start passing up reports of major ransomware attacks in the same way that I sometimes don't bother reporting on every single data breach that occurs.



LEO:  Breach.  I've hit that point with breaches, man; right?  I mean, every minute there's a new breach.



STEVE:  I know.  Yeah, like there was a Capital One, like last week.  It's like, okay, 105 million, like another one, blah blah blah blah blah.



LEO:  Every day.  Every day.



STEVE:  I know.  And if that happens, it's going to be some sad business because - and unfortunately, as we know, money is the motivator, and the bad guys have figured out that municipalities have deep pockets.  So once again we have the state of Georgia.  We talked recently, just after the multiple ransomware attacks on Florida, about how Georgia's courts system had at the time just been hit.  In the most recent attack, the Georgia Department of Public Safety, the DPS, was hit.  And this encompasses the Georgia State Patrol, the Georgia Capitol Police, and the Motor Carrier Compliance Division, which performs safety inspections.



As a result of the attacks, many police functions have been thrown back to the days before the Internet.  Steve Nichols, the Chief Technology Officer for the Department of Public Safety, said that they were forced to take all servers offline while the Georgia Technology Authority investigates the attack, including email servers and servers that support the department's public website and backend.  As a result, state troopers have had to resort to old-school law enforcement.  If a trooper is out on a highway, writing a ticket, they now have to use a pen and paper.  Remember that?  A pen and paper instead of a tablet.  Or, if they're looking up a license plate, they have to radio it in to a dispatcher instead of, again, using a tablet.  So, like, all of their network technology is down.



So anyway, the Georgia State Police, the Georgia Capitol Police, the Department of Motor Vehicle Safety, all have had to switch to older radio and phone systems, the way they used to in the old days.  I mean, so literally thrown into, like, pre-Internet.  And for some reason Georgia has seen more than, I was going to say more than its share, but now that we have Florida maybe it's about on a par.



Back in March of last year, the first attack that we reported on destroyed years of stored police dash cam video, as well as freezing systems.  That outage rippled such that a week later Atlanta was still rescheduling court dates.  Police and other employees were writing reports by hand.  Residents couldn't go online to pay their bills for, like, water, or their parking tickets.  That attack a year ago was the SamSam ransomware.  And I got a kick out of this because I was looking back at it.  The ransom demanded to provide the decryption keys was almost quaint by today's standard.  They wanted 52K, $52,000 in bitcoin.  They didn't know what to ask for.



LEO:  Guess not.  



STEVE:  And as it turns out, the perpetrators of that attack were a pair of Iranian men who were tracked down and indicted in the U.S.  We talked about Georgia's court systems being encrypted by Ryuk.  And somebody tweeted to me, and I forgot to write it down, so I don't recall.  What he was guessing was the way to pronounce it.  I think it was - could it be "Reyook?"  I don't remember.



LEO:  I don't know.  I mean, it's made up.  Nobody - there's not a...



STEVE:  Yeah, right.



LEO:  They just do it from, like, strings in the code and stuff.



STEVE:  Something that is found in the code or the way a file is named or something.  So a few weeks ago, on July 17th, the government of Henry County in Georgia, which is...



LEO:  Oh, wait a minute, though.  Wait a minute.  Ryuk is a character in a Japanese Manga series, "Death Note."



STEVE:  Ah, I think that's it.  Yes, yes.



LEO:  So there must be a Japanese pronunciation of Ryuk.  That would be, I guess, correct.



STEVE:  And so it's not Russian, even though it kind of looks Russian.



LEO:  No.  It sounds Russian, yeah.



STEVE:  Yeah.  Anyway, so also Henry County in Georgia, which is the fastest growing county in the metro Atlanta area, announced that it had been hit by malware, and that the county officials lost access to the Internet and most online services.  And four days before that, on the 13th of July, the police department in Lawrenceville, Georgia, was hit with ransomware where the attackers encrypted most of the department's data, including body camera footage.  So, what, like five attacks in Georgia?



And so in covering the news of this, Sophos had some reporting.  And they finished with some bullet point guidelines.  They said pick strong passwords and don't reuse passwords ever.  Make regular backups.  Sophos wrote, they could be your last line of defense against a six-figure ransom demand, which we now know is no longer 52K that these guys are asking for.  Patch early.  Patch often.  Lock down RDP.  Amen to that.  They said criminal gangs exploit weak RDP credentials to launch targeted ransomware attacks.  Turn off RDP if you don't need it.  Yes, by all means.  Use two-factor authentication or a VPN, as I have said.  Hide your RDP.  That was the title of our podcast in the last couple weeks.  And then they said use anti-ransomware protection.



What was missing from their list of bullet points, and I think is glaringly obvious, is educate and test your employees.  Which is something we've talked about often, and I think that's important.  I mean, there really has to be an effort among the government municipalities to explain to everyone that they will be - they, individually, will be extremely inconvenienced by the loss of computation and network resources.



Maybe to drive home the point, hand out pencils and ask them to practice.  Explain that the number one most common entry vector for malware into organizations, especially municipal agencies, is email which is baiting them to click links, and explain that this is not like being at home because deep-pocketed and insured government agencies are now being actively targeted by foreign attackers, and that all some one person needs to do is click on a malicious link in an email, or open a PDF and allow its macros to run, or open a Word document and release it from protected mode, and the entire municipality can get taken down.  So it's not just like spam now.  People in government are being targeted.



And then the final thing that is often missed, but we've also talked about this, is test.  Send out baiting email from your IT department, disguised to look like something people should open, to your own organization, to your own government group, written to look like something that they should open, and see how many people take the bait.  And then hold them to account.  Certainly the word will spread, you know, at the water cooler that, oh, my goodness, I just got brought in, my manager just called me into his office because I clicked on email that was designed to test me.  Make sure you don't do that.  And so, again, I think education and sending out some of your own probes to test to see whether your people can be fooled is powerful protection.  And that's something that I think ought to be part of a plan.



In something of a tease for its full disclosure presentation which will be occurring later this week during Black Hat USA 2019 in Vegas, Microsoft reported yesterday that it had detected Russia's Strontium Group, and we know them by I think about 12 different names.  The most popular alternatives beside Strontium is APT28 and Fancy Bear, but there are several others that they've been named by.  Microsoft detected that this state-backed, Russian-backed group are targeting, have successfully targeted VoIP phones, printers, and video decoders.  And so these are considered IoT-class devices which are being used to provide them with an entry point into enterprise networks from which, once they gain a foothold there, they then pivot onto the networks' higher value targets.



Microsoft's Threat Intelligence Center said that attackers have been observed in the wild since they first spotted the campaign in April.  Microsoft's researchers spotted Strontium attempting to, quote, "compromise popular IoT devices across multiple customer locations," where the hacker group exploited a VoIP phone, an office printer, and a video decoder that had not been updated.  And I guess there are some printers which are publicly exposed to the Internet?  I mean, that seems crazy to me because we've talked about how insecure printer firmware often is.



LEO:  Well, there's a lot of printers on the Internet; right?



STEVE:  Yeah, that's what I mean, printers on the Internet.



LEO:  Mine is because of Google Cloud Print and stuff like that.  But they're behind routers.



STEVE:  Yeah.  So as long as they're behind routers, like the printer has reached out to a server where it is maintaining some presence.



LEO:  That's right.  That's how Cloud Print works, yeah.



STEVE:  Okay, good.  Good, good, good.  So it's not something like that Shodan scan will turn up.  And of course remember that we know that printers are exposed because there have been, we've talked about it, hacks where people are printing, like, warning pages to your printer saying "Your printer's been hacked, sucker."



LEO:  Or "Please to subscribe to PewDiePie."



STEVE:  That's right, it was the PewDiePie guy.  Yes.



LEO:  Yeah, yeah.  By the way, not to interrupt, but I have spoken to Masako, who works here.  She's Japanese.  Get ready for this.



STEVE:  Uh-oh.



LEO:  In Japanese, there is no "R" sound.  So it's pronounced Dyuku.  So from now on, when you say Ryuku, it's Dyuku.  



STEVE:  I'm not saying that.  Nobody will know what I'm talking about.



LEO:  No.  Dyuku.  Okay.  There you go.  I'm just telling you what she told me.



STEVE:  Leo, I think Skype is dropping out.  I didn't hear that correctly, yeah.  So anyway, Microsoft said:  "The investigation uncovered that an actor had used these devices" - that's VoIP, office printer, or an un-updated video decoder - "that used these devices to gain initial access into corporate networks.  In two of the cases, the passwords for the devices were deployed without changing the default manufacturer's passwords" - guys, at least make it hard for the state-sponsored Russian bad guys to get in - "the passwords were deployed without changing the default manufacturer's passwords.  And in the third instance, the latest security update had not been applied to the device."



Microsoft said that after gaining entry through the compromised IoT devices, they would then scan for vulnerable systems to expand this initial foothold.  They said:  "After gaining access to each of the IoT devices, the actor ran tcpdump to sniff network traffic on local subnets.  They were also seen enumerating administrative groups to attempt further exploitation.  As the actor moved from one device to another, they would drop a simple shell script to establish persistence on the network which allowed extended access for their continued hunting."



Microsoft said it identified and blocked these attacks in their early stages, so its investigators weren't able to determine what Strontium would then attempt to steal from the compromised networks.  And again, I'm sure Microsoft had some network surveillance technology they were probably providing some enterprise security.  They saw this happen, and they blocked it, as you would want them to because, unless it was a honeypot, which would have been nice to have, you know, you would want to keep them from exfiltrating anything further.



Strontium's targeting of IoT devices, as we know, isn't new.  That same group previously created a botnet of tens of thousands of home routers using the VPNFilter malware, which we talked about a few months ago.  And in addition to Strontium, other state-sponsored groups have also started targeting IoT devices, primarily routers at this point.  Microsoft plans to reveal more information about the Strontium April 2019 attacks later this week at the Black Hat USA 2019 security conference.  They'll be talking about the indications of compromise that they found, IP addresses of the Strontium command-and-control servers.  And actually armed with that information, organizations might want to block those on their networks, which is always useful.



Remember that one of the CIO people in Florida said - oh, in fact it was one of our listeners.  It was our listener who wrote, when we talked about those attacks, that his group were automatically blocking Russian IP networks at their firewall so that there was no way even clicking a link would bring up something from a Russian IP address.  You know, it's not perfect protection, but why not?  Because you probably don't want the people in your municipality to be either deliberately or, much more likely, inadvertently bringing up Russian websites.



Okay.  So what's our takeaway from IoT devices being compromised and serving as a pivot for deeper penetration?  Again, it's something we've talked about before, and that is network segmentation, which is really the solution.  And of course the problem is it's not easy.  It's not the default.  It's not automatic.  What's automatic is just plugging things into your network and oh, look, it works.  It lights up.  Everything's connected.  Unfortunately, yes, everything's connected to everything else.



I have here in my show notes, the easy automatic default is that, as Khan put it in the second Star Trek movie, "The Wrath of Khan," we're all one big happy family.  And he said that just before he blew the crap out of Kirk, who had not raised his ship's shields for exactly that reason, because it was another ship of the Federation was approaching and hadn't responded to call signs or hails that had been issued.  And Kirk said, that's mighty peculiar, I wonder why?  And of course Khan said, oh, yeah, we're all one big happy family.



So unfortunately, the point is you cannot trust the VoIP system that you have sharing your common network, especially if you don't change the default login credentials for your VoIP system, and you give it an Internet-facing presence for whatever reason you might have.  So that's a perfect example of a subsystem in an enterprise that can be on its own network.  And network segmentation can be accomplished using Virtual LANs, VLANs.  VLANs now have been available and around for so long that they are widely supported.  You do need smart switches within an organization which supports VLANs.  But those don't cost more.



There is some overhead in terms of management and setting them up and configuring.  They're not as, again, they're not as default and automatic as just clicking wires into LAN switches.  But they provide you with physical network segmentation.  Well, physical at the packet level.  The idea is that the Ethernet packets contain a 12-bit VLAN tag which is assigned to the Ethernet packet as it leaves the network adapter.  When it then travels to the switch, the switch knows which VLAN the wire on its port is connected to and so is able to block any traffic that isn't that VLAN.  So what it allows you to do, it essentially allows you to create within a single physical electrical LAN network, you are able to isolate SubLANs, that is to say VLANs, within the entire LAN.



So, for example, all of your VoIP devices could be on their own Virtual LAN.  In that case, if the VoIP device were compromised, then a scan of the network would only show other VoIP devices.  There would be no servers.  There would be no workstations.  There would be nothing else because probably your VoIP system doesn't need to have contact with your printers, for example, or your enterprise servers.  They don't need it.  And so the point is, unfortunately, the default is everything sees everything else.  And if that's the way your network is set up, and an IoT device, some security camera is plugged into your network, then it sees everything else.



Well, it's probably the case that a security camera doesn't need to see everything else.  So it should be on a separate VLAN, that is, its traffic should be tagged with a Virtual LAN tag.  Since they're 12-bit tags, you can have 4,094 of them.  The zero tag and the all ones tag are reserved.  So otherwise it's be 4,096.  So it's 4,094.  So, I mean, you've got lots of tags.  Everything can, in groups, can be connected to its own subnet and thus blinding, for example, your IoT devices to your enterprise servers, and maybe your workstation networks.  Or if departments don't need to see other departments, you could create VLANs for use within a department.  And then it is possible to soften the rules on a per-machine basis where you need to.



But anyway, I just wanted to sort of put this notion on everyone's radar.  We have not talked about VLANs a lot from a security standpoint.  Because the network VLAN tagging occurs at the Ethernet level on the device driver, or sometimes the tagging can be done by the smart switch so that it's completely outside of the realm of the attacker.  There's nothing the attacker can do in order to see or mess with that VLAN tag.  So it can be a high-security solution.  It has a big overhead.  I mean, I don't mean to be minimizing the management overhead because it really increases the complexity of your network.  Suddenly now there's another big layer of additional work that is to establish and then to manage lots of subnetworks on your LAN.



So nothing works automatically when you just click it into a network switch.  Now you've got to think about where you want to have it participating, what networks it wants to be a part of.  But as we know, good security isn't free.  Real security comes at some cost.  And it just seems to me that this is another one of these things where this is the direction we're heading in.  We're moving into a world where - we used a term the other day about IoT.  It wasn't Internet of Trojans.  It was something, Invitation of Trojans or something, I can't remember what the term was that I used [Installation of Trojan SN-721].



But the point is that, in a large enterprise, where you have lots of things going on, you cannot trust - you should not trust, you cannot trust - the absolute security of every single device on your network.  And now, where there is tremendous pressure on the part of the bad guys getting in, they're not just wanting to set up a botnet or to use your servers as a proxy.  They're wanting to encrypt everything that they can get their hands on and then hold your organization for ransom.



So it is also the kind of thing that you can start gradually.  As long as you have VLAN-aware switches, that would be the first thing to do, if you don't, would be to rotate the hardware through.  So there's some hardware expense.  But you could start moving things over gradually into individual virtual LANs.  It's not the kind of thing where it has to be an all-or-nothing proposition.  So it can be done incrementally.  So anyway, I just wanted to talk about it because it's not something that we had ever talked about before.



And it's, you know, we've talked about network segmentation.  My favorite router was the little Edge router because it had individual NICs instead of just being a switch to creating multiple ports.  Each of the ports was a NIC.  And my now current favorite is the SG-1100 from Netgate.  I love that.  And it's got two ports on the inside so you could easily segment to a WiFi and a wired and put them on separate networks, or an IoT network and a non-IoT network and have those on physically separate networks.  So it's not easy, unfortunately, but we're in a world now where we just can't trust the devices that we have on our network.



Last week Google released Chrome 76 for, you know, across all of the platforms - Windows, Mac, and Linux.  And two things happened.  Thankfully, Adobe's Flash, which as we know has caused so much destruction and disruption for decades, they made another change.  I mean, they're sort of lowering the boom on Flash.  They're not going to fully discontinue support for Flash until the end of 2020.  But here we are, a year and a half away from that, and so they've gone another step.  It had been the case that it was disabled by default, but you could enable Flash on a per-site basis.



That changed last week with Chrome 76.  Now, even had you previously enabled Flash for some sites, Chrome will no longer remember that.  Meaning that users of Chrome will now be forced to permit Flash to run on an "every single time they use it" basis.  So it still will work.  But it's just basically upping the ante further.  It's making it further burdensome to run Flash on sites that require it, which clearly is Google's way of pushing back even harder on any site.



I ran across one the other day.  I can't remember now what it was.  The entire site was a Flash site where you would just go there, and it said - all you got was a page saying - and Leo, you remember those days, I mean, it was like, you know, it hasn't been that way for a long time.  But it would be like, "You need to load Flash in order to use this site."  Somebody wrote the entire website in Flash.  And without it, you didn't have a site.  And so, you know, fortunately those days are long gone.



But anyway, with any luck there will be enough annoyance from users that the sites that are still, for whatever reason, running Flash, they'll either just - maybe they just don't matter at this point, if they're still running Flash, and they'll just fall by the wayside.  But for now, every time, with Chrome 76, you've got to say yes, I want to enable Flash.  And it'll work until you come back the next day, and then you've got give it permission again.



And the other thing that they did is something that we talked about while you were on - it was on your week off, Leo, that I talked about it with Jason, which is that there are sites that have pay walls which plant a cookie on their visitors' browsers in order to track how many times they view what would otherwise be behind a pay wall.  And you get a little note saying, you know, this is your third article that you've viewed this month.  This is the last one you can view.  Please consider paying.  The point is that they are storing a little bit of history in the user's browser.



Well, apparently people realized that Incognito mode would automatically flush that, and the pay walls would allow you to have an infinite number of free views.  That word spread.  And so in order to counteract that, the sites that are behind pay walls, and I think the Washington Post is one of them, would refuse to work at all if they sensed you were in Incognito mode.  Well, that was worrisome because people felt that having sites know they were in Incognito mode wasn't very incognito.  That is, they didn't want it to be obvious that they were using Incognito mode.



Well, it turns out that what was happening was that in Incognito mode Chrome disables the file system API.  And there are sites which are now checking for the functioning of the file system API as a means of sensing whether Incognito mode is in use, and that's what brings up the notice saying, sorry, we're happy to have you use our for-pay site, even on a trial basis, but not in Incognito mode.



And so anyway, with Chrome 76, this changed such that it is no longer possible for those sites, or any sites, to probe with the file system API to determine if you're visiting them in Incognito mode.  And Google has said, if sites start doing something else in order to sense Incognito mode, then they will respond again, so don't bother.  If users want to be incognito, they want to be incognito, and we're going to support their ability to be so.  So anyway, not a huge incremental change in Chrome, but just something.



Oh.  I have a link.  I'm confirmed to present SQRL in two and a half weeks at Southern California's Orange County OWASP meeting, which is on August 22nd.  So I did want to make a note for any of our listeners who might be within reach of Orange County, that I'll be presenting sort of the history of SQRL, where it all came - how it happened, all of the considerations that went into it, how it works, the whole kit and caboodle.  And I also did mention that I'll be in Dublin, Ireland and Gothenburg, Sweden in September.



LEO:  This is the world tour now.  And of course Boston in October.



STEVE:  And Boston in October.



LEO:  By the way, we have just learned who the fourth person on our panel will be, and I'm really thrilled to say it's going to be Bill Cheswick, "Ches," the man who invented the firewall, wrote one of the very first books on firewalls.  He's a fantastic security expert.  And like you...



STEVE:  I know the name, yeah.



LEO:  Ches is a polymath in all sorts of directions.  And he's going to be a great - he's been writing a lot lately about what's next after the password.  So it'll be really good to have Ches on that panel.



STEVE:  Very cool.



LEO:  Yeah, it's going to be a fantastic panel.  We'll give you more details soon.  It's not - all the details aren't in completely.  But it'll be the first week in October in Boston, and it will be open to the public.  And we'll let you know as soon as people can sign up to come see it.  It'll be Steve Gibson, the CISO of LogMeIn Gerald Beuchelt, and now we know Bill Cheswick, and myself.  



STEVE:  Very cool.



LEO:  And then that could be another opportunity for us to talk about SQRL because authentication is the topic, you know, how to prove you are who you say you are, and what's after passwords.  So couldn't be more right on with SQRL.



STEVE:  Yeah, nice.



LEO:  Yeah.



STEVE:  So I got a tweet from a listener, Joshua Kelley, who said:  "Hey, Steve.  I know you've talked about Firefox Send in the past on Security Now!.  I found this great command line tool for interacting with it."  And I just wanted to give our listeners a heads up.  It does look very cool.  It's multiplatform.  It's on GitHub.  It's github.com/timvisee/ffsend.  Probably if you just google "GitHub ffsend" for Firefox Send.



LEO:  Well, you just go to send.firefox.com.



STEVE:  Well, that's the browser based.  



LEO:  Oh, I see, for the command line; right.



STEVE:  Yeah, yeah.  So GitHub says:  "Easily and securely share files from the command line.  A fully featured Firefox Send client."



LEO:  This is somebody's third-party version of it.



STEVE:  Correct.  So it uses the Firefox Send API.



LEO:  By a POST command?  Yeah, yeah.



STEVE:  Yeah.



LEO:  Oh, okay.



STEVE:  And so Linux, Mac, Windows, FreeBSD.  Other BSDs might work.  You need a terminal and an Internet connection.  And so I just - I thought it was cool.  I mean, I'm a fan of Firefox Send.  I have the client for the one I was using.  I don't see it in front of me right now.  Oh, yeah.  No, it's not FreeFileSync.  Maybe it's - no.  I'm not seeing it.  But anyway, the problem with it was that the free version of whatever the thing I was using, it would only give you a couple files in some period of time, and then it would say no more for you.  Whereas Firefox Send is limited to a gig file if you don't log into Mozilla, 2.5GB if you do, but there's no limit on the number that you can use.  So I just thought I just wanted to - I know that we have listeners who like command line things.  So this is a command line tool for using Firefox Send, which I thought was very cool.



LEO:  Yeah.



STEVE:  And Leo, let's take our last break, and then we're going to talk about file synchronization.



LEO:  I feel like you could probably use cURL or something like it to do a POST to Send.  I'm going to have to look at that.



STEVE:  Well, there is an API, and it involves client-side encryption.



LEO:  I'm looking at this code.  There's a lot of code here.



STEVE:  Yeah.  Client-side encryption and synchronization and pulling the API.  Oh, and you're able to type in a history, and they'll show you a history of all the things that you've sent.



LEO:  So Send encrypts on your end in JavaScript.  Is that correct?  Yeah.



STEVE:  Right, yes.



LEO:  I've used for years Transfer.sh, but I don't know how well encrypted it is.  You're storing a file on a third party's drive.  Although you could have your own Transfer.sh running on your system.  I've used that for years.  Anyway...



STEVE:  Ah.  Filemail.  That's the service.



LEO:  Oh, yeah, that was the big fat one, yeah, yeah, yeah, Filemail.



STEVE:  Yes.  That was the one I would - that's the one that I was thinking of.  But I switched over to Firefox Send because it's a win.



So as I mentioned at the top of the show, I have two workstations now where I spend a substantial amount of time.  I don't need to keep everything synchronized.  So I don't need, like, cloning of drives or something.  I only wanted to have synchronized the things that I'm actively working on at each location.  And of course I have plenty of my own resources.  So my first thought was to make a Drobo.  Actually I have Drobos at each location also.  So my first thought was to make a Drobo here at my primary location available to my secondary location.  So that would be like my own cloud, and then both workstations would synchronize to it.



So I tried that first.  I experimented with a number of directory synchronization utilities and tools.  I did not find the one when I was initially looking that I ended up finding, that I will talk about at the end here.  So I was looking around for some way of, with mapped drives, of keeping things synced.  I found something that I liked a lot that's just called FreeFileSync.  And so I want to touch on it for our listeners because it might solve some problems or be useful for some people's purposes.  It's free, as its name sounds, FreeFileSync.  It's FreeFileSync, S-Y-N-C, dot org.  It describes itself as a folder comparison and synchronization software that creates and manages backup copies of all your important files.



Instead of copying every file every time, FreeFileSync determines the differences between a source and target folder and transfers only the minimum amount of data needed.  FreeFileSync is open source software, available for Windows, Mac, and Linux.  And I liked it a lot.  I was using it for a while.  And when used manually, it's pretty terrific.  It scans, and you can see what it proposes to do before it does it.  It's got lots of fancy rules and all the exceptions and controls and bells and whistles that you could want.



But I was never able to get it to work automatically.  And obviously, for reliable synchronization, you don't want like a timed recheck for synchronization, or even like synchronization on shutdown.  You want it just to be happening constantly in the background.  I was never able to get - it sort of has like an automated system, but it just was kind of funky and didn't work right.  Then finally, a couple times, I had forgotten to manually perform the sync to the Drobo before I left one end or the other.  And so I was annoyed by not having the latest copy of my files that I wanted to work on.  So that wasn't the right solution, obviously.



I have secure tunnels between my two locations.  That's easy.  I'm using OpenVPN.  So I did for a while consider mapping a drive - so in that model I had sort of the Drobo as the common store, and I was synchronizing both workstations to that single Drobo.  So that was a problem.  But then I considered mapping a drive through a tunnel so that my secondary location would simply reach into the drive of my primary location.  And so I sort of wouldn't, like, have any synchronization problem at all.  But of course that meant that the end that did not have the files, that was reading them through the network, would have to be continuously reading and writing over the Internet.  So that was obviously not the right way to go.



And I should mention, too, that it wasn't my intention when I began to solve this problem to make this a career.  I just, I mean, I was like in the middle of working on this stuff, and I thought, okay, I just - I don't want to be copying this with a thumb drive and then taking it to the other location, which I was initially doing.  That's dumb in today's world.  So the first thing I thought was, okay, I'll just turn on Google Drive.  I'd used it in the past.  I hadn't been that impressed with it.  But you get 15GB for free with Google.



As I've mentioned, I use Google Drive.  I use Google Docs to create the show notes every week.  I have a bunch of stuff up on Google Drive.  So I thought, okay, I'll download the client for Windows and see how that works.  And I had not had great success with it in the past, as I mentioned, but I was also on XP.  So since then I've moved to 7.  And I thought, well, maybe it is more friendly, or it works better or whatever.  But no.  It didn't work for me.  I was getting, I don't know if it's because of the apps I was using, but I was getting weird "file locked" errors and complaints from apps that files were locked and kind of weird behavior.  So after a while I thought, okay, well, this is an annoyance.  This isn't working.



So I thought, ah.  If anybody knows how to do a networked drive, kind of a virtual drive client, it would be Microsoft.  So I thought, what about OneDrive?  Let's give that a try.  And I was using the newer Office 2016, which was OneDrive aware.  So I was sort of seeing it offering me OneDrive stuff, and I hadn't been paying attention to that.  But it was on my radar.  So I downloaded the OneDrive client on both machines.  And I was just using OneDrive.  You get 5GB free from  Microsoft.  And again, I don't need terabytes of data.  I just - I'm only using, you know, the documents that I'm working on.  And I'm working on the third of the four SQRL documentation documents now.  And so I just, you know, or source code for SQRL also, or other projects and documents.  So I don't need a huge amount of storage.



Anyway, it worked for a while.  And then I started getting weird merge errors where it would complain.  It would say that it was unable to reconcile the differences between files at each end.  And I ought to mention that of course the way this whole idea works is that I had local copies of the files on each workstation's hard drive.  And so I was working with them locally so everything was quick and snappy and fast.  And then the idea would be that file sync would synchronize to a common third cloud store, and so it would just all happen automatically.  Except that it didn't work after a while.  Finally, it got stuck where I don't remember which of the workstations it stuck on, but all of the little icons showed the synchronizing spinning arrows, and I couldn't get that to stop.  I shut it down.  I restarted.  I rebooted.  It just was stuck.



And when I went online, it turns out - this is like a problem with OneDrive that Microsoft has had from the beginning.  Oh, and when it was complaining for a while with this merging problem, it was creating duplicates.  That's the problem that people complain about online is that Microsoft is saying, well, we were unable to reconcile the differences, so you got two copies of this.  And it's like, I don't want some sort of reconciliation of files.  I just want, you know, I'm not in two places at once.  I'm not changing two files in different locations at the same time.  I just want the newer one to replace the older one.  So that didn't seem hard, but seemed somehow to be confusing Microsoft.



So also, for quite some time, I've had a free 2GB Dropbox web account.  I've had that because sometimes I'll create or edit a video here in my daily workplace, like I'll download something from my TiVo, quickly edit out the commercials, and then it's something that I want to share with Lorrie in the evening.  So I had been sticking it on this Dropbox web account because there is an app for the Roku called Roksbox.  It's a free app, R-O-K-S-B-O-X, which is a nice little streaming thing that will stream from cloud services.  And so I would drop things in Dropbox here.  And then when I'm at my other location in the evening, I'd share something with Lorrie that I wanted to share.



So I'd sort of had Dropbox on my mind.  And I thought, you know, they've been doing this from the beginning.  Maybe they figured out how to get it right.  So I figured I would give it a go.  But 2GB is not much storage.  They're the leanest of any of the free services.  So I thought, okay, I'm going to - oh, I guess I did try it briefly with clients on both systems, just using the free account.  And it really did seem to work right.  And I really liked the features that it had.  There are a bunch of extra goodies that sort of made sense, like I could create a video folder within my Dropbox folder and mark that as "Online Only," which meant that it did not keep a local copy when I dropped something there.  It only sent it to the cloud, which was perfect for my mode of just wanting to put something there to watch through the Roku later.



And the more I got to know it, the better I liked it.  So I bit the bullet, and for $10 a month I went from 2GB to a 1,000GB, which is to say a terabyte of storage, which means I no longer need to be deleting videos all the time that we've watched because I'm running out of room.  And I've got, like, more than enough space.



LEO:  Is it just videos you want to do this with?  Or it's other files, too; right?



STEVE:  Oh, it's all kinds of other stuff.



LEO:  Because for video, I mean, running a Plex server would be the easiest thing to do, and then get the Plex client in your Roku, and you'd be done.



STEVE:  Okay.  But yes.  So much more than video.  All of my other - so all kinds of, you know, source code, assembly source...



LEO:  Stuff you're working on.  That makes sense.



STEVE:  Yeah, exactly, all the stuff I'm working on.  So the one thing I haven't talked about with all of this is encryption.  But I'll just finish by saying that, until this morning, I was completely happy.  Well, I'm still completely happy with Dropbox.  It is, I mean, in my opinion, they have nailed this problem.  It's not free.  It's $10 a month for a terabyte.  But I've been using it now for about a month, and it never misfires.  It never gets anything wrong.  It doesn't complain about synchronization.  It's never telling me that my files are locked or, like, I mean, it is beautiful.  It's got all the features I want.  I can create a directory hierarchy.  I can mark things for online and things for local storage.



I have not bitten the bullet and added encryption.  And of course we started talking about cloud storage and encryption as we mentioned with Jungle Disk back in the day, the idea of doing client-side encryption and then storing things in the cloud.  But at the moment, nothing I'm doing is secret.  The SQRL docs aren't secret.  My source code for SQRL, you know, I've given big chunks of it, and in fact the entire thing, to several other developers who wanted to use it as a reference.



LEO:  You don't use any repository-style, like Git or something.



STEVE:  No.  No.  I never have.  So I have not added TNO-style encryption to this.  Once upon a time, Boxcryptor was a favorite.  And like its name, the way its name is, it's an encryption for - it is encryption for Dropbox.  And it does client-side encryption.  And I looked at it some time ago.  What's annoying to me is that today everybody wants us to sign up to a service model.  And I'm still a little too old school for that.  I'm a little - I have to confess I'm very resistant to that.  If I'm actively using someone else's bandwidth or their storage on an ongoing basis, then a pay for what you use  model makes sense.  But for me, not for software.  I'm not ever going to do some charge, like a pay-as-you-go for SpinRite.  You get it, and it's yours.  You can use it as much as you want forever.



However, there is a free version of Boxcryptor which has otherwise also switched to the service model.  There's a free version which does allow you to use one cloud provider on two devices.  So if it does the file syncing thing correctly, and it's not something that I've looked at yet, that might be a solution for providing encrypted cloud storage if I'm ever in a - if I'm doing something where I really want TNO, you know, true TNO client-side encryption, and if it would allow me to encrypt a subset of folders under Dropbox.  Or, if not, since, for example, if it were the SpinRite source that I will be working on as soon as I wrap up the documentation for SQRL and get that fully launched, then what I could probably do, since my source code is not big, is get a different free 2GB Dropbox account for SpinRite and then use Boxcryptor in that mode from my two locations.  So I just sort of wanted to share all of that.



I was also conscious of the fact that we have a cloud storage provider who is a sponsor of the TWiT Network, Wasabi.  So I was sort of curious to see what they were up to.  I'm impressed by what I saw because I dug through all of their stuff.  They look like a very good solution as a CDN.  The fact that they don't charge for downloads is huge.  And in fact, I'm using AWS, some S3 buckets for some SQRL videos, and I've noticed that, as that's become more popular, my cost has been increasing.  That would not be the case with Wasabi.  But they're more of an enterprise-oriented CDN profile, as opposed to an end-user consumer profile.



They do export an S3 API.  And so I was curious to look and see if there was any sort of a good - and I had looked before, looked for an S3 sync provider.  There is something on GitHub called S4 for S3 solutions.  So another S, thus S4.  But it didn't look like - it was written in Python, and it didn't look like it had all the bells and whistles that I was interested in.



While finishing this discussion for this podcast, I stumbled upon four other solutions:  ownCloud, Nextcloud, Seafile, and SparkleShare.  And of them, ownCloud looks very nice.



LEO:  So you are not party to the ownCloud drama, so I'll just fill you in real quick.



STEVE:  I am not.  So tell me.



LEO:  So ownCloud, which is an open source platform, kind of got turned into a business.  And a lot of people were unhappy about what happened to ownCloud.  Nextcloud was created with a fork of ownCloud.



STEVE:  Okay. 



LEO:  And so anything you like about ownCloud should be available in Nextcloud, and then some, in my opinion.  I think at this point I would strongly prefer Nextcloud over ownCloud for a variety of reasons.  You can read about the drama.



STEVE:  Okay.



LEO:  You'll find it if you do a little search.



STEVE:  And so is it as feature-complete and a good client?



LEO:  Yeah, yeah.



STEVE:  Because what I wanted to mention was - so I will take your advice immediately, Leo.  Oh, one reason I liked it was I thought I remembered that I saw it as an app on Drobo, and sure enough, there is an ownCloud server for Drobo.  So maybe there is a...



LEO:  Yeah.  There's probably a Nextcloud client, as well.  That's pretty usually common.  But there's nothing wrong with ownCloud.  I think it's fine.  I think a lot of the drama has to do with personalities as much as anything else.  But I'll send you a link or two, and you can read it online.



STEVE:  Okay.  So what I like about ownCloud, well, first of all, so...



LEO:  There were also security issues with ownCloud which you should read up on.  I don't know if they've been resolved.



STEVE:  Oh, good, good, good.  Well, I should also mention there is no way I would ever publicly expose any of this.  I mean, I just...



LEO:  Right, so it doesn't really matter, yeah.



STEVE:  Exactly.



LEO:  Do you not have a Synology or another NAS solution?  Is Drobo your only NAS?



STEVE:  Drobo is my only NAS.



LEO:  I would look at Synology.  I think you'll be really impressed.  And a lot of these things will be solved.



STEVE:  Well, actually, yeah.  What I'm thinking of is FreeBSD and...



LEO:  Do it yourself, sure.



STEVE:  And stick a FreeBSD server up at Level 3.



LEO:  There's also FreeNAS, which is open source Linux NAS software.



STEVE:  You know, I tried that first.



LEO:  Oh, really.



STEVE:  And I didn't find that it added anything.  It was, I mean, it's on top of FreeBSD.



LEO:  Right.



STEVE:  And so it gives you sort of a GUI, but...



LEO:  Right, headless interface, yeah.



STEVE:  Yeah, exactly.  So where I ended up was, if I were a person who did not have the storage and networking and other resources, I am just bullish on Dropbox.  I mean, for, like, what - I know I'm, like, probably telling people things they already know.  I mentioned this to Lorrie yesterday, and she looked at me, and she said, "Yeah, that's what everyone uses."  I said oh.



LEO:  I love Dropbox.  I use it, too.  But I use them all.  There's also Tresorit and SpiderOak, which are both Trust No One, that you probably should look at.  I like Tresorit a lot.



STEVE:  Well, but do they do sync?



LEO:  Yeah.  Absolutely.



STEVE:  And, I mean, because that's been the problem is I had not found a really good sync solution.  So I thought that SpiderOak...



LEO:  SpiderOak is less desirable because I think it's Java based.  But look at T-R-E-S-O-R-I-T.  They also have a file send capability much like Firefox Send.



STEVE:  Okay. 



LEO:  I used them for a while.  I don't need eight different clouds.  But I was very happy with Tresorit.  And it claims to be end to end, Trust No One.



STEVE:  Okay.



LEO:  Which I know you would like.



STEVE:  Yeah, yeah, yeah.  So anyway, I had not found, until this morning, when I stumbled on ownCloud and Nextcloud, I had not found a robust sync that really seemed to work for me.  And of course I like the idea of having my own server that I'm not having to pay Dropbox $10 a month.  And I just feel much more comfortable with my own data in my own cloud.  And of course everything is TLS and secure.  So anyway, if that's of any use  to anybody, if the story of my journey to synchronize two different systems - I mean, again, maybe this is like, duh, Gibson, where have you been; you know?  But I had never had the need before.  And finally, thanks to Dropbox, I found something that works and is not very expensive, and maybe even a free solution.



LEO:  Yeah.  You know, this is a universal problem.  It's just now, because you finally left the house...



STEVE:  Yes.  That's what happened.  I finally left the house, Leo.



LEO:  Almost everybody at least goes to a job; right?  So you just stay there.  So you don't have to worry about this.  



STEVE:  Correct.



LEO:  So, yeah, I'm really interested to watch your journey on this because you're tracking through all the things that we have kind of piecemeal tracked through.  So it's nice.  It's really, in a way, you have an advantage by doing it all at once.  You can really go head to head and see what does what.



STEVE:  Well, and I was disappointed because, like, Google was having these problems, and then OneDrive was having these problems.  And it's like, I thought these things would work.  But no.  It turns out it's not an easy problem to solve.  So I have to say I will try these other things.  And I have no experience yet with ownCloud and Nextcloud.



LEO:  I think either would be fine.  There was a schism.



STEVE:  Ah.  As there sometimes is.



LEO:  In open source it's very common.



STEVE:  Yup.



LEO:  But they started with the same code base a few years ago, probably Nextcloud, and they've probably diverged quite a bit by now.  It's interesting because one thing that I do, and I think a lot of other people do, is depending on the type of data, that's why I was saying for videos I use Plex.  Video, audio, podcasts, I use Plex.  For source code, I use Git.  You know, so I think a lot of people have said, well, I'm going to, for this kind of there's this.



And then for just my standard, I have a standard data folder, a documents folder that's on every system I have.  And I have more than a dozen.  And it's hosted on my Synology NAS.  And Synology has synchronization software that I put on everything at the beginning.  And at that point I have a common documents folder that is stored on every computer.  And that is really great if you're moving around a lot and you want to make sure that whatever laptop or phone or whatever that you bring with you will have a common set of data.  And at this point it's several hundred gigabytes, I think.  I don't know exactly how big it is.  But it does a good job.



STEVE:  And so when you do that, you're not bringing local copies to the machine?



LEO:  I am.  No, no, no, no, no, no, no.  The first time I sync.



STEVE:  Several hundred gig comes onto the machine.



LEO:  Yeah.  I mean, I could if I want copy the documents folder from one machine to the other.  Sometimes I'll do that.



STEVE:  Hah, right.



LEO:  The sync is smart enough to say, oh, you have these, but you don't have this.  I don't have a big duplication problem.  The Synology does an excellent job.  It's called Synology Drive.  And it's on the Synology.  So my Synology at home has the master copy.  But this laptop, that desktop, these phones, they all have access to it and to a greater or lesser degree will have - something with enough storage will have the full documents folder.



STEVE: Right. 



LEO:  And that's great because then I never have to say - and you could use it as a poor man's Git because, you know, if I'm saving code, that's fine.  If you're working with somebody else, it's another matter.  Then shared code you want to keep track.  You want to source - you want somebody to blame, to be honest.



STEVE:  Right.  Well, of course, so one of the other nice things, I've been talking about the fear of ransomware.  And Dropbox does versioning.  So you can...



LEO:  Yes, that's nice.



STEVE:  You can reach back in time.



LEO:  I love that.



STEVE:  And on the other hand, both ownCloud and Nextcloud presumably, I know that ownCloud does, they also do versioning. 



LEO:  Yeah.



STEVE:  So you have that.  And I'm still using, you know, I'm old school.  I've talked about it before, FileBack PC.  It's only PC hosted.  But the guy's still there.  It's still supported.  And it does very sophisticated versioning that can be well tuned.  And so that's what I use right now is like, whenever I'm working in assembly code, incremental versions are going to my Drobo just all the time because I don't ever want to lose anything.



LEO:  If you went to FreeNAS or a FreeBSD solution, you'd be on ZFS.  You'd be doing snapshots, so you'd have that versioning.



STEVE:  Yes.



LEO:  With Synology, I've never had occasion to use it, but Synology's on Btrfs and so it does snapshots, as well.  So there's behind-the-scenes versioning, even lower level than just that app running.  Which I think is a very handy thing because you could say, oh, crap, I just cryptoed everything, blast it away, and restore from a snapshot.  And that would be a really nice thing to have.



STEVE:  Yeah.  I'm just in the process of getting ready to put another server up, a big NAS up at Level 3.  And now that I've run across these open source, I mean...



LEO:  Oh, there's some great stuff out there, yeah.



STEVE:  Given that it works reliably.  Again, I was hoping for Google, and I was hoping for OneDrive, and they both screwed up. So far, Dropbox is the only thing I've found.  But I'll give the ownCloud or the Nextcloud a try.



LEO:  Nextcloud will do a lot of the things that you want to do, including, by the way, for a lot of people, when Evernote went commercial, they replaced Evernote.  My Synology has a note replacement that's every bit as good as Evernote, but I host it.  Any NAS is a great backup solution, too.  And because they tend to be giant storage units, I back up all my music, all my photos, all my data on the Synology and still use it to do my notes and my documents.  So a NAS is a wonderful thing to have, I think.



STEVE:  And the one other nice thing I forgot to mention about the open source cloud solutions, ownCloud and Nextcloud, is they allow you to designate other directories on your systems to be synced.  Whereas Dropbox...



LEO:  So annoying.



STEVE:  ...everything is under, yes.



LEO:  The Dropbox folders, yeah.



STEVE:  And that's really what I want because I have a whole assembly code tree, and I would just like to be able to say, instead of having to move all of my work underneath Dropbox,  I'd like to be able to say "keep this thing current."



LEO:  I seem to remember Tresorit also will let you designate arbitrary folders, as opposed to OneDrive, Dropbox, and Google Drive, which say it's all got to be in this special folder.  Being able to designate arbitrary folders, say "back up my assembly folder and my music folder and this," is, I agree with you, very important.



STEVE:  Yeah, yeah.



LEO:  Nextcloud will also do calendaring, so you can have a shared calendar that's not running on Google.  I mean, Google works fine for me, but, you know.  Same thing with address book.  If you wanted to go really full private, that's the kind of solution, those are the kinds of solutions you want to look at, I think, running your own NAS.



STEVE:  So Tresorit Personal, oh, try it free for 14 days.  So it's not free.



LEO:  Yeah.  It's only a gig.  Oh, no, it's not free.  It's just like Dropbox.  It's a Dropbox competitor.



STEVE:  $30 per month.



LEO:  That's for teams, I think.  Look at the personal.



STEVE:  No, that's personal.



LEO:  Really.



STEVE:  Tresorit Solo is designed for individual users.



LEO:  That's ridiculous.



STEVE:  Yeah.



LEO:  But you get 2TB.  I see.  That's why.  I wonder if they have a smaller tier because that's an awful lot.



STEVE:  Yeah, that's the bottom one, designed for individual users.



LEO:  Oh, maybe that's why I didn't - wait a minute.  No, no, no, no, no, no.  200GB is $10 a month.



STEVE:  Oh, okay.



LEO:  Yeah, yeah.  I think you're looking at the 2TB, which is nice.  But if you're going to go that big, go with a NAS.



STEVE:  So 200.



LEO:  200GB is $12.50 monthly, or $10.42 if you do it annually, 120 bucks.



STEVE:  So I guess, if you needed end-to-end encryption, because that's more expensive than Dropbox, but you do get encryption.



LEO:  You know, if you're price sensitive, Google Drive's probably the cheapest.  But it doesn't do what you want it to do, so...



STEVE:  It doesn't work.  Yeah.  And actually, I'm not price sensitive.  But, as I said, I also like - I imagine a lot of our listeners are, like, already downloading copies of Nextcloud.



LEO:  Yeah, Nextcloud is pretty amazing.  And by the way, that will run on another sponsor, Helm.  They're going to add Nextcloud capability to turn Helm into a NAS, basically.  But again, if you really want to do it right, I would get a NAS.  Either roll your own, because Steve likes to roll his own, or get Synology.



STEVE:  And Leo, a NAS is nothing these days but a PC with a whole bunch of hard drives.



LEO:  Yeah.  Right.



STEVE:  And you run FreeBSD or Linux and ZFS, and now, yeah, I mean, so it's not a big deal.



LEO:  No, no, no, not at all.  I think you probably, you know, if you're going to build it yourself, you can put more horsepower in it, a lot more RAM, stuff like that.  That tends to be pricey on these commercial NASes.



STEVE:  Yeah.  And I am a little annoyed that the Drobo's processor seems to be a little slow.



LEO:  It's anemic, yeah.  



STEVE:  They did not, yeah, like you're sitting here waiting for the web page to come up.  It's like, come on.



LEO:  In Synology you can get a better - it's still an Atom.  It's not the super fastest processor.  You can put cache memory in.  You can put additional memory or a little NVMe cache card which speeds up small file writes.  I have not had an issue with Synology.  I think it's sufficient for a single user.  I don't think we would use it here for...



STEVE:  For an enterprise, yeah.



LEO:  For enterprise, yeah.  But I think it's...



STEVE:  And again, it's easy to give a big, beefy machine a bunch of drives.



LEO:  Why not, if you've got a machine.



STEVE:  Like here you go.



LEO:  Yeah, that's right, yeah.  And then you can choose your own RAID card.  You can do hardware RAID, and you can get a good Promise card or something.  You have a little more control over it.  But that takes some more work; right?  So it's whether you've got the time or not.



STEVE:  Well, yeah.  And I didn't start, I mean, I started just trying to stop carrying a thumb drive back and forth.



LEO:  You know, it's funny because you have gone through in a month the evolution that's taken the rest of us decades.  You've gone from thumb drive...



STEVE:  Only because I'm, like, so old school.  And as you said, I never left the house, so I didn't need - there was nowhere I was going anywhere, so...



LEO:  When you get around to trying Keybase, one of the things that I use and I love the best about Keybase is they have a private encrypted Git that's all yours.  And so I use that for taxes, financial information, stuff like that.  Because it's a Git, I can sync it, which is fantastic.  And it's fully encrypted, end-to-end encrypted, which I really like.  And that's free.  I don't know, eventually they've got to start charging for that.  But that's a really remarkable solution.  They don't have nearly the storage capacity, though, of these big cloud companies.  It's a great subject.  I would love to hear your continued notes as you evolve.



STEVE:  If I come up with any - and I imagine our listeners have probably enjoyed hearing us talk about these things, too.



LEO:  You'll hear a lot more from them at GRC.com.



STEVE:  Oh, I knew that was going to happen.  Oh, Steve, try this, try this.



LEO:  GRC.com/feedback or, even better, the Twitter DM.  He accepts DMs from anybody at @SGgrc on Twitter.  Of course, if you go to GRC.com, my god, it's just an endless treasure trove of information.  Starts with the bread and butter, which is of course the world's finest hard drive maintenance and recovery utility, SpinRite.  Buy a copy.  Just say thank you, Steve, I'm buying a copy.  And that way you'll get the update when it comes out.  But you'll also want to read all the free stuff he's put up there, all the information.  And you can get a copy of this show there.  He's got 64Kb audio.  He's got such a low quality audio we don't even want it, the 16Kb audio for the bandwidth impaired.



He also actually has the smallest version of the show, which is beautifully written transcripts Elaine Farris puts together every week.  So if you like to read along while you listen, that's a good suggestion.  And the best value of that is that it makes it googleable.  You can search into the shows and find the part you're looking for.  So that really is a boon.  All his shows are up there, all the shows, 726 of them are up there.



We also have audio and video at TWiT.tv/sn.  But of course we always encourage you to subscribe.  That's the best way to get our shows because that way you get it automatically.  You don't have to think about it.  It'll be on your phone when you get in the car tomorrow morning.  And you can do that in any podcast application.



It's funny, I was talking to my physician, who is a podcast listener and listens to a number of our shows.  And I don't think he knew that you could do that.  He was like, downloading it to his computer and then syncing it over to his phone.  I said, "Get a podcast app.  Any one will do.  Overcast, Pocket Casts.  Google or Apple both have podcast apps."  Subscribe.  That way you'll get it automatically.



Steve, always a pleasure.  What are you watching on TV these days?



STEVE:  We just sort of wind down at the end of the day.  Lorrie had never seen the series "Medium," which I really liked.



LEO:  Oh, what a great show.  Yeah, that was a good show, yeah.



STEVE:  Seven seasons, and we just finished that.



LEO:  So it's kind of fun to go back in time and look at the old shows.



STEVE:  Yeah.  And she also had never seen - what was the one with Simon Baker?  



LEO:  I don't know.



STEVE:  "The Mentalist" she had never seen.



LEO:  Oh, fun.



STEVE:  And that was really, really fun also.  So we're just sort of watching old stuff and staying...



LEO:  Till something great comes along, huh?



STEVE:  Exactly.  Although we're going to watch the Showtime series, the Roger Ailes.



LEO:  Roger Ailes, "The Loudest Voice" in the room.



STEVE:  That sounds fun.



LEO:  It's so good.  And to see Russell Crowe, who's a great actor, he really embodies Roger Ailes in this, in the biggest fat suit you ever saw.



STEVE:  Well, and didn't he...



LEO:  Did he gain weight, or is it a fat suit?



STEVE:  I was assuming he gained weight because I saw like a preview, and I thought, oh, my goodness.



LEO:  I have to think it's a fat suit because...



STEVE:  But his face is full.  I mean...



LEO:  Well, they put - it's prosthetics.



STEVE:  Wow, okay.



LEO:  You remember the Dick Cheney movie, "Vice."



STEVE:  Yeah?



LEO:  Same thing.  I've gotten his name.  They got one of our best actors they got to do him.  And it was all prosthetics and a fat voice.



STEVE:  Wow.



LEO:  A fat suit.  Or a fat voice.  I'll find out.  I hope Russell Crowe didn't gain weight for that because frankly, if he did, he doesn't have long to live.  It's a couple hundred pounds.  You know?



STEVE:  Woody Harrelson.



LEO:  No, not Woody Harrelson.



STEVE:  It wasn't Woody Harrelson, huh.



LEO:  He was Batman.  What's his name?  I don't know.  The chatroom.  Christian Bale.  Thank you.  The chatroom would know.



STEVE:  Ah, that's right, that's right, that's right.



LEO:  Steven, have a great week.



STEVE:  Okay, my friend.



LEO:  We'll see you next time on Security Now!.



STEVE:  Right-o.



Copyright (c) 2019 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#727

DATE:		August 13, 2019

TITLE:		Black Hat and DEF CON

HOSTS:	Steve Gibson & Jason Howell

SOURCE:	https://media.grc.com/sn/sn-727.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week, as expected, we look at some of the events and announcements from last week's Black Hat and DEF CON conference events.  Microsoft and Apple have upped the ante for bug hunters, the Chaos Computer Club shreds a hotel's door lock security, a serious philosophical design flaw is revealed to be present in 40 signed device drivers, and Google vows to continue its Incognito-mode battle.  We also have some SQRL news, some fun miscellany, and some interesting closing-the-loop feedback from our terrific listeners.



SHOW TEASE:  It's time for Security Now! with Steve Gibson.  I'm Jason Howell, filling in for Leo this week.  Steve's going to dive deep on a number of topics that arose from both the Black Hat and DEF CON conferences.  Also Incognito and publisher sites are still playing that cat-and-mouse game.  Apple's offering an insane payout for its bug bounty program.  That and a whole lot more coming up next on Security Now!.



JASON HOWELL:  This is Security Now! with Steve Gibson, Episode 727, recorded Tuesday, August 13th, 2019:  Black Hat and DEF CON.



It's time for Security Now!.  I'm Jason Howell.  But you don't want to hear from me - I'm filling in for Leo - because you want to hear from the man, the myth, the legend, Steve Gibson.  How are you doing, Steve?



STEVE GIBSON:  Jason, thank you for standing in for Leo.  He's off to - is it Orlando? - Florida for a podcast conference.



JASON:  Podcast Movement, all week, yeah.



STEVE:  I don't know that I like the idea of calling it the Podcast Movement.  But anyway.



JASON:  Yeah, that's the name of it, so...



STEVE:  I guess that's what they decided to call it.



JASON:  So that's what they're calling it, so we'll call it that.



STEVE:  I hope he's having a good movement wherever he is.  This is Security Now! Episode 727 for August 13th.  Lucky Tuesday the 13th.  This happens also to be the second Tuesday of August, so it's Patch Tuesday.  But we have no news about that whatsoever.



JASON:  Oh, no.



STEVE:  We were expecting last week that we would be talking about Black Hat and DEF CON, which concluded this past weekend, and that is indeed the case.  So we're going to take a look at some of the events and announcements from last week's paired Black Hat and DEF CON conferences.  Before we began recording we were going over whether it was one word or two and how we should capitalize the second half, and we reached no conclusion whatsoever, John chiming in that it's a puzzle.



JASON:  It's a real confusing situation, although some people in the chatroom were like, who cares?  But I care.  I want to know.



STEVE:  Okay, thank you very much.  Yes, well, we do.  It's because we care that software doesn't crash more often than it does.



JASON:  That's true.  Very good point.



STEVE:  Details matter.  Microsoft and Apple announced that they are upping the ante, in some cases quite significantly, for bug hunters, which is interesting.  We have the Chaos Computer Club, which is the group in Germany that are hacking things.  They shredded a hotel's door lock security, embarrassingly.  That also was announced at DEF CON.  We have a serious philosophical design flaw which was revealed to be present in 40 signed device drivers.  And actually, one of the best-named blog postings I think I've ever seen we'll be getting to.



Google's vowing to continue its Incognito mode battle with sites that are trying to detect Incognito mode.  We also have a bit of SQRL news, some fun miscellany, and some interesting closing-the-loop feedback from our terrific listeners.  So I think for podcast number 727, post-Black Hat and DEF CON, another interesting conversation.



JASON:  Legendary, if I could be so bold.  Possibly one of your best episodes.  Mind you, we are only beginning to record it right now.  But we were talking before the show how Black Hat and DEF CON really, you know, it's like Disneyland.  It's like Christmas for security.  So for this show, your added fuel.



STEVE:  And leave your tech at home when you go because - in fact, I think one of the favorite things that we showed last month - or, sorry, last year - was there were two maps had been made of before Black Hat and DEF CON and mid-Black Hat and DEF CON, the number of cell sites in that area of Las Vegas.  And there were, like, 15, like the week before the conference, and 50 during the conference.  So it's like, uh, okay, wait a minute.



JASON:  Somehow people are brave enough to go out there and to stay safe.  And that's just one of those conferences that, I mean, who knows?  Maybe somebody I'll be sent out there for whatever reason.  But I will be frightened if that is the case.  And I will probably want to leave my phone at home.



STEVE:  Bring a large roll of tinfoil, Jason.



JASON:  Oh, okay.  That's all you have to do.



STEVE:  Just wrap yourself, and then...



JASON:  That's not weird at all, Steve.



STEVE:  No.



JASON:  No, actually, that's normal.  That's how I ride to work every day.



STEVE:  I was going to say, especially there, they would think that was funny.  If you poke out some little holes for your eyes, you know, that'd be like, okay.



JASON:  There you go.  It's kind of like Comic-Con at that point.  What is this strange piece of technology I'm staring at, Steve?  I don't understand.



STEVE:  Yes.  Our Picture of the Week, a listener sent this to me because for the last few weeks, well, actually, yeah, at least, we've spent a lot of time talking about the ransomware which has been encrypting systems.  And last week we spent some time talking about - I was, like, trying to think about how a municipality could explain to its employees that they would really be inconvenienced if they clicked on a link in email which allowed something malicious to crawl into the municipality's network and encrypt all the computers because it was really no fun to have to use a pencil and paper like we used to in the old days.



And in fact we talked about how it was in Georgia that the State Police were now having to write tickets the old-fashioned way, with a pad and pen, rather than being able to just pull it up on a pad.  And if they wanted to run someone's license plate, they'd have to actually pick up a microphone and talk into a radio to central dispatch and read the license plate in and get the results back because literally all of that technology was down in the state of Georgia.



So what we have today for our Picture of the Week, for those who are not looking at the video feed, we have a yellow piece of paper that reads at the top:  "Computer Problems?  NO PROBLEM!  Use this manual data entry device."  And then we have the abbreviation P.E.N.C.I.L., which of course spells "pencil."  And this is "Personal Emergency Non-Computerized Information Lifesaver."  Then there's a pencil, a physical, yellow, good old-style yellow pencil has been glued to it.  And we have the opposite ends labeled.  The pointy end has an arrow pointing to it labeled "ENTER."  And of course the end with the pink eraser has an arrow pointing to it labeled "DELETE."



So, yes, and this is what some regions of Georgia are currently using as a consequence of the fact that one of their employees just couldn't resist opening that email and finding out what it was that they were being threatened with or promised or who knows what scam being perpetrated.  But anyway, thank you to our listeners for sending that.



JASON:  I appreciate the simplicity of this user manual.  It gets to the point.  There's no question.  Enter.  Delete.  It makes sense to me.



STEVE:  There is some need...



JASON:  At least they didn't have to run the information back.  At least they could call it in; right?  They didn't have to, like...



STEVE:  That's true. 



JASON:  It could have been worse.



STEVE:  Now, once upon a time it was necessary for us to keep our, well, in modern technology we have to keep our devices recharged.  So I suppose the equivalent here is needing to resharpen the pencil from time to time as it becomes dull and also shorter.  And of course we also know, if we're going to really extend this painful analogy any further, that the more you use lithium polymer batteries, which our devices have, the less remaining lifetime they have.  Similarly, the more you use this emergency data entry device, the shorter it becomes, thus similarly reducing its remaining available life.



JASON:  Thankfully, buying more of this technology is not difficult.  The technology has come down in price over the years, so it's pretty easy to stock up on it.



STEVE:  That is true.  That is true.  And you know, we really don't trust the battery charge remaining meters in our devices.  There have been scams perpetrated on how much life is actually left in the battery.  In this instance, with the pencil, again, as I said, really stretching this analogy past its breaking point, there is no doubt about how much life you have left remaining in that particular stylus.



JASON:  Yes.  And if you happen to snap it in half, it's not going to explode, so that's a positive, as well.



STEVE:  Yes, that's a very good point.  You want to be careful.



JASON:  Right.



STEVE:  Conferences have started, I guess historically they always begin with some major kickoff keynote speech.  And that was the case with this year's 23rd annual Black Hat Las Vegas speech, which was delivered by Dino Dai Zovi.  He's the mobile security lead at Square.  And of course Square has become an important player in our info ecosystem.  Dino titled his keynote "Start with Yes."  And in it he discussed the ongoing transformation of, I guess it would be, security's role in the workplace.



I mean, when you think about it, we used to have a COO, the Chief Operations Officer, and a CEO, the Chief Executive Office, you know, the so-called "C-Suite" people.  Well, there was never a CIO, but today we all have CIOs, Chief Information Officers.  So as a consequence of the fact that, first of all, information has become a commodity, and then on top of that the security of that information has not only, I mean, everyone always gave it lip service.  Oh, yeah, we've got to have secured information.  Well, now it's like a big deal.



So Threatpost wrote up a very nice and succinct summary of the high points from Dino's commentary, which I want to share with our listeners since, first of all, it's a great introduction to this year's Black Hat; but also I think that Dino touches on something very important, which is that there is this, you know, we've often talked on the podcast about the tension that exists between the security guys, who the executives sort of, I do want to say, I guess, they sort of tend to look down their nose at them.  Like security is still seen as a necessary evil.



It isn't, you know, it's sort of like healthcare.  I mean, it's not obvious that it proactively provides benefit the way marketing does.  It's something you have to have because there are hackers.  There are bad guys.  And there are, unfortunately, there are turncoat employees who could get you from the inside.  And so, and of course, the security people tend to speak their own language.  It's like, wait, did he just - would somebody repeat that to me in English, please?  I don't know what that geek just said to me about why we're not giving them enough money.



So, you know, the point is that's been the way things are.  And so Dino titles his keynote "Start with Yes."  Whereas it's so often the case, I think, that security people start with no.  They're like, of course, that's not the right way to deal with the CEO, who doesn't want you around in the first place and is trying, you know, trying to sleep at night worried that there's going to be a problem that is going to bite his organization that he'll ultimately be held responsible for.



So Dino says, and this is the summary thanks to Threatpost:  "Taking as a first principle the idea that security teams now have the ear of company boards and the C-Suite, the challenge becomes figuring out how to communicate most effectively within this newly collaborative environment, and how to have the most impact organizationally."  In other words, Dino is saying this is reality.  This has happened.  Let's succeed, and let's not try to do it by forcing our will upon those pencil necks.  Let's start with yes.



So:  "One of the first things to do is to realize that in today's software-centric world, where internal teams rely on software-as-a-service and the cloud for core missions, and where DevOps is becoming the norm, security must become a shared responsibility and resource.  Thus, listening to the 'asks' from different division leaders in terms of building security processes that don't cause friction is in many ways Job One for dedicated security personnel."  Again, let's not run off in a huff.  Let's figure out how to make this sort of inherently fraught relationship work.  Dino said:  "Saying 'yes' keeps the conversation going, keeps it collaborative and constructive, and opens the door for real change and real impact."



"Digging beyond this umbrella idea," writes Threatpost, "Dino highlighted three transformational principles for boosting the impact of security within organizations.  First, work backward from the job to be done.  Second, seek and apply leverage, develop feedback loops, and scale with software automation.  And, third, understand that culture trumps strategy and tactics every time."



So taking those three, looking in more detail at those.  Regarding working backwards from the job, which he highlighted as the first principle:  "Job theory says that a job is both a function, but also represents emotional context.  He used the example of milkshake research at McDonald's.  Market researchers found that most milkshakes were sold before 8:30 in the morning, often as the only item, via the drive-through.  In asking buyers what their motivation was, it turns out that they wanted something that would be easy to consume and would occupy a long commute."



So Dino said:  "So the job the milkshake was doing wasn't solving hunger, it's alleviating boredom on a long commute.  Similarly, security teams need to determine what the job is to be done.  Talk to internal teams.  Try to understand their struggles.  Listen.  What are they setting out to do?  What adds friction, and what makes things easier?  When and why do they interact with security?  In understanding what their 'hiring' criteria is for a security solution, as well as the 'firing' criteria, it becomes possible to build in an agile way for the need at hand, rather than spending time overlaying security principles that may or may not be useful, adopted, or practical."  In other words, fit security into what's really going on, rather than just make declarations.



And he says, under seeking and applying leverage, Dino touched on the idea of how to have the most impact with limited resources:  "Taking Archimedes's classic idea of using a lever as a force multiplier to lift an object much heavier than oneself, it's possible to see this play out in security, with automation as the lever.  For example, security is still a small community, and the problems that we tackle can be huge, using fuzzing for finding vulnerabilities as an example of scaling security's effectiveness via automation."  He says:  "We must work smarter, not just harder, through better software and by applying automation where we can."



Stressing the importance of having automatic feedback loops, Dino said:  "We have to build them explicitly, and the tighter feedback loop wins.  We have to build security services for observability, so you can understand if the protections are working and also perform anomaly detection.  We have to be able to identify attackers when they're probing, learning, attacking, and succeeding."



And finally, as for culture, strategy, and tactics:  "Culture is, of course, the term for what companies value and promote, and how its employees interact and communicate.  Without a culture shift towards embracing security, the technical aspects will fail despite any best-laid plans.  Dino explained:  'We in security are not outsiders anymore; we're inside communities and companies.  We need to use that access to improve things.'



"He noted that making security a shared issue can go a long way to creating a safer organization.  For instance, he said, at Square, security engineers have to write code just like everyone else.  He says this is a cultural change.  There's a lot more collaboration and empathy for how people are operating.  A software engineering team would write security features, then actively go to the security team to talk about it and for advice.  We want to generate generative cultures where risk and involvement is shared.  It's everyone's concern.  If you build security responsibility into every team, you can scale much more powerfully than if security is only the security staff's responsibility."



Which I think is absolutely the way it needs to go.  I mean, that makes sense.  It can no longer be something treated as an afterthought, added later, and then, like, with a lot of finger-pointing going on when something goes wrong.  This involves implementing a blame-free post-mortem process when it comes to responding to an anomaly or a vulnerability report.  Dino said:  "Turn these events into inquiries where you focus on getting at the root causes of the problems.  In the end, security teams should see themselves as an extension of internal teams, not a separate division apart."  He said:  "Instead of saying no, start with yes, and here's how we can help.  It's all about cultivating empathy.  It's something you practice and grow.  This is the way we meet the challenge of leveling up on security."



So I think his points were very well taken.  I thank Threatpost for providing their summary of it.  And of course it was with that spirit that this year's Black Hat took off.  And I think that's exactly the right position.  We are, over the last couple years, really seeing an evolution in security's role.  It can no longer be something freestanding and separate.  It's got to be something that everybody's aware of.



JASON:  Importance raising, although we continue to hear about IoT, all the IoT insecurities and these smaller companies that are creating these products but never consider the security implications of them.  And it sounds like what he's saying is, when we're creating these products or creating these companies around these products, security should be at the start of that discussion.  I mean, do you think that we're heading in that direction?  Feels like some places that hits it on the nose, and in other places that seems like a lot to ask.  Just for some companies that's never going to be top of their mind.  It's all about money more than it is about security.



STEVE:  I think that's right.  I think that security is, for example, like quality.  You will have companies that produce a quality product.  You will always have companies that produce crap.  You know, they just don't care about the quality of whatever it is they're doing.  They're operating on the principle of, well, some people will purchase it.  And since we're not spending any money to create this crap, we can squeak by and make a living just because some people will buy it.



JASON:  Right, for as long as we can get away with it.



STEVE:  Yes.  And so if you think of security that way, that is, as something that a company can produce, a company can produce security.  It can produce quality security.  Apple probably is a great example of that.  We know that Google is involved.  We know that Android is continually improving its security.  So there are organizations where security is necessarily their product.  It's, yeah, because of what it is they're offering, security and high-quality security is integral in their offering.



But similarly, even though there are companies that should have high-quality security, just as it would be nice if they had a high-quality product which had that high-quality security bound into it, there are companies that are just going to produce crap.  And their security is going to be crap.  So that's the way it's going to be.  The good news is that it used to not be the case that security was regarded as the product, that is, that a company was producing security.  And as we're about to be talking about with the, in some cases, breathtaking strides that have been announced in this year's Black Hat and DEF CON by some of the major players who recognize that security is their product, they really are making moves in that direction.  So, yeah.  Not everybody's going to have it.



Now, there are things we can do that we've talked about on the podcast.  Making security easier will go a long way to improving the security of things.  It's not exactly on point, but for example the idea that we now have the ACME protocol which Let's Encrypt was the first deployer of, and other certificate providers are beginning to deploy their own ACME protocol offerings, where a server can obtain a free TLS certificate just by running an ACME client on its platform and automatically receive certificates, well, that made security easy.  It made it simple, it made it free, and it made it easy.  As a consequence, there are a ton of ACME-based certificates now in the world and will always be.  So there's a good example of dropping the friction, making it easy, making it free.  And so it's like, okay, why not have it?  Because there's no reason not to now.



Similarly, we're beginning to see security packages targeted at IoT vendors where they're drop-in solutions that solve the problems that the IoT vendors weren't even bothering with because they didn't want to spend any money on it.  Well, if they're available, and they're on GitHub, and it's free and easy, why not have it?  So I do think that there will always be crap security.  But to the degree that, you know, as the available means of adding security improve, I think that we'll see the companies operating in the margins or on the border, where they're willing to expend a little bit of effort, above none, in order to secure their stuff, if we make it easy for them to do that.  If there are players in the industry who decide we're going to lower the bar for the good of all, then those companies can increase security, and it'll only be the real deadbeats that just don't bother.



JASON:  Right, right.  Yeah, that makes a lot of sense.  Microsoft's not a deadbeat, though.



STEVE:  No.



JASON:  Heck, no.



STEVE:  So they have dangled a $300,000 bounty, which they announced last week at Black Hat, for the discoverers, payable to the discoverers of unknown bugs lurking in their Azure cloud platform.  Which is, you know, a nice piece of change.  Microsoft launched a dedicated Azure cloud host testing environment which they call Azure Security Lab, ASL.  This ASL program allows security researchers to test attacks on this so-called "infrastructure as a service," IAAS scenarios, without impacting customers.



So these hosts, the hosts that Microsoft will be making available, will be isolated from the mainstream Azure production environments which customers use, allowing researchers to attack this ASL, the Azure Security Lab infrastructure, as much as they want to, testing live exploits without fear of bringing down or adversely affecting other Azure clients.  Microsoft's Kymberlee Price, who is the principal security program manager for the Microsoft Community and Partner Engagement Programs, wrote in a blog post about this last week.



She said:  "The isolation of the Azure Security Lab allows us to offer something new.  Researchers can not only research vulnerabilities in Azure, they can attempt to exploit them without fear."  She said:  "To make it easier for security researchers to confidently and aggressively test Azure, we are inviting a select group of talented individuals to come and do their worst to emulate criminal hackers in a customer-safe cloud environment which we're calling Azure Secure Lab."



Researchers with access to Azure Secure Lab may also attempt scenario-based challenges, meaning I guess Microsoft provides the scenarios and the researchers try to attack them, with top awards of $300,000.  Starting yesterday, that is, August 12th yesterday, researchers can apply for this access at Microsoft's website.  And on top of the ASL announcement, Microsoft announced that it is doubling its bug bounty rewards for researchers who discover Azure vulnerabilities.  At the start of this year, Microsoft announced a bug bounty program designed to find flaws in Azure DevOps, with top rewards of up to 20,000.  Now that 20,000 has been doubled to 40,000.



For those who aren't aware, Azure DevOps is a cloud service launched in 2018 that enables collaboration on code development across the breadth of a development lifecycle.  The two in-scope services for the bounty program include Azure DevOps Services, which was formerly known as Visual Studio Team Services - I guess I'm going to have to update myself because that's how I still think of it.  So now it's Azure DevOps Services - and the latest publicly available version of Azure DevOps Server and Team Foundation Server.



Microsoft has quietly been paying out some nice hefty rewards.  Microsoft announced at the conference that it has paid out, get this, a total of $4.4 million in bounty rewards during just the past 12 months across its various programs.  So yup, there's some money there.  And actually we'll be talking a little bit more about that a little bit later because they also announced the top 75 researchers.



Last month Microsoft initiated a bug bounty program, offering payouts as high as $100,000 for the discovery of holes in identity services and implementations of the OpenID standard.  These include Microsoft Account and Azure Active Directory, which offer identity and access capabilities for both consumer and enterprise applications, as well as its OpenID authentication protocol implementation.  And, of course, for a company with the cash and resources of Microsoft, spending their money like this really does make sense because why not incentivize people to tell Microsoft rather than to tell, like, sell these things on the dark.



And also, back last March, inspired by the Meltdown and Spectre flaws - we talked about this at the time - Microsoft started another new bug bounty program targeting speculative execution side-channel vulnerabilities.  And at the time when we mentioned this, I remember it was interesting, and I'll remind our listeners that it is, kind of oddly, but I guess it kind of makes sense, it's a time-limited program that will be operating only through the end of this year.  It offers a quarter million dollars for identifying new categories of speculative execution attacks that Microsoft and other industry partners are not yet aware of.



So again, researchers have been prolific the last year and a half in finding new speculative execution side-channel attacks.  So if you are someone who's interested in poking around at the Intel processor and seeing if you can find something else, there's $250,000 potentially waiting for you, if you're successful.



And in keeping with doing this the right way, on Monday Microsoft also implemented explicit safe harbor terms and conditions which clearly outline how researchers who are acting in good faith can safely report bugs without fear of facing any legal repercussions.  And of course this is something that ought to be codified into law, in my opinion, for the benefit of our cyber future.  I mean, I know that Leo gets questions, I get questions from individuals who believe they have found a flaw, and they're worried about being attacked by the people whose software is flawed as a hacker if they report it.  So we really need to fix that problem.  That should not be something people worry about.



Kymberlee Price wrote:  "Microsoft is committed to ensuring our cloud is secure from modern threats.  We built Azure with security in mind from the beginning, and work to help customers secure their Azure cloud environment with products such as Azure Sentinel and Azure Security Center.  If a situation arises, our Cloud Defense Operation Center (CDOC) and security teams work around the clock to identify, analyze, and respond to threats in real time."  So again, perfect example of a company who clearly perceives that security is not an afterthought.  It is a product that they are offering, and clearly important.



JASON:  And Microsoft's been in the business long enough to have seen probably both sides of that coin, too; right?



STEVE:  Oh, yes, yes.  Thus we call it Patch Tuesday for a reason.



JASON: Yes, exactly.  Forever, yes.  Patch Tuesday is legendary. 



STEVE:  So as I mentioned at the top of the show, one of the presentations at Black Hat I got a kick out of, only because you just have to ask yourself, how can a company do something, in this day and age, so wrong?  The Chaos Computer Club, which is a group of researchers in Germany, tackled the question of hotel key, you know, door key security.  It turns out that the latest trend in high-end, high-tech hotel door keying is something known as "mobile keys."  And in this case "mobile" as in mobile phone.



So I think all of us who travel, and I will be joining the ranks of people who travel next month, and we'll be talking about that a little bit in a minute, will be encountering probably in the future the technology of mobile keys.  So the credit card-like thing which we're used to sticking in a slot or sliding down the side of the hotel door will be switching to an app that runs in our phone, whether Apple or Android, you know, iOS or Android.  So the site that I was led to from this, and we don't know that these guys are the provider that the Chaos Computer Club attacked.  But, boy, is it typical.  It's OpenKey.co.



And so the headline reads:  "Universal Mobile Keyless Entry For Hotels Worldwide.  Improve guest reviews and gain competitive advantage with the latest hotel technology, the industry standard for hospitality technology."  And their site reads:  "The mobile revolution is here.  Guests want to use their smartphones to control every aspect of their stay, and the major hotel chains around the world are responding.  Keyless entry, mobile check-in and check-out equal a quantum leap for the guest experience.  OpenKey" - this particular company - "makes delivering on guest expectations simple, fast, and affordable."  And they say they have four steps.



Step 1:  Reservation confirmation email sent 24 hours prior to arrival contains mobile key info and a download link.  Naturally, if you're going to be using an app for your phone during your hotel stay, you need to have it in your phone.  So by sending your prospective guest a link the day before, the guest is able to go, oh, cool, I get to use my phone to get into my room.  So you download the app and so that you're ready when you show up at the hotel.



Step 2:  Guest downloads the app and registers with name and mobile number.  Step 3:  Guest can check in via mobile at a kiosk or at the front desk.  The mobile key is sent to the app upon check-in.  Step 4:  Guest uses smartphone to access their room with the tap of a key button.  What could be easier?  Unfortunately, the guys at Black Hat demonstrated Step 5:  Bad guys can waltz into any locked hotel room they choose.



So, yeah.  It sounds really great.  It reduces check-in friction.  But unfortunately, I guess we shouldn't be too surprised to learn that it wasn't done properly.  The reporting on this reads:  "Researchers developed an exploit that allowed them to perform an array of malicious functions against these so-called 'mobile keys,'" meaning the apps and their use.  "A vulnerability in a popular IoT lock key" - and again, we don't know that it's the site I just showed, but it was cited - "used chiefly by a high-end hotel in Europe" - so these guys focused on one particular European high-end hotel - "allowed researchers to break into hotel rooms.



"The locks in question are dubbed 'mobile keys' because of their reliance on mobile phones as opposed to card-based access such as those based on mag stripes and RFID.  Researchers at Black Hat USA 2019 showcased how they were able to circumvent an Internet of Things-connected key system utilized by an unnamed European hotel."  And the name of the hotel and the specific IoT system were not identified for safety reasons, as the locks are still deployed in the hotel and have not been updated.



The researchers explained:  "We went to do the one thing a mobile hotel key is supposed to prevent:  wirelessly sniff someone entering his room or just unlocking the elevator, and then reconstruct the needed data to open the door with any Bluetooth Low Energy (BTLE) enabled PC or even a Raspberry Pi." Okay.  So without knowing any of the details, this is horrifically, ridiculously horrific because it is such a trivial problem to solve with a proper design.



JASON:  And you apparently have the solution here.



STEVE:  Well, a solution.



JASON:  A solution, okay, you're right, yeah.



STEVE:  Yeah.  And that's the problem.  Or that's the annoyance is that, in this day and age, this problem is so trivial to solve.  So from what we just heard, at its root, we're talking about some form of classic replay attack.  All that's needed to prevent this, for example, is for the door, when challenged to unlock, to provide a nonce for the phone to sign and return.  The door contains a software ratchet.  In this instance that would be a counter which feeds a secretly keyed AES symmetric cipher.  So of course that turns the counter into an unpredictable sequence of, for example, 256 bits.



Each door lock is configured with its own secret key, which is never exposed.  The AES cipher which encrypts that counter produces a public elliptic key which is used to verify signatures.  So the door lock, when challenged, first checks the signature for the key that is currently valid, the one that it's been using, assuming that, you know, like if it didn't know that the previous guests had departed.  So it assumes that.  It checks to see if the signature returned from its nonce is one that it's expecting.



If that fails, it checks ahead, because it's able to increment the counter to produce successive candidate public keys; right?  So it checks ahead to the next public key to see whether that one can verify the returned signature.  If not - and it doesn't only have to test one ahead for robustness.  It might test a couple ahead.  But if not, if it doesn't find a signature that it's expecting, it simply ignores the request.  But if the next key does successfully verify the signature on the request of the nonce that it sent out, it makes that next key permanent, ratcheting forward, thus forgetting the previous guest's key, so that a previous guest can no longer unlock the door from the instant that the next guest has successfully been able to do so.



So, and what this little simple system that I just proposed does is it means that the door locks have no need to communicate with the hotel.  Each door lock is able to operate autonomously with its own secret key forever, which determines the sequence, as I said, of its public keys.  The hotel system, the hotel's master system knows each room's secret key, so it's able to autonomously issue the proper private signing key to each guest for the proper room.



And if the system is designed correctly, no one with, like in the case of the Chaos Computer guys, no one who had a copy of the mobile key software and the ability to eavesdrop on the conversation would be able to gain any advantage in doing so because this is inherently, thanks to the fact that the door always challenges a request to enter with a nonce, which needs to then be signed by the private key contained by the mobile key software which it received at the kiosk or during check-in, there's no chance for having a replay attack.



And what's annoying is that this is Crypto 101.  Right?  This is, you know, this is like not even in the final exam.  This is in one of the early tests in Crypto 101, is design a system, blah blah blah blah.  So, and this is what I've often said on the podcast, that our modern cryptographic tools that we have today are so powerful and so cool that they can provide any kind of nifty functionality like this that we want.  And, for example, it's all available for free in the well-tested and audited libsodium library on GitHub, which runs on all platforms.



So I just have to scratch my head when some, I mean, the website is beautiful, looking at this one particular company.  Again, we don't know that they're the ones who had this problem.  But I was going to say, if they spent one tenth of the money designing their website that they did designing the technology that they're trying to promote with this website, I was going to say that, but it doesn't even - it's free.  It doesn't cost anything to have state-of-the-art technology like this any longer.  I just - I don't understand it.  It's crazy. 



JASON:  And they're a company with security as, like, the main component of the business; right?



STEVE:  Exactly.  Right.  That's what they're selling is locks.



JASON:  Kind of ties back to what we were talking about at the very beginning, you know, with the keynote and the grand vision of companies putting security first.  But still, even when the company is about security, apparently it's not important enough to put it through those proper paces, I guess.



STEVE:  No.  And that simple system that I just outlined is, I mean, it's so cool, I almost want to go start a hotel lock company just so I can write the code to do this, except that, again, it's not even worth doing, it's so simple.  Unbelievable.



JASON:  That should not be a reason to stop you, though, Steve.  I would love to see you championing SQRL and whatever you would name your new hotel lock business simultaneously.  I think that's possible.



STEVE:  So there was another presentation that was at DEF CON, which followed Black Hat, which generated some frightening-looking tech press headlines.  But there was an interesting moral to this.  Okay.  So I grabbed three headlines from the press, all reporting on the same story.  First one:  "Driver disaster.  Over 40 signed drivers can't pass security muster," read one headline.  Another one:  "Researchers find security flaws in 40 kernel drivers from 20 vendors."  And the third one:  "Over 40 drivers could let hackers install persistent backdoor on Windows PCs."  All of that was true.  No hyperbole.  No exaggeration there.



And when you stop to think about it, okay, think about drivers.  It does make sense that this could be a problem.  Device drivers occupy a sort of security loophole in our systems today.  They're not the primary OS, so they don't get the scrutiny that the OS gets from its vendor.  They're provided by random third parties who probably have the best of intentions, you know, they don't want to produce an insecure driver.  But the developers there are likely operating under pressure to ship, and their focus is on the driver working, and not crashing, and being stable, not on it being bulletproof against direct attack.  And, finally, the drivers often run down in the kernel alongside the OS with direct access to the system's hardware, the physical hardware, and the high privileges that such direct access requires.



So, I mean, it's sort of like the perfect storm.  It's not the OS.  It's written by random third parties to make their stuff go.  The OS sort of has to accept it, I mean, like agree to take it in, in order to be friendly and compatible with whatever thing the user needs the driver for.  And the driver has to, by definition, have kernel-level access permissions.  So it was sort of inevitable that we would have a problem.



In ZDNet's words - they wrote about this.  ZDNet said:  "At the DEF CON 27 security conference today in Las Vegas, security researchers from Eclypsium" - I like that, Eclypsium [E-C-L-Y-P-S-I-U-M] - "gave a talk about" - and this is, as I said, there's sort of a moral here.  There's like a double whammy ooh - "gave a talk about common design flaws" - that's what kind of surprised me.  Not, like, 40 different problems, but a common design philosophical flaw "they found in more than 40 kernel drivers from 20 different hardware vendors.  The common design flaw," ZDNet wrote, is that low privileged applications can use legitimate driver functions to execute malicious actions in the most sensitive areas of the Windows operating system, including the Windows kernel.



"Mickey Shkatov, principal researcher at Eclypsium, told ZDNet in an email earlier last week:  'There are a number of hardware resources that are normally only accessible by privileged software such as the Windows kernel and need to be protected from malicious read/write access from user space applications.  The design flaw surfaces when signed drivers provide functionality which can be misused by user space applications to perform arbitrary read/write of these sensitive resources without any restriction or checks from Microsoft.'



"Shkatov blames the issues he discovered on bad coding practices which don't take security into account.  He said:  'This is a common software design anti-pattern where, rather than making the driver only perform specific tasks, the driver itself is written in a flexible way to perform arbitrary actions on behalf of user space.'"



Okay.  And so when I read that, I was taken aback.  That's way worse than what I was expecting.  This means, essentially, that the driver is an interpreter that is accepting commands from user space and following them.  To me, that's unbelievable.  Now, what that means is it's an easier way for a developer to write a driver, is just to sort of create their own little interpreter so the driver is a shim into the OS to allow it access to the kernel, and then their userland application will use that to sort of, like, make it up as it goes along.  Which is unconscionable.



Anyway, Mickey explained:  "It's easier to develop software by structuring drivers and applications this way" - uh-huh - "but it opens the system up for exploitation."  And I'm just - I'm astonished that that's what it turns out we have in our systems right now.  Shkatov said his company has notified each of the hardware vendors that were shipping drivers which allow user space apps to run kernel code.  Oh, my goodness.



Vendors who issued updates are listed below.  And they are, in looks like alphabetical order, AMI, American Megatrends International; ASRock; ASUSTek Computer; ATI Technologies, which is to say AMD; Biostar; EVGA, the display guys; Getac; GIGABYTE; Huawei; Insyde, I-N-S-Y-D-E; Intel (what?); Micro-Star International, MSI; NVIDIA; Phoenix Technologies; Realtek Semi; Supermicro; and Toshiba.  Shkatov told ZDNet:  "Some vendors, like Intel and Huawei, have already issued updates."  Well, good for them.  That's good to know.  "Some, which are IBVs" - which are independent BIOS vendors - "like Phoenix and Insyde are releasing their updates to customer OEMs."  So they'll go through that second-level channel.



Mickey said he did not name all the impacted vendors because some needed 'extra time due to special circumstances,' whatever those are, and future fixes and advisories will be released in the future.  He said he plans to publish the list of affected drivers and their hashes on GitHub after his talk so users and administrators can block the affected drivers.  In addition, Shkatov said Microsoft will be using its HVCI, which is the Hypervisor-enforced Code Integrity capability, to blacklist drivers that are reported to them, that is, to Microsoft.  However, Shkatov said that the HVCI feature is only supported on the 7th gen Intel CPUs and subsequent.  Manual intervention will be needed on older systems, and even on newer Intel CPUs, where HVCI can't be enabled.



Microsoft said, and this is unimpressive CYA nonsense, in my opinion:  "In order to" - this is Microsoft.  "In order to exploit vulnerable drivers, an attacker would need to already have compromised the computer."  Eh, okay, not really.  The attacker would need to have userland presence, but that's easy.  Microsoft said:  "To help mitigate this class of issues, Microsoft recommends that customers use Windows Defender Application Control to block known vulnerable software and drivers.  Customers can further protect themselves by turning on memory integrity for capable devices in Windows Security.  Microsoft works diligently with industry partners to privately disclose vulnerabilities and work together to help protect customers."  Okay.  Which is to say someone asked the techies, uh, what can we say about this?  What do we have in our stuff that can kind of help?  And this is what they came up with which is, as I said, nonsense.



Okay.  So the talk was on Saturday.  The day afterward, last Sunday, the Eclypsium guys published additional details in what has to be, as I noted at the top of the podcast, one of the best titled blog postings, if not of all time, at least in recent times.  It's titled "Screwed Drivers - Signed, Sealed, and Delivered."  And there is both - I have it in the show notes - a link to the blog posting, and also they have the whole thing available as a PDF where they go into additional details.



So, unbelievable.  We have, basically, we have lazy drivers which allow code running in userland.  And remember that a driver, once installed by something, is globally available to any application that knows how to access it, and any application can, which are essentially written to make it easy to do and are way too capable of reaching into the system and performing mischief.  And the problem, of course, is that they're going to persist in systems that have them installed and do not update for a long time.  So we may be hearing about these problems moving forward in the future.  Wow.



JASON:  No kidding.



STEVE:  Not surprising, but really, really irresponsible.



JASON:  And I'm looking, I admit I'm looking a little bit further ahead because I knew that this would end up being a ping-pong battle, which is basically the battle between Google and any sites trying to lock down their content in a paywall.  You had to know that this was going to be a tennis match that was going to keep hitting back and forth, and yet here we are.



STEVE:  Yup, it's going to be cat and mouse, Google's battle to allow its Incognito users incognitoness to be incognito.  We've talked about this before.  We saw this, we knew this was going to be happening in their announcement of the features in Chrome 76.  As we know, and I'm now using Chrome 76, as is everybody who's been updating recently, release 76 was intended to close a loophole that various commercial paywalled websites were using to detect when their visitors were viewing the site through their web browsers' Incognito mode.



As we know, Incognito detection had been implemented because Incognito mode inherently flushed the simple-minded cookie histories that were being used to permit a paywall access compromise and tease, where a limited number of pages could be viewed before the site's paywall would slam shut to block further access.  The understandable feeling was that, if a visitor wished to be Incognito when surfing the web, they should have that right.  And that should also extend to include the fact that they were choosing to be Incognito in the first place.



And as we know, we talked about this before, prior to Chrome's release 76, Incognito mode simply disabled the JavaScript file system API.  Since this was trivial for a site's scripting to detect, Incognito mode visitors were being blocked from having any access.  So it was essentially - so if you visited The New York Times, and I think the Washington Post, with Incognito mode, you wouldn't get anything.  You would be blocked, saying you are using your browser's Incognito mode.  We'd be happy to have you come visit, but please come back with Incognito mode turned off.



Well, that annoyed everybody.  So essentially it was let us store our crap on your computer, or you don't get to see any of our site.  Because of course that's what cookies are is stuff that a website is storing on your computer.  So what Chrome 76 now does is implement a RAM-based file system API so that the file system appears to work, rather than just being like returning an error, it appears to work.  But it's inherently volatile because it's just in RAM.  And thus it won't store anything permanently.



So we know what happened next; right?  A RAM-based file system won't behave exactly like a nonvolatile file system, and that makes it detectable.  And yes, some sites such as The New York Times immediately adapted their code to do just that.  Chrome's file system API presents a smaller maximum file system size.  It says that it's going to be - that there is 120MB of available file system.  When the script, the JavaScript queries the file system API on how much space you've got, Chrome 76, rather than saying, error, file system API not available, it says, oh, we're here and open for business.  We've got 120MB.



Well, it turns out that's a fixed number.  And this is not what's seen when the browser is not in Incognito mode.  So that's one feature.  Also, writes to RAM present a consistent timing compared to writes to physical media, where the timing will vary.  Both these incognito detection bypasses have been demonstrated.  And as I mentioned, they've been found to be used in the wild.  In the show notes I have a snippet which was taken from The New York Times website.  And it says in the JavaScript code:  "Quota for an incognito Chrome window is a fraction (10%) of the device memory with an upper limit of 120MB."  Then it says, "More info:" and gives a link to someone's posting, bypassing-anti-incognito-detection-google-chrome.html.  And then INCOG_MAX_QUOTA = 120, which is exported as a constant.  So cat and mouse.



Bleeping Computer reached out for their reporting of this to Google to inquire about these two new Incognito detection methods.  They were told that Google stands by their previous statement and position, that they will "work to remedy any other current or future means of Incognito mode detection."  So we have another new cat-and-mouse game afoot.  And this is dumb, since this is a game that Chrome can and will ultimately win.  Chrome can trivially simulate the varying timing of reads from and writes to volatile media.  And they can remove the fixed declared RAM-based file system size limitation trivially.



So in Chrome 76 I'm sure they'll do that.  And with any luck, the guys who are doing the web design at the other end will tell their bosses, you know, we could do something else, but then they'll just go around that, too.  So as Google said, if sites wish to be paywalled, they should require all visitors to create an account.  Once that account is created, then some free access could be metered out under the site's control.  But since visitors could still create throwaway accounts, a credit card or other payment means would probably also be needed to provide an anchor to that user's identity.  And all of this would, of course, clearly reduce the site's traffic, since many users would choose to go elsewhere rather than to create yet one more account anywhere.  And it would also mean that no one could use the site without creating at least a temporary account.



This is all clearly a mess, which arises from the fundamentally irresolvable conflict inherent in the goal of wanting to provide some access to visitors who wish to have complete anonymity.  Right?  I mean, that's what this comes down to.  It's an irresolvable conflict to provide some access to visitors who wish to have complete anonymity.  I would argue, and Google does, that anonymity ought to be a feature that a web browser can offer.  And that falls in fundamental conflict to the fact that, to provide some limited access, having some way of identifying a visitor is necessary.  Which means they're not completely anonymous.  So I say bravo to Google.  And to websites I say, good luck with that because this is not a game they're going to win.



JASON:  Yeah.  And of course sites are going to be hesitant to implement some sort of like a mandatory account creation for their visitors because a lot of publisher sites, they just want people to know what's there with little to no friction, but not too much.  Like a little bit, but not so much.  And so that ended up being...



STEVE:  Right, I mean, I completely get the idea.  And I'm appreciative of the fact that I just don't subscribe to the Washington Post and The New York Times, but sometimes I want to read a story that they cover, and it's nice to be able to do that.  Again, I'm not reading the paper.  It wouldn't make sense for me to sign up because I'm just not going to get enough use out of it.  The only thing I could imagine they could do is if, in the future, we get some form of micropayment system.  Then if I went there, and they said this story will cost you five cents, you know, and I'd go, okay, yeah, it's worth a nickel.  So ding me a nickel, and I'll read the story.  I mean, then it would make sense.



And I wouldn't even mind, if there were a means to do it securely, having a payment balance the way I do on my Starbucks card, you know, having a payment balance on those sites that I could draw a nickel from.  That, too, again, it's infrequent.  I don't like the idea of paying for something I'm not really, really, really, rarely going to use.  At the same time, I'd pay a nickel in order to read a story that I wanted to.  And running a positive balance I think makes a lot of sense.  We don't yet have that infrastructure in place to make all that work seamlessly.



JASON:  I know at some point Google had gone down this road.  And there was also a service a couple of years ago called Blendle that was all about paying on a per-article basis.  But nothing universal; right.  And that might be something more that needs to be hashed out in order for it to be successful.  But I feel like Google had something along those lines where, instead of seeing ads on certain partner sites, you paid this monthly fee, and I can't remember the name of it, and that monthly fee kind of...



STEVE:  Actually, you're right.  And I remember I used it.  I had it also, yeah.  And you know, what this conversation reminds me of is that sometimes ideas which are good are just premature.



JASON:  They're too early, yeah.



STEVE:  Like the Atari 400 trying to sell itself as a home computer.  You know, like the first round - or the Apple II, which was a neat machine.  But, you know, when Dad brought one home, Mom said, "How much did this cost?"  And Dad says, "Oh, honey, you can store your recipes on this."  She's like, what?  You know, and so the point is we often see good ideas which were just ahead of their time.  So they happened, and they died.  And when they came back for the second try, then it was okay.



And so the idea of micropayments may be sort of like that; you know?  It may be that now, finally, the media companies are well enough established, people are comfortable with creating a balance, or maybe you use your phone, for example, you know, you have an app in your phone that maintains your balance, and you use something like SQRL to say, here's my micropayment for my app, and it securely deducts from the app in your phone, you know.  Those sorts of things are possible in this day and age now.



JASON:  Yeah.  Like those ideas.



STEVE:  So once again, as Microsoft is beginning to do - it's becoming a Black Hat tradition - they are ranking the industry's top bug hunters.  They announce the top industry security researchers and enterprise partners who are responsibly discovering and disclosing the greatest number of vulnerability and zero-day reports affecting its software products.  And many in the security industry are now using Microsoft's annual published list as a guide to the identities of today's top bug hunters.



And I think with good cause, security researchers who rank on this list often tout it was one of their highest career achievements because there is a lot of competition out there.  According to Microsoft, this year's top security researcher is Yuki Chen of Qihoo, Q-I-H-O-O, Qihoo 360's Vulcan team.  And taking second place is Yuki's colleague, and this name I cannot pronounce, Q-I-X-U-N Zhao, Z-H-A-O, who also won a Pwnie Award for Best Privilege Escalation Bug.  All in all, Qihoo 360's Vulcan team managed to place eight researchers in this year's ranking of the top 75.  And I have a picture of that ranking in the show notes.  You've got it onscreen right now.  And I have to say that Asia looks like, just from these names, they are really kind of taking over.  I mean, you have to look here to find some Anglo-Saxon looking names.  Like, okay, there's Joshua Graham.  I found one.



JASON:  Danny Grander.



STEVE:  Scott Bell.  There's two.  Danny Grander, three.



JASON:  Matt Nelson.  Four.



STEVE:  Boy.  They're few and far between.  Yeah, Mario Gomes?  Gomes, maybe?



JASON:  Yeah.



STEVE:  Five.  Boy, but, I mean, the rest is like - James Forshaw, okay.  But, wow.  Yeah.  Interesting lineup shown there.  So anyway, very cool that there is this kind of ranking, and it is serving as the industry's benchmark for this.  And I am about to announce a stunning escalation in bounty payout.



JASON:  All right.  The mother of bounties.  Is that what this is?  The mother of all bounties?



STEVE:  Yes, it's possible to become a millionaire hacker.



JASON:  Wow.



STEVE:  In a single blow.  Not to be left behind, Apple has bumped its bounties.  Apple just updated the rules of its bug bounty program, announced at Black Hat.  These effects are not available; they haven't come into effect yet.  This happens this fall, so a few months from now.  Apple has enormously increased the maximum reward for its bug bounty program from what was already pretty good, $200,000, to $1 million, making it by far the biggest bug bounty offered by any major tech company for reporting vulnerabilities in its products.



The $1 million payouts will be rewarded for a severe deadly exploit.  So, you know, it's got to be bad.  But still, a million dollars.  Severe deadly exploit, a zero-click kernel code execution vulnerability that enables complete persistent control of a device's kernel.  In other words, the worst possible of all possible things.  But if you can find it, you're a millionaire.  Well, before taxes.  Less severe exploits will qualify for smaller payouts.  So it's now possible to become a millionaire hacker overnight by finding just one serious bug in an Apple product.



And Apple's bug bounty program does not only apply to security vulnerabilities in the iOS mobile operating system, which of course has been the focus of all of this in the past, but now also covers, or will when this goes into effect, all of its operating systems - macOS, watchOS, tvOS, iPadOS, and iCloud.  I didn't know iCloud was an operating system, but I guess for this purpose it qualifies.  So find a bug in iCloud.  Until now, for the past three years, Apple's bug bounty program has only rewarded security researchers and bug bounty hunters for discovering vulnerabilities in iOS.  And as I said before, remember that that limitation does remain in effect until this fall.



But wait, there's more.  Or as Steve Jobs would have said, "One more thing."  Actually, two more.  Starting next year, Apple will also provide, get this, pre-jailbroken iPhones to select trusted security researchers as part of the iOS Security Research Device Program.  That's so cool.  These devices will have far deeper access than iPhones available to everyday users.  This is just smart on Apple's part.  They've been doing some thinking.  This includes access to ssh, the root shell, and advanced debug capabilities, allowing researchers to hunt for vulnerabilities at the secure shell level.



Although anyone can apply to receive one of these special iPhones from Apple, the company will hand out only a limited number of these devices, and only to qualified researchers.  So you know, if you crawl out of the basement and say, hey, I want a pre-jailbroken iPhone, you're unlikely to get one.  You probably have to be somebody who Apple recognizes and a legitimate researcher.  But this is smart because this allows the researcher to dig in deeper and find things.  So, very cool.



And I said there were two more things.  The last thing is, in addition to its maximum reward of a million dollars, Apple is also offering - it turns out a million is not the biggest possible because they're offering a 50% bonus on top of it to researchers who find and report security vulnerabilities in the pre-release software, ahead of its public release.  So potentially a maximum reward of up to $1.5 million.  So you could be a millionaire after taxes.



This is smart, of course, since finding pre-release bugs is better for everyone.  Get them before it ships.  So that incentivizes researchers to take time looking at pre-release stuff rather than, like, I'll get around to it eventually because you could get an additional 50% on top of whatever it is that you find.  Even if it's not the million dollar jackpot, still it bumps whatever you would have had up by 50%.  Applications for Apple's revised bug bounty program will be open later this year, and this will be open to all researchers rather than some limited number of security researchers approved by Apple.  So that is to say, they're now going to open the bug bounty program widely, rather than keeping it much more closed.



This expansion and massive boost in the payout of Apple's bug bounty program are likely to be welcomed.  I mean, I'm sure they're going to be welcomed by security researchers and bug bounty hunters who either publicly disclose vulnerabilities they discovered in Apple products, or sell them to private vendors such as Zerodium, Cellebrite, and Grayshift, who as we know deal in - basically create and have a gray market in zero-day exploits for profit.  And this seems like a clear smart move on Apple's part, to me.  A researcher who does find a showstopping exploit in an Apple device would have been a little hard-pressed to offer it to Apple for a pittance when the likes of Zerodium would be willing to pay much more.  Now, a developer can be a good guy, do the right thing, and receive a top-of-the-industry class monetary reward for their efforts.



So I think this represents some good maturity in the industry.  And anything that upsets Zerodium and Cellebrite and Grayshift seems like a good thing to me, and this news from Apple would certainly upset them.  And who knows?  Maybe we're going to see other major vendors respond in kind, if there's a little bit of an escalation war here in bug bounty payouts.  Apple has just upped the ante.  Microsoft may just decide to follow suit.  So that would be good, too.



A listener of Security Now! wrote a nice piece of email regarding SQRL and my upcoming travel to Europe that I wanted to share.  The subject was "Dublin Visit."  Ben Fletcher wrote:  "Dear Steve, I was delighted to hear on Security Now! that you're coming to Dublin, and I'll be at your OWASP talk."  He says:  "I moved to Dublin recently, having served in the Royal Air Force in the U.K. for the past 16 years.  I was originally a mechanical engineer by degree, but quickly realized" - smart man - "that IT was the future.  I started volunteering for IT engineering jobs and haven't looked back.



"However, this transition left me with a pretty steep learning curve.  You might remember the days when Conficker hit a number of networks.  We were not immune.  As I frantically searched the web for information to be able to understand the issue and brief my superior officers, I came across Security Now!, having listened ever since and all previous.  Suddenly, I was the subject matter expert, deploying and commanding tiger teams to remediate the problem across the U.K. and the world."



He says:  "I honestly can't thank you enough for the invaluable service you've provided me and the knowledge I've gained over the years.  I'm now working in Grant Thornton as a cybersecurity consultant, doing all sorts of fun.  I'm sure your timetable will be tight while in Ireland, but I'd be delighted to take you out for dinner with our team or anything you fancy, whether it be something cultural or touristy.  It's the least I can offer you for helping me develop as an IT professional.  If there's anything we can do for you while you're in Ireland, please do not hesitate to ask.  I look forward to meeting you in person.  Kind regards, Ben."



So first off, I very much appreciate the offer, Ben.  But Lorrie and I are already committed to sharing a meal with the Dublin OWASP gang.  And when we're not doing that, I strongly suspect that we're going to want to just wander around aimlessly, soak up the local environment and culture.  But again, thanks.



And I did want to note for our listeners that I've put up a short calendar listing the three forthcoming planned SQRL presentations, the first one - or the next one - here in Orange County, California next Thursday; and then both Dublin, Ireland and Gothenburg, Sweden toward the end of September.  The calendar entries contain links to the presentation announcements, as well as links to register to attend the meetings if you're interested and able to.



And so just go to GRC.com/calendar.  You can add a .htm to that, if you want.  Otherwise the site will do it for you.  GRC.com/calendar.  Anybody who is interested in attending Orange County, California next week or, what is it, the 24th and 26th of September in Ireland and Sweden, respectively, there are links to - both of the OWASP groups in Europe used Eventbrite in order to manage the invitations for the event.



So I did expect a robust response from our listeners to last week's Steve's File Sync Journey podcast, and our listeners did not disappoint.  It's very clear that this topic is of great interest to our audience.  So there will be a follow-up extensive results podcast.  As it turns out, the way the timing of the SQRL OWASP presentation trips have worked out, with my trip abroad ending in Boston to meet Leo for the LastPass event the Thursday, I think it's October 4th, which is a Thursday in Boston, I'm going to end up being away from Security Now! Tuesday podcasts for two weeks.



So Leo and I will be prerecording the Security Now! podcast for that first Thursday the previous Saturday afternoon before I depart.  But for the following Tuesday, since we were unable to find a time soon after we return to record for that one late, instead we're going to prerecord a podcast to fill in that week.  And that podcast will be my comprehensive results of my several months, by that time, of exploring multisystem file synchronization solutions.  So I will be working on that in the background, experimenting with all kinds of things and putting together a pretty comprehensive review of what I find.



And along those lines, a note from a listener, Cliff Spooner in Utah, struck me as intriguing.  And I thought it was very clever.  The subject was "SN-726 File Encryption Option."  And he wrote:  "VeraCrypt with Dropbox."  Okay, now, I thought, what?  Okay.  He wrote:  "Steve, after listening to your podcast #726 I thought I would share what I do to secure sensitive information in Dropbox."  He says:  "I use VeraCrypt."  Which, now, remember, VeraCrypt is a whole drive encryption tool; right?  I mean, it is what TrueCrypt became.  It is the inheritor of the TrueCrypt code.  It's been audited and is being maintained, and it's like a strong whole drive encryption.  But remember that you can also encrypt a volume, that is, you can encrypt a file and mount that as a drive.



So he says:  "I use VeraCrypt, which sounds like a terrible idea because of the large file size of one encrypted container.  But Dropbox is magic, and it is the only sync solution that is able to transfer only the changes to the container when they are made.  Other sync solutions need to re-upload the entire container."  He says:  "You will upload a large container once, which may take some time.  But after that, it will only be the changes."  He says:  "Maybe this will help solve some of your issues.  Thanks, Cliff."



Now, I don't think that's the solution I would choose, but I loved it from a cleverness standpoint.  Now, okay.  So first of all, remember that what this does is this creates a large file, and a hard drive is at the lowest level a - can be viewed as a block of sectors, but at the practical level it's a block of clusters, where you typically have, for example, eight-sector clusters where the sector is half a K; eight of them make 4K.  So you have 4K clusters.  Well, what that means is that changes to the file system are reflected by changes to clusters, and those clusters will be bitmaps, they will be directory chunks, and they will be file system contents.



So it certainly is the case that, if you were to create a large file container, and you use VeraCrypt to turn it into an encrypted file system container, you could then send that to Dropbox, like put it in Dropbox.  Dropbox, your local client, would start monitoring it for changes.  And what Cliff discovered or noted or knew or found or something was that Dropbox would be very smart about syncing changes to this massive, potentially multi-gig blob.  And of course the fact is, when you do update a file, like you've got files living in there, and you're hitting Control Save, Control Save, Control Save like all the time, as I do, you're only changing a few clusters in that file.



And so the Dropbox local client sees that only - so it doesn't know that that's a file system.  It sees just a monster single container where a few little 4K chunks of that are changing.  And so that's all it's uploading.  I mean, arguably, this would be more efficient than other sync solutions, which are resending the entire file over and over and over, if indeed that's what they're doing.  I don't know that that's what they're doing, that they're not getting smarter.  But this is very cool.



So I just wanted to share that with our listeners.  Again, I don't think that's the solution I'm going to be suggesting because of this.  I have in the show notes:  "Although I need more time to reach conclusions about robust, flexible, and secure multi-machine file syncing, I do need to acknowledge the massive response I received about one solution in particular:  Syncthing."  Probably 90+, 95%?  I mean, there were, like, other things people have suggested.  But by far and away Syncthing, S-Y-N-C-T-H-I-N-G, was most often suggested.



I now have it running on my Drobo, which was nice that I was able to have my own existing local Drobo operating as a Syncthing node.  And I have it running on both of the workstations, here and in my second location.  Both locations are behind NAT routers, and that secondary site is actually behind chained double NAT because I wanted to place, and I did, I placed a Netgate SG-1100 pfSense firewall/router in front of the ASUS router that we already have there because I wanted to do a bunch of manual fancy port shifting stuff, and IP filtering.



And although I do have static mappings and filters in place at each point with pfSense, and I could manually punch holes through the NAT routing again if necessary, because I wanted to be able to report on Syncthing for our listeners, I first wanted to try firing up these babies with everything behind those NAT routers.  Syncthing is UPnP capable, so IF I had UPnP enabled anywhere, which of course I don't, we know that it could have set up port mappings itself.  But as we talked about many moons ago on this podcast, it is possible to punch through NAT routers if you have a properly set up Rendezvous server.  And Syncthing maintains a Rendezvous server, actually multiple of them.  So it can work through NAT when it's done right.



And Syncthing does it right.  All of the Syncthing instances that I am running are directly connected to each other thanks to NAT punch-through, without any external relaying, despite the fact that everything is safely behind NAT.  Meaning that other IPs cannot see any of my Syncthing instances because they're other IPs.  I mean, they're going to get hit.  They're going to get stopped by the NAT in the same way that we often talked about NAT being inherently, natively, automatically a very good firewall.  And if anyone feels skittish about having a third party performing NAT punch-through for them, Syncthing allows you to operate your own external NAT Rendezvous server.



So anyway, so far I'm very intrigued by this.  I'm still going to make myself take a look at other options because there are several that I had not found before I settled on Dropbox, which is what I talked about using last week.  But this is, so far, this is - I'm very impressed.  And I even have an idea for an interesting hack that might work to bridge Syncthing to hosted cloud providers for some of the benefits that are provided, such as automatic versioning, which you wouldn't have unless you did something more, and the ability to create public links for people who wanted to be able to share arbitrary files which are being synced with Syncthing publicly.



So anyway, lots more to come.  That will be a podcast, let's see, it's going to be in the beginning of October.  So October, yeah, October 1st.  That will be the follow-up on file syncing after lots of experimentation by me on that.



JASON:  So 731, 732, somewhere around there.



STEVE:  Yeah.  And also our Picture of the Week last week was one that we had fun with.  We always do.  It's where the combination lock has the combination right next to the lock saying, in this case it was, "To open the door, press #2 and #4 together, then #3."  And a number of people commented.  A good friend of mine was familiar with that particular combination door lock.  And he noted that press #2 and #4 together, then 3 is, in addition to being printed on the paper right there on the door, also the default factory set combination for those locks.



JASON:  Of course.



STEVE:  Unbelievable.



JASON:  Of course it is.  



STEVE:  Of course it is.  Unbelievable.  I also posted - I have a nice comment about SpinRite.  I posted two weeks ago, actually almost exactly two weeks ago, a post called "Toe Stubbed."  I posted it to the SQRL newsgroup.  My bookkeeper/operations officer gal who's been with me for about 35 years, Sue, her trusty and crusty very old Windows XP machine which she had been using for GRC business finally died.  I remember talking about this a couple years ago because I'd had a mirror - I had a pair of mirrored hard drives, and it is being backed up using Jungle Disk.  So all of the accounting information and stuff that was important on that machine was being continually backed up to the cloud.  So nothing was ever in danger.  But it's a pain to, like, rebuild a machine.



So two years ago I picked the machine up, brought it here.  One drive was completely belly-up.  The other one was in bad shape.  I ran SpinRite on it overnight, brought the drive back to life, was able to clone it to a new pair of mirrored drives, and gave it back to her.  One of the reasons that we stayed on XP is that we have some 16-bit code.  The very first database we used for managing SpinRite 1 sales, well, 1, 2, 3.1, 4, and 5, until toward the end of 5 I wrote GRC's ecommerce system, which we've been using through the end of 5, well, immediately after finishing 5 I wrote the ecommerce system and then wrote SpinRite 6.  And then all of SpinRite's 6 sales has been through that ecommerce system.  But earlier customers want to be able to upgrade.  And so we've kept that - it was FoxPro v2.6 that ran in a DOS box.  And it is very 16-bit code.



Anyway, so this happened again.  Two weeks ago, email from Sue, machine was dead.  She switched to using her laptop to continue doing email.  Wednesday morning, since I couldn't do it because of the podcast on Tuesday, I ran down, picked the machine up.  Once again, one of the two mirrored drives was completely dead.  The other one was in really bad shape.  The system wouldn't boot anymore and so forth.  Ran SpinRite on it overnight, came in, it had fixed a bunch of problems, enough so that I was again able to make an image of the drive.  But I thought, okay, no more XP.  It's finally time to move her forward.



So I decided to go to Windows 7 because Windows 7 much more easily runs XP mode, which I thought I might need.  But as it turns out, FoxPro was able to run in the cute little DOSBox tool.  You know, DOSBox is - it's called DOSBox - was developed so that gamers who were wanting to do old-school 16-bit DOS gaming would still be able to do so.  So it's a neat little lightweight - it's only a few megs - lightweight 16-bit DOS emulator, and that's all that FoxPro needed.  It will work just beautifully.



Anyway, the point of all this is that one of the people hanging out in the SQRL newsgroup, Jeff Root, he wrote:  "I was a SpinRite version 5 owner," he says, "and when I wanted to get the latest, I just bought the version 6 release."  He says:  "It honestly never occurred to me that GRC would be able to find my previous purchase in their records, especially since years had passed."  He says:  "Now you're running FoxPro in an emulator, just in case someone wants to upgrade?"  He says:  "I wish Microsoft had as much respect and sense of responsibility for their customers.  Perhaps if GRC had shareholders, the answer would be different."  And then a smiley face.



So anyway, fun story.  Sue's machine is up, running 7.  No data was lost, thanks to SpinRite.  And, yes, if somebody purchased a copy of SpinRite in 1987, we can find you, confirm your purchase, and offer you a discount to SpinRite 6.  However, as I did say, when we finally do release SpinRite 6.1, which will be the thing I start on immediately after finishing the SQRL documentation, which means it's not far from now, 6 will have existed for 15 years.  So at that point we're going to say, okay, we're going to finally give FoxPro a rest and not continue to upgrade people.  Because anyone will have had 15 years to upgrade to 6.  But anybody who has 6, as I have always said, will get 6.1 for free, and then we will be moving forward.



So a couple of closing-the-loop bits from our listeners.  A listener whose Twitter handle is @spawnandjesus, I think that's how I would pronounce it, he said:  "Hey, Steve.  I haven't jumped into this week's podcast yet, but I thought I'd let you know your latest ransomware creators are obviously into anime."  And I mispronounced it again.  He says:  "The word R-Y-U-K should be pronounced ree-ook," he said, "as he's a character from a series called Death Note.  Also, thanks for the best tech podcast on the wire.  I've been listening for over a year, originally discovered your website as a kid in the 1990s."  He says:  "I love SQRL.  I love SpinRite.  I'm patiently waiting for the next version to purchase another copy."  Wow, well, thank you.  I appreciate that.  And as I said, I will be getting to that before long.



Graham Booker wrote:  "Random SN question."  He says:  "I've noticed in many of your show notes for Security Now!, you have ~30~ at the end.  What's the history/meaning of this?"  He says:  "The SQRL logo in its place in #725 reminded me of this."  And anyway, so Wikipedia has this to say about -30-.  Wikipedia says:  "-30- has been traditionally used by journalists in North America to indicate the end of a story.  It's commonly found at the end of a press release.  There are many theories about how the usage came into being, for example, from that number's use in the 92 Code of telegraphic shorthand to signify the end of a transmission in the American Civil War era."



JASON:  Wow.



STEVE:  Also, Wikipedia says:  "It was included in the Associated Press Phillips Code of abbreviations and short markings for common use.  And it was commonly used, when writing on deadline and sending bits at a time to be typeset, as a necessary way to indicate the end of the article."  So that's what the -30- is, although I confess I have switched to using the SQRL logo because it's cute, and it just occurred to me that I could.  So there.



JASON:  Yeah.  I think the SQRL works better than -30-, in my opinion.



STEVE:  Yeah, I do.  And thank you.  And Loren Burlingame tweeted:  "Steve, I just listened to the latest SN where you were talking about synchronization.  I am not sure if you have run across my favorite synchronization utility, SyncBack," and he gives the URL:  2brightsparks, 2 as in numeral 2, brightsparks.com, syncback/sbpro...  He says:  "But figured it is worth mentioning in case you haven't.  It is Windows software and can run from Scheduled Tasks.  But it supports everything you could ever want in a sync utility, including synchronization to S3 and other cloud providers (OneDrive, GDrive, et cetera), ransomware protection, versioning, encryption, et cetera.  You have to pay for the Pro version to get all those features, but it's worth the money, in my opinion.



"Thank you so much for Security Now!.  I never miss an episode, and every Wednesday morning is like a mini Christmas when I see the show in my feed.  I should almost mention that I have" - and I was wondering why I had this whole thing here in the notes, but then here's the reason why.  "I should also mention that I have also run into all of those issues you described with OneDrive and Google Drive clients, and I stopped using them in favor of SyncBack.  The beauty is that, since I am using OneDrive's (and GDrive's) back-end, I can access the files on mobile platforms, as well."



Anyway, so I did appreciate the fact that Loren commented that he or she had stumbled upon the same problems that caused me to finally start looking for something other than the first kind of more obvious choices, Google Drive and OneDrive.  Thank you, but neither of those work.  And we will be, in a month from now or so, talking about what it is I have found that does.  And I'm going to try to keep up with all the tweets and all of the feedback in the GRC.com/feedback of people telling me what their sync discoveries are so that I can produce a comprehensive report.  I think I'm going to be able to do one, and even come up with some interesting hacks that maybe no one has thought of before.



So I think that's going to be a great podcast.  It won't have any news, but we'll catch up on two weeks' of news that I miss on the podcast that follows that.  So I think it's good stuff.



JASON:  Absolutely.  GRC.com/feedback, for anyone that wants to participate?



STEVE:  Yup.



JASON:  Right on, GRC.com/feedback.  Jam-packed, as usual.  What are you going to do next week because you don't have Black Hat or DEF CON to draw from?  And nothing happens in security if there's no Black Hat or DEF CON.



STEVE:  Oh, Jason.  That's true, although, well, I mean, yeah, right.  There were a couple stories that I just I ran out of page count.  This page count generally brings us to about two hours, which is sort of my target, and that's where we are at an hour and 54 minutes right now.  So there will be a couple things I did not get in this week that I will be covering next week because they are also interesting.  And I have no doubt that another week will bring us another week's worth of security events.  That always seems to be the case.



JASON:  Always; right?  You can go to GRC.com to find everything you need to know about everything Steve is working on.  SpinRite, of course, best hard drive recovery and maintenance tool.  You can find a copy by going to GRC.com.  Information about SQRL, Steve, you've got to be so excited to be this close to finishing the documentation on this.  I can only imagine.  You've been working on this for a while.  Information on SQRL can be found there.  Audio of this show, as well as transcripts, if you want to find transcripts of Security Now!, GRC.com.  That's where you're going to find them.  I don't believe those are serviced anywhere on our side; right?  It's only at GRC.



STEVE:  Correct, correct.



JASON:  There you go.  So that's where you go.  If you want to come to our site for the show, the website is TWiT.tv/sn for Security Now!.  There you're going to find the audio, the video links, podcast subscribe links, everything that you need to know, all episodes, so you can start from the beginning.  And you'll also find that we record this show, usually record it live on Tuesdays at 1:30 p.m. Pacific, 4:30 p.m. Eastern, 20:30 UTC.  And all that information can be found there, TWiT.tv/sn.  And, yeah, I think we've reached the end, Steve.  Thank you so much, man.  It's always fun hopping on with you.



STEVE:  Jason, it is, and thanks for standing in for Leo.  It's always a pleasure working with you.



JASON:  You bet.  We'll talk to you soon.  And Leo and Steve will see you next week on Security Now!.  Bye, everybody.



STEVE:  Thanks, buddy.



Copyright (c) 2019 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#728

DATE:		August 20, 2019

TITLE:		The KNOB Is Broken

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-728.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we look at last week's monthly Patch Tuesday and its collision with third-party AV add-ons.  We examine four years of Kaspersky unique web user tracking.  We look again at Tavis Ormandy's discovery of the secret undocumented CTF protocol, wondering WTF is CTF?  We note a new and devastating strategy in the ransomware battle which hit Texas last Friday.  We also have the sad demise of Extended Validation certificates, the further removal of FTP support from web browsers, Google's campaign to still further reduce web certificate lifetimes, and Netflix's discovery of eight implementation flaws in the new HTTP/2 protocol.  We'll cover a bit of miscellany, update on my file syncing journey, touch on SQRL news and SpinRite, then conclude with a look at the most recent attack on Bluetooth pairing negotiation which renders all Bluetooth associations vulnerable to a trivial attack.



SHOW TEASE:  It's time for Security Now!.  We end our 14th year of broadcasting and begin our 15th with a banner episode.  We will talk about a banner update from Microsoft.  How many things did they fix?  A bunch on Patch Tuesday.  Why you might want to consider dumping your Symantec and Norton antivirus, and maybe Kaspersky, too, while we're at it.  Then we'll talk about a new Bluetooth attack called KNOB.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 728, recorded Tuesday, August 20th, 2019:  The KNOB Is Broken.



It's time for Security Now!, the show where we cover your security, your privacy,  We cover how Internet, computers, technology, networks work.  We do it all with this guy right here, Mr. Steve Gibson of the GRC Corporation.  Hello, Steven.



STEVE GIBSON:  Hello, my friend.  Welcome back.  Jason held the fort down while you were having a podcast movement in...



LEO:  It came out beautifully, too.  



STEVE:  ...Orlando or something, wherever it was.



LEO:  Thank you, Jason, for filling in for me.  Yeah, we were in beautiful hot Orlando.



STEVE:  Ah, yes.



LEO:  It was sweaty.  But the funny thing is, it's very tropical.  They have these massive rainstorms every afternoon.  But it was a lot of fun.



STEVE:  And moments ago I saw the news on, I think it was The Verge, announcing that we were going to have a fourth Matrix movie, that Keanu and - what?



LEO:  Matrix 2 and 3 were so darn good, why would you stop there?



STEVE:  Ah, well.  



LEO:  I can't say I'm excited about another bad sequel to one of the great movies of all time.



STEVE:  Yeah, it was one of the great movies of all time.



LEO:  Yeah.  But it really fell down in 2 and 3, I thought.



STEVE:  Yeah.



LEO:  Oh, well.



STEVE:  I agree.  So, well, Leo, I don't know if you looked at the Picture of the Week, but it kind of gives away what's happening.  The very first podcast you and I ever recorded was August 19th - today's the 20th - August 19th of 2005.  So I remembered Elaine's correction from a couple months ago, that we would be starting Year 15 on the podcast of August 20th, which is today.



LEO:  Oh, Happy Birthday!



STEVE:  This is the podcast's 14th birthday.  We have survived 14 years of escalating security dilemmas and resolutions and interests, and we're plowing now into...



LEO:  It's hard to believe.



STEVE:  ...today Year 15.



LEO:  If you had gone to Podcast Movement, the podcast convention, you would have realized how old we are compared to this group of young people who have just discovered podcasts.



STEVE:  And Leo, we look the same as we did 15 years ago.



LEO:  We have not changed in 14...



STEVE:  Although this moustache seems to be getting a little whiter, so.



LEO:  The rest of you looks the same.  That's the good news.  Keto is keeping you in good shape.



STEVE:  That's right.  We're going to take a look at last week's monthly Patch Tuesday, which was interesting, and its collision with some third-party AV add-ons.  We examine four years of Kaspersky's unique web user tracking that nobody knew was going on.



LEO:  Can you believe that one?  That's so annoying.



STEVE:  Oh, yeah.  We're going to look again at Tavis Ormandy's discovery of the secret undocumented CTF protocol, wondering WTF is CTF?  We also note a new and devastating strategy in the ransomware battle which hit Texas last Friday.  We've got the sad demise, well, pending, of extended validation certificates.  Not looking good for those EV certs.  The further removal of FTP support from web browsers.  I don't think anybody cares.  And of course, if you do, you can just get an FTP client, so come on.  We also have Google's campaign to still further reduce web certificate lifetimes that I have some strong opinions about.



Also Netflix, twice now, like I didn't even know Netflix had a security group.  Actually, it's the same guy, Looney, who isn't, because he's discovered eight implementation flaws in the new HTTP/2 protocol, which we're going to talk about.  We'll also cover a bit of miscellany, a quick update on my file syncing journey, touch on a little SQRL news and SpinRite, and then conclude with a look at the most recent attack.  And, boy, I was getting some dj vu because I'm thinking, didn't we already know about this?  But I guess not.  The most recent attack on Bluetooth pairing negotiation...



LEO:  Oh, this one's fun.



STEVE:  ...which renders all Bluetooth associations vulnerable to a trivial attack known as "the KNOB attack."  So I think, yes, another great podcast for our listeners as we start into a robust Year 15.



LEO:  A robust 15th year.  That's kind of amazing.  Happy Birthday.  Or Happy Anniversary.  I don't know.  I guess you can't call it a birthday if it's a podcast.  Steverino, let's go on with the show.



STEVE:  So last Tuesday was another busy and important Patch Tuesday.



LEO:  No kidding.



STEVE:  Of the 93 vulnerabilities Microsoft patched, a third of them, 29, or actually more than a third, are rated critical, and 64 were rated important.  Happily, for a change, none of the vulnerabilities patched last Tuesday were known to be under active attack, nor had any of their details been published publicly.  I did see a tweet from someone, and I attempted to follow up, but I couldn't, saying that SandboxEscaper was tweeting again yesterday.  But I looked, and there wasn't anything on her GitHub account, and I didn't further track her down.  So I don't know what it was she was tweeting about.  But because of course she's been responsible for lots of zero days recently.  The two previous Patch Tuesdays, what were patched in some cases were zero days being exploited at the time.  So in this case no.



But there was still a bunch of stuff.  There were four remote code execution bugs in Microsoft's recently troubled remote desktop system.  Of course we talked about the RDP protocol and how with BlueKeep, you know, everybody was expecting a worm.  I wasn't because it was so easy to exploit, you really didn't need a worm's automation to help you.  You could just go log into somebody else's Bluetooth system.  So unlike the EXIM email server vulnerability, where it took a week of dribbling out bytes in order to hold the connection long enough for that weird undelivered mail timeout to occur, and then you're able to exploit a problem with the server, in this case you just say, hey, I want to connect you.  And thanks to this vulnerability, the BlueKeep vulnerability, that was possible.



Now we have four more, two of which are of big concern.  However, these are not in the Remote Desktop Protocol.  They're in sort of the higher level enterprise version, so-called "terminal services."  So that's RDS, the Remote Desktop Services.  That's what enterprises use when people want to actually log into a window instance through terminal services.  It's like the professional enterprise version of Remote Desktop.  With Remote Desktop, you can connect to your one Windows instance.  But, for example, you're logged off of the desktop if you try to log in through Remote Desktop because Microsoft says, oh, no no no no, only one person can be logged into your consumer Windows at a time.  So that happens.



Anyway, so those are being - four of those things were found by Microsoft themselves.  So they found the problems internally when they were taking a closer look at Remote Desktop in general, no doubt motivated by BlueKeep, thinking, hmm, we haven't looked at Remote Desktop for a decade.  Maybe we should take a look at it again and see if there's something else we need to fix.  And they found four remote code execution problems, two of which were also wormable.  Again, not known to be happening out in the wild in this case.  And those got fixed last week, a week ago.



And beyond that, there are seven other remote code execution problems which impacted the Chakra scripting engine.  That's the original scripting engine in Edge and is used in some Microsoft apps.  Of course they'll be moving away from that as Edge goes over to Chromium.  There were also two remote code execution vulnerabilities in Microsoft's Hyper-V virtual machine technology, six remote code executions in Microsoft Graphics component, one in Outlook, two in Word, two in the Windows DHCP client - and you don't want that because that's heavily networked.  And also two in the older Scripting Engine component.  Oh, and one in the VBScript engine.



And I also noticed some dialogue somewhere, it didn't make it into the podcast formally, but just that VBScript was really being sunset at Microsoft.  I mean, they, of course, when JavaScript was happening, they're like, oh, well, let's see, what do we have?  Oh, we have Visual Basic.  Let's turn that into a scripting language and so people can write Visual Basic script for their browser.  And people did.  And of course it lost the battle to what is now formally ECMAScript, which is the formalization of browser-side scripting, which has now become fully standardized.



There's also a patch for a vulnerability in this, as I mentioned, this CTF protocol.  Just not clear how they're going to patch this because this is so badly damaged.  And even Tavis is kind of scratching his head, thinking, okay.  Anyway, this CTF protocol impacts all versions of Windows since XP, when this was mysteriously introduced.  And we'll talk about that in a minute further.



So overall, this month's August 2019 Patch Tuesday is large and important.  Oh, and Microsoft also wanted to remind users in one of their "what's happening this month" notices, they just said, oh, by the way, don't forget, Windows 7 and Windows Server 2008 R2 will be out of extended support and no longer receiving - and they didn't say it, but I'll just say "free updates," because we know that enterprise users will be able to pay an escalating, please keep us on the IV drip for, what is it, one, two, or three years continuing.



And I'm really interested to see whether Microsoft holds to this plan.  I mean, we know how desperately they are working to get people off of 7 and Windows Server 2008 R2, which is the server version of Windows 7.  But it's still neck and neck with Windows 10.  It was at the beginning of the year that they switched places in terms of who had the lead, but 7 hasn't been dropping that much.  I mean, some, as the curtain is trying to fall on it.  But again, we've seen Microsoft say, oh, maybe we'll - well, I mean, they've done it with Windows 7.  We're now in the extended support period which they just decided to give to everybody.



So we'll see what happens.  That's going to be interesting because they just, you know, what'll happen, I think, is that over time machines will die, and you can't get a new machine to run Windows 7 any longer.  Windows 7 won't install with USB3 support, that is, its installer doesn't know about USB3.  So you have to jump through some hoops, and I've jumped through those hoops, in order to get Windows 7 to set up on a machine with USB3, which all machines now and for quite some time have had.  So it's, you know, what'll happen is I think these machines will just die, and they'll get replaced with machines that have Windows 10 for better or for worse.  But I encounter people, you know, like in the real world all the time that just hate Windows 10.  And so we'll see what happens.



Also Adobe, SAP, and VMware had respective Patch Tuesdays last week.  Adobe published fixes for Photoshop, Experience Manager, Acrobat and Reader, Creative Cloud desktop app, Prelude, Premiere Pro, Character Animator, and After Effects.  And no Flash security updates this month.  Maybe it's just because people have, you know, it is certainly, although it's kind of around, it's certainly of diminishing impact.  There are much larger targets to attack these days, like all versions of Windows since XP in the case of this CTF exploit that we'll talk about in a second.



But what was interesting was there was a problem last Tuesday.  Recall how we talked about this at the beginning of the year.  Microsoft announced that they are going to stop cosigning their Windows Update packages which have for quite a while been signed with both SHA-1 and SHA-256 hashes.  They decided that July would be the last month where the cosigning would occur, and that after July, meaning starting in August this month, that is to say last Tuesday, updates would only be signed with SHA-256.  And that makes sense because it made sense to sort of straddle the hashes.  But at some point, if you really do believe that the weaker of the two, that is, SHA-1, isn't trustworthy, then that's the one you need to remove; right?  Because if you cosign, your signing is only as strong as the weaker of the two signatures, if you're going to accept either one.  So they had to get rid of it.



So a week ago things broke, although not what Microsoft or anyone else was expecting.  Remember that there was an update to Windows which, at the time it was first announced, it wasn't clear whether they were going to push it out automatically because the wording in their update announcement sort of made it sound like you had to go get it.  Like you were going to be in trouble if you didn't go get it.  But no.  They did push it out.  And so people would have updated.  And that was the update which mostly affected Windows 7, because it didn't know about SHA-256 back in the day, which would be required for any updates to be accepted from August on.



Well, it turns out that Symantec and Norton third-party AV tools have been making it their business to protect users from unsigned or mal-signed Windows updates, which is kind of weird since that's the whole reason they're signed in the first place, so that Windows can and will and does already robustly protect itself from any and all possibly weird or unsigned updates.  I mean, it's like, it's not going to accept them.  But I guess in this, like, well, what can we do to justify our existence more, Symantec and Norton said let's check those.



So apparently the Symantec and Norton AV update protectors did not get the memo about the signing changing on Windows Update, with SHA-1 dropping last month.  They were both only checking older SHA-1 signatures.  Which of course broke last week.  Both of those AV systems refused to allow their client machines to receive any of Microsoft's valid Patch Tuesday updates.  Whoopsie.



There's little that Microsoft can do at this point.  That stuff, as we know, is installed in users' machines, and it's blocking Microsoft from doing anything.  So it knows, Microsoft Update does know about the inventories of the machines it's updating.  So Microsoft has updated Windows Update to stop attempting to send any of this month's, last week's, updates to any machines containing Symantec or Norton AV.  And as a consequence...



LEO:  Geez, that's a real secure situation.



STEVE:  Isn't that a mess, Leo?



LEO:  Oh, my god.



STEVE:  It's just unbelievable.



LEO:  Oh, you've got an antivirus?  By the way, those are the two most common and most popular and often included as trials on new machines.  So it's highly likely that a vast number of Windows users already have it.  Which means they're not getting updates?



STEVE:  Now, get a load of what Symantec says.  It's just the most mealy response.  Symantec wrote:  "Symantec has identified the potential" - uh-huh - "for a negative interaction between Symantec Endpoint Protection and the contents of future Windows Updates" - okay, current and future - "as a result of the changes in this Microsoft KB," you know, Knowledge Base.  "Out of an abundance of caution," Leo...



LEO:  Yes?



STEVE:  Uh-huh, "Symantec and Microsoft worked together to only allow the update to be visible to versions of Symantec Endpoint Protection that fully support SHA-2 signed Windows executables replaced by this and future updates to Windows 7 SP1 and Windows 2008 R2 SP1."



LEO:  What?  I don't understand what that just said.  You just said it.  I heard it.  It was in English.  But what does it mean?



STEVE:  It said nothing.



LEO:  Are they going to fix their problem?



STEVE:  Yeah, Symantec is - oh, Leo, here's the good news.



LEO:  Oh.



STEVE:  "Symantec is actively working on multiple releases" - because this affects everything - "multiple releases of Symantec Endpoint Protection to address this situation."  Which of course what you just read explains nothing about.



LEO:  Yeah.



STEVE:  "This document will be updated."  Whew.  We'd just rather have the software updated, but okay.



LEO:  Oh, please.



STEVE:  "[This document] will be updated as each release becomes available for distribution and include details on how each update can be acquired."   So, yeah.  I'm glad you and I are not using any of this nonsense.



LEO:  Oh, what garbage.



STEVE:  I know.



LEO:  So it's unconscionable that an antivirus would block Windows updates, period.



STEVE:  Correct.



LEO:  I guess they're not, it's just they're using SHA-1, which is broken.  And we've known that for how long?



STEVE:  Well, and that's the point is that, well, we've known that all year, that it was going to - that SHA-1 was...



LEO:  This was coming.



STEVE:  ...was going away at the end of July; that everything from August on.  So they're blocking the signatures.  And again, they're not offering any benefit by doing this because Windows won't install an update that it hasn't verified.  So they're just, like, one additional monkey in the works.  Or wrench in the monkey or something, I don't know.  There's definitely a wrench somewhere.  It's just amazing.



LEO:  Wow.  By the way, Symantec and Norton I think are the same companies.  I know they are.



STEVE:  Right, someone bought the other.



LEO:  Symantec bought Norton.  I think they rebranded older Norton stuff.  No, maybe not.  I think they still use Norton for the consumer brand and Symantec for the business brand.



STEVE:  Ah.



LEO:  And I think Symantec Endpoint is business.  So anyway, this is terrible.  That's not what a security software should do.



STEVE:  Right.  So you install this protection, and it prevents you from obtaining your monthly IV drip of security patches.  It's just incredible.  So anyway, for our listeners who are using Symantec or Norton, you keep an eye out.  I mean, I'm sure that those packages will - and I checked.  The link in that note was current as of yesterday, where they said, oh, uh, we're working on it, and we'll update this as updates become available.  Well, there were none available yesterday.



So we're now, you know, thank goodness that none of the things that Microsoft fixed are, like, being exploited in the wild, or you'd really be in much worse trouble.  So I'm sure that Symantec and Norton will update themselves, and then I don't know what.  Then maybe users, if you see that, you should go and check for updates.  I mean, I'm sure Windows will get around to pushing those out again.  It's just, as you said, Leo, it's a mess.



LEO:  Ugh.  Ugh.  Ugh.



STEVE:  And in another foul-up this month, Microsoft, in that same note where they talked about this problem, speaking of one of the patches from last week's updates:  "After installing this update, applications that were made using VB6 (Visual Basic 6), macros using Visual Basic for Applications, and scripts or apps using Visual Basic Scripting Edition (VBScript), may stop responding" - and actually it's one of those, again, will stop responding - "and you may [and you will] receive an 'invalid procedure call error.'"



They said:  "This issue is resolved in KB4517297, which is an optional update."  In other words, last Tuesday they broke Visual Basic and only found out after the fact and said whoops and so have now got an update to their update which unbreaks it.  So there's that.



And while we're on the topic of AV that nobody wants, Kaspersky, it turns out, has for four years, since late 2015, been facilitating independent web tracking of every single one of their users.  Both the free and the paid editions have been injecting a snippet of JavaScript into every page, every web page displayed by their users.  The JavaScript, and I've got a picture of it in the show notes, which is placed into the page's DOM, the Document Object Model, can therefore be readily seen by and parsed by any other script running on the page.  That's the whole point of having a Document Object Model is we've standardized the way pages are described.



And anybody looking at this little bit of script - you've got it on the screen right now, Leo - you can see that it's a script calling out the downloading of, over HTTPS from a server, gc.kis.v2.scr.kaspersky-labs.com.  Then we have forward slash, then a GUID, you know, a Globally Unique ID, then /main.js, and then close script.  So this little bit of code causes the user's browser to go download main.js from Kaspersky.



Well, this GUID, this Globally Unique ID, is the ID from the individual user's instance of Kaspersky.  And that GUID is inserted into every page downloaded.  And that GUID can be seen by any script running on the page, including advertisers.  Meaning that, since late 2015, any user of Kaspersky has been having it embed their unique ID in every single page and every single ad that they download, making them absolutely trackable.  I mean, you can't shake this.  It's just there.



Not to mention the fact that Kaspersky themselves, since every single page you download is fetching this main.js from Kaspersky Labs' servers with this unique user tag in the query, and we know that the browser will provide a referrer header telling Kaspersky exactly where and what page who you are is on, they've had the ability to globally track their entire install base everywhere they go on the Internet.  We don't know that they were, but that's what this does.  And so they certainly could have been.  Until it was made public two months ago, when they said, oh, okay, we won't do that anymore.



So they've changed it to a new ID, which is no longer user unique.  It's now Kaspersky version unique.  So now what's being transmitted, it's a much less probably source of concern hopefully, is the fact that you're a Kaspersky user, and exactly which version of Kaspersky AV you are using.  And Leo, I am so glad that neither of us have any of that crap on our systems.



LEO:  Yeah, no kidding.



STEVE:  Thank you anyway.



LEO:  That's just bad behavior.  And as somebody's pointing out, I mean, you're announcing at the very least that you're using this antivirus.



STEVE:  Right.



LEO:  So, I mean, just - yeah.  Unbelievable.  Unbelievable.



STEVE:  So what the heck is CTF?  When I was talking about Windows updates, I mentioned that there had been a patch to it.  So what is it?  Once again, Google's Tavis Ormandy discovered this buggy protocol which, if hackers or malware - and this has been around since Windows XP, when it was introduced.  If hackers or malware had already gained a tentative foothold on a user's computer - so this is not a remote vulnerability, this is any Windows app running on anyone's machine - they can use it to take over any app, high-privileged applications, or the entire OS as a whole, Tavis explains.



It turns out that CTF, which no one has ever heard of before, is a little-known Microsoft protocol used by all Windows operating systems since XP, right up through 10.  And naturally it's insecure, and turns out can be easily exploited.  So what is CTF?  Nobody knows for sure.  We don't know what CTF stands for.



LEO:  Capture The Flag?



STEVE:  Love it.  Perfect.



LEO:  Yeah, yeah.



STEVE:  Tavis was never able to determine what it means, despite rummaging through all of Microsoft's documentation.



LEO:  Continuous Thermonuclear Fusion?



STEVE:  That'd be good.



LEO:  Yeah, yeah.



STEVE:  What Tavis did learn...



LEO:  It's all from the chatroom.



STEVE:  Thank you, chatroom - that CTF is part of the Windows Text Services Framework, so "T" of CTF might be Text.  We don't know.



LEO:  Casper the Friendly Protocol? 



STEVE:  There is a TSF, the Text Services Framework.  So this is the system that manages the text shown inside Windows and Windows apps, that is, Windows itself and Windows applications running on Windows.  When users start an app, Windows also starts a CTF client for that app.



LEO:  Interesting.



STEVE:  The CTF client receives instructions from a CTF server about the OS system language and the keyboard input methods, so it's tied in to Text Services and multi...



LEO:  I wonder if it's a clipboard function of some kind.  Clipboard.  Right?



STEVE:  "C" could be clipboard, yeah.  Clipboard Text Facilitator.  Anyway.



LEO:  Yeah, something like that, yeah.



STEVE:  If the OS input method changes from one language to another, then the CTF server notifies all CTF clients, who then change the language in each Windows app accordingly in real time.  So this whole thing was a hack to allow you to, on the fly, change the language of the system, and suddenly everything would go gabloop and change.



LEO:  We should send this Microsoft note to Tavis.  It's the Collaborative Translation Framework.



STEVE:  Ah, interesting.  I mean, it's funny that at the time of his reporting he was unable to determine what the heck it was.



LEO:  Yeah, yeah.



STEVE:  So Tavis dug into it, and he quickly discovered - to unfortunately not surprisingly, and not, well, yeah, not surprisingly because it's he - to his shock and dismay that the communications between CTF clients and the CTF servers are not properly authenticated or secured.  Or, as Tavis put it:  "There is no access control in CTF."



LEO:  Yeah, because the point of it - by the way, it's been deprecated - was collaborative translations.  So you can submit a sentence translation.  Others can submit.  And then it collaborates based on - it returns the translated content in its total count from your account.  Yeah, it's an API for collaborative translation.  So it makes sense.  It is a server.



STEVE:  And it's deprecated but still present because, you know, five people are using it somewhere.



LEO:  Right, yeah, you can't kill it.  It was only deprecated last February 2018, so yeah.



STEVE:  Yeah.  Any application, any user, even sandboxed processes can connect to any CTF session.  Clients are expected to report their thread ID, their process ID, and their main Windows messaging handle.  There's no authentication involved.  And this is Tavis:  "No authentication involved, and you can simply lie."  Tavis added:  "So you could connect to another user's active session" - I mean, this is complete violation of all containment within Windows - "connect to another user's active session and take over any application, or wait for an administrator to log in and compromise their session."



An attacker that hijacks another app's CTF session can then send commands to that app, posing as the server, normally expected to be the Windows OS, but there's no enforcement of that.  Attackers can use this loophole to either steal data from other apps, or they can use it to issue commands in the name of those apps.  If the apps run with high privileges, then those actions allow the attacker to take full control over a victim's computer.  And, according to Tavis, any app or Windows process is up for grabs.  Because of CTF's role to show text inside any app or service, there is a CTF session for literally everything and every user interface element on Windows OS.



To demonstrate the dangers, Tavis recorded a demo in which he hijacked, get this, the CTF session of the Windows login screen which, as we know, Microsoft has made a huge deal about being highly privileged, isolated, and sacrosanct.  Thus Tavis easily demonstrated that everything in Windows is hackable because of CTF.  I have a link to the Google Project Zero Blogspot posting.  Tavis titled it "Down the Rabbit Hole."  And he said toward the end, he concluded, he says:  "I've implemented this attack in ctftool.  Follow the steps here to try it."  And it's on GitHub, a link to ctftool, where it shows you how to achieve this.



And he concludes:  "So what does it all mean?  Even without bugs" - and there are bugs.  But "Even without bugs, the CTF protocol allows applications to exchange input and read each other's content.  However, there are a lot of protocol bugs that allow taking complete control of almost any other application.  It will be interesting," he's writing, "to see how Microsoft decides to modernize the protocol."  He says:  "If you want to investigate further, I'm releasing the tool I developed for this project."



And then, under conclusion:  "It took a lot of effort and research to reach the point that I could understand enough of CTF to realize it's broken.  These are the kind of hidden attack surfaces where bugs last for years.  It turns out it was possible to reach across sessions and violate NT security boundaries for nearly 20 years, and nobody noticed."  So Tavis...



LEO:  Wow.



STEVE:  Yeah.  Thank you, once again.  I mean, you know, anybody who's programmed Windows recognizes how much danger the whole Windows messaging model creates.  I mean, you are able to enumerate processes.  You are able to get the messaging queues of other apps.  I mean, this is what macro recorders and players do is they send keystrokes to apps, causing them to do things.  And it's nice when it's your macros that are being sent, and your apps, and they're doing what you want them to do.  But nothing enforces that in Windows.  I mean, so essentially you really need to make sure nothing gets in because once something has, it's game over.



LEO:  Unbelievable.



STEVE:  So the news, as of Friday, from Texas was government agencies, nobody knew what government agencies, but we knew there were 23 of them, were all hit with a - well, now we know it's cities, we'll get to that in a minute - were all simultaneously hit with a well-coordinated, simultaneous, and effective ransomware attack last Friday, August 16th.  Twenty-three Texas entities, the majority of which are local governments, were hit by a ransomware attack on Friday that Texas officials say is part of a targeted attack launched by a single threat actor.



Details still remain scant about the specific agencies hit by the ransomware attacks, which began on the morning of August 16th, as well as exactly which systems were impacted.  However, the Texas Department of Information Resources, DIR, as of Saturday night did say that responders are actively working with all 23 entities.  And who knows how many were attacked that didn't get infected.  But 23 different things, different networks, different agencies, now we know it's actually 23 cities were brought down by a massive coordinated attack.



So this DIR, the Department of Information Resources is currently working, as of Saturday night, to bring their systems back online; and the state of Texas systems and networks itself were not impacted.  So what we do know is that these 23 agencies were knocked offline and presumably encrypted by this simultaneous attack.  I checked yesterday in local reporting, like I don't know, the Texas Gazette or something, and it didn't have any more information still.



The Texas Department of Information Resources website posted a statement saying that, quote:  "Currently, DIR, the Texas Military Department, and Texas A&M University System's Cyber-Response and Security Operations Center teams are deploying resources to the most critically impacted jurisdictions.  Further resources will be deployed as they are requested."  When pressed for additional details, the Texas DIR declined to elucidate any further, stating "due to security concerns" - read embarrassment, perhaps - saying only that they were smaller local governments.  Yes, 23 of them.



The DIR did not provide information about which systems are down, how systems were first infected, and the specific amount of ransom.  You know that's going to be adding up.  In their reporting of this, Threatpost reached out to representatives from Dallas, Houston, and Austin for comment on whether they were impacted by the attack.  While representatives from Dallas and Austin have not yet responded - I wonder if their email's down - a spokesperson from Houston told Threatpost that:  "As far as we know, Houston has not been affected."  I think they would know if they had been.



According to a statement sent to Threatpost:  "The city of Houston is aware that a ransomware attack has affected several local government agencies throughout Texas.  We are in contact with the Texas State Operations Center and will monitor the latest developments."  Whew.  "The Mayor's Office of Homeland Security and the IT Services Department will continue to proactively work to secure and protect the city's assets."  However, the DIR said that at this time, and not surprisingly given what we know, evidence gathering indicates the attacks came from one single threat actor.



Allan Liska, who is a threat intel analyst with Recorded Future - we've spoken of them a number of times, they're an in the middle of the thick of things security firm - told Threatpost that the attacks signify an important shift in the ransomware threat model.  "Typically, state and local governments have been targets of opportunity for ransomware attacks, with the gangs behind these attacks, Ryuk and SamSam, appearing to stumble onto previous state and local government targets."  Or, that is, stumble previously onto them.  "However, this incident," he says, "appears to be the first where a string of governments were actively being targeted in an attack."



He said:  "This is the first time there's been an attack against several local governments in a state."  He says:  "This is big.  It's a game-changer.  This will change the model going forward for attackers, and that will be a problem for governments."  Allan also noted that one advantage Texas has is it has a coordinated incident response.  The response team is centralized for cities and counties in emergencies, which makes it easier when there is a problem like this to find and reach out to a main contact.  There's someone to call.  So anyway, as I said, I looked for any update as I was putting these notes together yesterday, and everyone is being quiet.  They're now saying that it's 23 individual cities across Texas.  But that's all we know.



And I shudder to think of the ransom that is being requested.  I mean, say there are some cities that do have current backups.  In our previous reporting of this over the last few months, we've noted that recovering from an attack like this is sometimes more, like without paying ransom, is sometimes more expensive than if you pay the ransom and you are able to decrypt the systems in place, just due to the logistics of having systems which have been there, who knows how old they are, what software they have on them, how recent the backups are, whether the backups were online and therefore themselves also got encrypted in the process.  It's a mess.  I mean, this really is an interesting, really significant new problem for U.S. municipalities.  I mean, it's just not exaggerating to say that.



We have, unfortunately - because, I mean, I guess I understand this, but it's still unfortunate - the coming demise of Extended Validation (EV) certificates.  Safari has already removed all EV certificate company info from the address bar, where it had been before this most recent major update to iOS and macOS.  Most mobile browsers are not showing it because they have scant real estate, and they just don't want to take the space for it.  Now both Chrome and Firefox browsers for the desktop have announced that they, too, will soon be removing any EV indication from the main URL UI.



Chrome's Google Groups post was titled "Upcoming Change to Chrome's Identity Indicators."  And it reads:  "As part of a series of data-driven changes to Chrome's security indicators, the Chrome Security UX [User Experience] team is announcing a change to the Extended Validation certificate indicator on certain websites starting in Chrome 77.  On HTTPS websites using EV certificates, Chrome currently displays an EV badge, containing the name of the EV certificate holder" - you know, like mine says Gibson Research Corp. - "to the left of the URL bar.  Starting with Version 77, Chrome will move this UI indicator down into the Page Info" - which of course you have to click on in order to see, meaning it just might as well not exist, most people don't even know it's there - "which is accessed," they say, "by clicking the lock icon."  In other words, effectively nullifying its impact.



They said:  "Through our own research" - and again, this is where I get the, yeah, I understand.  "Through our own research, as well as a survey of prior academic work, the Chrome Security UX team has determined that the EV UI does not protect users as intended.  Users do not appear to make secure choices, such as not entering a password or credit card information, when the UI is altered or removed" - meaning actually they never knew what it meant, and maybe they wouldn't have cared even if they did - "as would be necessary," they write, "for EV UI to provide meaningful protection.  Further, the EV badge takes up valuable real estate, can present actively confusing company names in prominent UI, and interferes with Chrome's product direction towards a neutral, rather than a positive, display for secure connections."



And I'll intervene here just a moment to say, in other words, as we know, secure is intended to be the norm now going forward, and non-secure sites are what will then be denigrated for, like, something - as we know, the UIs will begin saying "not secure" proactively; whereas, if they don't say that, then it's secure.  And that will then just be the de facto.



Anyway, they said:  "Because of these problems and its limited utility, we believe it belongs better in Page Info," meaning if somebody cares.  And of course, who does?  "Altering the EV UI is part of a wider trend among browsers to improve their Security UI surfaces in light of recent advances in understanding of this problem space."  They write:  "In 2018, Apple announced a similar change to Safari that coincided with the release of iOS 12 and macOS 10.14 and has been implemented as such ever since."



And shortly following Chrome's announcement, Mozilla also announced that, starting in Firefox 70, they will be removing the EV certificate's identity information from the address bar.  Mozilla wrote:  "In desktop Firefox 70, we intend to remove Extended Validation (EV) indicators from the identity block, the left-hand side of the URL bar which is used to display security/privacy information."  Actually, mine's - I'm noticing mine's getting kind of full of little icon-y things.  I had, like, five of them this morning when I was looking at something, you know, the half shield and all kinds of stuff.



They said:  "We will add additional EV information to the identity panel" - uh-huh, thank you - "instead, effectively reducing the exposure of EV information to users while keeping it easily accessible."  And once again, yes, where nobody will notice.  They wrote:  "The effectiveness of EV has been called into question numerous times over the last few years.  There are serious doubts whether users notice the absence of positive security indicators, and proof of concepts have been pitting EV against domains for phishing.  More recently, it's been shown that EV certificates with colliding entity names can be generated by choosing a different jurisdiction."



What that means, for anyone who's interested, is, for example, it would be possible, I can't remember the example that was given, but for example it would be possible for a Gibson Research Corp. to be incorporated out of California, because I'm a California Corp., in some other state.  Nothing prevents that.  And that's their real name.  They can't get GRC.com, but they could get something, you know, and get an EV certificate for their real name.  And so it would show Gibson Research Corp., just as mine does, and it would be a valid EV certificate.  So that's what both Google and Mozilla mean when they say, you know, it's not really clear what it means to show the company name because you can have multiple companies with the same name.



Then they said, "Eighteen months have passed since then, and no changes to address this problem have been identified."  They said:  "The Chrome team recently removed EV indicators from the URL bar in Canary and announced their intent to ship this change in Chrome 77.  Safari is also no longer showing the EV entity name instead of the domain name in their URL bar, distinguishing EV only by the green color.  Edge is also no longer showing EV entity name in their URL bar."  So RIP, EV.



I think that pretty much, you know, given that EV certificates are significantly more expensive for website domains to obtain - I read a bunch of other reporting of this that wasn't necessary to put into this relative long summary.  But, for example, it is substantially more difficult for an EV certificate holder to prove and need to continuously update and re-prove their identity.  You know, I do it because it was a badge of honor to have an EV showing.  I wanted to.  That's going away.  So I think, you know, it's unfortunate because I'll be talking in a minute about Google's efforts to work to shorten certificate lifetimes rather than to deal with revocation.  



So as Chrome moves forward, things change.  They have posted their intent to remove and deprecate FTP support.  And again, it's like, okay, fine.  



LEO:  How many people really use your browser for FTP?  If you're serious...



STEVE:  I kind of forgot it was still there.  You know, when have you have seen an FTP URL?



LEO:  Ftp://; right? 



STEVE:  Yes, exactly.  Yes.  So they said:  "The current implementation in Google Chrome has no support for encrypted connections, FTPS, nor proxies.  Usage of FTP in the browser is sufficiently low that it is no longer viable to invest in improving the existing FTP client.  In addition, more capable FTP clients are available on all affected platforms.  Google Chrome 72 and on removed support for fetching document subresources over FTP and rendering of top-level FTP resources.  Currently, navigating to FTP URLs result in showing a directory listing or a download, depending on the type of resource."  In other words, the document subresource, meaning like components of documents.



So from Chrome 72 you could not have a web page that might have, for example, been loading a picture.  I don't know why you would.  But normally it's https:// and then the URL to .jpg.  Well, you could have had that picture on an FTP server, and in the document, in the page it would have said ftp://.  But anyway, so that was removed from Chrome 72 onward.  Those are document subresources.  But you could still, in the URL bar, you could put ftp://, you know, ftpforever.com, and up would come the directory of that root, that directory of the FTP server.



So they said:  "A bug in Google Chrome 74 resulted in dropping support for accessing FTP URLs over HTTP proxies.  Proxy support for FTP was removed entirely in Google Chrome 76."  So rather, you know, there was a bug.  They said let's just kill it.  Let's not fix it.  No one cares.



Finally, they said:  "Remaining capabilities of Google Chrome's FTP implementation are restricted to either displaying a directory listing or downloading a resource over unencrypted connections.  We would like to deprecate and remove this remaining functionality rather than maintain an insecure FTP implementation.  So to that end, Chrome will soon be getting a new DisableFTP flag which will initially not be enabled at first," meaning that DisableFTP won't be active.



So this remaining shred of FTP functionality will still be present.  But over time we can expect them to migrate that away so that it will eventually be enabled.  Then that remaining bit of FTP won't be enabled by default.  Those who do want it will be able to turn it on - I'm sorry, will be able to turn off the Disable, thus enabling it.  But that will really be a clue that they ought to go find themselves another client.  So anyway, FTP is going, and no one is going to miss it.



Unlike shorter certificate lifespans.  Two months ago, in June, Google's Ryan Sleevi, who's a good guy, by the way, introduced a ballot measure during the CAB conference proposing to amend one of the documents, removing lines and adding lines, to reduce the maximum browser security certificate lifetime to essentially one year, you know, one year plus a little bit of fudge so that you don't have a problem with overlap.  Basically a year.  Right now EV certificates are two; OV certificates are three.  So for those using OV certificates, the more popular certificate, that triples the rate at which certificates must be reissued, which is annoying because the crypto is secure.  I mean, there's no reason.  Well, okay, except there is kind of one.



Anyway, the measure was titled "Ballot SCXX."  I love this.  "Improve Certificate Lifetimes."  That's right, it's an improvement.  Just the fact that Google said "improve certificate lifetimes" when "reduce certificate lifetimes" is what they really mean demonstrates that they clearly recognize that certificate revocation is still broken.  My first thought upon seeing that this was in the news was of course they are, since as our longtime listeners of this podcast know, that is, those who have been listening for at least five years, Google's Chrome browser certificate revocation is not completely and utterly broken; rather, it is nonexistent for all practical purposes.  Chrome doesn't even bother checking non-EV certificates with their proprietary CRL - that was supposed to be Certificate Revocation List - CRLSET system.  And now of course they're working to kill EV certificates, too, so they won't even need to be checking those in the future.



For some unfathomable reason, back in the dawn of Chrome, someone must have made the decision that, since certificate revocation was currently imperfect, it was therefore of no value at all.  So unlike every other browser on the planet, Chrome chose to simply ignore it.  It was, at the time, I think, an irresponsible and unconscionable choice.  Oh, sure.  They have their own web browser rigorously checking any and all of their own properties' certificates.  You don't dare mess with a Google-derived cert, as we have often noted.  But they don't bother to check anyone else's, only their own.



When I clearly demonstrated this five years ago, back in May of 2014, by deliberately creating a revoked certificate, which Chrome gleefully accepted, I mean, it was revoked.  Chrome said, eh, fine.  The other browsers said, no, wait, this is revoked.  No, you can't go there.  We won't show you that page.  Chrome said, yeah, come on in.  What happened?  Google manually added that one certificate's signature to Chrome's short certificate blacklist.  So I created another, which was once again honored because, you know, why not?  And that one they didn't bother blacklisting.  I haven't looked recently.



So I'm sure that this, and I, annoyed them by bringing attention to this original sin of Chrome's.  It wasn't my intention to annoy them, but I did hope that by bringing this to light we might see this fixed, since back when Chrome was becoming more and more popular and influential, it mattered.  And of course it matters now.  All of the other browsers were doing the right thing.  And again, yes, the existing system was imperfect.  But it made more sense then, and it still does now, to fix it, rather than just simply ignore it.



I haven't revisited any of this since then.  So that's been a little over five years.  But anyone who is interested in that research and coverage can find those original pages at GRC.com/revocation.  And when I was working on this back then, doing that research, I encountered the perfect solution to the problem, which is known as OCSP stapling.  OCSP stands for Online Certificate Status Protocol.  Certificates contain the URL of their issuer's OCSP server, which allows the current instantaneous status of the revocation state of the certificate to be verified.  This enables web browsers to query certificates' OCSP servers to receive a completely up-to-the-instant check on the current validity of the certificate.



The trouble is this has been an emerging standard.  And in the beginning OCSP servers could not always be counted on to reply quickly, if at all.  So this either slowed things down while the browsers waited - something browsers really hate to do - or browsers were forced to take the position of trusting unless denied.  In other words, failing open rather than failing closed.  But it turns out a perfect solution does and has existed for years.  It's called OCSP stapling.



When OCSP stapling is used, the web server that's offering and asserting the validity of the certificate, it itself obtains and caches an updated and signed assertion of the current validity of the certificate and then staples that to the certificate, which it offers to the web browser.  The web browser checks the signature of the certificate and of the recent OCSP validity assertion.  So think of the power this gives certificate authorities.  Now they can revoke a certificate, and it will become untrusted instantly, and with zero overhead introduced by browsers, and no browser delay.



In fact, two years ago, in July of 2017, Cloudflare's Nick Sullivan blogged with the title:  "High-reliability OCSP stapling and why it matters."  And I won't go through the whole posting because it's very lengthy, but interesting.  I have the link to it in the show notes.  But he began, to introduce the concept:  "At Cloudflare our focus is making the Internet faster and more secure.  Today we are announcing a new enhancement to our HTTPS service:  high-reliability OCSP stapling.  This feature is a step towards enabling an important security feature on the web:  certificate revocation checking.  Reliable OCSP stapling also improves connection times by up to 30% in some cases.  In this post, we'll explore the importance of certificate revocation checking in HTTPS" - which again, just to rub it in, which Chrome doesn't bother to do anyway - "the challenges involved in making it reliable, and how we built a robust OCSP stapling service."  Anyway, for anyone who's interested, the link is there.



So today, OCSP stapling is universally available.  Windows Server supports it, Apache, Nginx, all the various CDNs, AWS and other web providers.  So there is no longer any good reason not to simply make its support mandatory in the future.  Incredibly, as I've said, Chrome doesn't perform any useful revocation checking.  So to minimize the exposure to revoked certificates, languishing for their current up-to-three-year period, their solution is to shorten the certificate's maximum life.  They want, Google wants, all certificates to die more rapidly through their natural self-expiration.  But of course this still leaves us with a big mess and a gaping hole for exploitation because you still have a year.



You know, it really feels as though Google is working to incrementally chip away at the certificate authority model, with the aim of eventually bringing us to this ACME protocol, where we know nothing about a certificate.  Nothing.  Where the certificate essentially makes no assertion other than the fact that one automated web server requested and received a domain validating certificate from some ACME server.  Essentially, all it then gives you is encryption.  It gives you privacy, but it gives you nothing in terms of who the certificate is being issued for because it's now just an automated handshake.  And you know, that doesn't feel like the right future for the Internet.



Moving forward, it seems so clear that we're going to need more assurance, more than just encryption.  We need to know who it is we're talking to for our connections on the Internet; right?  And this is going in the other direction.  We're going to really need to know who the server is and have some reason to trust the entity that's there, rather than a bot that issued the certificate in response to an API request.  It seems to me this is the vital service that certificate authorities have always provided.  It's not perfect, no.  But neither was OCSP, and that has been fixed completely with stapling.  There's just been no pressure to implement OCSP stapling.



So it seems to me that, rather than discarding certificate authorities, which seems to be the direction that Chrome is wanting to go because they're not doing any revocation checking, they're wanting to just chip away at certificate life, that certificate authorities provide this vital service of having done some due diligence about who it is the certificate is for.



Now, we've talked about alternatives.  Someday the DANE protocol, the DNS-based Authentication of Named Entities, that might happen.  But DNS needs to be made significantly stronger with DNSSEC, which would need to be universally supported before that could happen.  So it seems to me that the much more practical proximate solution is to move OCSP stapling, which is done, it exists, it's ready, we ought to move that to the forefront by visually rewarding in the browser's UI those sites whose certificates carry a freshly stapled OCSP assertion.  That would give sites an incentive to implement readily available OCSP stapling.



Basically, they just have to turn it on, which would give us true zero-overhead certificate revocation checking to solve the revocation problem once and for all.  And then, yes, we need to figure out how to strengthen the assertion that a manually certificate authority issued certificate is able to make.  But it seems to me going forward that it's crazy to imagine an Internet where you've got secure connections, and you have no idea who you're connecting to.  I just - that makes no sense to me.  So, but it'll be interesting to see how this evolves in the future.



Netflix, as I mentioned, has been providing some security oversight, which the first time I saw it - there was something else a few months ago where I said, wait, what?  Netflix?  But yeah.  So this is the second time this is happening.  They took a look at the HTTP/2 protocol, some actual implementations.  For a long time we've had HTTP/1.1.  And we've talked about HTTP/2's many new features previously.  And just to give our listeners a quick short summary, whereas a single HTTP/1.1 connection makes a request and waits for a reply, then it might make another request during the same connection, or it might terminate it and open another.



With HTTP/2, what we have is an inherently powerful, very powerful multiplex connection supporting multiple simultaneous overlapping streams which support multiple simultaneous overlapping queries and responses, differing stream priorities, and even the ability for the server to see what things are being requested and to anticipate future not-yet-requested by the client assets and send them ahead, even before they're being requested.  I mean, it really does feel over-engineered, but it's arguably what today's massive web pages require.  And you could imagine some, I mean, these features are present, even though they're not even being used yet.  I don't even know of a server that is heuristically analyzing the requests and looking at previous requests of the same stuff and noticing what replies were always being followed up, and then saying, oh, well, if you ask for this, you're probably going to be asking for these things, and let's send them ahead.



So the trouble is all those features, and as I said, many of which are not even being used today, are in the spec.  They're in the protocol.  So you've got to build them in order for them to be used if you're going to say that you're HTTP/2.  And everybody wants to be that now.  And they are, it turns out, extremely tricky to implement.  Netflix took a look at a number of actual real-world implementations of HTTP/2 deployed in the field and found eight different ways to completely clog up the pipes and bring even a burly web server to its knees with just a few carefully chosen packets.



What Netflix wrote was, in their overview:  "Netflix has discovered several resource exhaustion vectors affecting a variety of third-party HTTP/2 implementations.  These attack vectors can be used to launch DoS" - you know, now, this is not the flooding-style denial of service.  This is a resource depletion.  Remember back in the early days you could just send a server sequence, SYN, S-Y-N, packets, and it would start setting up connections which you never followed through on and exhaust its ability to create any more connections so that normal visitors coming were unable to get a connection.  It would just return a "server busy" or just a spinning wheel rather than giving you a connection.  Not because you were flooding it with traffic, but just you depleted the resources at the server end.  This is like that.



So they said:  "These attack vectors can be used to launch DoS attacks against servers that support HTTP/2 communication.  Netflix worked with Google and CERT to coordinate disclosure to the Internet community.  Today, a number of vendors have announced patches to correct this suboptimal behavior."  And, by the way, there were a bunch of those in last Tuesday's Patch Tuesday from Microsoft.  "While we haven't detected these vulnerabilities in our open source packages, we are issuing this security advisory to document our findings and to further assist the Internet security community in remediating these issues."



So under impact they said:  "There are three broad areas of information security:  confidentiality, so that's information cannot be read by unauthorized people; integrity, information can't be changed by unauthorized people; and availability, information and systems are available when you want them.  All of the changes announced today," they wrote, "are in the availability category.  These HTTP/2 vulnerabilities do not allow an attacker to leak or modify information.  Rather, they allow a small number of low-bandwidth malicious sessions to prevent connection participants from doing additional work.  These attacks are likely to exhaust resources such that other connections or processes on the same machine may also be impacted or crash.



"HTTP/2, which was defined in RFCs 7540 and 7541, represents a significant change," they wrote, "from HTTP/1.1.  There are several new capabilities, including header compression and multiplexing of data from multiple streams, which make this attractive to the user community.  To support these new features, HTTP/2 has grown to encompass some of the complexity of a Layer 3 transport protocol," meaning the lower level protocol where there's a lot more going on than just get a connection, ask for something, get the results, and disconnect.



So they said:  "Data is now carried in binary frames.  There are both per-connection and per-stream windows that define how much data can be sent.  There are several ICMP-like control messages - ping, reset, and settings frames, for example - which operate at the HTTP/2 connection layer.  And there's a fairly robust concept of stream prioritization."



So what did they find?  The description of the eight CVEs will give us enough of a feel for it.  They wrote:  "Many of the attack vectors we found, and which were fixed today, are variants on a theme.  A malicious client asks the server to do something which generates a response, but the client refuses to read the response.  This exercises the server's queue management code.  Depending on how the server handles its queues, the client can force it to consume excess memory and CPU while processing its requests."



So there are eight CVEs, and I'll sort of run through their names and a brief description.  So there's 9511, the Data Dribble.  They wrote:  "The attacker requests a large amount of data from a specified resource over multiple streams.  They manipulate the window size and the stream priority to force the server to queue the data in one-byte chunks.  Depending on how efficiently this data is queued, this can consume excess CPU, memory, or both, potentially leading to a denial of service."



Then we have the Ping Flood.  "The attacker sends continual pings to an HTTP/2 peer, causing the peer to build an internal queue of responses.  Depending on how efficiently this data is queued, this can consume excess CPU, memory, or both, potentially leading to a denial of service."



Then we have the Resource Loop.  "The attacker creates multiple request streams and continually shuffles the priority of the streams in such a way that causes substantial churn to the priority tree."  Once again, consumes blah blah blah.



Reset Flood:  "The attacker opens a number of streams and sends an invalid request over each stream that should solicit a stream of reset stream frames from the peer.  Depending on how the peer queues the reset stream frames, this can consume excess" blah blah blah.



Then we have the Settings Flood.  "The attacker sends a stream of settings frames to the peer.  Since the RFC requires that the peer reply with one acknowledgement per settings frame, an empty settings frame is almost equivalent in behavior to a ping.  Depending upon how efficiently," blah blah blah, blah, blah, blah, blah, blah.  Another denial of service.



The zero-length headers leak:  "The attacker sends a stream of headers with a zero-length header name and zero-length header value, optionally Huffman-encoded into a one-byte or zero-length headers.  Some implementations allocate memory for these headers and keep the allocation alive until the session dies.  This can consume excess memory, potentially leading to a denial of service."  Then we have the Internal Data Buffering.  And similarly.  And the Empty Frames Flood and so forth.



So anyway, Netflix exercised existing implementations, found many of them wanting, and set up, got eight CVEs, coordinated the disclosure, responsibly disclosed these problems to the various vendors of HTTP/2 supporting servers, Microsoft among them.  Microsoft fixed it last Tuesday.  Presumably the other guys have or will, and we now have an improvement as a consequence in the actual implementation of HTTP/2.  I mean, it is an incredibly complex protocol.  But once the implementations get implemented and have been pounded on by researchers like this guy at Netflix, we'll have the potential for much better utilization of the bandwidth between browsers and clients, which we know everybody, especially Google, wants as they move toward web-based, browser-based applications.



LEO:  Yeah.  Hey, can I put in a little plug for Friday's Triangulation?  I think you'll be interested in this.  We've got...



STEVE:  Yeah, please.



LEO:  Yan Zhu is lined up.  She's currently Chief Security Officer at Brave, which is the privacy browser I've talked about built on Chromium.



STEVE:  Right.



LEO:  She helped build HTTPS Everywhere, SecureDrop, Let's Encrypt.  Really interesting person.  She's also a DJ.  She just performed at DEF CON.  And she's going to be our guest on Triangulation this Friday.  I'll be there - bcrypt.  You know bcrypt.



STEVE:  Yeah, yeah. 



LEO:  I should have probably said bcrypt.  More people know her by her handle, bcrypt, than anything else.  We'll be talking - she was a Technology Fellow at the EFF, as well.



STEVE:  Very cool.



LEO:  Dropped out of high school.  Got her bachelor's from MIT in physics.  It's an interesting mix, isn't it.



STEVE:  Love it, love it.



LEO:  Yeah.  So really excited.  Former maintainer of HTTPS Everywhere and a co-creator of Packet City.  Really an amazing person.  11:30 a.m. Pacific, 2:30 Eastern time this coming Friday I'll be talking to bcrypt.



STEVE:  That's be a great Triangulation.



LEO:  Yeah, yeah.  She'll be fascinating.  Sorry.  I didn't mean to interrupt.



STEVE:  Oh, no, I'm glad you did.  I'm sure our listeners will be interested.  I definitely will be, too.



I got a tweet from a David Theese who said:  "Traveling from Phoenix on Thursday to hear your SQRL talk.  Looking forward to meeting you.  Long-time enjoyer of Security Now!."  And just as a reminder, two bits.  I am speaking the 22nd, in two days, Thursday, to the Orange County chapter of OWASP.  And I have the link to the Meetup.com event.  And also remember, I mentioned it last week, but Leo, you were not there, well, you were elsewhere, GRC.com/calendar is the link to the three upcoming OWASP presentations, the first one in two days here in Orange County, Southern California.  And then I've got the one in Dublin, Ireland, followed by Gothenburg, Sweden, in late September.  And I know that Rasmus Vind, who did the SQRL implementation for XenForo, I think I'm going to get to meet him in Sweden.  And I'm going to get to meet Jeff, who did the iOS implementation of SQRL.  He's going to come to Dublin.  So it's going to be great.  I'm going to get to meet lots of people.



LEO:  Anders, is it Gothenburg?  Or Gothenburg?  Gothenburg.



STEVE:  Okay.  No.



LEO:  So I hope you get it right.  Gothenburg.  Okay.  Okay.  You're going to Gothenburg.



STEVE:  I'm going to just...



LEO:  Just wanted you to know.



STEVE:  I'm - yeah.



LEO:  And by the way, you'll be talking about SQRL at our presentation in Boston, too, and that'll be another opportunity for people to see you.



STEVE:  Yeah, cool.  Cool.  Also last week I explained I wanted to make sure that our listeners knew that two weeks ago we did Steve's File Sync Journey.  On Episode 734 - which, Leo, you and I will be pre-recording before my trip...



LEO:  Oh, I'm good.  This is a good idea.  Because we want to make something a little bit evergreen.



STEVE:  Yes, exactly.



LEO:  This will be great, yes.



STEVE:  Exactly.  So Episode 734 will be Steve's File Sync Conclusions.  I've been keeping up with Twitter.  I've been keeping up with the mail.  I, oh my goodness, I mean, it's - I now am an expert on Syncthing.  I even set it up on a Raspberry Pi and used Rclone and Mount in order to get Syncthing to sync to Dropbox, just as an experiment, so that it wasn't just peer to peer, but it was also to the cloud.  And I've been playing with Sync.com and everybody else's, I mean, just a whole bunch of things.



So, and I'm happy to say things are beginning to gel.  I think I'm developing sort of a, I mean, all of our needs are different.  And so there isn't one solution for everybody.  But I think I'll be able to present an updated sort of state of the where we are with applications and services and how they can be knit together, in some cases in some interesting ways that probably haven't been before.  So that's a few weeks away, Episode 734.  And, you know, this is our birthday, Security Now!'s birthday.  And this is also the second anniversary of my first date with Lorrie is approaching.



LEO:  Aww.



STEVE:  So I went looking for...



LEO:  Now you know it's serious, if you remember that.  That's good.  That's a good sign.



STEVE:  Well, we were wanting to celebrate the second anniversary of our first date.  So I went looking for the exact date from our early email correspondence, and I found it.  And right next to it was a note from a SpinRite user.  And I thought, oh.  So I read it, and it was one of those little heart warmers.  So I just thought I would reshare.  I did, I'm sure, two years ago.



This is from Yann Fitzmorris.  The subject was "Another Success Story SpinRite Data Recovery."  He wrote to "Dear Steve and GRC Team."  He said:  "I purchased SpinRite a few years ago and have been using it to keep my drives in good health.  I personally have never had to use it for data recovery."  Well, of course, because he's using it preemptively.  He said:  "However, a friend asked for my help this week because her laptop would no longer boot to the login screen.  Her laptop contained the only copy of pictures and videos of the first two years of her daughter's life.



"It wasn't looking good for the patient.  When I plugged the drive into an external dock, no OS would recognize the drive.  I used my dedicated PC for SpinRite, plugging in the drive, and ran Level 2.  Success.  We plugged the drive into the dock and were able to recover all pictures and videos, over 70GB."  Whoa.  "Needless to say, my friend will now seriously consider a backup solution.  And," he says, "I'm hoping she will buy her own copy of SpinRite to show her gratitude for this amazing product.  Thanks again for all your hard work and research.  Love the Security Now! podcast, as well."  He says:  "I've been a listener since 2015."  And Yann, maybe you're still listening, and here it is, 2019.  So anyway, thank you for the nice little walk down memory lane.



LEO:  A stroke from the past.  Oh, we don't have an ad here, so have your cup of coffee, but we will continue.



STEVE:  I just did.



LEO:  We will continue on.



STEVE:  Just needed to take a sip.  I was dry.  So this paper, the following paper was included in the proceedings of the 28th USENIX Security Symposium last week in Santa Clara, California.  The paper's full title is "The KNOB Is Broken:  Exploiting Low Entropy in the Encryption Key Negotiation of Bluetooth BR/EDR."  Now, Key Negotiation Of Bluetooth is K-N-O-B, thus KNOB.  Three security researchers reported on their analysis of more than 14 different Bluetooth chips from different vendors.  If anyone is interested in the 16-page PDF, I have the link in the show notes.



So first of all, Bluetooth BR/EDR, that's basic Bluetooth.  That stands for Basic Rate/Enhanced Data Rate, the original Bluetooth protocol, sometimes referred to as "Bluetooth Classic."  For example, to distinguish it from Bluetooth Low Energy or something.  That's present in more than, I mean, who knows how many, a billion, more than a billion Bluetooth-enabled devices.  So the abstract of their paper explains, sort of sets this up.



They said:  "We present an attack on the Bluetooth key negotiation protocol of Bluetooth BR/EDR.  The attack allows a third party, without knowledge of any secret material such as link and encryption keys, to make two or more victims agree on an encryption key with only one byte of entropy."   Yes, eight bits.  Not many.  "Such low entropy enables the attacker to then easily brute force the negotiated encryption keys, decrypt the eavesdropped ciphertext, and inject valid encrypted messages in real-time.  The attack is stealthy because the encryption key negotiation is transparent to the Bluetooth users.  The attack is standards-compliant because all Bluetooth BR/EDR versions are required to support encryption keys" - now, this is the part that is just difficult to believe.  All Bluetooth versions "are required to support encryption keys with entropy between one and 16 bytes and do not secure the key negotiation protocol.  As a result, the attacker completely breaks Bluetooth security without being detected.



"We call our attack Key Negotiation Of Bluetooth (KNOB).  The attack targets the firmware of the Bluetooth chip because the firmware, the Bluetooth controller, implements all the security features of Bluetooth BR/EDR.  As a standards-compliant attack, it is expected to be effective on any firmware that allows the specification and on any device using a vulnerable firmware.  We describe how to perform the KNOB attack, and we implement it.  We evaluate our implementation on more than 14 Bluetooth chips from popular manufacturers such as Intel, Broadcom, Apple, and Qualcomm.  Our results demonstrate that all tested devices are vulnerable to the KNOB attack.  We discuss countermeasures to fix the Bluetooth specification and its implementation."  And that's the beginning of their 16-page paper.



CERT did a kind of a friendly explainer for this that I thought I would share.  CERT's vulnerability notice, they use the common Alice and Bob as parties wishing to communicate securely, and Charlie in the role of attacker.  They said:  "To establish an encrypted connection, two Bluetooth devices must pair with each other and establish a link key that is used to generate the encryption key.  For example, assume that there are two controllers attempting to establish a connection, Alice and Bob.  After authenticating the link key, Alice proposes that she and Bob use 16 bytes of entropy.  This number N could be between one and 16 bytes.  Bob can either accept this, reject this and abort the negotiation, or propose a smaller value.  Bob may wish to propose a smaller N value because he does not support the larger number of bytes proposed by Alice.  After proposing a smaller amount, Alice can accept it and request to activate link-layer encryption with Bob, which Bob can accept."



They write:  "An attacker, Charlie, could force Alice and Bob to use a smaller N by intercepting Alice's proposal request to Bob and changing N.  Charlie could lower N to as low as one byte, which Bob would subsequently accept since Bob supports one byte of entropy, and it is within the range of the compliant values.  Charlie could then intercept Bob's acceptance message to Alice and change the entropy proposal to one byte, which Alice would likely accept because she may believe that Bob can only support a value N of one.  Thus both Alice and Bob would accept N of one and inform the Bluetooth hosts that an encryption is active, without acknowledging or realizing that N is lower than either of them initially intended it to be."



LEO:  My key is 43.



STEVE:  So what we have here, obviously, and we've encountered this many times through the 15, or the 14, the now-completed 14 years of this podcast, another classic cryptographic security downgrade attack.  It's amazing that Bluetooth is this mature, and that we are only now noticing this oversight.



Under impact, CERT notes:  "An unauthenticated adjacent attacker can force two Bluetooth devices to use as low as one byte of entropy.  This would make it easy for an attacker to brute force as it reduces the total number of possible keys to try, and would give them the ability to decrypt all of the traffic between the devices during that subsequent session."  And here's the really nutty part.  The researchers note the following about halfway through their 16-page paper, because I read the whole thing, and I was just like, you're kidding me.  Well, not surprising, but they did say this:  "We do not see any reason to include the encryption key negotiation protocol in the specification of Bluetooth."



LEO:  Yeah, why would anybody want that?



STEVE:  "From our experiments, presented in Section 5, we observe that, if two devices are not attacked, they always use it in the same way.  A device always proposes 16 bytes of entropy, and the other always accepts it.  Furthermore, the entropy reduction does not improve runtime performance because the size of the encryption key is fixed to 16 bytes, even when its entropy is reduced."  In other words, this was just, again, over-engineering.  Someone said, hey, let's add a pre-link negotiation encryption key negotiation to allow the end points to negotiate down the amount of entropy.



Now, I mean, okay, was this the NSA who was secretly participating in some committee?  Because this is nuts.  I mean, this is loony.  Anyway, it's what we have.  And so a bunch of Bluetooth stacks are now being busily updated.  Essentially, the problem is this is not the software.  This is the firmware because this is all being done by the Bluetooth controllers.  So we're going to need the Bluetooth software to update the firmware in the Bluetooth controllers to no longer accept entropy low values.



I mean, really, since these guys have never found a single instance in their testing where 16 did not work, where 16 bytes of entropy wasn't enough, that's what everybody should use.  The whole point was - and this is why they said we do not see any reason to include the encryption key negotiation protocol in the specification.  That's the protocol that allows the negotiation down from 16 to one of the entropy.  It should just not be there.  Nobody has a problem with 256 bits.  So that's what it should have always been.  Crazy.



And of course our long-time listeners will recall that I have several times observed that there is a large, though brief, period of inherent vulnerability during Bluetooth pairing.  You have two unauthenticated devices, that is, they don't know who each other is.  They're just near each other.  And they're saying "go" at the same time.  So you have two unauthenticated devices, hoping to perform a secure negotiation.  It's simply not possible to do that securely without some covert out-of-band channel.  It's just not.



So I recommended, and our listeners may recall, that if someone really needed Bluetooth security, that they should stand out in the middle of a completely deserted parking lot to perform the pairing and hope that no one is aiming any high-gain antennas at them.  Because that's the only way you can do this.  I mean, maybe, if you really wanted to, just bring a large roll of tinfoil, wrap yourself in tinfoil, and then pair your devices under your tinfoil cone of silence, and you'd be good to go.  But otherwise, I mean, the problem is it's super convenient to pair, you know, to make a device discoverable, and the other device sees it.  But it's inherently an unauthenticated pairing process.



And there isn't any way to make it absolutely secure unless, as I said, you use some sort of out-of-band channel, you know, where for example one device has a screen and displays a long PIN, where you key it into the other device's keyboard.  There you're out of band, not just relying on radio.  But nobody wants to do that.  You just want to say, oh, look, magic.  We're paired.  It's like, yes.  To whom?  No way to know for sure.



LEO:  To whom are we paired?  Somebody you should look at Bluetooth LE, which is crazy.  I was driving down - I guess I was - what do you call Bluetooth wardriving?  Bluedriving?  I was bicycling through a neighborhood.  I was trying to pair my phone to my helmet, my Bluetooth helmet.  And all of these - I'm seeing, like, speaker after speaker.  And not just speakers, I mean, all kinds of Bluetooth devices popping up here.  And I don't think LE does any authentication; does it?  It just goes, yeah, yeah, fine, good.  We'll make this easy.  Let's make this easy on both of us.



STEVE:  Yeah.  There has to be a pairing event.  So you do typically need to make both devices, like prime them to connect to each other.



LEO:  These devices were broadcasting their Bluetooth identity, and I think most do because I think manufacturers say we don't want this whole pairing key thing.  Let's just talk to each other.  So there's a lot of new devices, Bluetooth LE devices that just talk to your phone.  Like there's nothing, you don't do anything.



STEVE:  Wow.



LEO:  I think that's crazy. 



STEVE:  That is crazy.



LEO:  Yeah.  I would love to know more about that.



STEVE:  Especially when your house alarm security system is using it.



LEO:  Oh, you wouldn't believe all the things I saw.  Refrigerators, I mean, all sorts of stuff.  Well, you know, I mean, this is that age-old tension between convenience and security.



STEVE:  Yup.



LEO:  And the Bluetooth folks just want to make it easier and easier and easier.  I think it has to do with beaconing.  I think there was this push for a while to have these Bluetooth beacons.  And it turns out every Apple device is a beacon.  They just never told anybody.



Steve, we do this weekly.  And you're still not jaded, 14 years...



STEVE:  For 14 years going.  Here we are, the beginning of number 15 and going strong.



LEO:  Happy birthday.  I don't know, I have to look up what the 14th anniversary gift is.  It's probably not anything nice.  It's probably paper or something.  But happy 14th anniversary.



STEVE:  Yeah, this actually - I did the TechTalk column for eight years in InfoWorld.



LEO:  So this is the longest thing you've ever done.



STEVE:  Yeah.  No relationship.



LEO:  Including your marriage.



STEVE:  No relationship has lasted this long.  Well, Leo, it's you and me, baby.



LEO:  It's you and me.  Okay.  I'm just going to say the traditional 14th anniversary gift would be ivory.  Now I know why the chat room kept talking about ivory.  Gold jewelry or an opal, agate, or bloodstone. 



STEVE:  Oh, Leo, you really shouldn't.



LEO:  Gold is accepted, I'm sure.  A Krugerrand or two would be fine.  You'll find Steve at his website, that's GRC.com, the Gibson Research Corporation.  That's where ShieldsUP! lives.  That's where SQRL lives.  That's where DCOMbobulator and Shoot the Messenger and all the great freebies Steve has been giving away over the decades.  You'll also find his bread and butter, SpinRite, the world's best hard drive recovery and maintenance utility.  Hie thee hither and get thou a copy of the beautiful SpinRite.  You need it if you've got a hard drive.  We will also have the show there.  We're going to edit it, send it over to Steve.  He puts 16Kb - do we send you 16Kb?  Or do you down sample? 



STEVE:  I down sample.



LEO:  So he has a little script he's doing, taking the 64Kb, which he also has for an audio version, and puts it...



STEVE:  And then I put it up for Elaine because she likes to have the smaller one just to conserve her own bandwidth.



LEO:  Oh, yeah, because she lives in the middle of nowhere.  And so she types it in.  Which is great because we sound like Thomas Edison in the first recording.  So poor Elaine's going, "Hello, Elaine.  We are talking to you through our microphones."  Sorry, Elaine.  She had to write that.  The funny thing is, everything we say she has to write.  That's kind of mean of me.  I'm sorry, Elaine.  GRC.com.  Handwritten transcripts by a professional, 16Kb audio, 64Kb audio, all that great stuff.  SpinRite.



You want video of the show or higher quality audio, we have it.  Actually, we don't have higher quality.  We have the same quality audio.  But that's at our website, TWiT.tv/sn.  That's the page.  All the shows, all 14 years are there, 728 episodes.  I'll give you a little tip.  You can go to any episode by just typing TWiT.tv/sn and the episode number.  So the very first episode is TWiT.tv/sn1, if you want to hear what we sounded like when Steve's moustache was still black.



What else?  You can subscribe.  Actually, I really encourage people to subscribe.  We love it if you subscribe.  Just get your favorite podcast client and search for Security Now!, and all of the shows will be yours.  Thank you, Steve.



STEVE:  My friend, a pleasure.  I will see you next week for 729.



LEO:  Geez.  



STEVE:  As we cruise further into Year 15.



LEO:  We're entering our adolescence.  Explains a lot.  Thanks, Steve.  We'll see you next time.



STEVE:  Bye.



Copyright (c) 2019 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#729

DATE:		August 27, 2019

TITLE:		Next Gen Ad Privacy

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-729.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we check in on Texas, and on the Kazakhstan government's attempt to be their own CA.  How did that work out for them?  We note a troubling increase in attacks on the open source software supply chain.  Google's announced plans to add data breach notification to Chrome.  We look at a surprising Apple iOS v12.4 regression (whoops!) and at another Microsoft RDP component in need of updating.  I update our listeners on the state of SQRL (another of its documents is completed) and on SQRL presentations past and future.  I share some news from my ongoing file sync journey.  We conclude by looking at some very interesting and promising moves as browser-based advertising matures from the ad hoc mess it has always been into a privacy-respecting Internet citizen.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  Of course we've got security flaws - what would Security Now! be without security flaws? - in Webmin.  Another RubyGems flaw.  We'll talk about the ransomware attacks in Texas.  Steve has an interesting theory about what's really going on.  And then Google's proposal for ad privacy.  Why Google thinks cookies should stick around.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 729, recorded Tuesday, August 27th, 2019: Next Gen Ad Privacy.



It's time for Security Now!, the show where we cover your safety online, your privacy, very much privacy online and all of that jazz with this guy right here, Security Now!'s own Steve Gibson.  Hello, Steve.



STEVE GIBSON:  Hey, Leo.  Great to be with you once again.  



LEO:  Yes, for a thrilling [crosstalk].



STEVE:  We're going to be recording in two weeks on Monday, rather than on Tuesday, for any avid stream followers, because of the big anticipated - I guess it's not anticipated anymore, it's real, isn't it, the Apple event on Tuesday.



LEO:  Yes.  Well, no.  Still a guess.



STEVE:  Ah.



LEO:  Still a guess.  So Apple has not yet sent out invitations; to my knowledge hasn't confirmed it.  But I'm hoping, we're expecting September 10th.  And if that's the case - now, if it's not the case, we won't move.



STEVE:  Yeah, good, okay.



LEO:  Yeah.



STEVE:  And we'll know by then, so...



LEO:  Well, we should know, well, I hope we'll know by then.  We should know by Thursday is what Lory Gil said.  She thought we'd get the invitations Thursday.



STEVE:  So Episode 729 for August 27th, the last episode of August, our second podcast of our 15th year.  And this is interesting.  There have been, well, we often talk about the whole tension, I mean, the increasing tension that exists between online advertising and the propensity it has for tracking, and kind of the icky feeling that the idea of being tracked across the web gives people.  I've had, in like watching talking heads shows, from time to time they'll talk about how creepy it feels for them to be texting about something, and then they go to the Internet, and they start seeing ads for that.  And it's like, okay, wait.  I'm not sure that actually happens that way.  But we know that tracking is an issue.



Apple has been making a series of announcements over the last couple years about standing up more for their users' privacy.  We know that Firefox has.  Well, of course the biggie in the industry is Google with Chrome.  And also Google, who purchased - what was the big ad company, used to be independent and is now part of Google...



LEO:  Yeah, I want to say Overture, but that was Microsoft.  I can't remember what they bought, yeah.



STEVE:  Yeah, I can't think of them.  Anyway, it was the biggie.  So what finally has happened is that Google has recognized that, unless they do something proactive, they're in danger of having their own revenue impacted by other browsers proactively taking measures.  I mean, and to Google's credit, we've been talking about how Google is trying to react against fingerprinting.  Chrome and the fingerprinters have been going back and forth.



Well, it turns out that it's looking like what we're on the cusp of is the adoption of, and the development of, and then adoption of some technologies to essentially legitimize profiling without tracking, the idea being that, for example, where we've only had cookies, and cookies would allow you to get tracked across the Internet, there really hasn't been an alternative to tracking individuals and building profiles around individuals, which is what privacy advocates are so upset about.



But when you think about it, I mean, if the advertiser knew you anonymously as a member of a cohort of similar people such that they could get what they want, which is targeting, without knowing what we don't want, which is us, that is, at the individual granularity level, then everybody could win.  And to the degree that it's true that ads run the 'Net, and in fact Google has some instrumentation where they're claiming to have seen a 50% drop in advertising return for anonymous ads versus targeted ads, then that's significant.



Anyway, this week's podcast is titled "Next Gen Ad Privacy."  And we will wrap up the podcast by talking about in more detail some of what Google is saying, what Apple has said.  It turns out that even Cloudflare has a technology which they developed along with a plugin with a summer intern a couple years ago to minimize the number of times you have to do the CAPTCHAs on Cloudflare to say that you're a human and not a bot.  And so I think a really interesting podcast.  And this feels like the right thing.  Advertising and tracking had just been ad hoc.  What really, and our listeners know, those who have been listening for 14 years, that I've always been annoyed because this wasn't what cookies were meant for.  And it was just the idea of the abuse of a mechanism from a technology standpoint always got under my skin.



And so the idea of saying, okay, look, advertising is here, it's real, it's not going anywhere, so let's embrace it, if we can do so in a way that respects our privacy.  And we know that cryptographic technology, as I've often said, can do anything we want it to.  Turns out it can solve these problems, too.  So we're going to talk about, first, we're going to check in on Texas and find out what happens when you mess with Texas.



LEO:  Uh-oh.



STEVE:  Also on the Kazakhstan government's attempt to be their own certificate authority, how did that work out for them.  We'll note the troubling increase in attacks on the open source software supply chain that's getting worse.  And also on Google's announced plans to add data breach notification to their own browser.  We know that Firefox had done that previously.  Also we look at a surprising Apple iOS version 10.4 regression which was rapidly fixed in 10.4.1, and at another Microsoft RDP component which is in need of updating for, oddly enough, for Android users.



Then I'm going to update our listeners on the state of SQRL.  Another of its four documents, that is, the third of four is completed.  And also on SQRL presentations past and future.  I also have some news on my ongoing file sync journey.  And then we'll wrap up by talking about what I think is the soon to come legitimization of advertising and its need to understand something about who we are without telling them who we are.  And so I think another great podcast for our listeners.



LEO:  Yeah, it's a really interesting topic.  And I've also read some comments from people that say what Google is doing benefits Google.  But, you know...



STEVE:  Well, of course it does.



LEO:  Yeah.  They're not getting rid of tracking.  They just have other ways of doing it besides cookies.



STEVE:  But it's different than that, though.  People who said that did not understand what Google is doing.  And so we'll talk about that.



LEO:  All right.



STEVE:  So our Picture of the Week, Leo...



LEO:  This reminds me of another one we've done.



STEVE:  We've done several like this, yes.  The other one I loved because you could see the tire tracks that were rolling around on either side of it.  And we've done several of the locks, the combination locks that had the combinations printed on the window.  I was looking at the latch on this, and I was thinking, I hope that's not a lock on that gate so that, you know, presumably it keeps it from swinging through, so it only opens in one direction, rather than them imagining that they can lock that gate.  But really, I mean, could anybody explain this?  I mean, I just, like, I don't understand.



LEO:  It's to keep baby strollers out or something.  I mean, it's got to be, I mean, there's a gate across a path, but of course there's no wall attached to the gate, so you can just walk right around it. 



STEVE:  Yeah, I mean, or bicycles?



LEO:  Yeah, I think it's kind of...



STEVE:  Not a car.  It wouldn't block a car.  It wouldn't block anything.



LEO:  Not really.  Anything that wanted to get in.



STEVE:  Because, I mean, it's narrower than the space on either side, so it's not like it's going to block something of a certain width.



LEO:  Maybe they plan to build a wall.



STEVE:  Maybe it's so you can get arrested if you're on the wrong side; and they can say, look, there's a gate.



LEO:  Yes, we told you.



STEVE:  You didn't go through the gate, did you.  And it makes it very clear.  It's painted yellow.



LEO:  It's very funny.



STEVE:  So, like, see the gate.  I just, really, I just...



LEO:  It also clearly was added after the path was built because there's a patch of concrete where the gate is.



STEVE:  Yeah.  So they thought, you know what this path needs?



LEO:  A nonfunctional gate.



STEVE:  We forgot.  Someone, like, we forgot the gate.  Out in the middle of nowhere, out in the open.  We're going to put a gate here just because.  We have an extra one, and so we have to put it somewhere.  I don't know.



So Texas ransomware update.  We of course talked about the number, 23 townships in Texas.  Now, with a little bit of reading between the lines I'm developing a theory of what happened, and it sort of puts - it changes the profile of what we thought.  Everybody got off on, like, the coordinated attack against 20 or 23 cities at once, and on my god, this ups the ante and changes the territory.  It's looking like maybe that didn't happen.



NPR carried a story of this, and I've been checking back because one of the weird things is there was so little information that was coming out.  We've been talking about all of these attacks.  Certainly the ones in Florida were interesting.  We got lots of interesting details.  This one was odd because, like, due to its lack of specificity.  It was like, yeah, this happened.  But no one could get any information about it.  And again, this also follows my theory as to kind of why.  But theirs was the only reporting, NPR's, which caught the mayor of one of the Texas towns stating that the attackers were demanding $2.5 million.



LEO:  Wow, that's a lot more than the Florida attacks.



STEVE:  That is.  And of course, as we know, it took out - it did something to - the number dropped also, I guess it was just overstated, from 23 to 22.  So that's now the number that I'm seeing reported is 22 municipalities affected.  So they're wanting $2.5 million to provide the decryption keys for these machines that were successfully attacked and encrypted.



And what we know is that, as of last Wednesday, only two cities have come forward to say that their computer systems were affected.  Officials in Borger in the Texas Panhandle said the attack has affected city businesses and financial operations.  Birth and death certificates are not available online, and the city cannot accept utility payments from any of its 13,250 residents.  City officials said that "Responders have not yet established a timeframe for when full normal operations will be restored."  And then Keene, Texas, a small city with 6,100 residents outside of Forth Worth, was also hit, officials announced.  That city's government is also, and this was the first clue I had, also unable to process utility payments.



And to me that suggested that these two small cities, also notable because of their small size, might be sharing outsourced payment systems to handle their utilities.  And then that appeared to be confirmed by Keene's Mayor Gary Heinrich, who told NPR that the hackers broke into the IT system used by the city and managed by an outsourced company.  And so that, which is what he said, also supports many of the other municipalities targeted.



So I think that's what happened.  I don't think this was a big, mass, ooh, the game has changed coordinated attack because we also heard from the three big Texas cities, like Dallas and Fort Worth itself, yeah, there were, like, three that were asked.  And they said, as far as they knew, nothing in their city had been attacked.  So what the model appears to be, there are small towns in Texas that aren't big enough to do their own thing, so they outsourced these services.  And I'll bet it was that servicing company.



LEO:  You bet.



STEVE:  Yes.  And that explains the simultaneity of this, how it was suddenly, like, oh my god, everybody on Sunday morning woke up and, well, everybody, 23 townships that this happened to.  So anyway, I think that's what's happening.  We still haven't heard about the payments.  It's looking like it won't be up to the cities.  The cities, these 22 cities are just waiting for their systems to come back.  They're not theirs to fix.  They're this outsourced company.



And that also explains why there's sort of been this weird silence.  It's because stuff that they were contracting for, the services they were contracting for, stopped working.  And they're like, hmm.  Well, we hope they're going to come back soon.  And it's also - it's looking like they're not going to be paying any ransom.  It's going to be this outsourced company, or hopefully that company's insured, blah blah blah.  So anyway, that's the update on Texas.



We talked about this Kazakhstan certificate, where the reporting was murky and sort of self-contradictory.  Some of the country's users apparently received themselves mixed messages.  Some citizens of Kazakhstan were told that they would need to install the government's private certificate if they wished to retain Internet access.  And of course we all know what that means.  We've worried ourselves in the U.S. that at some point we may find our ISPs telling us that we need to install certificates if we want to continue receiving service.  And I dread the day that that happens.



Although, boy, after the pushback that we're about to talk about, maybe that's looking less good or less feasible, less likely.  Some users reported that indeed they had been cut off from the Internet until they had installed that certificate, whereas others said they had not installed anything and that their service was just fine.  Then we heard that only the area surrounding the capital city would be affected.  But then others reported that they were in the capital, and everything was fine.  So it seemed really weird.  We also heard that Kazakh ISPs were forcing their customers to install this government-issued root cert on their devices to regain access to Internet services.



And we found the certificate.  So there definitely was one.  In a prior episode of this podcast we showed the certificate and put it side by - we showed the fake Facebook certificate and the real Facebook certificate side by side and how it just said something like Internet security service or something on the fake one, whereas the real one said Facebook, Inc.  And it was a clone except for the signature, the cryptographic signature, which cannot be spoofed.  And of course Facebook wasn't happy that somebody had a clone, a working fake of their certificate, which was only trusted because the person who had it had installed the Kazakhstani root certificate.



Anyway, so what we do know is that, amid all this confusion, Google, Mozilla, and Apple didn't find any of this confusion humorous, and they took immediate action.  Mozilla's blog posting about this last Wednesday contains some new information, so I'm going to share it.  And Google blogged, and Apple blogged.  Everybody was like, you know, these made - the major browser vendors did not find any of this amusing.



Mozilla said:  "In July, a Firefox user informed Mozilla of a security issue impacting Firefox users in Kazakhstan.  They stated that Internet Service Providers in Kazakhstan had begun telling their customers that they must install a government-issued root certificate on their devices.  What the ISPs didn't tell their customers was that the certificate" - this is Mozilla talking - "was that the certificate was being used to intercept network communications.  Other users and researchers confirmed these claims and listed three dozen popular social media and communications sites that were affected."



And I'll break here just to note that this perhaps suggests that the interception was selective, which would make sense and would also explain the inconsistency of the connectivity reports.  So my point being that it may well have been that ISPs were filtering specific domains like Facebook, but not the Internet as a whole.  So that would explain why people who were trying to go to Facebook or Instagram or whatever, three dozen popular social media and communications sites, were having a problem; whereas others who weren't doing that said, no, everything's working just fine.  Yeah, until you try to go to one of the proscribed sites.



So anyway, Mozilla continues:  "The security and privacy of HTTPS-encrypted communications in Firefox and other browsers relies on trusted Certificate Authorities to issue website certificates only to someone that controls the domain or website."  Which of course is what you have to prove in order to qualify for a certificate is you prove domain ownership.  "For example," they say, "you and I cannot obtain a trusted certificate for www.facebook.com because Mozilla has strict policies for all CAs trusted by Firefox which only allow an authorized person to get a certificate for that domain.



"However, when a user in Kazakhstan installs the root certificate provided by their ISP, they are choosing to trust a CA that doesn't have to follow any rules and can issue a certificate for any website to anyone.  This enables the interception and decryption of network communications between Firefox and the website, sometimes referred to as a Monster-in-the-Middle (MITM) attack.  We believe," they write, "this act undermines the security of our users and the web, and it directly contradicts Principle 4 of the Mozilla Manifesto that states:  'Individuals' security and privacy on the Internet are fundamental and must not be treated as optional.'"  And note that privacy is in there, too.  So tracking.



They said:  "To protect our users, Firefox, together with Chrome" - and now we know also with Safari - "will block the use of the Kazakhstan root CA certificate.  This means it will not be trusted by Firefox, even if the user has installed it.  We believe this is the appropriate response because users in Kazakhstan are not being given a meaningful choice over whether to install the certificate, and because this attack undermines the integrity of a critical network security mechanism," the whole PK, public key certificate system.



They said:  "When attempting to access a website that responds with this certificate, Firefox users will see an error message stating that the certificate should not be trusted.  We encourage users in Kazakhstan affected by this change to research the use of virtual private network software" - in other words, they're saying go around your ISP - "or the Tor Browser to access the Web.  We also strongly encourage anyone who followed the steps to install the Kazakhstan government root certificate to remove it from your devices and to immediately change your passwords, using a strong, unique password for each of your online accounts."  And so that concludes Mozilla's posting.



And that bit about changing passwords is a very good point, too, when you think about it, because any login username and password credentials which were used on those sites would have been compromised by the ISP's ability to intercept everything.  So even the login with username and password, that's all broken if they're able to intercept TLS.  In any event, Google and Apple both followed suit with Mozilla, with Google adding the errant cert's fingerprint to Chrome's CRLSET.  So the CRLSET is good for something - if not blocking revoked certificates, at least explicitly blocking certificates that you absolutely don't want to accept.  Also, Google has added that to the Chromium source code base so that blocking of that root will eventually filter out to all Chromium-based browsers.  And presumably that includes Edge, as well.



So this has been an interesting little dance that we've just watched happen, and one has to imagine that other countries, and hopefully ISPs in the U.S., are recognizing that they don't have the ability.  Yes, technically you could ask users to do this.  But the whole point is to intercept communications by browsers.  The only recourse would be to go the next step, and we talked about this at the time, which would be for Kazakhstan to take an open source browser and then make it their own, essentially.  And nobody wants that to happen because it would be very unlikely that they would keep up with current security fixes on an ongoing basis.  And, boy, forcing everyone in the country to use the Kazakhstan browser if they want access to 30 particular social and communications websites seems like a heavy lift.  So again, we're at this juncture where governments are saying it's not okay for them not be able to monitor the communications taking place within their borders, and this is creating a lot of tension.  We don't have a resolution to that yet.



So the headline on The Hacker News site was "Hackers Planted Backdoor in Webmin, Popular Utility for Linux/Unix Servers."  And that did happen.  "With over three million downloads per year, Webmin," they wrote, "is one of the world's most popular open source web-based systems for managing Unix-based systems, primarily servers running Linux, FreeBSD, or OpenBSD."  And I have to say I was unfamiliar with it.  I'm always happy at a command line.  I'm very comfortable at the command line of a FreeBSD or Linux server.



But so I took a look at the screenshot.  It's really pretty.  I mean, I found myself thinking, huh, that looks like a nice way to admin a remote server.  On the other hand, if you thought that, and you had downloaded this particularly vulnerable version of Webmin, you might change your mind.  Although, I have to say, nothing of those sorts of services is ever exposed to the public.  I can't bring myself to do that.  So I'm a big believer in filtering traffic so that anything that you can't really vouch for is just not available to the open public.  And so if I were to ever use that, I would make sure that that web portal was only available, over VPN, to systems that I have secured through other means.  But it offers a very neat-looking UI for managing users and groups, databases, the BIND DNS server, Apache, Postfix, Sendmail, Qmail, your backups and firewalls, monitoring alerts and logs and so forth.  So it looks like a nice piece of work.



Anyway, what happened was, it was with some concern that, following the surprise and irresponsible public disclosure of a critical zero-day vulnerability in this Webmin on August 10th is when this irresponsible public disclosure occurred during a presentation at DEF CON, that the project's maintainers revealed last Tuesday that the flaw, which was until then believed to be a bug, was not actually the result of a coding mistake made by the programmers.  Instead, it was the result of a secretly planted by an unknown - it was the result of software, malware, that was secretly planted by an unknown hacker who had successfully managed to inject a backdoor at some point into the project's build infrastructure which had persisted through a number of releases.  It appeared first in v.1.882 and was there through 1.921, which meant that it had been present and hidden for over a year.



So this all began when a Turkish researcher presented a zero-day remote code execution vulnerability in Webmin at DEF CON on August 10th.  He'd given the Webmin project no advance notice of either his discovery or his plan to disclose his finding.  Joe Cooper, who's one of the Webmin project's developers, said:  "We received no notification of it, which is unusual and unethical on the part of the researcher who discovered it.  But in such cases," he said, "there's nothing we can do but fix it ASAP."  And they did.



Besides revealing the flaw to the public, the Turkish researcher also released a Metasploit module for the vulnerability to automate its exploitation using the Metasploit framework.  The vulnerability, which is now tracked as 2019-15107, was introduced in a security feature that was designed to let a Webmin admin force a password expiration policy for other user accounts.  Which, and as we've talked about many times on this podcast, I've never understood the logic behind forcing people to change their password.  It just creates a mess, certainly in their little password notepads, where they're recording all their passwords.



According to this Turkish researcher, the security flaw resides in the password reset page.  And get this.  It allows a remote, unauthenticated attacker to execute arbitrary commands with root privilege on vulnerable servers just by adding a pipe command - that's the vertical bar - into the old password field through POST requests.  So in the first place, I mean, nothing about that feels like a mistake; right?  So the backdoor is, if you send a POST query to the server, which is what the browser would normally submit, if you were using a web admin, and you were going to be updating your password, you would fill in your old password and then your new password maybe even twice.



Well, that would be submitted with a POST.  So what was discovered was that, if you put the pipe vertical bar into the old password field, you could then pipe in commands through that POST submission, that POST query, that would be executed with root privilege.  So this is like, okay, that's not going to happen by mistake.  That wasn't a coding error.  It has all the feeling of something that was deliberately snuck in.



So in a blog post published yesterday, Cooper said that the team is still investigating how and when the backdoor was introduced.  But he confirmed that the official Webmin downloads were replaced by the backdoored packages only on the project's SourceForge repository, though that actually is the primary download point, and not on Webmin's GitHub repositories.  SourceForge was like the official distribution for this particular package.  Joe also stressed that the affected password expiration feature is disabled by default for Webmin accounts, which means that most versions will not be vulnerable in their default configuration, and that the flaw would only affect Webmin accounts, or Webmin admins who had manually enabled this feature.



He said:  "To exploit the malicious code, your Webmin installation must have," and then there's the nested menu tree:  "Webmin > Webmin Configuration > Authentication > Password expiration policy set to prompt users with expired passwords to enter a new one."  He said:  "This option is not set by default; but, if it is set, it allows remote code execution."  Which is not a good thing.



However, another security researcher on Twitter later revealed that Webmin v1.890 is affected in its default configuration, as hackers appear to have modified the source code to enable the password expiration feature by default for all Webmin users.  So the plot thickens.  An unknown attacker made a subtle change to a Webmin script called password_change.cgi.  This change gave attackers the ability to send a command through a special URL that an infected Webmin server would then execute with root privileges.



Anyway, it turns out in version 1.890, which had more than 421,000 downloads between June of 2018 and last weekend, when all of this got fixed, the backdoor was turned on by default.  In 1.90 through 1.92, which collectively had more than 942,000 downloads, the backdoor was active only when changed and deliberately enabled.



So a Shodan search showed that Webmin has nearly a quarter of a million Internet-exposed instances, 218,000 Internet-exposed instances.  So it is popular.  So a quarter million exposed Webmin portals, of which more than 13,000 are running the default vulnerable Webmin version 1.890.  And of course since this one is a bit old, this suggests also that those Webmin instances are not being kept current since a whole bunch of versions since then don't have it on by default.  So the threat intelligence firm Bad Packets tweeted that there are several actors now actively exploiting the Webmin vulnerability, and that that exploitation began the day after its zero-day disclosure at DEF CON.



So, I mean, this demonstrates a number of things.  First of all, these systems are not updating themselves.  It really, really should be, especially for something like this.  It also demonstrates the danger, I mean, that in fact it's hard to argue that this was not unethical for this disclosure to be made without notifying the Webmin admins because it did take them time to respond, and attacks began the day after, the next day after this disclosure.  And of course this was all aided by the fact that the guy created a Metasploit exploit, which took all the mystery out of how to do this, although I just explained it.  I mean, it's trivial to exploit.  And unfortunately, Shodan finds these for people so they don't have to even scan for them.  13,000 instances now of as a consequence of this web portal server administration tool having this backdoor installed which was present for quite a while and is still now present.  They're all subject to this remote vulnerability.



The Webmin developers promptly removed the malicious backdoor, of course, and released clean versions.  But again, it's not looking like these systems that are going to be attacked are going to get any benefit from that because they're already using one, which is very much older.  So needless to say, if any of our listeners are Webmin administrators, that is, you are using Webmin on any of your Linux/FreeBSD/OpenBSD systems, absolutely, I mean, I guess check them.  Of course, this is a big problem; right?  If something gets in, if you happen to be using the vulnerable one, or you have turned that option on such that you're vulnerable, how do you ever trust that server again?  So that's the first of several stories about the problems we're seeing with attacks on the open source supply chain.



LEO:  So.



STEVE:  So RubyGems is once again in trouble.



LEO:  Uh-oh.



STEVE:  I know.



LEO:  This just happened.



STEVE:  Yup, just happened.  A second backdoor came to light on Monday in 11 libraries available in the RubyGems repository.  According to an analysis by the developer, Jan Dintel, the backdoor allowed attackers to use preset credentials.  So they had, like, they built in, right, username and password, preset credentials to remotely execute commands of their choice on infected servers.  The malware included a variety of other capabilities including code that uploaded environment variables which are often the source for static credentials used to access databases, service providers, and other sensitive resources.  The exfiltrated material was sent to a server.  And I had to look up .ua.  That's the Ukraine.  So located in Ukraine.  RubyGems officials also found the malicious code included a miner for cryptocurrencies because, you know, why not while you're at it.  Download counts showed that the backdoor libraries had been downloaded nearly 3,600 times.  Rest-client versions 1.6.10...



LEO:  Wait, wait, what was that?



STEVE:  That was email coming in - through 1.6.13, accounting for more than 1,200 of those downloads, they were backdoored by someone who compromised an old developer account protected by a previously cracked password.  So it's unclear how the other RubyGems libraries were infected.  That was one of 11 that were.  So standing back from that a bit, both the Webmin and RubyGems libraries turn out to - because we've touched on these over time.  They're only the latest in a series of supply chain attacks which have impacted the software of the open source community.



And most people don't think twice about installing software or updates from the official site of a known developer.  I know I do it all the time.  But as software and websites have continued to tighten down their security and become more difficult to exploit, which is what we're seeing over time, the black hats have increasingly turned to exploiting this trust that we have in well-meaning developers who are doing their development in plain sight to poison the code at its source.  This increase in focus first came, I think, to light last October, when two unrelated supply chain attacks against two open source projects were discovered.  The first was the Vespa control panel, the VespaCP control panel interface; and the other was the "colourama" package that was slipped into the Python repository.  And I remember we talked about both at the time.



Then a month after that, malicious code designed to steal funds from bitcoin wallets found its way into event-stream, which is a code library with two million downloads that's used by Fortune 500 companies and small startups.  Officials from NPM, the Node JS Package Manager, said that they were hosting backdoored software, saying that the malicious code was designed to target people using a bitcoin wallet which was developed by Copay, which is one of the companies that incorporated event-stream into its app.  So it would make sense that, if you could compromise this event-stream code and get that into this NPM package, then that would achieve your ends from a malware standpoint.



Then, finally, a few months ago, in March, researchers found, as we talked about it at the time, that that first RubyGems library called bootstrap-sass was also backdoored.  And early last month we talked about the RubyGems library called strong_password being backdoored.  And then, like the attack discovered this week which infected the 11 RubyGems projects, that bootstrap-sass and the strong_password backdoors used a browser cookie function to give attackers the ability to execute code on infected servers.  That strong_password backdoor also interacted with a server in Ukraine, smiley.zzz.com.ua.  And of course the previous Ukraine server was mironanoru.zzz.com.ua, which obviously bears a strong resemblance to smiley.zzz.com.ua, making it feel like this is all a single actor.



Anyway, all of this raises the specter of what hasn't yet been discovered; right?  I mean, so there's an incredible number of open source packages that represent a tremendously beneficial resource to the computing industry.  All we can talk about are the problems that have been found.  We don't know what may be present and not found.



So HD Moore, whom we've spoken of many times, he's a network security expert, codes on open source projects.  He's the hacker who originated and developed the Metasploit framework.  He said:  "The recent discoveries make it clear that these issues are becoming more frequent and that the security ecosystem around package publication and management isn't improving fast enough."  In other words, that security needs to get tightened up.  He said:  "The scary part is that each of these instances likely resulted in even more developer accounts being compromised through captured passwords, authorization tokens, API keys, and SSH keys."



He said:  "The attackers likely have sufficient credentials at hand to do this again, and repeatedly, until all credentials are reset, and appropriate multifactor authentication and signing is put in place."  So he said the impact of open source supply chain infections is often hard to gauge also because backdoored applications might be included as an upstream dependency by another package.  So it's not just direct downloads, but it's indirect downloads as a consequence of package dependency managers pulling things down that are needed by other packages.



Anyway, finally, he said:  "The way that dependency management tools push for the latest packages by default makes a successful attack in the case of a backdoored dependency even more likely."  So anyway, it's something that we need to keep in mind, which is that we're not sure who's watching.  You know, we've talked about, for example, a concern that maybe there was subtle influence by the NSA on the entropy being produced by pseudorandom number generators, like maybe there was a way for the NSA to have skewed the output in a way that benefited U.S. law enforcement at the expense of the absolute entropy that was presumed to be generated by these things.  I mean, so there's that.  That happened in plain sight.



But it does look like the security of these open source projects is something that really needs to be paid attention to because this to some degree represents some low-hanging fruit.  While other sources of compromise are getting more secure, the bad guys are going to look for the easiest thing to infect.  And if these resources are not kept secure, then they could be subject to attack.



Chrome is going to be adding data breach notification.  We talked about Mozilla doing this.  The genesis of Chrome's decision was Mozilla's partnering with Troy Hunt's HaveIBeenPwned service to create its Firefox, they call it the Monitor, data breach notification service, which checked for the presence of users' credentials, their passwords or emails, to see whether they are among those leaked in past data breaches.  I'm sure, although that's not what the Firefox Monitor Data Breach Notification Service does, I'm sure I've had the experience, I've talked about it, in fact, on the podcast, of going to a site and receiving one of those little dropdown notifications from Firefox saying this site has suffered a breach in the past.



So it turns out that's where Chrome is going to be going.  What Google first created was what they called their "password checkup," which was a Chrome extension to perform a function similar to Firefox's Monitor Data Breach Notification.  In this case Google has been maintaining a list of credentials, much as Troy has been.  Apparently they've got four billion leaked and disclosed credentials, which the password checkup Chrome extension was able to check against.  But using the analytics which was built into this password checkup extension, Google was able to get a sense for what people were doing.  They conducted a study using the analytics which showed that 1.5% of all logins have been compromised in data breaches, 1.5%.  The study also showed that 26% of users, so just over one out of every four, who were shown a data breach notification, changed their password as a result.  So it wasn't ignored.  It was like, oh, crap, I'm going to change my password.



Since this study demonstrated to Google that providing notifications of compromised login credentials was beneficial to users, Google is now building the support directly into Chrome.  So once this feature is in place, Chrome will begin alerting users when they are logging into sites with credentials that have been exposed by breaches, much as that notice I'm sure I received from Firefox was.  So not that your credentials specifically have been exposed, or not that there's been a collision of your password and email with, for example, the HaveIBeenPwned database that Troy maintains, but just this website has been exposed to a breach.



And I remember when I talked about this now with Firefox.  I liked that because that's not something that any website has control over, if Firefox and Chrome both note that for future visitors.  And that's a blemish that no website wants to carry.  And I think I remember noting that maybe it ought to expire after some number of years.  It's not something that ought to be, like, forever and ever.  But still it would provide additional incentive for a website to work for that not to be true of them.



So there's also a Check Your Passwords button.  In the reporting of this and in Google's own disclosure, they showed some screenshots, and there was a Check Your Passwords button.  But nobody that I could find was sure what it was that that would do.  Maybe that will - it might be that the dropdown says this website has suffered a breach in the past, and then you can have it explicitly check your password to see whether there's any collision that Google is able to find with those that have been disclosed in the past.



I wanted to briefly mention that I was surprised that so much was made of basically a code regression mistake that Apple made in their recently released v10.4...



LEO:  12.4.



STEVE:  Oh, right, I wrote that [crosstalk] 12.



LEO:  Yeah, I saw you had 10; and I thought, it's 12.



STEVE:  Yeah, yeah, yeah, it is 12.4 because of course we're all waiting for 13.



LEO:  Right.



STEVE:  Lucky 13 in a couple weeks.



LEO:  Right.  



STEVE:  So, yes, 12.4, thank you.  What they did was they had a bit of a code regression.  There was a jailbreak which had been found in an earlier version of iOS and fixed.  And a hacker noted that it had reappeared.  Oh, it was fixed in iOS 12.3, and then reappeared, to everyone's surprise, in 12.4.  Apple quickly said whoops and fixed it in 12.4.1, which we all now are using, those of us who've updated our iOS devices.  That got fixed.  But it was interesting, I mean, it was just like, all of the tech press was breathless over the idea that a jailbreak was now available.  Of course, you know, I guess it's because they're so rare and would be sought-after.  If they were kept secret, and if nobody told Apple, it would be handy for people who like the flexibility of jailbreaking their phones.  I mean, once upon a time it was, like, common.  And now it's like, no, that's not something Apple wants to allow to have happen.



And lastly, Microsoft's RDP client for Android needed and received an update.  We've recently been discussing the various concerns over vulnerabilities in the Remote Desktop Protocol, and then subsequently in the Remote Desktop Services, which is an enterprise-grade service where users can actually connect to their desktop remotely, you know, multiple people hooking to a licensed server, that's licensed for that.  Whereas those of us who have standard Windows, we're able to connect one session to Windows, and it logs off the desktop session if you happen to be logged on at the same time.  And then, of course, the most notable of the RDP Protocol was BlueKeep that everyone keeps expecting to go into a worm, but hasn't so far.  I've seen other commentary, I mean, my theory is it's not difficult enough to be worth doing.  Others have said it is not easy enough for a worm to take over.  So anyway, we'll see.



But recall that some time ago, predating these, there was also a means for Remote Desktop Servers to sort of reverse compromise remote desktop clients.  The trouble was the recurring problem with vulnerabilities in interpretation.  And in this case RDP clients, which are displaying the desktop, they are inherently interpreting and trusting the data they receive from the RDP servers, in order to draw the desktop.  They are doing lots of interpretation.  We talked about it at the time, that there was a way that a client could be taken over if it connected to a malicious server.  And at the time I said, you know, that seems unlike - it's not like a big worry because people are not running around connecting their remote desktop clients to random Windows servers.  Normally you're just connecting to your own machine at home or to an enterprise system.  But still, it should get fixed.



Anyway, it turns out that Microsoft has an RDP client for Android, and that there were also similar exploits possible against it.  So Microsoft has patched that.  So although the threat is minimal, I just wanted to put it on our listeners' radar that it was worth, if you were a person who used an Android device for RDPing to some Windows system, again, if you're RDPing to your own system, it's unlikely that your own computer is going to attack your Android client.  But I'm not sure what the update channel is for Android software from Microsoft.  So if that's something that you use, why not update?



I did mention, and I'll say more, of SQRL.  I just finished the third of the four SQRL documents.  That was SQRL's cryptography documentation.  And I am now on, well, I will shortly be on SQRL's what I call "On the Wire," which is basically, it's the fourth of the four documents, and this is all of the - basically everything that's left, since it's the last document, which is the way you format the keys and the signing and coding and so forth into ASCII for transmission on the wire and what the various bits mean in the protocol and so forth.  So I will be starting that.



The reason I haven't quite is that after Thursday's Orange County OWASP SQRL presentation, I decided to spend a little bit of time on my PowerPoint presentation.  First I should say that Thursday's OWASP presentation was truly terrific.  I think it was 117-some people had signed up, which was between two and three times the group's normal size based on previous numbers that I saw in Meetup.  And I asked for a show of hands of how many people there were Security Now! podcast listeners.  And I expected, you know, maybe half?  Everybody's hand went up.  Which was surprising.  And then I said, okay, wait a minute.  Who here is not a Security Now! podcast listener?  And maybe two or three hands were short of sheepishly raised.  And then I said to them, I said, well, maybe there's a clue there.



Anyway, it was really a fun evening.  And I got to meet a lot of the podcast listeners for the first time.  But my concept, and I think I mentioned it earlier, was I wanted to see if I could just have all the diagrams on the screen and randomly select them, rather than following a bullet-pointed presentation.  And I found that that didn't really work, either.  But I got a sort of an updated inspiration for the SQRL presentations.  And so I'm spending a little time working on that.  And then as soon as that's finished, I'm back to the fourth of the SQRL documents, which will be "SQRL on the Wire."



Also the two upcoming presentations which are taking me out of the country, in response to a tweet from OWASP in, and I can't pronounce this the way they do, so I'll just say Gothenburg, which is in Sweden.  So OWASP...



LEO:  You'll learn it before you get back.  You'll know it.



STEVE:  Probably will.  Yeah, GBG said...



LEO:  I'll buy you some Slivovitz if you can do it right.  It helps.  The more Slivovitz you have, the more you sound right.



STEVE:  So they tweeted two days ago:  "Yihaa," I guess, "Yihaa, what an amazing response we got for the event with Steve Gibson @SGgrc!!"  They wrote:  "200 tickets, and we're fully booked."



LEO:  Oh, great.



STEVE:  And they actually said 218 in a private email, so he says:  "As usual, you can sign up on the waiting list through  Eventbrite."  And then after that I shot a note to Mick Ryan, who's the organizer of the Dublin, Ireland event.  And he wrote back:  "It's full, Steve!  300 people."



LEO:  What?  What?



STEVE:  He said:  "The room fits 200 people seated, with lots of standing room also."



LEO:  You've got fans all over the world, my friend.



STEVE:  I guess so.  



LEO:  That's awesome.  That's awesome.



STEVE:  So anyway, but so I just thought I would give a hint to our listeners.  Unfortunately, maybe they're all going to be our listeners.  They're 50% overbooked.  So it might be a good idea to show up early if you want to get a seat.  And I was mentioning this to Lorrie last night, and she said, "Well, yeah, but they could sit in the aisles and on the floor and so forth."  So it's like, yeah, well, okay, that's true.  But the full - this presentation ends up being like one of our podcasts.  It's about two hours of just nonstop information transfer.  So you really would rather not stand for that, if you didn't have to.  So I imagine that people will be sitting.  And, boy, I mean, the time just flies by.  So we're going to have a lot of fun a month from now in Dublin and then wherever that is in Sweden, Gothenburg, something.  And again, GRC.com/calendar will inform you of those events.



LEO:  So great.  Wow.  I'm impressed.



STEVE:  Yeah, yeah.  And I'm still on the trek with the file sync journey, which you and I, Leo, will be doing a podcast devoted to that here in a couple weeks.  I'm looking around at different things.  I'm keeping up with Twitter and submissions.  I'm beginning to get a sense for what I like.  So we will do a complete rundown of options and sort of clever tricks that I've come up with on the podcast a few weeks from now.  So it'll be good.



Let's take our last break and then talk about the ratification of ads on the Internet.  I mean, they're going to stop being, I think I see this, it's going to make sense, stop being sort of an ad hoc, shoehorned-in thing that no one likes and become probably well-behaving citizens, thanks to some work that Google is doing.



LEO:  You're such an optimist.  But we need it.  It's the ecosystem.  We want great web content.  It's got to be paid for somehow.



STEVE:  Yup.  Exactly.



LEO:  There's got to be a way to do it.  All right.  Ad privacy with Mr. Gibson.



STEVE:  So Apple, who of course doesn't depend upon revenue from web ads, has been moving forward on the anti-tracking front for some time.



LEO:  Same thing with Firefox; right?  I mean,  it's not their business; right?  So they don't need to worry about it.



STEVE:  Yes, yes, yes.  Exactly.  So on June 5th of 2017, so a little over two years ago, we got from Apple Intelligent Tracking Prevention.  And we talked about it at the time.  We liked it.  The idea was that they were going to be expiring cookies after some length of time so they didn't persist forever.  So it was sort of a nice compromise.  You would sort of, like, have the right to be forgotten if there wasn't something keeping the cookie persistent.



Then on March 14th of 2018, we got the update, Intelligent Tracking Prevention 1.1.  Then on June 4, a few months after that, 2.0.  Then February 21st of this year, 2019, we went to Intelligent Tracking Prevention 2.1.  And then in April of this year, ITP 2.2.  And then, most recently, toward the end of May, on May 22nd, Apple posted Privacy Preserving Ad Click Attribution for the Web, which is Apple's attempt to essentially solve this problem.  My point is I'm not going to go into this in great detail because I don't think it makes sense until we get some consensus, and we're still pre-consensus.  But Apple has a proposed solution - Google refers to it in their own brainstorming for a solution - of preserving ad click attribution while also preserving privacy.



So my point is that there is a, you know, we've seen Apple continuing to refine tracking prevention.  Apple has put their stake down as "We're the security and privacy protecting company," and we're seeing them move on this.  Certainly, as you said, Leo, Google's revenue is from advertising.  We know that, and Google tells us.  That's why all this stuff can be free is advertising revenue.  So what appears to be happening is that we're approaching an inflection point for the industry, the fact that it's sort of a convergence.



As I said at the beginning, advertising and tracking has just been ad hoc to this point.  What annoyed me was that it was abusing a system that was never intended for third-party tracking.  Cookies were meant to create a stateful relationship with the site you were visiting.  But then things like the Like buttons and where there is a little presence of Facebook Like all over the place, well, your browser, that's a link to Facebook which your browser retrieves.  And in doing so, it sends Facebook the cookie you have with them in that query.  So they know where you are.



And of course the same thing is true for ads.  Apparently, I mean, it's not just theoretical that the ad companies desperately want to track us because when Chrome does something like blocking fingerprinting by - like we were just talking about the Incognito mode.  Well, in this case that wasn't advertising tracking, it was The New York Times and the Washington Post saying, wait a minute, we're not going to show you anything if you're in Incognito mode because we want to be able to meter our free sample of our paid content, and you need to drop Incognito mode.  So Chrome said, if users are incognito, then they need to be incognito.



Last Thursday over on the Google side, Justin Schuh, I guess that's how you pronounce it, S-C-H-U-H?  Schuh, maybe?  He's the director of Chrome Engineering.  He posted two short overviews about what Google is calling their Privacy Sandbox initiative.  And they're both short.  I want to share them, and then we'll dig more into the details.



The first blog posting was titled "Building a More Private Web."  He said - and yes, I understand we have to consider the source.  He said:  "Privacy is paramount to us, in everything we do."



LEO:  [Buzzer sound]



STEVE:  Okay.  Right.  He said:  "So today" - yeah, we who read all of your email on Gmail.



LEO:  That's a bad lead.  That's really a bad lead.



STEVE:  Yeah.



LEO:  Okay, go ahead.



STEVE:  Okay.  Again, consider the source.



LEO:  I don't believe anything he says after this.  Go ahead.



STEVE:  Have your buzzer ready, Leo.  He said:  "So today we are announcing a new initiative to develop a set of open standards to fundamentally enhance privacy on the web.  We're calling this a Privacy Sandbox."  He says:  "Technology that publishers and advertisers use to make advertising even more relevant to people is now being used far beyond its original design intent, to a point where some data practices don't match up to user expectations of privacy."



LEO:  Okay.  That's true.



STEVE:  That's true.  "Recently, some other browsers have attempted to address this problem," which is like saying we're not happy with what Apple is doing because they're really saying they're going to...



LEO:  And Firefox, yeah.  Yeah, Firefox is really doing a great job on this.



STEVE:  Yup.  He says:  "But without an agreed-upon set of standards, attempts to improve user privacy are having unintended consequences.  First, large-scale blocking of cookies undermine people's privacy by encouraging opaque techniques such as fingerprinting.  With fingerprinting..."



LEO:  Okay.  Put an asterisk there because I disagree with that, but go ahead.



STEVE:  Okay.  "With fingerprinting, developers have found ways to use tiny bits of information that vary between users."  And of course we know, and maybe this is the nature of your asterisk, is that fingerprinting is not unique to an individual.  You know, when you go to Panopticlick, for example, to look at it, it shows you how many bits of entropy they found, and how many other users had the same fingerprint as you.  Meaning, you know, you're not a unique little flower.  You're one in a cohort.



LEO:  My asterisk is really, yes, that's true, that's a bad thing.  But that doesn't mean, well, we've got to keep cookies.  They're both bad things.  But go ahead.



STEVE:  Yes, yes, thank you.



LEO:  That doesn't justify cookies, let's put it that way.



STEVE:  Correct.  So they say little "...bits of information that vary between users such as what device they have or what fonts they have installed to generate a unique identifier which can then be used to match a user across websites.  Unlike cookies, users cannot clear their fingerprint, and therefore cannot control how their information is collected.  We think this subverts user choice and is wrong."  Okay.



LEO:  And I should point out that both WebKit (Apple) and Firefox have done some things to block fingerprinting.



STEVE:  To thwart fingerprinting, yes.



LEO:  Fingerprinting is thwartable.



STEVE:  Yes.  He says:  "Second, blocking cookies without another way to deliver relevant ads significantly reduces publishers' primary means of funding..."



LEO:  On that I'll agree, yeah.



STEVE:  Yes.  And I was really affected, Leo, by - I was listening before our podcast once sometime ago to your MacBreak Weekly, where you were able to talk to, I think it was Rene, about the degree to...



LEO:  Oh, it's killing iMore, yeah.



STEVE:  Yes, the degree to which they're dependent upon advertising for revenue.  And it was the more effective blocking of cookies and what - or, well, blocking of ads actually.



LEO:  And yet...



STEVE:  It was the ad blockers is what it was.



LEO:  And yet I would point out, we can, without any tracking at all, make a good living here at TWiT.  Advertisers continue to buy ads.  NBC, CBS, and ABC continue to sell ads without any tracking.  So there's a lot of media that works quite well without tracking.  The problem is advertisers want it.  They wish they could get it.



STEVE:  Right, exactly.



LEO:  Despite the evidence it doesn't help, by the way.  There's very little evidence that tracking does much good.



STEVE:  Well, now, and that would be worth digging into because what he's about to say is the first number I've seen.  And I've always wondered whether this was just completely bogus or not.  And we still don't know.  But he said:  "Many publishers have been able to continue to invest in freely accessible content because they can be confident that their advertising will fund their costs.  If this funding is cut, we are concerned" - yeah, "we," right, Google - "are concerned that we will see much less accessible content for everyone.  Recent studies" - and here it is.  "Recent studies have shown that when advertising is made less relevant by removing cookies, funding for publishers falls by 52% on average."



LEO:  Okay.  That is a study from Google.



STEVE:  Uh-huh.



LEO:  Based on analysis of a random selection fraction of traffic on each of the 500 largest Google ad manager publishers over three months.  That's not study after study.  It's one paragraph in a Google blog post from Google ads.



STEVE:  Okay.



LEO:  So that's also a little disingenuous.  There isn't study after study.  This is a Google kind of not such a great study.



STEVE:  Right.  So when they say "recent studies" have shown, it would be a study we conducted showed...



LEO:  Yeah.



STEVE:  Okay.



LEO:  Which is a randomly selected fraction of traffic on each of the 500 largest Google ad manager publishers.



STEVE:  Right.



LEO:  And that's where the 52% comes from is that study.



STEVE:  Okay.  So he said:  "We are doing something different.  We want to find a solution that both really protects user privacy and also helps content remain freely accessible on the web."  He said:  "At I/O, we announced a plan to improve the classification of cookies, giving clarity and visibility to cookie settings, as well as plans to more aggressively block fingerprinting.  We are making progress on this."  But where we're headed is different.  He says:  "And today we are providing more details on our plans to restrict fingerprinting.  Collectively, we believe all these changes will improve transparency, choice, and control.



"But we can go further.  Starting with today's announcements, we will work with the web community to develop new standards that advance privacy, while continuing to support free access to content.  Over the last couple of weeks we've started sharing our preliminary ideas for a Privacy Sandbox, a secure environment for personalization that also protects user privacy.  Some ideas include new approaches to ensure that ads continue to be relevant for users."  And I've never found them to be relevant for me, but what the heck.



LEO:  That's to me the real issue; right?



STEVE:  Yeah, I mean, I just...



LEO:  More relevant ads?  No.



STEVE:  No.  Okay.  "But user data shared with websites and advertisers would be minimized by anonymously aggregating user information, and keeping much more user information on-device only."  That is on-hyphen-device only.



LEO:  Sounds good, yeah.



STEVE:  It does.  It sounds good.  "Our goal is to create a set of standards that is more consistent with users' expectations of privacy.  We're following the web standards process and seeking industry feedback on our initial ideas for the Privacy Sandbox.  While Chrome can take action quickly in some areas - for instance, restrictions on fingerprinting - developing web standards..."



LEO:  Wait a minute.  I thought you couldn't do that.



STEVE:  Right. 



LEO:  It's kind of contradicting what he said earlier.  Oh, no cookies, but you get more fingerprinting.  Oh, you can block that.  Oh, okay.



STEVE:  Well, what he said was, if you restrict cookies, then you're driving advertisers to find some other means, and he used fingerprinting as an example of what they would resort to if you blocked cookies.  He said:  "Developing web standards is a complex process, and we know from experience that ecosystem changes of this scope take time.  They require significant thought, debate, and input from many stakeholders."  And of course we want everybody involved because there are a lot of privacy advocates, you know, the EFF, they'll jump into this and get their two cents' worth in.  So that's good.



He says:  "To move things forward as quickly as possible, we have documented the specific problems we are trying to solve together, and we are sharing a series of explainers with the web community.  We've also summarized these ideas today on the Chromium blog."  Which is where we're going to go next.  "We look forward to getting feedback on this approach from the web platform community, including other browsers, publishers, and their advertising partners.  Thank you in advance," he finishes, "for your help and input on this process.  We believe that we must solve these problems" - and I want, I mean, this is all sounding good, depending upon what it is - "together to ensure that the incredible benefits of an open, accessible web continue into the next generation of the Internet."



So now we switch to his posting on the Chromium blog, which gets into more nitty-gritty, titled "Potential Uses for the Privacy Sandbox."  And he says:  "Today on The Keyword" - which is what I just read - "we outlined our vision for an initiative aimed at evolving the web with architecture that advances privacy while continuing to support a free and open ecosystem.  In order to work toward that vision, we have begun publishing a series of explainers that are intended to be shared and iterated on across the community.  Below, we've summarized each of these early proposals, which we are collectively referring to as the Privacy Sandbox."



So first is user information.  He says:  "First, let's identify how user information is currently used in the advertising ecosystem so that we can explore the development of the Privacy Sandbox's privacy-preserving APIs."  And here I'm kind of glad that they've got an advertiser, that they own an advertiser, because they also own a browser.  And it would be nice, I mean, so they understand what the advertiser's view is.



LEO:  No coincidence, of course.



STEVE:  I know, of course.  So then ad selection.  For ad selection they said:  "One of the most challenging questions is what your browser could do to allow a publisher to pick relevant content or show a relevant ad to you" - and I still say good luck with that - "while sharing as little information about your browsing history as possible.  We're exploring how to deliver ads to large groups of similar people without letting individually identifying data ever leave your browser, building on the differential privacy techniques we've been using in Chrome for nearly five years to collect anonymous telemetry information."  There are some cool crypto hashing things that can be done.



Anyway, he said:  "New technologies like Federated Learning show that it's possible for your browser to avoid revealing that you are a member of a group that likes, for example, Beyonc and sweater vests until it can be sure that group contains thousands of other such people."  In which case...



LEO:  Let me interrupt before you move on.



STEVE:  Yeah, good.



LEO:  Because some people have talked about this.  And the issue is not merely that I don't want you to know who I am, but that you don't want to assign me to a group.  Let's say that you've collected over time information that leads me to think that I am bipolar, and that I'm in a manic phase.  And, oh, good, I've got 50,000 other people bipolar in a manic phase.  I know they're going to be particularly susceptible to a certain kind of advertising.  Admittedly, you don't know who is, what their name is, but you still can target them in a vulnerable situation.  You could still use it for a targeted ad in political situations.  Are you a gun owner?



I mean, in other words, this is a little disingenuous in the sense that, yes, you're protecting an individual's personal information, but it's not stopping the kind of targeting that could be and has been used in the past, thanks to Facebook and Cambridge Analytica, in a very detrimental way.  So I just - I want to point out it doesn't eliminate all hazards.  There's still this hazard.



STEVE:  I agree.  And in fact, to broaden that a little bit, what you're saying is that there is a fundamental tension - I mean, fundamental, not about technology - between what advertisers want and what users want.



LEO:  In a nutshell.  That's exactly it.  That's exactly it.  And you can come up - because we're technologists, we love technological solutions.  The idea of differential privacy is fascinating.  But you nailed it.  That's exactly what the tension is.  Advertisers want this information.



STEVE:  And that's not going to go - yes.  And there was, many, many years ago, what was the show with Paul Reiser?  He was married to a woman...



LEO:  I know what you mean, yeah.  "How I Met Your Mother" or something.  Anyway, yeah, yeah.



STEVE:  Yeah.  It was just - it was about their relationship.  And he was there, and TiVo was sort of a new thing.  And he was watching sports.



LEO:  "Mad About You."  "Mad About You."



STEVE:  "Mad About You," yes.  "Mad About You."  He was watching football or something.  And his wife walked in and says, "What are you doing?"  And he says, "I'm watching sports."  And she says, "Why?"  And he said, "TiVo thinks I'm gay."



LEO:  There was a whole article about that, actually.  That really happened to somebody, yes.  Too many thumbs up to the wrong shows.



STEVE:  And so TiVo was profiling and doing recommended shows, and it had decided - anyway, I got a kick out of that.



LEO:  The Netflix recommendations, the Amazon recommendations, the TiVo, it just doesn't work.  But even - but what if it did?  That's almost even more terrifying.



STEVE:  Well, and exactly to our point is that you brought up the point, which I think is accurate, I mean, it obviously is, that advertisers want something we don't want them to have.



LEO:  Right.



STEVE:  And isn't it our right to deny them that?



LEO:  And I would say of course they want it.  They ask us for this all the time.  Can we put ad trackers in, you know.  And there's a huge movement afoot in the podcast industry to add ad tech to podcasting, which I'm virulently against because you see what ad tech did to blogs and online periodicals.  I don't want that to happen to podcasting.  But there is a strong movement because it's demand from advertisers.



STEVE:  And so you ran into this when you were in Florida two weeks ago?



LEO:  Absolutely.  In fact, there's new ratings techniques.  There's all sorts of ways that - get ready.  Somebody came up to me, said, "Oh, there's this new technology.  We put a little subsonic sound in the podcast, and then we can match it."  It's bad.  It's really bad.  And I don't - I know our audience doesn't want that.  And I don't want to participate in it.  And, oh, by the way, our ads work without that.  Of course advertisers are going to want everything they can get.  But we don't have to give it to them.



STEVE:  Yeah.  And it's not like ads become zero effective.  We saw, I mean, if we believe - well, first of all, we don't know if we can believe the number that Google said, that is, it cuts the revenue in half.



LEO:  No, that's made up.  And there's a lot of evidence that it doesn't, that in fact there's academic studies that show...



STEVE:  I've always wondered.



LEO:  ...a small, like, well, I'm looking at an article from Princeton's Center for Information Technology Policy about people.  I don't if you've read this:  "Deconstructing Google's Excuses on Tracking Protection."



STEVE:  No, I've not read it.



LEO:  Highly recommend it.  And one of the things they say is one of the academic studies that's out there says that tracking helps by about 4%, which is, I think, small enough that we can, without putting people out of business, say, yeah, let's not do that.



STEVE:  Yeah.



LEO:  New York Times International Edition recently switched from tracking-based behavioral ads to contextual and geographic ads, and it did not experience any decrease in advertising revenue.



STEVE:  That's interesting.  So not who you are at all, just what your IP reveals about where you are.



LEO:  Yeah.  We don't do that, by the way.  But there are, and I think we probably will end up doing that, there's something called "direct insertion ads."  Our ads are live, and they're built into the show.  But most podcasters are moving to ads that are inserted, and it has to do with, if you download it from an IP address, we know where you are geographically.  So somebody - see, right now when you buy an ad, you buy an ad for the entire TWiT audience.  But if somebody wanted to buy an ad for the West Coast or England, they could do that with direct insertion.  Only people in that geographic region would get that ad.



STEVE:  Well, and there's clearly some means for that happening in a TV stream because I sometimes get, like, local car dealership ads on CNN, which is...



LEO:  Right.  So CNN sells national ads - same thing with my radio, my national radio show - which are heard throughout CNN.  And there's a gap.  In that gap, local affiliates can put in their local ads.  So your cable company is doing that.  They're putting in local ads in the national feed.  And that happens across all networks.  So that's geographic.  That's a good example.  Because if you're a car dealer in Irvine, you don't want to have Leo see your ad.  It's a waste of money.  So there are things that - this is a Wall Street Journal article from August 27th, 2019.  Why, that's today.  "Behavioral ad targeting not paying off for publishers, study suggests.  Are creepy ads necessary to support the free web?  A new academic study suggests they're not."  So look at The Wall Street Journal today.



STEVE:  So "creepy," I mean, that reflects...  



LEO:  Creepy's subjective, I understand, yeah.



STEVE:  Well, but that's the way I've heard...



LEO:  Feels that way.



STEVE:  ...people describe it.  If they're, like, talking to somebody - in fact, I think it was Joe Scarborough who thought that, like, Siri was listening to him because he was talking about something, then when he went on the web he saw ads for it.  It's like, uh, no.



LEO:  Joe's a little...



STEVE:  I don't think so, yeah.



LEO:  So this is a study from University of Minnesota, University of California Irvine, Carnegie Mellon.  It suggests publishers only get about 4% more revenue for an ad impression that has a cookie enabled than for one that doesn't.  The study tracked millions of ad transactions at a large U.S. media company over the course of a week.  That's still a small sample.  However...



STEVE:  Yeah, but still, it's not coming up with a lot of uncertainty.  I mean, 4%...



LEO:  It's pretty low.



STEVE:  If they're showing 4% even on a small sample, it's not actually going to be 50%.



LEO:  But this confirms your hypothesis.  A 2009 study showed that advertisers are willing to pay 2.68 times, 268% more, for a behaviorally targeted ad than one that wasn't.  It's worth, they think - they're wrong - 268% more.  That's why this is happening.



STEVE:  Interesting.  Yeah.



LEO:  Yeah.  Isn't that interesting?  This is today in The Wall Street Journal.  I think this is - you are exactly right.  This is a conversation and a battle that has just - that is important and is just beginning because nobody wants to kill the web.



STEVE:  No.



LEO:  We want these people who - ad supported media is a good model.  I like it.  That's what I've lived on for 43 years.  So we don't want to kill it.  But we want to do it in a respectful way to our listeners or readers.



STEVE:  Yeah.  Well, it'll be interesting to see how this evolves.  It sounds as though we may get more controls.  And moving this from an ad hoc - although it also sounds like advertisers may choose to go around this if we are given too much control.



LEO:  Well, that's true, too, yeah.



STEVE:  Because of course why wouldn't they?  So they have a series of explainers.  I've got the links in the show notes for anyone who is interested.  They have one called "conversion measurement."  And they describe that as "click-through conversion measurement event-level API."  It is an explainer for a potential new web platform feature which allows for measuring and reporting ad click conversions.  Then there's something called the "trust token API."  This document is an explainer for a potential future web platform API that allows propagating trust across sites.  And that uses what's called a Privacy Pass protocol that actually is something that the Cloudflare guys came up with a couple years ago.



LEO:  I trust them.



STEVE:  I do, too.  So, I mean, I do feel like there are some interesting concepts here.  They have something called the "privacy budget," and they said "combating fingerprinting with a privacy budget."  And they explained it as the current state of affairs is browsers have been making changes to how cookies are treated.  "Blunt approaches" - and here's that Google again, not happy.  "Blunt approaches to cookie blocking have been tried.  And in response, we have seen some user tracking efforts move underground, employing harder-to-detect methods that subvert cookie controls.  These methods, known as 'fingerprinting,' rely on various techniques to examine what makes a given user's browser unique.  Because fingerprinting is neither transparent nor under the user's control, it results in tracking that doesn't respect user choice."



So their "end state to aim for" is:  "Fundamentally, we want to limit how much information about individual users is exposed to sites so that in total it is insufficient to identify and track users across the web, except for possibly as part of a large, heterogeneous group, or heterogeneous groups."  And of course, Leo, we keep coming back to your point, which is people don't even want to be grouped.  You know?  People just want to be anonymous.



LEO:  Yeah.  Anonymous is anonymous, not like in a big group.



STEVE:  Right.  And in fact they address - this next one they call "FLoC," Federated Learning of Cohorts.  They said:  "This is an explainer for a new way that browsers could enable interest-based advertising on the web, in which the companies who today observe the browsing behavior of individuals instead observe the behavior of a cohort or FloC of similar people."  They said:  "The choice of what ads to show on a web page may typically be based on three broad categories of information: One, the site or page, irrespective of who is reading it."  And that's a good point.  Put this ad on web pages with motorcycles, for example.



LEO:  Yeah.  That's what people who buy TWiT are doing; right?



STEVE:  Yes, exactly.  And that makes absolute sense.  I mean, and why not?  "Two, general information about the interests of the person who is going to see the ad, for example, show this ad to classical music lovers."  Okay.  And, "Three, specific previous actions the person has taken, for example, offer a discount on some shoes that you left in a shopping cart.  This document," they said, "addresses category two, ads targeting based on someone's general interests."



They said:  "In today's web, people's interests are typically inferred based on observing what sites or pages they have visited, which relies on tracking techniques like third-party cookies or less transparent mechanisms like device fingerprinting.  It would be better for privacy if interest-based advertising could be accomplished without needing to collect a particular individual's browsing history.  We plan to explore ways in which a browser can group together people with similar browsing habits, so that ad tech companies can observe the habits of large groups instead of the activity of individuals.  Ad targeting could then be partly based on what group the person falls into."  And I don't, you know, how do you do that?  I mean, there are so many different, like, talk about a Venn diagram that would give you heartburn.



LEO:  Well, that sounds like tracking still.



STEVE:  It does.  "Browsers would need a way to form clusters that are" - browsers, that is, web browsers - "would need a way to form clusters that are both useful and private, useful by collecting people with similar enough interests and producing labels suitable for machine learning, and private by forming large clusters that don't reveal information that's too personal when the clusters are created, or when they're used."



So in other words, what that would sort of - the way that would happen would be that websites would have like a huge bitmap of things they could turn on that describe what they are.  The browsers would collect those bitmaps as you went from browser to browser and accrue them.  And then the browser would be able to present, would then be able to re-aggregate its user based on the aggregation of bitmaps that it had received from websites over time.  Which, if I'm right, that's a lot of work.  So who's going to do all that?  It's not clear.



Anyway, they finish, saying:  "A FLoCK," that is, what was the FLoC, was Federated Learning of Cohorts, so they then add a "K" to the end.  The FLoC Key, or F-L-O-C-K, the flock, "is a short name that is shared by a large number, thousands of people, derived by the browser from its users' browsing history."  So again, there's got to be a way for browsers to compare notes in order to do this.  "The browser updates the flock over time as its user traverses the web.  The value is made available to websites via a Client Hint."  Okay, so actually this wouldn't have to be the individual sites.  The browser's publisher, like Mozilla, could know about the sites you visit, and then it could be doing the aggregating in order to create these flocks that then its users are part of.  So that would be a way of doing it.



And then, finally, they said:  "A Privacy Model for the Web."  "Sharding" is what they call it, sharding web identity.  "The identity model for the web has been the implicit result of two interacting browser capabilities:  per-domain state, especially cookies, which let one top level domain plus one" - that is, like GRC.com, TWiT.tv - "maintain a consistent notion of a visitor's identity.  This identity extends across top-level sites due to third-party cookies, storage within frames, et cetera."  And then the second thing is "in-browser passing of information among the parties co-occurring on a web page," in other words, by sharing state, the DOM, JavaScript, HTTP redirects and so forth, like where an ad is on a site.  That site is able to get information shared with the third parties that it's sharing content with.



They said:  "This combination has led to widely shared cross-site identities, and so to an ability to perform web-wide tracking of a person's browsing activity.  Global static identifiers like device fingerprinting or like personally identifiable information provided by or covertly taken from the person browsing also offer an independent path to global identity.  Limitations on cookies, fingerprinting, and other browser state all aim to reduce this ability to create or access a global identity."



So they say:  "On the one hand, global identity gives rise to the capacity to weave together a record of much of a person's browsing history, a core privacy concern with today's web.  Browsers are well-positioned to take action on this issue" - and of course this is the genesis of all this is Google is seeing this is happening, as you noted, Leo, with Apple and Firefox - "by imposing limits on the underlying capabilities exposed to developers.  On the other hand, global identity also plays a substantial role in today's web advertising ecosystem.  Browsers that impose limitations on these technical capabilities can directly affect publishers' economic viability and encourage workarounds, if they haven't provided for the legitimate needs of the ecosystem.  This document describes a way the web could potentially work that would not require cross-site tracking, but would still let publishers support themselves with effective advertising."



They conclude:  "We need a dialogue within the web platform community  including browsers and the many stakeholders that thrive in and contribute to the ecosystem  so that we can clearly describe a new identity end state that works for everyone.  For browsers, this gives us a framework for evaluating proposed changes, standards, and APIs; what boundaries we need to enforce; and where we should innovate to improve the web.  For developers, a clear long-term vision provides stability and predictability of the changes along the way.



"Any discussion of a new identity model must answer two specific questions.  First, across what range of web activity does the browser let websites treat a person as having a single identity?  And, two, in what ways can information move across identity boundaries without compromising that separation?  This document offers one possible answer to these questions.  The goal is a balanced way forward, dramatically improving web privacy" - or I guess we would say dramatically altering the nature of web privacy because we've seen that advertisers really don't want true privacy - "while allowing enough information flow to carefully support key needs of publishers and advertisers."



So anyway, I'm interested to see how this develops.  And it does look like there is a non-tracking solution, but it's still going to be profiling because that's what advertisers want.  And it's certainly - so this sort of changes our complaint.  Our complaint has been "I'm being tracked, and I'm being profiled based on where I'm going."  Google is saying, okay, we can arrange not to track you if you will still let us profile you.  And so then we need to say, okay, was it tracking we objected to, or was it profiling?



LEO:  Yeah.



STEVE:  And I think it's probably all the above.



LEO:  It is such a challenge.  I don't know if there's a good answer, and certainly Google's voice is welcome in this.  I just, you know...



STEVE:  And the idea that they're opening this discussion, and it's going to be open standards, and maybe we'll get something to replace cookies and give users more control.



LEO:  Is it cynical to say...



STEVE:  But, you know, I would just turn it off, Leo.  I never - I don't click on an ad.  I don't think I have in my entire life.  I'm sorry.



LEO:  Well, I have bought way too many...



STEVE:  Is it cynical to...



LEO:  I've bought way too many Instagram things to think that ads don't work, Instagram things I don't need.  I'm wearing my Instagram...



STEVE:  You were saying is it...



LEO:  Is it cynical of me to think that Google's intent here is to enter the standards process and come out with nothing in a few years?  That's what happened with Do Not Track, by the way.  Six years in the making.



STEVE:  I know.  I know.



LEO:  And often...



STEVE:  And when you said to me, Steve, you're such an optimist, I was thinking, oh, yeah, I was all for DNT.



LEO:  DNT.  I mean, I don't want to be cynical, either.  I want to be open to the idea that this is an important discussion that we have to have.  And there are competing interests, and not all of them are malign.  But I also - you could say cynically, well, Google's doing - this is a common strategy, which is to kind of embrace - the bear hug strategy.  Embrace it.  It makes you look good.  And just make sure nothing happens.



STEVE:  Yes, I think Microsoft had...



LEO:  Yeah, it's a bear hug.  So I just don't know.  I don't know.  We will watch with interest.  And, you know, the funny thing is this is a conversation that happens on Wednesdays on TWiG almost every week.  You know, I mean, it's a really tough conversation, and I don't know if there's any answer to it, yeah.



STEVE:  Yeah.



LEO:  I have been lately using Firefox because it does seem to have the best privacy protections.



STEVE:  I heard you say that on - I think it was on last Sunday's show, yes.  I'm glad.



LEO:  And I had to get used to it because it does look a little different.



STEVE:  Yeah.



LEO:  But I turn it on, you know, even DNS over HTTPS is in there.  And so I think as a single browser goes, this is a pretty - you have to go through the settings.  But there's a lot you can do including turning off all the trackers, cryptominers, fingerprinters.  You can say I don't want Twitter or Facebook or Google to know I'm here.  And it does change the way your experience, you know, how you experience the Internet, but not necessarily for the worst.  I can understand why people want to do that, I really can.



STEVE:  Yeah.  Well, and I've mentioned, our listeners know that I'm a fan of uBlock Origin.



LEO:  Which is not going to work anymore on Chrome, by the way, because of Manifest V3.  They're really limiting what uBlock can do.  Has Gorhill said anything about that?



STEVE:  Yes.  He's not happy about it.



LEO:  Right.



STEVE:  Yes, he has talked about it.  And it's unfortunate because, boy, I look, sometimes I'll be on someone else's machine and just look at all this crap jumping up and down, and it's like, whoa.  Whoa.



LEO:  Not to mention the security, the weight of all that stuff, the megabytes you're wasting, the security of running these third-party - you talk about this all the time.  JavaScript from god knows where.



STEVE:  Yup.  And actually that's a place, it turns out.



LEO:  God Knows Where.  It's where all the bad scripts...



STEVE:  You do not want to go there, that's right.



LEO:  Steve Gibson, we know where he comes from, GRC.com, the Gibson Research Corporation.  That's his home on the web.  Great place to go just to hang out, browse around.  We have a Colonel, Colonel Newton in the studio, a long-time Army chaplain who loves your sleep formula.  He uses it, and he says, "Boy, I travel a lot, do a lot of jet lag.  This sleep formula really works."  So that's great.



STEVE:  Cool.



LEO:  That's there.  That's free.  SQRL.  So many good things.  While you're there, do us a favor.  Support Steve, his bread and butter, the one things he sells, SpinRite, the world's best hard drive maintenance and recovery utility.  You can pick yourself up a copy there.  You can also pick up the 64Kb audio version of the show, the 16Kb audio version of the show, and it's the only place you can get the transcripts.  Elaine Farris writes those, and they come out a few days after the show comes out.  GRC.com.  Leave him feedback at GRC.com/feedback, or his Twitter.  He takes DMs from anybody, crazy man.  His Twitter handle is @SGgrc.



We have audio and video of the show, if you want to watch Steve's moustache grey over the years.  We should do a montage of you and me, just 14 years in.



STEVE:  How we have evolved.



LEO:  How we've changed over the years.  All the episodes are at TWiT.tv/sn, starting with TWiT.tv/sn1, going all the way through TWiT.tv/sn729, this episode, audio or video.  Best thing to do, though, you don't need to do any of that, is subscribe.  That way you'll get it automatically on your phone or on your laptop, on your device.  Or you can ask your Echo or your Google Assistant.  Just say, "Hey, Goog, play Security Now! podcast," and it'll just play the most recent version for you, which is the most easy way to do it.



I just looked at some stats.  About 5% of the audience listens on a device.  But that's pretty good.  I think that's a number that's growing fast.



STEVE:  As opposed to...



LEO:  Your phone or, you know, the way...



STEVE:  Oh.  Oh, you mean, like...



LEO:  A voice assistant.  Actually, that's a bigger number than I would have thought.



STEVE:  And we should tell people about Boston; right?



LEO:  Yeah, I'm so excited.  This is going to be amazing.  We are going to be the guests of LogMeIn and LastPass.  We are going to do an - I'm calling it an "authentication summit."  It'll be a live - now, tickets are going to be limited.  I think it's only a hundred people.  And I don't yet know how those are going to be allocated.  But they will be available.  It's for charity, so I think they'll probably be selling tickets, again,  as a benefit.  I'll have more details of that.  And you know the date.  I don't remember the date.  It's in early November.



STEVE:  Yeah.



LEO:  Or October.



STEVE:  October 3rd.  October 3rd.



LEO:  Thank you.



STEVE:  Is a Thursday.



LEO:  October 3rd, Thursday.  I will be moderating the panel.  Steve will be there.  We'll definitely talk about SQRL because that's what it's all about.  Also the CISO for LogMeIn will be there, and the world-famous creator/inventor of the firewall, Bill Cheswick.  He's also recently been writing a lot about why passwords don't work and has lots of thoughts.  Really a smart fellow.  Lots of thoughts about authentication, passwords, and what we do going forward.  So I think it's going to be a fascinating summit.  We will record it and offer it later for download.  So if you can't get to Boston October 3rd, don't worry.  You'll be able to see and hear it all.  But more details to come on how to get tickets and all that other stuff.



STEVE:  Cool.



LEO:  Thank you, Steve.  Always a pleasure.  Have a great day.



STEVE:  Okay, buddy.  Bye.



Copyright (c) 2019 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/. 






GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#730

DATE:		September 3, 2019

TITLE:		The Ransomware Epidemic

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-730.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  Rather than looking at many small bits of news, this week we take longer looks at a few larger topics.  We'll examine several pieces of welcome news from the bug bounty front.  We also take a look at Google's Project Zero revelation of a comprehensive multiyear campaign aimed at iOS visitors to specific websites.  Then we conclude with a distressingly large array of news from the ransomware front.  We figure out how to pronounce Sodinokibi (so-dee'-no-kee-bee) and ponder the future of ransomware.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  Three big topics today.  We're going to talk about bug bounties, all the money going out.  That ties right into that zero-day that Google just revealed on iOS.  We'll talk about that, too.  And of course the ransomware epidemic sweeping the world.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 730, recorded Tuesday, September 3rd, 2019:  The Ransomware Epidemic.



It's time for Security Now!, the show where we cover security, privacy, how things work, and a few science fiction shows along the side with Mr. Steve Gibson right here.  He's the GRC.com major domo and our regular on here, our security expert.  Hi, Steve.



STEVE GIBSON:  Leo, great to be with you once again as we are continuing to plow into our 15th year with no sign of anything letting up.  And in fact this week's podcast is titled "The Ransomware Epidemic" because a security firm, Armor, has been tracking what's been going on with ransomware.  We of course have been talking about it more over the last few months than we ever have.  And a number of things have happened that sort of bring this to the fore.  We have a little bit of news from Texas that's been surprisingly quiet.  Thirteen new victims last week.  The emergence of a - well, okay.  There's a new ransomware known by two names.  I posted to my Twitter feed this morning, how do people think I should pronounce this for the podcast:  S-O-D-I-N-O-K-I-B-I.  And the consensus came...



LEO:  Well, everybody knows it's Sodinokibi.



STEVE:  Well, yes.  That's what we decided it was, Sodinokibi.



LEO:  It's phonetic, Sodinokibi.



STEVE:  So it's also known as...



LEO:  It's probably Japanese, though.  It looks like it's Japanese, so I'll have to ask Asako.



STEVE:  That's what we were thinking, yes.  So anyway, rather than looking at lots of small bits of news, as we sometimes do when I talk about like what we're going to do, and it just goes on and on and on, we're going to take longer looks at a few larger topics.  We examine several pieces of welcome news from the bug bounty front.  We'll also take a look at Google's Project Zero revelation of a comprehensive, multiyear campaign aimed at iOS visitors to specific websites.



And then probably we'll conclude, but with probably at least half of the podcast, talking about this distressingly large array of ransomware news which suggests that we're heading into a new era.  I mean, we've sort of been teasing at this for the last few months while we've been looking at these municipalities that have been hit by major ransom demands that have been met, thanks to them being insured, which of course sort of changes the dynamic of all of this.  So I think another great podcast for our listeners.



And a rather sobering Picture of the Week, not surprisingly, has a bunch of dots on Florida, where we've been seeing ransomware attacks of late, and Texas, where the same has been happening as a consequence of those 22 different municipalities.  This is a diagram of the U.S. produced by Armor, which shows their tracking of ransomware attacks just this year, just 2019.  So this is something that is really happening.



And I'll note, and we'll be getting to this later in the show, there are without question many more unreported, less public attacks.  The FBI's number, which because they deal with privately reported incidents, is far higher than the number of dots that we see here.  So the FBI gets called in when some private company gets attacked, along with local authorities, to see what can be done, what should we do, and so forth.  But anyway, so this Picture of the Week went well with the main topic of the show. 



But I want to start by talking about the Android Developers blog which was posted last Thursday entitled, by Google of course, "Expanding Bug Bounties on Google Play," which is some welcome news.  After some introductory intro stuff in the blog, they explain that the Google Play Security Reward Program, that's the GPSRP, has increased its scope to now include all apps in Google Play Store with 100 million or more installs.  So all of those apps.  And, you know, I looked around to get some sense for how many apps there were with that count or greater, 100 million or more installs.  And I wasn't, in the time that I gave myself, I wasn't able to get some sense for the shape of the curve.  We know that there are a gazillion apps with small followings that are just posted and put up by smaller developers.  But it'd be interesting to know, and I didn't have a chance to find it, what this means in terms of how many apps are now eligible.



But any apps, Google is announcing, with 100 million or more installs are now covered by their reward program, even if the individual app developers themselves don't have their own vulnerability disclosure or bug bounty programs.  Or if they do, then Google will augment those programs in order to meet Google's declared rewards.



Google wrote:  "In these scenarios, Google helps responsibly disclose identified vulnerabilities to the affected app developer."  They said:  "This opens the door for security researchers to help hundreds of organizations identify and fix vulnerabilities in their apps.  If the developers already have their own programs, researchers can collect rewards directly from them on top of the rewards from Google."  They said:  "We encourage app developers to start their own vulnerability disclosure or bug bounty program to work directly with the vulnerability researcher community."



Then they said:  "Vulnerability data from this GPSRP program helps Google create automated checks that scan all apps available in Google Play for similar vulnerabilities," which is sort of clever.  So the things that are manually found where researchers have the incentive to find them and report them in return for receiving a bounty, Google will look at those and go, oh, that's interesting.  Obviously they hadn't found it from their own automated scanning.  Thus human researchers found them.



But then Google would then enhance their automated scanning to help find those in the future and also, of course, look to the lesser highly downloaded apps, scan those in order to see if they're able to find similar vulnerabilities.  So it's sort of a win-win-win-win-win.



"Affected app developers," they wrote, "are notified through the Play Console as part of the App Security Improvement program, which provides information on the vulnerability and how to fix it."  They said:  "Over its lifetime, ASI" - that's the App Security Improvement program - "has helped more than 300,000 developers fix more than a million apps on Google Play.  In 2018 alone," they said, "the program helped over 30,000 developers fix over 75,000 apps.  The downstream effect means that those 75,000 vulnerable apps are not distributed to users until the issue is fixed."  So that's the automated side which benefits from what Google learns from what the human security guys find first.



Then they also said, summing it up:  "To date, GPSRP has paid out over $265,000 in bounties."  They said:  "Recent scope and reward increases have resulted in $75,500 in rewards just across the last two months, July and August."  They said:  "With these changes, we anticipate even further engagement from the security research community to bolster the success of the program."  Obviously, by expanding this, they're creating a much larger, I don't want to really say "target-rich environment," but from the standpoint of the security researcher who would like to see if they could support themselves by finding bugs, many more apps to go looking around in.



Then they also introduced something new, which is the Developer Data Protection Reward Program.  They said:  "Today we are also launching the Developer Data Protection Reward Program."  Again, note the term "data protection."  DDPRP is a bounty program which will be run in collaboration with HackerOne, meant to identify and mitigate data abuse issues in Android apps, OAuth projects, and Chrome extensions.  So these are technically not bugs.  But we've also talked often about how apps are exfiltrating data that is in breach of Google's terms and conditions.  So this data protection program is aimed at finding those.



They wrote:  "It recognizes the contributions of individuals who help report apps that are violating Google Play, Google API, or Google Chrome Web Store Extensions program policies."  So not bugs, but policy violations.  They said:  "The program aims to reward anyone who can provide verifiably and unambiguous evidence of data abuse, in a similar model as Google's other vulnerability reward programs.



"In particular," they said, "the program aims to identify situations where user data is being used or sold unexpectedly, or repurposed in an illegitimate way without user consent.  If data abuse is identified related to an app or Chrome extension, that app or extension will accordingly be removed from Google Play or the Google Chrome Web Store.  In the case of an app developer abusing access to Gmail restricted scopes, their API access will be removed.  While no reward table or maximum reward is listed at this time," they said, "depending on impact, a single report could net as large as a $50,000 bounty."



They said:  "As 2019 continues, we look forward to seeing what researchers find next."  They said:  "Thank you to the entire community for contributing to keeping our platforms and ecosystems safe.  Happy bug hunting."  So Google has expanded the scope of their bug bounty program, not only, as I said, by adding more apps to qualify for bounties, and by encouraging app developers who are able to, to create their own bounty programs, or maybe team up with HackerOne, and also by expanding the range of what qualifies to include not only bugs, but also the abuse of information disclosure.



So as we know, Google has deep pockets, and it's nice to see them using some of that to further strengthen the Play Store.  You know, we know that apps are still misbehaving, despite all of the efforts that those who curate their stores, Google in the case of the Play Store and Apple in the case of the App Store there, but stuff still gets through.  So it requires just more eyeballs looking at this.



And speaking of the bug bounty industry, we covered back in March the first millionaire, a 19-year-old white hat hacker, Santiago Lopez, who has as his Twitter handle @try_to_hack, was the first to pass the target of $1 million in earnings from HackerOne.  He earned himself more than a million dollars by identifying vulnerabilities in the software or systems belonging to Twitter, HackerOne themselves, Automattic, Verizon, a number of private companies who were participating in HackerOne's bug bounty program, and the U.S. government, among others.



Now HackerOne has announced that Santiago is no longer alone.  Five additional hackers have joined Santiago to become bug bounty millionaires by finding and reporting security vulnerabilities through HackerOne's vulnerability coordination and bug bounty program.  And these new members of this small group are all over the place:  Mark Litchfield from the U.K., Nathan Wakelam in Australia, Frans Rosen in Sweden, Ron Chan in Hong Kong, and Tommy DeVoss from the U.S.  So now there are six millionaires who have made themselves such by finding and reporting, responsibility disclosing bugs to HackerOne.



And I should also note my thanks to HackerOne for their very kind shout-out about me, as it turns out, in their most recent blog post last Friday, which they titled "HackerOne Praised by an Original Hacker," yours truly.  That was cool.  They were referring to this podcast 10 weeks ago, number 720, which we titled "The Bug Bounty Business."  And as our listeners well know, I consider bug hunting for profit to be an emerging 100% legitimate and very intriguing career.  And I've sort of been saying to our listeners, hey, you know, if you'd like to maybe make some extra coin on the side in the evenings, see what you can do.  And now, I mean, we really have a new industry here in independent hackers practicing their craft by seeing if they can find problems; and, if so, reporting them and being rewarded.  So yay to that.



LEO:  Pretty neat.  Millionaires, man.  That's incredible.



STEVE:  Yeah, six millionaires.  Yeah, so, I mean, that's some serious money.  And again, it's interesting, too.  I noticed the Asian guy in Hong Kong, he's got some mileage on him.  He looked a little more like my age.  I thought, wow.  Because the other guys are kids, of course, as you typically see.



LEO:  Yeah.  You don't have to be a kid.



STEVE:  Don't have to be a kid.



LEO:  Right?



STEVE:  And HackerOne said, hey, Gibson's an original hacker.  So, yeah, okay.  I'll accept that.



LEO:  That's nice.



STEVE:  Meanwhile, also last Thursday, Google's Project Zero dropped a bit of a bomb on iOS.  First of all, everything we're going to talk about has been fixed.  So none of this, I mean, you know, Project Zero operates with full responsibility.  In some cases where they find something really worrisome, they don't do their 90-day, take your time to fix it.  In one instance, they gave Apple seven days.



LEO:  And Apple responded.  They had an out-of-band fix within seven days, which is pretty impressive.



STEVE:  Yes, they did.  They did it - they were told on the 1st of February, and their patch was out in six days, on the 7th.  So yay to them.  Certainly they've got good communication back and forth.  So what we learned from Project Zero was that for a period of at least several years a small group of websites was successfully infecting anyone who visited them with a RAM-based monitoring malware through the use of a quite sophisticated iOS version-based multistage exploit chain.



And on one hand, as I was digging into this, I was stunned by the work that the group at Project Zero did in order to figure this stuff out.  But that has to parallel the work that somebody else did to make this happen.  I mean, so the work of figuring out what the malware is doing probably about echoes the work that some agency somewhere went to to figure out that these things could be exploited.  So this sort of suggests that there was some serious effort put into creating these exploit chains.  As far as everyone knows, all of this finally ended with the emergency jump to iOS v12.1.4, which is what you're referring to, Leo.  And not to confuse that with 12.4.1, which is what...



LEO:  Another one.



STEVE:  Which is what we just did when we got the emergency, oops, regression of the jailbreaking problem.



LEO:  Jailbreak, yeah.



STEVE:  So as Project Zero's Ian Beer explained, the story before then, that is, before 12.1.4, is not only troubling due to the exposure created for those who were infected, and we'll talk about what this implant does once you get it, but also more so due to the conclusions Ian draws based upon the exact nature of what he found.  So his blog posting was titled "A very deep dive" - and, oh, boy, is it.  I mean, it's so deep we're not going to go that deep on this podcast.  And as our listeners know, we're not shy of depth here.  But this is beyond anything I could convey through this medium.  But there's links to what he posted, for anybody who really wants the nitty-gritty.  Anyway, so he says:  "A very deep dive into iOS exploit chains found in the wild."



So Ian wrote:  "Project Zero's mission is to make zero-days hard.  We often work with other companies to find and report security vulnerabilities, with the ultimate goal of advocating for structural security improvements in popular systems to help protect people everywhere."  Of course we've been reporting on Project Zero since it began.  And I remember when this first happened we were like, wait, they're going to look at other people's stuff, not just theirs?  And yes, as we know, they do that.  They've fixed all - they've helped to fix all kinds of things.



So Ian said:  "Earlier this year, Google's Threat Analysis Group (TAG)" - which I'll be referring to a bit later - "discovered a small collection of hacked websites.  The hacked sites were being used in indiscriminate watering hole attacks against their visitors, using iPhone zero-days."  Okay.  So there's a lot there to unpack.  So a small collection of hacked websites.  The hacked sites were being used in indiscriminate - meaning anybody who visited would get themselves infected.  "Watering hole" suggests that these were set up and then either based on the demographic of who would visit, or other people may have been induced to go there, that is, thus watering hole, that is, the malware was staged on the site, and then something would be used to induce targets to go visit that site - because they were thirsty, thus watering hole - and that would infect their iPhones.



I should note also that this was never - the infection, the malware was never written to non-volatile storage.  So it sat in RAM, despite the fact that it was very capable, only until they rebooted their phone.  Most people are typically only rebooting when they have an iOS update from Apple.  Sometimes you may shut your phone off completely, but that's not the way most people operate.  So this thing's going to tend to stay around for quite a while.



He wrote:  "There was no target discrimination.  Simply visiting the hacked site was enough for the exploit server to attack your device and, if it was successful, install a monitoring implant.  We estimate that these sites received thousands of visitors per week."  So notice that's not, I mean, thousands of visitors per week, there are sites that are getting thousands of visitors per minute.  So these were non-high traffic sites, probably.  And at no point does he talk, I mean, he refers a little bit to dissidence, and the presumption is that this was probably a state-backed attack because you certainly needed state-scale resources in order to mount an attack like this.



And he says TAG, that's their Threat Analysis Group, was able to collect five - and this is where things get interesting - five separate, complete, and unique iPhone exploit chains, covering almost every version from iOS 10 through to the latest version at the time of iOS 12.  He says:  "This indicated a group making a sustained effort to hack the users of iPhones in certain communities over a period of at least two years."



And then he said:  "I'll investigate what I assess to be the root causes of the vulnerabilities and discuss some insights we gain into Apple's software development lifecycle."  He says:  "The root causes I highlight here are not novel and are often overlooked.  We'll see cases of code which seems to have never worked, code that likely skipped QA [Quality Analysis] or likely had little testing or review before being shipped to users."



Now, we know that iOS and Android are inherently competitive, that Ian is on Google's team, and that what he wrote here sounds a bit harsh.  But when you dig deeper into his work - and it's truly stunning work on Ian's part, and his group - you'll see what he's been working for years is making Apple's iOS better and more secure for everyone, and that comparatively few people are ever going to see and appreciate the work that he's done.  So despite the way that sounds, he's really not attacking Apple.  And I have looked at the details of the code.  And what he's really only doing is drawing the only conclusion you could from the code that he sees in front of him when he looks at what it was that was being exploited.



So in his posting he has a chart showing moving through time, starting on the 13th of September 2016 through late January of this year, 2019.  And the individual stacks of five different exploit chains, which some entity developed in order to move from iOS 10.0.1 and basically get coverage through 10 and through 11 up to the beginning of iOS 12.  So through a period of iPhone 7 through the launch of 8, 8+, X, XR, and XS.  So a serious bunch of work.



He said:  "Working with TAG, the Threat Analysis Group," he wrote, "we discovered exploits for a total of 14 vulnerabilities across these five exploit chains, seven for the iPhone's web browser."  And I will get back to it.  But I'll also note that, in fairness, he notes that Chrome would have been similarly vulnerable to those, although the attacks were against Safari.  And he says:  "Five for the kernel and two separate sandbox escapes."



He wrote:  "Initial analysis indicated that at least one of the privilege escalation chains was still a zero-day and unpatched at the time of its discovery.  We reported these issues to Apple" - and here's what we were talking about before, Leo - "with a seven-day deadline on the 1st of February, 2019, which resulted in the out-of-band release of iOS 12.1.4 on the 7th of February," so just six days later.  We also shared the complete details with Apple, which were disclosed publicly on the 7th of February.



He says:  "Now, after several months of careful analysis of almost every byte of every one of the exploit chains," he says, "I'm ready to share these insights into the real-world workings of a campaign exploiting iPhones en masse.  This post will include" - and actually it's not this post, it's seven subsidiary posts.  He says:  "...detailed write-ups of all five privilege escalation exploit chains; a teardown of the implant used, including a demo of the implant running," he says, "on my own devices, talking to a reverse-engineered command and control server and demonstrating the capabilities of the implant to steal private data like iMessages, photos, and GPS location in real-time; and analysis by fellow team member Samuel Gross on the browser exploits used as initial entry points."  And that's where they noted that Chrome would have also been vulnerable to the same initial entry point exploits.



He says:  "Let's also keep in mind that this was a failure case for the attacker," meaning that they were discovered.  He says:  "For this one campaign that we've seen, there are almost certainly others that are yet to be seen."  In other words, they first discovered a website.  The way this was found was they discovered a website that was doing this, then followed that trail back into iOS to understand what the site was doing to iOS, thus the exploit chain.  But they reasonably assume that there were very likely other websites that were also exploiting the same suite of vulnerabilities, and those were never found.



He says:  "Real users may make risk decisions based on the public perception of the security of these devices.  The reality remains that security protections will never eliminate the risk of attack if you're being targeted.  To be targeted might mean simply being born in a certain geographic region or being part of a certain ethnic group.  All that users can do is be conscious of the fact that mass exploitation still exists and behave accordingly, treating their mobile devices as both integral to their modern lives, yet also as devices which, when compromised, can upload their every action to a database to potentially be used against them."  And, you know, this demonstrates the truth of that.



He says:  "I hope to guide the general discussion around exploitation away from a focus on the million dollar dissident and towards discussion of the marginal cost for monitoring the n+1'th potential future dissident."  He says:  "I shall not get into a discussion of whether these exploits cost $1 million, $2 million, or $20 million.  I will instead suggest that all of those price tags seem low for the capability to target and monitor the private activities of entire populations in real time."



So he says:  "I recommend that these posts be read in the following order."  And this is where he now then, in this original anchor posting, has seven links to the details.  The first five of them have one extensive amazingly detailed dive into each of these five individual exploit chains that I'm not going to go into because there's just no way to do that in a podcast.  It's all there for anyone who is our listener who is interested.  I mean, they are amazingly detailed reverse-engineering works.  In the sixth of those seven he says:  "We examine the WebKit exploits used to attain an initial foothold in iOS."  And that's where he notes that, for the record, Chrome on iOS would have also been vulnerable, even though it was Safari that was attacked.



And he says in his final, seventh posting, he examines the operation of the implant that is finally dropped into the device's RAM.  He said:  "In the earlier posts we examined how the attackers gained un-sandboxed code execution as root on iPhones.  At the end of each chain we saw the attackers calling posix_spawn, passing the path to their implant binary which they had dropped into /tmp folder."  He says:  "This starts the implant running in the background as root.  There's no visual indicator on the device that the implant is running."  And of course iOS doesn't give users any sense for what's running.



He says:  "There's no way for a user on iOS to view a process list.  So the implant binary makes no attempt to hide its execution from the system.  The implant is primarily focused on stealing files and uploading live location data.  The implant" - and of course you maybe wonder why the perpetrator of this wanted that.  Who knows.  On the other hand, it's in a position to get everything.  And that's pretty much what it does.



He says:  "The implant requests commands from a command and control server every 60 seconds.  Before diving into the code," he says, "let's take a look at some sample data from a test phone running the implant and communicating with a custom command and control server I," he wrote, "developed.  To be clear," he says, "I created this test specifically for the purposes of demonstrating what the implant enabled the attacker to do, and the screenshots are from my device."



He says:  "The device here is an iPhone 8 running iOS 12.  The implant has access to all the database files on the victim's phone used by popular end-to-end encryption apps like WhatsApp, Telegram, and iMessage.  We can see here screenshots of the apps on the left, and on the right are the contents of the database files stolen by the implant which contain the unencrypted plaintext messages sent and received using the apps."



And so of course this is what we've often spoken of is that, yes, you've got end-to-end encryption.  And so what leaves the device is encrypted strongly, and we know of no means for anyone intercepting that to be able to determine what's being messaged.  On the other hand, this demonstrates beautifully that WhatsApp, Telegram, and iMessage have databases of everything that they've been doing in plaintext.  In fact, they even added Hangouts.  And all of that is available to something that gets into the phone.



He noted that the implant can upload private files used by all apps on the device.  He shows the contents of emails sent via Gmail being uploaded to the attacker's server; notes that the implant also takes copies of the user's complete contacts database, takes copies of all of their photos.  The implant can also upload the user's location in real-time up to once per minute, if the device is online.  He says:  "Here's a sample of live location data collected by the implant," he says, "when I took a trip to Amsterdam with the implant running on an iPhone in my pocket."  And then that's detailed in his posting.



He says:  "The implant uploads the device's keychain, which contains a huge number of credentials and certificates used on and by the device," he says, "for example, the SSIDs and passwords for all saved WiFi access points.  The keychain also contains the long-lived tokens used by services such as Google's iOS Single-Sign-On to enable Google apps to access the user's account.  These," he notes, "will be uploaded to the attackers and can then be used to maintain access to the user's Google accounts, even once the implant is no longer running."



And he says:  "Here's an example using the Google OAuth token stored as [in this case] com.google.sso.optional.1.accessToken in the keychain being used to log into the Gmail web interface on a separate machine."  So he took that and actually demonstrated its ability to be used by an attacker to gain persistent access to the accounts that had been exfiltrated.



Finally, he finishes, after he gets way down into the weeds of the way the implant works, and finishes by talking about the impact.  He says:  "The implant has access to almost all of the personal information available on the device, which it is able to upload, unencrypted, to the attacker's server.  The implant binary does not persist on the device.  If the phone is rebooted, then the implant will not run until the device is re-exploited when the user visits a compromised site again.  Given the breadth of information stolen, the attackers may nevertheless be able to maintain persistent access to various accounts and services by using the stolen authentication tokens from the keychain, even after they lose access to the device."  



So there is a deep look inside someone's, we don't know whose, I mean, Google may know based on the sites that were infected.  And that's not been made public.  But those sites were infected, low-traffic, probably special interest sites.  You know, he referred to dissidents, so it's likely that they do know which sites were - we know that they know which sites were infected.  They may have some sense for who the perpetrator was who would have wanted the information of people visiting those sites.



LEO:  TechCrunch has since had an article that they said it was Uyghur Muslims, and it was the government of China.



STEVE:  Ah, okay.  Wow.



LEO:  And there's a company that does this kind of analysis for NGOs and dissidents, and they actually named the sites.  And it's the kind of sites that Muslims would go to.  But it's a drive-by in the sense that anybody can go to those sites.  You might not be inclined to, but anybody could.



STEVE:  Right.  And as they noted, the attacks themselves are indiscriminate.  Anybody who goes there picks up this implant.  And it just demonstrates, I mean, we know how much time and attention Apple spends on security and has spent, much as Google does.  I mean, any of our major suppliers in this day and age, it has to be a focus of your work.  Yet even so, cranking out the amount of code that is being produced, mistakes get made.



LEO:  And this was also a nontrivial hack.  I mean, it would take a government to do it.



STEVE:  Oh, my goodness, yes, yes.  I mean, they're not chaining multiple exploits for fun.  They're chaining them because they need each one to perform some little bit of an overall attack in order to achieve their goal.  And it's just not easy.  But that just also demonstrates, if you put enough focus on this - well, and I've often, and I will be later, talking about how security needs to be viewed as being porous, meaning if you don't put much pressure against it, it's okay.  But if you put enough pressure against it, you'll find some leaks.



LEO:  Yeah.  So Google never did say where they found this, but I think it's kind of escaped out now.



STEVE:  Yup.



LEO:  And it wasn't, by the way, the only attempt by the Chinese government to get this Muslim minority.  They hacked OAuth to get Gmail.  We actually talked about another example where they put malicious apps on people's phones as they crossed into the country that would monitor everything they did.  So they were very active in this.



STEVE:  Yeah, or proactive, even.



LEO:  Proactive.



STEVE:  Yeah.  So the ransomware epidemic.  We don't have anything super definitive yet, even now, from Texas, although nine of the reported 22 affected state municipalities have been identified.  And I'm still struck by the surprisingly different feel that these attacks have from those that we've covered before.  So I continue to think that an attack on a common service provider is the most likely explanation for everything we're seeing, though evidence to support that opinion is still scant.



Meanwhile, we have a bunch, actually 13 new ransomware attack victims which have come to surface in the last week.  While most of them are school districts, which of course the timing of that is unfortunate because school is just starting up again, we also have a county in Indiana, a hospice in California, and a newspaper in Watertown, New York.



Armor, the cloud security firm whose data generated our Picture of the Week, has tracked the following new ransomware infections:  Lake County, Indiana has been infected with ransomware; the Rockville Center School District in Rockville Center, New York; the Moses Lake School District in Moses Lake, Washington - that attack actually occurred back in July, but was only reported to be ransomware recently; Mineola Public Schools in Mineola, New York; the Stevens Institute of Technology in Hoboken, New Jersey; New Kent County Public Schools in New Kent, Virginia; the Nampa Idaho School District, Nampa, Idaho; Middletown School District, Middletown, Connecticut.



It's odd that there were five in Connecticut because we also have the Wolcott Public Schools, the Wallingford School District, New Haven Public Schools, all in Connecticut.  So maybe there again is some common linkage.  We have the Watertown Daily Times in Watertown, New York; and the Hospice of San Joaquin in San Joaquin, California.



So again, lots happening in ransomware.  And we may be starting to have a clue as to what's going on and why that we will get to in a second.  We still don't know what's going on in Texas, but we do know that our old friend Ryuk, which we now know is pronounced ree-ook - and this week we have a few other pronunciation challenges we'll get to in a second - has been identified as the culprit in at least three of these additional recent attacks.



Newsday reported that the Rockville Center School District in New York initially received a ransom demand of $176,000.  The district's insurance company negotiated with the ransomware perpetrator, managing to cut that to essentially half, reducing the payout to $88,000; and the school district then themselves, because they were insured, obviously, by the insurance company, only needed to pay a deductible of $10,000.  So the district paid $10,000 against an initial demand of $176,000, and the ransomware guys ended up getting half of that.  But on the other hand, this minimizes the impact to the district.  The insurance company took a hit.  Yet the ransomware guys were certainly encouraged to do this again, netting $88,000.



There's no word on whether other victims have paid any ransoms yet.  Back in June, Brian Krebs did some reporting on the Baltimore, Maryland attack which took Baltimore's servers down on May 7th.  After communicating with Armor's Joe Stewart, Brian reported that Baltimore was the victim of a ransomware strain known as "RobbinHood," which we've never heard of before.  Now here we are four months later, and Baltimore's leadership recently revealed that $6 million of the money needed to cover the city's more than $10 million ransomware cleanup operation would be pulled from funds earmarked for upkeep of city parks and public facilities.



So what's unfortunate is that it looks like Baltimore, it's not clear where they are in terms of paying ransomware.  But they're saying that it's going to cost them $10 million to clean up, and more than half of that money gets taken from funds that were earmarked for city parks and public facilities.  So this is being expensive for municipalities that are not insured.



They said that so far the RobbinHood ransomware has cost the city over 8 million in lost revenue.  So 6 million of the money needed to cover more than 10 million in cost, and it's cost them 8 million in lost revenue and what they termed as "interest on deferred revenue."  So as a consequence of all of this, after the fact, they're now considering a contract for a $20 million cyber liability insurance plan.  There's a vote planned on the city council proposing an $835,000 contract, which has been deferred.  So now we're seeing it costs them $835,000 to get a $20 million coverage cyber liability insurance plan.  Yikes.



These 13 new attacks bring the total known publically reported ransomware attacks this year up to a total of 149.  So of those, 30 have involved educational institutions.  Chris Hinkley, the head of Armor's Threat Resistance Unit, told Ars Technica in their recent reporting of ransomware events, he said:  "Just like municipalities, which rely on critical systems to manage records and revenue in a community, school districts host data and systems critical to their community and its students."  He said:  "Thus hackers know that schools cannot afford to shut down, that budgets are typically stretched thin, and they often have few security protections in place," of course all making them ripe targets.  He said:  "...all aspects which make them a viable target."



And of course now we know that insurance carriers are indirectly providing these institutions with virtual deep pockets, so they're able to pay up when necessary.  Last May, Ars observed that school districts are particularly easy targets for ransomware operators because of their generally low budget for IT and limited security resources.  According to data collected by Armor, schools have become the second largest pool of ransomware victims, only second to local governments, and then closely followed by healthcare organizations in third place.



So the ransomware perpetrators are looking at the history of what attacks are being made and are succeeding, and setting their sights accordingly.  And we should also note that these numbers, this 149 this year, are only for publicly reported incidents.  CNN also reported last week, and we'll be getting into some more details of this in a second, that a firm named PerCSoft and Digital Dental Record are two companies that collaborate to handle the online services in the dental industry.



They reported, and I've got a PDF I'll share in a second, that roughly 400 of their customers, that is, customers of this Digital Dental Record firm connected to individual offices had been infected with ransomware earlier in the week.  Dental administrators told CNN they were unable to access basic information such as patient charts, X-ray data, or payment services.



Digital Dental provided a statement.  They said that 100 of the affected 400 practices had been restored since the attack.  So these more private incidents highlight the fact that many ransomware attacks may be going unreported because, again, if it's not a big - there's no way to hide a municipality or a school district being attacked.  But many smaller problems are occurring.  The FBI had reported 1,493 ransomware cases last year, which is far more than the number that were publicly known.



Other than New York's Rockville School District, which was insured and negotiated that $88,000 ransom payment, as I just said, cutting it down in about half, it's still too soon to know whether any of the other newly attacked public organizations have cyber insurance in place or yet in place, and whether they even plan to pay the ransoms.  We may find out in time.  The payouts which ransomware operators have recently received from similar targets, we've covered the Riviera City in Florida paying $600,000 ransom.  In Lake City, also in Florida, paying half a million dollars in ransom signaled certainly to attackers that attacking large communities can be quite lucrative.



ProPublica did an investigation which revealed that insurance companies are fueling the rise of ransomware threats as a consequence of covering the cost minus a deductible, and also of course negotiating the value paid to attackers down.  So what this essentially does is turning, in fact, we talked about this a few weeks ago, creating a new category of companies that you would almost term "cyber extortion negotiation services," where they're offering their services to negotiate on behalf of the victims and basically handle, serve as an intermediary between victims that have no idea what to do and the attackers who have encrypted all their systems.  And of course, as we know, by rewarding the hackers, it encourages more ransomware attacks.



But also in the news last week was this DDS Safe.  And what's somewhat ironic, or I guess even maybe more than somewhat, is that this DDS Safe company was specifically talking about how the cloud-based services they were offering were protecting companies from ransomware.  And unfortunately, the company that they were using as their cloud services provider, PerCSoft, based in West Allis, Wisconsin, themselves were victims of an attack.



So in his reporting on this, Brian Krebs was unable, as we know I would have also been, to resist titling his coverage "Ransomware Bites the Dental Data Backup Firm."  He explained that PerCSoft was this company that manages a remote data backup service relied upon by hundreds of dental offices across the country through its provision of cloud services to Digital Dental Record, and that Digital Dental Record offers an online dental data backup service called "DDS Safe" that archives medical records, charts, insurance documents, and other personal information for dental offices across the U.S.



The ransomware attack hit PerCSoft on the morning of Monday, August 26th, so last week, and encrypted dental records for some, but not all, of the practices that rely on DDS Safe.  Brenna Sadler, the director of communications for the Wisconsin Dental Association, said the ransomware encrypted files for approximately 400 dental practices, and that somewhere between 80 and 100 of those clients have now had their files restored.



Sadler said she did not know whether PerCSoft and/or DDR had paid the ransom demand, although it looks like Bleeping Computer has some updated data on that.  And she did not know which ransomware strain was involved or how much the attackers had demanded.  But updates to PerCSoft's Facebook page and statements published by both PerCSoft and this Dental Data Record suggest that someone may have paid up.  The statements note that both companies worked with a third-party software company and were able to obtain a decryptor to help clients regain access to files that were locked by the ransomware.



In Brian's reporting, several sources were reporting that PerCSoft did pay the ransom, although it's not clear how much was paid.  Bleeping Computer did have some ideas.  One member of a private Facebook group dedicated to IT professionals serving the dental industry shared a screenshot, which is purportedly from a conversation between PerCSoft and an affected dental office, indicating the cloud provider was planning to pay the ransom.



What's still very unclear, due to conflicting accounts using annoyingly fuzzy reporting, probably because they either don't know or they don't know that there's a difference or a distinction, is whether the infection of PerCSoft's cloud infrastructure allowed the infection to spread down the connection into the individual dental offices which relied upon DDR and PerCSoft.  It looks like they're doing cloud-based backup, but there was some indication that they were unable to access the records they use on a daily basis, suggesting that it's not just their backups, but their actual local records that had been infected, which suggests that the malware did in fact get down into their systems.



And some of their documentation suggests that they were using remote management systems.  I was able to find a PDF of the media statement that was put out by DentalRecord.com, and they said in their disclosure:  "At 8:44 a.m. on Monday, August 26th, we learned that ransomware had been deployed on the remote management software our product uses to back up client data.  Immediate action was taken to investigate and contain the threat.  Our investigation and remediation efforts continue.  Unfortunately, a number of practices have been and continue to be impacted by this attack."



So we know that it was about 400 dental offices spread around the country.  And the fact that they said that the ransomware had been deployed on the remote management software product, that suggests to me that whatever it is they're doing to perform this backup uses remote management, which does suggest that maybe malware was able to get down into the networks through remote management software and actually in fact the real-time data of these services.  And of course, not surprisingly, cloud data and backup services are a prime target of cybercriminals who deploy ransomware since ransomware encrypts data, and a lot of data is stored in the cloud.



Also, we didn't talk about this at the time, I don't think it had come to light yet, last month attackers hit the QuickBooks cloud hosting firm iNSYNQ and held the data of many of that company's clients hostage.  Earlier this year, last February, cloud payroll provider Apex Human Capital Management was taken down for three days following a ransomware attack.  And back on Christmas Eve, at the end of last year, the cloud hosting provider DataResolution.net took its systems offline in response to a ransomware outbreak on its internal networks.  The company was adamant that it would not pay the ransom demand, but it ended up taking several weeks for customers to fully regain access to all of their data.



So this is happening all the time and everywhere.  And we may not be seeing as much of it as is actually happening.  As we know, the FBI and many security firms have advised victims not to pay any ransom demands, arguing that doing so encourages the attackers and may not result in their regaining access to encrypted files.  But in reality, also, many cybersecurity consulting firms quietly note to their customers that paying up is probably the fastest route back to business as usual.  And we've seen that even, I mean, just the disruption of having, as we saw in several of these recent attacks, the disruption of having computers encrypted is not quick to recover from.



In a report published by the cybersecurity firm Fidelis last week, this next major event or malware that we're about to start talking about, which is known as REvil, but also as this Sodinokibi, it has just emerged as the fourth most prevalent strain of ransomware - and we'll talk about why in a second - at 12.5% is this Sodinokibi.  Ryuk holds the lead at 23.9%. of incidences, followed by Phobos at 17% and Dharma at 13.6%.  So there's a bunch of ransomware out there which is attacking people.  Sodinokibi, S-O-D-I-N-O-K-I-B-I.  And it's so fun to say.  Maybe I'll learn how to say it one of these days.  It's also known as REvil, capital R, capital E, v-i-l, as in R-Evil, REvil.  Although Sodinokibi - I've got to practice that a few more times.



So Bleeping Computer, who as we know were the first and very early to shine a bright spotlight on the ransomware scene, have a bunch of great information about this newest kid on the block, Sodinokibi.  Bleeping Computer notes that although it is relatively new on the ransomware scene, and we have some theories about why, Sodinokibi has already made impressive profits for its administrators and affiliates, some victims paying as much as $240,000 as tracked from their wallet, and network infections netting an average of $150,000.



Now, our listeners may have noted that I used the term "affiliate," which should be a clue to this new terror's distribution and exploitation model.  Yes, like GandCrab, it is RaaS, Ransomware as a Service.  We talked about back in April the guys behind - I guess we started talking about it in March because they shut down at the end of April.  So the guys behind GandCrab, claiming to have made all the money they needed, and having laundered it and reinvested it in legitimate businesses, they decided to shut down their operation.  We wondered and speculated at the time what might surface to fill the void left behind by this diabolical network marketing model.  And it looks like we have an answer:  Sodinokibi.  It turns out it's the thing that knocked those 22 towns off the map in Texas.



LEO:  Ah.



STEVE:  Uh-huh.  And then nuked those 400-some dental practices.  Yes, it is truly REvil.  Bleeping Computer reports that the Texas extortion, we now have numbers from them, totaled $2.5 million.  So remember that was the 22 townships or municipalities and towns.  So the Texas extortion came in at $2.5 million, and the ransom to restore those 400-odd dental offices machines was $5,000 per, now, I wrote "per computer."  But I must have meant per office.  So somewhere around another $2 million.  So since its first appearance in April, meaning that it appeared immediately, I mean, like almost immediately after GandCrab disappeared, Sodinokibi has become prolific and quickly gained a reputation among cybercriminals in the ransomware business and security researchers.  And as we know, this is aided by the fact that it's making others among the underworld quite a bit of money in turn.



Although one of the things that really stood out was what a small piece of the action the GandCrab people took.  These Sodinokibi guys are taking more, at least initially.  After its first month back in mid-May, to put itself more squarely on the map, a Sodinokibi advertiser, who we now believe is the one behind it, they're going by the forum name UNKN, deposited over $100,000 on underground forums to show that they meant business.  Online underground advertisements for the new file-encrypting malware appeared in early July on at least two forums.  And this UNKN group said that they were looking to expand their activity; that it was a private operation with a limited number of seats available for experienced individuals.



So again, they're describing their malware as private ransomware, though it is flexible enough to adapt to the Ransomware as a Service business model.  UNKN offered their affiliates 60% of the payments, that is, the ransomware, at the beginning, and a 10% increase to 70% after the first three transactions.  So the people who indirectly exploited this basically, again, ransomware as a model, meaning that this UNKN would make their ransomware available to third parties, who would then go do the - who would arrange to get victims infected.  They would initially receive 60% of the payout, and that would bump to 70% after the first three.



And this actor, this UNKN, also made it clear that they would not be working with English-speaking affiliates as part of this private program.  Bleeping Computer posted a screenshot monitoring the ransomware's bitcoin transactions, showing the money pouring in from its victims.  Looking at the second one -  I have a screenshot here in the show notes.  Looking at the second one shows that one victim paid 26.388 bitcoins, which at the time converted to 240,143 USD.  And so when the screenshot was taken, that one, that 240,143 USD was 11 hours previously.  There was a 0.43 bitcoin, and at the time that was, looks like about 4,000 USD at what was seven hours ago.



Then there were one, two, three, four, five that were 12 hours ago, all about the same amount, that $4,000.  Then two days previously there had been a, looks like nearly $20,000.  Also two days before about $4,000 and another $4,000.  So, I mean, this thing is really making some money.  And for affiliates who arrange to infect an entire network, the Sodinokibi developers allow a victim to purchase a decryption tool for the entire fleet of infected computers.  According to a forum post shared with Bleeping Computer, those network-wide decryptors have an average cost of $150,000.



So with revenue flooding in, and everybody is able to see what's happening here with the bitcoin wallet, and the people in the underground forums are, I mean, there's a lot of buzz around this, other malware "distributors" are attempting to gain access to the program.  But this UNKN group stated that there are currently no available openings for affiliates at this time.  Bleeping Computer also reported that the operators behind the Sodinokibi ransomware started searching for affiliates to distribute their software shortly after GandCrab had formally shut down.



The underground reactions toward the new product suggests that there may have been a connection with the administrators or the affiliates of what is now the defunct GandCrab operation and this new Sodinokibi ransomware.  Some malware analysts pointed to some code level similarities between the two ransomware strains, although many differences also exist between the two.  And you would expect some similarities because one could be just based on some of the way the other works.



However, there was one similarity noted is that the administrators of both malware families are refusing to carry out business in the Commonwealth of Independent States, the so-called CIS area.  This is Russia, Ukraine, Moldova, Belarus, Armenia, Turkmenistan, Uzbekistan, and a bunch of other 'stans that I can't even pronounce.  Oh, there's Kazakhstan.  I know how to say that one.  But there's K-Y-R-G-Y-Z-S-T-A-N and T-A-J-I-K-I-S-T-A-N.  Whew.



LEO:  Tajikistan.



STEVE:  Tajikistan.



LEO:  Tajikistan.



STEVE:  Anyway, these little breadcrumbs, along with the rapid ascension of this new malware on the scene, do suggest some sort of involvement of the previously well-established GandCrab crew or its affiliates, and that they may already have private connections on these forums that have allowed them to quickly promote Sodinokibi and be selective about their partners.  So there's no clear undeniable evidence that Sodinokibi is run by the same individuals that administered GandCrab.  But they obviously know the ransomware game and are once again making some serious money from this.



So I guess I'll conclude by observing, again, that everything changed the moment we moved from viruses and malware, which were just being created because they could be for their own sake.  You know, Leo, you and I often commented in the early years of this podcast that the viruses were spreading, but they weren't doing anything.  I mean, they were just sort of annoying.  You didn't want to have it in your computer.  But it just, you know, it was being identified by the early AV tools, and it was sort of just there to spread itself.  You know, kiddies were messing with it because they could.



But as soon as cryptocurrency exchange - well, first of all, cryptocurrency happened.  And that was interesting.  Then cryptocurrency exchanges appeared so that cyber currency could then be used to pay the rent and to buy a burger.  And so what happened then was that stealing CPU cycles through cryptojacking jumped to the foreground.  But then this emergence of cryptocurrencies also meant that there was now a means for ransomware perpetrators to safely receive extortion payments.  Remember that, again, in the early years of this podcast, we talked about, like, Western Union payments, right, being made to Russia, I remember.  And the problem was wiring money made the funds traceable, given that you had some relationship with the country of destination.  But cryptocurrency enabling a much higher degree of anonymity meant that ransomware soon followed.



So today there isn't a malware author alive who isn't aware that it's now possible to live well by finding, infecting, and encrypting the data of the right targets.  And this really changes everything.  The twisted brilliance of ransomware is that the victim's data is still there.  They can see their files, now with a .crypto or other extension appended to the end.  The files aren't deleted.  They've simply been moved just out of reach, which allows the carrot of full data recovery to be dangled in front of the victim.  And as a result, more often than not, though no one wants to knuckle under to extortion, the sanity of self-interest does prevail, and money does flow into the bad guys' cryptocurrency wallets to further encourage them to find another victim.



One of my favorite analogies that I mentioned before for this podcast has been how porous our computer security really is.  And I think that this notion of porosity is just the right term, you know, because security does exist.  We know it exists.  It's kind of there.  It's mostly useful.  It pretty much protects us.  But it isn't absolute protection.  And we especially see that when security is put under pressure.  And of course pressure is what the emergence of cryptocurrency exchanges has created because now there's a model for bad guys to make a bunch of money.



So what does the future look like?  I think we could predict what's going to happen based on what we've seen happen in the past.  When the world learned that the NSA had installed data-sucking taps at many of the Internet's major Internet exchange points, our collective reaction was to treat encryption as more than an expensive option.  It suddenly became important.  And so as we know, today, to a far greater degree than previously, the entire world's communications are now encrypted end to end.  I expect we're going to see something similar happening with system backup imaging.  IT departments, just like in the old days with encryption, IT departments have not prioritized the maintenance of good hot and cold backup because they have not had to until now.  For the most part, things have been running along just fine.



But that's no longer true.  It will no longer be reasonable for a ransomware event to take a municipality or a school district or a corporation offline for months.  That won't fly in the future.  Storage has become so cheap that there isn't any - there's no reason any longer not to have backups of everything.  And where security concerns are paramount, if someone's worried about shipping corporate records into the cloud, there's no need to move backups into the cloud.  I mean, that's certainly an advantage for some instances.  But it's possible to get a large rack of local storage and secure it and use it.



So I think we're going to start seeing something we haven't really seen yet, which is the emergence of explicitly ransomware-oriented backup protection, where nightly snapshot images of individual workstations are rotated after a day where the computer has been used.  And those snapshots will be deliberately kept firmly offline and inaccessible so that, if some malady should hit a machine or machines, those machines can be restored to their previous night's image without a lot of trouble.



I've previously mentioned that this is something I'm already doing using the Windows mountvol command, which allows you to take a volume which is offline and bring it online.  And then I also do some registry editing to further hide the unmounted volumes, since mountvol will look at the registry in order to show what it knows is available.  And that allows me to transiently bring a well-hidden offline drive online briefly while an image is being made of my main working system, but to keep it hidden otherwise.  So I have cold backups which are current.  And then of course I have other means for doing incremental backups for the things I'm working on during the day.



And we should also note that, at the operating system level, what ransomware is doing on a system really does stand out.  You know, it's unusual behavior for any program to suddenly be performing wholesale encryption of a large number of files.  That's something that future OSes themselves should readily be able to observe, detect, and put a stop to.  And although whitelisting every program that a system uses can be quite burdensome, whitelisting the few programs that might legitimately need to encrypt large numbers of files is entirely feasible.



So the idea would be that there could be something added, I mean, maybe we'll see future AV doing this, who knows.  But the idea of stopping something unknown from suddenly encrypting a bunch of files on the hard drive seems like something that could be added to the OS.  And certainly all of our OS vendors are now doing AV and taking responsibility increasingly for malware which is affecting their customers' machines.  It's no longer the exception.  It's now the rule.



So it really looks to me like we are seeing the beginning of what I titled this podcast, a ransomware epidemic, as a consequence of the fact that it is now possible for bad guys to make some serious money, depending upon the size of the entity that they're able to take down.  And it's no longer the case that these are exceptional events.  And so I think we're going to have to see IT departments getting very serious about coming up with some countermeasures for allowing quick recovery in the event that that happens.



LEO:  Of course remember FileVault and BitLocker are locking all the files, too.  I mean, there are some...



STEVE:  BitLocker doesn't.



LEO:  It doesn't encrypt all the files?



STEVE:  Well, it does, but when you're inside it's decrypted.



LEO:  It's unencrypted, yeah, yeah.



STEVE:  Yeah, yeah.



LEO:  I mean, you could distinguish between the two, obviously.



STEVE:  Well, and if you encrypt the encrypted files, then you can't get them back, either.



LEO:  Right.  Even better.  



STEVE:  That's right.



LEO:  So here is your short URL:  twit.to.  That's our URL shortener, twit.to, not .tv, twit.to/unlocked.  Because that's the name of our event:  Cybersecurity & Identity Trends, Unlocked.  It is coming Thursday, October 3rd, one month from today, in Boston at the Intercontinental Hotel.  Now, don't panic, Steve.  I know it says 3:45 p.m. to 8:00 p.m.  We will not be onstage that whole time.  We're only going to do an hour and a half.  But there will be cocktails afterwards, and people can mingle.  So they're not going to throw us out until 8:00 p.m.  But it will begin around 4:00 p.m., so you want to get there early.



And as I said, there are a limited number of tickets for this.  So if you're going to be in the Boston area October 3rd, then please.  It's free to go, but every attendee will get a $100 token they can donate to the charity of their choice.  This is part of LogMeIn's corporate social responsibility program, Mission Possible.  So thank you, LogMeIn, for doing this, and LastPass.  Of course I'll be there.  You will be there, Steve Gibson.



STEVE:  Yup.



LEO:  Bill Cheswick, "Ches" will be there.  And the CISO of LogMeIn, Gerry Beuchelt, will be there, as well.  So it's going to be a great panel.  I'm really looking forward to it.  And you're all invited.  It is free to attend.  Twit.to/unlocked, if you're going to be in the Boston area October 3rd.



STEVE:  And I'll just mention, as a courtesy, if your plans change and you can't make it...



LEO:  Let us know.



STEVE:  Please uninvite yourself so that people who are actually able to attend will be able to.



LEO:  Always a problem with free tickets.  If you made people pay a thousand bucks, they'd show up.  You can almost promise.  But you give away tickets, sometimes people change their minds.  So please, yeah, let us know.  That would be a good courtesy.  Twit.to/unlocked.  I'm going to give you another URL:  GRC.com.  No shortener needed.



STEVE:  Oh, I know that one, yay.



LEO:  Yeah.  No shortener needed for that one.  That's as short as can be.  That's Steve's home on the Internet, the Gibson Research Corporation.  While you're there, pick up SpinRite, the world's best hard drive recovery and maintenance utility.  And you can also of course get copies of this show.  He makes the unique 16Kb versions available for the bandwidth impaired.  Also text versions, human transcribed by Elaine Farris.  So that's a really nice feature there at GRC.com.  We have audio, but also video, at TWiT.tv/sn.  That's our website, TWiT.tv/sn.  It's also on YouTube on the Security Now! channel.



You know the best thing to do, why worry about where it is.  Subscribe.  Get a podcast application and search for Security Now!, press the subscribe button, you'll get it automatically every Tuesday, the minute we're done.  Well, give the editors a few minutes to edit it up.  We do do the show every Tuesday around 1:30 Pacific, 4:30 Eastern, 20:30 UTC at TWiT.tv/live.  You can watch live.  The chatroom is irc.twit.tv.  Steve?



STEVE:  Except for next week, when we are...



LEO:  We're going to be on Monday.



STEVE:  We're going to be on Monday at 1:00 p.m., you and I, in order to make lots of room for the big Apple event on Tuesday.



LEO:  The following day, yeah.



STEVE:  Yes.



LEO:  So we're going to make it an all-Apple day on Tuesday.  10:00 a.m. the Apple event, right after that MacBreak Weekly, then iOS Today.  That squeezes Security Now! out, so we're going to move you to Monday because we know everybody wants their Security Now!.  We're not going to put you off.  So we'll do it a little early.  That way you'll get it a little sooner.  So if you want to watch live next Monday, which is September 9th, you can do that.  Just watch at 1:00 p.m. Pacific on September 9th, 4:00 p.m. Eastern.



STEVE:  Will the podcast be available for download a day early?  Or will it appear at the same time?



LEO:  It's up to you.  What do you want?



STEVE:  I don't care.  I just thought, if it was there, our listeners I know are anxious for it.  So...



LEO:  Subscribe.  That way you'll get it the minute it's available.



STEVE:  Hey, there you go.



LEO:  I see no reason not to put it out early, unless there's no editors here.



STEVE:  Yeah.  Yeah, I agree, Leo.  I don't see any reason.



LEO:  We won't have editors?  It's possible we won't have editors because we don't do much on Monday.



STEVE:  Okay.



LEO:  So if we don't, it'll come out Tuesday.  But no later than usual.



STEVE:  And that's useful for Elaine, too, for her planning, because she's got to know when it's going to appear.  



LEO:  So next Monday.  Just once for the Security Now! show.  And then you and I, because you're going to Europe soon...



STEVE:  Yes.



LEO:  So you and I are going to do some early Security Nows for your trip.



STEVE:  Yes.  We will be recording on several Saturdays, two successive Saturdays before I leave so that we can keep our Security Nows flowing.



LEO:  Yes.  I think I have those right in front of me.  Saturday September 14th and the 21st.  



STEVE:  Yup.



LEO:  We're going to do an evergreen one on the 14th, that'll be for October 1st.  We don't want to do it too early because then everything could change by October 1st.  So we'll do a more evergreen show then.  And on the 21st we're doing it for the week of the 24th, so it's just a couple of days away.



STEVE:  Yeah.  We're going to - I have continued to explore intersystem file synchronization.



LEO:  Oh, good, so it's going to be a whole show?



STEVE:  That's what we're going to talk about.  It's Steve's File Sync Destination or conclusion or something.  I don't remember what I was going to call it.



LEO:  I can't wait.  That's going to be very interesting.



STEVE:  Oh, I think I was going to call it "Sync or Swim."



LEO:  "Sync or Swim."  I like it.  Steve, have a great week.  We'll see you next Monday for Security Now!.



STEVE:  Right-o, my friend.



Copyright (c) 2019 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.






GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#731

DATE:		September 10, 2019

TITLE:		DeepFakes

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-731.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we look at a forced two-day recess of all schools in Flagstaff, Arizona; the case of a ransomware operator being too greedy; Apple's controversial response to Google's posting last week about the watering hole attacks; Zerodium's new payout schedule and what it might mean; the final full public disclosure of BlueKeep exploitation code; some potentially serious flaws found and fixed in PHP that may require our listener's attention; some SQRL news, miscellany, and closing-the-loop feedback from a listener.  Then we take our first look on this podcast into the growing problem and threat of "Deepfake" media content.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  Yes, we're recording a little early to make room for the Apple event on Tuesday.  But don't worry.  We've got a great show planned for you.  Steve will give you a rundown of some of the latest ransomware wins and losses, including one ransomware author that decided to charge way too much.  We'll also talk about deepfakes, how to tell if something's really real.  We're quickly headed to an era where not everything you see can be believed.  It's all coming up on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 731, recorded Monday, September 9th, 2019:  Deepfakes.



It's time for Security Now!, the show where we talk about the latest security and privacy and all that jazz.  Steve Gibson is here.  He's the man in charge at GRC.com.  Hi, Steve.



STEVE GIBSON:  Hey, Leo.  I'm looking at one screen where I'm normally looking at you, which says:  Working on updates, 23%.



LEO:  Oh.  It's not even Patch Tuesday.  That's tomorrow.



STEVE:  Yeah, well, it was, you know, I'd been deferring, right, updates because I only use this particular Windows 10 machine for you.  It is reserved.  It's my Leo computer.



LEO:  Thank you.



STEVE:  And so several hours ago I thought, because it turned on, and it said, oh, you've got updates, you know.  And I thought, okay.  I looked at the clock.  Lots of time.  No problem.



LEO:  Yeah.



STEVE:  Uh-huh.



LEO:  You know, I can install an entire Linux OS in a tenth of the time that it takes to update Windows.  It's crazy.



STEVE:  You could probably - you could write an operating system in Lisp...



LEO:  I probably could.



STEVE:  ...in the time it takes to install updates.



LEO:  Well, I do see - I just ran my update thing.  Maybe you're getting 1903, which is kind of ironic because 19H2 is going to be out sometime.



STEVE:  What I'm getting is dizzy from looking at these little dots running around in circles.  It's like...



LEO:  And then in a few minutes it'll remind you, all your files, exactly where you left them, and getting things set up.



STEVE:  So let's not pay attention to the fact that I moved from 21 to 23, and it only took half an hour to make that 2% jump.  We're doing odd percentages for some reason.  I don't know why.  And actually we have an odd podcast.  It's 731, and we're recording it on the 9th; but it's technically Security Now! for September 10th because tomorrow is a big Apple announcement day.  And I will be glued to the TWiT channel, following...



LEO:  Will you really?



STEVE:  Oh, yeah.  I love...



LEO:  You still care.



STEVE:  It's like MSTK, whatever that dumb...



LEO:  3000?  2000?



STEVE:  Yeah, right, right, where the little shadows were down in the front, making wisecracks at the movie.



LEO:  Yeah.  That's what we do.



STEVE:  That's like you guys.  That's watching TWiT during an Apple thing.



LEO:  And it's going to be a little different because it's the debut of Mikah Sargent doing it with me this year.  So that'll be kind of fun.  And actually I am kind of - I have a dog in this hunt because I lost my iPhone.  Lisa cracked hers.



STEVE:  Whoa.



LEO:  Well, I didn't lose it.  I gave it to Lisa.  Last week she cracked hers.  And I said, I'm not buying you a new iPhone a week before an Apple event.  That would be insane.



STEVE:  Right.



LEO:  So I said, "Please, use mine."  And, you know, I have enough phones to survive, I guess, a couple of weeks.  But I'm iPhoneless for the duration.



STEVE:  You guess?  Oh, my god.



LEO:  I guess.



STEVE:  Yeah, uh-huh.



LEO:  Should I use the Note 10 Plus or the Galaxy S10 Plus or the Pixel 3 or the...



STEVE:  You know, the only danger would be that expensing these might trigger a red flag at the IRS because they'd be saying, now, whoa, okay, now, wait a minute.



LEO:  How many phones does a person need?



STEVE:  The first 50 phones we could allow.  But, you know, when you are writing off the 51st phone, that's like, ah, yes.



LEO:  This is a terrible time of year for me because I do have to buy all the new phones.  We like to buy them instead of take loners because that way we get a real experience, you know, as a user.



STEVE:  Well, it's good that you're saying this because these tapes can be replayed to the IRS agent when they come.



LEO:  Exactly.  When the audit happens, you say...



STEVE:  And you say, look, look.



LEO:  Please, watch Security Now! 731.



STEVE:  Had no choice.  Now, that in no way relates to the fact that this podcast is titled "Deepfakes."  Deepfakes has been in the news a lot, I mean, in the popular press because what's happening is, of course, with this crazy increase in technology that we're seeing, in available processing power - the concept of a photoshopped photo, I mean, "photoshopping" is now in the common vernacular because we know we cannot trust pictures any longer because they can be faked.



Well, what's happening now with the advent of all this computing technology is audio and video can be faked in real-time.  Anyway, Facebook and Microsoft just announced an initiative to work on sort of counter AI that would be able to do deepfake detection.  And so we've never talked about deepfakes before, and I thought that would be a great topic for us to wrap up the podcast.



But of course lots more is going on.  We have the forced two-day recess of all schools throughout Flagstaff, Arizona last Thursday and Friday, which we have to talk about; the case of a ransomware operator being too greedy and what happened there;  Apple's somewhat controversial response to Google's posting last week about the watering hole attacks that we talked about during last week's podcast.  We have Zerodium's updated payout schedule and what it might mean for the industry.  The final full public disclosure of BlueKeep exploitation code.  There's no longer any mystery about how to execute remote code through this vulnerability that Microsoft released patches to, uncharacteristically, all the way back to Windows XP.



We also have some potentially serious flaws found and fixed, but not auto-updating because it really can't auto-update, in PHP that may require some of our listeners' attention.  We've got a little bit of SQRL news, some miscellany, closing-the-loop feedback with a listener, and then you and I are going to talk about deepfakes.  So I think another great podcast for our listeners.



LEO:  Sounds like we have, you know, when I was a kid, the best thing that could happen to you was a snow day during the winter.



STEVE:  Yahoo!



LEO:  It was a free day because of all the snow.  I guess now we have to have ransom days.



STEVE:  Now we apparently do, yes.



LEO:  School's closed.  Wow.



STEVE:  And I have to say I kept looking back at their Facebook page all weekend and got incremental updates.  In fact, that's our Picture of the Week is what was posted on Thursday, August 5th.  So we'll be talking about that.



LEO:  Wow.  That's just crazy.  Just crazy.  Ransomware Monday.



STEVE:  So the news was posted to their Facebook page, probably on Wednesday.  This is from the Flagstaff Unified School District.  And it read:  "Due to a cybersecurity issue that has impacted the ability of FUSD" - that's Flagstaff Unified School District schools in Flagstaff, Arizona - "to operate normally, there will be no school on Thursday, September 5th."



LEO:  Woohoo.



STEVE:  And they said FACTS, which is probably, I don't know what, Flagstaff something or other [Family and Community Teaming for Students].  They said their childcare centers and FUSD, the Unified School District, preschools have also been canceled. 



LEO:  That bad.



STEVE:  Yes.  And, not surprisingly, recess was then extended through Friday.  They posted:  "All Flagstaff Unified School District schools will be closed on Friday, September 6th, 2019 due to the continuing work to respond to the cybersecurity attack.  Progress was made today in securing critical FUSD systems; but, unfortunately, work will need to continue through the weekend to ensure that students can return to school on Monday.  The FACTS, childcare centers, and FUSD preschool remain closed on Friday, September 6th, as well.  FUSD understands this decision impacts families and the community.  We appreciate your patience as we work through this situation."



And so then I went, as I said, I was checking back through the weekend because I was curious how this was going.  So they had a big "Important Message" banner that was up on their own page.  They were cloning things over to Facebook also and just announcing that they were continuing to work.  And they said, "An announcement regarding school on Monday [today], September 9th, will be made on Sunday, September 8th, when additional information is available."  So of course they weren't and couldn't be sure until they were sure what the story would be.



And so toward the end of the day yesterday they updated their important message:  "All FUSD schools will be back in session on Monday, September 9th.  We appreciate your patience," blah blah blah.  So at this point we don't have any additional information.  But I did check this morning, and everybody's back in school.  So the IT department for the Flagstaff Unified School District worked for some portion of four days in order to bring their systems back online.  We don't know, again, as I said, anything.  There's been no other information.  Did they pay the ransom?  Did they restore from backups?  Was the impact minimal?  You know, we have no more information.



But here, rather than talking about these events as we normally do after the fact, we watched it happen as this was going on.  So yet another instance of a ransomware attack on a public entity having some consequence in the real world.  There was, however, an interesting press conference held last Wednesday by Mayor John Mitchell, who's the mayor of New Bedford, Massachusetts.  He held the first press conference of their adventure, which at that time was two months in the making, telling their interesting story of their own ransomware attack.



The city of New Bedford, Massachusetts' IT network was hit in the wee hours after the July 4th holiday, so like early, early on July 5th, with Ryuk ransomware, which by the way Malwarebytes now places at the top of their list of file-encrypting malware targeting businesses and municipalities.  So it'll be interesting to see whether, moving forward, whether Sodinokibi's, as we've decided we should be pronouncing it, aka REvil, whether its affiliate marketing model is able to displace Ryuk as the number one threat in the ransomware business.  But in any event, fortunately for the city of New Bedford, hackers breached that city's IT network and got Ryuk running on it in the wee hours of the morning following the U.S. annual Fourth of July holiday.  Which was fortunate only inasmuch as apparently the IT staff came in early that day, maybe just taking advantage of the extended four-day weekend, to get some work done.



They discovered that the malware had at that point encrypted the files of 158 workstations, which, while significant, accounted for only 4% of the city's total fleet of 4,000 PCs.  The attack would have been much worse had most of the city's systems not been offline at the time as a consequence of this extended four-day holiday.  So that prevented the ransomware from spreading more deeply throughout the city's entire network.  They were able to move quickly to disconnect the infected machines from the network so that they would not continue trying to infect more machines.



And in this press conference which was held last Wednesday, exactly two months following the attack, this Mayor John Mitchell said:  "While the attack was still underway, the city, through its consultants, reached out to the attacker, which had provided by that time an email address."  He said:  "The attacker responded with a ransom demand specifically that it would provide a decryption key to unlock the encrypted files in return for a bitcoin payment equal to" - get ready for this number.  $5.3 million they were hoping to get.



Now, as it happens, at the moment the city - apparently, it's not clear to me whether he said this or this was as the result of some reporting, but they didn't pay primarily because they didn't have that much money available.  There was no $5.3 million with which to pay.  If it had paid it, it would have made it the largest ransomware payment ever, which would have dwarfed the previous record, which was a million dollars paid by a South Korean web hosting firm not that long ago.  I remember talking about them previously.  But that didn't happen.



Knowing that they had no way of making that kind of a ransom payment, Mayor Mitchell said the city decided to engage in a conversation with the hackers primarily as a stalling tactic to give their IT staff more time to bolster the city's defenses.  I'm sure that he was talking to IT, and they probably said, well, you know, we're really not sure how they got in, so give us a little time, if you can, to shore things up so that we can be sure they can't do more damage, like if they were actually more actively inside the network's system.  So they stalled them a little bit, as much as they could, in order to keep them from running additional ransomware or whatever.



So anyway, at the press conference last Wednesday the mayor said:  "In light of these considerations," he says, "I decided to make a counteroffer using our available insurance coverage in the amount of $400,000," he said, "which I determined to be consistent with other ransoms which had recently been paid by other municipalities."  So of course I guess all city mayors at this point in the U.S. are probably aware of what the going rate is for ransomware.  And clearly, 5.3 million was like, whoa, way over the top.  We are seeing sometimes, we covered one a couple weeks ago, $600,000.  And, you know, so 400,000 is in the ballpark.



Anyway, the attacker declined to negotiate at all, wouldn't even counter offer, and completely rejected the city's position outright.  And since they wouldn't negotiate, and the city didn't have $5.3 million anyway, they decided to restore from backups, also since it was not a catastrophic 4,000 PCs that were hit, but only, as they said, 4% of their total fleet of PCs.  So they restored from backup.  It still took quite a while.  I'm sure it was expensive.  But it ended up being way less expensive than paying the ransom, which, as they said, they had no insurance coverage for anyway.



So here's a story of the people behind Ryuk, and we don't still know whether there are multiple attackers using Ryuk or one group somewhere who own Ryuk and are using it exactly in this fashion to attack municipalities.  But it's probably served as a good lesson to the Ryuk people, the Ryuk operators that, well, you know, yeah, you can get multiple hundreds of thousands of dollars from a municipality, but not high millions.  Not in the five million range.  That's just not going to happen.  So it turns out that the city probably did the right thing by saying no to these guys.



We do have a little bit of additional information on Texas, which as we know was hit by this Sodinokibi ransomware and which affected 22 municipalities.  What we've learned is that the $2.5 million in ransom that was demanded by the Sodinokibi attackers was also declined, and that about half of the 22 municipalities that were attacked have managed to get themselves back online.  Three weeks after the incident took place, the Texas Department of Information Resources (DIR) said that more than half of the impacted entities are now back to operations as usual.  Some cities restored impacted systems from backups, while others rebuilt their networks from scratch.  They just said, okay, I mean, you know, again, these were small municipalities who did have, we have verified, in common the use of a common cloud-based management backup facility.  And so, as a consequence, they were able to avoid the ransom demand.  So here again, $2.5 million?  Nope, that's too much to ask.



The incident responders who managed the ransomware infections on behalf of these 22 Texas municipalities did publish some advice last week for companies and governments to follow.  They had five points.  They said:  "Only allow authentication to remote access software from inside the provider's network."  That, it turns out, was one of the problems.  And actually one of our listeners had some detailed feedback about this particular instance which I've been unable to verify independently, and we'll share that when we get to it.



They also said:  "Use two-factor authentication on remote admin tools and Virtual Private Network tunnels rather than remote desktop protocols."  So that's interesting.  You know, if we read between the lines, that suggests that maybe the problem was an RDP-based attack.  But that's not what the feedback we have received suggests.  So that's not clear, either.  They also said:  "Block inbound network traffic from Tor Exit Nodes.  Block outbound network traffic to Pastebin.  And use Endpoint Detection and Response (EDR) to detect PowerShell running unusual processes."  So some good feedback from those guys.



And, you know, it's too early to say, but it feels a little bit like everybody now really has their guard up.  It'll be interesting to see whether there was sort of a summer flurry of this which is going to slow down, or maybe whether the attacks move more toward corporate entities.  And being nonpublic, if that happens, we may not be able to see that happening.  We won't have the clarity that we have at the moment.



So last week we talked a lot about the interesting blog that Ian Beer at Google's Project Zero made where he told us about, basically, what they characterized as non-discriminating waterhole attacks on a small number of servers targeting specific groups.  And Leo, you had some information that had just been made public, I guess it was in...



LEO:  TechCrunch had it.



STEVE:  TechCrunch, yes.



LEO:  I think - Alex Stamos was tweeting about this.  He suggested, and I think he's probably right, that Google gave this information on background to TechCrunch.  Because Apple confirmed it.



STEVE:  Ah, okay, yes.  And in fact we have some interesting code actually in the show notes that resulted from some additional research.  I think it was RiskIQ that was able to provide additional detail.  So anyway, so what we know is that last week Google published their blog about vulnerabilities that Apple fixed - oh, I'm sorry, what Apple posted at the end of last week in their Newsroom posting was, they said - oh, and I just got a big bright screen in my face.  Windows 10 is back up.  



LEO:  Or just rebooting.



STEVE:  Yeah.  No, it did that about five times; and then it gave me, like, we're updating stuff.  Okay, so I'll turn that big bright screen off.



Anyway, so Google said last week - I'm sorry, Apple.  Apple posted the following:  "Last week, Google published a blog about vulnerabilities that Apple fixed for iOS users in February."  They said, "We've heard from customers who were concerned by some of the claims, and we want to make sure all of our customers have the facts."



Apple wrote:  "First, the sophisticated attack was narrowly focused, not a broad-based exploit of iPhones 'en masse' as described."  So Apple was pushing back about some of what Google was saying.  They wrote:  "The attack affected fewer than a dozen websites" - and we'd heard 11 - "that focus on content related to" - and I can't pronounce this.  What's the name of the Muslim community?



LEO:  The Uighurs.  The Uighurs.



STEVE:  The Uighurs.



LEO:  Uighurs, yeah.



STEVE:  Uighurs community.  They said:  "Regardless of the scale of the attack, we take the safety and security of all users extremely seriously."  They wrote:  "Google's post, issued six months after iOS patches were released, creates the false impression of 'mass exploitation' to 'monitor the private activities of entire populations in real time,' stoking fear among iPhone users that their devices had been compromised.  This was never the case," wrote Apple.



They said:  "Second, all evidence indicates that these website attacks were only operational for a brief period, roughly two months, not two years as Google implies."  They wrote:  "We fixed the vulnerabilities in question in February, working extremely quickly" - which of course you and I, Leo, made a point of noting last week - "to resolve the issue just 10 days after we learned about it.  When Google approached us, we were already in the process of fixing the exploited bugs."  Which we didn't know of, if that's the case.



They said security - this is Apple - "is a never-ending journey, and our customers can be confident we are working for them.  iOS security is unmatched because we take end-to-end responsibility for the security of our hardware and software.  Our product security teams around the world are constantly iterating to introduce new protections and patch vulnerabilities as soon as they're found.  We will never stop our tireless work to keep our users safe."  So that was their blog posting.  Which a lot of those in the tech community were a little put off by.  They weren't really super happy with the tone of Apple's response. 



Last week the head of Threat Research for RiskIQ told ZDNet for their reporting on this that the attacks were indeed very targeted, and that Google was wrong in its initial assessment.  He later shared some thoughts in a public tweet, and provided us with some code.



LEO:  Very targeted in their regard is, well, it could only have affected the one billion people who are in China.



STEVE:  Exactly.



LEO:  Okay.



STEVE:  Exactly.  So, yeah.  Of course, as we know, that was the group.  We have reason to believe that it was - and I think you had said this also last week, Leo, that it was China, no doubt the Chinese government that had infected some of these sites that was looking to pursue some of these groups.  The JavaScript which was posted as part of RiskIQ's research actually shows them pulling the country of the user through JavaScript, looking for a match of the country equal to China.  And if it's a Chinese person located at an IP address based in China, then it injects some additional JavaScript onto the page, all of this after waiting 60 seconds, which then performs this hack of iOS devices.



So, and actually I learned a little something from looking at this JavaScript code.  There is a cool site, ip.nf, which if you visit, if you just put ip.nf into your browser, it pulls up a description of the API that this site provides.  And if you put ip.nf/me.json, what you get back is a little bit of JSON, you know, JavaScript Object Notation, where some information about you, based on your IP, is shown.  And when I did it, it was extremely accurate in its location.



And what the script does is it offers a little bit more parameterization.  It's ip.nf/me.json?callback=jsons, which gives a more detailed result of sort of a full JSON hierarchy, and then the JavaScript parses that in order to get the country of origin of the user from which the user is connecting out of the JSON.  And in my case it said "United States."  And no doubt for people in China it comes back saying "China."  So anyway, that's that story.



There was a very comprehensive analysis and reporting on Volexity, and I have the link in the show notes for anyone who's interested, who also noted that the same targeted attacks were also being waged against Android visitors.  



LEO:  Not the same ones.  Different ones.  Very different ones.



STEVE:  Well, and not overlapping.



LEO:  Not at all.



STEVE:  So, right.



LEO:  And we'd reported on that, I don't know if you did, but we talked about that.  That was the attempt to put on everybody's phone who came into Xinjiang province, which is where the Uighurs are, East Turkistan, a malware, effectively a spyware application.  So it's not the same, and I wouldn't expect Project Zero to even mention it.  That's not what their mandate is. 



STEVE:  Right.



LEO:  I think Volexity kind of wanted to kind of somehow - I feel like it was an Apple apologist post.  I didn't really think it was a right-on post.



STEVE:  Well, and of course it can't be the same attack because the two platforms are radically different.



LEO:  No, it isn't.  The only thing that's the same is they were targeting - it was apparently China targeting the same minority.



STEVE:  Right.



LEO:  China's also throwing those people into concentration camps.  Is that the same attack?  No.  It's not.  And that's not what Project Zero is about.  It's talking about zero-day exploits.  So the question I want to ask you is, this word "indiscriminate" attack.  Apple wanted it to - really wanted everybody to know, look, you're not a target of this attack.  It's only people in China who aren't white.  It's only Muslim people in China.  Don't worry.



STEVE:  It's only targeted people.  Thus it is clearly a targeted attack.



LEO:  But if you target [crosstalk] as one billion people, is that, I mean, how targeted is that?



STEVE:  Good point, yes.  I mean, we do know from this code that it would launch on your iPhone if you were in China.  Period.  Not a subregion.  This code demonstrates very clearly that it was the country of China.



LEO:  Right.  So what in your estimation - so normally when I think of a targeted attack, I think, you know, China's found a dissident they want to - or some country's found a dissident they want to get into their stuff.



STEVE:  Good point.



LEO:  That's a targeted attack.  Or maybe a company that they want to get into.  That's a targeted attack.  It was indiscriminate in the sense that anybody in China who visited those 11 sites would be hacked, right, on an iPhone.



STEVE:  Yes, yes.  And also remember that one of the reasons attacks are targeted is specifically to keep them from being discovered.



LEO:  Right.



STEVE:  The likelihood of seeing an attack that was truly indiscriminate would be far higher than if it only affected people in China.  So essentially it was a proscribed attack.  Maybe that's...



LEO:  There's a better word, "proscribed."



STEVE:  Yes.



LEO:  It was not a universal attack.  And it was targeted in the other sense that we often think of, you know, there are ransomware attacks that are aimed at anybody who happens to get the malware.  So we mail it to everybody.



STEVE:  Right.



LEO:  Those are clearly indiscriminate.  And then there are ransomware attacks that are trying to get a specific company.  Those are clearly targeted.  In that sense, I guess it was targeted in the sense that they were trying to get certain people.



STEVE:  Right, right.



LEO:  So the language doesn't live up to the description.



STEVE:  Well, yes.  And I guess also there isn't really - this is an instance where we don't really have well-defined language for specifying the degree of targeting.  It's certainly not an individual, and they were not targeting that population because they don't know how to do that.  So they were just saying, well, at least we know they're going to be in China somewhere.



LEO:  Right.  And they're reading these Uighur news sites, or they're on a Uighur social network.  So it's targeted to that respect.



STEVE:  Right, right.  Anyway, so I guess we think, what?  First of all, I completely agree with you that Project Zero is helping Apple.  I mean, after all, we know they have reported privately more than 200 vulnerabilities to Apple.  I mean, that's all for the good.  That's helping everyone who is an iOS user, the fact that Google is going out of their way to improve the iOS platform.  It's not as if they're saying, oh, you're on your own, Apple.  We're going to only fix our own stuff.



LEO:  Volexity and some implied that Google was doing this as anti-competitive FUD, like, oh, see, Apple's got problems; and Volexity said, but they ignored Google's problems.  I don't think that's what Project Zero was doing.  I don't think that's a fair characterization.



STEVE:  Yeah, I agree.



LEO:  Yeah.  And I think Apple's blog post was kind of unnecessarily critical of Google.



STEVE:  And they did not apologize.  I mean, so...



LEO:  Yeah, not really taking responsibility for this.  And almost implying, well, don't worry, it was just these Muslims, no big deal.  It wasn't a good thing.



STEVE:  Yes.  Dan Goodin concluded his coverage for Ars by saying:  "Another key criticism is that Apple's statement has the potential to alienate Project Zero, which according to a Google spokesman has to date privately reported more than 200 vulnerabilities to Apple."  He wrote:  "It's easy to imagine that it wasn't easy for Apple to read last week's deep-dive report publicly documenting what is easily the worst iOS security event in its 12-year history."



LEO:  They would far prefer that Google had not said anything at all; right, yeah.



STEVE:  Yes.  He said:  "But publicly challenging a key ally on such minor details with no new evidence does not create the best optics for Apple."  Which is really the point you were making, Leo.



LEO:  Alex Stamos, who used to be the security guy at Yahoo! and Facebook, has said:  "I've had a lot of experience with mishandled security flaws and long delays before revealing them."  I think he got it right.  He said that the real issue is the security teams at Apple and the security teams at Google, those are colleagues and have respect for each other.  This was a PR release from marketing or legal.  And he said:  "This is not a path you want to take.  Apple does some incredible security work, but this kind of legal and comms-driven response can undermine that work."  So actually it was a message to Apple employees.



He says:  "I've worked for companies that took too long to publicly address their responsibilities.  Demand better."  And I think that that actually is a reasonable point of view.  This is a marcom response.  Apple clearly didn't want the world to worry that their phones were insecure.



STEVE:  Right.



LEO:  And so they thought that that maybe was the takeaway from Google's post.  But the security professionals at both companies understand that's not really what's going on.



STEVE:  Yes.  And when I looked at what Ian posted, at the technical details, I mean, there's no politics.  There's no loyalty at the technology level.  It was just - it's just simple facts.  And I did feel like he was just saying this is what I found.  And given the nature of this mistake in the code, I don't know how this could have gotten through QA.  So he was just saying, well, I see what I see, and this is what I see.  Without really putting a...



LEO:  Project Zero skewers Google just as much.  If they find a Google zero-day, they'll do the same thing.  I don't think they're biased.



STEVE:  Yes.  And remember that they were originally doing that.  And it was when they opened themselves up to non-Google stuff that we all said, wow, holy crap, really.



LEO:  These guys are good.



STEVE:  That's going to be really neat.  And everybody has gained from it.  



LEO:  Yeah, I don't think it's any anticompetitive move at all on Project Zero's part.



STEVE:  Yeah.  Zerodium, the controversial company we talk about from time to time...



LEO:  Are they the good guys are the bad guys?  I can never remember.



STEVE:  We're not sure.



LEO:  Okay.



STEVE:  Yeah, we're not sure.  It's funny because they have this chart of payouts which is meant to look like a chart of the elements.  And then I realized, oh, yeah, of course, because Zerodium.



LEO:  Oh, it's an element.  Ah.



STEVE:  It's meant to sort of - it's supposed to be sort of an element, yeah.  And the chart is not clear.  You have to sort of study it for a while.  But there is - they announced on September 3rd, last Tuesday, some major changes to their payout schedule.  I first encountered this from a story at the Hacker News.  And the way they put it I take a little bit of issue with.  They said:  "There's some good news for hackers and vulnerability hunters, though terrible news for Google, Android device manufacturers, and their billions of users worldwide."  Okay, and that's what I sort of take issue with because I don't think it's ever bad to find bugs because they're going to get fixed if you find them.  And the chance of them being found is greater if they're being exploited, even though, yes, there are victims of the exploitation until they're found.



Anyway, it appears that the zero-day marketplace has recently shifted toward the Android operating system with Zerodium now suddenly bumping payouts for researchers of the most severe class of Android zero-days up by a factor of 12.5.  They were previously offering $200,000.  They are now offering $2.5 million.  Okay, hackers, take note:  $2.5 million.  So that moves it above the maximum $2 million payout for the equivalent gold standard exploit of that kind of bug for iOS.  The gold standard is zero-click, persistent, root-level kernel access of a device, meaning that the user doesn't even have to click, doesn't even have to acknowledge or make a single click.  A zero-click exploit is the gold standard for one of these.



So, and the reason I guess I take a little bit of issue with that characterization of calling it terrible news for Google, Android device manufacturers, and their billions of users worldwide is, you know, yes, if you were a member of an oppressed group or minority which some repressive regime might have a strong interest in monitoring, while also having that much money to spend in order to purchase what is arguably going to be a transient ability to do that, to monitor you, to intercept your communications or whatever, then yeah, you know, all other things being equal, the increased bounty on the Android side, and it is a significant increase, would likely increase the likelihood that hackers would be trying to pry into Android rather than iOS.



And the Hacker News notes, and I agree with them, they said:  "Just like other traditional markets, the zero-day market is a game of supply, demand, and strategy, which suggests either the demand for Android zero-days has significantly increased, or somehow the Android OS is getting tougher to hack remotely," they said, "which seems unlikely."  And of course, as we know, Zerodium is a controversial enterprise which purchases zero-day exploits from hackers and then, as far as we know, almost certainly resells them - because otherwise how could they afford to pay $2.5 million to hackers for exploits - resells them to law enforcement agencies and nation-sponsored spies around the world.  So the only way the economic model would work would be if they were functioning as a middleman, essentially.  So it seems that that is probably what's going on.



I got confused by my notes here because I kind of went off.  A potential exploit purchaser might have said to Zerodium - yeah, right.  I was sort of drawing a little bit of theory about how it is that they could be offering $2.5 million.  So the theory would be that a potential exploit purchaser, a nation-state, for example, would say to Zerodium, look, we have a need.  We will pay $3 million for 90 days of exclusive access to a zero-click persistent exploit for a fully patched Android device.  Whereupon Zerodium turns around and makes this offer, which they did last week, subtracting their middleman commission, saying to the hacker/cracker community, okay, we're upping the ante for this class of exploit to $2.5 million.  Give it your best. 



So as I said, the same class of exploit on an iOS device is $2 million.  So that's where the market is.  In addition to this big change in Android, they also announced some app-targeted payouts, half a million dollars for submitting new persistence exploits or techniques for iOS and payouts for WhatsApp and iMessage.  So now those two messaging apps for the first time are receiving additional attention for mistakes, not at the OS level, but at the app level, because of course that's where the action is also.



LEO:  These prices are driven by customer demand, do you think?  Or by difficulty?  More by customer demand, I would think; right?



STEVE:  I would think yes.  I would think by customer demand.  Certainly they're not going to offer $2.5 million...



LEO:  Unless they know they can sell it for that.



STEVE:  Exactly.



LEO:  Yeah, yeah.



STEVE:  Exactly.



LEO:  Sell it for twice that.



STEVE:  Either that, or they know that they have a demand pool that would pay, for example, a total of five. 



LEO:  Right.



STEVE:  So maybe they have five customers who they know they can sell that kind of exploit to for a million dollars each.  In which case they know they're going to be able to double their money.  I mean, it's clear they are a commercial entity.  And so they've got an established customer base.  They know what the going rate is for these things.  And so, but something just happened.  You know, something happened to move that from $200,000 on the Android platform to $2.5 million.  And really, Android being open source, so hackers can look at it, it's much more difficult to attack iOS.  It's a closed, to a much greater degree, a closed hardware and software ecosystem, so it made sense that it would be 10 times harder and therefore worth 10 times.  On the other hand, a lot of people are using Android devices.



LEO:  Yeah.  I think I've also heard it said that...



STEVE:  The size of the market?



LEO:  Yeah, well, often when you see this sudden jump, it's because some nation-state came along and said, you know, we really want to get those Uighurs, let's say.  And most of them are using Android.  We need an Android exploit, stat.



STEVE:  Yup.



LEO:  And so it could also be that simple.  There could actually be a buyer out there saying I need it right now.  And boy, if you look at WhatsApp, of course.  That's, I mean, that's what a nation-state would want to hack.



STEVE:  Yes.  And Apple's maximum payout for this kind of exploit is $1 million.  Okay, yeah.  A lot of money.  But Zerodium will pay two for the same thing.



LEO:  By the way, that's also the point.  They're going to always want to beat Apple.



STEVE:  Yes, yes.  And so you've got to say, if you're a hacker who does manage, I mean, it's not easy to find an exploit now in iOS.  Well, unless they make a bad mistake like they did recently with 12.4.  But still, you know, would you rather double your money or just get a million dollars?  It's like, no, I think I'd go for two.  But it is when these things are found,  I mean, and this is exactly the point of Google's Project Zero announcement last week that we covered, which was the result of several years of their study.  These things are found when they are exploited in the wild.



So what's really interesting also is that it has to be the case that somebody paying $2 million, I mean, that has to be chump change, essentially, for the kind of purchaser of these things because they know by the act of exploiting this, there's a window before it gets discovered.  And so they've got to be able to say, hey, we'll pay X amount of money, millions of dollars, for the opportunity for some period of time until it's seen and then reported and then patched, and then until the patch actually gets pushed out.  Now, that's the other thing about Android, Leo.  Consider how much more valuable an exploit is on Android devices that are not being patched because - so that also suggests...



LEO:  Which is the majority of them.



STEVE:  Yes, yes.  So that also suggests that there is much greater value to the holder of an unknown exploit.  Even if it is patched, it's still going to hold its value because it only matters if your target is patching in order to close an exploit, not if Google is patching in order, you know, after they discover the problem.  So that suggests there is a much longer tail on these things and that they have a lot more residual value over time. 



So BlueKeep.  We've been talking about this all year.  And somewhat to everyone's bemusement, nothing has ever happened with this thing.  It was, oh, my god, super critical problem.  A readily exploitable flaw in the remote desktop protocol came to light at the beginning of the year.  Microsoft was so freaked out they went all the way back to XP and offered patches for XP.  This does not affect the newest versions of Windows, not 8.1 and 10, only 7 and Server 2012 R2 and previous.  But still, it was trivial to cause a denial of service effect.  It was much more difficult to leverage that into a remote code exploit.



But of course we're always talking about how the first thing that you discover is a way to crash the machine.  And it's only then, after much greater analysis, and by applying much more finesse to the problem that you are, if you're sufficiently skilled, able to turn this into a remote code exploit.



Well, that happened last Friday.  A Metasploit module was released on Friday, which puts it now in the open source community and public.  So our friend Marcus Hutchins, aka MalwareTechBlog, posted a very clear expos.  He titled his posting "BlueKeep," and this is the perfect way to frame it, too.  He said:  "A Journey from DoS to RCE."  And he wrote, he started his posting:  "Due to the serious risk of a BlueKeep-based worm, I've held back this write-up to avoid advancing the timeline.  Now that a proof of concept for remote code execution has been released as part of Metasploit, I feel it's now safe for me to post this.  This article will be a follow-on from my previous analysis."



So his previous analysis was posted back in May, simply titled "Analysis of CVE-2019-0708," which is BlueKeep.  And that posting, his posting several months ago, begins, he says:  "I held back this write-up until a proof of concept was publicly available, as not to cause any harm.  Now that there are multiple denial-of-service proof of concepts on GitHub, I'm posting my analysis."



And so that was the so-called "denial of service" means you're crashing the remote server because other people who want to use it are denied its service because it crashed.  So what we've seen is, although this took quite a while, we had the, okay, we now know how to remotely crash a server whose unpatched remote desktop protocol service is exposed.  Then, as of Friday, now in the public, is the remote code execution.  So this follows exactly the process that we've talked about.



I'm not going to go in any more depth because there's, again, it's very much like Ian Beer's analysis of the iOS stuff.  It immediately drops you into the weeds.  I got a kick, though, out of the fact that his intro to his second posting, which he just posted, picks up, like, right from where he left off.  It's as if it's really one long posting, and he stopped with the first one exactly where what was known to the public stopped, where he went - he got to parity, and then the second one picks up, like with the next sentence.



So anyway, the takeaway for us is that the world now has access to everything needed for less skilled attackers to use the RDP protocol on still-unused machines to execute code on them.  And of course this is much more worrisome now because I'm sure it's probably not even a pyramid.  That is, if you were to graph the degree of expertise versus population, it's very rarefied at that high end, which is why it's taken until now for anyone to successfully reverse engineer a remote code execution.  Remember that it was known it could be done, but they were very rarefied.  And, boy, if you want to get a sense for how difficult it is, Marcus's write-up, it's a beautiful write-up that I have a link to in the show notes.  So I recommend to anyone who's interested to take a look at it.



PHP has been moving along.  As we know, this so-called Personal Home Page continues to be, by a huge margin, the number one most popular server-side web programming language in the world.  It is able to lay claim to more than 78% of the Internet's servers.  So it's significant when the maintainers of PHP release updates to it which patch multiple high-severity vulnerabilities in PHP's core and in some of its bundled libraries, noting that the most severe of these could allow remote code execution by attackers targeting and compromising servers.



So I wanted to give our listeners a heads-up.  There are no publicly exposed services that PHP produces that automatically expose, like, all these servers in the world.  It's not like RDP where there's a well-known service exposed.  PHP operates behind the scenes as an enabling language in which web apps are written.  But it does mean that, if the web apps your server is using uses some of these libraries which provide an exposure, then they're subject to arbitrary remote code execution.  Quoting from the tech press about this, depending on the type, occurrence, and usage of the affected code base in a PHP application, successful exploitation of some of the most severe vulnerabilities could allow an attacker to execute arbitrary code in the context of the affected application with associated privileges.  The most likely exploits or consequences are going to be denial of service conditions where you crash the system.



The tech press wrote:  "The vulnerabilities could leave hundreds of thousands of web applications that rely on PHP open to code execution attacks, including websites powered by some of the most popular content management systems - WordPress, Drupal, TYPO3 and so forth."  There are use-after-free code execution vulnerabilities in a regular expression library that comes bundled with PHP and is used by a number of programming languages.



So anyway, PHP doesn't update itself.  My own server, I have a server at GRC where I run PHP.  It's the core component, obviously, behind my own WordPress blog; the SQRL forums, which are XenForo forums, XenForo being a large PHP application.  The little link shortener that I talked about recently is written in PHP, as are some other services.  I've checked.  My stuff is clean.  It doesn't have - it's not using any of these vulnerabilities.  But I nevertheless updated because I would recommend everyone to do that.



It's not something that can happen automatically because there isn't an automatic update service for PHP.  The process, obviously, for anyone who's maintaining PHP, I'm sure they know, you have to bring the server down, then update the core PHP components because the server is using those, often keeping them in use in memory, depending upon how you are causing PHP to be executed.  And then bringing the server back up after you've got PHP updated.  So anyway, my point is it's not a super emergency, despite the fact that it is, like, incredibly widely used on the Internet.



Probably, if you're the party responsible for maintaining PHP, you're aware of how to update it.  I just did want to put it on everyone's radar that there was just recently a round of critical updates affecting the 7.3 chain, the 7.2 chain, and the 7.1 chain.  So when you can, it's worth doing.



Since last week I wanted to mention that the SQRL documentation project is completed.  Four documents which are 79 pages across four individual PDFs are now in place, which completely explain SQRL's features, its operation, and its implementation in sufficient detail for anyone interested to create SQRL clients and servers.  There's still some discussion over in the SQRL newsgroup about some little edge conditions around some condition flags.  So as soon as I'm through with the podcast, I'm going to get back to get that final stuff squared away.  But basically the documentation project is finished.



Also Firefox, I'm noticing, is getting some increasing attention.  I know you, Leo, have been looking at it more closely, liking it, after you've gotten sort of used to the way it differs from Chrome.  I've heard you talking on some of your other podcasts and to different groups about Firefox.  I have a machine which is RAM constrained, which is funny to say about a machine with 8GB these days.



LEO:  That's funny, yeah.



STEVE:  Isn't that unbelievable?



LEO:  Yeah, yeah.  But true, it's true.



STEVE:  Oh, my goodness.  It is.  And so sometimes when I'm preparing the podcast on that, it's an older Lenovo X1 Carbon.  And I could only get 8GB.  That's the most...



LEO:  I remember when you bought that, yeah, yeah.



STEVE:  Yup.  And I still love that machine.



LEO:  No, it's a great machine, yeah.



STEVE:  I know that the X1 Carbon is your favorite machine, too.  If I were to buy another one, and I eventually will, I again will get the maximum memory, which now is 16GB.  And I probably wouldn't have a problem if I were to have that.  But I'm sometimes getting warnings that my system is out of RAM when I have many tabs open in Firefox.  And - I know.



LEO:  You must have so many tabs.



STEVE:  Well, actually, they are - I think they're large pages.  And so, for example, BleepingComputer has very large, very content-heavy pages.  And so what I'm doing is I'm going through stories, clicking on pages that I will then come back and read.  So my work methodology has me open a bunch of pages, which I'm then going back to examine and read and study and decide if they make it onto the podcast.  All of this by way of saying that I was reminded that Firefox, in I think it was 67 - we're now at 69.  And 67 was supposed to have obsoleted the manual release page, release tab memory feature.  But it apparently isn't working.  Or maybe it isn't fast enough or aggressive enough.  Or it doesn't look to see whether the system is running out of memory.  I don't know what the problem is.  But I'm still running out of memory.



And in the show notes, Leo, I have a screenshot of Task Manager running in Windows when I was nearly out of memory and I did something in Firefox.  And so this is a Firefox user tip.  There is a very lightweight add-on called "UnloadTabs."  And the option it offers is unload all but the current tab.  And so you right-click on the current tab, down on the bottom.  Unload all but this one.  You click it, and you can see what happens to memory.  I mean, it almost dropped, well, not quite in half.  I've seen it drop in half.  It makes an immediate release; and then a few seconds later, I don't know, some other stuff gets released, and then it makes for a nice savings.



So anyway, I just wanted to pass this as a tip to our listeners.  There are a couple of them.  UnloadTabs was recently updated to reflect the change that Firefox made to their add-on system so it again - it stopped working.  The author invested 40 hours of time bringing it back.  And I'm thankful that he did because it's handy just to be able to say, if you have a bunch of tabs open, they can be using a lot of memory.  And so this just allows them to just - the tabs stay, so as placeholders, which is all I'm using them for.  You get to keep your placeholders.  But you'll only then reload the memory as you then click the tab to bring it current again.  So just a little tip for our listeners.



Speaking of tips, I found - it was a tweet from a Notre Poubelle.  On Saturday he tweeted:  "Hi, Steve.  I have a SpinRite question.  I am not currently a SpinRite customer."  He says:  "I have a hard drive that I suspect is having problems.  I ran Windows chkdsk C: /f /r /x a couple of times, and it gets stuck for a very long time at the same percentage point.  In this case, I actually don't care at all about the content of the hard drive."  Oh, that's interesting.  So it's C:, but he still doesn't care about it.  That's cool.  He says:  "My question is, is it worth purchasing SpinRite, given that I don't care about the contents of the hard drive?  Or would it be wiser to just buy another hard drive?  Assuming I did buy SpinRite, and it fixed the hard drive, is it likely a temporary fix, and I'll end up having to buy a hard drive anyway?"  Great question.  



LEO:  That is.  That is.  Because hard drives are so cheap now.



STEVE:  Yes, yes.  And so that was one of the things I thought was that hard drives are cheap.  On the other hand, you have to pay for a cheap hard drive every time you purchase a new one.  You only have to pay for SpinRite once.



LEO:  True.  Good point.



STEVE:  And it will fix all the hard drives you have every time they get in trouble, until they finally do really die.  And it is the case, I mean, first of all, there's no question that SpinRite would get past that sticky point, and it might be, for example - well, for example.  Say that the drive was writing when the power failed on it, like if it was an external drive, and you pulled the cord.  Well, that would definitely damage the sector that was being written at the time.  But that by no means means that the hard drive is no longer any good.  It just means that that spot, that sector, while being written, didn't get its checksum set correctly.



So SpinRite will fix that.  It'll either fix the checksum - if writing it is able to fix it, SpinRite will do that.  If rewriting it won't fix it, then SpinRite will get the drive to relocate it, basically removing it from service forever and replacing it with a good one.  So your drive will get fixed.  And SpinRite will do this every time something happens to any drive you own.  So again, and of course in the case of SpinRite, as you know, then you'll be able to get all of the other data that was hiding behind that bad spot, especially, for example, if it happened to be a directory sector.  Then you would lose access without SpinRite to everything else that was downstream of that point in your directory hierarchy.



But again, this guy said he didn't care what was on the drive.  So it's certainly a bit of a tradeoff.  For what it's worth, SpinRite would fix it, and the drive is probably okay.  And the other cool thing is that, when you're running SpinRite, that SMART data will show you if the drive is getting soft.  If SpinRite pushes the health parameters down, which exposes red bars in the SMART data's bar chart, that's a sign this drive is getting soft; and, yes, SpinRite is then advising you that it's probably good to replace the drive.  So anyway, great question, and not a simple answer.



LEO:  I like it.  So it kind of depends why you're losing sectors.  But I like it that SpinRite'll give you some idea of whether the drive is healthy or not.



STEVE:  Yeah, yeah.  It really is great about doing that.



LEO:  That's handy.



STEVE:  And that's the other thing people miss is that that SMART, the Self Monitoring Analysis and Reporting Technology, SMART, it's only useful if the drive is being worked, if it's under load.  If it's being asked to do work, that's where you see sectors being relocated, or seeks having to be retried too much, which is an indication that the low-level format is getting soft.  So you have to put it under load.  And so SpinRite together with that SMART analysis is just an incredibly powerful tool.  



Anyway, Steve S. tweeted me.  He says:  "I'm an avid listener of Security Now! and wanted you to know that you are correct about the 22 towns that were infected with ransomware.  A third party was providing IT services" - now, that we knew last week - "access to state DMV resources, utility payments, et cetera."  So he has some more information than we had.  And he continues:  "The third party had an OpenVPN connection to each municipality.  Not sure where it started, but it spread everywhere.  Keep up the good work.  I enjoy the podcast."



And so that's interesting because we've only talked about, and we generally talk about, OpenVPN in the context of on-demand.  I still want to use the term "dial-up," you know, because that's the way it was once upon a time.  But, for example, that's the way I'm still using or I'm currently using OpenVPN.  I normally don't maintain a static connection bridging networks together because I don't have a need.  I will bring up an OpenVPN tunnel the way - and that's the way, of course, most people use OpenVPN, when you're out roaming, and you want to connect to your home base, to your corporation, to their OpenVPN services.  You bring up an OpenVPN tunnel.  It establishes.  You authenticate.  And then you're now part of that network on the other end of the tunnel.



But the other way to use OpenVPN, and it is arguably, I mean, it's robust, and it's a cool approach for those who need it, is for it to be used in a static network bridging mode where - and again, it does this really well, where you have two endpoints, and they establish a link across the Internet, bridging their different subnets, basically their non-Internet LANs into a single LAN.  And OpenVPN will even transmit broadcasts.  So you can create a common broadcast domain across disparate LANs so that you can do full discovery of systems and services across that link.  It is very cool.  So it doesn't just run at level 3.  It's able to run at level 2 and actually create an Ethernet bridge, not an IP bridge, between separate networks.



So anyway, in this instance, it looks like, from what Steve S. has tweeted, that what this group probably had was a bunch of static OpenVPN connections from this IT service out to these 22 municipalities.  And so we don't know where the - we don't know that the IT service even themselves was the source of the infection.  It may have been one of those 22 municipalities, in fact, it seems more likely, one of those 22 got infected.  The malware rode up the VPN connection into the IT service and then back out.  It went, aha!  It's like, it struck gold, then ran out the other 21 VPN connections to get into all of the other 21 municipalities and brought them all down.  So a real interesting case of an OpenVPN-enabled network dramatically magnifying the size of an attack.



But even so, the bad guys didn't get paid.  They asked for 2.5 million.  These guys, the small municipalities said, no thanks, we're not that big.  The damage done wasn't that extreme.  So we'll restore from backup where we can, or we'll just rebuild our networks.  Thanks very much.



LEO:  Happy ending.  Sort of.



STEVE:  Happy ending.



LEO:  Sort of.



STEVE:  Oh, yeah, well, yeah.



LEO:  Now let's talk about Deepfakes.



STEVE:  Deepfakes.  So this first, well, I've been aware of it happening for a while.  I've been noticing that there's been a lot of conversation about it and demos and things on standard broadcast TV.  A story caught my attention that I was going to cover this week.  And I decided when I saw the second story about a move being made to create deepfake detection, that it was worth just pushing back a little bit and spending some time talking about it.  The first story was in The Wall Street Journal, which is behind a paywall.  So I was able to get the first paragraph of it and then dug around and found some more information from others who have Wall Street Journal subscriptions.  The Wall Street Journal story was titled "Fraudsters Use AI to Mimic CEO's Voice in Unusual Cybercrime Case."



LEO:  This was wild; wasn't it?  Oh, my god.



STEVE:  Yes.  And their coverage said:  "Criminals used artificial intelligence-based software to impersonate a chief executive's voice and demand a fraudulent transfer of 220,000 euros ($243,000) in March in what cybercrime experts described as an unusual case of artificial intelligence being used in hacking.  The CEO of a U.K.-based energy firm thought he was speaking on the phone with his boss, the chief executive of the firm's German parent company, who asked him to send the funds to a Hungarian supplier. The caller said the request was urgent and directed the executive to make this transfer."



So this sort of put me in mind of the way the world is changing.  In the very early days of email, the very, very early days, we might have taken a note from apparently our boss at face value and acted upon it.  But obviously those days are long gone.  And for some time, due to its power to convincingly fool and spoof, the term "photoshopping," as I mentioned at the top of the show, has become a well-established term now.  Everyone knows what a "photoshopped image" means, and Adobe gets a lot of credit for Photoshop being the app behind that.



But now, as I said at the top of the show, the crazy power of today's computational resources has moved us to an entirely new level of spoofing.  Not only are we able, we have been long-time, been able to spoof a static image, now we're able to spoof, and this is what has changed so much, complex time-varying signals, both audio and video, in real-time.



So today, upon receiving something important and unexpected via email, we might give our boss a call and say, hey, did you just send me an email asking, you know, whatever.  But notice how we would confirm it.  We would confirm by voice because today we still trust that.  So the point here is no one trusts a photo anymore.  That's been spoiled.  And we're also on the brink of losing our ability to trust real-time non-face-to-face electronic communication, as well.



We're entering a future where it might be necessary for the instructions to be, if I ever tell you by email or even by phone, I guess even in a video conference, to do something you don't expect, I want you to confirm it face to face.  You know, just walk down the hall, stick your head in the door, and ask, "Hey, did you just call me and ask me to something?"  And I guess my point is, think, Leo, how much that changes our world, if we can't trust, not only our own ears, but our own eyes.



LEO:  A number of online brokers, I know Charles Schwab does this, and I know Fidelity do this, have voice authentication.  And I always thought that was a little sketch.  But, you know, you still have to have the password.  It's like a third factor.



STEVE:  So you mean, so they try to do voice recognition?



LEO:  Yes, on you.



STEVE:  Wow.



LEO:  You have to train it before you use it, obviously.  So that's an option.  And I've always just ignored it because I thought, well, my voice has been recorded so many times, I don't think it'd be too hard to synthesize, you know.



STEVE:  Well, and of course that's where the whole AI thing comes in; right?  Because there's this notion of training and AI to be able to sound like Leo.



LEO:  I have to say, though, the giveaway will be the content, not so much my voice.  You could make something sound exactly like me.  I've got John Legend coming out of my Google Assistant you know.  That's one of the voices now you can use for your Google Assistant is John Legend.  And it sounds just like him.  But you can still tell, partly because of the content, partly because voice synthesis isn't perfect, that it's not actually John Legend.



STEVE:  Well, and good point.  For example, some of the spam email you get, you look at, and it's like, okay.



LEO:  Yeah.  That isn't a human.



STEVE:  This is not my mom who sent this.



LEO:  It's grammatically incorrect, you know.



STEVE:  Exactly.



LEO:  So I think we still have a way to go, even those deepfakes.  You've seen some of these deepfakes.



STEVE:  Oh, yeah.



LEO:  They're good, but they're not perfect.  You can tell.



STEVE:  Yes, yes.  



LEO:  I guess really what we have to prepare for is a day when they are perfect because photoshopping is completely, you know, it's very hard to detect.



STEVE:  That's a very good point.  And I think we have to acknowledge, and that's why it's worth talking about, that that day is coming.  And that we know there are entities that are willing to invest in that technology.  I mean, certainly academically, it's an interesting academic exercise to see whether you can create that kind of fake.  But, for example, the U.S. election, we know that Russia messed with our past presidential election in order to stir things up in various ways.  And there's every indication that, hey, they were successful, at least in creating some chaos.



LEO:  Thousands of Twitter bots.  A Twitter bot is easy, is an easy thing to fake.  And a Facebook bot.  That's not hard.



STEVE:  Yes.  So the second part of this is an interesting challenge that has just been announced.  It's called the Deepfake Detection Challenge, DeepfakeDetectionChallenge.ai.  I thought that was a very cool domain for it to have.  All one word, or all run together:  DeepfakeDetectionChallenge.ai.  The subhead reads:  "Deepfake Detection Challenge invites people around the world to build innovative new technologies that can help detect deepfakes" - good luck with that - "and tampered media.  Identifying tampered content," they write, "is technically challenging as deepfakes rapidly evolve, so we're working together to build better detection tools."  And of course this is going to be cat and mouse; right?  Because all of this is going to be done in public.  All of the detection stuff is going to be done in public.  The test sets will be public.  And so the people making the deepfakes will also be able to check their fakes against the detection systems.



Anyway, Facebook and Microsoft, in partnership with academics from Cornell Tech, MIT, University of Oxford, UC Berkeley, University of Maryland, College Park, and University of Albany, New York, have joined forces to sponsor a contest promoting research and development to combat deepfakes.  Videos altered through - well, videos because audio, if you can do video, you can obviously also do audio - altered through artificial intelligence to mislead their viewers.



So we have the DFDC, as it's been called or being called, the Deepfake Detection Challenge, which aims to spur the industry to create technology that can detect and prevent deepfakes, according to a blog post by Facebook's CTO, Mike Schroepfer.  Mike posted under a title "Creating a dataset and challenge for deepfakes."  So this is the Facebook CTO who said:  "Datasets and benchmarks have been some of the most effective tools to speed progress in AI.  Our current renaissance in deep learning has been fueled in part by the ImageNet benchmark.  Recent advances in natural language processing have been hastened by the GLUE and SuperGLUE benchmarks."



He says:  "Deepfake techniques, which present realistic AI-generated videos of real people doing and saying fictional things, have significant" - and you can imagine why Facebook is particularly sensitive to this now.



LEO:  Oh, yeah.



STEVE:  Uh-huh, "...have significant implications for determining the legitimacy of information presented online."  And of course, Leo, this is why it occurred to me that this also is a little relevant to the discussion we're going to be having in Boston on October 3rd.



LEO:  Oh, yeah.



STEVE:  Because it's about identity.  And of course this is spoofing identity all the way up to a video that is convincing.  So he says:  "Yet the industry doesn't have a great dataset or benchmark for detecting these fakes.  We want to catalyze more research and development in this area and ensure that there are better open source tools to detect deepfakes.  That's why Facebook, the Partnership on AI, Microsoft, and academics from" - and all those universities that I just mentioned - "are coming together to build the Deepfake Detection Challenge."



He says:  "The goal of the challenge is to provide technology that everyone can use to better detect when AI has been used to alter" - god, this will be really interesting - "to alter a video in order to mislead the viewer.  The Deepfake Detection Challenge will include a dataset and leaderboard, as well as grants and awards, to spur the industry" - and I saw elsewhere, oh, yeah, it's later in his posting - "to spur the industry to create new ways of detecting and preventing media manipulated via AI from being used to mislead others.  The governance of the challenge will be facilitated and overseen by the Partnership on AI's new Steering Committee on AI and Media Integrity, which is made up of a broad cross-sector coalition of organizations including Facebook, WITNESS, Microsoft, and others in civil society and the technology, media, and academic communities.



"It's important to have data that is freely available for the community to use, with clearly consenting participants and few restrictions on usage.  That's why Facebook is commissioning a realistic dataset that will use paid actors with the required consent obtained to contribute to the challenge.  No Facebook user data will be used in this dataset.  We are also funding research collaborations and prizes for the challenge to help encourage more participation.  In total, we are dedicating more than $10 million to fund this industry-wide effort.  To ensure the quality of the dataset and challenge parameters, they will initially be tested through a targeted technical working session this October at the International Conference on Computer Vision.



"The full dataset release and the DFDC launch will happen at the Conference on Neural Information Processing Systems (NeurIPS) this December.  Facebook will also enter the challenge, but not accept any financial prize.  Follow our website for regular updates.  This is a constant evolving problem," he finishes, "much like spam or other adversarial challenges.  And our hope is that by helping the industry and AI community come together, we can make faster progress."  So, wow.



LEO:  Interesting.



STEVE:  Yeah.  It is.  This is very cool.  It's clear that this is going to engage the interest of the industry's top AI academics and dynamic media people to see whether we can come up with some means for detecting this.  And of course it's also going to have an effect of providing feedback to the bad guys, who look at how this detection is being done and then see if they're not able to avoid being detected.



LEO:  You know, it strikes me that the human brain is marvelously attuned to detect fakes, to detect things that seem off.  And it's an evolutionary, I think an evolutionary ability because, if people are ill, you want to stay well away.  It's a survival instinct.  And so anything slightly off, that's the "uncanny valley," right, when we do animation, and we make it look very human.  We can tell it's not human.  There's just something subtle that maybe we're better at than any machine will be.  But what I was thinking is the real issue with deepfakes is not a three-minute video of the President saying we're going to attack Iran.  The real deepfake is a subtle, maybe a one-word change to an existing video.



STEVE:  Yes.



LEO:  That would be - you would be very hard pressed to detect that if it's done right.  I think we have the tools to do it now.



STEVE:  Yes.  Very, very, very, very good point, where you just make a...



LEO:  Just a little.



STEVE:  A subtle alteration that has dramatic magnifying, sort of multiplicative effect.



LEO:  Remember the video that the administration proposed when they banned Jim Acosta, the CNN reporter from the White House press briefings, which now of course they don't do anymore, so it doesn't matter, but they had a video of him pushing an aide away.  She was trying to take the microphone.  And it was a modified video of him pushing the aide away.  People figured out it was modified.  But it was such a subtle little thing.  And that's the key is, if you just make a little difference, no, that puck made it into the net.  See, right there.  That would be a lot harder, I think, to detect than a longer video.



STEVE:  Well, yes.  And even from an automated standpoint, so I think your point is not only for humans to detect, but for, like, I mean, AI is going to have to look at the whole thing, frame by frame.



LEO:  Yeah, what am I looking at, yeah, right.



STEVE:  Yeah.  Yeah, I would argue that - and I think you're right, Leo.  Humans, due to our nature, I mean, you know, for example, think about how like in poker there are tells.



LEO:  Oh, yeah.  Oh, yeah.



STEVE:  And you're able, like a good poker player is able to look at something that isn't fake, but it's just a tiny little tick that indicates something going on inside the person's head.  I mean, wow.



LEO:  We're very good.  That's our whole biology is tuned to do that.  And it's a little harder for a computer, which can't even really - a two year old can recognize his mother's face.  A computer has to work a lot harder.  It's a good challenge, though.



STEVE:  Well, and how many spouses are sure they can tell when their spouse is not telling them quite the whole truth. 



LEO:  Yeah.  That's exactly right.  That's exactly right.  We've never made a good lie detector.  There's never yet been a lie detector that actually works.  Better than a spouse, anyway.  Great subject.  I really look forward to - see, we're going to be doing this for a long time now.  Couple hundred more episodes.



STEVE:  That's right.



LEO:  By then we'll have lots more to talk about with deepfakes and everything else.  Poor Steve.  You're really getting to the end of the rope, aren't you.  You're starting to look ahead to 999.



STEVE:  I have a feeling maybe we're going to have to do some renumbering here somehow.



LEO:  Really?  Are you getting tired?



STEVE:  Ah.  What?  No.  No.



LEO:  No, come on.  You're going to have a rebellion on your hands if you say I'm quitting or retiring.  I do want to mention, you mentioned our October 3rd event that's coming up.  That's very exciting.  Steve will be relaxed because he'll just be back from a few weeks in Europe, right, as you're touring.  The SQRL Tour heads to Dublin and Copenhagen?



STEVE:  Yup, Dublin and then Gothenburg, Sweden.



LEO:  Goteborg in Sweden, yeah.



STEVE:  Yup, yup.



LEO:  So we'll be in Boston October 3rd, 3:45 p.m.  It's a summit.  I think it'll be a great panel discussion about this whole problem of authentication, how you prove who you are, including not just Steve.  I'll be there, the CISO Gerald Beuchelt from LogMeIn will be there, and the legendary Bill Cheswick (Ches) will be there.  And, I mean, we've got four people who have spent a lot of time thinking about what's next after passwords.  If you would like to go, it's free.  You do have to obviously be in Boston.  And if you're not going to be in Boston, never fret.  We're going to make a video and post it online.  It'll be a TWiT Special.



But if you want to get tickets, if you are going to be in Boston on October 3rd, go to twit.to, that's our URL shortener.  Not TWiT.tv, twit.to/unlocked, twit.to/unlocked.  You could fill out the form there.  I think there's still room.  We've had to expand the auditorium.  And LastPass, who's our sponsor, says they're willing to expand it to the point that they get too big for the Intercontinental Hotel, and at that point we'll have to stop.  But there are bigger ballrooms, and we're getting them.



What's really cool is it's a benefit.  It's a charity.  LastPass is donating $100 on behalf of each attendee to the Boys & Girls Club of Boston, the Greater Boston Food Bank, or KodeConnect, and you get to choose.  So this is a really neat event, and it's going to be a lot of fun.  There's a cocktail party afterwards.  I can't promise Steve will stick around, but maybe we can persuade him to.



STEVE:  Oh, I will for sure.



LEO:  If it helps, I'll bring a cardboard cutout of Captain Kirk and - too soon?



STEVE:  It'll always be too soon.



LEO:  Find out more.



STEVE:  I'd had nothing to drink, Leo.



LEO:  No, that's the funny thing.  He doesn't need it.  Twit.to/unlocked.  That's the short URL.  And there's still room; so, please, we'd love to see you on October 3rd in Boston.



This is an unusual record time because of tomorrow's Apple event.  Thank you, Steve, for being flexible.  We'll be back on Tuesdays, 1:30 Pacific - what?



STEVE:  We also have another unusual record time on Saturday.



LEO:  Oh, I forgot, yeah.  Because, again, this European tour, the grand tour that Steve's doing, we're going to record two episodes ahead of time.  This Saturday; right?



STEVE:  Yes.  It will be "The Joy of Sync."



LEO:  We're going to find out what Steve finally came up with in his quest for the perfect file sync solution.  I think this is - I can't wait for that.  So that's Security Now!.  That'll be a pre-record for the following week.  So, yeah, we're going to be all messed up, aren't we.



STEVE:  Yeah.



LEO:  So this Tuesday we're doing a show, 1:30 Pacific, 4:30 Eastern, 17:30 UTC.  Not this Tuesday, a week from tomorrow.



STEVE:  Correct.



LEO:  So confusing.  So September 17th.



STEVE:  Because we've got Apple Day tomorrow.



LEO:  Yeah, Apple Day tomorrow.  September 17th we'll be back on schedule.  But September 21st we'll be recording for September 24th, that's a Saturday afternoon right after the radio show.  And then you'll be gone for a couple of weeks.  Is that right?



STEVE:  Yes.  Correct.



LEO:  All right.  All right.



STEVE:  And then Episode 734 is where we will air the "Joy of Sync" episode that we record this first Saturday.



LEO:  This Saturday, yeah.



STEVE:  Yes.



LEO:  So I'll be spending a lot of time with you over the next two weeks.



STEVE:  That's going to be great.



LEO:  September 14th, this Saturday, we'll do another one.  That'll be for 10/1.  That'll be for October 1st.  And that'll be a lot of fun.



STEVE:  Exactly.  Exactly.



LEO:  I'm sure I've just confused the heck out of everybody.  We'll be here a week from Tuesday and for the next two Saturdays.



STEVE:  And if they subscribe to the podcast...



LEO:  That's true.  Don't have to worry.



STEVE:  Never fear, they'll get them all.



LEO:  It'll all be the same, just click the button at TWiT.tv/sn, subscribe to the podcast.  Steve, of course, has copies of every show, 16Kb for the bandwidth impaired, 64Kb for those of you who like the full sound.  And of course very handy, the transcripts that Elaine Farris does for each and every episode.  Those are all at GRC.com.  While you're there, pick up a copy of SpinRite, Steve's bread and butter and the world's best hard drive maintenance and recovery utility.  There's lots of great stuff, too.  Just, you know, if you're going to GRC.com, block out some time.  Get some tabs ready because there's a lot of great stuff there.  You might want to turn on that UnloadTabs out-of-memory thing before you do that because there's so much good stuff there.



We have it on our site, as I mentioned, TWiT.tv/sn.  And if you subscribe, then you don't have to worry about anything.  It'll just automatically come to you whenever it's done.  Thank you, Steve.  Have a great week.  I'll see you a week from tomorrow.



STEVE:  Right-o.



LEO:  I'll see you Saturday.



STEVE:  Actually you will, yes.  I was just going to go with it because you'll also see me a week from tomorrow.



LEO:  It's so confusing.  I'll see you Saturday.



STEVE:  Okay, buddy.



LEO:  Take care, Steve.



Copyright (c) 2019 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#732

DATE:		September 17, 2019

TITLE:		Simjacking

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-732.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we continue following the DoH story, which we begin discussing two weeks from now as a result of a rip in the space-time continuum.  We also look at recent changes to Chrome 77 and the forthcoming Chrome 78, the already compromised iOS 13.0, and Mozilla Firefox's new browser VPN offering.  We take a look back at last Tuesday's Patch Tuesday, take note of Chrome's Remote Desktop feature, cover another serious Exim mail server problem, handle a bit of miscellany, and examine a serious vulnerability affecting essentially ALL smartphone users known as "Simjacker." 



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  We have a catch-up on DNS over HTTP.  We talk about some great new features coming to Chrome and Firefox.  Steve sings the praises of the Chrome Remote Desktop.  And then hold onto your hats because there's bad news about simjacking.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 732, recorded Tuesday, September 17th, 2019:  Simjacking.



It's time for Security Now!, the show where we talk about the latest in security and privacy and keeping yourself safe online with this guy right here, Steve Gibson, in the house.  Hello, Steve.



STEVE GIBSON:  Yo, Leo.



LEO:  We've done a lot of Security Nows, it feels like, in the last few days.



STEVE:  It has.  And in fact I made note of that.  For this week I said we continue following the DoH story, which we begin discussing two weeks from now in the future, as a result of a rip in the space-time continuum.



LEO:  Exactly.



STEVE:  We also look at recent changes to Chrome 77 and the forthcoming Chrome 78; the already compromised, though not yet released, iOS 13.0; Mozilla Firefox's new browser VPN offering; and a look back at last Tuesday's Patch Tuesday.  We take note of Chrome's remote desktop feature, which I just discovered, cover another serious Exim email server problem, handle a bit of miscellany, and then conclude with an examination of a serious vulnerability affecting essentially all smartphone users, known as simjacking.



LEO:  Oh, man.  This has been a big problem.



STEVE:  Yeah.  It got a lot of coverage in the tech press.  And I had to, like, I had no idea that a SIM could be jacked.  I just figured it was a bit of - I thought it was like a bit of ROM or something.  But it turns out there's a browser in your SIM.  It's like, what?



LEO:  What?



STEVE:  Yes.  And so the good news is that firewalls, cellular carrier firewalls can be erected to stop this.  But at the moment, there isn't any.



LEO:  They're not doing it, yeah.



STEVE:  And it's possible for bad guys to send you an SMS message which jacks your SIM and can take over your phone.  And it's like, because it's down at the SIM level, it's carrier agnostic.  It's phone source agnostic.  It's, if your phone has a SIM in there, and they all do, you're in trouble.  I mean, but not...



LEO:  Even an eSIM, even an eSIM would be vulnerable to this.



STEVE:  Yes.  And it's going to be targeted attacks.  It's not like you're going to get sprayed with it.  But for people who are subject to targeting, this is a problem.  So anyway, we have lots of stuff to talk about.



LEO:  I can't wait, as always.  Security Now!, we look forward to it every week.  A reminder, Steve and I are going to be in Boston.  That's why we were pre-recording our October 1st episode.  On October 3rd we will be doing an event on behalf of LastPass.  It's exciting.  It's an event about the future of authentication.  It'll be Steve.  It'll be the legendary William Cheswick, Bill Cheswick, who actually created the first firewall at Bell Labs and has written a lot about security and has recently written about passwords and why they're bad.  We'll also have Gerry Beuchelt, who is the CISO of LogMeIn.



It's going to be a fantastic panel event.  It is absolutely free.  If you're in Boston on that Thursday afternoon, October 3rd, you can go to our website, twit.to, that's the URL shortener, twit.to/unlocked.  We have, I think, some room left.  What's happened is that they've slowly expanded the venue.  It was initially 100 people, then 200.  I think we're up to 350.  I think they're going to have to go to five or 600.  But we're slowly expanding the venue to accommodate at the Intercontinental Hotel in Boston.  If you will be in the Boston area on October 3rd, twit.to/unlocked.  It's free.



It's actually for charity.  Everybody who attends will get a $100 token they can donate to one of LogMeIn's three choices of charity.  So that's going to be really nice, too.  You get to put your token in a charity of choice.  And I can't wait to do that with you, Steve.  It's going to be a lot of fun.



STEVE:  We'll have a ball.



LEO:  Yeah.  I mean, eventually we will sell out.  This may be the last round of announcements.  So don't delay.  Let's get going, Steve.  We've got a picture.  We've got a picture.



STEVE:  I've been sitting on this one for a while.  I just get a kick out of it.  So this is from the iconic sci-fi movie "The Terminator" with Arnold the robot sent back in time from the future, who's at a phone booth looking up the...



LEO:  You remember he was looking for Sarah Connor.  He had to find her.



STEVE:  Exactly.



LEO:  He took the phone book.



STEVE:  And so there's a scowl on his face down at the bottom frame because in the phone book Sarah Connor's name has been distorted with a line running through it to create a CAPTCHA.



LEO:  I am a robot.  I cannot read it.  What have they done?



STEVE:  Anyway.  Just a little bit of geeky security humor.



LEO:  I don't think these CAPTCHAs fool anybody, especially not robots.



STEVE:  No.  They fool us more than they fool, you know, robots.



LEO:  Yes.  They're hard for humans.



STEVE:  I'm looking at them going, I have no idea what that thing says.  Just give me another one, please.



LEO:  Yes.



STEVE:  So Chrome follows Mozilla to DoH with a bit of a twist.  Google has announced that they, too, will soon be performing a trial of DNS over HTTPS, DoH, in the upcoming Chrome beta 78, which will be releasing this Thursday, September 19th.  We're currently at Chrome 77.  What's interesting is that, rather than having Chrome preconfigured with a default DoH server like their own, Google will instead attempt to preserve whatever DNS the user already has chosen.  And I love that idea.  I think that's very clever, and that it would be really cool if they were to probe the user's currently selected DNS server to see if it was offering DoH support, test it locally, then switch to it.



But apparently that's a bit too aggressive, at least initially.  So what they'll be doing is only if the user has already configured their DNS to one of six providers:  CleanBrowsing,  Cloudflare, DNS.SB, Google, OpenDNS, or Quad9.  On the other hand, those are high-reputation, well-known services, and I imagine a lot of our listeners probably have done that.



So initially, for a small group of users running Chrome 78 beta, Google will be running an experiment that checks to see if their provider is on that short list of well-known DoH-compatible providers.  And if the user's provider is, Chrome will automatically upgrade to that provider's DoH server for its DNS resolution.  And if they're not already using one of those servers, nothing will change.  So this will affect all platforms of Chrome other than Linux and iOS.  And on Android 9 and later, if a user has already configured a DNS over TLS provider, Chrome will use that instead of the ones that are listed.



So by cleverly leaving the DNS provider as is, and only upgrading to the provider's equivalent DoH service, what I like about this is that the user experience should remain the same.  For instance, any malware site protections or parental control features that are offered by the DNS provider, which presumably the user has chosen for that reason and enabled and configured, those would continue to work.  If DoH fails, then Chrome will revert to the provider's regular DNS service, that is, not try to do DoH.  And any of those early adopters will be able to opt out of this with a flag, chrome://flags/#dns-over-https.  That'll bring you to a flag which you can turn off if you don't want it.



And on the Mozilla side - and Leo, thanks to the time machine we used last Saturday to record podcast 734, I happen to know that two weeks from now on our "Joy of Sync" podcast #734, we'll be discussing Mozilla's own move to begin experimenting with enabling DoH by default for their users.  But it turns out that, as news of Mozilla's plans spread, which by the way I was unaware of two weeks from now due to a temporal paradox, Mozilla will have since received some pushback from Linux distro maintainers and some network admins.



LEO:  Oh, really.  Oh, why?



STEVE:  Yes.  In an example quoted by BleepingComputer, OpenBSD developer Peter Hessler tweeted that OpenBSD has disabled DoH in their Firefox package in the current and future releases as, and this is what he said:  "Sending all DNS traffic to Cloudflare by default is not a good idea."  So people are objecting to Mozilla's sort of default focus on a single DoH provider.



LEO:  Actually, that makes sense.  If it's default, and it's defaulting to Cloudflare, that does make sense.  It should, yeah.



STEVE:  Yes, yes.



LEO:  You should have to turn it on.



STEVE:  And that's what is cool about what Google did, was they're trying to honor the user's existing override, if any, rather than just saying we're going to send everything to Cloudflare.  Kristian Khntopp, who's a senior scalability engineer, stated that Mozilla is about to - and this is a little extreme, maybe - he says "about to break DNS" because Cloudflare will be used for DNS resolution over what was assigned by the system administrator, which of course is a concern.  And he felt that this would leak the names of all the websites visited in a corporate environment to Cloudflare.



So, you know, the Internet purists are a little annoyed by this.  But users are saying, hey, you know, we would prefer not to have our ISP and anybody else on the wire looking at everything, like using DNS to determine everything we're doing.  And it turns out that, in the future, in two weeks from now, Leo, we note that, well, yes.  But, you know, our IP address, the actual IP traffic is still somewhat of a giveaway, with the understanding that it doesn't disambiguate multiple homed sites at a given IP.  But still.  So anyway, some interesting movement.



And ZDNet, I picked up on a tip from them that I thought was interesting.  For any of our listeners using Chrome now, who have an interest in enabling DoH today, it turns out that that's actually supported.  Chrome lacks any user interface for configuring this, and you really are going to want to look at the show notes for this.  It turns out that Chrome dutifully obeys launch time startup parameters, which can be added to the shortcut that you use to start Chrome.



So, for example, in Windows you would modify the startup link to add a bunch of command parameters.  There's --enable-features, and then the feature is "dns-over-https<DoHTrial."  That was one parameter.  The next one, --force-fieldtrials, and then that one takes a "DoHTrial/Group1."  And then --force-fieldtrial-params, and here's where you specify deliberately what you want to use as the DoH server.  And in this case, in the example that ZDNet had, they were using 1.1.1.1 with some parameters.



So again, the show notes have the details for anyone's who's interested.  And you can then go to, for example, 1.1.1.1/help, and you'll get a status screen to confirm that your browser is resolving through DNS over HTTPS.  So anyway, just a cool little tip from ZDNet for our users who are really wanting to operate on the bleeding edge.  Although, you know, DoH works.



In other news, there is no waiting to experience Chrome's deprecation of all obvious display of EV certs.  We've talked about this coming.  Basically, sadly, the death, the probable death of EV certs because really, if the browser's not giving the user any affirmative obvious indication that you are at a site who has paid extra for extended validation - as they did for a while.  You got the nice big, it was like in green, Gibson Research Corporation, woohoo.  And that's gone.  So I have in the show notes two screenshots showing GRC.com and Wikipedia.org.  GRC.com is EV.  No indication.  Oh, and by the way, www dot for both GRC and Wikipedia are gone.  And so Chrome is back to doing that again.  They decided no one cares about www, even though, okay, well, it's there in the URL.



LEO:  Yeah, and there are a lot of links; right?  But it still works.  The links will work; right?



STEVE:  Yes.  The links work.  It just doesn't...



LEO:  Doesn't show it.



STEVE:  I guess the argument is people don't care.



LEO:  Right.



STEVE:  And you do see, for example, GRC.com/intro.htm where that's now - the tail is a dimmer gray, whereas GRC.com is strong.  So they're sort of trying to say, you know, this is where you are.  But they're just, like, for www it's like, no one cares.  Okay.  Anyway, so I clicked on the Show Me More to get the dropdown.  And so this is now the only indication, which for all intents and purposes who even knows.  You can see that I clicked on the Tell Me More for GRC.  And under the certificate, which it shows as valid, it says "Issued to: Gibson Research Corporation."  So that versus Wikipedia, where Wikipedia, that's not an EV cert, just says "Certificate Valid," and nothing else.



So given the fact that non-EV certs have a longer life compared to EV - I think EV's limited to two years; non-EV can be three.  And if the browsers aren't showing you any benefit, it's like, that's going to be hard to justify the additional cost and the fact that you've got to do that every two years versus getting an extra year of time.



So that's in Chrome 77 now, and I think it really represents a death knell for extended validation.  Which is, you know, unfortunate because you did have to jump through additional hoops, although there have been arguments that, for example, there could be a Gibson Research Corporation that was incorporated in a different state.  And so that's not going to, you know, it's going to confuse people.  Wait a minute.  I thought I was at GRC.  And now it says Gibson Research Corporation, but it's some other domain name.  So I guess I can understand that.



LEO:  So this is Firefox, and it does show the owner if you have an extended cert.  But if you don't, like Wikipedia, it just says this website does not supply ownership information.



STEVE:  Ah.  So Firefox is...



LEO:  In Firefox, yeah.



STEVE:  Ah, there it is, nice big green, yup.



LEO:  Right.



STEVE:  So it'll be interesting to see if Mozilla follows suit, you know, what their position is on this.  But, yeah, it's over as far as Chrome goes.  And of course Chrome is, as we know, the majority browser on the 'Net.



LEO:  Right.



STEVE:  So lucky iOS 13.  We're getting iOS 13 in two days, on Thursday, September 19th.  And frankly, I'm excited for one feature in particular.  I love swipey phone keyboards or tablet keyboards.  I really do.  But I've never been happy with any of the third-party iOS add-on keyboards.  I've tried them all - Gboard, Swype, SwiftKey, and their ilk.  And I found that they all misbehave in various ways, most often by failing to deploy when they are needed.  You know, you'll sometimes come to a form, and you tap in the field, and sometimes the bottom will blank out, and it's like, hello.  I need a keyboard here.  Nothing.



So anyway, iOS 13 finally has a swipey keyboard built-in.  And I say yay because I would just love to stay with the one that will hopefully deploy when it's supposed to, unlike any of the - like all of those third-party keyboards don't.



LEO:  I predict, in a future episode, you will also, whether we've done it or not, I don't know, but you will also find praise for iOS 13's privacy measures.  They really now, recognizing that the biggest problem with iOS is the third-party apps, Facebook can lock everything down.  But if Facebook sucks, you know, it sucks.  So in fact, if you when you first...



STEVE:  Sucks your data.



LEO:  Sucks your data.  When you put iOS 13 on, all of a sudden you're going to get a cascade of warnings from Facebook saying, hey, they're looking at your location.  Here's the map.  You know, I think this is really good.  Apple is aggressively...



STEVE:  That's going to force behavior on other sites, essentially.



LEO:  Well, what it forced Facebook to do is already publish an article saying here's why we really should know where you are.  This is - please don't.  And it even says, you know, we recommend you don't - you always allow us to look at your location.  We strongly recommend that.  And, yeah, get ready.



STEVE:  And I heard Rene mention that Pokmon, like he gets a popup every week to say...



LEO:  He wants - yeah.



STEVE:  ...oh, by the way, it's still got your location information.  I think that kind of a persistent reminder is very cool.



LEO:  Basically, and Steve Jobs said this years ago, he said, "Let them know.  Let them know again.  Make them tell you to stop letting me know because people" - and Rene's concern, which is legitimate, is that people might just go, yeah, yeah, yeah, yeah, yeah.  I don't think so.  I've already experienced this.  I've been using the beta for some time.  It is a great - I really appreciate it.  And I always say do not allow unless, you know, usually if it's a mapping app I just say don't allow unless the app's open.  If I've opened the app, yeah, you've got to look at where you are.  There's no reason that map app needs to know where I am if it's not running.



STEVE:  Right.



LEO:  Or Facebook or anything else.  So I think it's...



STEVE:  Right.  And especially if these things are power consumers also.



LEO:  Yes.



STEVE:  If it's, like, draining your battery because it wants to keep, like, an eye on you, it's like, no.  Go away.



LEO:  No.  Go away.  Yeah.



STEVE:  So you've been using the beta.  It has a swipey keyboard; right?



LEO:  I actually haven't tried it.  I should try it.



STEVE:  Oh, Leo.  Okay, well, I'm dying.  I got, I mean, for me it's just, like, such a convenience to have...



LEO:  Oh, I far prefer swipe, yes, absolutely.



STEVE:  Yeah.



LEO:  And of course I use, on my Android devices, I use it all the time because the default keyboard always on Android has it.  I don't know...



STEVE:  Yeah.



LEO:  Let me start typing something and see.



STEVE:  While I tell our users why we won't be using 13.0 for long.



LEO:  Yeah. 



STEVE:  It turns out that we will be jumping to 13.1 very soon.



LEO:  On the 30th, yeah.



STEVE:  After the release, yes, of 13.0, since a headline-grabbing lockscreen bypass bug is already...



LEO:  Oh, that's why.  Oh.



STEVE:  ...known to exist.  And it still exists in the Golden Master version of iOS 13 that has already been loaded into the many hundreds of thousands of iPhones in shipping containers out there on the high seas.



LEO:  Ah.  Now I understand.  Ah.



STEVE:  Yup.  iOS 13 contains a vulnerability that allows anyone to bypass the lockscreen protection to access sensitive information on that user's phone.



LEO:  This has historically been a problem on Apple.  For years.



STEVE:  Yes.  Well, and you know, it's because there are just so many of those accessibility and Siri and convenience features, they just keep finding a way around.  In fact, this guy, Jose Rodriguez, he found the one that was the bypass, I think it was in 12.1.  So he's revealed that he discovered a lockscreen bypass bug in 13 that allowed him to access the full list of contacts on his iPhone and every piece of information saved within them.  Jose discovered the newly introduced lockscreen bypass bug on his own iPhone while he was running the iOS 13 beta and reported it immediately to Apple two months ago, actually exactly two months ago, on July 17th.



However, even that was apparently too late for Apple to do anything about it.  They had already got their supply chain ramped up and were stamping out iOS 13 into all the phones that they were prepping for the big release.  So that bypass remains working in the Golden Master version of iOS 13, which we all get in two days, even those of us who aren't jumping on new pads or phones, and I'm not because, I mean, I did watch all of your coverage on it last Tuesday, and it's like, okay.  I'm not a huge camera person.  But I definitely want iOS 13.



So we'll be able to get iOS 13 in two days.  The lockscreen bug is like those we've seen before, where someone having physical access to a targeted iPhone is able to trick the phone into granting them access to the full list of stored contacts, as well as detailed information for each individual contact, including names, phone numbers, emails, and so forth, using a FaceTime call.  This is also similar to the same one that Jose discovered last year in iOS 12.1, just a few hours after Apple released 12.1.  It allowed anyone to bypass the phone's lockscreen using the built-in voiceover feature.  So this bug requires activating a FaceTime call on the target iPhone and then accessing Siri's voiceover support feature to obtain access to the contact list and all the information saved there.



However, the problem won't exist for long, as it is very much expected to be patched in 13.1, which is expected to begin trickling out to the public 11 days later, on Monday, September 30th.  So if this really worries you, you could disable automatic updates until October 1st and jump right over from 12.4.1, where we are today, over to 13.1, and skip that.  But I'm not worried about it.  Besides, someone has to physically have your phone in order to get to your contacts.



LEO:  And there's a not-insignificant amount of fiddling they have to do.  So they'd have to have your phone.



STEVE:  Yes, that's true.



LEO:  Without you looking at them for some, you know, a minute or two.  Because we just ran the video.  It's a lot of steps.  It's not an easy - yeah.



STEVE:  Yeah, yeah.  And Leo, in another of those bizarre time travel paradoxes, in the future, two weeks from now, during Episode 734, I have it on very good authority that I will mention that Cloudflare is launching a mobile device-oriented VPN, whereupon you inform me of the just breaking news from the  past that Mozilla is also launching a privacy-focused VPN service.



LEO:  Yeah.



STEVE:  That takes me by surprise, once again, because we've been messing around with the space-time continuum for the benefit of our faithful listeners.  You know we'll do anything for our listeners.



LEO:  Yes.



STEVE:  So due to the time warp, today, two weeks earlier than then, I am now fully up to speed...



LEO:  Oh, good.



STEVE:  ...on Mozilla's announcement, even though I'll know nothing about it two weeks from now ago.



LEO:  That's because I'm about to use the neuralyzer on Steve, and he's going to forget everything.



STEVE:  That's right.



LEO:  Yes.



STEVE:  In any event, Mozilla has indeed officially launched a new privacy-focused VPN service called Firefox Private Network.  It runs as a browser extension to encrypt all of a Firefox user's online activity and limiting what websites and advertisers know about Firefox users, that is, those who might have been watching.  The Firefox Private Network service is currently undergoing beta testing and is available only to desktop users in the U.S. as part of Mozilla's recently reborn Firefox Test Pilot program that lets users try out new experimental features before they're officially released.  The Firefox Test Pilot program we talked about a long time ago.  It was initially launched by Mozilla three years ago, but was shut down at the beginning of this year.



Anyway, Mozilla has decided to bring it back in an updated and changed fashion.  Marissa Wood, the vice president of product development at Mozilla, said:  "The difference with the newly relaunched Test Pilot program is that these products and services may be outside the Firefox browser and will be far more polished and just a step shy of being ready for general public release."  So this newly announced Firefox Private Network is part of this relaunched Test Pilot program's first new project.  And as we would expect from any VPN, the Firefox Private Network masks its users' IP address from third-party trackers and protects sensitive information like the websites you visit and your financial information, when using public WiFi.



However, it's important to note, of course, that all by itself it's not offering any anti-tracking protection since these days that's primarily done from within the browser and is not dependent upon IP addresses which change as mobile users switch hotspots, cellular regions, and jump between home and office and so forth.  And as you will note in the future, Leo, this is only a benefit for the browser session, not your mobile platform or your desktop in general.



LEO:  Future Leo is so smart.



STEVE:  You were right on the ball.  Just the Carnac of TWiT Network.



LEO:  I foresee.



STEVE:  So Mozilla says its Firefox Private Network "provides a secure encrypted path to the web to protect your connection and your personal information anywhere and everywhere you use your Firefox browser."  So my feeling is a built-in facility which is easy to use and provides useful VPN services to many people who might not otherwise go to all the trouble to set up a VPN, you know, seems like a good idea.



It, as we noted, encrypts and tunnels Internet browsing activity, but only your browser activity, through a collection of remote proxy servers which thereby mask the user's actual location and block third parties like your ISP or your hotspot provider, anybody who might be sniffing on you, including the government, for example, from snooping on your browser traffic, at least until it emerges from the other end of the VPN connection.  Now, interestingly, the proxy servers used by Firefox Private Network are also provided by Cloudflare.  So it looks like Mozilla and Cloudflare...



LEO:  Wow.



STEVE:  Yeah, have agreed to provide strong privacy controls to limit what data Cloudflare may collect and for how long it may store any data.  We've often on this podcast, and I heard you talking about it just the other day, about a VPN, we've talked about VPN services.  And one of the many issues is whether they log; and, if so, what log retention policies they follow.



LEO:  Right.



STEVE:  Cloudflare has stated:  "Cloudflare only observes a limited amount of data about the HTTP/HTTPS requests that are sent to the Cloudflare proxy via browsers with an active Mozilla extension.  When requests are sent to the Cloudflare proxy, Cloudflare will observe your IP address, the IP address for the Internet property you are accessing, source port, destination port, timestamp, and a token provided by Mozilla that indicates that you are a Firefox Private Network user.  We call this 'Proxy Data.'  All proxy data will be deleted within 24 hours."  So they're being very clear about what they are observing, collecting, and what their retention policy is.



So anyway, anyone who's interested, our listeners, again, only on desktop Firefox.  It is slated to be available to mobile Firefox users, as well, once the VPN exits its beta stage.  And although it's currently free, Mozilla has hinted that the company is exploring the possibility of adding value-added commercial service pricing options in the future so that it's able to be a self-sustaining service.



Anybody who has a Firefox account, for example, you use that in order to sync Firefox shortcuts and favorites and links and things between browsers.  If you go to https://private-network.firefox.com, you can sign up and join the beta, and then all of your Firefox desktop browsing usage gets proxied by Cloudflare, and you are in a secure browser tunnel for all of the browser stuff you do, which I think seems like a good idea.



So last Tuesday was September's Patch Tuesday.



LEO:  Yes.



STEVE:  And it did not disappoint.



LEO:  Well, depends on what you were hoping for.



STEVE:  That's true.



LEO:  There was a lot there.



STEVE:  It provided fixes for a whopping 79 vulnerabilities.



LEO:  Geez.



STEVE:  Remember when it was 16, Leo?  Remember those quaint days of yesteryear?



LEO:  It's not getting better, is it.



STEVE:  No, it's really not.  And 17 of those 79 vulnerabilities were considered to be critical.  Among the many, we received a further fix for last month's very worrisome CTF flaws which were discovered and explored in excruciating detail by Google's Tavis Ormandy.  As we'll recall, Tavis discovered how unprivileged attackers could launch their attack code with elevated privileges by leveraging this CTF client and server system in Windows.  And we've long since been disabused of the notion that elevation of privilege is nothing to worry about because it can, as we have seen, really be leveraged to the advantage of attackers.  So during the previous Patch Tuesday in August, Microsoft dealt with one of the related vulnerabilities, CTF vulnerabilities.  But at the time, they indicated that there was still more to come.



So as part of this month's offerings, Microsoft has released another fix for this range of flaws titled "Windows Text Service Framework Elevation of Privilege Vulnerability," and addressed another one.  Quoting Microsoft, they said:  "An elevation of privilege vulnerability exists in Windows Text Service Framework when the TSF server process does not validate the source of input or commands it receives."  And that is, of course, what Tavis found, although he worked on it, I think, for a month.



"An attacker who successfully exploited this vulnerability could inject commands or read input sent through a malicious Input Method Editor (IME).  This only affects systems that have installed an IME.  To exploit this vulnerability, an attacker would first have to log onto the system.  An attacker could then run a specially crafted application that could exploit the vulnerability to take control of an affected system."  And you never want attackers to take control of your system.  You're not going to have a good day.  "The security update addresses this vulnerability by correcting how the TSF server and client validate input from each other."



So as we know, the other continuously troubled area of Windows recently has been Remote Desktop.  So also in this month's patch batch we get four more fixes to the RDP's client-side fix.  Remember that we saw this also the previous month.  Microsoft was clearly looking at this code, the RDP code, not only on the server side, which has been a victim of exploitation recently, but also on the client side.  And they have found it wanting.  So there are four more fixes of what is really not such a big problem which Microsoft has discovered.  But now that they're looking at it and giving it the focus of their attention, they're finding more things to be fixed.



I consider it really not such a big problem because being on the client side, this would only affect people who connected their Remote Desktop Protocol client to a malicious server.  And I think most people are connecting them to their home base, which is probably not malicious.  If it is, you probably have bigger problems.



Anyway, Microsoft again said:  "Remote code execution vulnerabilities exist in the Windows Remote Desktop Client when a user connects to a malicious server.  An attacker who successfully exploited this vulnerability could execute arbitrary code on the computer of the connecting client.  An attacker could then install programs; view, change, or delete data; or create new accounts with full user rights."  And, you know, blah blah blah, you'd have to connect to a bad server, or a man in the middle could also interject traffic that would cause a problem.  So that was fixed last week.



Oh, we should also note that Microsoft has said that three of those four vulnerabilities have been publicly disclosed, and two of them have known exploits.  So hopefully by now, since that was last Tuesday, everybody has run themselves an update and reboot cycle on their important machines and has got those fixed, even though I don't think it's that big a problem to start with.



One of those 17 critical vulnerabilities which was also fixed this month is a VBScript remote code execution vulnerability.  So it's just as well, as we have previously noted, that Microsoft will be throwing in the towel on VBScript in favor of the now industry-wide standard ECMAScript, also known as JavaScript, since as we all also too well know, legacy usage and dependency will continue for VBScript forever.  I'm sure there are corporations that have written gobs of VBScript stuff, and they need it to, even though it has no future, they need it to stay around.  And so it's a good thing that Microsoft is continuing to fix it.



And Microsoft said:  "A remote code execution vulnerability exists in the way VBScript engine handles objects in memory.  The vulnerability could corrupt memory in such a way that an attacker could execute arbitrary code in the context of the current user.  An attacker who successfully exploited this vulnerability could gain the same user rights as the current user," blah blah blah.



So hopefully enterprise use is the only place we will see this, and that it will disappear from the public Internet, because at some point you're probably going to want to disable VBScript usage out on the public Internet.  I mean, it's good that these things are being patched.  But, you know, this has always been there, and you don't want to have your system taken over remotely by visiting a site that hosts a malicious ad because an advertisement that is being served from an ad rotating service could run script on your browser and do bad stuff to you.  So not good.



And I just wanted to take a moment to make sure everybody knows about something I was completely unaware of.  And Leo, I'm sure you know about this because apparently it's been around for a couple years.



LEO:  Well, also I live in the future.



STEVE:  Oh, you have that benefit, don't you.  You're bringing us wisdom from the future.



LEO:  Yes.  Yes, that's it.



STEVE:  I, however, code in assembly language, so I am stuck in the past.



LEO:  Oh, my god, are you.



STEVE:  Chrome Remote Desktop.



LEO:  Yeah.



STEVE:  Oh, my god.



LEO:  Really.  You're that excited, huh?



STEVE:  Well, it's just so simple.



LEO:  Right.



STEVE:  It's so easy.



LEO:  Right.  I think they did it for Chromebooks initially because it just, you know, you kind of need it on a Chromebook.



STEVE:  Right.  Well, in fact you could just consider that to be a remote desktop client, where you don't actually load any apps there, you just run things on a desktop at home.



LEO:  Precisely, yeah.



STEVE:  So my beloved Lorrie had a need to work with a client of hers who was just really unskilled with computers.  She was doing remote neurofeedback...



LEO:  Oh, wow.



STEVE:  ...and needed to set up this client's computer.  And, I mean, there was just, like, she had to get onto her desktop.  And so I looked.  I said, oh, yeah, no problem.  Windows, it's built into Windows.



LEO:  It's built in, RDP, yeah.



STEVE:  Yeah.  And there is that, what is it, there is Remote Desktop Protocol, but there's Remote Assistance.



LEO:  Right.



STEVE:  And so I showed her how that worked.  And she, like Lorrie was saying, oh, you know, honey, that's...



LEO:  This isn't going to work.



STEVE:  That's going to be too much.  And so there was like, you know, a bunch of commercial services that were doing it.  But they were like, you know, the good ones weren't free, and blah blah blah.  And finally I stumbled on remotedesktop.google.com.  And it's like, oh, my god.  And so, I mean, so it's like - anyway, I just wanted to make sure, I wanted to bring it to our listeners' attention.  I'm sure that our listeners are from time to time having to help somebody that really, you know, just like they should not have a computer.  But they have one anyway.



And so, wow.  In fact, I don't even think you need Chrome.  You originally did.  But now you need - they have revamped it.  You need a browser that supports WebRTC.  And maybe that's too - I think that you need WebRTC to be on the controller side.  I think you probably still need Chrome, the Chrome browser, to be on the side which is taken over.  So but, you know, again, Chrome is the majority browser on the Internet, and you just go to there, and you click a button, and it downloads an extension to Chrome, and you can then view their desktop.  And, wow.  So anyway, just a little heads-up because, boy, I mean, in terms of a slam dunk for helping somebody who really has a hard time pushing buttons, like do I click once or twice - well, actually that is a question for the ages - it's just a win.  So, yay.



LEO:  Yeah.  And it's free.



STEVE:  Yeah.  Yes, exactly.  It's free.  And works through NAT and, I mean, you know, because everybody...



LEO:  Yeah, does NAT traversal, yeah, yeah, that's right, yeah, yeah.



STEVE:  NAT traversal.  And I was just - I was very impressed with this thing.



LEO:  Works with Firefox.  At least that's - I just set it up, yeah.



STEVE:  Yes, yes.  That is exactly what I was going to say.  And so Firefox in addition.  And of course that also probably means it will work with the new Edge, if it doesn't work with the old Edge.  I imagine it probably does work with the old Edge.  But we know it will work with the Edge based on Chromium.  Chredge or something?  What is it you and Mary Jo and Paul were calling the...



LEO:  Chredge, yes, Chromium Edge, Chredge. 



STEVE:  Chredge, Chredge.



LEO:  WebRTC is such a great standard.  You can use it for video calls, for phone calls.



STEVE:  Yup.



LEO:  Really, this is really - it's working well.  I think it's great.  It's browser-based.



STEVE:  Yeah.  It's really moving, moving to the future.  Where you have been for quite some time, Leo.



LEO:  I'm glad you finally got here.



STEVE:  Yeah.  So Exim.  Exim email servers are in trouble again.  I wanted to make sure - this news is a little old because I just - it had been on my list of things to get to, and I haven't been able to get to it until now.  But it's important.



We first talked about them several months ago.  This was that bizarre, it takes a week to exploit the server thing.  Remember, Leo, where you would be a client of a vulnerable server, and they were all vulnerable, and you'd basically create a bogus email and then send them a byte a minute so that the connection wouldn't time out, and then something would get tired after a week of waiting for this thing, this final email message to get sent.  It wouldn't happen.



So then it would tend to - it would try to send you a bounce message.  But in the envelope that you had sent, you had put shell commands, you know, exec shell commands in, like, the mail bounce or the reply-to, whatever it was, which this Exim server would execute.  So even though you had to be patient, you would be able to execute as root any commands which you had stuck into the email envelope and then waited.  Which I argued a worm would be able to take great advantage of.  And in fact there were worms that were doing this, as it turns out, not surprisingly.



Well, unfortunately, Exim is back again with another problem.  This one is way worse.  The Exim maintainers have released Exim v4.92.2 after publishing an early warning two days beforehand to give sysops an early heads-up that they would really need to patch this as soon as this went public because they knew that bad guys were going to jump on it.  And it affects all email server versions up to and including its immediate predecessor, 4.92.1.  And just to remind everyone why this matters, Exim is a widely used, open source, mail transfer agent for all Unix-like operating systems - Linux, macOS, Solaris, et cetera.  And it is currently behind nearly 60% of the Internet's email servers today.  So it's the majority email transfer agent.



This new vulnerability is CVE-2019-15846, affecting, as I mentioned, all Exim servers previous to the one that was patched only recently which accept TLS connections.  And of course that's now considered best practice.  GRC, you know, accepts TLS connections.  But the fault that was discovered allows attackers to obtain root-level access to the system by sending an SNI, that's the Server Name Identification, ending in a backslash null sequence during the initial TLS handshake.  Whoopsie. 



As we know, SNI is an extension to the TLS protocol which allows servers to host multiple TLS certificates on a single IP.  It allows a connecting client to tell the server in the first TLS packet which server certificate it wants to use for this TLS connection.  And according to the Exim team, since this vulnerability does not depend on the specific TLS library being used by the server, both GnuTLS and OpenSSL are affected.  And though the drop-in default configuration of the Exim mail server software does not have TLS enabled by default, since TLS certs need to be supplied and configured, that makes sense.  Still, some operating systems do bundle Exim with the default vulnerable feature enabled.



So what this means is immediately a large, large population of Exim servers are vulnerable.  And there's no question.  We did verify three months ago when the previous vulnerability was made public that Exim servers declare their version in their hello message when they answer a connection, which means it is very easy for Shodan to index them by version, and for bad guys to find them.  And you don't have to wait a week now to take one over.  You can do so instantly.  So with any luck, this is old news to any of our listeners who may be responsible for Exim servers.  If not, you want to update to 4.92.2 immediately.



Also, a follow-up from a listener last week - I'm not sure which  space-time continuum we're in, Leo, but I think it was last week.  I was complaining, or I mentioned the Firefox browser memory consumption problem and how I was recommending an add-on.  I think somebody in the chatroom, because I remember you supplied the information on the fly, Leo, mentioned that there was an about:config switch in Firefox which was disabled by default.  It was browser.tabs.unloadOnLowMemory.  It defaults to false.



LEO:  Right.



STEVE:  And I said, yay.  I didn't know about that.  I'm going to turn it on, and next week I will know because the whole process I go through for building the show is to have a bunch of tabs open.  I had 40-some open at the beginning of production of this podcast.



LEO:  Wow.



STEVE:  So I set it to true, and memory consumption immediately fell.



LEO:  Woohoo.  Yay.



STEVE:  So it works.  So, yes.  To all of our listeners, if you're a Firefox user, and you have multiple tabs open in a memory-lean machine - I only have 8GB, as I mentioned, on the Lenovo X1 that I use.  I use it in a closed configuration with a Lenovo dock in my other location.  But unfortunately, 8GB is all the RAM it's got, which is a little tight these days.  And this really solved the problem for me.  So thank you, listener or chatroom person, for mentioning that.  It works great, and I wanted to let everyone know.



Also we've been talking about ransomware a lot recently because - ransomware.  So I wanted to note something that someone tweeted in my direction, which was that Windows 10 Windows Defender includes a ransomware protection feature which enables various protections against ransomware.  We were talking about, and will be actually in the future, Leo, in two weeks, talking about synchronization and how having file sync systems which store previous versions is good protection against ransomware.



Well, it turns out that Windows 10 also builds some in.  However, it is disabled by default because it requires some handholding and tuning.  And it's not for the faint of heart.  But I wanted to point out that it is there, and it is useful, and it works.  It involves two features.  One is Controlled Folder Access, and the other is Ransomware Data Recovery.  Controlled Folder Access is definitely disabled by default, and it will definitely cause you some annoyance while it's being trained.  Which is why I'm sure Microsoft has it off by default.



And what really surprised me is that it even blocks Microsoft's own apps, like IE and Edge, which I thought, well, okay, maybe they're just trying to be really evenhanded here.  But once it's been trained, it will definitely be useful until and unless it, too, is bypassed somehow.  You know, everything ends up being bypassed eventually.  But maybe it's rooted so deeply in the kernel that it's going to be difficult to have that happen.  It will probably keep unknown baddies from touching your user data filled directories, you know, like everything underneath your documents, you know, the things you tend to do.



The second component is Ransomware Data Recovery, which automatically syncs those same common user data directories up with your Microsoft OneDrive to keep those files backed up.  Anyway, so ransomware victims with this feature enabled can then use OneDrive to recover their files if they ever become encrypted with ransomware.  And I don't know how Microsoft does this, but presumably there's something going on where it's not going to get fooled.  If it already has a file that's been backed up, ransomware won't, you know, it won't overwrite that on OneDrive if it's changed by ransomware.  Maybe some heuristic that Microsoft is employing where suddenly the entire file changes dramatically, Microsoft says, not so fast there.  I wasn't able to get any specific information about how that works.



But I do know that Controlled Folder Access is a useful, but painful feature.  I had to enable it toward the end of my work on the SQRL client because SQRL stores its user SQRL identity in a SQRL folder under the user's documents.  And also SQRL has a self-install feature that really freaked out Windows 10 because installing themselves is what malware wants to do also.  So that's how I learned a lot about Controlled Folder Access.  And essentially you're getting notifications constantly of things that are wanting to write into your documents folder, so you're having to say, you know, look at what it is and go, yeah, okay.  Yeah, okay.  Yeah, okay.  Yeah, okay.



And so there's sort of an exponential falloff of yeah, okays that you're needing to explicitly do.  But frankly, I like the idea of having to whitelist things because, again, the need to train them falls off pretty rapidly over time, and then you end up with a system that's pretty well locked down.  So anyway, I just wanted to put that on everybody's map as something to consider.



Okay.  You've got to look at the logo.  It's animated.  They did SVG animation on the header of the site.  And it's just wonderful:  S-I-M-J-A-C-K-E-R dot com.  Simjacker.com.



LEO:  So there's the SIM.



STEVE:  There it is.



LEO:  But watch carefully.  What?



STEVE:  Agh.



LEO:  Turns into a voodoo mask.



STEVE:  It'll repeat.



LEO:  That's really good.  That's really...



STEVE:  I loved it.  They did a really nice job.  So unfortunately, that's the end of the good news.



LEO:  That's the end of the funny stuff.  Oh, boy.



STEVE:  That's - the rest is not funny.  And when I read this, like, what?  You what?  It's a new SIM card flaw.  And get this, not theoretical, not like some university saying, oh, you know, maybe if the moon is full and you click your heels three times you can leak some data from an Intel side channel.  No.  This is discovered being actively exploited in the wild, which allows attackers to hijack ANY, capital, all bold, ANY phone just by sending it an SMS message, which is like, what?



So this comes from AdaptiveMobile Security.  And in their overview they said:  "AdaptiveMobile Security have uncovered a new and previously undetected vulnerability and associated exploits, called Simjacker.  This vulnerability is currently being actively exploited by a specific private company that works with governments to monitor individuals."



LEO:  What?



STEVE:  Uh-huh.



LEO:  Oh, man.



STEVE:  Yeah.  "Simjacker and its associated exploits is a huge jump in complexity and sophistication compared to attacks previously seen over mobile core networks.  The main Simjacker attack involves an SMS containing a specific type of spyware-like code being sent to a mobile phone, which then instructs" - and this is where I'm going, like, wait a minute, have we just jumped the shark here? - "instructs the SIM card within the phone to take over the mobile phone to receive and perform sensitive commands."  And I did some digging around in Wikipedia.  It's like, wait a minute, I thought a SIM was a ROM.  How can a ROM take over?



Anyway:  "The location information of thousands of devices was obtained over time without the knowledge or consent of the targeted mobile phone users.  During the attack, the user is completely unaware that they received the attack, that information was retrieved, and that it was successfully exfiltrated.  However, the Simjacker attack can, and has been extended further to perform additional types of attacks.



"Simjacker has been further exploited to perform many other types of attacks against individuals and mobile operators such as fraud, scam calls, information leakage, denial of service, and espionage.  AdaptiveMobile Security Threat Intelligence analysts observed the hackers varying their attacks, testing many of these further exploits.  In theory, all makes and models of mobile phone are open to attack as the vulnerability is linked to a technology embedded in SIM cards.  The Simjacker vulnerability could extend to over one billion mobile phone users globally, potentially impacting countries in the Americas, West Africa, Europe, Middle East, and indeed any region of the world where this SIM card technology is in use."



They finished:  "We're quite confident that this exploit has been developed by a specific private company that works with governments to monitor individuals.  AdaptiveMobile Security has been working closely with the customers and the wider industry, including both mobile network operators and SIM card manufacturers, to protect mobile phone subscribers.  We've blocked attacks and are committed to using our global threat intelligence to build defenses against these new sophisticated attacks that are circumventing current security measures."



So I have a lot of information here, but I had to dig down further.  So I found a "how does it work."  They wrote:  "The main Simjacker attack involves an SMS containing a specific type of spyware-like code being sent to a mobile phone, which then instructs the SIM card within the phone to take over the mobile phone to retrieve and perform sensitive commands."  So that repeats what I said earlier.



"The attacks exploit the ability to send SIM Toolkit Messages and the presence of the S@T browser on the SIM card of vulnerable subscribers."  And I thought, what?  Maybe we are in the future.  "The S@T browser is normally used for browsing through the SIM card.  The Attack messages use the S@T browser functionality to trigger proactive commands that are sent to the handset."  And it turns out that, like, the SIM is in line between the radio and the rest of the phone handset functionality.  Anyway, the responses to these commands are sent back from the handset to the SIM card and stored there temporarily.  Once the relevant information is returned from the handset, another proactive command is sent to the handset to send out an SMS containing the information.  So I thought, what the hell?  An S@T browser of some kind on SIM cards?



So at this point I jumped over to Wikipedia to get a bit more background, and that just made things worse.  Wikipedia says:  "SIM Application Toolkit (STK) is a standard of the GSM system which enables the subscriber identity module" - which is what SIM, S-I-M, stands for - "to initiate actions which can be used for various value-added services.  Similar standards exist for other network and card systems, with the USIM Application Toolkit (USAT) for USIMs used by newer generation networks being an example.  A more general name for this class of Java Card-based applications running on UICC cards is the Card Application Toolkit."  Okay.  So there's just way too much technology which apparently some committee once upon a time said, oh, that'll be cool to have in there.  Let's stick that in.  And, like, nobody - everyone just kind of forgot about it.



Anyway, so the SIM Application Toolkit, this SAT, which is in all GSM network phones, "consists of a set of commands programmed into the SIM which define how the SIM should interact directly with the outside world" - what? - "and initiates commands independently of the handset and the network.  This enables the SIM to build up an interactive exchange between a network application and the end user and access, or control access to, the network.  The SIM also gives commands to the handset such as displaying menus and/or asking for user input."



LEO:  Wow.



STEVE:  It's like, it what? 



LEO:  Who knew?



STEVE:  Yes, exactly.  Well, unfortunately, bad guys.  "STK has been deployed by many mobile operators around the world for many applications, often where a menu-based approach is required."  Okay.  Maybe back in the day of a flip phone, where you actually  had a text screen that you had to, like, scroll through or something?  But it turns out everyone stopped using it, but it stayed in there.  And they give an example:  "...such as mobile banking and content browsing.  Designed as a single application environment, the STK" - this is still Wikipedia - "can be started during the initial power-up of the SIM card and is especially suited to low-level applications with simple user interfaces."



And they finish what I'm quoting here, saying:  "In GSM networks, the SIM Application Toolkit is defined by the GSM 11.14 standard released in 2001."  So, yes, for the last 18 years.  Then they said:  "From release 4 onwards, GSM 11.14 was replaced by 3GPP TS 31.111, which also includes the specifications of the USIM Application Toolkit for 3 and 4G networks."  So in other words, we all have it.  And if our phone receives an SMS that was carefully crafted completely without our knowledge or permission, things can be done to our phone behind our back.



Let's see.  "AdaptiveMobile Security explained that their global threat analytics system allowed them to correlate the Simjacker sources with known malicious threat actors.  As a result, they can state with a high degree of certainty that the source is a large professional surveillance company with highly sophisticated abilities in both signaling and handsets.  These types of companies exploit the fact that mobile operators may incorrectly regard core network security as solved, if they deploy a standard GSMA-compliant firewall."  But that's not the case.



So they've revealed the existence - "they" meaning AdaptiveMobile - the existence of the vulnerability and associated exploits that they call Simjacker.  They believe this vulnerability has been exploited for at least the past two years by a highly sophisticated threat actor in multiple countries, primarily for the purpose of surveillance.  Other than the impact on its victims, from their analysis, Simjacker and its associated exploits is a huge jump in complexity and sophistication compared to attacks previously seen over mobile core networks.



I've got a bunch more information about IMEI recovery, location, data messages, a big graphic here in the show notes showing the way, on the way in, the SIM intercepts a Simjacker attack SMS and essentially takes over the phone and is then able to probe, execute commands on the device, obtain information back, and then forward it out to an accomplice device.  And so at the minimum it's able to obtain location information, essentially pinging the phone without any notification of the user.



But it's also able to play a tone; send a short message; set up a call; send USSD information; send SS, whatever that is; provide local information, the IMEI, the battery, the network, the language, et cetera; power off the card; run an AT command behind the user's back; send DTMF commands; launch browser; open a channel; send data; get service information; submit multimedia requests; determine geographical location.



Anyway, so they said:  "By using these commands in our own tests, we were able to make targeted handsets open up web browsers, ring other phones, send text messages and so on.  These attacks could be used to fulfill such purposes as misinformation, by sending SMS/MMS messages with attacker-controlled content; fraud, by dialing premium rate numbers; espionage, as well as the location retrieving attack, an attacked device could ring another number, thus turning it into a listening outpost; used for malware spreading by forcing the browser to open a web page with malware located on it, which it would then execute; denial of service, by disabling the SIM card; or information retrieval, retrieve other information like language, radio type, battery level, and so forth."



So basically we all have this capability in any GSM network-participating phone right now.  So all of those examples that we see in the movies where the person takes the SIM card out, those suddenly sound like a much better idea than they were before.  Of course you take the battery out, too, and the phone is shut down.



LEO:  Or completely, for no reason at all, break it in half and throw it out the window.



STEVE:  You've got to crack it in half, Leo, just for dramatic effect.



LEO:  Breaks the hinge.  That's all you're doing.  Everything's fine.



STEVE:  I do have another graphic in the show notes which is interesting, which is a distribution, if you scroll way down, against a black field, a distribution of attack targets that they have seen.  So it's an extremely long tail, but clearly a very few number of specific targets were receiving a high rate of probes about their location.  They said:  "In one country we are seeing roughly 100 to 150 specific individual phone numbers being targeted per day via Simjacker attacks."  They said:  "Although we have witnessed bursts of up to 300 phone numbers attempted to be tracked in a single day, the distribution of tracking attempts varies."



So anyway, this is being used for surveillance in targeted attacks.  And the good news is it looks like these could be caught because they have to transit the mobile phone network.  And right now there are no firewalls that are blocking this from happening.  Basically it means getting smart about this at the mobile phone infrastructure level because it's certainly not the case that all of our SIM cards are going to be replaced any time soon.  But, boy, I mean, this looks like a capability that was overdesigned once upon a time, that had a brief window of usage, and then nobody ever turned it off or took it away or shut it down.  And it's just a glaring vulnerability in GSM phones.



LEO:  Oh, only GSM.



STEVE:  Yes.  Only, well, GSM, SIM-based GSM.  I'm not enough of an expert in other non-GSM networks.  But there are alternative SIM-like systems in non-GSM phones.



LEO:  Right.  There are SIMs in LTE phones.  But do you think that's a different kind of thing?



STEVE:  I'm not, again, not an expert enough to know.



LEO:  Yeah, interesting.  I bet it is the same.  Why would, you know, they'd never take out a capability; right?  When you can put in more.



STEVE:  Exactly.  Wow.



LEO:  Mark says it's the same.  He says the only thing resistant is CDMA.  Mark's a security expert and fan.



STEVE:  Ah, cool.  And does anyone use CDMA anymore?  Did that go away?



LEO:  You know, some older Verizon and Sprint handsets maybe.  But everybody's moved to LTE.  CDMA over LTE in some cases.  But I think you'd still be using the SIM.



STEVE:  Yes.  In that case the underlying transport would be LTE still.



LEO:  Right.



STEVE:  Wow.  So anyway...



LEO:  Not good.  Not good.



STEVE:  Our listeners, in general we don't have anything to worry about.  It's nice that this came to light.  Certainly there are people somewhere whose location is, well, and you can imagine that this is the kind of thing that law enforcement could ask a provider to provide for a given individual.  We want to know where they are right now, and there could be an answer from virtually any - virtually anybody can be targeted whose phone number is known.



LEO:  Right.



STEVE:  By sending them an SMS message.  Their phone will say,  yup, here's where I am, and other stuff, too.



LEO:  Wow.  Wow.  Just - and of course hijacked for malicious purposes, not merely spying on you.  But, you know, you could make money off of it.



STEVE:  Right, right.



LEO:  Well, well, well.  Isn't that special.  Thank you, Steve, for really cheering up - I think that's why people listen.  They listen for the happy news here at Security Now!.  And that's why they keep tuning in.  They still haven't gotten any.



We do this show every Tuesday, 1:30 Pacific, except when we're doing it in the time zone shift thing.



STEVE:  Yes, when we are dropping through...



LEO:  We're in our time tunnel.



STEVE:  ...a spatial rift.



LEO:  When we're in the Star Gate, you never know when it'll be.  But so we have recorded the October 1st show ahead of time.  And I think we're going to record again on Saturday afternoon; right?



STEVE:  Yes, we are.  We're going to record on Saturday afternoon for Tuesday.  



LEO:  Steve's headed out - when are you leaving?



STEVE:  I leave on Sunday.



LEO:  Okay.  Steve's going on his European tour to tell the world about SQRL.  By the way, Mark was at the event in - where was it?



MARK:  Orange County.



STEVE:  Oh, cool.



LEO:  And he said there were over a hundred people there, and you asked, "How many of you listen to Security Now!?"  And when everybody raised their hand, you said, "Well, let me put it this way.  How many of you don't?"  And it was three people.



STEVE:  Yeah.  And I said, "Maybe you three should get a clue because, you know..."



LEO:  Yes.  Everybody else is listening.  I hope you all got that clue.  If you like Security Now!, tune in.  We do it live, as I said.  We won't be for a couple of weeks.  But normally it's Tuesdays at 1:30 p.m. Pacific, 4:30 Eastern, 20:30 UTC.  You can watch live at TWiT.tv/live.  So we're not going to be back here until October 8th.



STEVE:  Correct.



LEO:  October 8th we'll be back on our regular schedule.  But as I said, we're not missing an episode.  We recorded an evergreen for October 1st, and we're going to do one on Saturday.  If you want to watch that, by the way, it'll be about 2:30 Pacific, 5:30 Eastern, 21:30 UTC Saturday, right after the radio show.



STEVE:  Even if we have to bend the space-time continuum to make it happen, Leo, we will...



LEO:  Well, this one's not so bad because you're recording Saturday for Tuesday.



STEVE:  Yeah.  So there'll be a less dramatic snapback when the fabric restores.



LEO:  The rent is repaired.  You can get copies of this episode.  You know, it's all in order on the website.  Go to GRC.com.  While you're there, you can not only get a 16Kb version of this show, but 64Kb audio.  You can also get a great transcription by Elaine Farris.  She writes it all down.  It makes it a lot easier to understand if you can read along.



We also have lots of other good stuff there, including Steve's bread and butter, SpinRite, world's best hard drive maintenance and recovery utility, GRC.com.  Steve can be messaged there, GRC.com/feedback.  But you can also "at" him on the Twitter.  He's @SGgrc.  And he takes DMs, as well, there.  So if you have something secret to tell him, @SGgrc. 



I will be also putting the show up on our website, TWiT.tv/sn.  And when I say "I," I mean those guys down the hall because I'm out of here.  TWiT.tv/sn.  Or best thing to do, you know what, you should all be doing this, don't be one of those three people who says I don't know what you're talking about.  Subscribe.  Find your favorite podcast application or subscribe on YouTube, and you'll get it the minute it's available in the proper order.  The packets will be reordered for your convenience.  No buffering.  Thank you, Steve.



STEVE:  Okay, my friend.  I will talk to you Saturday afternoon, after The Tech Guy.



LEO:  All right.  All right.  I'll see you then.



STEVE:  Bye.



Copyright (c) 2019 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.






GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#733

DATE:		September 24, 2019

TITLE:		Top 25 Bug Classes

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-733.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we look at the driver behind this summer's comeback in cryptocurrency mining.  We also check out a managed security provider's summary of the biggest problems they encounter with their more than 4000 clients.  We look at the revised and worrisome update after six years of SOHO router and NAS device security, and we suggest that everyone using Chrome go to Help > About.  I found three notes about SpinRite that I'm not sure I ever shared, so I will.  Then we conclude with the result of processing the massive CVE vulnerability database which reveals the top 25 most enduring classes of software bug impacting the security of our industry.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  We recorded this Saturday for a Tuesday airing, but there is so much to talk about, as always:  the three most attacked ports, what are they, and what you should do to protect them; why SSH isn't as secure as it ought to be; and why people are still running old versions of Windows.  And then the top 25 software vulnerabilities.  This is a list every programmer should memorize.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 733, recorded Saturday, September 21st, 2019:  The Top 25 Bug Classes.



It's time for Security Now!, the show where we cover the latest security, privacy, and ransomware and breaches with this guy right - right?  It's now the Ransomware Breach report with Steve Gibson.



STEVE GIBSON:  It's funny you should say that, Leo, because at the top of the show I talk about how we've pretty much been dominated by ransomware recently.  But as a consequence of an interesting driver, there has been a comeback in cryptocurrency mining that we're going to also be talking about.



LEO:  Oh, interesting.



STEVE:  But today is Episode 733 and counting down to 999, which we titled "The Top 25 Bug Classes."  The Mitre group that manages the CVE, the Common Vulnerabilities and - it's not exploits.  I can't - and something.  Anyway, we'll get to it when we get to it.  They've processed their massive vulnerability database.



LEO:  Exposures.



STEVE:  Exposures, that's it.  I can never remember.



LEO:  I couldn't remember it, either.  I had to look it up.



STEVE:  They've processed their massive vulnerability database to - and they do this periodically - to reveal the top 25 most enduring classes of software bugs, which impact the security of our entire industry.  And actually, they weren't happy limiting it to  25, so they actually have, like, another 15 afterwards.  But we're going to talk about that.  Because, I mean, there are, like, lots of problems.



But a managed security provider also that has more than 4,000 small and mid-sized businesses, which they call SMBs for small and mid-sized businesses, has done a summary of all the problems that they continually encounter among their 4,000-plus clients.  So that we're going to have fun talking about.  We also have a revised and worrisome update after six years.  It was in 2013  that a group took a look at 13 consumer routers and NAS devices to see how vulnerable they were.  Well, they've updated their report six years later, and the name of their report is just - it's one for the ages.  But we'll leave that for when we get to it.  



And also we need to have everyone go to Chrome and go Help > About, yeah, Help > About, or no, About > Update.  Anyway, whatever it is in Chrome.  You need to just go into the About box.  I'd been using Chrome all morning and yesterday, and it had not fixed this.  There is an emergency five-alarm alert which they're not even talking about because it's so bad.  It's a no-click total takeover of the user's system that Chrome affords.  And so when you go into Help > About, it'll say, oh, look, I've got an update.  And then it'll do it right there.  And then restart, and then you're safe.



But until then, and I don't know, like I guess they're just pushing this thing out lazily, but here it is, you know, several - this happened on Wednesday, and so it hadn't happened for me yet.  So anyway, lots to talk about.  Some fun stuff.  I have an interesting graph from this enterprise, this small to mid-sized business company which we will talk about after our first break.  And I think we've got a great podcast for our listeners.  



LEO:  Now, those of you who are eagle-eyed and watching the video might have noticed that the date is Saturday.  We're recording this on Saturday because Steve's about to get on a "big ol' jet airliner" and fly to Europe so that he can do some SQRL presentations.



STEVE:  Yup.



LEO:  And then he'll end up in Boston on October 3rd, where we're going to do our special event on the Future of Authentication with Bill Cheswick.  Steve and I will be there;  Gerry Beuchelt from LogMeIn.  It's going to be a great event.  I think it's almost sold out.  We talked to them, when was that, Steve, on Thursday?



STEVE:  On Thursday, yeah.



LEO:  And I don't know if there's a ballroom big enough at the Intercontinental Hotel to hold all of us.



STEVE:  Well, yeah, they're running out of room.  And in fact this morning I mailed Jennifer 450 SQRL stickers.



LEO:  Oh, good.



STEVE:  So that she could add those to the swag bags that I guess everyone's going to be getting.



LEO:  Nice.  Yeah, I think we're going to have that many and maybe more.  But if you - look.  At least, if you want to go, and you're going to be in Boston, 3:45 p.m. October 3rd, that's a Thursday.  And we'd love to see you.  There'll be an event, about an hour and a half talking event, and then a panel discussion, and then a cocktail party afterwards.  You're invited to come and see and speak.



And all you have to do, it's free, is go to twit.to/unlocked, twit.to/unlocked.  And that'll be the LastPass page where you can sign up.  It's a free event.  But here's some cool news.  Each of you gets a $100 token to donate to one of three charities.  So it's a charity event.  So it's kind of a neat way to get 100 bucks to donate.  I like that.  Twit.to/unlocked.



So we're recording it now.  The show will air or go out on the podcast network at its usual time on Tuesday.  So if you're listening on Tuesday, it's possible that something has happened, some dramatic event has happened between Saturday and Tuesday.  We don't know about it yet.  We haven't perfected time travel.



STEVE:  And that time machine that we were using...



LEO:  It's broke.



STEVE:  It's down for maintenance right now.  So we're unable to jump forward in order to provide that in advance in this instance.  Well, and you know it was the flux capacitor burned out.



LEO:  Oh, no wonder.  



STEVE:  So we're trying to get one from Marty.



LEO:  They put a hole in the Stargate, and now we're stuck, yeah.



STEVE:  Don't you hate when that happens.



LEO:  We're stuck in 2019.  Who ever thought that would happen?



STEVE:  It was a problem at the Event Horizon of the Stargate Portal.  There was just too much ripple.



LEO:  Oh, boy, are we nerds.  So, yeah.  Anyway, that's why whatever - I know this is confusing, but that's what's going on.  And we'll be back on our normal schedule.  Oh, I should mention this.  Next week is a special kind of evergreen Security Now!.  We do a little news, but mostly we're going to talk about Steve's search for the perfect sync solution.  So that is going to be really good.



STEVE:  The Joy of Sync.



LEO:  The Joy of Sync.  That'll be a week from Tuesday.  And then we'll be back on track.  And you'll have gone to, where, Dublin, Copenhagen - no, no.  Gteborg.  Gothenburg.



STEVE:  I'm speaking to the Dublin OWASP group on Tuesday, and then the Gothenburg, Sweden group on Thursday.



LEO:  Wow.  Wow.  That's great.  And so you get a little European jaunt.



STEVE:  Yeah.  And both of those are, like, way oversold, as well, by several hundred.  So I think there are going to be some fun presentations.  And I added some more schmaltz and polish to the presentation after the Orange County presentation.  So I think it's going to be a lot of fun.



LEO:  So the schmaltz and polish will not be brought to Boston, however, because we don't have a screen.  So it's just you're going to have to talk your way through the schmaltz and polish.



STEVE:  But I think we still need to do one with a TWiT audience.



LEO:  Oh, we're definitely doing that, yeah.



STEVE:  I would love to do that.



LEO:  That's at your leisure, yeah, whenever you want.



STEVE:  That'll be the schmaltziest that we have because I will be extreme, I will be over-polished.  



LEO:  Extreme schmaltz.



STEVE:  I'll have a simonized hard wax shine on it.



LEO:  Okay, Steve.  On we go.



STEVE:  So this chart jives with what I expected.  And I thought it was very interesting.  This was part of the - this is from the report that we'll be getting to a little bit later from this managed security services provider who's got more than 4,000 clients.  And the chart is the distribution of Windows operating systems within their client base.  And not surprisingly, it shows the number one OS is Win 2008, which of course we know is the server version of Windows 7, at, oh, looks like maybe 33%.  About a third of them are that.  Windows 7 is in second place at looks like about 30%.  Then Windows 2012 at 20%, that's again the server version of, what, I guess that must be Windows 8 server version.  And this, again, sort of jives with what I'm expecting.



Windows 10 is down at 5% in the enterprise.  And there's even some XP and a little bit of Windows 8.  No Windows 2000.  That's finally really dead.  But interesting also, no Win Server 2019, which of course is the server version of Windows 10.  So I think what this is showing us is what we expect, which is that the overall distribution of Windows 7 and Windows 10 is hugely skewed in the end user direction for Windows 10, as a consequence of Microsoft's really strong push in the early days of Windows 10 to end users.



You know, end users are like, oh, new Windows, yeah.  And, I mean, as we talked about often, you really had to fight as an end user not to get yourself updated to Windows 10.  And of course I famous created Never10 as a little bit of freeware just to help with that process because Microsoft was being so aggressive.  So as we would expect, a huge number of end users are on 10.



But also because the enterprise is like, well, you know, we know that when you buy new machines you're just going to get Windows 10.  The newest machines it's very difficult to get Windows 7 to even install on them because Windows 7 lacks USB3 drivers.  And so you have to - and I do, because I have, because I want Windows 7 on some machines - you've got to jump through some hoops in order to do all kinds of crazy Windows stuff because otherwise the installation just stalls at one point when Windows 7 attempts to run on hardware that has USB3, which all new hardware does.  So it's possible to do it.



So I guess my point is that here we're looking at combining Win 2008 and Windows 7.  We're looking at more than two thirds of the operating systems in this sample set of 4,000, more than 4,000 small to mid-sized businesses are running some version of Windows 7, which of course is end-of-lifeing at February of 2020.  So, and we know that Microsoft has said, well, we're going to make it cost you if you want to stay with Windows 7 by then beginning to charge for extended support past then.



Anyway, it's going to be interesting to see what happens, whether maybe Microsoft capitulates and just says, you know, we really wish people were running 10.  But, boy, we acknowledge that nobody wants it, so who knows what's going to happen.  Maybe extend free support.  Maybe decide they really do need to push people into 10.  We'll know here at the beginning of next year.  It's going to be interesting to see.  But I thought it was interesting to see within that subset of 4,000 clients, two thirds of the operating systems deployed in small and mid-sized businesses were still running Windows 7, again because it works.  I mean, it does everything the companies need it to.  And so I'm sure they're just deferring it, not that 10 wouldn't.  But it's like, let's not mess with it.



So as I mentioned from your perfect segue, Leo, at the beginning of the show, cryptomining has been making a comeback.  Obviously we've been focusing a lot on ransomware recently.  But it turns out that cryptocurrency mining operations are far from gone.  And those who watch cryptocurrency mining have been noticing that it's coming back.  The short version of the reason for this is that prices have been going up.



As we know, there was sort of a crash in cryptocurrency pricing  back during 2018 from those high-flying days in 2017.  In the case of Monero - and I should just note that Monero is the cryptocurrency of choice for those who are stealing cycles from other people's machines.  The reason is that Bitcoin's proof of work, and we talked about this back in our Bitcoin podcast before Bitcoin was a thing, Bitcoin's proof of work uses just a simple SHA-256 hash where the challenge is to add something to the blob of data that you're trying to hash to cause some number of least significant bits of the result of the hash to all be zero.  That's the challenge is you take the blockchain update, you add something to it, and so you're trying to generate a hash where some number of least significant bits out of the 256 bits the hash produces, some number are all zero.



And what was clever about that was that, over time, by slowly incrementing the number of zero bits that were required, you were able to scale the work required, that is, you kept making it increasingly more difficult to add something to the blockchain chunk that you're signing in order to solve the problem.  But the Achilles heel of this approach is that, because the algorithm is just SHA-256, that allows GPUs and then later ASICs to be custom made for ultra high-speed mining of bitcoin.



And of course, as soon as ASICs, you know, Application Specific Integrated Circuits, were available, that upped the ante for bitcoin mining to such a level that commandeered PCs just didn't stand a chance.  You just can't compete with a chip which has been custom engineered to just blast SHA-256 in order to solve the bitcoin proof of work.  Monero, its proof-of-work algorithm by contrast was deliberately designed to resist acceleration by ASICs, which is why we keep hearing about it being used in hijacked browsers and PCs and servers because it kept the playing field leveled to a much greater degree.



So anyway, over the course of this year, 2019, Monero's price, which had collapsed from its original high of around three to $400, it collapsed down.  It lost about 90% of its value during 2018, dropping down to like around 40 to $50.  And again, we know that everything for bad guys now is about money.  As soon as cryptocurrency became a commodity that you could exchange for currency that you could actually spend, that created a lot of pressure for mining.



Well, what's happened during 2019 is we've seen a gradual recovery, not to the heyday of the flying high days, but to around $115 for a Monero coin.  And with that has come increased mining pressure.  So anyway, I found an interesting summary of events involving Monero cryptocurrency that I thought I would share, just because it gives you a sense for this.



So in May of 2019 an Intezer, I-N-T-E-Z-E-R, Intezer Labs report described the battle between two cryptomining operations, the Rocke and Pascha groups, who were fighting to infect the same types of Linux-based cloud-based apps.  Same month in 2019, like earlier this year, a Guardicore report detailed a Chinese-based cryptomining group that infected over 50,000 Windows MySQL and phpMyAdmin servers in the Nansh0u campaign.  And our listeners may remember we talked about that at the time.  Also in May, Trend Micro reported that the infamous RIG, R-I-G, exploit kit had started to deploy a Monero miner as its final payload, again because it was beginning to pay again.  That cryptominer was aimed at Windows desktop users rather than servers, like most Monero mining operations tend to be, as we know.



The next month, in June of this year, another Trend Micro report detailed a new malware strain called BlackSquid.  And we talked about it at the time also.  That malware can target both Windows and Linux servers, and also uses additional exploits to move laterally through networks that it's able to get into, to infect as many systems as possible, all to deploy cryptomining payloads.  Same month, Trend Micro also reported another malware operation whose final goal was to deploy a Monero cryptominer.  Just like BlackSquid, this different malware also relied on EternalBlue to spread laterally through internal networks after compromising an initial point of entry.



Also in the same month, Trend Micro reported details of how the AES DDoS botnet, previously focused on infecting servers to carry out DDoS attacks, had shifted its strategy toward delivering Monero mining because DDoS, maybe if it's DDoS-for-hire it makes money, but not as clearly as being part of a Monero mining pool.  So now that's what they're doing.  Oh, and they were going after Docker servers as a means of attaining a foothold on a machine on which to run Monero.  Same month, June 2019, a security report described another cryptomining malware operation that infected web servers and used a cron job to obtain persistence on infected hosts.  And, you know, these are things we have talked about as they have come up in the news over the months.



In June, a Kaspersky report described a new malware strain named Plurox.  It targets Windows and comes with several modules for performing cryptocurrency mining in various forms based on the specific nature of the Windows machine that it infects.  ESET researchers in June detailed LoudMiner, a malware family that targets both macOS and Windows.  And according to ESET, LoudMiner uses virtualization software, QEMU, on both macOS and VirtualBox on Windows, to mine Monero on a Tiny Core Linux VM.  Same month, in June, Trend Micro detailed a Monero mining operation during which crooks scan the Internet for Android devices exposing their ADB, which we talked about in the past is the debug port, which then allows them to use that port to plant a cryptominer on unprotected Android machines.



The next month in July, back to Intezer Labs, who detailed the WatchBog cryptocurrency mining botnet which had been operating since late 2018, now had compromised 4,500 Linux machines to run Monero mining.  And, finally, last month, in August, a Carbon Black report detailed changes in the activity of Smominru, one of the oldest and largest cryptocurrency mining botnets around.  Besides running cryptomining payloads, the botnet also stole credentials from infected hosts, which it later put up on sale online.  But cryptocurrency mining was its main focus.  So anyway, that gives everyone a sense that, I mean, cryptocurrency  mining, because it produces money through exchanges, which are now prevalent, as that price goes up, the pressure to get miners running goes up because it becomes worth the bad guys' time.



So pretty much we're looking now at two major malware campaign systems.  We're looking at ransomware, which we've of course been talking about a lot this summer, and cryptocurrency mining, both tied for using cryptocurrency as the common connection.  You know, it's no longer the case, as it once was, where viruses and malware just existed because, oh, cool, look, we can propagate across the Internet.  No.  Now, thanks to cryptocurrency, it's about making money.



BleepingComputer had a headline which caught my eye.  The headline was "The Top Three Most Attacked Ports."  And I thought, well, that's interesting.  I wonder if it's anything we don't know.  Well, that story led me to a recently issued 16-page report by a different managed security services firm that we'll be talking about later.  In this case, the firm is Alert Logic, that titled their report "Critical Watch Report SMB [Small to Medium-Sized Business] Threatscape 2019."



Their report contains some interesting information and analysis that I thought our listeners would find interesting.  They analyzed - these are the guys that have more than 4,000 clients, small to mid-sized business clients, which gives them a really good perspective over what's going on.  To produce the report, they analyzed 1.3 petabytes of data, 10.2 trillion log messages, 2.8 billion intrusion detect system events, 8.2 million verified incidents, all across their more than 4,000 customers.



They explained and summarized what they found, as follows.  They said:  "Small to mid-sized businesses (SMBs)," they said, "are under greater pressure than ever to address cyber threats. Cybercriminals are increasingly targeting smaller businesses in addition to larger enterprises.  The principal challenge for SMBs is that they must face these threats with fewer security resources than large enterprises.  Limited budgets and staff constraints are causing many organizations to make inadequate cybersecurity investment decisions that continue to put them at risk, but forward-looking SMB leaders are seeking new ways to be 'security smart' as they address cyber risks and respond to attacks."



And of course this is a little self-serving since this group, Alert Logic, they are the people that small organizations outsource their security responsibilities to, to some degree.  But it does give them a lot of interesting visibility into a large breadth of organizations and provides some interesting material for us.



They said:  "In providing managed security services for over 4,000 organizations, Alert Logic has first-hand insights into how small to mid-sized businesses are being attacked, and the best methods for responding and reducing their attack surface.  Since the publication of our last look into the cyber security threatscape in 2018, we've observed a steady increase in attacks and changes in attack methods.  Based on an analysis of the" - get this - "5,000 attacks per day we detected across our customer base during the period from November 2018 to April 2019, we identified threat patterns and incorporated those into better defenses for our customers.  Additionally, our security researchers actively monitored emerging and evolving vulnerabilities and the attack methods across the threatscape of the open Internet beyond our customer base.  This research has uncovered several patterns that specifically affect small to mid-sized businesses, so we've chosen this focus for this latest Critical Watch Report on this SMB Threatscape for 2019."



They said collectively this research is based upon close examination of - and here they talk about their 1.3 petabytes of security data, 2.8 billion IDS [Intrusion Detection System] events, 8.2 million verified incidents and so forth.  So they have nine key takeaways.  The first, encryption-related misconfigurations are the largest group of SMB security issues.  Encryption-related misconfigurations.  In small and mid-sized business AWS environments, encryption and S3 bucket configuration remain a challenge.  Weak encryption is a top SMB workload configuration concern.  They said most unpatched vulnerabilities in the SMB space are more than a year old, which is I think a really interesting and important takeaway.



The three most popular TCP ports account for 65%, so essentially two thirds, of the vulnerabilities in small and mid-sized businesses.  So the top three account for the top two thirds.  They said unsupported Windows versions are rampant in mid-sized businesses.  And I got a kick out of that because of course that takes us back to Windows 7, which will be unsupported in 2020.  Outdated Linux kernels, which is something that really hadn't occurred to me before, they say are present in nearly half of all small and mid-sized business systems.  Outdated Linux kernels.  Active unprotected FTP servers lurk in low-level SMB devices, like IoT things have unprotected FTP servers exposed.  And SMB email servers are old and vulnerable.



Let's see if there's anything more.  I have some - basically those nine takeaways I also expanded in the show notes.  For this encryption-related misconfigurations they said:  "Automated patching has made inroads in the fight to eliminate vulnerabilities in the SMB space.  Patches are often distributed and can be done automatically across ecosystems.  What remains as an issue is misconfigurations which can require remediations ranging from manual reviews to complete architectural redesign."  They said:  "In our analysis, we determined that 13 encryption-related configuration issues account for 42% of all security issues found."



Talking about AWS, they mentioned that Amazon Web Services - and actually I wasn't aware of this.  I was aware that they're a strong player, but they said "with a market share equivalent to that of the next four public cloud providers combined."  So, boy, AWS is a heavyweight.



LEO:  Well, they're also probably doing more business than all the four combined.



STEVE:  Right, right.  They said:  "Our analysis of AWS configuration issues shows that encryption issues affect 33% of the SMB instances we scanned.  This indicates encryption is not yet an instinctive behavior," as they put it, "despite being a best practice and a requirement of many regulations including PCI-DSS, HIPAA, HITECH, GBLA, GDPR, NIST," and so forth.  



They said that weak encryption remains a continuing problem.  And that fourth point, they said most unpatched vulnerabilities - I don't know what's happened to my voice, Leo.  It's like yours was at the end of The Tech Guy today.  They said:  "Most unpatched vulnerabilities in the small and mid-sized business space are more than a year old."  They wrote:  "Even though automated updates have vastly improved software patching, organizations are still having difficulty keeping pace.  When examining the top 20 unpatched vulnerabilities present in the small and mid-sized business space, Alert Logic found that 75% of them are more than a year old.



"The use of open source software, a widespread and established technique for building software projects efficiently, can complicate the patch cycle.  This is particularly true when the open source software is embedded.  This is a challenge for organizations that leveraged open source resources and libraries.  To uncover and reduce the vulnerabilities left by this unpatched code, it is critical for all organizations to invest in third-party validation of the efficacy of the update process in the software development life cycle.  Regular vulnerability scanning is a requirement."



So of course we've talked about this often, that the major OSes and now the major browsers have taken this responsibility on.  But we're still not seeing this, for example, even in our routers and, to a much lesser degree, in low-end IoT devices.  They almost all use Linux micro kernels and various open source resources.  And if they've been around for a while and haven't been keeping themselves updated, they can represent a problem.



And on these three most popular TCP ports, they said:  "Port scanning is done regularly by both attackers and defenders.  Internal security teams, blue teams, can use regular port scanning to help identify weaknesses, firewall misconfiguration issues, and to discover unusual services running on systems.  When considering their attack surface, organizations should be aware of which ports have the most vulnerabilities, which is a factor of port popularity more than a statement on the port's relative security."



In examining ports, given that these ports are the ones that are exposed to the Internet, it is no surprise that SSH port 22, that's number one.  HTTPS 443, and HTTP 80 made the top three, with 65% of the vulnerabilities.  They said:  "However, it is interesting to note that the recent MS RDP BlueKeep attack targets the fourth most popular port, RDP 3369."  And in fact what was interesting was, in the show notes, under the top missing patches, I pulled a graphic from their report.  And it's sort of sobering how many of the patches they show missing are OpenSSH.  And that is also the number one attacked port.



So the big takeaway from this is, if your organization is using OpenSSH, it really is crucial to keep it current.  Depending upon the device, that has the OpenSSH server running on it, you really want to make sure that it is up to date.



They showed their Table 4 from their document.  And I have it in the show notes.  OpenSSH Security Bypass is number one.  There's an RC4 Plaintext Recovery is number two.  Then back to OpenSSH Man-in-the-Middle issue, number three.  OpenSSH Arbitrary File Overwrite in fourth place.  OpenSSH, a Man-in-the-Middle issue in fifth.  OpenSSH User Enumeration issue in sixth place.  OpenSSH User Enumeration also in seventh.  I mean, it just goes on and on and on.  In fact, almost all of the rest are OpenSSH issues.  So as we know, OpenSSH is giving very powerful remote access capabilities to enterprises that use it.  And it is, when it's secure, it is a very secure protocol.  But it's just software, and it's a server, and it is the number one most attacked port on the Internet.  So you want to...



LEO:  Is an attack as simple as a password attempt, a login attempt?



STEVE:  It probably qualifies, yes.



LEO:  Because I see a million of those on my logs all the time.



STEVE:  Yes.  And so I think you're right.  I think it's not necessarily a, you know...



LEO:  It's an unsuccessful attack.



STEVE:  Exactly.  So it's an attempt, as opposed to a success at anything.



LEO:  Right, yeah.  Because I just, by the way, the simplest fix for that is turn off password logins for SSH.  Use certificates or use keys.



STEVE:  Absolutely.  Well, I would do that, and I would run on a randomly numbered obscure port.  There is no reason to leave SSH sitting on 22.  I mean, again, obscurity we know is not security.



LEO:  Yeah, I mean, you could scan all the ports pretty quickly, right, if you're looking for - I guess you'd have to - there's 65,000 of them, so it'd take a while.  But...



STEVE:  There are 65,000, and of course there's lots of IPs.  So if your IP is not well-known, then that really dramatically increases the scan space.  Typically, well, for example, you would not like to be enumerated by Shodan as somebody running an SSH server on port 22.



LEO:  Yeah, that's a good point.



STEVE:  So staying off to the side somewhere probably does make sense.  And Leo, let's take our second break, and then I'm going to talk about what they had to say about unsupported Windows versions that was kind of interesting.



LEO:  Lord above.  Just goes on and on and on.



STEVE:  Oh, yes.  So more than 66% of scanned devices are running Microsoft OS versions that will be out of support by January of 2020.  Just, I mean, this is their summary, and this is what I had already noted from the graph that I put up at the top, our Picture of the Week, which I thought was so interesting, you know, Server 2008 and Windows 7.  They said:  "The current Windows Server release 2019 is almost undetectable"- meaning in terms of its presence among their customer set - "while the majority of devices scanned during the period analyzed are running Windows versions that are more than 10 years old.  Additionally, there are still a non-trivial number of Windows XP and even 20-year-old Windows NT devices out there."  You know, again, people just - it's running.  It's just like, just don't open the closet.  Just be careful, you know, just maybe look and see if the lights are still blinking every so often.  It's just like they're leaving it alone.



They said:  "Even if they are not exposed to the Internet, these targets make lateral movement relatively easy once a host has been compromised.  With the discontinuation of security updates and bug fixes for Windows Server 2008 scheduled for 2020, combined with the SMB trend of holding onto old operating systems" - can you say Windows 7, Windows Server 2008 - "this security issue," they say, "is likely going to get much worse next year."  I think that's true.  I mean, like, bang.  This is why I think it's going to be really interesting to see what Microsoft chooses to do about this.  I mean, we know it's unreasonable to ask them to continue supporting, except the other thing is you could say, well, if they're going to be supporting anybody for pay, as they are, for what is it, I think three years, three more years where you pay...



LEO:  A lot.



STEVE:  ...additional for each year, yeah.  Well, that means that they're doing all the work anyway.  So they're saying to other people, well, yeah, we have the updates, but you've got to pay us if you want them.  I guess it's reasonable.  I don't know.  Then they also found, these guys, that outdated Linux kernels were present in nearly half of all systems.  They said:  "Kernels are the heart of an operating system.  They manage everything including hardware, memory, applications, and even user privileges.  Kernel vulnerabilities are discovered quite frequently, and fixes are only released for supported versions. In a 2017 article, Computerworld described these outdated Linux kernels as the 'working dead.'  Many deployed application systems mask the underlying OS distribution flavor, making it difficult to determine which kernel version is being run."  Right?  They're just turnkey things, like NAS boxes, for example, that are just like, oh, yeah, don't worry about what's going on here, just we're offering you these services, and they work.



They said:  "However, about half the systems we identified are still running a version 2.6 Linux kernel, which has been out of support for more than three years.  There are at least 69 known vulnerabilities for this kernel, with many of them relatively easy to exploit, and with 24 of the Common Vulnerabilities and Exposures (CVEs) scoring 7 or above on the severity scale."  So that was half the systems still that are out there in their customer base, still running a 2.6 kernel with at least 69 known vulnerabilities, and 24 of those 69 scoring with a CVE score of 7 or higher in vulnerability.  So lots of problems that are not being exposed.



And then they talk about FTP being exposed in these environments, and that email servers, of course we've been talking about the Exim problems recently, both that really slow to take over and the new one, the problem in the Server Name Indication (SNI) handshake in TLS, where you just add a backslash null to the end of the server name, and you immediately have a root-level remote code execution compromise.  Pow.  So, yeah, lots of problems.  So anyway, I thought that was an interesting look at, you know, it's not the entire industry, but there's 4,000 customers of a managed security provider which gives us a good snapshot.



Okay.  I talked about one of the best named problems I've seen in a long time.  There was a Baltimore, Maryland-based security consultancy, Independent Security Evaluators (ISE).  This is the group that I mentioned six years ago, back in 2013.  They did an analysis of 13 small office/home office, so-called SOHO, small office/home office, SOHO routers and wireless access points, and also some network attached storage devices.  At that time they found, that is, six years ago, 57 security bugs, and were able to take over 11 of the 13 devices remotely from outside the local network.  Their report was wonderfully titled "SOHOpelessly Broken."  So six years ago made a big splash.  Lots of popular devices that they tested.  We would hope that our beloved industry would have taken this to heart and enhanced its security, fixed all those problems, and - right?  No.



ISE, this Independent Security Evaluators group, recently updated its test to what they now call SOHOpelessly Broken 2.0.  They've got a whitepaper and a PDF.  I've got links to those in the show notes.  But I'll just summarize what they found.  Once again, they tested 13 devices, some from the same vendors and some new.  So they found more than double the number of flaws now as opposed to six years ago.  They filed 125 new CVE bugs based on their research.  This time around, they were able to obtain remote root access on 12 of the 13 devices.  They tested equipment from ASUS, Buffalo, Drobo, Lenovo, Netgear, QNAP, TerraMaster, Seagate, Synology - and by the way, Leo, you'll be glad to know that Synology was perfect, the only one.



LEO:  Yay.  That's the one I use, yeah.



STEVE:  Xiaomi, Zyxel, and Zioncom.  The attacks included bypassing authentication mechanisms altogether.  On one device, the team was able to hijack a cookie authentication system by changing the IP address to 127.0.0.1 and issue unauthorized requests via the authentication API.



LEO:  Wow.  Wow.



STEVE:  Basically localhost.  And you've got the chart right now up onscreen.  The project found that some things had changed since 2013 and others had not.  Device vendors had taken steps to protect their software.  For example, several used Address Space Layout Randomization, ASLR, to make memory-based attacks like buffer overflows more difficult to successfully exploit.  However, they were able to exploit other flaws to bypass the ASLR and still launch the buffer overflow attacks anyway.



So I have in the show notes here a grid.  Command injection was the most often vulnerable exploit.  The Buffalo TeraStation, the ASUS RT-AC3200, the Netgear Nighthawk R9000, all of these were vulnerable.  The TerraMaster F2-420, the Drobo 5N2 was, although I should note that was when the optional web interface was enabled and publicly exposed, which would require some customization.  The Zyxel NSA325 v2, Totolink A3002RU, an Asustor AS-602T.  The Seagate STCR3000101 was not, the only one along with the Synology DS218 that was not.  The QNAP TS-870 was, the MI Router 3 was, and the Lenovo ix4-300d was.  That's just command injection.



There were two that suffered from SQL injection.  A bunch of them had cross-site script problems.  The ASUS RT-AC3200 suffered from a buffer overflow.  Five of them suffered from an authentication bypass.  Oh, two different authentication bypasses and also the same five in those cases.  There was a cross-site request forgery problem.  So it looks like a lot of these are web-based problems in web servers that they have exposed and web content that they have exposed.  And a bunch of them also had a file upload path traversal problem.  And of course, as we know, path traversal is another just ongoing headache for systems.  They just are not good about catching those problems.



They said that one device encrypted the PHP files used to process requests through its web interface.  But of course, if it's going to encrypt the PHP files, it itself has to be able to decrypt them.  So it had to store the decryption key on the device.  So they found the decryption key, decrypted the files, and then used those with PHP's system function to obtain shell access.



Paraphrasing from their report, they wrote:  "Perhaps more interesting are the number of features that are still missing since SOHOpelessly Broken 1.0.  Features such as anti-CSRF tokens and browser security features, which are commonplace now in mainstream web applications, are still rare among the sample devices that they looked at."  They wrote:  "If vendors had implemented these basic protections, we would not have been able to hack the devices."



ISE tried many different types of attack, as I mentioned in that chart, often chaining them together when necessary to successfully exploit the device.  The most successful were cross-site scripting and command injection, which are, as we know, well worn, old, and well-understood vulnerabilities that should also by now be well understood by firmware developers.  But in this cross-section of devices, at least, they were not.  And they wrote:  "Based on their research, Synology comes out on top.  Its DS218J," they said, "a device that ISE had also included in the 2013 test, did not show up in any of the broad attack categories, and it had the fewest CVEs at only two:  a session fixation bug in its Photo Station application, and the ability to determine metadata of arbitrary files."  Which, you know, it's like, okay, who really cares about that?



"Moreover, Synology responded promptly to ISE's bug reports, which isn't something ISE was able to say about all manufacturers."  They wrote that some vendors' methods for handling bug reports had improved in the last six years, and others were entirely MIA.  Reporting bugs to several of the vendors was not even possible.  From some of them, ISE got either no cooperation or no response.  Six years ago, back in 2013, none of the manufacturers tested had bug bounty programs in place.  Today, Netgear, Synology, Xiaomi, and QNAP all have bug bounty programs in place.  So yay for that.  That's good news.  And I wouldn't be surprised if they're farmed out to HackerOne, which manages these on behalf of companies like this.



So what are our takeaways?  ISE's reporting says that, when buying a device, we should look for a history of security vulnerabilities with the vendor, along with how long the vendor takes to fix them.  Those can typically be found by searching the CVE repository.  We should also avoid using the device with the default configuration.  Turn off features that are not needed, especially all remote access features.  And please change the username and password from what its default is.



The report also noted that a significant number of devices are deployed, and then never updated afterwards.  These devices will be vulnerable, as we know, to any publicly disclosed issues, even if patched firmware is subsequently made available, since they're not going to get the advantage of that.  So we need to somehow create some sort of tickler system to remind us to periodically check for updates from that device's vendor and apply them.  Some devices, when you log in, they will now at that time, at the time you log in, that kind of wakes them up, and they'll check for an update and, like, give you a warning flag if there is one, or provide some means for manually checking.



But, for example, now would be a good time, like take this opportunity while we're talking about it on this podcast to go check to see if your router has newer firmware because it would be a good thing to update it, if it has.  I mean, there's  reason they updated it.



The problem is these are by their nature set-and-forget applications.  Unfortunately, they are too often set and forgotten.  And I'll note that my own advice, and I take my own advice, as I've mentioned previously, is to use a pfSense-based firewall router out on the edge.  pfSense firewall routers run FreeBSD.  They are turnkey.  They have a beautiful web interface, which should not be over anyone's head.  I mean, they are very powerful.  There's a lot that they can do.



But it is kept current, and it is updated as needed, and it offers every feature you can imagine:  OpenVPN.  OpenSSH.  It has mind-blowing NAT capabilities so you can do things like port shifting in order to get around your ISP that is blocking ports that you'd like to be able to get to remotely.  Anything you could ask for.  It's 100% open source and free.  So you could easily install it in an older machine that's got a couple NICs that you're no longer using.



My favorite drop-in turnkey box is a little Netgate SG-1100 that I've mentioned before, which contains three NICs built in.  So you have a WAN and two LANs.  That allows you to easily create a truly isolated dual LAN environment with complete network isolation, and then you're able to like, for example, bridge ports only where you want to.  For anyone who's interested, I have a link in the show notes.  It's not cheap.  It's pricey at $180, compared to less expensive hardware that you can set up for yourself if you want to.  pfSense.org has a list of all of the hardware that they run on, and they note there that, as pfSense is based on FreeBSD, its hardware compatibility list is the same as FreeBSD's.  The pfSense kernel includes all FreeBSD drivers, and the current release of pfSense, which is 2.4, is based on FreeBSD 11.2, which is current.



So anyway, my feeling is - in fact I mentioned, Leo, in our podcast in the future next week, that I have an ASUS router at one location which is doing NAT, and I have it behind a little pfSense firewall because I want all of the fancy features that it offers.  And so even if that ASUS router had some security vulnerabilities that existed between the time they were found and published, and I updated its firmware, I've got pfSense as the first device that is on the edge, the public-facing device that deals with the Internet, and I feel very secure with that.  So anyway, I just think that's something for our listeners to consider is putting something really strong as the device on the edge.  And it doesn't get any better than things running pfSense.



I mentioned the need to go to Chrome About.  Chrome got an emergency update on Wednesday.  Let's see, that would be Wednesday the 18th of September.  They updated to 77.0.3865.90.  So you want to make sure you are at least at 77.0.3865.90.  Google noted in their Chromium blog, under their release notes, they said:  "Access to bug details and links may be kept restricted until a majority of users are updated with a fix.  We will also retain restrictions if the bug exists in third-party libraries that other projects similarly depend upon, but haven't yet fixed."  I guess that would, for example, be - is it Chredge, the Chromium version of Edge?



LEO:  That's what Mary Jo and Paul call it, yeah.  The new Edge, yeah.



STEVE:  That's right, the one we want to be using Edge.  So in their blog post they said:  "Google has released an urgent software update for its Chrome web browser and is urging Windows, Mac, and Linux users to upgrade the application to the latest available version immediately.  It started rolling out to users worldwide Wednesday, containing security patches for one critical and three high-risk security vulnerabilities, the most secure of which could allow remote hackers to take control of an affected system."



LEO:  Yikes.



STEVE:  Yes.  But that is with doing nothing.  As I noted from their statement above, Google is choosing to withhold details and keep them close to the vest for the time being to prevent hackers from exploiting them and to give most users time to run Chrome and have itself update.  And again, I'll note that mine didn't.  So I don't know what's going on.



LEO:  But you were manually able to do it.



STEVE:  Yes.  All I had to do was go to About > Help, or Help > About.  But when I did that, I saw that it said, oh, hold on a second, and then immediately...



LEO:  Yeah, you have to restart.  Or did it...



STEVE:  Yes.



LEO:  Oh, it hadn't even downloaded it.  Oh, that's...



STEVE:  Yes.  It hadn't downloaded it.  So it downloaded it, and then it said, okay, we're going to do a restart, and I said yeah.  And it did, and then I was up to the latest.



LEO:  Do you leave Chrome running in the background?  You don't use Chrome, do you?



STEVE:  No, no, I don't.



LEO:  Yeah, so it couldn't update until it ran.



STEVE:  Well, but I had run it.  I had it running, and it had been running all morning, and it had not done that.



LEO:  Oh.  Oh, it had not.



STEVE:  Yeah.



LEO:  Well, I'm just looking at mine, and it's updating to 76, which is one behind; right?



STEVE:  Yeah.  You want to be at 77.



LEO:  Yeah.  But I don't keep Chrome running in the background.



STEVE:  And nor do I.



LEO:  So it's going to download, presumably to 76, and then update for 77.  I have "automatically update turned on."  So I don't know.



STEVE:  Yeah, they're being a little lazy about it for some reason.  I mean, it's the number one browser in the world.  And so you can imagine that the update burden is pretty significant.



LEO:  Yeah, oh yeah, yeah.



STEVE:  I mean, it'd be like a DDoS on Google if they tried to have everybody update at the same time.



LEO:  Well, it looks like they are because this is unusually slow just downloading 76.



STEVE:  And you have a good speedy connection.



LEO:  I have a gigabit connection.



STEVE:  That's right.



LEO:  It should be all right here.  Actually, yeah, gigabit because of the card; 10 gigabits actually to the outside world.



STEVE:  Yeah.  So it's the sending end that is the problem.



LEO:  Yeah, must be.



STEVE:  So there were four problems.  They were all Use After Free vulnerabilities in Chrome, meaning that there was some way to use the buffer that had been released from Chrome's memory and could be leveraged to create a remote code execution.  Google has paid out a total of $40,000 in rewards to one of the discoverers, Man Yue Mo at Semmle for both of the vulnerabilities, two of the four.  And then $20,000 each to discoverers of the other two.  So it's nice.  They paid out $20,000 per bug in their bounty program.



And they do note that successful exploitation of these vulnerabilities only requires that a specially crafted web page be displayed.  They do not require any user interaction.  That would mean that a malicious ad that was displayed on an otherwise good website could run code on a user's machine.  So we understand why Google is saying, yeah, let's get this pushed out immediately, and why they're not talking about what they've done.  They are just making the patch available.  And I imagine, given what you and I are experiencing in terms of updating, they're going to be quiet for quite a while, given how bad this thing is.  I'm sure the bad guys...



LEO:  Yeah.  It's finished now.  It was on 76.  It's now at 77.



STEVE:  Yay.



LEO:  So it took maybe five minutes, yeah.  Worth doing.



STEVE:  And it took you doing it; right?



LEO:  Yeah.  But I hadn't run Chrome in ages.  I'm a Firefox guy, thanks to you.



STEVE:  Yay, we got you to Firefox.



LEO:  You got me.  You got me, baby.



STEVE:  So when I was going through - I have an outline of, like, stuff where I organize and pull all of the show notes together.  I use a program I mentioned before that I really love.  It's probably now in abandonware.  It's called "Thought Manager Desktop."  Thought Manager was an app - are you sitting down, Leo? - for my Palm Pilot.  And it was a...



LEO:  Do you still have a bunch of Palm 7s in the freezer?



STEVE:  I sure do.



LEO:  Oh, my god.  Some archeologist is going to find those and really wonder.



STEVE:  Yeah.  You know, once upon a time I thought, well, I love them, so I need to go back to them.  But of course that was before the smartphone happened, and that's pretty much...



LEO:  Yeah, yeah, everything changed, yeah.



STEVE:  Yeah.  I sure did like the way that handwriting recognition worked.  Jeff did just a beautiful job, inventing a slightly modified...



LEO:  Once you learned Graffiti, you could do it so fast.



STEVE:  Yup.  Yup.



LEO:  It was highly accurate, too.



STEVE:  Better than keyboards now on our smartphones, I think in many instances.  Anyway, so I had this outline.  There was a Windows version called Thought Manager Desktop that would synchronize with the Palm.  And I just fell in love with it as an outliner.  I'm a person who loves outlining.  For me, the ability to move things around and organize them, I mean, I was using John Friend's outliner back in the day under DOS.  It was PC something.  Maybe it was PC Outline.  Anyway, the point is that I found, I stumbled upon three SpinRite notes that I don't know I had ever shared because normally I delete them after I put them into a show.  And so I don't know if I did or not.  So I'll share them now.



One was from Scott in Woodland, California, whose email was alumni.ucdavis.edu.  And the subject was "SpinRite Guide for New Hard Drives."  He said:  "Hey, Steve.  I've heard you speak briefly about running SpinRite on brand new hard drives, but what metrics should we be looking at to determine if the drive is healthy or should be returned?"  That's an easy one.  Run SpinRite, probably on Level 4.  At least on Level 2.  Level 4 is a little more rigorous.  And maybe after an hour or two switch the UI to the SMART page.



SpinRite monitors SMART on the fly.  There are bar graphs, horizontal bar graphs which start out at 100% on the various parameters that SpinRite reports.  No red should be showing.  As SpinRite is running, if the SMART data becomes distressed and depressed, then SpinRite will shorten the green, revealing some red.  And certainly a brand new hard drive should not be showing it's relocating sectors.  It should not be showing that it's struggling to read.  It should not be showing that it's got an excessive number of seek errors.



Those are the three things we generally see in old drives when SpinRite is running on them, when they're beginning to send up smoke signals.  Hopefully not actual smoke.  We'll get to that in a minute because one guy in Yorba Linda had that happen, apparently.



So anyway, so that's what you want to look for is SMART will show you if the drive is being used.  And that's the other thing is SMART data, what we've noticed is, if you look at the SMART data immediately upon starting SpinRite, it'll look all fine.  Give it an hour or two, then check it out and see if it still looks okay because it's only when the drive is being asked to do things that it's able to realize it's having trouble.  And then SMART says, ooh, ouch, we have a problem.  The SMART data tends to be self-healing, that is, it shows recent history, but not forever history.  And so, if you aren't running SpinRite, a few months later you'll find the SMART looks good again because you're not asking much from your drive.  So it's important to be asking something from your drive.



Speaking of smoke signals, Jeff Gros in Yorba Linda, his subject was "SpinRite is too hot to handle."  He said:  "Hey, Steve.  I'm building a Synology 1815+.  I bought six 6TB HGST 7200 RPM drives..."



LEO:  Holy cow.



STEVE:  Yeah, he's building a big monster, "with 128MB cache to run in SHR-2, essentially RAID 6" - which of course is the way I run also - "and a 240GB Kingston HyperX Savage SSD for the Synology to use as a cache.  As usual, I run SpinRite Level 4 on all spinning drives and Level 1 on the SSD before putting them to use.  My resident SpinRite machine is a 2000s era machine which now runs Linux.  However, because this machine is a tower, and the SATA cables are near the top, I have to make my own tower out of books and boxes to get the drive to reach the cables.



"I made the tower out of some boxes with some computer paper toward the top.  However, I was worried about the computer paper getting hot and starting a fire.  I had some antistatic bubble wrap, so I put this directly under the drive.  If it gets too hot, it will just melt the plastic.  What could go wrong? Perfectly safe..."



LEO:  Whoa.



STEVE:  He says:  "Well, you can probably see where this is going.  When I came back to check on the drive a few hours later, SpinRite greeted me with a red screen of death.  SpinRite had halted because the drive was too hot.  The screen showed 133 degrees Fahrenheit."  Then he says:  "Not sure if SpinRite dynamically updates this screen" - it does - "or if it was the last reading before halting."  He says:  "I immediately turned off the machine and detached the drive.  Boy.  It was hot to the touch.  I cooled it as much as possible with some compressed air and let it sit.  Now I know why you have heat sinks on your hard drives in that new machine you built.  Drives need to breathe.



"The next day I tried again, but gave more thought to my tower, no flammables and no plastic.  I also added a desk fan that I use during soldering.  Net effect was about 10 Fahrenheit with the fan on full blast, according to SpinRite."  Does he mean 10 Fahrenheit?  Maybe he means...



LEO:  A hundred.  Maybe a hundred.



STEVE:  Or a 10 Fahrenheit gain over...



LEO:  Oh, yeah, maybe that's it, yeah.



STEVE:  Yeah.  He says:  "The drive seemed to have no memory of going to hell and back."  He said:  "Steve, thanks for saving my $250 drive from the stupidity of this now humbled SpinRite user.  With your help, I now have the Synology up and running."  Oh, and he says:  "Also, thanks for your recent tip of Afraid.org and the Hover CNAME for DDNS.  I'll be trying that out soon enough.  Thanks.  Jeff."  So anyway, Jeff, thank you for the report, in case we didn't talk about it before.  And I don't remember ever sharing that story, that harrowing story.  So I'm glad I have.



And finally, John Fletcher in Nottingham, England, regarding an SSHD drive.  He said:  "Having just installed a new Samsung 1TB SSHD with 32GB NAND cache, my thoughts turned to what SpinRite  would actually be testing on Level 4, the memory or the platters?"  He says:  "I've been listening to Security Now! since the first episode.  The first hundred" - ooh.  Get this, Leo.  "The first 149 episodes I listened to in the hospital in one go as I had to remain awake for 24 hours after eye surgery and stay in for a few more days."  In other words, he listened to the first 149 episodes back to back in one shot.



LEO:  I'm more impressed by the fact that Security Now! keeps him awake because I think it puts a lot of people to sleep.  I'm very impressed.  It's not the first thing...



STEVE:  Well, if you're sufficiently geeky...



LEO:  Yeah.  It's keeping the brain alive.  Wow.



STEVE:  Anyway, John, it's been a couple years, I think, since you wrote this.  But for what it's worth, SpinRite is smart about NAND cache.  It is able to shut that down on the hard drive in order to bypass it and actually test the platters themselves.  So not to worry.  SpinRite does what you would want it to do when testing a hard drive that is behind a cache.



LEO:  Yeah.



STEVE:  And on that note, Leo, we are exactly on schedule for our third and final break.  And then we're going to talk about what was discovered by processing the massive common vulnerabilities and exposures database to reveal the top 25 most repeated bug classes.



LEO:  Cool.  Can't wait.  All right.  Speaking of bugs and flaws and security, the top 25. The big 25.



STEVE:  So we are all familiar with the CVE, the Common Vulnerabilities and Exposures.  We talk about them pretty much every week.  It turns out there's also, produced by Mitre.org, is a CWE.  It's the Common Weakness Enumeration.  Common Weakness Enumeration.  They wrote:  "The Common Weakness Enumeration Top 25 Most Dangerous Software Errors" - and Leo, there's no surprises here, but it's interesting - "is a demonstrative list of the most widespread and critical weaknesses that can lead to serious vulnerabilities in software.  These weaknesses are often easy to find and exploit.  They're dangerous because they will frequently allow adversaries to completely take over execution of software, steal data, or prevent the software from working.



"The CWE Top 25 is a community resource that can be used by software developers, software testers, software customers, software project managers, security researchers, and educators to provide insight" - oh, also podcasters - "to provide insight into some of the most prevalent security threats in the software industry.  To create the list, the CWE team used a data-driven approach that leverages published common vulnerabilities and exposures, the CVE data, and related CWE mappings found within the National Institute of Standards and Technology, the NIST National Vulnerability Database, the NVD, as well as the Common Vulnerability Scoring System" - boy, we're in acronym land here - "the CVSS scores associated with each of the CVEs.  A scoring formula was then applied to determine the level of prevalence and danger each weakness presents.  This data-driven approach can be used as a repeatable scripted process to generate a CWE Top 25 list on a regular basis" - what fun - "with minimal effort."



So the number one ranked most, what is it, Common Weakness Enumeration most widespread worrisome class of bugs, to no one's surprise they name Improper Restriction of Operations Within the Bounds of a Memory Buffer.  In other words, the memory overrun, buffer overflow, whatever you want to call it.  It was to give some people a sense, because this falls off relatively quickly, this was at 75.56 was the score.  So a 75.56 score for basically buffer overrun.



Number two, at 45.69, so look at the size of that drop, even in second place, meaning that buffer overruns, they still rule the roost.  Number two, Improper Neutralization of Input During Web Page Generation, in other words, cross-site scripting.  So Improper Neutralization of Input During Web Page Generation, so that is taking user input and, as they call it, "neutralizing it," meaning sanitizing it, purifying it, maybe just rejecting it, depending upon what it's got.  But that's still, that's number two on the hit list of problems.



More generically, and therefore at about the same level, 43.61, they call Improper Input Validation.  So again, it's a problem that developers continue to have, which is assuming that users are going to behave themselves, assuming that everybody using a web form wants it to work the way it was designed to.  Well, by definition, bad guys want to break that assumption.  So it is really important that input gets validated.  Fourth is Information Exposure, sort of generically.  And that's got a score of 32.  Out-of-bounds Read comes in at about 27.



Number six, Improper Neutralization of Special Elements used in an SQL Command, in other words, SQL Injection.  So that's the formal term for SQL Injection, Improper Neutralization - there again, they like the term "neutralization" - of Special Elements used in an SQL Command.  So of course we know what that means.  It's too often the case that the system will interpret as an SQL query something - or an SQL command, something that the user inputs.  So that's sort of a subset of not doing proper input validation.



And then coming in seventh is those three problems that we just had with Chrome, Use After Free problems, where you still have a means of having a handle to a buffer which has been released to the operating system, and you're able to get up to mischief with that.  Then number eight - that was at a rank of 18, or seventh in ranking, but a score of 18.  Then we have the Integer Overflow or Wraparound, which is now - so that's less, this is less of a public exposure problem and more of an internal bug problem, where as a consequence of an integer wrapping around from its maximum value to its minimum value, which happens where you're forced to store integers in a fixed-sized field, that's a bug, and it can be leveraged to create problems.



Number nine, Cross-Site Request Forgery, CSRF, which is another way of abusing web pages on web-facing applications.  Number 10, Improper Limitation of a Path Name to a Restricted Directory.  And then that's sort of their formal term for path traversal.  And we've often talked about the problem that that creates for people.



Number 11, and here again we have neutralization:  Improper Neutralization of Special Elements Used in an OS Command.  So that's OS Command Injection.  Once again, doing something where, for example, that was the bug in the Exim server that took a week to exploit was it was possible to use an exec function to get the Exim server to pipe OS commands where the field itself was like a reply-to field in email that should never have been executable.  So that would be improper neutralization of special elements used in an OS command.



Twelfth position, Out-of-bounds Write, where you are writing where you're not supposed to, one way or another.  Thirteen, kind of generic, Improper Authentication.  That would be the problem that we've been having with the RDP servers recently because they were saying, hey, we're here, waiting for someone to connect on port 3369, you know, or was it 3389?  Anyway, I think it was 89.  And so unfortunately there was a bug in authentication that allowed anybody to do that.



Null Pointer Dereferencing, that typically causes a crash.  The idea is that a pointer is something which points to something else.  And so dereferencing a pointer is the act of accessing the thing that the pointer points to.  It's often the case that pointers have a null value in them, unfortunately.  And so when you try to access the thing the pointer points to, you're accessing address zero, which is almost always a bad idea and causes bad stuff to happen, crashing and so forth.  So that's ranked 14.  Although now we're down to a score of 9, where we started at 75.  So, or actually 9.74, so about 10.



Fifteenth rank, Incorrect Permission Assignment for a Critical Resource.  Yeah.  So you basically haven't properly restricted access.  Sixteen, Unrestricted Upload of a File with a Dangerous Type.  Yeah, don't do that.  That's not good.  Number 17, Improper Restriction of XML External Entity Reference.  Again, XML gives people a lot of power.  You want to make sure that you've got it under control.  Eighteenth in rank, Improper Control of Generation of Code, in other words, Code Injection.  Use of Hard-coded Credentials.  Whoops.  That's 19th.  Uncontrolled Resource Consumption.  Yeah, that's not good.  That's 20th.



Missing Release of Resource after Effective Lifetime.  So that's probably a problem with garbage collection, where you have not released something after you no longer need it, typically considered garbage collection in automatic languages.  And that can be a problem.  Untrusted Search Path is in 22nd position.  Deserialization of Untrusted Data.  Whoops.  It's interesting that that's as low as it is.  We've talked about this.  Deserialization is the process of basically interpreting the serialization of a complex object like a JSON blob.  JSON can be serialized.  And when it's deserialized back into the data structure, normally the interpreter trusts the serialization.  But that trust can be abused.  And deserializing untrusted data has been a source, as we've talked about before, of all kinds of exploits.  Twenty-fourth position is Improper Privilege Management.  That's never good.  And then 25th, Improper Certificate Validation.  You want to validate your certs, otherwise what's the point of having them?



And then they weren't happy, as I mentioned at the top of the show, stopping at just 25.  So they had a section:  Weaknesses on the Cusp.  And they said:  "As in years past, the CWE team feels it's important to share additional CWEs that scored just outside of the top 25.  Per the 2019 scoring formula against the NVD dataset, these weaknesses were potentially not severe enough, or not prevalent enough, to be included in the 2019 list.  Developers that complete mitigation and risk decision-making on the 2019 CWE Top 25 may want to look for these other weaknesses potentially present in their software.  For these reasons, users of the 2019 CWE Top 25 should seriously consider including these additional weaknesses in their analysis."  And I'm going to run through these just very quickly.  Here we have the Loop with Unreachable Exit Condition.  And Leo, of course that's more colloquially known as the "endless loop."



LEO:  Endless loop.



STEVE:  The infinite loop.  Yes.  And Windows users are used to seeing the "application not responding" message when that happens.  We have Insufficiently Protected Credentials.  Incorrect Type Conversion or Cast, yeah, that's actually - we've had a couple of security problems over the years where that was the underlying vulnerability.



LEO:  I'm surprised that's not higher because casting...



STEVE:  I am, too, yeah.



LEO:  Casting is always a problem, yeah.



STEVE:  Yeah.  I am, too.  Concurrent Execution using Shared Resource with Improper Synchronization.  That's the formal term of the Race Condition, which is also something that happens all the time.  Server-Side Request Forgery, as opposed to client-side.  But that's there.  Double Free.  You only want to free your memory once.  You don't want to free it twice because after the first free it's free.



LEO:  There's nothing there, baby.



STEVE:  When you try to free it again, all kinds of bad things are going to happen.  Then we've got the URL Redirection to an Untrusted Site, known as Open Redirect.  That's not good.  Incorrect Authorization.  Missing Authorization.  That's worse than incorrect.  Inclusion of Sensitive Information in Log Files.



LEO:  Oops.



STEVE:  Yeah, that's bad.  Missing Authentication for Critical Function.  Session Fixation.  Inadequate Encryption Strength.  Allocation of Resources Without Limits or Throttling.  Ooh.  And a Reachable Assertion.  And I have no idea what that is, a reachable assertion.



LEO:  That's from unit testing, maybe, where you assert, try - you try code, and then if it's...



STEVE:  Yeah.



LEO:  Yeah, I don't know.  That wouldn't be necessarily a bug.  It would just be a test that...



STEVE:  Yeah, I know.  May all your assertions be reachable.



LEO:  Yes.  Huh.  They have a much larger description of each of these.  It says this product contains an assert or similar statement that can be triggered by an attacker, which leads to allocation exit.



STEVE:  Ah.  Oh, a reachable.



LEO:  Reachable.



STEVE:  Ah, a reachable...



LEO:  So in other words, your tests should not be reachable.



STEVE:  Yes.  Turn those off when you ship the code.



LEO:  Yes.  While assertion is good for catching logic errors and reducing the chances of reaching more serious vulnerability conditions, it can still lead to a denial of service if you can get to it.  Wow.  That's good.  Every programmer should read this and memorize it and put it on the wall.



STEVE:  Yeah.



LEO:  Because these are just easy mistakes to make, and it's good to be thinking about these.



STEVE:  Well, and it's so nice to, I mean, like when you write some code, then just sort of go over the list and check it in your head.  You know, like just sort of cross-check what you write with this list.  And if you do, the problems you have will not be on the list.



LEO:  Yeah.  Well, also it's a great way to debug, like did I do this?  Did I do this?  Did I do this?  That's great.



STEVE:  Yeah.  It's a checklist.



LEO:  Should be in your head.  I love it.  That is really actually a great list.  I'm going to print that out, put that on the wall.  Very useful stuff.



STEVE:  Well, and what's interesting, too, is it wasn't some guys that sat around and made up the list.  It wasn't a committee.



LEO:  This is real.



STEVE:  It was 100% data driven.  They analyzed, I mean, they used a scripting process to analyze the preponderance of CVEs.  And what came out of it has no surprises.  It's exactly what we would expect.



LEO:  Yeah.  Yeah.  Not a surprise at all.  Which even underscores further the truth that every programmer should be well aware of these because these are mistakes that keep getting made.  Wow.  Great.  Good stuff.  So somebody in the chatroom was saying does Mitre still maintain the CVE?



STEVE:  It's still there.  Anybody is able to apply for one in order to actually get a designation.  But it's at Mitre.org is where that lives.



LEO:  Okay.  He thought they'd moved on.  Somebody else had acquired it.  But maybe not.  Steve, what a good one.  A great - this is an evergreen, too.  Those are bugs everyone should know and memorize.  And we all do.



STEVE:  Unfortunately, they will still be relevant five years from now.



LEO:  Yeah, yeah, it's evergreen.



STEVE:  Because they show no sign of actually going away anytime soon.



LEO:  No, no, no.  We do this show normally - we're doing it Saturday because Steve's about to head out to Dublin and Gteborg, Sweden.  He's going to have a great time showing off SQRL.  By the way, I haven't mentioned this, but anybody who downloads the show notes already knows it, but the new end of show notes logo is a little SQRL.  It used to be a 30.  But now it's a little SQRL.  I like that.  Do you put that everywhere?



STEVE:  Yeah.



LEO:  It's on your mic flag.  It's everywhere.



STEVE:  I don't yet have a tattoo on my butt the way you do, Leo.  



LEO:  That's coming soon.  There's some good people there in Gteborg who could do it.  If you get a chance to see Steve there, or in our upcoming event in Boston, you've got to.  We'll be talking about SQRL in Boston, and I know he's going to be - that's the whole presentation in Sweden and in Ireland.  But of course you can come back here every Tuesday once Steve gets back.  So you're going to be gone - this coming Tuesday you're going to be gone.  The following Tuesday you're going to be gone.  We'll be back doing the show October 8th.  But we will not miss an episode because that's Steve's...



STEVE:  Well, for what it's worth, I just should mention that the show notes will be delayed because I'm not making any attempt to do them long distance.  I'm shutting everything down.  I'm locking down my security.  I'm just not going to attempt to do that remotely.  So I will catch up with all the show notes.  I'm sure that - I talked with Elaine about this.  She's going to grab the podcast and transcribe it.  So everything will be waiting for me when I return, and I will then catch up.



LEO:  Nice.  You go to GRC.com to find all that, the Gibson Research Corporation.  That's where Steve hangs his hat on the Internet.  It's also where SpinRite lives, Steve's bread and butter, the world's best hard drive maintenance and recovery utility.  Get it at GRC.com.  Everything else is free, including ShieldsUP!, including SQRL, including everything.  And it's a lot of fun.  It's one of those sites you go to and you end up five hours later going, where did the time go?  There's so much stuff there.



GRC.com.  Look for 16Kb or 64Kb audio versions there, the transcriptions.  We have audio and video at our site, TWiT.tv/sn.  And of course you can always subscribe on your favorite podcaster.  That way you get it automatically.  Just subscribe to Security Now!, and you don't have to worry about when we're recording it.  You will get it at an appropriate moment in the space-time continuum.  Your fabric will not be rent.



STEVE:  Yes.  We will straighten all that out for you.  



LEO:  Yes.  All the wrinkles in time, flattened.  Steve, have a great trip.  Man, I'm not going to see you till the other side.  I'm sure you're going to have a blast in Ireland.  I'm sure there'll be a few people trying to buy you a drink at the pub there.



STEVE:  I've heard that happens.  



LEO:  I think it might.  Think it might.  But we'll see you next time on Security Now!.



STEVE:  Okay, my friend.  And I'll see you in Boston.



LEO:  Bon voyage.  Yeah, I'll see you in Boston.



STEVE:  Bye.



Copyright (c) 2019 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#734

DATE:		October 1, 2019

TITLE:		The Joy of Sync

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-734.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  With this week's "The Joy of Sync" podcast, we focus upon the latest state-of-the-art secure solutions for cross-device, cross-location device synchronization.  But before we delve into that abyss, we'll update on Mozilla's recently announced plans to gradually and carefully bring DNS-over-HTTPS to all Firefox users in the U.S.  It turns out it's not quite the slam dunk that we might imagine.  We'll also check in with the EFF to see what they think, and remind our listeners about the 100% free VPN offering coming from our friends at Cloudflare.



SHOW TEASE:  Hey, everybody.  It's time for Security Now! in a special episode.  Folks, this is one you're going to want to download and keep.  You're going to want to get the show notes, too.  Steve Gibson - we'll talk a little bit about some new stuff in Firefox and so forth.  But the bulk of this show is about Steve's research on file syncing, something more secure, more private than Dropbox or Google Drive or OneCloud/Drive or any of those.  Steve says he's found the answer.  "The Joy of Sync" is next.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 734, recorded September 14th for airing October 1st, 2019:  The Joy of Sync.



It's time for Security Now!, the show where we cover your security and privacy and, today, data synchronization with this guy here, Steve Gibson of GRC.com.  Hi, Steve.



STEVE GIBSON:  Leo, great to be with you once again for, well, in this case we're actually - we're sneaking in an early record.  So we do not have lots of security news because while we're recording this I have no idea what has been going on recently.  However, I do promise that the next podcast, 735, is going to be a big mega catch-up, and we will catch everybody up on anything that happened while Lorrie and I were traveling around Europe, meeting with OWASP groups and telling them about SQRL.



LEO:  Nice.



STEVE:  Which is actually where I will be on October 1st, which is nominally the date of this podcast.  And of course you and I are going to be, two days later, on Thursday, in Boston for the LogMeIn/LastPass identity over the Internet panel.



LEO:  Yeah, this is going to be fun.  I'm really looking forward to that panel.  And I think it's probably too late to get tickets to that now.  But just in case, if you want to see if there's any tickets left, twit.to/unlocked is the short URL.  It is this Thursday.  You need to be in Boston Thursday, 3:45 p.m., at the Intercontinental Hotel.  Steve and I and Bill Cheswick and Gerry Beuchelt, who is the CISO for LogMeIn will be talking about the problem with passwords.  And I suspect we'll get a little SQRL in there.



STEVE:  I think maybe we will.  And we will definitely - apparently there will be sort of a cocktail hour.



LEO:  Yeah, there's a reception afterwards.  So we're going to be onstage for about an hour and a half.  We'll take questions for half an hour.  So it'll be an hour of us talking, half an hour of questions.  And then we'll be around for an hour or so after that to talk individually with people.  And that's going to be, I think, the highlight of it.  I can't wait.



STEVE:  Very fun.  I really enjoyed that when I did the OWASP Orange County group.  I mean, it was basically a roomful of Security Now! listeners, so we had a lot of fun.



LEO:  I think that's what it's going to be this time.



STEVE:  We also had people bringing their SpinRite CDs for autographs.



LEO:  Oh.



STEVE:  So I autographed a lot of SpinRite CDs.



LEO:  Bring your SpinRite CDs, kids.



STEVE:  Lorrie said, "People still use CDs?"  I said, well, it's not easy to autograph a thumb drive.  You know, there's just not  really enough room there to...



LEO:  They are an excellent substrate for a Sharpie.  They really do [crosstalk].  So this week we wanted to do something that was a little more evergreen because we are recording it a couple of weeks ahead of time.  But this is a project you've been working on for some time.



STEVE:  For several months now.  So this week's podcast we titled "The Joy of Sync."  And I talked about this a couple months ago, where I was struggling with operating in two different physical locations.  I've maintained where I'm podcasting from.  Everybody knows that hasn't changed in 15 years.  It's the way it always has been.  But I'm now spending my evenings in a different location with Lorrie.  And we normally both - she's busy in the evening, and so I get another burst of opportunity to work.  So I was copying stuff onto a thumb drive and using sneakernet, even in this day and age, or sticking things in a Dropbox folder and then bringing them down.  But basically I didn't have automation for keeping things dynamically synchronized.  And so I decided finally, okay, this is crazy.  Let's use some technology here, Gibson, and figure out what we should do.



And so you and I talked about this briefly, about ownCloud, and you pointed me at Nextcloud, and I had sort of settled on Dropbox.  But I was uncomfortable with the fact that it wasn't encrypted storage.  I mean, it's encrypted at Dropbox, and it's encrypted in transit.  But if you read the fine print in the terms of service, it says that they're able to look at your files to verify that it doesn't violate their terms of service.  And the other cloud providers do the same thing.  So we're going to get to all that, and also to the solution that I have found.  I have something, actually two different tools and a trick for using one of them, that I am really happy with.  It's completely solved my problem.  It's TNO, which of course in fact it was for cloud storage back in the Jungle Disk days that we coined that term, that acronym, TNO.



LEO:  Oh, really.  I didn't realize that.  Okay.



STEVE:  That's what it was.  It was Trust No One, the idea being to encrypt it on your machine before it leaves.  And of course that creates some challenges because one of the things that many of our listeners have said they want the ability to do is to selectively share specific files with other people.  But if everything that's stored in the cloud is encrypted, then how are you able to offer, like selectively let people have specific things?  And then of course there are people who don't have an interest in cloud stuff, but they still want to do interdevice synchronization.  Anyway, I have two tools that completely offer solutions to all of that.  Anyway, so we're going to talk about that.



But I do want to, because this is still Security Now!, take a look at Mozilla's recently announced plans to gradually and carefully bring DNS-over-HTTPS to all Firefox users in the U.S.  It turns out that it's not quite the slam dunk that we might imagine.  And I also want to check in with the EFF to see what they think about this, and then also remind our listeners about the 100% free VPN offering coming from our friends at Cloudflare.  So do a little bit of Security Now! business, and then we're going to plow into how you achieve The Joy of Sync.



LEO:  The Joy of Sync.  And of course we have a Picture of the Week because you wouldn't go a week without a picture.



STEVE:  No, we can't.



LEO:  All right, Steve.  Let's, well, actually, should we do the Picture of the Week before we get syncing?



STEVE:  I think we should.  And this is a little gruesome, actually, when you look at the second half of the picture.  But it's been in my archive of photos for a while, so I thought it would be sort of fun.  This is the CAPTCHA, the "I'm not a robot."  And the upper half of the picture says, "Are you a robot?"  And it shows a human finger touching the "I'm not a robot" checkbox to assert that fact.  And then the gruesome part is the second half of the picture.



LEO:  Oh.



STEVE:  Where we encounter that disturbingly lifelike Boston Dynamics robot holding, unfortunately, a severed human hand, and in its other hand the phone showing that the checkbox of "I'm not a robot" has been checked.



LEO:  Of course, it isn't a fingerprint, so this doesn't really scan.



STEVE:  Doesn't quite capture the [crosstalk].



LEO:  Sometime I would love to spend some time on this "I'm not a robot" reCAPTCHA because I understand how it works; but sometimes you click it, and it goes, okay, I believe you, and sometimes you have to look at road signs for hours.



STEVE:  Yes.



LEO:  And it's an interesting thing that Google's doing with this.  And so at some point it would be kind of fun to talk about that.



STEVE:  Yes.  It uses basically a deep reputation network in order to see if it knows your browser, if it knows your IP.  I mean, Google's watching us.  And so...



LEO:  It knows who you are.



STEVE:  Yeah, it's even like what have you been doing recently before you claim not to be a robot, or does this suddenly just come out of nowhere by surprise.



LEO:  Which shows ironically that a robot could click it, as long as it had your phone that you'd been using up to the point where it ripped your head off.



STEVE:  Yes, exactly.  Exactly.  So I did want to talk a little bit - we've been talking recently about DNS-over-HTTPS.  A month or two ago we talked about the setting that was not yet the default for Firefox, which Mozilla is beginning to experiment with.  And I remember that the default provider - the idea, of course, is that, rather than sending DNS queries out over UDP to your regularly scheduled DNS provider, which is more often than not your ISP, people's home routers will use DHCP to obtain, not only an IP address, its public IP for itself, but also the IPs, typically, of your local ISP, the idea being that DNS servers that are near you are going to be the fastest to respond.



DNS servers are caching, so if anybody in your area had looked at any of the same sites that you go to, and if you look at the distribution, lots of people are going to Google, lots are going to Facebook.com and so forth, then it's very often the case that, if your computer doesn't already know the IP address for www.facebook.com, that very short distance away your ISP's DNS server will.



So anyway, of course the problem with that is that UDP and DNS are unencrypted by default, and that represents a privacy concern for people.  So what Firefox has been experimenting with is bringing up an HTTPS tunnel so that all of your web browser's DNS queries, instead of going to that local ISP, will go to a DNS server at the other end of the tunnel, which at this point by default is Cloudflare because they offer, as it's called, a DoH, DNS-over-HTTPS service.



LEO:  Unless you just do your own.  But I don't do my own.  I use Cloudflare.  I mean, you're basically sending all that traffic to Cloudflare, and I think they're doing it for a reason.  They want to know all that stuff; right?



STEVE:  They really do seem to be good guys.  I mean...



LEO:  I don't think they're selling it, but I think they're probably using it to make their service better, that kind of thing.



STEVE:  They may be.  I doubt that they're monetizing.



LEO:  No.



STEVE:  I would be very surprised if they were.  So what was interesting, and because there is more technical depth to this, is I picked up on a posting, a blog posting by Selena Deckelmann, who's Mozilla's senior director of Firefox Engineering.  She had some very interesting details about their experiments that I knew our listeners would find fascinating.  Her blog posting was - it was from September 6th - "What's Next in Making Encrypted DNS-over-HTTPS the Default."  So she said:  "In 2017, Mozilla began working on the DNS-over-HTTPS protocol, and since June of 2018 we've been running experiments in Firefox to ensure the performance and user experience are great."  She said:  "We've also been surprised and excited by the more than 70,000 users who have already chosen on their own to explicitly enable DoH in Firefox Release edition."



And, you know, we probably contributed in some small measure to that because we talked a few months ago about where to go, what to flip, and there is a UI panel now in Firefox.  You don't have to dig into that about:config thing.  It's just right there.  And when you turn it on, your DNS queries for your browser, not for your system as a whole, because this is just browser-centric, they're all going to be encrypted.  And I think there is a client for pfSense and other routers which would allow then your entire network to have its DNS handled through DoH.



Anyway, so she said:  "We are close to releasing DoH in the U.S., and we have a few updates to share."  She wrote:  "After many experiments, we've demonstrated that we have a reliable service whose performance is good, that we can detect and mitigate key deployment problems" - and that's interesting because there are some - "and that most of our users will benefit from the greater protections of encrypted DNS traffic.  We feel confident that enabling DoH by default is the right next step.  When DoH is enabled, users will be notified and given the opportunity to opt out."



So of course this is huge; right?  They're talking about flipping the default to DoH, meaning that, as we know, my favorite term is, one of them, "the tyranny of the default," meaning that that's, you know, the default setting is what almost everybody ends up using.  So she said:  "This post includes results of our latest experiment, configuration recommendations for systems administrators and parental controls providers, and our plans for enabling DoH for some users in the U.S.  Our latest DoH experiment was designed to help us determine how we could deploy DoH, honor enterprise configuration, and respect user choice about parental controls."



And of course what that means is that filtering DNS is one of the ways you can keep your family away from bad websites, where "bad" can be selectively or deliberately or to some degree controlled, you know, like what class of sites do you not want people to get to.  And on the enterprise side, there are collisions with, for example, enterprise DNS might define non-public domains, which are of use to the enterprise.  But if you tunnel DNS outside, then public DNS servers won't know about the enterprise private DNS.



So she said:  "We had a few key learnings from the experiment," as she put it.  And there were two she cited.  First:  "We found that OpenDNS's parental controls and Google's safe-search feature were rarely configured by Firefox users in the U.S.  In total, 4.3% of users in the study used OpenDNS's parental controls or safe-search.  Surprisingly, there was little overlap between users of safe-search and OpenDNS parental controls.  As a result, we're reaching out to parental controls operators to find out more about why this might be happening."



Then she also said:  "We found 9.2% of users triggered one of our split-horizon heuristics."  So a split horizon means that you intelligently fall back to a different DNS server based on some heuristics, based on some rules of thumb.  So they were recognizing that there would be a problem if everybody was tunneled without exception.



So she said:  "The heuristics were triggered in two situations:  when websites were accessed whose domains had non-public suffixes" - not prefixes, but suffixes; right?  So like some different, some non-public TLD, Top-Level Domain, like maybe dot IBM or something, for example.  And she said:  "And when domain lookups returned both public and private IP addresses," so the so-called RFC 1918 IP ranges, like 192.168.



So she said:  "There was also little overlap between users of our split-horizon heuristics, with only 1% of clients triggering both."  She said:  "Now that we have these results, we want to tell you about the approach we have settled on to address managed networks and parental controls.  At the high level, our plan is to respect user choice for opt-in parental controls and disable DoH if we detect them."  So that is to say, if you are using a, for example, OpenDNS where you're taking advantage of their parental controls, they will not tunnel.  Firefox will not tunnel in that instance.



LEO:  Ah.  That's smart.  That's good.



STEVE:  Yes, yes.  And she said also:  "Respect enterprise configuration and disable DoH unless explicitly enabled by enterprise configuration. and fall back to operating system defaults for DNS when split-horizon configuration or other DNS issues cause lookup failures."  So, for example, if they're tunneling, and you do ask for johnsmith.mycomputer.ibm, well, that's going to fail because IBM is not a TLD.  So they will detect that and go, whoops.  This is a configuration where something special has been done for this user's DNS.  So then they will back off and disable DNS-over-HTTPS.



So essentially what that means is that what they found was that it was a very small percentage of Firefox users who would need that, and that by enabling it by default - so they will intelligently back off if they encounter that.  But by and large, the huge majority of users won't ever see a problem and will get the benefit of having their browser's DNS queries tunneled to Cloudflare or wherever.



And of course, as you noted, Leo, one of the concerns is that, very much in the same way that a VPN has the effect of concentrating all of your traffic out to a single point, of course, as you noted, whoever it is you choose as your DoH provider is seeing all of your DNS lookups.  So it has that same sort of concentration effect.  On the other hand, many people are more concerned about what their own local ISP is doing with, you know, having advantage of their DNS lookups.  Like, for example, it's considered controversial that you enter, you mistype a domain name, and you get your, like in my case, your local Cox Cable intercept page saying, oh, well, here's some things to consider.  I mean, they take it as a marketing opportunity.



Anyway, so her post continues:  "We're planning to deploy DoH in 'fallback' mode; that is, if domain name lookups using DoH fail, or if our heuristics are triggered, Firefox will fall back and use the default operating system DNS.  This means that for the minority of users whose DNS lookups might fail because of split-horizon configuration, Firefox will attempt to find the correct address through the operating system DNS.  In addition," she said, "Firefox already detects that parental controls are enabled in the operating system; and, if they are in effect, Firefox will disable DoH.



"Similarly, Firefox will detect whether enterprise policies have been set on the device and will disable DoH in those circumstances.  If an enterprise policy explicitly enables DoH, which we think would be awesome," she writes, "we will also respect that.  If you're a system administrator interested in how to configure enterprise policies, please find documentation here."  And she provided a link.  And I have a link to her posting in the show notes, for anyone who wants to follow up.  She said:  "If you find any bugs, please report them here," and another link.



So this was interesting, too.  She said, under "Options for Providers of Parental Controls," she said:  "We're also working with providers of parental controls, including ISPs, to add" - and I thought this was very clever - "a canary domain to their block lists.  This helps us in situations where the parental controls operate on the network rather than an individual computer.  If Firefox determines that our canary domain is blocked, this will indicate that opt-in parental controls are in effect on the network, and Firefox will disable DoH automatically.  If you are a provider of parental controls, details are available here."  And she provided a link.



"Please reach out to us for more information at doh-canary-domain@mozilla.com," so by email.  "We're also interested in connecting with commercial block list providers, in the U.S. and internationally.  This canary domain is intended for use in cases where users have opted in to parental controls.  We plan to revisit the use of this heuristic over time, and we'll be paying close attention to how the canary domain is adopted.  If we find that it is being abused to disable DoH in situations where users have not explicitly opted in, we will revisit our approach."



And then she concluded, saying, under "Plans for Enabling DoH Protections by Default," she said:  "We plan to gradually roll out DoH in the U.S. starting in late September.  Our plan is to start slowly enabling DoH for a small percentage of users while monitoring for any issues before enabling it for a larger audience.  If this goes well, we will let you know when we're ready for 100% deployment.  For the moment, we encourage enterprise administrators and parental control providers to check out our config documentation and get in touch with any questions."



So anyway, I think this is great.  As we noted, and as we know, most users, even Firefox users, just sort of say, okay, I know that Firefox is doing the right thing.  It's not monetizing what I do.  It's protecting my privacy more so than more popular browsers.  So I'm just going to leave it set the way it is.  You know, there are things one can do to increase that, and eventually all your DNS lookups will get tunneled.  And, you know, of course you can turn that off if you'd rather have a more dispersed DNS querying, although I can't imagine trusting anyone more than Cloudflare.  I mean, they're good guys.



LEO:  Yeah.



STEVE:  And I was curious about the EFF weighing in.  They recently posted:  "Encrypted DNS Could Help Close the Biggest Privacy Gap on the Internet."  And they said:  "Why Are Some Groups Fighting Against It?"  And I thought this was interesting.  They wrote:  "Thanks to the success of projects like Let's Encrypt" - which of course we know the EFF was big behind and has succeeded at their goal of providing free encryption for sites that weren't otherwise encrypted, with the recognition that of course it's domain-level encryption, not organization validation, and not extended validation, just domain-level encryption.  But it means that traffic is no longer to a great degree traveling over the 'Net unencrypted.



And the EFF wrote also, thanks to the success of "recent UX changes in the browsers, most page loads are now encrypted with TLS."  They wrote:  "But DNS, the system that looks up a site's IP address when you type the site's name into your browser, remains unprotected by encryption.  Because of this, anyone along the path from your network to your DNS resolver, where domain names are converted to IP addresses, can collect information about which sites you visit.  This means that certain eavesdroppers can still profile your online activity by making a list of sites you visit, or a list of who visits a particular site.  Malicious DNS resolvers or on-path routers can also tamper," they note, "with your DNS request, blocking you from accessing sites or even routing you to fake versions of the sites you requested.



"A team of engineers is working to fix these problems with DNS-over-HTTPS, a draft technology under development through the IETF that has been championed by Mozilla.  DNS-over-HTTPS prevents on-path eavesdropping, spoofing, and blocking by encrypting your DNS requests with TLS.  Alongside technologies like TLS 1.3 and encrypted SNI" - that's Server Name Indication, which is one of the last things that needed to get fixed, which TLS 1.3 does, because even on a secure connection, and even if DNS were encrypted, the browser now indicates what site it wants to connect to so that a multihosted server is able to select the proper security certificate for the connection.  So that was one remaining privacy leakage which is resolved in TLS 1.3.



So they say:  "DoH has the potential to provide tremendous privacy protections.  But many Internet service providers and participants in the standardization process have expressed strong concerns about the development of the protocol."  And of course we covered this, and they remind us:  "The U.K. Internet Service Providers Association even went so far as to call Mozilla an 'Internet Villain'..."



LEO:  Can you believe that?



STEVE:  "...for its role in developing DoH."  Of course they later backtracked on that, saying, whoops, we didn't realize that was going to be such a mistake.  So the EFF says:  "ISPs are concerned that DoH will complicate the use of captive portals, which are used to intercept connections briefly to force users to log into a network, and will make it more difficult to block content at the resolver level.  DNS-over-HTTPS may undermine plans in the U.K. to block access to online pornography."  They said:  "(The block, introduced as part of the Digital Economy Act of 2017, was planned to be implemented through DNS.)"



They say:  "Members of civil society have also expressed concerns over plans for browsers to automatically use specific DNS resolvers, overriding the resolver configured by the operating system, which today is most often the one suggested by the ISP.  This would contribute to the centralization of Internet infrastructure, as thousands of DNS resolvers used for web requests would be replaced by a small handful."  True.  "That centralization would increase the power of the DNS resolver operators chosen by the browser vendors."  Also true.



LEO:  That's a good point, yeah.



STEVE:  Yeah, "which would make it possible for those resolver operators to censor or monitor browser users' online activity.  This capability prompted Mozilla to push for strong policies that forbid this kind of censorship and monitoring."  Well, and think about it.  I mean, if Mozilla chooses you, as they chose Cloudflare, then in return Mozilla can say, hey, Cloudflare, we'd like to promote you, and implicitly your service, but you've got to promise in return no funny business.  And of course Mozilla has a lot of clout because who they aim all of their browsers at is entirely their choice, with the exception of users who manually override.  But as we know, that's going to be vanishingly small percentage.



So the EFF says:  "The merits of trusting different entities for this purpose are complicated, and different users might have reasons to make different choices.  But to avoid having this technology deployment produce such a powerful centralizing effect, EFF is calling for widespread deployment of DNS-over-HTTPS support by Internet service providers themselves."  In other words, let's bring a bunch of services, you know, like OpenDNS and Google and others, so that users will have more choice.  And you can imagine an easy-to-use list where you choose, or maybe even rotate them.  Doesn't always have to be the same one; right?  So there's no reason you couldn't bring up a couple tunnels and just hand out queries in a round-robin fashion to a spread, so you sort of have a spread spectrum DoH in that case.



LEO:  Ooh, that's clever.  What a good idea, yeah.



STEVE:  So they say:  "This will allow the security and privacy benefits of the technology to be realized while giving users the option to continue to use the huge variety of ISP-provided resolvers that they typically use now."  And they said:  "Several privacy-friendly ISPs have already answered the call."  They said:  "We spoke with Marek Isalski, Chief Technology Officer at U.K.-based ISP Faelix [F-A-E-L-I-X] to discuss their plans around encrypted DNS.  Faelix has implemented support for DNS-over-HTTPS on their pdns.faelix.net resolver.  They weren't motivated by concerns about government surveillance, Marek says, but by 'the monetization of our personal data.'"  Which is to say, they want to work against it.



They wrote:  "To Marek, supporting privacy-protecting technologies is a moral imperative."  He was quoted by the EFF:  "I feel it is our calling as a privacy- and tech-literate people to help others understand the rights that GDPR has brought to Europeans," he said, "and to give people the tools they can use to take control of their privacy."  So the EFF concludes:  "EFF is very excited about the privacy protections that DoH will bring, especially since many Internet standards and infrastructure developers have pointed to unencrypted DNS queries as an excuse to delay turning on encryption elsewhere in the Internet.



"But as with any fundamental shift in the infrastructure of the Internet, DoH must be deployed in a way that respects the rights of the users.  Browsers must be transparent about who will gain access to DNS request data and give users an opportunity to choose their own resolver.  ISPs and other operators of public resolvers should implement support for encrypted DNS to help preserve a decentralized ecosystem in which users have more choices of whom they rely on for various services.  They should also commit to data protections like the ones Mozilla has outlined in their Trusted Recursive Resolver policy.  With these steps, DNS-over-HTTPS has the potential to close one of the largest privacy gaps on the Internet."



So that's all for the best.  Now, of course, this is all happy news.  But we do need to remember that, even though our communications are encrypted, and now our DNS lookups would also be encrypted and hidden, the IP address to which we're communicating remains in the clear and fully visible.  So none of this does anything about that.  If you're on the wire, you're still seeing connections to IPs after they've been resolved by DNS.  And it's true you can't see into them.  But there is that technically metadata of who you are connecting to that continues to stand out.



So I did want to remind our listeners, we talked about it at the time, but Cloudflare also has a project called WARP which, at this point, they've sort of followed their 1.1.1.1 DNS offering with a WARP client for mobile.  At this point it's iOS and Android only.  There was a signup countdown where you could - I don't think it's widely deployed yet.  I haven't checked back to see what the status is.  But it was a 100% free for mobile VPN, which then would basically solve all of this and solve the IP monitoring problem, with the caveat that, yes, all of your traffic is then being centralized.



I mean, this was really the point that the EFF was making, was the power of the Internet is its decentralization.  So that's one of the downsides of any time you're creating a tunnel, any tunnel, whether it's a DNS tunnel or a VPN tunnel, it's going to be inherently providing some centralization.



LEO:  I don't know if you saw that Firefox is now offering a private network.  It's in beta right now.



STEVE:  No.



LEO:  Yeah.  I'm using it right now.  If you have a Firefox account, you install an extension and turn it on.  And I was just looking, and it says I'm coming from 8.45.41.210, which is a Level 3 domain, but I don't know anything else about it.  It's not mine, obviously.



STEVE:  Yeah.



LEO:  So it's working as a VPN.  Of course, it only works in the browser.  But it seems to be pretty transparent and painless, and it doesn't seem to be slowing things down much.  So that's kind of cool, too.



STEVE:  Very nice.



LEO:  I think we're making real progress.  And a lot of credit to Firefox, I have to say, and Cloudflare, for pushing this stuff forward.



STEVE:  Yeah.  Yeah.  Well, and I think we're rapidly approaching a point now with the way the Internet's CDN and multihoming structures have evolved, I mean, where if you use 1.1.1.1, you're actually going to a local resolver because all...



LEO:  Right.  Oh, that's interesting.  You're not going to Cloudflare.



STEVE:  Yeah, yeah, yeah.  Well, you're going to their IP, but the way the resolution happens is you're being - it's all fancy top-level Internet stuff.  But they've got resolvers spread all over the world that answer that IP local to you through the whole...



LEO:  Oh, that's why it's fast.  That's great.



STEVE:  Yes, exactly.



LEO:  Have you run it through your DNS Benchmark?  Have you checked its speed?



STEVE:  Yeah, it does pretty well.  It holds its own.



LEO:  Can we take a little break, Steve, before we get to Closing the Loop?  



STEVE:  Yup, perfect.



LEO:  Back to you, Steve.  We are back.



STEVE:  So I have two tweets, which sort of also double as our closing-the-loop feedback that takes us into our topic. 



LEO:  Oh, good.



STEVE:  Ashton, he said:  "Hi, Steve.  On the latest SN you mentioned Boxcryptor Classic for transparent encryption of synchronized files."  He says:  "I was using it for years, ever since you mentioned it on a previous SN episode, with great success; but the fact that they discontinued development on Classic never sat well with me.  A couple of years ago I found and moved to Cryptomator, which is open source donationware that does the same thing.  It's cross-platform and has Android/iOS apps, though those aren't free.  Thought you might want to check it out."  And that's Cryptomator, C-R-Y-P-T-O-M-A-T-O-R, dot org.



And so responding to Ashton, I agreed completely.  I said:  "First of all, Boxcryptor can still be used for free for TNO [Trust No One] cloud storage with the limitations that it will only sync two devices and only with a single cloud provider.  My big complaint with Boxcryptor is that they have switched to a software rental plan where it's no longer possible to purchase, for any price, a version that is not artificially constrained.  Anything beyond two devices and a single cloud provider requires a subscription.  And that's where I say no."



So Cryptomator is really the logical inheritor of Boxcryptor.  It runs on everything, it's open source, and encrypts all file metadata so that the cloud sees nothing.  It doesn't itself deal with synchronization, but relies on the cloud provider's folder and file sync.  Essentially you mount an encrypted volume which creates a virtual drive in your drive hierarchy.  And then files that are there, you know, you basically work with those.



So while Cryptomator is not one of the two tools or services that I have chosen, it may be perfect if it fits some users' needs.  So, for example, if you have an existing cloud provider that you want to continue to use, they don't offer Trust No One encryption, but they do do file syncing, then Cryptomator, I mean, it's a very nice piece of work.  I watched the early release and introduction videos by the authors, European authors.  It looks, I mean, they did a very nice job, and it is cross-platform, supports everything.  So maybe something to consider.



Paul Comfort tweeted from @PCComf.  He said:  "Hi, Steve.  I listened to your recent sync episode and felt your pain."  He said:  "I've done a lot of sync testing, and I have used or tested most major platforms, both with my own storage and using cloud storage.  I wanted to make you aware of a TNO solution I've found that meet my various needs best, and I think might also do the same for you:  Sync.com."  He says:  "I recommend the Business Solo plan, which is similar in pricing to Dropbox."  And actually it's better than Dropbox.



He says:  "Any TNO solution is going to have inherent limitations to what they can do simply because they can't see your data.  It's going to make them less feature-full than Dropbox or OneDrive, for example, but Sync.com makes the most of what it can do and provides enough features for me.  They still allow web access through an in-browser local decryption, similar to what LastPass does."



He says:  "I currently sync over 100,000 files with them across several systems, both macOS and Windows."  He says:  "During my testing, that number of files gave both Syncthing and SpiderOak trouble.  I make use of their archive function/location for things I never want synced.  I make use of their selective sync for systems that don't need my entire file structure.  The one Dropbox/iCloud/OneDrive-like feature I wish it had would be on-demand sync so that my smaller systems with smaller drives can still display the entire tree, but only download items when I need them.  But selective sync makes up for that."



He says:  "I just need to be smarter about what I sync.  I also have a system connected to a Drobo that synchronizes everything to the Drobo so I can take a full personal backup of all the online data in case something happens to them."  He says:  "I'm not affiliated with them at all except as a customer, even though this message probably sounds like an advertisement."  Well, believe me, compared to what I'm about to do with this podcast.  He says:  "I'm a satisfied user, and I think it would meet your requirements better than any other solution you mentioned."



LEO:  I've seen a lot of that from Sync.com.  You know, the price is 10 bucks a month for three terabytes.



STEVE:  Yup, three.  Three terabytes.



LEO:  So I pay $99 a year, that's compared to $120 a year for two terabytes on Google Drive.  So that, I think, is a very good price, if you have that much storage.



STEVE:  I tweeted back to Paul, and I let him know that, yes, indeed, Sync.com, for many of the reasons he noted, was one of the two tools I had settled upon after looking at everything.



LEO:  Oh, good.  My negative on it, no Linux.  Now, don't they say they're going to do Linux?



STEVE:  Yes, yes.  That is the - you hit it.  That is the one downside that will affect some of our listeners.  They do, however, have it on their road map.



LEO:  Yeah.



STEVE:  So they are intending to do Linux.  And I ended up with a kind of an interesting workaround using a Raspberry Pi that I will explain.



LEO:  Oh, clever.



STEVE:  And I also should note that I have it running on my Drobos, and that there's a Synology client for it, also.



LEO:  Oh, that's good.  I use Synology.  I mean, I basically do this with a Synology.



STEVE:  Right.



LEO:  So I have my own cloud.



STEVE:  So I looked at ownCloud, which is what I was talking about when you and I first breached this topic a couple months ago.  I looked at Nextcloud.  I saw what you were talking about, about the...



LEO:  The schism.



STEVE:  Basically, yes, the originator of ownCloud was annoyed that they were beginning to also offer some paid-only, like, super cloud sorts of things.  And he said, yeah, I really don't want to do that.  So he forked the source.  And, I mean, looking at them side by side, there's no difference between them.  I mean, they're, like, they are identical in every meaningful way.  So I don't know what the ownCloud people have been doing, but they sure haven't changed the UI at all that I was able to see.  Of course, as I mentioned, I looked at Cryptomator.  There's something called an encrypted file system, Cryfs.org.  There's also Syncovery.  There's Duplicati.  There's Duplicit - I don't want to say "duplicity."



LEO:  Duplicacy.  Duplicacy.



STEVE:  Duplicacy, thank you, duplicacy.  There's Tresorit.  There's pCloud.  There's something, AJC Sync.  We talked, in between my first mention and now, about that clever idea of using VeraCrypt, that is, mounting a VeraCrypt expandable volume, for example, in Dropbox because Dropbox is clever about only sending changed blocks.  And so it would make sense to mount a VeraCrypt volume because we know VeraCrypt's crypto has been looked at and scrutinized. and it's been audited and all that.



And so the idea would be, when you're working in a file system and changing files, you know, a file system is only changing clusters.  And so if the blocks were like clusters, it would make sense.  I mean, you could see how that would be efficient to do in a mode where only the things that are being changed in a big volume would be resynced.  It would never work in an environment where changing a file, changing part of a file meant the whole file was resent because then, if you touched anything in the VeraCrypt volume, it would send your entire volume back up to the cloud provider.  So that would not work.



But I ended up being very bullish on Sync.com.  As you noted, Leo, you get a lot more storage for the money.  Dropbox gives you, for our listeners who want to play with this, Dropbox gives you 2GB for free.  Sync gives you 5GB for free.  So even just paying nothing, setting up a free plan, you get 5GB.  And in fact there is an affiliate link which you can use to add an additional 20 if you've got friends.  I tweeted my affiliate link when I was discovering them.



I found out that it is capped at an additional 20GB.  I thought, you know, with my Twitter followers - oh, and people who use the link also get a free GB for using the link.  So they start out with six rather than five for free.  I ended up with 25 because I got the five plus the 20GB from people following my affiliate link.  And I told everybody, I said, well, this is an affiliate link.  We each get a free gigabyte.  So let's do that.  And then that's how I found that it was capped.



But they have, if someone wants to pay less, they have a $4 per month plan they call the Personal Plan, which is 500GB, so half a terabyte, so that would be, what, $48 a year.  But really the bargain, I think, is what they call - and this is what - was it Steve who tweeted?  No, Paul, who tweeted about the so-called Solo Business.  They have 2TB for $8 a month, whereas Dropbox charges $10 a month for 2TB.  But for that same $10 a month from Sync you can get 3TB.  And that's, 3TB, that's a lot of...



LEO:  Even two for most people is probably more than you need.



STEVE:  Yes.  Well, yes, because...



LEO:  One is probably more than you need because you have to upload it.



STEVE:  Yes.  And what I'm using it for is cross-system synchronization of just subsets of my data.  Okay.  So let me talk a little bit more about what Sync is and to introduce it to our listeners.  So basically you can think of it as Dropbox without all the other bells and whistles.  So it's just cloud storage.  But they did solve the TNO problem perfectly.  So there are clients for Windows and Mac, notably not yet for Linux.  So that still needs to get resolved.  And you and I, Leo, were also talking about the fact that, where Dropbox creates its own Dropbox folder, Sync does the same thing.  And so that chafed a little bit because, for example, I wanted my ASM tree, I have an ASM, for assembly language, on the root of my C drive.  And I wanted that to be synchronized, rather than stuff in the Sync folder.



I found a beautiful solution that involves junction points on NTF volumes, also known as hard links, the idea being that you move the folder from your normal folder hierarchy under the Sync directory.  And so like in my case I have moved my ASM folder.  I just dragged it under the Sync directory, and that was instant.  And then Sync, of course, went about doing its synchronization to the cloud.



And remember, I mean, these are my crown jewels.  This is all of the - this is like the SpinRite source and everything is now synced with Sync in the cloud.  Then Windows has a command, "mklink," and you use the /J for creating a junction point.  And basically you create a junction point at the \ASM, pointing it to the ASM folder under the Sync directory, and it just reappears.  For me, my ASM folder is back under C: where I like it, where all of my tools expect it to be, where all of my batch files and scripts and everything expects it to be.  Under a command window I can go to C:\ASM, so in no way is it different, as if it had never been moved.



LEO:  Is it like a hard link in Linux, or like a symbolic link?



STEVE:  Yes.



LEO:  It's like a hard link.



STEVE:  Yes.  The symbolic link is not what you want because those are kind of flaky, and it's sort of a pointer.  It is like Linux's hard link, and it is in fact a hard, well, a hard link in Windows cannot be a directory.  It's only a file.  So this is like a directory-level hard link.



LEO:  Yeah, you can do symbolic links to folders in Linux.



STEVE:  Folders, yes, yeah.  And so this solves that problem for me in Windows without any duplication.  So it's not like I have two ASMs now.  It's just that I moved my ASM folder under the Sync directory and then pointed to it from its original location.  And it works, I mean, absolutely the same.  I've been using it now for a month or so, and I've never seen it not do the right thing.



LEO:  Interesting.



STEVE:  So let's see.  I'm just going to scroll through here.  Protect your ideas.  I've looked at this stuff.  I've looked at their technology.  It's bulletproof.  So, for example, they get it that their competitive advantage is not only price, but privacy.



So they note that Google Drive, the Google terms of service gives their automated systems permission to access the data stored on their servers for the purpose of monetization through advertising.  The Dropbox terms of service gives Dropbox employees and trusted "third parties," whoever they may be, permission to access, view, and share the files stored on their servers at any time.  The Box terms of service gives Box permission to view the files stored on their servers to ensure users are in compliance with the Box terms of service.  And Microsoft OneDrive terms of service gives Microsoft employees permission to view the files stored on their servers to ensure users are in compliance with Microsoft terms of service.



So obviously I'm not putting my crown jewels with any of those guys without providing encryption.  And again, Cryptomator would be what you would use if you needed to use one or more of those services.  For me, Sync has it nailed.  So under features, it's 100% private cloud.  "End-to-end encryption," they wrote, "protects your confidential data in the cloud from unauthorized access at all times.  We can't read your files; and no one else can, either.  Sync doesn't collect, sell, or share your personal data or app usage information to advertisers or third parties, and we do not claim ownership of your data."



They said, under global data privacy compliance:  "Sync is safe to use, no matter where your business operates, with USA, EU,  U.K., GDPR, and Canadian compliance built in."  Oh, they are a Canadian company, by the way.  And they said:  "Including Canadian data residency.  Backs up your files in real-time and makes it easy to recover deleted files and previous versions of any file, any time.  Never lose a file again."  That's another one of the things that is in my criteria.  We've talked a lot about the danger of ransomware.  So the way you resolve that is you use a system that does file versioning.  And this does that.



And I feel almost guilty because I'm a person who's hitting Ctrl-S all the time, like to save, to take snapshots and save things I'm working on.  And when I look in my sync history, it just like scrolls off forever.  It's like, oh, my goodness, I hope they're doing, I mean, maybe they're doing something smart about eliminating redundancy.  But the problem is we're doing client-side encryption, so they're not able to see, they're not able to decompose this the way, for example, Dropbox is in order to do more intelligent synchronization.  But I do have versioning on all of these files.  They said:  "Data is replicated across multiple SSAE 16 Type II certified datacenter locations with SAS RAID storage, automatic failover and a 99.9% or better uptime SLA," you know, Service Level Agreement.



"For account security, two-factor authentication, granular user permissions, remote wipe, custom passwords, expiration dates, notification and more ensure you're always in control."  And Leo, the thing that I saw that immediately made me think, okay, I'm with the right guys now, is when I was setting up my account and created username and password and sent that in, I got a dialog prompt that said "Use email for password recovery," and it was not checked by default.



LEO:  Good.



STEVE:  And I thought, whoa.  When have you ever seen that?  I mean, so there, like, by default you're not using email for password recovery.  Because of course, as we know, that's the weakest link in password handling.  So in fact, in my SQRL presentation I joke with people that I've run across people who don't even bother to write passwords down.  They say, "No, I just gave up on that a couple years ago.  I just click 'I forgot my password.'"



LEO:  Every single time?



STEVE:  Every single time.



LEO:  And then of course it gets emailed.  But it's a reset, usually, which is - that's not totally insecure, right, a reset link?  Of course anybody could get that.



STEVE:  Yeah, it's totally, yeah.



LEO:  Yeah, because then they've got it, yeah.



STEVE:  Exactly.  So, I mean, it just makes you cringe.  But, you know, okay.  I mean, so here's, you know, that's what we've done.  The point I make during my SQRL presentation is we have so - "we," IT, Internet, techie world have so abused the common man, you know, the typical user, that they're just, like, okay.



LEO:  I give up.  I give up.



STEVE:  I give up.  I'm just going to say, no, it's true, I don't remember because I never wrote it down.  I just click the "I forgot," and I get it.  And look, oh, I'm logged in.



So they say, under their technical overview, it's zero-knowledge, end-to-end encryption.  File and file metadata is encrypted client-side and remains encrypted in transit and at rest.  The web panel, file sharing, and share collaboration features are also zero-knowledge.  Private encryption keys are only accessible by the user, never by Sync.  Passwords are never transmitted or stored and are only ever known by the user.



So we get some juicy crypto details:  "A randomly generated 2048-bit RSA private encryption key serves as the basis for all encryption at Sync.  During account creation, that unique private key is generated on the user's machine and encrypted with 256-bit AES GCM, which coincidentally is what I use to encrypt SQRL's identities in the SQRL system, so same bulletproof encryption.  And that is locked with a user's password, which also is what I do.  This takes place client-side within the web browser or app.  PBKDF2 key stretching with a high iteration count is used to help make weak passwords more cryptographically secure.



"Encrypted private keys are stored on Sync's servers and downloaded and decrypted locally by the desktop app, web panel, or mobile apps after successful authentication.  At no time does Sync have access to a user's private key.  A username and password is required to authenticate and log into the Sync desktop app, the Sync web panel, and Sync's mobile apps."  Oh, and there's another cool thing that happens, Leo, when you have the Sync app running on your desktop.  It has a minimal UI, so it will launch your web browser.  It then authenticates with your desktop app in order to get itself authenticated.  So you don't have to relog into the web browser.  It automatically does a single sign-on behind the scenes with the desktop app.



They said:  "During authentication, a bcrypt hash of the user inputted password is generated locally" - bcrypt being a hard-to-accelerate solution - "locally on the computer or device, using a unique salt that is stored on the server.  Bcrypt," they remind us, "is a one-way hashing mechanism, meaning the hash cannot be unhashed or deciphered.  The benefit of bcrypt is that it is slow by design, which prevents brute force or rainbow table attacks.  At no time" - and remember, every single user gets a unique salt which the server holds so that you're able to operate across platforms.  "The server authenticates against the hash, but at no time is the hash itself stored. Successful authentication allows the app to download the user's encrypted private key, which is then decrypted locally with the user's actual password."  In other words, they have done everything right from a crypto standpoint.



They say:  "Sync's web panel runs on the client, within the web browser, as a self-contained AngularJS app.  The web panel utilizes Stanford JavaScript Crypto Library" - which, by the way, is the right one - "a JavaScript implementation of ISAAC, which is a fast cryptographic random number generator" - so they're not trusting the one built into JavaScript - "and HTML5 local storage.  This means only modern web browsers are supported.  In other words, IE10 is the minimum requirement."  Which of course is no problem.



"The web panel authenticates the user, downloads the encrypted encryption keys, decrypts the keys locally within the web browser, then downloads and decrypts file and file metadata as required."  And so short version is your web browser is a very nice UI, presents a nice UI that gives you access to everything that is there as if it wasn't encrypted, but it is.  "Passwords are never transmitted or stored during this process.  The web panel is open source.  Source code is easily viewable from any web browser for those technically inclined to see how it works.



"File data is always encrypted, in transit and at rest.  Sync utilizes a unique 256-bit AES GCM data key on each file, locked with the user's 2048-bit RSA key.  File metadata is encrypted separately with the user's password, PBKDF2 key stretched, to generate a key for the 256-bit AES GCM.  This all happens locally on the user's computer or device before the files are transferred to Sync's servers.  This ensures that the encrypted file data stored on Sync's servers is impossible to access, even in the event that the servers themselves are compromised."



And they've solved the problem of users.  One of the requests I got from our listeners is, yeah, I'd like to store things encrypted in the cloud, but I also want to be able to share specific files with people.  Like give somebody a link that they're able to use.  They explain:  "Secure links are locked with a unique password appended to the link after the hashtag."  That's very clever because what's after the hashtag, the pound sign, is never transmitted.  "The data appended to a URL after the hashtag is called a 'fragment identifier,' which is never transmitted or stored, meaning the password is never known to Sync.  The password itself is key stretched using PBKDF2 with a high iteration count, to generate the encryption key used to unlock the link."



So that's a link that an individual can generate and share with a friend, and then tell them what the password is.  When they use the link, they'll be prompted for the password.  Put that in, and then that file will be, after the information provided is verified, be downloaded and decrypted locally for that user to gain access to.  So again, they've solved the link sharing, the specific file sharing problem, and maintained TNO encryption.



Then they also provide a shared folders facility.  Shared folders utilize key wrapping, which is multikeying, basically, to allow files to be shared between different Sync users privately, without the need to reencrypt the actual file data when users are added and removed from the share.  Sync never has access to file data, even when being shared.  When a new share folder is created, the unique encryption key on each file within the share is encrypted with a unique 512-bit share key that is created specifically for the share.  The share key is then encrypted with the RSA 2048-bit public key of each user and locked with the user's RSA private key, which is encrypted with the user's password.



So in other words, they do, again, exactly the right thing.  They meta encrypt the share under a 512-bit key, which is then multiply encrypted under all of the people who are sharing that folder, multiply encrypted under all of their public keys, which means that any one of them are able to decrypt that 512-bit key with their private key and so, again, create basically a network of truly encrypted shared folders where you get to use Sync to, like, create the cloud-based infrastructure for sharing folders among multiple Sync users.



And the whole process is transparent.  Basically, you end up with a folder on your system that you are securely sharing among other Sync users.  I mean, they've just nailed this.  Single sign-on, and here they mention what I was saying before.  The Sync desktop app allows the user to log into the web panel automatically.  The web panel is loaded locally using a unique URL that contains a memcache lookup value.  And here they're getting into the details of how the web panel works.  I'll skip all that.



And they note that Sync uses SSL and TLS for all data transfers, but does not rely on SSL or TLS for any meaningful security.  They say SSL on its own cannot be trusted.  SSL is applied as an extra layer on top of the 2048-bit RSA and 256-bit AES encryption used to encrypt each file.  This ensures that in the event that SSL is compromised, for example, a man-in-the-middle attack or an SSL software vulnerability, and of course we're talking about certificate authorities being compromised, or middleboxes that are performing on-the-fly decryption of SSL, none of that affects the security of this system.  It remains encrypted inside that tunnel and thus impossible to access.



So anyway, I mentioned how I had solved the problem of things needing to be in the Sync folder by using the junction point or directory junction, actually, /J, in the mklink command for Windows.  So I am, actually, I am super happy with it.  For me, it's the right cost, the right technology.  These guys nailed it.  Somehow I like the idea of the things I absolutely really care about never losing access to.  And also, of course, I have access now on the road.  So all of these things are available while I'm traveling just by tying into a publicly available cloud provider without sacrificing any security.  So that is one of the two things I have found.  And Leo, let's take our last break, and then I will tell everybody about the second really cool tool.



LEO:  Thing 2.  Boy, if this only had a Linux solution.



STEVE:  Yeah.



LEO:  I wonder how close they are to solving that because that really would - see, right now I'm doing it all to a Synology.  But with 3TB, I mean, because I use a Synology basically as a backup tool, but then I also have a Documents folder which is the stuff I want on every machine.  It's about 20GB.  It's not that big.  I use a git for my source code, which I don't - I'm always amazed that you don't use version control of any kind in your source code.  But to each his own.  And I use an encrypted git, which I like a lot.  But I could also, you know, I could do this, too.  And I think probably I could get even like my music and photos back up.  I think those are each about a terabyte.  I can get it all into 3TB.  And it would have the advantage of it would be offsite.  My Synology's in the house.



STEVE:  That is exactly it, is that just having it somewhere else.  I mean, I have servers at Level 3, so I could run...



LEO:  Well, that's the thing.  You could do this.



STEVE:  Yeah.  Although I'm just - I'm so security conscious that having a server, having my server at Level 3 that has a public port, I just, you know, it just makes me uncomfortable that there isn't going to be some buffer overrun or buffer overflow.  And so here it's somewhere else.



LEO:  It is public.  I mean, that it's accessible by the Internet.



STEVE:  Yeah, well, so, yes, but it's not my server that would be exposed, it's their server that would be exposed.



LEO:  And your data on there is encrypted.



STEVE:  And my data in there cannot be accessed, exactly.



LEO:  I think you may have convinced me that this would be a - and as you said, there's a Synology app, so I could easily use my Synology, too.  That's interesting.  I just wish there was a Linux.  All right, Steve.  Thank you.  Back to the show.  Steve Gibson, Leo Laporte.  The Joy of Sync continues.



STEVE:  Now, I just gave everybody my absolute first choice of a service that did everything right.  There is another incredibly cool tool which, Leo, you will be glad to know does support Linux, and has iOS and Android apps, and is also just, I mean, it is addictive seductive, it is done so well.  A number of people recommended it.  I looked at it.  And, I mean, just in terms of their philosophy and what they've done, it's so cool. And that's called...



LEO:  Now you've got me excited.



STEVE:  It's called Syncthing.



LEO:  Oh, yeah, okay.



STEVE:  Yup.



LEO:  Yup.



STEVE:  Yes.  So Syncthing is a peer-to-peer solution.  And it is what I have running on both Drobos and also on my workstations.  What I'm now thinking I'm going to do is I'm going to use Sync.com as my glue between my locations, and Syncthing as my LAN-level sync, although it is not constrained to that.  So, okay.  So what is it?  It is an open source, open protocol - you can think of it, if you wanted to think of it like something like BitTorrent, which is now Resilio... 



LEO:  Resilio, yeah.



STEVE:  Yes.



LEO:  And not free.



STEVE:  And not free, and not open.



LEO:  And not open.  I love that the Syncthing is, which is really great.



STEVE:  Yes.  And open protocol, open source, and their heart is in the right place.  So the project's formally stated goals are Syncthing should be, one, safe from data loss.  They said:  "Protecting the user's data is paramount.  We take every reasonable precaution to avoid corrupting the user's files.  This is the overriding goal, without which synchronizing files becomes pointless.  This means that we do not make unsafe tradeoffs for the sake of performance or, in some cases, even usability."  And I should note that it's written in Go, so it outperforms other solutions that are written in PHP, like ownCloud, or what's the other one?  OwnCloud or...



LEO:  Nextcloud.



STEVE:  Nextcloud, yes.  Secure against attackers, they say.  "Again, protecting the user's data is paramount.  Regardless of our other goals, we must never allow the user's data to be susceptible to eavesdropping or modification by unauthorized parties.  This should be understood in context.  It is not necessarily reasonable to expect Syncthing to be resistant against well-equipped state-level attackers.  We will, however, do our best.  Note also that this is different from anonymity, which is not currently a goal."  And I'll explain what that's about.



Ease of use:  "Syncthing should be approachable, understandable and inclusive.  Complex concepts and math form the basis of Syncthing's functionality.  This should nonetheless be abstracted or hidden to a degree where Syncthing is usable by the general public."  Certainly by our listeners.  There's no question about that.



LEO:  I admit, I have installed Syncthing and am having a little trouble figuring out how to get it syncing.



STEVE:  I did, too.



LEO:  Okay, good.



STEVE:  I didn't initially understand the model. 



LEO:  Yeah, yeah.



STEVE:  I've nailed it now.



LEO:  And maybe because I was thinking of it as a BitTorrent sync client, kind of.



STEVE:  Yes.



LEO:  And that made it hard to understand, yeah.



STEVE:  Yes, because one of the things that's noted is that BitTorrent is sort of more automagic than this is, whereas - but here you get control.  And now that I understand what it's doing, I really like that control.  So our listeners should know there's a little bit of a learning curve.  But just take it easy, and you'll get there.



So then they said it should also be automatic.  "User interaction should be required only when absolutely necessary.  Specifically, this means that changes to files are picked up without prompting, conflicts are resolved without prompting, and connections are maintained without prompting.  We only prompt the user when it is required to fulfill one of the overriding secure, safe or easy-to-use goals."



Also, universally available.  "Syncthing," they say, "should run on every common computer.  We are mindful that the latest technology is not always available to any given individual.  Computers include desktops, laptops, servers, virtual machines, general purpose computers such as Raspberry Pis and, where possible, tablets and phones.  NAS appliances, toasters, cars, firearms, thermostats, and so on may include computing capabilities, but it is not our goal for Syncthing to run smoothly on these devices."  So maybe not your toaster or your shotgun, but things where it's reasonable for it to run.



And they said:  "For individuals, Syncthing is primarily about empowering the individual user with safe, secure, and easy-to-use file synchronization.  We acknowledge that it's also useful in an enterprise setting and include functionality to support that.  If this is in conflict with the requirements of the individual, those will, however, take priority."



And then they said:  "Everything else.  There are many things we care about that don't make it onto the list.  It's fine to optimize for these values, as well, as long as they're not in conflict with the stated goals above.  For example, performance is a thing we care about.  We just don't care more about it than safety, security, et cetera.  Maintainability of the code base and providing entertainment value for the maintainers are also things that matter.  It is understood that there are aspects of Syncthing that are suboptimal or even in opposition with the goals above.  However, we continually strive to align Syncthing more and more with those goals."



So I am 100% sold on this thing, this Syncthing, now that I understand it.  My original setup, I was really interested to know whether a Syncthing instance at one of my locations would be able to connect to my Syncthing at the other because in my place with Lorrie, I'm not behind one NAT, I'm behind two NATs because I ended up putting a pfSense router in front of my ASUS WiFi router, so they're both NATing.  So there's double NAT there.  And of course here I'm in the Fortress of Solitude.  I mean, I am really locked down.  They instantly found each other, and I had no relay required.



So to explain a little bit about this architecture, it is peer to peer.  There are a network of global discovery servers and relay servers out on the public Internet so that an instance of Syncthing is able to use the discovery servers to get the IP and port that another instance of Syncthing has a public appearance on.  And so they solve, using STUN and TURN that we talked about a long time ago, NAT piercing.  My Syncthing clients, each running behind all of that NATing, are able to connect directly to each other across the public Internet with no relays.  So nobody sees my traffic except anybody in a direct connect.  And actually since both locations are on Cox.net, it doesn't even leave Cox Cable's infrastructure.



I set up a five-node Syncthing peer-to-peer network.  Both Drobos had Syncthing running.  They found each other.  So they created a synchronized folder hierarchy there.  Then I was running on - my workstation at the other end found the local Drobo.  Syncthing is able to use the public discovery servers if it needs to find a Syncthing instance elsewhere on the public Internet.  But on the LAN it uses broadcast or multicast because it supports IPv6.  So it'll use a LAN broadcast or an IPv6 multicast in order to find other instances of Syncthing on your own LAN.  So again, what I will end up doing is using Syncthing on my LAN, on each of the LANs.  But because I like Sync.com so much, I will use it to perform my interlocation straddle, and also because Sync.com gives me roaming access to everything that I am syncing in my locations with full TNO encryption.



Syncthing does not encrypt, the argument being, I mean, its connections are encrypting, but it's not itself a storage provider.  It is a synchronizing interconnector.  It works on a directory basis.  So it works in a headless fashion.  There is a, for Windows users, what I would recommend is something called SyncTrayzor, as in the Windows tray.  So it's called SyncTrayzor.  If you just run SyncTrayzor under Windows, it includes Sync with it.  The version of Sync is out of date, but the first thing Syncthing does is check for any updates, and immediately updates itself.  The version, for example, that I found that I was able to install on my Drobo was older.



Again, first thing it did was perform and in-place upgrade to the latest and greatest.  So using a browser UI, you decide which folders on your local machine you want to sync and with which other instances of Syncthing you want to sync them.  There are two, like, weird-looking crypto tokens.  One is really long, and that's the Syncthing instance identifier.  When you install Syncthing, it synthesizes on the fly a TLS certificate for itself.  So each Syncthing instance has a private and public key in the form of a formal TLS certificate.  That identifier is a hash, an SHA-256 hash of the certificate, which is Base32 encoded with some check characters added to create that identifier.



So the beauty of that is each instance has a unique identifier which is the hash of its certificate.  And this is where you were talking about how it's not quite so automatic, is it is necessary to provide that thing to the other side in order for instances of Syncthing to find each other.  On the LAN, that's not necessary.  What I find - it's a little jarring, actually - is that, like I'll bring up an instance of Syncthing, and it'll say, hey, who do you want to share this with?  How about this guy?  It was like, oh, yeah, that's the one I wanted to share it with.



LEO:  Oh, that's nice.



STEVE:  So on the LAN it's...



LEO:  That's convenient.



STEVE:  Yeah.  On the LAN it's able to do discovery, and you're not having to, like, it's a really long thing.  I mean, it's not unwieldy.  You could read it over the phone.  But I just stuck it on a thumb drive when I went to my other location and put it in.  And once, for example, once remotely located instances have connected, then they all share all of the Syncthings that they know about with each other.  So, I mean, it really does form a Syncthing hub or cloud or network.  Syncthing does updates on a block level.  They use 128K blocks by default.



Remember that the tweet that I read before about someone who had found that Syncthing he was having trouble with, they have addressed that since then by now supporting much larger instances of things that are being synced.  So initially when you set up, there's like a startup process because it hashes all the files that you're wanting to sync.  And then it's able to track changes.  And I have found that, I mean, it immediately spots a change that I've made.  I think there's a deliberate 10-second delay.  But after 10 seconds, it finds it, and it clones it.



And because I was curious in my early experimenting phases, and because this isn't about cloud storage, it is point-to-point device synchronization, I wanted to see whether I could create a cloud presence of my Syncthing synchronization.  And so I got a $35 Raspberry Pi, and I used "rclone" and "mount" to mount my Dropbox account.  This is before I was playing with Sync.com.  So I mounted my Dropbox account on the Raspberry Pi running Linux and then ran Syncthing.  Because Syncthing is able to sync a mounted drive, I used Syncthing to sync with my own LAN directory syncs, and it worked perfectly.



It isn't something I would recommend on a Raspberry Pi because in order - and this is an rclone problem.  In order for rclone to work reliably, it has to make a full copy of the file that it is going to be syncing to remotely, or mounted to.  And so that's going to, you know, Raspberry Pi runs typically on a little SD card.  And those things do not have the resilience to handle me hitting Ctrl-S every 30 seconds while I'm saving copies of things.  It would chew up the little SD card and cause it to die.  But what you could do, if you really wanted to do this, would be just to get, you know, any older PC and just set it up with Linux and then run Syncthing on it, and then also mount a Dropbox or even a Sync.com drive and have Syncthing sync to that.



Anyway, I mean, they've nailed the way this thing works.  I mentioned that there were two different crypto things.  The other thing is that folders are identified by a unique string to be known to the system, that is, network-wide.  But then you decide which local folder you want that to be synced to.  So I think that our users, our listeners of this podcast, once they get the hang of this, will love Syncthing and the power it gives them for what it is.  It does take, I mean, there's a learning curve.  This is not for non-technical users.  But for technical users, because the philosophy is right, it is very fast block-level synchronization between devices.



It runs on anything.  There are iOS and - I think there are iOS and Android clients.  I have to - oh, there is Syncthing for FreeBSD.  So I was happy to see that in the FreshPorts collection.  I don't know, in my notes here, whether - I don't remember now where it is with mobile apps.  But anything that runs - no, no.  You don't have access to a browser.  I think there are mobile clients, but I'm not sure.  Anyway, it does solve the Linux syncing problem and Mac and Windows.  And I'm annoyed that I didn't have in my notes here whether it does - let's see.  Syncthing, oh, not iOS, but yes to Android.  That's what it was.  There was something called "fsync" that was an older version of something on iOS, no longer supported.



So we do not have - Syncthing does not currently have an iOS client, but it does have an app in the Google Play Store and also F-Droid.  You're able to grab it there.  There's a Mac, Windows, Linux cross-platform.  It's got GUI wrappers using Syncthing-GTK.  And there is a Syncthing for macOS.  So  FreeBSD, NetBSD, Dragonfly, Solaris, OpenBSD, Windows.  The source code is there.  It is terrific for interdevice sync.  And it is, if I had not discovered Sync.com, Syncthing is what I would be using.  Oh, I forgot to mention.  I feel a little queasy about using public discovery servers.  You are able to run your own discovery server.  So if you have any sort of a hosted presence on the 'Net, you can run your own discovery server and point your Syncthing instances at it in order to allow them to find each other if you're roaming around.



You are also able, if for example endpoints have DynDNS or relatively static IPs, you're also able to just simply point these things at each other across the public Internet.  But as I said, having found Sync.com, for the price, and remember, 5GB just to play with for free, that's what I'm using for my public presence, Trust No One synchronization.  And then Syncthing is what I'm using - oh, and also it handles deep versioning.  So, for example, I don't do versioning on my workstation instances of Syncthing, but I have versioning turned on on the Drobos.  So that's perfect.  That's what you want is all the things I'm doing are automatically being synchronized to the Drobo.



So it's got deep RAID backup and very sophisticated versioning.  It's like for the first hour it keeps a copy every 30 seconds.  Then for the next day it keeps one every hour.  Then for the next week it keeps one every day, and blah blah.  So it's like, you know, a whole hierarchical staged file synchronization.  So, I mean, which is exactly what you want, so that you're able to go as far back in time as you want to, and all of that is user configurable and tweakable.



So anyway, they nailed it for peer-to-peer synchronization across all of our desktop platforms and Android mobile.  And Sync.com, the only thing it's missing, because it does have apps for iOS and Android, the only thing it's missing - and of course you have a web browser interface, as well.  The only thing it's missing is Linux desktop support.  So that's my Joy of Sync.



LEO:  You did look at pCloud, which...



STEVE:  I did look at pCloud.



LEO:  Which has Linux support.  If that was a deal breaker, would you consider pCloud?  Or are there issues with pCloud that you've seen?



STEVE:  I don't remember now.



LEO:  You looked at a lot of them.



STEVE:  I looked at so many things, I'm at risk of getting them confused.



LEO:  One thing I do like about - it's only 2TB, but they offer a lifetime subscription for 350 bucks, and then you can pay for crypto.  You can have arbitrarily synced crypto folders, which is nice.  So you can encrypt certain folders, but have other folders unencrypted for things like - because they have audio playback, stuff like that, which obviously encrypted folders you couldn't do.  And then the other thing I like, you could sync arbitrary folders, so you don't have to do the junction link thing.  You can say this folder is synced, this folder is synced, this folder is synced.



STEVE:  Ah, that is nice, yup.



LEO:  Yeah.  And they do have a Linux, as well as iOS, Android, Mac, and Windows clients.



STEVE:  And pCloud was recommended by some people.  So I think, you know, I may have been a little put off by the lifetime, only because...



LEO:  Well, you don't have to do that.



STEVE:  Right, right, right.



LEO:  It's the lifetime of pCloud, not of you, obviously.



STEVE:  Correct.



LEO:  So you don't have to do that.  In fact, the pricing's very comparable.  It's, I think, 100 bucks a year for two terabytes.  So it's a little bit less storage.  Honestly, though, for most people, two terabytes is infinite, as is probably one terabyte.  Microsoft famously called one terabyte "infinite" on its OneDrive offer.  It's really nice that there are these good Trust No One choices out there.



STEVE:  Yes.  I was going to say that the world has evolved and matured.  I mean, we are, if anything, we are just buried in riches through this whole category.



LEO:  Yeah, yeah.  It's very exciting.  And I think that I will look at all of these.  I think, I'll tell you what, I'll give a try to pCloud only because of the Linux client, which really for me is kind of important because I use Linux as much as anything else.



STEVE:  That does make sense.  And when I had tweeted about Sync weeks ago, I did get some listeners say, yeah, looks great except for Linux.



LEO:  Yeah, yeah.  Really cool.  I think this has been so important.  By the way, get the show notes.  They're available at GRC.com, especially if you're going to set up Syncthing, because Steve does a great job of walking you through that process, better than the documentation, I think.  So if you're curious about how to do it, I was a little stymied by it.  I've installed it everywhere, but I still haven't hooked it up.  But I will be starting to walk through your show notes to get that information.  Show notes...



STEVE:  So I really...



LEO:  Go ahead.



STEVE:  Go ahead.



LEO:  I was just going to say, show notes are available, along with 16 and 64Kb audio versions and a transcript, a full transcript at Steve's site, GRC.com.  While you're there - he didn't even mention SpinRite.  I will.  Get yourself a copy of the best file and hard drive recovery and maintenance utility out there.  There's only one.  It's called SpinRite, and you get it at GRC.com.  And since it's Steve's bread and butter, it's how we keep him afloat.  So we want to do that.



SQRL information there, as well.  And if you're going to see one of Steve's presentations in Europe or our presentation in Boston in a couple of days, you might want to read that first, get an idea of SQRL.  Lots of other great stuff, as always, at GRC.com.



We have audio and video of every show.  And this is one, this is a keeper.  This is an evergreen show.  And this will be available at TWiT.tv/sn.  That's the Security Now! folder.  All the shows are there, audio for every show, video for all the shows for which we recorded video, which is several hundred of them, including this one.  You can watch us do the shows.



Normally, we're a little off today, but normally we do these Tuesday, 1:30 Pacific, 4:30 Eastern, 20:30 UTC.  And if you are around at that time, the stream is live.  You can ask your Amazon Echo to listen to TWiT Live, or you can do that with your Google Assistant, or you can just go to TWiT.tv/live and get an audio or video stream.  Join us in the chatroom if you're doing that, irc.twit.tv.  And by all means, subscribe in your favorite podcast application so you can start building your superlative collection of Security Nows.



Steve, thanks so much.  Great job on this.  I think this is going to be a favorite for years to come.  See you next time.



STEVE:  I love sharing good solutions with our listeners.  And these are really - I really encourage people to look at Syncthing for what it does, and Sync.com.  Or pCloud, if you need Linux.  That makes a lot of sense.



LEO:  I'll report back.  Everything I've seen looks pretty good for pCloud as an alternative.



STEVE:  Cool.



LEO:  So thank you, Steve.  We'll see you next time on Security Now!. 



STEVE:  Thanks, buddy.



Copyright (c) 2019 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.






GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#735

DATE:		October 8, 2019

TITLE:		Makes Ya WannaCry

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-735.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we reveal a miracle mistake made by a hacker more than years ago that saved the world from devastating ransomware.  But first we catch up on recent ransomware activities, examine the detailed handoff from the GandCrab shutdown and the Sodinokibi startup, a welcome change in Microsoft's Extended Security Update policy for Windows 7, a nasty zero-day RCE in vBulletin, and a bit of nice SQRL news.



SHOW TEASE:  It's time for Security Now!.  Yes, Steve is back.  Yay, we're all back.  We're going to do a great show.  We're going to talk about ransomware.  There's a lot of it going around.  And an amazing story about the ransomware epidemic that never happened, but could have very easily.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 735, recorded Tuesday, October 8th, 2019:  Makes Ya WannaCry.



It's time for Security Now!, the show where we cover your security online with this man right here, Steve Gibson, who in his time at @SGgrc on the Twitter and at GRC.com on the Internet has become the premier guy for security, but also for hard drive recovery with SpinRite and for checking your router with ShieldsUp.  And now I know, Steve, that you are in fact a bona fide, legitimate, absolute superstar because, man.  I think you know that, too.



STEVE GIBSON:  Well, yeah.



LEO:  You did Dublin.  You did Gteborg, Sweden.



STEVE:  Yup.



LEO:  And then we were in Boston.  I can't speak for the first two, but it sounds like those were massive sellouts.  Right?



STEVE:  Well, and really it's just - I think it's a consequence of 15 years of trying to really do a good job and having something useful to say.  And I happened to be in those neighborhoods thanks to the OWASP groups that wanted me to tell the SQRL story.  And so, I mean, again, in every one of those, just as happened with the LogMeIn/LastPass event, we'd say, okay, how many people here listen to Security Now!, and it's like, whoa.



LEO:  All the hands.  All the hands, yeah.



STEVE:  Yeah.  Like, yeah, yeah.



LEO:  It was amazing.  Steve and I did a great panel discussion which you can get at TWiT.tv/specials.  You can watch Steve, me, Gerry Beuchelt, who's the CISO at LogMeIn, that's LastPass's parent company, and Bill Cheswick.  Had you ever met Bill before?



STEVE:  I had never met Bill.  He is a great character. 



LEO:  You clicked.  You guys are on the same - because you're very similar.  I mean, he is a legitimate geek, a security guy, a guru, Bell Labs and all that.  But he also is an enthusiast like you.  So he does beekeeping.  He's got a project to make a poster of a movie with every frame on the poster so you could see the color.  It's just like he's got all these crazy fun  things he does.



STEVE:  Deeply into IoT stuff and leveraging the technology, yes, leveraging the technology to its limit and, yeah.



LEO:  So it was really, really a great panel.  I hope you get to watch it.  And thank you to everybody who came, about 250 people.  They were glad they came.  Open bar, lots of food.  They got SQRL stickers.



STEVE:  Yeah.



LEO:  And we stood in line for two hours and took selfies with everybody.  And it was so much fun to meet you all, so thank you for coming out.  We had people - one guy flew out from, weirdly,  from Tustin, which is your neck of the woods.



STEVE:  Yeah.  Oklahoma.



LEO:  Austin.  Seattle.



STEVE:  Someone from Texas.  Yup.



LEO:  It was amazing.  People from all over the country.  Because Steve never goes out of the house, so...



STEVE:  I don't, no.



LEO:  ...it's a rare opportunity to see the man.



STEVE:  And it's good to be back in my little cave here.



LEO:  Are you happy in your man cave?



STEVE:  I am happy.  I'm trying to get readjusted to the time zone.  But, yeah.  In fact...



LEO:  I have bad news for you, Steve.  LastPass wants us to do it again next year.  Not necessarily in Boston.  But, yeah, we  want to do more of these.  They're so fun.



STEVE:  Oh.  It was fun.  I'm up for it, yeah.



LEO:  Good, yeah.  It was really great.



STEVE:  And also we do have a great recording from the Gothenburg presentation.  One from Dublin is still, I think, is still - last I heard it was still being converted, so a 4K video, and it takes a while.



LEO:  Oh, nice.



STEVE:  And then you and I are going to do like what I'm considering sort of like the reference, the "SQRL story" presentation on Saturday, November 30th, after the U.S.'s Thanksgiving Thursday and Friday, like, long weekend, which I'm really looking forward to.  And that'll be like my final, okay, this is it SQRL presentation.



LEO:  So this will be the one for everybody who couldn't make the live presentations.  This is going to be very geeky.  This is going to be, if you're a developer, and you want to implement SQRL, if you really want to understand the nuts and bolts, we are going to have a live studio audience.  We're going to shoot it in the big studio over here.  So even as big as it is, it even then is limited to, I think - John just said we have 28 chairs.  But if you want to come to that, November 30th, right after the radio show, so about 2:30 in the afternoon, we would love to have you.  But you must email tickets@twit.tv because I know how many fans you have, Steve.  There are going to be a lot more people than we can accommodate.  So if you're listening to this, right now, tickets@twit.tv, say "I will be there."  And you'd better darn well show up because we want to get [crosstalk].



STEVE:  And, you know, I guess it's a good thing that Petaluma is a little bit out of the way, too.  So it's, you know, you've got to...



LEO:  Oh, yeah, should mention that, Northern California, yeah.



STEVE:  Yeah, you've got to take that goat trail out from Marin north in order to get to Petaluma.



LEO:  Oh, boy.  But we do have a big parking lot, so all of you will get to park your goat.



STEVE:  Yay.  Well, so, yeah.  So the Gothenburg presentation was recorded.  Anybody who can't wait, if you go to GRC.com/sqrl, I've added it, because it's up on YouTube, I've added it to that page so it's easy to find.  But I want the final official one to be done at TWiT, with TWiT, since it's a consequence of the podcast, Leo, that this whole thing happened in the first place.



And as you said, I call it the "SQRL story."  I sort of explain how it happened, what it is, and completely explain it.  It's not short.  It's about two hours.  It just always ends up that it takes me two hours of nonstop hand-waving and pointing at diagrams on the screen to explain what it is.  But anybody who watches it comes away saying, oh, holy crap, I understand that.  So that's really my goal is to just have something that - because nobody wants to read, even though all the documentation is now up and relevant.  It just makes sense to have a video presentation to sort of put the seal on it, and so we're going to do that.



LEO:  Well, and that was one thing that was really fun is that Bill Cheswick got it immediately.  I mean, he asked one question, said, "Oh, yeah."  And...



STEVE:  Yes.  Yeah, they asked...



LEO:  Said, "Oh, I get it.  Sounds good.  Like the idea."  And I said, "Break it, Bill.  Come up with something."  And nobody could come up with anything that broke it because obviously you've spent a long time, years now, thinking about all the ways it could break.  And I think everybody agreed.  And here's the deal, and I'm going to continue to pressure them.  The LastPass CIO was there; Gerry, the CISO was there.  They were interested.  So I would, boy, that's what you need is LastPass to incorporate that in there and have single sign-on and other things.



STEVE:  Well, and in fact we already had our tickets to head back here the next morning.  And he said to me, he said, "Are you leaving tomorrow?"  Like if you're going to be around, let's sit down.  But unfortunately I just didn't have any chance.  But yeah.



LEO:  There'll be opportunities.



STEVE:  Yeah.



LEO:  There'll be opportunities, I'm sure.  But that's what you need.  And that's what Cheswick suggested is that you work with somebody to make a PAM module.



STEVE:  Actually since then I found out we have one.



LEO:  Thought you did, good.



STEVE:  So you can use it for logging onto your Linux machines.



LEO:  And then the next step is to get the Debian maintainers to make it part of the standard distribution so that everybody has SQRL on their Linux distro.  Then, man, why wouldn't a website implement it?  I mean, it just would be automatic.  And I think that's the key.  Well, we've got other stuff to talk about.  What are we doing today?



STEVE:  We do.  We're going to reveal a miracle mistake that was made by a hacker more than two years ago that saved the world...



LEO:  Oh, dear.



STEVE:  ...from devastating ransomware attack.  This podcast, #735 for October 8th, is titled "Makes Ya WannaCry" because we've often talked about how our friend Marcus Hutchins registered a domain name that he found buried in the original WannaCry very potent ransomware worm, which stopped it.  Well, it turns out that within two days a variant had been created that was unstoppable, but a mistake was made which prevented it from wiping us off the face of the Earth.  So we're going to talk about that after we catch up with some other recent ransomware activities, examine the detailed handoff, which has now been sort of deeply reverse-engineered by security firms, with that GandCrab shutdown and then the not long afterwards Sodinokibi startup.  And now it's really looking like they're all one and the same.



We also have a welcome change in Microsoft's Extended Security Update policy for Windows 7.  And I have to confess it sort of launched me into a bit of a rant, but that's the nature of how I feel about Windows 10.  We have a nasty zero-day remote code execution in vBulletin, and a little nice bit of SQRL news, and a fun Picture of the Week.  So I think we're back in the saddle, Leo.  We're both back home.  And we're going to do another great podcast for our listeners.



LEO:  Yay.  I know they can't wait.  Loved the Sync thing podcast last week.



STEVE:  Yeah.



LEO:  "The Joy of Sync."  And I hope you got to listen to that.



STEVE:  And we have retired our time machine for the time being.  We've got a lot of listeners who - we're back to knowing nothing in advance and hopefully remembering what's happened.



LEO:  On we go, Steve.



STEVE:  I've been holding this Picture of the Week for October because of course it is the Halloween month.  And I just got a kick out of this.  It's meant to look like a bulletin board maybe in an elementary school.  So it says "Halloween."  And the question asked to these kids is "What scares you the most?"



LEO:  And they're little kids.  You know, they're like first-graders, yeah.



STEVE:  Yeah, exactly.  Paul says, "Werewolves."  And Nina says, "Sharks."  And down at the bottom, Catherine says, "Ghosts."  But Dylan, who is a little bit, you know, he's following a different path about what scares him most.  He says:  "The demise of Moore's law and its ramifications for modern computing innovation."



LEO:  Ask Little Leo, he might say, "Quantum computing."



STEVE:  Ah, that's right.



LEO:  We talked about - I was stunned.  We talked about this on the panel in Boston.  I thought, oh, you know, this is decades off.  And everybody on the panel said, not only is it coming, and it will break current crypto, but the good news is everybody's already planned for this.



STEVE:  Yup, the academics have needed something to do for the last five years, so they've got post-quantum crypto already ready to launch.



LEO:  We're good.  And I love - the best advice I got, practical advice:  Double your key size, you're good.



STEVE:  Yes, exactly.  That was - exactly.  And it turns out you were at 4096.



LEO:  Yeah.



STEVE:  And you said, "Should I double that to 8192?"  And Bill said, "No, you've already doubled it."



LEO:  I did.  I doubled it.



STEVE:  Because 2048 is all - you already doubled it to 4096.



LEO:  I doubled it years ago from 2048 to 4096.  I thought, you know, just in case.



STEVE:  Yeah.  And Leo...



LEO:  It doesn't hurt.  It's not like modern machines are slowed down by this.



STEVE:  Leo, we have long said, many people have said you are already post-quantum.



LEO:  I'm post-quantum, baby.



STEVE:  You are Post-Quantum Leo.  That's right.  So it seems we are unable to get away from ransomware.



LEO:  Uh-oh.



STEVE:  I know.  This is the current scourge of our time.  The security firm Armor, and we've talked about some of their analysis in the past, they've been tracking ransomware attacks across the U.S.  And last week they published an updated report on the state of the chaos.  In total, and this was a number that caught me by surprise, they report more than 500 U.S. schools were hit by ransomware so far in 2019 alone.  And just in the previous two weeks, 15 U.S. school districts that are responsible for 100 schools were hit, in just the last 14 days.



They tracked ransomware infections at a total of 54 educational organizations, meaning school districts and colleges, which account for this total, the total disruptions at more than 500 schools.  And ransomware attacks appear to have picked up further steam, just as I said, in the last two weeks, with 15 school districts, 100 K-12 schools getting hit, apparently deliberately because they're, like, extra critical at the start of their new school year.  And of those 15 most recent incidents, attacking 15 school districts, Armor said that five of them were caused by Ryuk.  So there has been some identification of the ransomware behind the problem.



Connecticut, the state of Connecticut, was hit by ransomware infections at seven school districts so far during 2019, giving it the dubious honor of being the state whose educational institutions were compromised more than any other this year.  And while Connecticut was visited by the greatest number of ransomware infections targeting school districts, it was the state of Louisiana, and we reported this at the time, that handled the attacks best.  Governor John Bel Edwards declared a state of emergency for his state in response to the wave of ransomware infections that hit three of their school districts.  And as a consequence, they were able to rally multiple state and private incidence response teams together to help their impacted districts just before the start of the year, and they chose not to pay the ransom demand.  It cost them dearly, but they just decided to stick by their guns and not do it.



The Armor report doesn't specify which districts paid the ransom demands and which did not, since that information is not widely available.  Sometimes they'll say, "Yes, we did."  They're public institutions, so to some degree more than private organizations, as we've been talking about, that has to be made public.  But we do know that apparently the bad guys are getting more aggressive.  We talked in the last couple weeks about some demands that were so large that the districts were unable to pay them, even if they wanted to.  It turns out that Crowder College of Neosho, Missouri, reported receiving the highest single demand of all school districts, with the hackers requesting $1.6 million to provide the district with the means to decrypt their systems.



LEO:  That's just an incompetent hacker.  You know?  You've got to know your market.



STEVE:  It is.  Exactly.  It's like, how many movies have we seen where the ransom demand from kidnapping is so over the top that, while Mom and Dad, they want Susie back, it's like we can't raise that amount of money.  We just - we don't have it.



LEO:  Yeah.  That's just dumb.



STEVE:  So there's some uncertainty since this reporting is inconsistent.  A different AV company, Emsisoft, reported that it had identified 62 ransomware incidents impacting U.S. schools in 2019.  And they said that those 62 incidents took place at school districts and other educational establishments.  And their number was also concomitantly higher.  They ended up saying it was 1,051 individual schools, colleges, and universities that they had identified, making it more than double that 500 number that was reported by Armor.  So, clearly, it's a huge problem.



In fact, I have here in the notes somewhere that the - oh, yeah, here - that in response to this, last week the U.S. Senate passed a bill called the "DHS Cyber Hunt and Incident Response Teams Act" which aims to create incident response teams for helping private and public entities to defend against cyberattacks including ransomware.  That bill had already passed through the House, so our two legislative bodies have both said "Yea," and it's expected to be signed into law by our illustrious President probably within the next couple months, as soon as he frees up some time.



So anyway, this all makes very clear that we are in the middle of a serious ransomware, I mean, you would be tempted to call it a "holocaust" because essentially the bad guys have figured out that there is a soft target that has money that cannot afford to be down for long.  And in the reporting that we've done it's been clear that, whether or not you pay the ransom, just bringing these systems back online, even if the ransom is paid and the decryption keys are made available, still costs hundreds of thousands of dollars.  So, I mean, it really is a mess.



Well, that's the public sector.  In the private sector, we talked about that large aluminum producer Norsk Hydro, whose remediation cost in recovering, because they're so large and sprawling, and I guess the stuff really got deep into their network, their remediation cost was originally estimated at $40 million.  That's now expected to actually hit 70, once all is said and done.  But it turns out that an even more expensive attack hit Demant, which is the world's largest manufacturer of hearing aids, Leo, Demant.  I had never encountered them before.



LEO:  Huh.  I wonder if they use another brand.



STEVE:  They may, yes, because they have so many different divisions.  They've been hit by ransomware network-wide, and they're expected to incur losses of up to $95 million following an infection that hit them just last week.  Their troubles started at the beginning of the month, on September 3rd.  I guess, no, wait, at the beginning of last month on September 3rd, when they issued a short statement on their website saying that they were shutting down their entire IT infrastructure following what they initially described as a "critical incident," but didn't provide any additional details.



So what happened to the company's network we'll never know for sure, as Demant never revealed anything except that its "IT infrastructure was hit by cybercrime."  However, reports in Danish media soon pegged the incident as a ransomware event, and it did bear all the hallmarks of such an event from the outside.  From its own statements, the company's entire infrastructure was severely impacted.  And of course we know nothing can do that quite like a prolific ransomware attack.  Their enterprise-wide resource planning system, their production and distribution facilities in Poland, their production and service sites in Mexico, cochlear implants production sites in France...



LEO:  Oh, wow.



STEVE:  ...amplifier production site in Denmark, and its entire Asia-Pacific network were all taken down.  So anyway, it's clear just from that enumeration that this is a big operation.



LEO:  Yeah.  The chatroom has found all the brand names they use.  And the biggest is Oticon, which is very, very big.  But there's also Bernafon, Sonic, Audika, MAICO, Interacoustics, Amplivox, Grason-Stadler, MedRx, and Sennheiser.  So they are...



STEVE:  Sennheiser.



LEO:  Well, Sennheiser Communications.  I don't know if...



STEVE:  Oh, okay.



LEO:  Maybe they own Sennheiser, or maybe they just make - I bet you they make hearing aids under the Sennheiser brand, yeah.



STEVE:  Sennheiser, right, right.



LEO:  But Oticon is a big hearing aid brand.  So that's probably the main one.



STEVE:  Wow.



LEO:  Yeah.  Wow is right.



STEVE:  So they've been out of action for weeks, and in fact they're still recovering assets today, expecting to take an additional two weeks to recover in full.  So anyway, of course, all of those systems are important to their ongoing business.  And they said that the biggest losses came from the impact of not having access to those systems in the first place, of course.  So they have both employee-required networks and then background infrastructure operations stuff.  So they're saying that they're going to cash in - they do have a $14.6 million cyber insurance policy which presumably they once upon a time thought was sufficient to cover the cost of their losses, which unfortunately is now looking like it's going to top $95 million.



LEO:  Whew.



STEVE:  Yeah.  So in a press release last week, Demant said:  "Approximately half of the estimated lost sales relates to our hearing aid wholesale business."  They said:  "The incident has prevented us from executing our ambitious growth activities in some of the most important months of the year, particularly in the U.S.," which they said is their biggest market.  They said:  "A little less than half of the estimated lost sales relates to our retail business, where a significant number of clinics have been unable to service end-users in a regular fashion."  So basically it killed, even like all the way out to the clinic service endpoints across the U.S.



They said:  "We estimate that our retail business will see the biggest impact in Australia, the U.S., and Canada, followed by the U.K.  The vast majority of our clinics," they said, "are now fully operational.  However, due to the effect of the incident on our ability to generate new appointments during September," I mean, so even, I mean, these guys are fully automated, and it shut down their appointment system, all the way out of the endpoints.  They said:  "We expect some lost sales in the next one or two months, which is also included in our current estimate."



They said:  "Our remaining business activities - Hearing Implants, Diagnostics, and Personal Communication - have also been impacted by the incident, but with a relatively smaller overall group impact due to the nature and size of those businesses."  Demant indicated that it expects the incident to have a long-lasting effect on its bottom line.  Previous customers may have been driven to a competitor during the Demant outage, and of course they may never return, having been forced to switch.



So it's certainly useful that they were insured.  But as I noted before, the insurance payout didn't even begin to cover the whole cost of restoring all services.  So this gives us a snapshot into how truly devastating this kind of deep infection that gets into a private enterprise's network can be.  And we don't know, because they're private, whether or not they paid, or whether they recovered from backups. We don't know the inside of this.



However, we do have a story that a three-hospital system in Alabama that was hit did decide to pay its attackers after a ransomware attack knocked its systems offline exactly one week ago, last Tuesday, October 1st.  Officials at the DCH Health System in Alabama declined to say how much the hospitals paid for the decryption key.  But they noted that they have started a methodical process of system restoration.



A notice posted on their website read:  "In collaboration with law enforcement and independent IT security experts, we've begun a methodical process of system restoration.  We've been using our own DCH backup files to rebuild certain system components, and we have obtained a decryption key from the attacker to restore access to locked systems.  We've successfully completed a test decryption of multiple servers, and we are now executing a sequential plan to decrypt, test, and bring systems online one by one.  This will be a deliberate progression that will prioritize primary operating systems and essential functions for emergency care.  DCH has thousands of computer devices in its network, so this process will take time.



"We cannot provide a specific timetable at this time, but our teams continue to work around the clock to restore normal hospital operations as we incrementally bring system components back online across our medical centers.  This will require a time-intensive process to complete, as we will continue testing and confirming secure operations as we go."  Wow.



LEO:  Wow.  It's as bad as it gets, really.  Wow.



STEVE:  It really is.  The system consists of the DCH Regional Medical Center, Northport Medical Center, and Fayette Medical Center.  DCH administrators said that, in the wake of the attack, medical staff have shifted operations into manual mode and are using paper copies in place of digital records, and that new patients are being turned away.  The process will take a while - this is only a week ago that this happened - with the hospitals having a sequential plan in place to decrypt, test, and bring the network systems back online.



DCH officials said:  "Although the attack has impacted DCH's ability to accept new patients, we are still able to provide critical medical services to those who need it.  Patients who have non-emergency medical needs are encouraged to seek assistance from other providers while DCH works to restore our systems," they said.  The hospitals said they are working with law enforcement, outside IT security and forensics experts to address the incident.  For their reporting on this, Threatpost asked DCH how the attack was initiated, but DCH elected not to reply.



BleepingComputer's coverage of this included the additional information that the infecting agent was our new friend Ryuk, that DCH was still not stating how much they paid for the decryptor, but they did confirm that they successfully decrypted multiple servers using the key they received from Ryuk's attackers in return for ransom payment.



LEO:  Oh, so they paid the ransom.  Wow.



STEVE:  They paid the ransom.  But look at it.  Thousands of machines, Leo, thousands.  I'm just like, yikes.  You know, yeah, you may have the key.  But they're all down, and you need to then go in and decrypt it and make sure it's working.  I mean, this reminds me a little bit, I don't think, I don't know if we mentioned on the podcast that you guys were having to turn the lights out after midnight tonight.



LEO:  No, yeah.  Anybody listening live, we may be dark tomorrow, we don't know yet, because of fire danger in the area.  PG&E, our local power company, has decided that somehow that will magically help prevent forest fires or wildfires.  And so  starting at 4:00 a.m. they're going to turn off the power.  They haven't told us when it will come back on.



STEVE:  Yeah.  So, I mean, and so, you know, you've got cameras all over the place.  You've got servers and racks and lots of stuff.  And, like, what's the Skypasaurus or something?  You still have the Skype...



LEO:  Yeah, yeah.



STEVE:  Yeah.  I mean, so it just, you know, this stuff all has to be turned off and then turned back on.  And there is some...



LEO:  We're turning off everything tonight when we go home.  And we're hoping that, when we come in in the morning, we'll be able to turn it back on.  But, you know.



STEVE:  Yeah.  And so, for example, when a server comes up, or a system, it wants to have a DHCP server available to receive its request for an IP.  And like all of the stuff kind of has to come back up in the right order, too.



LEO:  Yeah.



STEVE:  So, boy, it just, you know, I think what we see is, in an instance like this, with this hospital system, no doubt they started with one hospital, and then another one.  And they linked them together, and that was a major project.  And then they linked in a third.  And over time, you know, everything is computerized.  And it's easy to grow what ends up being a huge system, but you just kind of grow it by accretion over time, you know, the way barnacles form on the hull of a ship.  And then, boy, if something comes through and wipes all that out, it's tough to recover.



LEO:  Well, that's why the Battlestar Galactica was able to stay afloat, because Commander what's-his-name didn't like the modern networking technology; right?  Even the phones were just connected by wires.  No wireless.



STEVE:  Yup.



LEO:  Haven't we learned anything since Galactica?



STEVE:  Apparently not, Leo.  And that was actually set a long time ago in the past; wasn't it.



LEO:  Was it?  I know Star Wars was.



STEVE:  I think it was.



LEO:  Oh, well, then there you go.  We should have learned something.



STEVE:  I think that Galactica may have also been.  Anyway, on the same day, also last Tuesday, in Australia, seven major hospitals and several smaller health services from Australia's Gippsland and southwest Victoria regions were forced to either completely shut down some of their systems, or go to manual operation mode following a widespread ransomware infection throughout their IT systems.



The advisory issued from Victoria's Department of Premier and Cabinet said:  "The cyber incident, which was uncovered on Monday," so eight days ago, they wrote, "has blocked access to several systems by the infiltration of ransomware, including financial management.  Hospitals have isolated and disconnected a number of systems" - I love this - "such as the Internet to quarantine the infection, with the isolation leading to the full shutdown of multiple systems, including but not limited to patient records, booking, and management systems."  Which pretty much shuts the hospital down.  The advisory added:  "Where practical, hospitals are reverting to manual systems to maintain their services."  And remember we were talking about the police who were having to fill out tickets by hand.



LEO:  Shocking, shocking.  Savages.



STEVE:  So as expected in the case of ransomware attacks, although the Victoria police and the Australian Cyber Security Centre are investigating the incident, they found no evidence that personal patient information has been accessed.  Well, if you don't count encryption as an access.  So seven major hospitals and smaller health services off, I mean, like shut down, off the 'Net, thanks to another cyber incident.



And in our final piece of ransomware news - actually, let's take our second break, and then we're going to wrap this up with...



LEO:  At some point, Steve, you've going to have to stop doing ransomware reports because it's just going to take over the whole podcast; right?  I mean...



STEVE:  It is, exactly.



LEO:  I mean, just at some point it's going to be shorthand.  Well, they got this guy, this guy, this guy, this guy, this guy, and this guy.  Because...



STEVE:  Exactly.  And then there was ransomware, and then we'll move on, exactly, rather than...



LEO:  All right.  I think honestly, in the long run, this is - I think this is going to be the worst threat of 2020.  Forget 2019, which has been a terrible year. 



STEVE:  Yes.



LEO:  We're just getting started.  And, you know, I predicted back in August, when we were talking about the first ransomware attack on a school, I said, you know, I bet there's going to be a rash of them because all these school systems have been off all summer.  They're going to turn them on, they're going to, oh, look, I got some email.  They're going to click the link.  And I was right, boom.



STEVE:  Yeah.  And lots of them are insured, so they do have deep pockets.



LEO:  They're good, yeah.



STEVE:  And, yeah, and they represent soft targets.  As you said, somebody comes back from a long summer off and goes, oh, look, a prince in Nairobi really needs some help with his cash management.



LEO:  Has some money for me.



STEVE:  That's right.



LEO:  You know, I just knock on wood, we haven't been bit here at TWiT.  We've got really good IT people.



STEVE:  And Leo, I'll just say it is the thing that I worry about more than anything else.  And so my backups have backups and backups because it is potentially such a problem.



LEO:  We do use G Suite.  All our corporate mail is through Gmail.  And Gmail, I think Google does - there's one compelling reason to use Gmail.  They do a very good job of filtering out malware.  And so those malware payloads often just don't get through.



STEVE:  Oh, Leo, in fact, sometimes I try to use my Gmail account to send something to someone.  You can't send anything through there.



LEO:  No.



STEVE:  It doesn't matter, I make the mistake of, like, sending an EXE [buzzer sound].  I zip it up [buzzer sound].



LEO:  Good, good, good.



STEVE:  I mean, you just - you can't...



LEO:  Stop it, Steve.  You should know better.  That's what Dropbox is for.  Or, wait a minute, you're using Sync.com.



STEVE:  And, yeah, I know, I have lots of alternatives, yeah.



LEO:  You've got a way to share, yeah, yeah.  Yeah, I'm not going to say what else we use, but we have a fairly - I think we have, as with any good security system, layers upon layers of defense.  And I don't want to talk about it because I don't want to give anybody any ideas.  But knock on wood.



STEVE:  And as we know, ultimately, backups.  Have offline backups is what you need.



LEO:  Yup, yup, cold backups.



STEVE:  And it's not a nice day when you have to restore from backups.



LEO:  No.



STEVE:  But it's sure better than, like, losing anything.  Or everything.



LEO:  To the ransomware report.



STEVE:  Of course I can't promise we're never going to talk about ransomware again.  I keep trying to promise that, but it just...



LEO:  Well, no, it's important.  You could see these businesses could fail.  I mean, this is huge.



STEVE:  Yes.  In fact, there was an item that I didn't cover where a business did fail.  They were hit, and it was going to take them more time and effort to recover than it was worth, and they just shut the doors.  They just - they're gone.



LEO:  By the way, I love this Ransomware as a Service model.  Oh, my god.



STEVE:  I know.



LEO:  Don't skip this story.  This is too good.  Well, it's up to you.



STEVE:  No, we have to, yeah. 



LEO:  Yeah, we've got to do that.



STEVE:  No, no, no.  Yeah, yeah, yeah.  So that's the new acronym, RaaS, Ransomware as a Service.



LEO:  Oh, my god.  Oh, my god.



STEVE:  So BleepingComputer is reporting that our new friend Sodinokibi, which is also known as REvil, R-E-V-I-L, has successfully assembled what they're describing as an "all star group of affiliate attackers."  Sodinokibi ransomware, as we know, we've talked about a couple times as they're making news lately as they target the enterprise, managed service providers, and government entities.  Remember it was managed service providers that were the key to gluing together all of those small organizations, the small schools, I don't remember what state it was in now, but a whole bunch of them were hit.  It's because they all used a common managed service provider.  The bad stuff got in there and then caused trouble.



So anyway, and also, of course, they're attacking government entities using a handpicked team of what they're describing as "all-star affiliates."  What's interesting is these affiliates appear to have had a prior history with the GandCrab Ransomware as a Service model which used similar distribution methods.  Sodinokibi was first discovered in April exploiting vulnerable WebLogic servers.  And it has seen wide success worldwide through exploit kits; phishing campaigns; remote desktop attacks; and, as we mentioned, large-scale attacks through hacking managed service providers.



In two new reports from McAfee, the Sodinokibi ransomware has been analyzed to provide information about code similarities between this ransomware and its sort of predecessor, GandCrab.  The affiliates of both of these RaaS operations have also been analyzed to reveal similarities between the two and how many affiliates probably switched to Sodinokibi as GandCrab began shutting down.  Remember we talked about this earlier this year, that GandCrab announced a month ahead of time their plans to roll up the sidewalks.  They said:  "We've made all the money we need to.  We've reinvested our ill-gotten gains in legitimate enterprises and in cryptocurrency and online and offline things, and we've decided we're done."  And so they preannounced their shutdown a month ahead of time.



And this was also done to get their affiliates, who were in the process of infecting other people, to make sure the ransomware payments were paid because after a month they were going to shut down their whole payment, the ransomware payment processing infrastructure.  So that all happened.  And we posed the question at the time, what will fill this vacuum which was created by GandCrab's shutdown?  Well, we have the answer now, and it is Sodinokibi.



Okay.  So we know how GandCrab was operating.  We know that they created an affiliate structure.  And to their credit, they gave the affiliates the lion's share of the proceeds.  That is, they only took between - the GandCrab operators took between 30 and 40%, which covered their ransomware management, the creation of the ransomware, and they maintained the payment system.  So we have a chart of this, a sort of a flowchart diagram that shows how the affiliates would have outbound infections to the victims.  But the way the system works, the victims pay the ransomware developers directly, who then provide the decryption keys to the affiliates to then provide to the victims, and then they take a piece of the action.



LEO:  Sure.



STEVE:  Yeah, and then provide 60 to 70% of the proceeds back to the affiliates in order to provide the incentive.



LEO:  Money ruins everything.  We had this nice little ransomware cottage industry, and now big business is moving in, and their affiliate scheme.  Geez.



STEVE:  That's right.  That's right.  So what's interesting is that McAfee analyzed everything that was known of GandCrab and looked at the distribution of proceeds from GandCrab.  There was one particular affiliate, ID 99, that was at the top of the heap.  There were a total of 292 affiliates that were registered with the original GandCrab, although many of those were not highly active.  And you have onscreen right now, and I have in the...



LEO:  I can't even fit it on the screen.



STEVE:  I know.  It's amazing.  So there was an ID and a SubID.  And apparently affiliates were - they were assigned an ID from the GandCrab operators, and then the affiliates were able to assign their own SubID for their own internal tracking purposes.  And so this chart shows all the SubIDs and which infections were found in the wild of ransomware flowing from the SubID, all of which were aggregated under the single affiliate's primary ID.  So, I mean, it's a big deal.



LEO:  Multilevel marketing comes to ransomware.



STEVE:  Yes.



LEO:  You need your down line, man, to really produce here, if you're going to make money on ransomware.



STEVE:  That's right.



LEO:  Geez.  Aw, geez.



STEVE:  Yeah.  And these guys, if we believe the report, they made - the GandCrab operators made many, many millions of dollars, funneled back to them through the work of their affiliates.



LEO:  Because they made it easy for any script kiddie to do it.



STEVE:  Exactly.  Exactly.  So that was GandCrab.  That's shut down.  So now onto Sodinokibi, the inheritor of that.  And here's what's interesting.  A month before Sodinokibi became active, McAfee noted that the highest profile affiliates suddenly went missing from GandCrab's final version 5.2 build.  And then, shortly afterward, as we know and discussed at the time,  a completely new and at the time unnamed RaaS, Ransomware as a Service, started being marketed on online hacker forums such as Exploit.in, where a member named UNKN, you know, unknown, UNKN, was now recruiting affiliates.  So what we believe is a selective pre-recruitment process, which only accepted a limited number of highly vetted applicants, had been initiated.



And in fact the cream of GandCrab's crop was offered to move over to the next-generation ransomware.  One of the people who replied to the topic in the forum and vouched for the Ransomware as a Service, was a member named Lalartu, who stated that they were previously a GandCrab affiliate.  And very soon afterward Sodinokibi exploded with ransomware distribution that was hauntingly similar to the high-profile attacks that had been seen previously with GandCrab.  The evidence collected by McAfee strongly suggests that the GandCrab operators privately informed their top affiliates that they would soon be shutting down and either transferred them to Sodinokibi or that the affiliates decided to move to the new RaaS on their own.  And really, with GandCrab shutting down, I mean, these guys are making a lot of money.  It's more widely distributed than the malware operator, but they're making money.



So check this out.  This is when analyzing the Sodinokibi sample code, McAfee found that Sodinokibi used affiliate IDs and SubIDs in exactly the same way as GandCrab, so the same design.  But the clincher came when the infection code of the two was broken down and placed side by side.  And I have in the show notes a graphic of Sodinokibi on the left, GandCrab on the right, just showing the block level architecture that was automatically, through automation, created.



LEO:  Pretty similar.



STEVE:  Well, is there any doubt in anyone's mind that this is the same architecture?  Yes.  It's quickly clear that the two code bases were in many places virtually identical.  Also, when Sodinokibi connects back to the ransomware's command-and-control server, it does so through a randomized runtime-generated URL.  And as previously found by other researchers, McAfee confirmed that the URLs generated between the two ransomware families are nearly identical.  So, I mean, it must be that Sodinokibi is the new GandCrab.  That is, for whatever reason, maybe it was just time to clean house.  They had acquired those 292 affiliates, and a bunch of them were just sort of deadwood.  So it's like, okay, we're going to start over.



Despite the preponderance of evidence that suggests it is the same, of course we can't be 100% certain.  It is the case that a great deal of malicious code is routinely reused, begged, borrowed, or stolen among the criminal underground.  And who knows.  Maybe the operators of GandCrab really did shut down and give or maybe privately sell their code to another group who said, hey, wait a minute, we want to continue this.  This thing's working for you guys.  So let us do it.



So on the "yes, this is the same group" side, we have the strong similarities and the affiliates who were previously part of GandCrab and are using the same distribution tactics with Sodinokibi.  But on the "maybe not" side is the observation that the Sodinokibi operators' approach seems to be significantly different now as opposed to previously.  With GandCrab, the operators were open and public with their communications.  They  joked with and poked the research community and generally had a good time running their operation.  By comparison, the Sodinokibi operators have been so far much more quiet, secretive, and almost reclusive in how their Ransomware as a Service operates.  So it's left people in the security industry sort of puzzled.



BleepingComputer concluded:  "While personalities can change, the stark contrast between the two makes BleepingComputer believe that Sodinokibi is being operated by the programmers of GandCrab, while the original operators have since retired or moved on to new things.  This would explain the code similarities, yet the different and more secretive nature of the Sodinokibi RaaS as opposed to GandCrab."  And so I summed this up in the show notes by just saying, "Huh, what a world."  Unbelievable.



LEO:  Yeah.  Well, where there's money to be made, that's when the entrepreneurs jump in.



STEVE:  Yeah.



LEO:  Lots of innovation and creativity.



STEVE:  And as we covered in the first 40 minutes of this podcast, there is unfortunately money to be made by causing a great deal of pain to organizations and saying, hey, give us some money, and we'll minimize your subsequent pain.



So in happier news, Microsoft has announced that they will accept more money from more people in return for offering to make their Windows 7 Extended Security Updates, which they have to produce anyway, also available to small and medium-size businesses, thus broadening its availability beyond its previous exclusive access only to their enterprise volume licensing customers.  I've heard you talking with Mary Jo about this, Leo.  And the idea is that the so-called ESUs, the Extended Security Updates, would start being available at the beginning of next year, when Windows 7 updates famously end on, what is it, January 14th is the last one.  So starting February there will be no update, no security updates for Windows 7.



Microsoft I guess looked at the distribution of Windows 7 in the enterprise.  We covered this a couple weeks ago, noting that the only reason that 7 and 10 finally swapped their places as number one and two, which occurred at the beginning of this year, was the combination of end user capitulation and new system purchases; but that in fact enterprise was overwhelmingly still using Windows 7 and Windows Server 2008 R2, which is the server equivalent of Windows 7.  So anyway, I'm using Windows 10 on many of my machines.



LEO:  Oh.



STEVE:  It was - oh, yeah.  It was the OS on the laptop that I traveled with to Ireland and Sweden.  So, you know, all of my loud protestations notwithstanding, I've made a tentative peace with Windows 10.  But I do have access to the Long-Term Servicing Channel, that LTSC edition, which blessedly allows me to avoid ever having to see Candy Crush Soda Saga - oh, my lord - and similar Windows 10 consumer atrocities ever appear on the Start Menu.



But for the most part, enterprises - think about this.  Enterprises small, medium, and large, they have not budged.  And really, why would they?  They have an installed fleet of perfectly working Windows 7 machines causing no one any trouble at all.  Everything is fine.  Work is getting done.  But in every quarterly and annual report since mid-2015, Microsoft has reminded its shareholders and customers that its business plan for Windows 10 includes "new post-license monetization opportunities beyond initial license revenues."  Hmm.



LEO:  What's that mean?  What is that?



STEVE:  Uh-huh.  Windows as a Service.  And forgive me, but I have to rant about this because yes, indeed.  What comes along for the ride in Windows 10?  Every time I see this I just - I can't believe it.  Candy Crush Soda Saga.  Bubble Witch 3 Saga.  I'll never know what happened to Bubble Witches 1 and 2, and I don't care.  We have March of Empires and Disney's Magic Kingdoms.  This is in Windows 10 Professional.



LEO:  Some of those, not all of them, are installed. Some of them are stubs that if you click them it'll bring you to the store and install it.  I agree, it's dopey.



STEVE:  There's, like, little download arrows in the beginning?



LEO:  Yeah, that's what it is.  But some of them are installed, which is really annoying.  When I first installed...



STEVE:  Also Bing, yeah, Bing Weather.



LEO:  Well, you might want that.



STEVE:  Microsoft Solitaire Collection.  The Mixed Reality Portal, as if this reality wasn't mixed already enough.  Microsoft People, whoever they are.



LEO:  You might want that.



STEVE:  Skype.  The Store Purchase App.  Well, yeah, but why not download it if you want it or if you need it, rather than it just being...



LEO:  I run a script.  When I first run Windows, I run a PowerShell script that deletes everything from the apps store except the apps store.  And then you have the choice of doing that.  The other thing I do is I delete all the tiles.  That's a little more manual process.  But then you have a traditional Start Menu without any of those silly tabs.



STEVE:  Yes.  There's even Zune crap in there, Leo.



LEO:  Yeah, I know.



STEVE:  Like two Zune things.  Let's keep that success alive.  Anyway, I can't - so Microsoft is imagining that enterprises - small, medium, or large - would want to jump on this?  Oh, my god.  So anyway, Microsoft is now saying that they will allow all of those businesses, which long ago already purchased and paid for their Windows 7 licenses, to continue using them, rather than submit to what Microsoft has deliberately created in Windows 10, if those businesses will effectively repurchase Windows 7 licenses every year moving forward.  And remember that what we're really purchasing is the privilege of continuing to receive monthly patches for all of the myriad bugs and mistakes which Microsoft already made in the past, and continues to make with every update to their software.  I mean, if this isn't the definition of a racket, I've never seen one.



LEO:  By the way, every year the amount you have to pay to continue to use Windows 7 doubles.



STEVE:  Goes up.



LEO:  Now, anybody who's studied exponentiation will know that it rapidly becomes untenable.  They don't want you to do this for very long.  It's just awful.



STEVE:  So I do imagine - yes,  you're right, Leo.  And this is where you remind us about Linux.  I do imagine that this will be a nice new revenue source for Microsoft.  If given a choice, businesses won't move to Windows 10 ever.



LEO:  Right.



STEVE:  They certainly, you know, they certainly are not doing so now.  Eventually, hardware will die, yes.  And those replacement PCs will come with Windows 10, decked out with all of its myriad "post-license revenue" sources preinstalled.  So, you know, I am truly sympathetic to Microsoft's need to stop servicing their older operating systems.  I am.  But the only option they have given us is to move to a replacement OS that no one wants because of what they have done in this effort to create an ongoing revenue stream for themselves.  And to my mind, that's not okay.



Jared Spataro, corporate vice president for Microsoft 365, said:  "Today we are announcing that, through January 2023, we will extend the availability of paid Windows 7 Extended Security Updates to businesses of all sizes.  Starting on December 1st, 2019, businesses of any size can purchase ESU through the Cloud Solution Provider program.  This means that customers can work with their partners to get the security they need" - for a price - "while they make their way to Windows 10."  Right.



LEO:  Linux.  Linux.  Linux.



STEVE:  Linux.  Linux, yes.



LEO:  I love Linux.  In fact, I don't buy a PC anymore unless I first check to make sure that, should I decide to at some point, I can take off Windows 10 and put Linux on it.  And most modern PCs it's not a problem.  It's just not a problem.  I mean, now Dell comes with - you can get them with Ubuntu.  But if you want to try Windows 10 for a while and then put Ubuntu on there, no problem because, I mean, they have all the drivers.



STEVE:  Yeah.  



LEO:  Linux is really good these days.  It's very good.



STEVE:  Yes.  I had an opportunity toward the end of working with SQRL, actually all the way through, I needed to make sure that it would work under Linux with WINE.  And so I put Ubuntu, the LTSC, the long-term servicing channel, I think it was - maybe I'm confusing my acronyms.



LEO:  There is, you're right, LTS.  No C.



STEVE:  LTS.  LTS.



LEO:  Yeah.



STEVE:  Right, 14 point something or other, I think it was.



LEO:  No, that's old.  No, must have been 1902 is the last one.



STEVE:  Oh, yeah, yeah, you're right, you're right.  And it just came up and recognized all my hardware, and it ran.



LEO:  19.04.



STEVE:  And it was like, oh, okay.



LEO:  That's the thing.  I think people from the old days, god, I remember doing Slackware, and you'd have to then run XFree86 and figure out how to configure your video card and...



STEVE:  X11 stuff and all that.



LEO:  Oh, it was a nightmare.  Now, truthfully, the PopOS, which is the Ubuntu Spin I use from System76, is a much faster, simpler installer than the Windows installer.  You answer, like, four questions, press "go," five minutes later you have a fully installed, running system that works.  It's easy now.  I mean, I just - I'm sitting in front of PopOS on a Lenovo laptop right now.  I love it.  And nowadays, because so much is in the cloud, if you're using Firefox, it's the same.



STEVE:  Yes.  Same Firefox.  You're able to get email, you know, Thunderbird email, for example.  You're browsing the web for most of the things, I mean, this is what you've been telling people for a long time about Chrome is, you know, for your typical purchase, that's all you need.  The era of this super heavyweight massive lumbering OS, it's, you know.  Maybe this  mess with Windows 10 represents the death throes because they're just trying to hold on to what they've got left.  I don't know.



LEO:  Neal Stephenson wrote a wonderful book, "In the Beginning There Was the Command Line."  He wrote it in the late '90s, but it still holds true.  Someday I'm going to bring this in and read the section about operating systems.  Because essentially the thrust of it is there really are only two.  There's the UNIX-based operating systems like Linux, macOS, BSD.  Those are all based on UNIX. 



STEVE:  Yeah.



LEO:  And then there's Windows.  And there's no reason to use Windows.  Because operating systems aren't that complex.  The problems have been solved for decades.  There's only a commercial reason to keep doing new versions of Windows.



STEVE:  Well, I would say they are complex, but they've been growing for decades.  And this is our argument about the idea of China deciding they're going to create their own Chinese OS from scratch.  It's like, you can't.  It's just not possible.



LEO:  No, don't do it.  Because you don't need to.



STEVE:  Exactly.



LEO:  You don't need to.



STEVE:  No one needs to.



LEO:  Don't reinvent the wheel.  They invented it.



STEVE:  If you don't - exactly.  And China, if you're listening...  Oh, wait.  Russia, if you're listening...  No.  China...



LEO:  They're all listening.  Well, actually China did have a Red Linux for a long time that they used that was a...



STEVE:  Yes, and that would be the thing to do.  Take, I mean, yes, do vet it.  But don't try to recreate it from scratch.  It just, I mean, you'll go back to the abacus if you try to do that.  Doesn't make any sense.



So we do have a nasty new - and this is sad, Leo, I know you're going to empathize with this - a nasty new zero-day remote code execution vulnerability in vBulletin.  vBulletin is a very widespread web-based forum discussion, you know, bulletin board.  Two weeks ago, back on Monday, September 23rd, a zero-day exploit written in just 18 lines of Python, basically a proof-of-concept demo, gives any remote attacker unrestricted shell access to any system running the very popular vBulletin forum software.  It was anonymously published to the full disclosure list, after which attackers wasted no time jumping on and using it to install bots, cryptocurrency miners, and whatever they wished.  I have the link here at the very bottom of page 9 of the show notes, the SecLists.org, where it just shows you just this simple little short bit of Python which anyone could easily translate into any other language if they didn't want to use Python.



Tenable Research wrote, they said:  "Tenable Research has analyzed and confirmed that this exploit works on default configurations of vBulletin.  Based on the public proof of concept, an unauthenticated attacker can send a specially crafted HTTP POST request to any vulnerable vBulletin host" - and, by the way, that's all of them, from v5 that's very old to the state-of-the-art latest one - "and execute commands."



LEO:  You shouldn't be able to POST a shell command to a website.  That's ridic- talk about sanitizing your inputs.



STEVE:  Yes, yes.



LEO:  The reason this is such a simple thing is all he's doing is using the POST to put echo shell_exec command.



STEVE:  Yup.  Yup.  Tenable says:  "These commands would be executed with the permissions of the user account that the vBulletin service is utilizing.  Depending on the service user's permissions, this could allow complete control of a host."



Defcon's site uses vBulletin; and Defcon's founder, Jeff Moss, told Ars his team took their site down immediately to avoid getting hacked.  He said:  "We tested it right away, and none of our defenses would have saved us.  We checked the logs and saw no attempts to attack us; but after we patched and went back online, there were two attempts in the first 30 minutes.  Definitely active attackers," he wrote.  And here's what struck me as a bit sad, Leo.  We've talked about the questionable ethics of the "We'll buy your bugs at premium prices" company, Zerodium.  In what struck me as a somewhat tacky and tasteless tweet, Zerodium CEO and founder Chaouki Bekrar, stated that the vulnerability has been privately circulated for years.



LEO:  Oh, yeah.  I'm sure, yeah.



STEVE:  I have the tweet in the show notes.  He said:  "The recent vBulletin pre-auth RCE zero-day disclosed by a researcher on full disclosure looks like a bug door, a perfect candidate for PwnieAwards in 2020.  Easy to spot and exploit.  Many researchers" - this is his tweet.  "Many researchers were selling this exploit for years.  Zerodium customers were aware of it since three years."  In other words, for the past three years, any Zerodium customer who wished to could quietly execute any command they wished on the server of anyone running vBulletin, while in the meantime knowingly leaving every vBulletin system in the world wide open and exposed.



I mean, I suppose that's the on-the-ground reality of any service such as Zerodium.  But it does somehow feel wrong, the idea that this has been sitting there listed in a catalog that Zerodium customers purchase, which would allow anyone - nation states, large corporations wanting to do their own backdooring, I mean, vBulletin is widely used.  And the idea that this thing was sitting there for three years, I mean, again, it's one thing for the problem to exist and not be known.  Here it's listed in a catalog of exploits that you get when you subscribe to Zerodium.



LEO:  It's probably been there forever.  I mean, it's not like  all of a sudden you stop sanitizing inputs.  It's probably been there since day one.



STEVE:  Right, right.



LEO:  But honestly, vBulletin and phpBB are the most popular PHP-based bulletin board softwares.  Just do a google.  Exploit after exploit after exploit.



STEVE:  I know.  I know.  Well, and it's why even though I'm not using them for SQRL's forums...



LEO:  You use XenForo; right?



STEVE:  Yes, I use XenForo.  And I established it on its own physical hardware box with a separate physical firewall between it and all the rest of GRC because I didn't write the code, and I can't vouch for it.  And it's PHP, baby.  So, I mean, and again, good as the XenForo guys are, you know, it's their second or third iteration of a system from scratch, it's just - I just can't afford letting anything get loose and get into the rest of GRC.  So it's physically isolated.



LEO:  We run - our forums, our new forums at twit.community are Discourse, which is a Ruby-based system, much more modern.  But you're right.  We don't run it on our servers.  We run it on their servers.  So if there's a problem, it's not our problem, you know.



STEVE:  Right.  Well, and that's very much the reason why, even though I could run ownCloud or My Cloud, I chose Sync because it's their servers that are doing the synchronization, and my files are encrypted before they leave and after they return.  And so I'm happy to have the cloud knitting things together.  But I'd just rather not have that open exposure because, again I didn't write it myself.  So I just can't vouch for it.



LEO:  Yeah.



STEVE:  I got a nice note.  This was posted in the SQRL forums by Daniel Persson, who's the guy that wrote the Android client for SQRL, the WordPress plugin, and the PAM module for Linux.  He translated from - I guess maybe it was posted in Swedish.  Somebody said:  "Ha.  Added SQRL to my WordPress today."  And then he said:  "@kalaspuffar's" - that's Daniel's Twitter handle - "plugin enabled this in under five minutes."  The guy said:  "In and reconnect to my admin account and remove email address.  Totally strange that this isn't standard for login everywhere."  And then a smiley face.  And then Daniel...



LEO:  Nice.  Can I make a request?  Oh, go ahead.



STEVE:  ...said:  "This made my day."  Yeah.



LEO:  Anybody who has - because I would love to add SQRL to our TWiT forums, but it requires somebody who has a working knowledge of Discourse, our software, and SQRL.  And so if there's anybody listening who could do that, I'm sure there are a lot of Discourse users.  Discourse is very widely used by companies for community forums, including Imager uses it.  Oh, I just saw another company that we were talking about uses it.  It's very popular.  It would be, you know, it'd be another great way to get the word out.



STEVE:  Good.  I will definitely put the word out in the forums and in the newsgroups, Leo.  Discourse.



LEO:  Ask around.  Discourse, Ruby.  Discourse, written in Ruby. 



STEVE:  And I'm sure that there is a plugin authentication option for it where...



LEO:  You know, it might support PAM.  I don't know.  I'll have to look into that.  If it did that, it'd be pretty easy, huh.  Yeah, there's an API.  I'm sure there's all sorts of authentication choices.  So, yeah. 



STEVE:  Hooks, hooks.  Cool.



LEO:  It supports OAuth.  It supports a lot of the more traditional authentications.



STEVE:  The standard, yup.



LEO:  Yeah, yeah.



STEVE:  Cool.  So more than two years ago, after the event which briefly rocked the world, and which Marcus Hutchins inadvertently but fortuitously stalled this groundbreaking worm, or earthshaking worm, Sophos recently took a look at the state of the WannaCry worm today.  We'd like to say that it's gone, but not forgotten; but we can't because it turns out it's not gone.  Okay.  So what happened?  As we know, on May 12th of 2017, organizations across the world were attacked by what was then a new and unknown, very rapidly spreading piece of malware which we now know as WannaCry.  It's now considered one of the most widespread and notoriously destructive malware attacks in history, which was halted only when, out of research curiosity, Marcus registered a domain name that he found embedded in the malware, which unexpectedly and happily acted as its kill switch.



But the kill switch didn't completely kill it.  And today, more than two years later, WannaCry continues to adversely affect thousands of computers worldwide, although it doesn't get any press attention, and we'll explain why.  In fact, it's joined the legions of worms, we know their names - Code Red, Nimda, MSBlast - which continue to contribute to constant Internet packet noise for which I long ago coined the term "Internet background radiation."  It's just like, if you put a system on the 'Net, packets start coming in that you didn't ask for.  It's just background radiation. It's these things that will never die, that are still running on servers in obscure places.  And then the machines haven't rebooted, or the worms are written to permanent storage, and they arrange to restart after reboot, whatever.



But on that fateful day in May 2017, WannaCry stormed across the world.  It was, as we know, made extremely prolific by its use of the EternalBlue vulnerability and exploit which was believed to have been stolen from the U.S. NSA, the National Security Agency, by a group of hackers calling themselves the Shadow Brokers.  And WannaCry provided another vivid example of the other thing we're often talking about, the so-called "patch gap," which continues to exist today since the Windows flaw, which was exploited by EternalBlue, had already been found, fixed, and patched in March of 2017's Patch Tuesday, a little more than two months before WannaCry's trans-Internet rampage.



If everyone had patched their Windows machines within those following two months, WannaCry would have never gotten a start.  Nothing would have happened.  It would have knocked, but not been able to get in.  It couldn't have propagated laterally across enterprises.  It would have just been game over.  But as it was, a great many Windows systems were behind on their patching.  And believe it or not, they're still not patched today.



So WannaCry entered into a target-rich environment and infected something like, it's estimated, 200,000 victim machines in the blink of an eye.  And as I said, not everyone is patched even now, more than two years later.  And WannaCry is not only still alive and, for reasons I'll explain in a minute, now ignoring the kill switch that was designed to stop it, but possibly more alive than ever.



Okay.  So what's with the original kill switch?  Why was it there?  Unless the creators of WannaCry themselves explain their motivation, we'll never know for sure.  But there are two prominent hypotheses.  Either the attackers wanted to have for some reason a way to stop the attack at their discretion, or the more likely hypothesis is it was a deliberate anti-sandbox evasion technique.  Some sandbox environments fake responses from connections to URLs to make the malware that they're examining think that it is still connected and able to access the Internet, when in fact it's being deliberately prevented from doing so, so that it can't do any harm.



Since the domain name was deliberately unregistered, the attackers knew that if a DNS lookup were to succeed, it could only have been because the malware was under analysis in a sandbox designed to make it think that it's on the Internet, thus positively responding to any DNS query.  So then the malware would end the attack to hide its true nature.  If this was the motivation for the kill switch, this meant that Marcus's actions effectively turned the entire world into a sandbox and shut down the worm's spread globally.



And it's been a long time since we've talked about this, two years.  But the domain name looked like the bad guys just pounded on the keyboard.  I mean, there's, like - in fact, it literally looks like that.  I see ja's occurring, jae, jap.  So there's like some recurring letters.  It's, you know, iuqerfsodp9ifjaposdfjhgosurijfaewrwergwea.com is the domain name.  So, yeah, bang on the keyboard for a while.  We don't have to worry about that being an actual site.  Marcus saw that in the code, registered that crazy domain name, shut the worm down.  And as we know, if that was not done, WannaCry's payload would execute.



It did execute in 200,000 machines and would encrypt the files of the victim, then post the infamous extortion screen, which we showed at the time.  I have it here reproduced in the show notes.  And I always got a kick out of it.  The headline, it starts off with, "Ooops," as if this was a mistake.  "Ooops, your files have been encrypted."  Yeah, ooops.  And then it's got, remember, the countdown meter over on the left showing that the payment cost will be raised in, and your files will be permanently lost within a week if you don't pay, and they're not even going to bother to keep the encryption key.



And what they were asking for at the time, it's kind of quaint now in today's numbers, was please send us $300 worth of bitcoin to the following address.  And there were four addresses in the code that would surface randomly that it would choose from.  Anyway, so back then the ransom was $300 to recover your files, presumably on a personal machine.  Of course now we know that 1.6 million was recently asked for the decryption of a sizable network.  So there are a couple of interesting notes.



LEO:  Oh, PG&E is really going crazy now.  They're sending out a warning.



STEVE:  Oh, no kidding?



LEO:  Yeah, we're getting an emergency alert.  "Caution.  Long-term..."



STEVE:  On your phone?



LEO:  Yeah, getting one of those presidential alerts, you know.  "Caution.  Long-term power outage plus high fire danger."



STEVE:  Wow.



LEO:  Like, come on, dudes.  Personally, I think they're blackmailing the California state legislature.  What they want to do - I know what's going on.  They're being sued for billions of dollars for the last 2017 fires.  And I know what they really want is the California state legislature to say, oh, you can't sue PG&E anymore.  And then the power will magically come back on.  You watch.



STEVE:  Wow.  So it's like, okay, if you're going to hold us responsible, we're going to turn...



LEO:  If you're going to sue us, well, we're just going to have to turn off the power.  They've never done it before.



STEVE:  Wow.



LEO:  It's a show of strength.



STEVE:  Well, when the lights go off, people are going to want their lights back on.



LEO:  Oh, yeah.



STEVE:  You know, your electric toothbrush runs for actually quite a while without power.



LEO:  Yeah, I'm worried about brushing my teeth.



STEVE:  Yeah.  But, boy...



LEO:  Sorry to interrupt.  But the state of California just did, or somebody.



STEVE:  Yeah.  You often go into a room and flip the switch, and it's like, wait a minute.  Oh, that's right.  It's amazing how handy power is.



LEO:  Oh, it's amazing when the power...



STEVE:  Especially for geeks like we.



LEO:  What am I going to do?  I'm going to have to go out and take a walk.



STEVE:  Charge up all your batteries, yeah.



LEO:  But I don't think I'll have Internet, either.



STEVE:  Oh, that's true.



LEO:  Actually, sadly, our house is - not sadly, happily.  Our house is not in the map that TWiT is.  The old studio's not.  We're just right on the edge of the map.  So I don't know what's going to happen.



STEVE:  Interesting.



LEO:  Yeah.  I'm sorry to interrupt.  Go right ahead.



STEVE:  Yeah, no problem.  That is definitely newsy.  There are a couple of interesting notes about how WannaCry was portrayed at the time of its initial outbreak.  For example, and our listeners will remember, despite the suggestions that it was unpatched Windows XP computers primarily responsible for WannaCry's rapid spread, this mistake was due to the fact that some of the more high-profile attacks were XP-based.  More than 97% of WannaCry detections at the time were actually coming from the newer Windows 7 operating system.



It's also worth noting that, while a computer patched against the EternalBlue exploit is no longer vulnerable to being infected by a remote connection from another WannaCry infected computer, in other words, that was the way things were getting in was over the SMB file and printer sharing connection and port.  If that computer, the patched computer was infected before it was patched, it will still be trying to infect other computers.  The anti-EternalBlue patch only prevents the vulnerability from being exploited, not from exploiting others.  And if nobody had since updated WannaCry, that is, if it was the original WannaCry, that file that started spreading on May 12th of 2017 would be the same as the file seen in the wild today.



But it turns out the reality is very different and much more intriguing.  There's a new WannaCry.  Sophos's research is based on a signature named CXmal/Wanna-A, which is the detection name that identifies when a computer suddenly finds the WannaCry payload, which was a file named mssecsvc.exe, so Microsoft Security Service, mssecsvc.exe, plopped into the C:\Windows directory.  On a Sophos-protected machine, the client applications immediately, meaning the client AV, immediately blocks and removes any such file.  Using this detection data, Sophos has been able to see how many computers are being attacked repeatedly by other computers, that is, causing new instances of that file to be dropped into the Windows directory, as well as the file dropped during the attack.



These infected machines could be on the same network as the ones being attacked, or possibly anywhere in the world.  All we really know about the infected machines that attempt to spread the infection is that they don't have a working AV on them because certainly by now all AV has been updated to detect WannaCry.  Otherwise they would have stopped WannaCry and would not be attempting to infect other machines.  In the three-month period from October 1st, 2018 through December 31st, 2018, so  the last quarter of last year, Sophos logged - get this - 5,140,172 detections of CXmal/Wanna-A, nearly two years after the original attack.  As nearly every machine that can install the EternalBlue patch has already done so, why are there so many detections?



LEO:  Yeah, good question.



STEVE:  As a sanity check, since the data was nearly a year old, Sophos just in August, two months ago, reran their queries, looking at just one month of attack data, August 2019.  They discovered that in that month alone, they had recorded more than 4.3 million attacks against their customers' machines.  It seems like a significant increase, but those numbers can be misleading because the data is based on customer machine feedback, and the number of customer reports changes over time as the size of their customer base changes, presumably increasing as they're growing.  So that can make the problem seem like it's getting worse when in fact it is uniform.



What was important to note is that the proportion of the total number of attacks targeting Sophos customers in specific countries remained consistent in the data from 2018 and now this recent data in 2019, with the machines in the U.S. topping the list of countries most subjected to failed attempts at WannaCry infections.  The fact that WannaCry is still going at all raises some interesting questions.  Are all these machines really still not patched?  Why is the kill switch not preventing the infected computers from trying to attack others, as indeed they are?  Why is no one complaining about files being encrypted?



So Sophos knew the answer to the first question already, that is, are all these machines really still not patched.  This CXmal/Wanna-A detection is only possible on unpatched machines.  To be sure of this, they investigated a random selection of computers to manually verify that they had indeed not been patched against EternalBlue or anything else in the last two years.  And that is the case, even though Sophos's AV is on those machines, they are never being updated for more than two years.



To answer question two, why is the kill switch not preventing the infected computers from trying to attack others?  Because that's what it's designed to do.  They know the computers reporting the detections have Internet access because that's how they obtain their data.  Since those machines are most likely being attacked by infected computers on the same network, it seems likely that those attacking machines would also have Internet access.  So why isn't the kill switch stopping them?



Analyzing those 5.1 million detections over last year's three-month period, from October 1st through December 31st, they discovered something unexpected.  The malicious file being dropped on these computers was not the original WannaCry mssecsvc.exe file.  In fact, among the 5.1 million detections, they identified 12,481 unique files.  The original true WannaCry file was only seen 40 times, a number so low that it could easily be attributed to testing rather than real attack.  12,005 of the unique files identified were seen fewer than a hundred times each.  So also rare.  476 of the unique files accounted for an overwhelming 98.8, almost 99% of the detections, with 10 of those files accounting for 3.4 million of the detections and the top three accounting for 2.6 million.



So they analyzed those top 10 most prevalent files and quickly saw that they had all been altered from the initial released WannaCry code.  The alterations in all 10 samples bypass the kill switch entirely.  This means that these 10 updated WannaCry variants' ability to spread is no longer restrained by the kill switch.  So they examined all of the files they had discovered and found four different techniques which have been used to render the kill switch ineffective.  They found one simply removing or changing the kill switch URL.  In roughly half the samples, they simply removed the URL completely.  The next most common approach was to change the last two letters from "ea" to "ff."  So modifying the kill switch domain resulted in these variants of WannaCry again being unable to obtain a DNS resolution, freeing them to attack.



The second method was to change the code to instruct the malware, regardless of the result of the kill switch test, move to the next command.  In other words, just change, tweak the code, a byte or two of the code.  They also found an instance where the kill switch was nop'd.  The actual op codes were just changed to do-nothing codes, so it didn't even bother doing the test.  And the fourth method was a 2-byte jump instruction to jump over the code that checks the result of the kill switch connection, also resulting in just a simple continuation of the attack.



What's really interesting is that all four of these techniques were implemented with hex editors, hex editing the original WannaCry malware executable binary file, not by recompiling from the original source code.  Since anyone who obtains a copy of the executable binary file is able to hex edit it, this suggests that many miscreants around the world, or four or five, were attempting to immediately, two days after Marcus stopped it, because that's how old these are, to immediately untether and unleash the original WannaCry malware worm upon a still unsuspecting world.



We know that the original WannaCry kill switch is still crucially important because, in a recent interview of Jamie Hawkins, who actually was working with Marcus on that fateful domain-registering day, Jamie indicated that in June, just a few months ago, of 2019 alone, the kill switch is known to have prevented about 60 million ransomware events.  In other words, today, if that domain were not still registered, 60 million systems would be encrypted by WannaCry, a virulent strain that is still potent and still trying to do this.  It's performing those DNS lookups.  The answer comes back, yeah, we've got an IP, and it just shuts it down.



If that domain name were not still registered, in one month, 60 million systems - well, or 60 million instances.  We don't know how many times each system is doing a DNS query, so it's not 60 million systems, it's 60 million queries.  Certainly a bunch of systems.  That indicates that there are still potentially at least thousands of computers infected with the original WannaCry, and keeping that kill switch domain online is the only thing preventing a second outbreak.



Okay.  So if all of those manually edited, with a hex editor, copies of WannaCry have been unleashed by the hex editing, why hasn't a second massive wave broken out?  To answer that question, Sophos executed a random - actually ran, they executed a random selection of samples, including the top 10 that were most prevalent on unprotected computers.  In each case, no files were encrypted, and no ransom notes were created.  Turns out there's one component that spreads the malware to other machines, and then there is a separate component that does the encryption.  This second component is contained within a password-protected ZIP archive.  The contents of the ZIP archive are extracted to the computer and then used to execute the ransomware phase of the attack.



In all 2,725 samples, that ZIP archive was corrupted.  Errors during extraction appeared after only a few of the archive files had been extracted from the contents, and the extraction stopped.  That was the discovery they were seeking which made everything else make sense.  The large volume of detections were due to the lack of a kill switch.  But nobody was complaining about their files being encrypted because, in every single sample seen in the wild, the archive was corrupt and would not decompress, so the system would not decrypt anything.  There but for the grace of God.



Sophos researchers were not the only ones to spot this.  Back in May of this year, Kevin Beaumont tweeted - and I have his tweet in the show notes.  He tweets, as we've often talked about Kevin's work, he's a well-known security researcher, he tweets as @GossiTheDog.  He said:  "Probably the most interesting WannaCry thing now is it is still spreading.  In fact, there's more spreading than when it began.  Why don't we hear about it?  Somebody broke the ransomware portion of the variants going around.  Almost all of them, like 99% plus, don't unpack."



So as it turns out, this broken payload that appeared almost immediately following the original infection was inadvertently duplicated and promulgated by the bad guys.  On May 14th, two days after the May 12th original WannaCry event, researchers at Kaspersky discovered a variant of WannaCry that had been uploaded to VirusTotal earlier that day.  They shared the sample with researcher Matt Suiche, and in a blog post that same day he confirmed that the sample did not have a kill switch, and that the archive was corrupt.  So that appeared two days after the original infection.  It was also noted that, while the sample had been uploaded to VirusTotal, it had not been seen in the wild.



This sample led to Sophos's final discovery.  The MD5 hash of the file uploaded to VirusTotal, which doesn't have a kill switch and doesn't encrypt files, is none other than the exact same file they now see causing the highest number of WannaCry detections.  It is number one on the unique file variants list shown earlier, causing 29% of all WannaCry detections in their data.



Even more amazing is that the top three files on their list are all variants of this same file.  So the bad guys copied the bad one and put it out there.  The other two files contain the same corrupt archive.  The only difference is in how the kill switch has been removed.  And we talked about those variants.  In other words, it's really sort of a true miracle that a subtle mistake made just days after the first WannaCry wave prevented another utterly unrestrained and unrestrainable devastation that would have occurred with WannaCry's immediate return.  We lucked out.



LEO:  But how long can we stay lucky?



STEVE:  Uh-huh.  Yup, yup.  Amazing.



LEO:  What a world we live in, my friend.



STEVE:  Fun to describe it, however, to our listeners, who really appreciate our efforts here.



LEO:  Yeah, yeah.  Yes, thank you to all of you.  It was great fun to see you in Boston.  We'll look forward to doing more of these events with Steve.  And don't forget our SQRL expos.  We'll talk about the ins and outs of SQRL next month, November 30th, the Saturday after Thanksgiving, a special event. 



STEVE:  Assuming that the lights are on.



LEO:  They will be.  The fires are over by then.  Fires always.  And, you know, a number of people have said, oh, be careful, stay safe.  There are no fires right now.  They're doing it proactively.



STEVE:  Preemptive, preemptive, yeah.



LEO:  There are no fires right now.  I just want to say that, although we had some terrible ones a couple of years ago.  Steve does this show, barring fires and trips to Europe, every Tuesday around 1:30 Pacific.  That would be 4:30 Eastern time, 20:30 UTC.  If you want to watch live, oh, you can.  TWiT.tv/live or listen live, too.  That's where our shows stream.  We make on-demand versions available on our website, TWiT.tv/sn.



Steve has them, too.  And he has a couple of unique versions of the show.  Of course he's got the regular 64Kb audio, but he also has 16Kb audio for people who want smaller files, and human-transcribed versions which really make it a lot easier to read along while you watch or listen.  And that's all at GRC.com.  That's Steve's website.  While you're there, pick up a copy of SpinRite, the world's best hard drive maintenance and recovery utility.



STEVE:  Which just for the record I will be getting back to very shortly.  I posted a plan to the SQRL groups, telling them that I was going to have to tweak the documentation a little bit to update it to some decisions we made just before I left for the European tour.  And then I'm going to make a tiny change to the server.  I'm not going to bother to change the client.  I've got a list of things to do, but none of them are showstoppers.  There's just some little cosmetic-y things.  As with the software that I create, there are no bugs in it.  So it's fine.  But I am not going to delay getting back to SpinRite.  I am going to get back to it very quickly.



LEO:  Yeah.  And Steve and I talked while we were in Boston about the plans, and this is very exciting.  So I look forward to that SpinRite 6.1 and then, maybe someday in the distant future, SpinRite 7.  But let's not rush things along.  GRC.com.  You can tweet at him, @SGgrc; or you can send feedback at GRC.com/feedback.  That would be a - both of those are good places.  We have our new TWiT Community.  I mentioned our forums.  Steve's got his forums.  They're the SQRL forums.  We have forums, too, at twit.community.  You can leave your thoughts and comments there.  All our shows our posted there, and that's a great place for you to get in a conversation with other people who watch the show, as is our chatroom at irc.twit.tv.



Don't forget to subscribe to the show.  That's really the best way.  That way you'll get every episode the minute it's available.  And you'll never miss an episode.  Even if, for instance, Windows Weekly gets deferred tomorrow, if you subscribe, you'll get it the minute we've recorded it.  Same with This Week in Google and all of our other shows.  So subscription is probably the best way.  Then you don't have to worry about our schedule.  You can get it on your schedule.  Thanks, Steve.  We'll see you next time on Security Now!.



STEVE:  Yay.  Thanks, Leo.



Copyright (c) 2019 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.






GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#736

DATE:		October 15, 2019

TITLE:		CheckM8

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-736.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we take a look at a sobering supply chain proof-of-concept attack, an update on the ongoing encryption debate, a blast-from-the-past password decryption, an intriguing security and privacy consequence of today's high-resolution consumer cameras, and the sad state of consumer security knowledge.  OpenPGP gets a nice boost, Windows Defender gets Tamper Protection, and SQRL gets a very nice mention by Google's Cloud Security architects.  We'll share a bit of sci-fi and fun miscellany, then conclude by examining the crucially important, widely available, and completely unpatchable Apple Boot ROM exploit known as "CheckM8."



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here with lots to talk about, including supply chain security, end-to-end encryption - it's coming to Thunderbird, very excited about that.  And they finally cracked Ken Thompson's Unix password.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 736, recorded Tuesday, October 15th, 2019:  CheckM8.



It's time for Security Now!, the show where Steve Gibson saves us.  Basically, we're going to change all the promotional copy to just "Steve Gibson saves us all."  That's all we need to say.



STEVE GIBSON:  Or sometimes worries us, sometimes confuses us.



LEO:  Scares the hell out of us and then saves us.



STEVE:  That's right.



LEO:  Goes hand in hand.  Hi, Steve.  How are you?



STEVE:  Great.  And great to be with you for Episode 736 for the middle of October.  Something really interesting has happened that I wanted to put into context because there's been some indubitably - or not indubitably, invariably is what I was looking for - invariably some confusion about...



LEO:  Inevitably.



STEVE:  Inevitably.  Inevitably.



LEO:  There you go.



STEVE:  What is probably going to be a significant event for the industry because a security researcher/hacker found a flaw in all Apple iOS boot ROMs...



LEO:  Oh, boy.



STEVE:  ...from the A5 through the A11, which is like from the iPhone 4S to up and including the iPhone X, which I have.  Not the next generation, the X whatever they are, XX, the XS and the...



LEO:  12 and 13, the A12 and A13, yeah.



STEVE:  Yeah.  So not the A12 and 13 Bionics, but up through the A11.  What's significant about this is that it cannot be fixed because it is in the Boot ROM, which is by definition, you know, it's the ROM that's burned in at the factory.  And anyway, so the exploit was called CheckM8, C-H-E-C-K-M-8, which is what we're going to talk about as our main topic at the end of the podcast.  And our listeners who are sick and tired of hearing about ransomware will be glad that, except for that, I will never mention the word during this podcast because there's a lot of other stuff to talk about.  We have a rather sobering supply chain proof-of-concept attack.  And I want to take a minute to use that to talk about supply chain attacks because, although we've had some false reporting, they are real.



We also have an update on the ongoing encryption debate.  A fun blast-from-the-past password decryption that you probably already know about.  Anyway, we will make sure all of our listeners do.  An intriguing security and privacy consequence of today's high-resolution consumer cameras.  The not very surprising, but interesting to have numbers, sad state of affairs with the knowledge of security in American consumers.  A nice piece of news about OpenPGP.  You've always been, Leo, a long PGP booster.  And a mainstream email client is going to get OpenPGP built in natively.



LEO:  Oh, that's great news.  That's awesome.



STEVE:  Yes, yes.  Also Windows Defender gets Tamper Protection.  SQRL gets a very nice mention by Google's Cloud Security Architects.



LEO:  Ooh.



STEVE:  We've got a bit of sci-fi.  Yeah.  A bit of sci-fi and fun miscellany.  And then we're going to finish by talking about what it means to end users, what it means to the security industry, the fact that essentially any earlier devices, many necessarily still supported by Apple, are now jailbreakable, and there's nothing Apple can do.  And that's got big consequences.



LEO:  CheckM8.



STEVE:  So I think another - yes, CheckM8.  Well named.



LEO:  Well named.



STEVE:  Well named.  Another great podcast for our listeners.



LEO:  I'm excited.  Are you going to talk about the Sudo flaw, the S-U-D-O flaw?



STEVE:  You know, I meant to.  It was in my notes.  And I didn't.



LEO:  Okay.  Because that's a - I don't know if it's something of real concern.  My suspicion is it's not.  But every Linux is updating.  Sudo is used to escalate your non-privileged account to a superuser.  And it's a great bug because, if you use the user minus one, it just authenticates without a password, and suddenly you're superuser.  And of course you know exactly what happened when you hear "minus one."  You go, oh, okay.  It's a buffer overflow.



STEVE:  Yeah, wow.



LEO:  And, you know, because Sudo is on everything, including all Macs, as well as all Linux computers, it's only Windows that wouldn't be affected by this.



STEVE:  Yeah.  In fact, I have to use it, and I do, when I'm playing with my Drobos because the normal user account is not privileged, and so you have to elevate yourself in order to do stuff with the file system.



LEO:  Yeah.  It's probably not going to affect anybody; but nevertheless, everybody is updating right now.  So that's something to be aware of.  But we'll take a break, and we'll come back, and I want to hear all the story that you've got about this flaw in iOS because god knows we're all using it.  All right.  Thank you, Mr. Steve.



STEVE:  So I titled this "A sobering reminder about supply chain attacks."  They're not common.  And, no, they won't be employed by random 400-pound antisocial hackers living in their moms' basements.  But anyone who completely discounts them and imagines that they are impossible is fooling themselves.



Andy Greenberg, writing for Wired magazine, reintroduced the idea by reminding us.  He wrote:  "More than a year has passed since Bloomberg Businessweek grabbed the lapels of the cybersecurity world with a bombshell claim" - as we covered at the time, as we well know, and watched them being debunked - "that Supermicro motherboards in servers used by major tech firms, including Apple and Amazon, had been stealthily implanted with a chip the size of a grain of rice that allowed Chinese hackers to spy deep into those networks.  Apple, Amazon, and Supermicro all vehemently denied the report.  The NSA dismissed it as a false alarm.  The DEF CON hacker conference awarded it two Pwnie points for 'the most overhyped bug' and 'the most epic fail.'  And no follow-up reporting has yet affirmed its central premise."



LEO:  However, it's not been debunked, either.



STEVE:  Exactly.  And what captivated me, as our listeners may remember at the time, was the total feasibility of it.  I mean, there was, like, yes, this could happen.  And the fact that it was a false positive report this time doesn't mean that it's never going to happen.  So, and actually that was sort of the impetus for an event that will be occurring next week in Stockholm, Sweden during the - and there's no way to say this.  It's CS3STHLM.



LEO:  Yeah.  Yeah, baby.



STEVE:  Yeah, rolls right off the tip of the tongue.  It's a security conference which bills itself as the premier cybersecurity conference for the ICS/SCADA and Critical Infrastructure world. 



LEO:  I'm guessing they probably call it CS3 Stockholm.



STEVE:  Ah, I bet that's what it is, yes.  So during that conference, security researcher Monta Elkins, who works as the self-described "hacker in chief" for the industrial control security firm FoxGuard, will be showing off his handiwork.  He will be next week vividly demonstrating just how easily spies, criminals, saboteurs with even minimal skills - I mean, literally the guy in his mother's basement could do this if he were motivated to and had some technical background - working on a shoestring budget can implant a chip in enterprise IT equipment to create a stealthy backdoor for themselves.



So in this instance, Elkins, armed with $150 hot air desoldering tool, a $40 microscope, and some $2 chips ordered online - and you're showing it now.  This is the Picture of the Week for the podcast, Leo.  That little red circle in the lower left is the chip he added, the $2 chip he added to a Cisco ASA 5505 firewall appliance.



LEO:  So this is a small board by itself.  It's kind of hard to tell the scale.



STEVE:  To get a scale for it, yes.



LEO:  Yeah, it's just a few inches across.



STEVE:  Yeah, maybe eight inches, kind of eight by eight.  Anyway, so Andy Greenberg interviewed Elkins, who said:  "We think this stuff is so magical, but it's not really that hard.  By showing people the hardware, I wanted to make it much more real for them."  He says:  "It's not magical.  It's not impossible."  He says:  "I could do this in my basement.  And there are lots of people," he says, "smarter than me that can do it for almost nothing."



So what he did was he used the ATtiny85 chip, which is about 5mm on a side.  It's got four tiny mounting and connecting pins on opposite sides from each other.  He pulled the chip from a $2 Digispark Arduino board.  He says it's not quite the size of a grain of rice, but it's tiny.  After writing his code into that chip, he desoldered it from the Digispark board and soldered it to the motherboard of that Cisco ASA 5505 firewall appliance.  He found an inconspicuous spot that required no extra wires, which could give the chip access to power and the firewall's serial port.



And as I mentioned, our Picture of the Week shows it.  I have a larger version of it a couple pages down.  He noted that he could have used an even smaller chip, but chose the ATtiny85 because it was easier to program.  He says he also could have hidden his malicious chip even more subtly inside one of several radio frequency-shielded cans on the board.



LEO:  Yeah.  I mean, if you looked at the motherboard you would see this.



STEVE:  Oh, yeah.  But looking around, I mean, unless it was circled, we wouldn't know.



LEO:  Yeah, yeah.  Good point, yeah.



STEVE:  You know, it looks likes all the other ones.



LEO:  It looks a little bit hand-soldered.



STEVE:  Yeah, there's a little lumpiness, yeah.  It's a little lumpy, yeah.  



LEO:  But still.



STEVE:  Anyway, so - and he deliberately wanted to be able to show, you know, to hold the board up at the security conference, he says, in order to point to it and say here it is.  So what does this tiny, itty-bitty chip do?  Elkins programmed his tiny stowaway to carry out an attack as soon as the firewall boots in a target's datacenter.  It impersonates a local security administrator who would access the firewall's configuration by connecting their computer directly to the firewall's hardware admin port.  So at boot the chip triggers the firewall's password recovery feature to create a new admin account for itself, which then allows subsequent full remote admin access to the firewall's configuration by a remote attacker.



He noted that he chose the Cisco's ASA 5505 firewall for his demo because it was the cheapest one he found on eBay.  But he noted that any Cisco firewall that offers that sort of recovery in the case of a lost password - and they all do, you know.  They presume, if you're able to attach a serial cable to the serial port of your computer to the firewall, well, then, you know, you're God of that device.  You are physically proximate to the device, and you're able to implement serial commands, not Internet port-based commands.  So he also noted that with a bit more reverse engineering it would also be possible to reprogram the firmware of the firewall to make it into a more full-featured foothold for spying on the victim's network, though he didn't go to any trouble of that sort for his simple proof of concept.



So I liked this story because I wanted to highlight the triviality of this proof of concept for our listeners.  For non-hardware types, this sort of thing might seem like exotic sci-fi.  But in today's world, it really isn't.  It is extremely possible to hide this sort of thing in plain sight.  Think of it like what's happened with today's operating systems.  The graybeards among us - you and I, Leo, and probably half of our listeners - probably fondly recall those days of yesteryear when we knew and could identify every file on our 10MB hard drive of our PC.  Back then we knew the five files that MS-DOS was using.  And when you installed a piece of code, you installed its executable and then an ini or a config file or something.  I mean, we knew exactly what was going on.  And we sort of thought of it as that's the way it's supposed to be.



Well, needless to say those days are long gone.  Now we have no idea what the heck is loaded on our multi-gig, if not multi-terabyte drives.  And if you look at the process monitor, you don't even know what is running on those machines.  It's just full of stuff.  So talk about hiding in plain sight.  Similarly, looking at any modern motherboard, or a security appliance like that firewall, and we saw the picture of it in the Picture of the Week, we see a literal sea of tiny chips.  We have no way of knowing whether they are from the factory or not, especially if they are cleanly and carefully added.



And again, I'm not wearing a tinfoil beanie here.  I know that this likely doesn't affect any of us, and never will.  But the lesson we keep learning, the thing we keep seeing is that what can be done is done.  And we know that cyberespionage is today a real thing.  And for a nation-state-scale actor, intercepting the physical shipment of a device is not difficult.  And the payoff from implanting an undetectable hardware backdoor could be significant.  And of course that's why the Bloomberg piece a year ago generated so much fervor is that no one discounted what it would mean if that was actually happening, if somebody had successfully planted a spy chip in a huge number of devices.  Well, for a nation-state actor it doesn't have to be a huge number.  You're not shooting a shotgun, you're targeting with a sniper scope.  And so intercepting just one device bound for a destination that matters is theoretically all you need to do.



So anyway, so I thought about it.  If I was really, really worried and had to be safe, what would I do?  Well, one possibility for someone who's really concerned would be to ask a favor of a completely unaffiliated company to purchase the hardware that you want on your behalf and then pick it up from them.  In other words, avoid the possibility of anybody monitoring the purchases that you're making and intercepting the hardware.  And of course another possibility is to note that virtually all of these turnkey appliances are typically just offering prepackaged solutions.  Anything they can do can probably be done just by gluing some off-the-shelf open source Linux or Unix apps together in any generic server PC.  For example, Linux and Unix firewalls today are extremely capable.  You can roll your own to do anything that you might purchase an off-the-shelf appliance to do.



So there are ways to work around this problem, if it was really something you were worried about.  I just sort of wanted to take this opportunity, the opportunity of this guy making this presentation next week, so easily making a modification with, I mean, he didn't have engineering plans.  He didn't have schematics.  He did a bit of reverse engineering of the Cisco device in order to find the place to put the chip.  But if he can do it, so can anybody else who is motivated.  It's just not that difficult.



And in Andy Greenberg's piece in Wired, he also mentioned something that we didn't cover, which is to prove the point that the Bloomberg report could have been true, somebody did make that change to a Supermicro motherboard subsequently, just to demonstrate that it could be done.  It was at, shoot, I can't remember the security conference now where it was shown.  But that was subsequently shown - oh, it was at the Chaos computing conference subsequently where he said, okay, so nobody found one, a Supermicro motherboard with that done.  But he held it up.  He says:  "I'm holding one where I did exactly what the report alleged, just for the sake of saying it's possible."



So the point is it is possible.  Again, it's not going to affect the majority of us.  But I guess, for some agencies, hopefully they are taking some sorts of measures to protect themselves from this kind of supply chain problem because they are possible.



LEO:  But they'd be targeted mostly; right?  I mean, you're not going to do a...



STEVE:  Yes, exactly.



LEO:  You're not going to say I'm going to take every - well, I guess you could, if you were the manufacturer, take every Linksys router and mess with it.  Much more likely you'd interrupt a shipment of an important piece of gear, a server, say, to the Pentagon.  You'd sneak in - that's why they call it "supply chain."  You'd sneak in the middle, put something on it, and let it go on.  That's much more likely, yeah.



STEVE:  Right.  Right.  And...



LEO:  It's not something you and I should worry about.



STEVE:  No, exactly.  And our listeners probably have no need to worry about it.  But I would imagine - I just sort of wanted to just revisit the issue that that kind of attack is targetable and eminently feasible.



LEO:  Absolutely.



STEVE:  Even though a year ago it was debunked.



LEO:  It's almost certainly happening.  We know the NSA has done it.



STEVE:  Yes, yes, exactly, exactly.



LEO:  So it's not a proof of concept.  It is a concept.  It's happened.



STEVE:  Yeah, it's a demonstration, exactly.



LEO:  Yes, exactly.



STEVE:  So last March 6th, Mark Zuckerberg famously posted his so-called, what was titled "A Privacy-Focused Vision for Social Networking."  And I snipped out from a very long thing sort of the salient piece of what he said.  He said:  "I believe the future of communication will increasingly shift to private, encrypted services where people can be confident what they say to each other stays secure, and their messages and content won't stick around forever."  He says:  "This is the future I hope we will help bring about."  And of course, yes, it was met with a lot of skepticism, and we talked about it at the time, and I know you did multiple podcasts, used it for content on multiple podcasts because it was an interesting statement coming from Facebook.



He said:  "We plan to build this the way we've developed WhatsApp."  And of course he didn't.  They acquired WhatsApp; right?  And doesn't WhatsApp use Signal as its core protocol?



LEO:  It does, yeah, yeah.



STEVE:  Yes.  And so of course he was leveraging Moxie Marlinspike's tremendous work that we talked about in detail at the time.  He said:  "Focus on the most fundamental and private use case, messaging.  Make it as secure as possible, and then build more ways for people to interact on top of that, including calls, video chats, groups, stories, businesses, payments, commerce, and ultimately a platform for many other kinds of private services."  So all that sounds good, if they actually execute.  And his posting was much longer, and it covers many other points.  But basically the entire thing was a manifesto for privacy and unbreakable-by-anyone encryption.



Well, predictably, in our current environment where this whole question of government and law enforcement legal access to any and all private communications remains very much up in the air and unsettled, several governments have now formally pushed back against Facebook's declared intentions.  In a 2.5-page open letter dated October 4th, so about a week and a half ago, which was cosigned by law enforcement authorities in the U.S., the U.K., and Australia, Facebook is strongly urged to halt its plans for stronger end-to-end encryption.  And I won't bore our listeners with the entire letter, but I will just share the beginning of it to get a sense for it.  It was addressed as an open letter.



"Dear Mr. Zuckerberg.  Open Letter:  Facebook's Privacy First Proposals."  And so the U.S., the U.K., and Australia write:  "We are writing to request that Facebook does not proceed with its plan to implement end-to-end encryption across its messaging services without ensuring that there is no reduction" - I love the way they phrased this - "no reduction to user safety and without including a means for lawful access to the content of communications to protect our citizens.  In your post of 6 March 2019, 'A Privacy-Focused Vision for Social Networking,' you acknowledged that 'there are real safety concerns to address before we can implement end-to-end encryption across all our messaging services.'  You stated that 'we have a responsibility to work with law enforcement and to help prevent,'" and he says, "the use of Facebook for things like child sexual exploitation, terrorism, and extortion."



They write:  "We welcome this commitment to consultation.  As you know, our governments have engaged with Facebook on this issue, and some of us have written to you to express our views.  Unfortunately, Facebook has not committed to address our serious concerns about the impact its proposals could have on protecting our most vulnerable citizens."  And so that's where they go with this is basically child sexual abuse.



LEO:  Of course.  They always do.  That's where they go.  Think of the children.



STEVE:  I know.  I know.  He says:  "We support strong encryption, which is used by billions of people every day for services such as banking, commerce, and communications.  We also respect promises made by technology companies to protect users' data."  And we'll just stop for a second and note that, yes, as far as we know, there is no technology in place that allows the government to intercept our HTTPS connections during banking and commerce and communications.  As far as we know.



LEO:  As far as we know.



STEVE:  But we do know because we know how the system works.



LEO:  The math works, yeah.



STEVE:  And that there are no backdoors in TLS at the moment.



LEO:  Although we also probably know that they're collecting everything, as much as they can.  They built that big server farm so that they can collect more data and then hope that they can crack it down the road, I guess.



STEVE:  Yeah.



LEO:  Does TLS support Perfect Forward Secrecy?



STEVE:  It does, yes.  TLS, yes.



LEO:  Oh, so good luck, NSA.



STEVE:  Exactly.  Exactly.  You cannot do retrospective decryption once you obtain the key of the server in the future.  So then they say:  "Law-abiding citizens have a legitimate expectation that their privacy will be protected.  However, as your March blog post recognized, we must ensure that technology companies protect their users and others affected by their users' online activities.  Security enhancements to the virtual world should not make us more vulnerable in the physical world.  We must find a way to balance the need to secure data with public safety and the need for law enforcement to access the information they need to safeguard the public, investigate crimes, and prevent future criminal activity."  You know, I mean, even as I'm reading this, I mean, it is self-contradictory; right?  I mean, they're contradicting themselves in what they're saying they want.



So they continue:  "Not doing so hinders our law enforcement agencies' ability to stop criminals and abusers in their tracks."  And I'll just read one more paragraph here because this is important:  "Companies should not deliberately design their systems to preclude any form of access to content, even for preventing or investigating the most serious crimes.  This puts our citizens and societies at risk by severely eroding a company's ability to detect and respond to illegal content and activity, such as child sexual exploitation and abuse, terrorism, and foreign adversaries' attempts to undermine democratic values and institutions" - oh, yeah, let's go there, too.



LEO:  They hit the big three on that one.



STEVE:  Let's go there, too, yeah:  "...preventing the prosecution of offenders and safeguarding of victims.  It also impedes law enforcement's ability to investigate these and other serious crimes.  Risks to public safety from Facebook's proposals are exacerbated in the context of a single platform that would combine inaccessible messaging services with open profiles, providing unique routes for prospective offenders to identify and groom our children," is what they wrote.  So it then congratulates Facebook on the number of child sexual abuse cases they helped law enforcement with and details one particularly poignant case involving an 11 year old, which I'm not going to go into.



It concludes, saying:  "Equally important to Facebook's own work to act against illegal activity, law enforcement rely on obtaining the content of communications, under appropriate legal authorization, to save lives, to enable criminals to be brought to justice, and exonerate the innocent.  We therefore call on Facebook and other companies to take the following steps."



And we have four bullet points:  "Embed the safety of the public in systems designs, thereby enabling you to continue to act against illegal content effectively with no reduction to safety, and facilitating the prosecution of offenders and safeguarding of victims; two, enable law enforcement to obtain lawful access to content in a readable and usable format; three, engage in consultation with governments to facilitate this in a way that is substantive and genuinely influences your design decisions; and, four, not implement the proposed changes until you can ensure that the systems you would apply to maintain the safety of your users are fully tested and operational."  I'm not sure what that fourth one is about.



Then they conclude:  "We are committed to working with you to focus on reasonable proposals that will allow Facebook and our governments to protect your users and the public while protecting their privacy.  Our technical experts," they're writing, "are confident that we can do so while defending cyber security and supporting technological innovation.  We will take an open and balanced approach in line with the joint statement of principles signed by the governments of the U.S., the U.K., Australia, New Zealand, and Canada" - as we know, that's the Five Eyes - "in August of 2018 and the subsequent communique agreed in July this year.  As you have recognized, it is critical to get this right for the future of the Internet.  Children's safety and law enforcement's ability to bring criminals to justice must not be the ultimate cost of Facebook taking forward these proposals."



And Facebook, for its part, just essentially replied with what is mostly a reiteration of some of its previous statements.  Facebook replied to this letter, saying:  "We believe people have the right to have a private conversation online..."



LEO:  Yes.



STEVE:  "...wherever they are in the world.  As the U.S. and U.K. governments acknowledge, the CLOUD Act allows for companies to provide available information when they receive legal valid requests and does not require companies to build backdoors.  We respect and support the role law enforcement has in keeping people safe.  Ahead of our plans to bring more security and privacy to our messaging apps, we are consulting closely with child safety experts, governments, and technology companies, and devoting new teams and sophisticated technology so we can use all the information available to us to help keep people safe.



"End-to-end encryption already protects the messages of over a billion people every day.  It is increasingly used across the communications industry and in many other important sectors of the economy.  We strongly oppose government attempts to build backdoors because they would undermine the privacy and security of people everywhere."



LEO:  And I would point out it's not just Facebook saying this.  Edward Snowden today published an op-ed piece in The Guardian in which he said:  "Without encryption, we will lose all privacy.  This is our new battleground."  He says:  "If Barr's campaign is successful, the communications of billions will remain frozen in a state of permanent insecurity.  Users will be vulnerable by design, and those communications will be vulnerable" - and this is the important part - "not only to investigators in the U.S., U.K., and Australia, but also to the intelligence agencies of China, Russia, and Saudi Arabia, not to mention hackers around the world."  I think Snowden got it right.  If you don't trust Facebook's response, I think it's safe to say Edward Snowden knows a little bit about this.



STEVE:  Yeah.  I mean, really it boils down to will you allow a third party to have access?  And, I mean, that's what it is.  We've talked about the math.  We've explained the math can do anything we want it to do.  The question is, what do we want it to do?  And as our listeners know, five years ago, when I first talked about SQRL, I explained that what it was was deliberately and explicitly a two-party solution.  It's an authentication technology between you and the website that you are authenticating yourself to, without a third party.



And the lack of a third party created huge complexities for me in its design.  It's part of what took so long was I had to deal with all of the what-ifs that would arise where there's no one to have an "Oh, I forgot my password" email recovery, you know, because there isn't.  There's just the two of you, you and the site.  But the point is it's a system, if we want it, for two parties.  And the good news is it's not doing anything other than authentication, so it doesn't get into the crosshairs of our government in the same way.  But, you know, this is exactly what Edward said, exactly what this letter from Bill Barr and his U.K. and Australia...  



LEO:  Compadres, compatriots.



STEVE:  ...compadres are saying.  I mean, this is where we are.  This is the battle.  And the problem is, in the U.S., we have Congress.  And the U.S. Congress is where laws originate.  Our Congress is demonstrating that it's rather timid at the moment.  More so than the U.K.  I wouldn't be surprised if something happens first in the U.K. because they seem more willing to break things over there.



LEO:  Well, hasn't it already happened in Australia?  Did they not pass that law at the beginning of the year?  I thought they did.  I don't know what its upshot is.  I don't think any - I think everybody's ignoring it.



STEVE:  Yeah, exactly.  Ignore it as long as you can.



LEO:  They said by law you have to be able to provide a cleartext version of any message.  They didn't tell you how.  Snowden says the true explanation for this is that it's not about public safety, it's about power.  End-to-end encryption gives control to individuals and devices they use, not to the companies and carriers that route them.  And that means government surveillance would have to become more targeted and methodical instead of their lazy, indiscriminate, universal surveillance.  It's about power.



STEVE:  Yeah.



LEO:  They always bring up the child pornography bugaboo.  But that's just an excuse, and that's not - they don't care about that.  That's not what this is about.  We should fight this.  Fortunately, the math is out there.



STEVE:  Yup.



LEO:  I don't know if there's any way they can shut the math down.  But maybe you can't use Facebook.  If you want privacy, why are you using Facebook anyway?



STEVE:  Right.  Well, and so ultimately that's what - we pretty much know what's going to happen.  There will end up being legislation, and then our major companies need to decide what they're going to do.  I mean, I don't imagine Apple and Facebook and Google can be outlaw organizations.  And we've talked about this.  And so if somebody then wants secure encrypted communications, you'll use some other tool, some add-on or some third-party solution.



LEO:  It's my guess, you know, we see Tim Cook having dinner with the President.  We see Mark having dinner with all sorts of interesting characters.  It's my guess these companies are already doing it.  And anything they say in public, like that letter from Mark, is really a distraction.



STEVE:  And I've argued that Apple already can.  If it wants to add another key to our iMessage conversations, we have no way of knowing that that's what's happened.



LEO:  They may well have done that.  We don't know.



STEVE:  Yup.



LEO:  We don't know.  And I think you'd be foolish to trust them.  Can we trust Signal?  It's open source; right?  You can check the source code.



STEVE:  Yeah.  Although I'm trying to remember, see, I think Signal does some automatic key management.  That's the crux.  As I've long said, if somebody else is managing your keys, then you're not managing your keys.



LEO:  And it uses your phone number as the identifier, which is not good, either.



STEVE:  Yeah.  And in fact we have been seeing lots of SIM attacks and attacks on two-factor authentication that way.  You know, it's why I gravitated toward Threema.  Threema has the problem that it's not open source.



LEO:  It's closed source, so you don't know what they're doing.



STEVE:  Yeah, exactly.



LEO:  Somebody, Steve, needs to write a completely open, encrypted, end-to-end communication technology.



STEVE:  Where the user manages their own keys.



LEO:  The user manages it.  It won't be easy to use.  You'll have to be responsible.  But that's fine.



STEVE:  Exactly.  You're going to have to have a tradeoff between convenience and security.  I mean, that's just the way it is.



LEO:  That's why Signal uses your phone number, because you can reauthenticate.  At least they notify you if the key's been changed.  But I think we need some - there must exist - it must exist.



STEVE:  And I think when we were talking about it, I'm trying to remember, in Signal and/or WhatsApp, you are able to display the user's QR code of your key and then physically show them to each other and pair them and, you know...



LEO:  Which is important because you need to verify it.  The one problem with WhatsApp is it doesn't let you know if the key's changed.  Signal does, so - unless that's changed recently.  But the last time I saw, WhatsApp, you can surreptitiously do a key change.  Which means somebody could impersonate you, basically, with that.  And you could check for that, but most people wouldn't. 



STEVE:  Yup.  So get a load of this, Leo.



LEO:  Oh, boy.



STEVE:  Unix's co-creator Ken Thompson's BSD Unix password has finally, after 39 years, been cracked.



LEO:  Oh, my god.  You mean, they couldn't log into his account?



STEVE:  No.  So, and it was a pretty good password. 



LEO:  Well, I bet it was.



STEVE:  So Ken Thompson, who was of course the co-creator of Unix...



LEO:  He's still alive; isn't he?  I know Ritchie passed away.



STEVE:  Yes, well, he's at Google.  And he did Go, the Go programming language.



LEO:  Oh, that's right.  Right.



STEVE:  Okay.  So the story begins five years ago when a developer, Leah Neukirchen...



LEO:  Neukirchen.



STEVE:  ...Neukirchen spotted an interesting /etc/passwd file in a publicly available source tree of the historical BSD version 3 from 1980.



LEO:  They left the password file in?



STEVE:  Yes, it was there.  It included the hashed passwords belonging to more than two dozen Unix luminaries...



LEO:  Oh, my god.



STEVE:  ...who worked on Unix development, including Dennis Ritchie, Stephen Bourne, Ken Thompson...



LEO:  The Bourne shell.



STEVE:  Yup, the Bourne shell.  Eric Schmidt...



LEO:  Who was an intern at Bell Labs when they were creating Unix.  I love it.



STEVE:  Yup.  The Eric Schmidt, Stuart Feldman, and Brian Kernighan.



LEO:  Kernighan.



STEVE:  Kernighan; right.



LEO:  Kernighan and Ritchie is the classic C book, yeah.



STEVE:  That's right.



LEO:  Wow.



STEVE:  Since those early passwords were protected using the long-since deprecated DES-based descrypt, D-E-S-C-R-Y-P-T...



LEO:  That should be crackable.



STEVE:  ...and they were limited to at most eight input characters, Leah decided to brute-force them for fun.  This was five years ago.



LEO:  I hold in my hands, by the way, this was a board created by Cryptography Research.



STEVE:  I remember, yup.



LEO:  This was a prototype.  It actually failed.  They only got one chip on it.  But it was going to be filled with chips to crack DES.



STEVE:  Right.



LEO:  This was all about - see, it says EFF, Cryptography Research, DES.  They were going to prove that DES was unreliable.



STEVE:  Nice.  Well, and itself it is a 56-bit block, which is now way too short.  But that's why Triple DES, essentially three of them, creates a much longer block.



So anyway, so five years ago she started to tackle this.  She successfully cracked the passwords for most of those Unix luminaries using standard off-the-shelf tools like John the Ripper and Hashcat.  But the toughest ones to crack, which she was unable to crack, belonged to Ken Thompson and five other contributors who helped to build the Unix system, including...



LEO:  Ken should be so proud.



STEVE:  Yes, including Bill Joy, who of course as we know later founded Sun Microsystems and designed Java for us.



LEO:  Yeah.  Wow.



STEVE:  So she wrote in a blog posting last Wednesday, she wrote:  "Ken's password eluded my cracking endeavor.  An exhaustive search back in 2014 through all lowercase letters and digits took several days and yielded no result."  She noted that, compared to other password hashing schemes such as NTLM, descrypt turns out to be quite a bit slower to crack.  And the problem was there was no special case software for it.  No one was, like, bothering.  You know, we can crack SHA-256 in a blink; but DES, just no one had bothered to special case it.



So earlier this month she posted all of her findings on the Unix Heritage Society mailing list, requesting help from other members to crack the remaining passwords.  And six days later an Australian engineer, Nigel Williams, responded with the plaintext password used by Ken Thompson, which he cracked after four days using an AMD Radeon Vega 64 running Hashcat, which was testing about 930 million hashes per second.  Thompson's password consequently has been revealed as...



LEO:  Monkey123.  Oh, no.



STEVE:  "P/q2-q4!."



LEO:  Huh.  That's chess.  That's a chess move.



STEVE:  Exactly.  Chess notation describing the move "Pawn from Queen's 2 to Queen's 4."



LEO:  Oh, that's hysterical.  That's actually a good idea for making - I'll have to remember that for making passwords.  That's good.



STEVE:  It withstood the test of time. 



LEO:  It's only seven characters.  Eight.  Was eight the max that you could use?



STEVE:  Yes.  You could only do an eight-character max.



LEO:  Okay.



STEVE:  So the next day another mailing list member, Arthur Krewat, successfully cracked and provided the passwords for four more remaining uncracked hashes.  Dennis Ritchie, who of course was the co-inventor of BSD and the creator of the C programming language, his was kind of simple:  "dmac," D-M-A-C.  Brian Kernighan's was "/.,/.,.."



LEO:  That sounds like a shell command.  At least a directory.



STEVE:  Stephen Bourne wasn't too concerned about security.  His was "bourne."



LEO:  Oh, dear.



STEVE:  B-O-U-R-N-E.



LEO:  Oh, dear.  Oh, dear.



STEVE:  Eric Schmidt, who of course is the former Google CEO, was "wendy," all lowercase, and three exclamation points.



LEO:  Oh, dear.  He was a dog even then.



STEVE:  And Stuart Feldman, who was the author of a Unix automation tool and the first Fortran compiler:  "axolotl."



LEO:  Axolotl, yeah.



STEVE:  Yes.  Which is that walking fish thing, A-X-O-L-O-T-L.



LEO:  That is bad.  That's a dictionary word.  That's the only one in the bunch that's a dictionary word, so that's the worst one; isn't it.



STEVE:  Yeah, that's true.  Yup.  But anyway, Ken Thompson's was a tough one to crack, and it was a chess move.



LEO:  Oh, you know what?  /.,/.,, that's actually pretty bad, too  If you look at the keyboard, it's the lower right, /.,/.,..



STEVE:  Oh, you're right, three in a row backwards, yes.



LEO:  That's a terrible - that's also a terrible password.  Every hacker in the world probably has that in their crack set.  Oh, man.  I like p/q2-q4!.



STEVE:  That's pretty good.



LEO:  And he gives it an exclamation mark, which means it's a brilliant move, so that's even better.



STEVE:  Pawn Queen 2 to Queen 4.



LEO:  Yeah.  Nobody would write it that way, but it's memorable.  Like the Queen's Gambit.  I love it.



STEVE:  So an interesting consequence, Leo, you'll get a kick out of this because it's about consumer high-resolution cameras.  We were beginning to see consequences of this.  We've covered the problem, for example, of just some keys being left out on the desk and how camera resolution is now so good, and coupled with computational capabilities, that a photograph of a key at a distance is enough to reproduce that key such that it will be able to open the lock.  We've seen an instance of bouncing a laser off of a window to pick up the vibrations of the windowpanes.  And in fact on this podcast we covered, you may remember, the balloons, inflated balloons in a room.  Pictures of the balloons were used to pick up the conversation in the room...



LEO:  Oh, from the vibrations.



STEVE:  From the vibration of the rubber surface of the balloon, which I thought was very cool.  Well, now we have a true story of a stalker locating a celebrity by examining the reflections picked up in her eyes' irises.  A Japanese stalker confessed to stalking and unfortunately attacking a young Japanese pop star by zooming in on the reflections that he was able to detect in her eyes...



LEO:  Oh, my god.



STEVE:  ...in photos she posted on social media.  After the assault, a 26-year-old man by the name of Sato...



LEO:  That's horrible.



STEVE:  ...was arrested and confessed to police that he'd used the star's selfies to figure out where she lived.  Each of her pupils reflected the nearby streetscape, which he was able to plug into the street map function of Google Maps to locate matching bus stops and scenery.



LEO:  That's horrible.



STEVE:  Isn't that, like...



LEO:  What a nightmare.  That's straight out of "Blade Runner."  



STEVE:  He confessed to observing other reflections in Matsuoka's - I don't know how to pronounce her name, M-A-T-S-U-O-K-A.



LEO:  Matsuoka.



STEVE:  Matsuoka's eyes - curtains, windows, and the angle of the sun, which enabled him to guess which floor she lived on in the building.



LEO:  Wow.  That's - wow.



STEVE:  So isn't that something?



LEO:  Center, enhance, zoom.  Center, enhance, zoom.  Wow.  That's terrible.  What kind of nut - geez.



STEVE:  I know.



LEO:  Horrible.



STEVE:  But the information is there, which is, like, chilling.  Eliot Higgins, the founder of investigations site Bellingcat, which has pioneered online investigative techniques, told the BBC that the better quality the image, the more potential there is for it to be used in geolocating us.  He said:  "Higher quality images allow for more details to be identified that can help with geolocation; and the more reference imagery there is from services like Google Street View, the greater the chance of determining a location.  Even the tiniest details can reveal a lot of information about where a photograph is taken, and information about the individuals in the photograph."



And of course, as we know, the photo's EXIF data, which may include the GPS coordinates of a photo, can be used to do the same thing.  And Google's computer vision specialists have worked to train deep learning machines to determine the location of almost any photo just by using its pixels and relying on image retrieval.  Google has access to an extraordinary number of images on which to train its deep learning systems.  So now we have one more thing to consider when posting photos online, and that is that subtle clues can exist.



LEO:  But we know you shouldn't post pictures of your keys.



STEVE:  Right.



LEO:  Because you can from a photo duplicate a key.  But, you know, I think back to Tech TV days.  This was around 2000.  One of our hosts used as her image a picture, you know, a headshot.  But it was a topless picture of her that she had cropped down.  And what she didn't realize was the image editor that she used to crop it didn't delete the thumbnail of the full image from the file.



STEVE:  Oh.



LEO:  So somebody was able to get the full image and propagate it.  It was very embarrassing.



STEVE:  Wow.  Wow.



LEO:  These things happen all the time.  And, you know, you hear all the time about redacted documents.



STEVE:  Oh, exactly, where you just take the black layer off.



LEO:  So times have moved fast, yeah.



STEVE:  Yeah, that is a reminder for people, by the way.  If you black out areas on a PDF, print the resulting blacked out PDF to another PDF, rather than just send the PDF with the blackouts because those are removable.



LEO:  Still cracks me up.



STEVE:  Yeah.  So Pew Research Center shared the results of their survey.



LEO:  Oh, this was interesting, yeah.



STEVE:  Yes, of Americans', I'll say, lack of understanding of technology-related security issues.  They found, not surprisingly, that a majority of U.S. adults were able to answer fewer than half of the survey's digital knowledge quiz questions correctly.  And, frankly, I'm surprised that a broad swath of America did as well as this shows.



LEO:  Well, some of these questions, like the last question, can you identify a picture of Jack Dorsey?  They showed them a picture of Jack Dorsey.



STEVE:  Yeah, well, I would not - I would flunk that one.



LEO:  Most people would fail that one.  That's...



STEVE:  I have no idea what...



LEO:  That's not digital literacy.  That's like digital gossip knowledge.



STEVE:  Yeah.  I would have no idea.



LEO:  It was the last question.  Maybe they just threw that in for fun.  I don't know.



STEVE:  Yeah.  They did note that people tended to say "not sure" rather than guess.  So I thought that was sort of, you know, I'm a big fan of saying "I don't know."  And so that was - it was nice that that was the case.



So a majority of U.S. adults were able to correctly questions about phishing scams and website cookies.  I thought that was good.  Because there's been enough in the popular press, I think, about that stuff, and people talking about it at cocktail parties, like, oh, do you flush your cookies?  And it's like, my what?  And so then they go poke around their browser and find out about that.  But other items were more challenging.  For example, just 28% of adults were able to identify an example of two-factor authentication.  Still, one out of four?  I think that's kind of good, actually.



LEO:  You know what we don't know, I'd love to see the images.  Apparently they used a set of images and said, "Which of these is two-factor?"



STEVE:  Oh, okay.



LEO:  So, you know, this might not - that Jack Dorsey question casts the whole thing into doubt, to be honest with you.



STEVE:  Yeah, like was Pew appropriately qualified to even put a survey like this together. 



LEO:  Yeah.



STEVE:  For me, my own takeaway is I guess what annoys me about these questions is that our moms should never have to know...



LEO:  They shouldn't have to.  No, no.



STEVE:  ...any of this stuff.



LEO:  Do you know who this is, Steve?  Do you know?



STEVE:  No idea.



LEO:  That's Jack Dorsey.  You see?  You'd have failed.  You'd have failed miserably.



STEVE:  Yeah.  Yeah.  Yeah.



LEO:  By the way, I can see where you are by the reflection in your glasses.  I just want to say...



STEVE:  In fact, you could probably read that screen from my eyeglasses.



LEO:  Almost can, yeah.  Almost can, yeah.



STEVE:  Yeah, yeah.



LEO:  But I do think - I think it makes perfect sense that only a quarter of America knows what - I think they've probably heard of two-factor.  You don't need to know what it is to turn it on.



STEVE:  True.  And you don't even have to know its name.



LEO:  Right.



STEVE:  That's a very good point.  It's like, oh, it's that extra six-digit thing I have to put in.



LEO:  Yeah, right.  I would hope they would know to turn that on.  And of course everybody listening to this show does.  But tell your friends and family.  I just wrote an article for the AARP magazine, the American Association for Retired People, old folk magazine.



STEVE:  Of which you and I are soon entering.



LEO:  We both get it.  I'm sure I get it.  But on two-factor, explaining what it is and why you should use it.  Because I think it is so important that people know this stuff.  So Pew's got a good point here.  I'm not knocking their point.



STEVE:  No.  But again, my take is that all of this is, in a sense, deeply abusive.  One of their things was that most people don't know what https:// is.  It's like, oh, my god.



LEO:  Why should they?  Why should they?



STEVE:  Yes, why should anybody have ever been exposed to this?



LEO:  Tim Berners-Lee, the guy who created the World Wide Web, has said, "I never expected anybody to see http://.  This was for machine-readable purposes only.  This is not supposed to be a human-readable [crosstalk]."



STEVE:  Yes.  And in fact I've told the story of how a dear friend of mine who is about the least techie woman on the planet just thinks Google is the Internet.  Like she puts it into the Google, and that's how she goes places.  And she doesn't know that it's not the Internet.



LEO:  She's kind of right.



STEVE:  So, you know, it was created by techies for techies.  But of course most of the world are non-techies. 



LEO:  Right.



STEVE:  And when I was thinking it, as I'm sort of mulling this over, remember back in the early days of automobiles, those darned horseless carriages were breaking down all the time.



LEO:  I remember that, yeah.



STEVE:  They were unreliable.



LEO:  That was terrible.



STEVE:  And finicky as hell.  So in fact, anyone driving one needed to also be a bit of an auto mechanic.



LEO:  That's right.



STEVE:  And that was part of the fun because those early autos were simple and understandable.  And in fact my own first car was a burgundy-colored Fiat 850 Spider.  I completely took it apart in our garage and rebuilt it from the ground up.



LEO:  Nice.



STEVE:  So I knew and loved that car inside and out.  And if something broke, I knew what and where I could go to get a part and fix it myself.  But not anymore.  The car I'm driving today is not, as they say, "user serviceable."  But what it is in return for being a black box is it is incredibly reliable.  I get in, and I turn the key, and it goes.  It does what it's supposed to do.



And so my feeling is we're heading in that direction, but we're not there yet, with today's computers.  And I think that needs to be our target.  These things should be black boxes.  It's fine.  They are becoming less and less comprehensible.  As we were saying before, I look at the process monitor in my PC.  I have no idea what that crap is.  It's just filled up with stuff.  It's all busy doing things.  And it's like, okay.  And they're not yet as reliable as we need them to be, but it's okay if they become opaque as long as they deliver reliability and security, very much like today's autos.  And I know, Leo, you sort of live on the cutting edge of auto technology, so you may not be able to put the key in and drive off.



LEO:  I don't have a keyhole.  



STEVE:  Ah.  That's even better.  You just approach.



LEO:  I just walk up to it.  You know, that was one thing I miss about the Tesla.  There is no start button.  You just put it in gear and go.  Because you don't need a start button.  What is that button for; right?



STEVE:  You're right.  You're right.



LEO:  Just go.



STEVE:  It was just servicing old-school mentality.



LEO:  That's exactly right, yeah.  If you put it in gear, it should just go.



STEVE:  So you will be glad to know, Leo, speaking of black boxes, OpenPGP will be built into Mozilla's Thunderbird email client.



LEO:  Nice.  Nice.



STEVE:  And it's the one I settled on.  When Eudora finally just got too long in the tooth even for me, I tried a whole bunch of different clients, and I settled on Thunderbird, thinking, well, okay.  I like Mozilla.  It'll be kept up with standards.  It'll be moving forward.  Thunderbird currently has a third-party plugin, Enigmail, E-N-I-G-M-A-I-L.



LEO:  It's kind of hard to use, yeah.



STEVE:  Yes.  And it requires users to separately install additional third-party software like GnuPG or GPG4Win before installing Enigmail itself.  And unfortunately, currently the licensing governing those libraries are incompatible with Thunderbird's, MPL v2 versus GPL v3+.  So the Thunderbird folks will need to find a compatible library.  But they've formally announced they're going to, starting with Thunderbird 78,  which I think we're on 65 or something right now.  So it's scheduled for release next summer.  So summer of 2020, OpenPGP will be built in and fully supported natively.



LEO:  So you won't have to download a PGP tool or anything like that, just have the full...



STEVE:  Anything.



LEO:  That's really great.



STEVE:  Yeah, it'll just be there.



LEO:  I mean, there are - I use Claws Mail on Windows, Mac, and Linux, and that supports PGP.  But you have to have PGP installed.  And it goes out and calls it, kind of like Enigmail would.  So I think that's great.  I hope they make it a very easy thing to do, to generate and use a key.  



STEVE:  Yeah, and manage your key.  I'm sure they will.



LEO:  Yeah.  That's great, yeah.



STEVE:  And so we will definitely be talking about that when that happens next summer.  And then, gee, Leo, I'll have PGP.  Whoo.



LEO:  PGP is, I mean, first of all, there are problems with it.  It's very old.  It's not how you would design it if you were going to design it today.  But it's not like we have anything much else out there.



STEVE:  Right.



LEO:  I use it mostly for signing, to kind of verify that I created this.



STEVE:  Right, right.



LEO:  Cool.



STEVE:  So a little bit of Windows 10 news.  Windows 10's Tamper Protection is being enabled by default.  And when you think about it, it really doesn't do much good to have Windows Defender watching the store if it can simply be turned off by sufficiently clever malware.  And there have been increasing reports of exactly that happening recently.



So yesterday, on the 14th, Microsoft announced that the new Windows 10 Tamper Protection security feature, which was added to Windows 10 in this most recent big update, 1903, also known as the May 2019 update, is now officially available for both enterprise and consumer users.  Along with this announcement, Microsoft will be enabling the security feature on all Windows 10 devices, obviously which need to have 1903 in them first, by default.



When enabled, as it will be by default, Tamper Protection prevents Windows Security and Windows Defender settings from being changed by programs, Windows command line tools, registry changes, or group policies.  Basically, nothing programmatic can change it.  The only way to configure it and/or modify its settings will be directly through the Windows 10 user interface or, in the case of enterprise users, with some management software such as Intune.  And before starting the podcast I fired up this Windows 10 machine that I'm talking to you on, Leo, and it was there in the settings.  It was off by default.



So for what it's worth, any of our listeners who are interested, who are Windows Defender users, who did update to this most recent Win10 1903, just can go into your update and security page under Windows Defender or whatever settings it's called.  I won't bring it up because I don't want to pause.  But anyway, it's obvious where it's there, and you'll find a switch which is called Tamper Protection.  Mine was off with a little yellow warning sign.  I flipped it on and made it happy.  And so now I've got it on, and it cannot be turned off, so says Microsoft, by any means other than doing it manually through the UI.  So a nice little bit of forward motion on Microsoft's part for security.



Two little bits of miscellany under the sci-fi and fun category.  Lorrie and I and my best friend saw "Ad Astra."



LEO:  Oh, I'm curious about that.  Brad Pitt's new space opera.  What did you think?



STEVE:  I can't recommend it highly, as I was hoping I might.  I described it here in the show notes as "Brad Pitt wanders around through outer space."



LEO:  That's what it looks like in the trailers, too.  Okay.



STEVE:  Yeah, it never gets much better than that.



LEO:  Oh, that's terrible. 



STEVE:  So it was meant to be set in an interesting future.  And as our listeners know, I am just steeped in sci-fi.  And so it was an interesting take.  There was no warp drive.  There was no beaming.  It was sort of like the reality moved forward another hundred years.  So there was like a Moon base and a Mars base.  And we had some bases, like, further out in the solar system.  No one quite explained to us why, but there was a base out at Neptune that was doing some things, and it's like, okay.  And so it was sort of like, and there was sort of a commercial - you could buy a ticket to go to the Moon.  And so it was just sort of an extension of where we are now.  But no, like, exotic technology, so it required patience to go anywhere.  And Brad was very patient.  And the watcher of the movie was asked also to be very patient.



LEO:  Oh, dear.  No kidding.  This does not bode well.



STEVE:  It's definitely a maybe, you know, wait for it to come out on some free streaming service that you're already subscribed to.  And get a big bag of popcorn.



LEO:  Good tip.



STEVE:  Okay.  Now, the fun piece is Friday we got an extension to "Breaking Bad."



LEO:  Oh, I haven't seen it yet.



STEVE:  And it was wonderful.



LEO:  This is "El Camino."



STEVE:  Two hours.  It is what happened to Jesse after he drove off at the end.  The last scene of "Breaking Bad" was he had escaped/gotten out of his cage.



LEO:  See, I wasn't clear at the end of "Breaking Bad" whether Jesse had survived.



STEVE:  Well, in what mental state had he survived.



LEO:  Right.



STEVE:  It's a little over two hours.  It was written by what's his name, Gilligan, who wrote the original, produced by - and of course now I'm blanking on the actor's name.



LEO:  Bryan Cranston.



STEVE:  Bryan is in it.



LEO:  Oh, he is?



STEVE:  Yes.  There are some - they had so much fun.  They went back and filled in some back story that we weren't originally given in order to carry the storyline forward.  So this is like - it's like the missing chapter.



LEO:  Oh, I can't wait.



STEVE:  It's the final closure that all "Breaking Bad" fans have been waiting for, and it will not disappoint you.



LEO:  I'm so happy.



STEVE:  And this actor is so good.  Holy crap.  I mean, he is so good.  Aaron Paul, that's the name I was trying to remember.



LEO:  Aaron Paul, yeah, Jesse Pinkman, yeah.



STEVE:  Yes.



LEO:  And Vince Gilligan is the author.



STEVE:  Yes.  And he wrote this.



LEO:  He created it, yeah.



STEVE:  And so I just wanted to make sure that all of our listeners who were "Breaking Bad" fans knew that Netflix did this two-hour movie.  It's not another series.  It's a two-hour final conclusion to what happened after Jesse drove off in his El Camino.  And it's...



LEO:  I have to say, I mean, I watched "Better Call Saul."  I mean, I'm such a "Breaking Bad" fan.  I can't wait.  I'm saving it because Lisa's in New York.  So when she gets back...



STEVE:  Yup, do it.



LEO:  I look forward to it, yeah.



STEVE:  Do it.



LEO:  Good.  "El Camino."



STEVE:  It's got top recommendation.  "Ad Astra," no.  "El Camino," all thumbs up.



LEO:  Yeah.



STEVE:  And a Twitter follower of mine, certainly a follower of the podcast, tweeted to me a pointer to something that I was so thankful for.  There are two documents produced by Google, "Modern Password Security for Systems Designers" and a complementary "Modern Password Security for End Users."  And this was what to consider when building a password-based authentication system, written by Ian Maddox and Kyle Moschetto, who are Google Cloud Solutions Architects.  And there's a chunk on SQRL.



LEO:  All right.  All right.



STEVE:  So Google Cloud Solutions Architects, under "Alternatives to passwords," for SQRL they wrote:  "The Secure Quick Reliable Login protocol is a recent addition to the security space.  It is designed for end-user authentication to websites and applications.  SQRL users run a small client application on their modern password security for system" - this looks like I've scrambled up - "run a small client application on their computer..."



LEO:  It looks like some garbled words, yeah.



STEVE:  Yeah, I mangled the text somehow when I did a copy and paste - "...or in their browser.  Instead of giving servers a password that they must keep secret, the client provides a public key that is unique to the application or domain the user wants to authenticate to.  The server provides a unique value to the client, and the client must then use their private key to sign and return that secret.  The server verifies the signature by using their public key and authenticates the user.  Most importantly, a compromised site or service cannot expose its users' credentials in a way that impacts any other site or service."  So these guys got it perfectly.  They said:  "Users can expect to see the option for SQRL login to appear in more places in the coming years."



LEO:  Right on, right on, right on.



STEVE:  "Developers and software architects will appreciate the deep level of technical detail written with an eye toward an evolving security landscape and the way real humans interact with security controls."  And that deep level of technical detail was a link in the PDF, in Google's PDF, to the SQRL explainer doc.



LEO:  Right on.  Bravo, Steve.  That's great.



STEVE:  That's very cool.  And thank you to Ian and Kyle for the inclusion of SQRL in this official Google Cloud Solutions Architects documents.  So very, very cool.



LEO:  That's really good, yeah.  All right.  Let's talk about CheckM8.  And we ain't talking p/q2-q4!.



STEVE:  No.



LEO:  No.



STEVE:  A recently discovered, unpatchable, iOS Boot ROM exploit.  Two weeks ago the iOS jailbreaking community received a welcome surprise when a security researcher going by the handle "axi0mX," A-X-I-0-M-X, dropped what's been described as a "game-changing," and I agree, new exploit affecting, well, pretty much all of Apple's mobile platform.  He called it "CheckM8."  It's a Boot ROM exploit which is being widely proclaimed the most important single exploit ever released for iPhone, iPad, Apple TV, and Apple Watch devices.



So what does that mean for us?  First of all, CheckM8 will most likely, at least initially and directly, prove to be a massive boon for the security researchers who are wishing to peek under the hood of iOS devices, despite Apple's every attempt to prevent that.  The only real threat to end-users might be that, by allowing researchers and hackers to get in under the hood, it will almost certainly facilitate the discovery of other weaknesses, which are, as we know, all too possible, since Apple is constantly patching discoveries which occur.  This is going to make those discoveries probably more likely.



But for now, at least, CheckM8 itself does not directly affect end users.  For one thing, there's no remote execution path.  An attacker cannot use CheckM8 to compromise an untethered device.  Which means that anyone who wanted to use this exploit would need to physically have the device and have it tethered.  Also it does not allow any sort of threat actor to bypass a device's Touch ID or built-in existing PIN protections.  In other words, it does not permit any compromise of the Secure Enclave which exists in these devices.



So the user's personal data continues to remain safe from attackers who are lacking the device's unlock credentials, that is, notwithstanding the possibility of there being other zero days that are not known.  But this is in itself a testament to Apple's multilayered security design philosophy.  The idea that even a full boot compromise, which is what CheckM8 enables, would still leave the user's private data secure should be comforting to all Apple device users.  And there's no persistence mechanism.  If an attacker were to gain physical access to an affected device and use the Boot ROM exploit to compromise that device, rebooting the device would restore its normal boot chain security, and any changes that the attacker may have made in RAM would be lost as Apple's security checks, which would then again be effective, would either delete any files modified by the attacker or would refuse to run them.



So, okay.  Which devices?  As I mentioned before, only the most recent devices are immune because Apple did discover this summer before last.  So the most recent devices - an iPhone XR, XS, XS Max, or any iPhone 11 series - all of which use the A12 Bionic or later chip, the A13, that Boot ROM exploit will not work because Apple did find it.



There is a use-after-free vulnerability which axi0mX found which appears only in devices using the A11 chips or earlier.  So that's from the iPhone 4S, which uses the A5 chip, all the way through the iPhone 8 and X models, as well as any iPad, Apple TV, or Apple Watch using A11 or earlier chips.  But on the other hand, think about that.  I mean, that's most generations of iPhones and iPads which are vulnerable.



LEO:  Now, how vulnerable is vulnerable, though?  I mean, a jailbreak nominally means you can use some store besides Apple Store.  Is this really like rooting the phone more?



STEVE:  So yes.  So what it means is that researchers and hackers will be able to get in and look around.  They'll be able to dump these files that Apple has protected and reverse-engineer them.



LEO:  But they'd have to get physical access to your phone to do it.



STEVE:  Well, okay.  So that's the point.  I don't think people care about our phones.  They care about Apple's code.



LEO:  Ah.



STEVE:  What this means...



LEO:  So this is more bad for Apple than it is for us.



STEVE:  Oh, my god.  It is horrible for Apple.



LEO:  I get it.  I get it.



STEVE:  Yes.



LEO:  Okay.



STEVE:  So on September 11th, so like two weeks ago, axi0mX tweeted - this is the fifth in a series of his tweets.  I have it in the show notes.  He said:  "During iOS 12 betas in summer of 2018, Apple patched a critical use-after-free vulnerability in iBoot USB code.  This vulnerability can only be triggered over USB and requires physical access.  It cannot be exploited remotely."  He says:  "I am sure many researchers have seen that patch."



So what has happened now is, on GitHub is the full exploit of this use-after-free vulnerability which Apple fixed last summer.  So what does it mean for end-users?  Nothing.  Apple is definitely not happy, whereas researchers and hackers are dancing a jig.  But that said, if a paranoid person, or perhaps someone who might be explicitly targeted, wants some takeaways from this, the only risk would be if their device were outside of their control.  In such a case, it would be possible for the phone to be jailbroken and for transient surveillance malware to be loaded into the phone.



LEO:  Right, right.



STEVE:  So if you've left your phone unattended and powered on in your whatever, a hotel room or on a desk in a shared office environment or whatever, or for example if it was temporarily confiscated by border security guards while you were traveling into China, you might consider rebooting the iOS device once it's back in your possession.  And you should do like a forced restart to ensure that malware hasn't found a way of simulating a fake reboot.  So do a full deep restart.  That will reestablish the secure boot chain, and you'll be okay.



We've previously talked about the way secure boot chains work.  I have a graphic here on page 14 of the show notes, which is from Apple's 2016 Worldwide Developer Conference presentation, showing the concept of the flow of the secure boot chain from power-on of an uncompromised device, where the Boot ROM, which is actually a ROM, gets control of the processor.  It then looks up at the Low-Level Bootloader, the so-called LLB.  And before it loads it and transfers control, it verifies Apple's signature.  And if the signature verifies, then it loads it and transfers control.



It then does its Low-Level Bootloader work, beginning to initialize and set up the hardware.  And then it wants to transfer control to iBoot.  Before it does that, it goes out and, similarly, verifies the Apple signature on that code which nobody knows how, thanks to a cryptographic signature, nobody knows how to spoof that.  That cannot be modified from the time Apple has done it.  And then it loads it and transfers control to it.  iBoot does the same thing for the kernel.  So it brings the kernel in, verifies the signature, gets the kernel going.  The kernel does the same thing for iOS, verifies the iOS signature, loads it, and gets it going.



So each step of that startup process verifies the signature, the digital signature signed only by Apple, using a super secret private key that doesn't exist in the device, never exists outside of Apple, in order to sort of literally to bootstrap itself, one module successively after another, until it's up.  What makes the CheckM8 so devastating is that it exploits a flaw which exists in the ROM that nothing can patch.  Apple can't go back and fix it.  It would require a factory recall, a physical recall of those devices because it's in the ROM.  So it intercepts it before the first signature check, which essentially allows the entire boot chain to be intercepted and all of that code to be studied, reverse engineered, dumped out.



And Apple now, all of these devices that are the more recent A11 and A10 devices, they're still receiving updates, right, from Apple.  That means that the instant an update is released, it can now be fully reverse engineered, analyzed, and compared against the previous version, which will allow both security researchers, but also bad guys, to figure out what Apple has changed, what it is exactly that Apple fixed.  And if they're able to get an exploit out into the wild before a targeted device has been updated, they could take advantage of that.



So Apple is famously rather quiet about their security updates.  We've often talked about how Apple just says, oh, we fixed this and this and this, but with no details.  Well, it hasn't been possible until now to easily get those details.  Now any device, the instant it is updated, can be jailbroken through this publicly disclosed and unfixable Boot ROM exploit in order to crack the device open, allow all the updated files to be dumped and analyzed.



So we probably are entering a new era in Apple device security.  As we know, they recently announced they would be making some selected special iPhones or iOS devices available to a few handpicked researchers who wish to explore iOS for bugs and security flaws.  Well, that's no longer necessary.  The entire world has that level of access now, and Apple has no choice but to continue supporting these devices which are vulnerable to this boot time bug.  So it isn't the end for - it's not directly a problem for iOS end users, but it really does change the security landscape for Apple, so long as devices that are still being updated and have this unpatchable Boot ROM flaw are out in the world.



LEO:  Now, it doesn't persist, but it's enough to get something else on your phone that would persist, like a spy program or that kind of thing.



STEVE:  Correct, until it's rebooted.  So if you lost physical control, you can imagine, I mean, imagine that the guys at...



LEO:  When you come into the United States, and they take your phone, and they go into another room.



STEVE:  Yup.  Yup.



LEO:  This happens all the time, not just entering the border.  If you're arrested, they take your phone, lock you up, modify your phone, give it back to you, say thanks, sorry, send you on your way.



STEVE:  Yup.  And Cellebrite, the guys who do the iPhone jailbreaking...



LEO:  We know they know about this; right?



STEVE:  They'll jump on this in a heartbeat.  And so it does mean that the world of jailbreaking, I mean, like being a problem, it's just not anymore.  Yes, on the A12 and A13 devices Apple fixed this.  And it was known...



LEO:  So the real risk to an end user would be if somebody got your phone, took it away.  They'd hack it with this thing, jailbreak it, which would allow them to sideload a surveillance app.



STEVE:  Yes.



LEO:  They could even reboot it, which would un-jailbreak it, but the surveillance app would still be there, and give it back to you.



STEVE:  Correct.  Correct.  So that's the direct effect of physical access to your device.  The secondary effect is this makes bugs way easier to find, and it makes Apple's changes easy to reverse engineer.  You know, we know because we're talking about it all the time, the instant Microsoft releases an update, the old version of the DLL is compared against the new version.  What it is they changed is found, and an exploit exists within hours of Windows release.  We're talking about this all the time.



Apple hasn't been exposed to that because they've been able to lock down their platform to a degree that Microsoft inherently can't.  That just changed.  Apple can no longer lock down their platform.  It is going to be open for anyone to reverse engineer any changes Apple makes to devices which are necessarily still being supported and are receiving updates.



LEO:  Even iOS 13 you could look into that and see what they're doing on the newest iOS, what new updates do.



STEVE:  On the older devices, yes.



LEO:  But you have to have an older device, yeah.



STEVE:  I have an iPhone 10.  I can do it on mine, if I cared.



LEO:  Wow.



STEVE:  Yeah.  It's a game changer.



LEO:  I had no idea.  I mean, we talked about it, but I didn't realize the real risk was this risk to Apple, more than to end users.



STEVE:  Yeah, I think so.  You know, Apple has enjoyed being coy, and they've been able to because they've had complete control of the boot chain.  And they just lost control.  



LEO:  This is why we need Steve Gibson in the world, and Security Now! is your "must listen to" podcast every week.  Thank you, Steve.  We do Security Now! Tuesdays, about 1:30 Pacific, sometimes later because it's a long day.  1:30 Pacific, 4:30 Eastern, 20:30 UTC.  There we go.  I got all the times right.  You can watch live at TWiT.tv/live.  If you do that, join us in the chatroom, irc.twit.tv.  Those are the kids talking about the show behind the scenes as we record it.  But there's also a great place to go now to talk about the show after we deliver it, because it is a podcast.  That's our new forums at twit.community.  Check it out, twit.community.



Steve has 16Kb audio versions of this show, 64Kb audio.  Plus, unique to his site, he's got the transcripts.  That's at GRC.com.  While you're there, pick up a copy of SpinRite, the world's best hard drive maintenance and recovery utility.  He didn't talk about it, but I will.  There's also lots of other great free stuff there, including ShieldsUP! and lots of information about SQRL:  GRC.com.  You want to ask Steve a question or talk at him, you can do that on Twitter:  @SGgrc.  He accepts DMs:  @SGgrc.  It's also a great place to follow Steve.  The show notes go up there ahead of time.  And anything he finds out during the week he tweets:  @SGgrc.



Our website for the show, TWiT.tv/sn.  You can get every episode ever recorded there, all what is it, 736 now.  You could also subscribe.  In fact, that's the best thing to do.  Get your complete set.  Just subscribe in your favorite podcast application, and it will be delivered to you the minute it's done.  But now we are done.  Thank you, Steve.  I will not be here next week, or the week after.



STEVE:  Ooh, your cruise begins.  Your cruise begins.



LEO:  My vacation.



STEVE:  It's only two weeks you're missing, not three?



LEO:  I'm missing a lot.  I'll be back.  I will be back November 20th, I think.  The first Tuesday - we get back November 15th.  The first Tuesday after that.



STEVE:  I think you're missing several weeks.



LEO:  I'm missing quite a few.  I'm sorry.  What can I say?  It's vacation.  I apologize.  I won't be here.  I'm not sure who's handling the show.  I'm guessing it's Jason Howell.  He's my usual fill-in.  But of course we tune in, not for me, but for Steve.  So he'll be here; right?



STEVE:  I'll be here.  I never go anywhere.



LEO:  I know.



STEVE:  Well, okay, not quite never.



LEO:  Yeah, that's changing, isn't it.  That's changing.



STEVE:  Not quite never, but I strongly resist.



LEO:  I got some bad news/good news for you.  LastPass enjoyed our visit to Boston so much they want to do two more events next year.  I'm not sure where they'll be.  They don't have to be with Steve, but I'm thinking they probably ought to be.  Anyway, I'll fill you in on that down the road.



STEVE:  Sounds good.



LEO:  Yeah.  We had a good time, didn't we.



STEVE:  We did.  And I will commit now.



LEO:  Oh, wow.  You just made somebody at LastPass very, very happy.  Thank you, Steve.  We'll see you next time on Security Now!.



STEVE:  Thanks, Leo.



Copyright (c) 2019 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.






GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#737

DATE:		October 22, 2019

TITLE:		Biometric Mess

HOSTS:	Steve Gibson & Jason Howell

SOURCE:	https://media.grc.com/sn/sn-737.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we check in on the frenzy to turn CheckM8 into a consumer-friendly iOS jailbreak, on another instance of stealth steganography, on a number of changes to Firefox's URL display, and on the state of Microsoft's ElectionGuard open source voting system.  We also look at a very serious flaw that was just found in Linux's Realtek WiFi driver and some welcome news from Yubico.  We touch on a couple of miscellaneous media tidbits, then take a look at the ramifications of two recent biometric authentication failures and consider the challenges and inherent tradeoffs of biometric authentication.



SHOW TEASE:  Coming up next on Security Now!, of course Steve Gibson is here, along with me, Jason Howell.  Steve's going to dive into, not one, but two big biometric mishaps on two of the biggest Android smartphones out on the market today.  Well, one of them's almost out on the market.  Also security display changes in Firefox 70, a big flaw in Realtek WiFi drivers, YubiKey for local Windows login is released, and a whole lot more, next on Security Now!.



JASON HOWELL:  This is Security Now! with Steve Gibson, Episode 737, recorded Tuesday, October 22nd, 2019:  Biometric Mess.



It's time for Security Now! with Steve Gibson.  I'm Jason Howell, filling in for Leo.  Leo's gone for a month, four episodes back to back that I'm going to be sitting in this chair along with you, Steve Gibson.  How are you, Steve?



STEVE GIBSON:  Hey, Jason.  Great to be with you.



JASON:  Great to see you.



STEVE:  I was looking.  I think it was like 726 or 23 or something was the last time we were together.



JASON:  Something like that.



STEVE:  So it's been not that long.



JASON:  Yeah.



STEVE:  And now we have four episodes...



JASON:  Absolutely.



STEVE:  ...that we're going to do while Leo is off on his cruise through Greece or wherever he's going.



JASON:  Yeah, you know, to be honest, like somebody asked me the other day, well, where is Leo going?  And I was like, you know what?  I can't remember.  I lose track of where Leo's going.  So I knew that he was going somewhere where Internet would be difficult.  He made some comment about that.  But good for him.  He should.  And, boy, we should all look forward to a month off somewhere down the line in our lives.



STEVE:  Well, we have Episode 737 for October 22nd.  And there were a couple of things that hit the news that, I mean, I was aware that you were going to be my co-host.  Of course you're Mr. Android, and these both affected Android devices.



JASON:  Yes.



STEVE:  Well, and there are, like, big-time security issues for us because it's about biometrics.  Two high-profile failures of biometric authentication were in the news in the last week.  So I thought, that's perfect for us to talk about.  And I've been wanting to talk about it in general.  We've been talking and sort of like following biometrics for at least a decade.  We were talking a long time ago about the fingerprint readers at the Disney theme parks and, like, wait a minute, do I want to be giving Disney my fingerprint for the privilege of, I don't know what, like getting in through the gate more easily than I would otherwise?  And so even back then we made a joke of it.  We were talking about giving them your knuckle instead of your fingerprint.  It's like, don't give them your fingerprint.  Give them your knuckle.



JASON:  Were you saying to punch them?  Is that - yeah, I'll give you my authentication.



STEVE:  So anyway, we are going to finish by talking about that.  But we've got a bunch of, of course, as we always do, interesting news of the week.  And I'm staying true to my promise not to beleaguer our listeners with talks of crypto malware because we sort of had, for a while there this summer, we were just - it was becoming the ransomware crypto podcast, and I think we maybe pushed our listeners a little bit too far.



But we're going to check back on the frenzy to turn the CheckM8 iOS jailbreak, well, iOS vulnerability, the Boot ROM vulnerability, into a consumer-friendly iOS jailbreak.  That's underway.  That was the title of last week's podcast was CheckM8 because it was such a big deal, the idea that all devices from the A5 chip up through the A11 all incorporate a Boot ROM flaw that Apple can never change because it's literally in ROM, which is soon to be leveraged into a jailbreak.  We're not quite there yet, but we'll talk about that.



We also have a new instance of stealth steganography which is sort of interesting because it's the whole concept of hiding something in plain sight.  We've got a number of forthcoming changes to Firefox's URL display in the next version of Firefox, which will be number 70.  Also an update on the state of Microsoft's ElectionGuard open source voting system.  Also there is a - of super important interest to all Linux users, and apparently that includes again Android devices, any Linux or Android device using the Realtek WiFi chip - a buffer overrun has been found which is as bad as it gets.  We'll talk about that.  No patch yet, but one is in the works.  And of course the problem is lots of people won't patch.



We also have some welcome news from Yubico.  We'll touch on a couple miscellaneous media tidbits just because our listeners know that I like some things that are on TV, and I'm an avid sci-fi reader, and John and I have just recently been exchanging  email about something that's about to happen.  And then we'll finish by taking a look at what's going on with biometrics, these recent high-profile failures and what it means.  So I think another great podcast for our listeners.



JASON:  And no surprise about that.  And I should also mention that when you were talking about the upcoming sci-fi news that you'll be talking about later, John's fist was raised in the air.  So he's very excited.  He's very excited.



STEVE:  Since you're able to see John, is he aware of Ryk Brown's - okay, I heard him.



JASON:  Oh, yeah.  The thumbs-up.



STEVE:  Okay.



JASON:  An emphatic, "Oh, yeah" and thumbs-up.



STEVE:  Okay, good.



JASON:  I, however, oh, no.  I'm sorry, I don't.



STEVE:  Well, you have children.  So, yes.  You're otherwise occupied.



JASON:  I'm distracted a little bit.  Although I did see the trailer for the next Star Wars earlier today.



JOHN SLANINA:  The last.



JASON:  Well, okay, sorry, the last "Star Wars," until the next trilogy that they decide to do.



STEVE:  And as a matter of fact, because I was curious about the IMDB rating for one of the things I'll be talking about today, I also went over to IMDB and watched the trailer for the final Star Wars.  It looks fantastic, actually.



JASON:  Of course.  It's so dramatic.  And I'm thinking as I'm watching, you know, my kids are six and nine; but, yeah, they've seen both of the previous two.  And I'm just thinking, all right, apparently I'm going to do the thing where we watch them in order again, leading up to when we go to the theater, and see how that goes.  So I'm looking forward to it.  All right.  Make it rain, Steve Gibson.



STEVE:  Well, our Picture of the Week was just - it was a fun one that I encountered when I was pulling the show together.  It's not something that most iOS users ever see, which is the screen of an iPhone showing a bunch of OS booting sort of stuff.  Anybody who's ever booted a Unix or Linux is used to seeing all of this amazing cruft scroll up the screen.  That's not something you ever experience on an iOS device.  So anyway, this is a screenshot of a jailbreak in progress showing the ELI stage 3 screen of text with a bunch of mumbo jumbo on it that doesn't really matter.  And then it does have the logo overlaying the Apple icon showing clearly a king up and then a king that has been knocked over, because of course flipping the king over is the famous acquiescence to a checkmate in chess.



So anyway, it also shows that the phone is tethered, which is one of the requirements for this forthcoming jailbreak for iOS devices.  And I've never had the occasion to jailbreak any of my i-things.  Just I haven't had any need.  I've been content with them the way they are.  But I have an iPhone 6 still, and I was forced off of it when Apple played those games of slowing down the 6 in fear that the battery was getting old, and it was just so frustrating that I finally said, okay, fine, and I got a 10, which is a smaller phone than - I have the big 6+.  And I'm happy with it, but I kept my 6 because then Apple figured out, oh, I guess we ought to check to see if the batteries are actually bad, rather than just assuming they are, because of course I had been babying mine for its entire life, and it was in perfect shape.



So anyway, the 6 came back to life after an update, and I still have it.  So I will probably, since it's got one of the chips that's right in the sweet spot of this forthcoming, presumed forthcoming consumer jailbreak for iOS, I think I'll probably play a little bit with that phone since it's a spare.



JASON:  You might as well.



STEVE:  Yeah.



JASON:  You might as well, if it's just sitting around collecting dust.  Now, so this is a tethered jailbreak, so does that mean that it will not survive a reboot?  You reboot this thing, it's going to clear it out, so you're going to have to tether it again in order to do that again?



STEVE:  Correct.  And that's fine for me because I would only do this just sort of as a lark in order to see what it was like and maybe screw around a little bit.  But so it's a tethered break.  It intercepts the boot process in a way then that will allow a full jailbreak to be developed, which Apple for that range of devices, from the iPhone 4S, I think it was, that used the A5 chip, all the way up through actually the iPhone 10 that I have, would also - it also uses the A11, which is the last one.  Apple found the problem last summer and fixed it.  So for the XS and all the other phones and everything since, the A12 and the A13 devices, this problem's not there.  So the problem's been known for a year, but no one had figured out how to leverage that into a boot time jailbreak, and that's now been done.



So an update, because this was the topic of last week.  There is a site where this jailbreak will probably first appear.  What's odd is that the name is the hacker-ish name, not the name that you would expect.  So the site is Checkra1n.com.  That's the site where we're expecting to see this appear.  And in fact I'm going to just check.  I didn't look - oh, yeah.  So I just brought it up, Checkra1n.com.  It's just a placeholder page with nothing but that logo that I mentioned seeing on our Picture of the Week, which is the king upright and then I guess it's the black king behind it knocked over.  The exact spelling is important because bad guys immediately jumped on the normal spelling of the site, Checkrain, R-A-I-N, and put some bad stuff on that site.  So I wanted to caution our listeners, anyone who may be interested in going to the Checkra1n spelled R-A-1-N dot com, that is, that's the proper site.  That's probably where this is going to appear.



And also note that it's still the case for our browsers that, if you just put the domain name in and hit Enter, the browser assumes non-HTTPS.  Which to me that's interesting because of course the most recent metrics are showing that 80% of Firefox users - we'll be talking about this a little bit later - 80%, eight zero percent of Firefox users are now at HTTPS-protected sites by default, or by habit, 80% of sites are now HTTPS as opposed to not.  At some point it would make sense to me that browsers would start assuming HTTPS if the user just puts the domain name in.  But that still hasn't happened.  I put it in.  I put Checkra1n.com into Firefox last night, hit Enter.  Up came this placeholder page.  Then I was curious.  So I manually put https:// in front of the domain, and I got an error, which was odd.



Well, it turns out that the site is, like, again, it's sort of not like official yet.  So it's got a bogus Let's Encrypt cert configured on that domain to a site.  The cert that comes up is Q-W-E-R-T-Y - of course "qwerty"; right?  Q-W-E-R-T-Y.  Then all lowercase, well, case doesn't matter for domain names, O-R-U-I-O-P.  So then it's not quite across the top, that first row of alpha, but then it kind of ends up with the U-I-O-P.  So let's see, I guess it's just they inserted an extra "O" in there, and then it resumes with Y-U-I-O-P dot com.  Anyway, so I thought, what the heck.  So then I went there.  And what's there is that same qwertyoruiop, and then it says hyphen security researcher and a few other lines of text.  But there's nothing else at that site either.



So I don't know what's going on.  But that certificate is currently configured on this weird Checkra1n.com site.  So your browser of course complains.  I had to push through a bunch of "Are you sure" and, you know, blah blah blah warnings.  So just put in, you know, just go http:// or don't do anything, and your browser will probably take you there.  That's where it's going to appear.  It hasn't yet appeared.  But it looks like that's going to happen.  There's been a lot of dialogue online.  The hackers are scurrying to leverage the CheckM8 vulnerability which is still up as sort of a proof of concept up on GitHub.  I imagine within a week or two, I mean, these things, it may take a while, but everybody believes we're going to soon have a jailbreak that Apple will not be able to fix.  So that's the site to keep an eye on.



And certainly, if that happens by next week, you and I will be talking about it; or the week after, you and I will be talking about it; or the week after, you and I will be talking about it; or the week after, you and I will be talking about it.  I have a feeling you and I are going to be talking about this.



JASON:  Probably so.  It's really unfortunate that this isn't the other way around because everybody's going to hear "Checkrain" and type it in the normal way and then, you know, be like, oh, I'm going to install whatever that is.



STEVE:  Yes, yes, yes.  Isn't that...



JASON:  It's really unfortunate.



STEVE:  Is that dumb that the hacker, like, did the hackeresque name, or didn't get both of them.  I mean, you know, dotcoms are cheap now.  And so it's like, wow, that just seems like so unfortunate because what is at Checkrain is, I mean, it's not malware, but it offers some kind of semi-malicious pay-to-load junkware apps which unwitting users are going to grab thinking that it's the jailbreak, and it's not.  Anyway.



JASON:  Indeed, yup.



STEVE:  As we know, steganography is the practice of hiding in plain sight.  Well, kind of.  A better way to put it would be adding unrelated - well, at least in the digital era.  You could certainly have used steganography back in Picasso's day to hide some image in another image.  But, you know, now that we're in the digital era, you would call it adding unrelated data to an existing file in such a fashion that the original appears to be unchanged while still carrying that unrelated data.  And we've talked about image files having that done to them.  JPEG and PNG image formats have had steganographic content added to them while still showing the original image.  With JPEG, it's very tricky because JPEG itself is deliberately a highly lossy compressed format.  As we know, it achieves very high compression of images by representing the original image in the frequency domain using a technique known as "discrete cosine transforms."



So the point is that what's put in there, the original image that's compressed is not at all what comes back out.  But it visually looks similar to our eyes.  By comparison, PNG images, when they are not using a reduced color palette, which they can be palettized images, which then approximates the exact colors in the image with a reduced color set in order to get additional compression.  But when it's not using a color palette, when it's using full 24-bit color depth, those images are inherently lossless.  And so that makes hiding data in their least significant image pixels much more straightforward.



Anyway, what security researchers have now detected in use in the wild is they're calling them "malicious WAV format audio files."  And I guess it's malicious content.  But the WAV format will not, you know, it's a non-executable format, very much like images.  So it's not like playing the WAV format would, unless there was a flaw in the WAV codec, which the WAV format was leveraging, playing the WAV format would not itself execute malicious content on your system.  The idea is that, for some reason, the bad guys are conveying malicious content hidden in a WAV file.  And in some cases the WAV file still sounds fine.  I mean, it seems unaltered.  In other cases it just sounds like noise, like static.



So it turns out that they've found them both using least significant bits in the audio sample, in which case our human ear would not notice a few, maybe it's even more than one.  We would not hear a little bit of Least Significant Bit noise in your typical audio playback.  In other instances they're just putting a WAV file header on the front of a traditional binary format file.  And when you play that back it just sounds like white noise.  It's just nonsense.  So anyway, Symantec researchers spotted the Russian-backed Turla threat group, also known as Waterbug or Venomous Bear.  No one seems to be able to agree on the names of these Russian guys.  They all make up their own names, and then they later realize, oh, those are the same people that somebody else was talking about.



Anyway, they're delivering the publicly available Metasploit Meterpreter backdoor embedded in a WAV file audio track.  And then subsequently Cylance found that the same steganography method was employed to infect targeted devices with the XMRig Monero cryptominer or Metasploit code designed to establish a reverse shell.  But again, you have to have some malware on the receiving end that knows to look inside the WAV file to obtain this content.  So it must just be that the bad guys have chosen to hide stuff in these formats because they are not typically scrutinized by firewalls or intrusion detection systems.  Those intermediate filters see a WAV file and go, yeah, okay, fine.  They're not listening to it.



Now, in the case of this one that's completely nonsense, it would be possible to listen to it, kind of, like to see whether it looks like a WAV file because visually we're able to examine the samples in a WAV file and see complex sine wave frequencies that are obvious.  And what a binary file looks like is just absolute nonsense, absolute gibberish, clearly non-audio content.  So if anyone really cared, it would be possible to discriminate that kind of a WAV header on the front of a binary file as a means for getting it through some sort of an intrusion detection system.  On the other hand, technically it's a valid WAV file.  So you'd have to decide if you really wanted to block something.  I mean, maybe somebody wants white noise WAV file for some purpose that's legitimate and isn't in fact nefarious in content.



So anyway, the researchers took these things apart, did determine in the audible one, the one that still remained playable, that there was Least Significant Bit steganography occurring, and that the receiving code was extracting the least significant bit, reconstructing a file that was of interest to it, and then doing something with it.  In the other instances, they were employing essentially a one-time pad based on a pseudorandom number generator to scramble the file, which would then be descrambled by XORing it with the same pseudorandom pad output on the receiving end in order either to decode it into a standard Windows PE format executable or to execute shell code.  So just another interesting instance of steganography that we've not seen employed before in a WAV file.



And I suppose, again, if intrusion detection systems were concerned about this, it would be unlikely that they could detect it as LSB, as Least Significant Bits.  But certainly, if something doesn't in any way look like it would be audibly playable, that is, it's just binary noise, I could see that an IDS might just decide to block it at the perimeter because it would be a means of getting a high-density executable file past a perimeter firewall and safeguards.



JASON:  My question on that is what exactly is the difference between binary noise, just jumbled binary noise, and real white noise?  They probably sound the same.



STEVE:  Yes.



JASON:  And there are reasons to have audio files that are just white noise.



STEVE:  Yes.  



JASON:  Yeah, I wonder if there's any sort of difference that could be detected along the way between the two.



STEVE:  Well, so I've spent a lot of time in my life looking at  binary executable code.  And generally you've got, I mean, there are definite regions that are not white.  Normally one of the reasons people compress Windows PE format files is they notoriously have huge blocks of zeroes.  Just, for example, data regions in executable files are initialized to zero.  They're not left uninitialized.  They're all set to zero.  The advantage is, if you make the mistake of using that as a pointer, you'll get a null pointer.  And that immediately, that's guaranteed to cause a crash rather than, like, well, it depends upon what data was already in RAM when the file loaded.  That never happens.



So you definitely have blocks that are non-white, that is, that are all null.  Of course that would be silence in a WAV file, which is also not illegal, so that could happen.  So it would be - it's certainly the case that you could create a heuristic algorithm to examine a WAV file to see if it makes sense, and just to decide this is not looking like a WAV file.  This looks executable.



On the other hand, we're also seeing them deliberately scrambling the binary content with a pseudorandom sequence, so that would turn it into complete white noise.  By definition, the output of a random number generator is white noise.  So it's absolutely uniformly distributed random samples.  So if you XOR anything with that, you're going to get white noise output, which is always sort of oddly counterintuitive, but it's true.  And then you re-XOR that white noise with the same pseudorandom data, and you recover your original non-random executable content, which is the way a lot of block ciphers, or stream ciphers especially, operate.  So, interesting.



JASON:  Yeah, very interesting.  You've got like a threefer on Firefox coming up.



STEVE:  Yeah, we do.  As happened with Chrome, well, and actually there's a nice reason.  I considered putting the third story first just to increase the significance of the other two because the third story is about - and we'll get to it in a second - about how the German security arm, Germany's cybersecurity - I'm at a loss for words.  Not foundation, authority - chose Firefox among four competing browsers:  Safari, IE, Edge, and Chrome.  No, not Safari.  So it was IE, Edge, Chrome, and Firefox.  Of those four, Firefox was the only one to qualify for all of the points that this German cybersecurity agency - "agency," that's the perfect word I was looking for - was looking for.  But we'll get to that in a second.



First, we are currently at Firefox 69.  And Firefox has been gaining in popularity.  Leo has switched to it.  I'm still with it.  I've always been with it.  Because it's turning out to be the one that is most - that's demonstrating the greatest concern for its users' privacy and security.  Google is in the understandably delicate position of being all about advertising, and advertising is all about tracking.  And so Chrome just, you know, which is a Google property, is kind of stuck there in the middle.  Firefox isn't.  So Firefox 70, seven zero, which will be released next - I think it's by the end of the year.  I don't remember.  I didn't have it in my notes here.  But we're at 69 now, so it's the next major release of Firefox will be following the industry to change the way sites appear in the URL address bar.



Currently, under 69, we get a nice, friendly, green padlock for HTTPS sites.  And for sites that are presenting an extended validation, an EV certificate, the company's full name is shown.  All of that changes with the next major release, unfortunately probably forever.  The padlock no longer is green.  It's kind of a dark gray.  And that's intended to signify that HTTPS is the new norm.  No more green.  Previously, only insecure pages with login forms would be shown an angry red slash through a black padlock.  Now, with release 70, the next one, all pages, every page of non-HTTPS sites, which is to say just HTTP, will receive the angry red slash padlock, regardless of whether or not they are requesting sensitive information from their visitors.



In other words, HTTPS is, as I said, the new norm.  And as with Chrome, the user will need to dig down into the URL window-shade dropdown if they're curious to inspect the type of certificate being presented by the site.  That will be the only way in the future to see whether a site is using an EV certificate, that they've decided they're going to economize on the space in the URL.



Also, as we've discussed previously, it is the case, for example, that right now, under this current Firefox 69, if you go to my site, GRC.com, it says "Gibson Research Corp." in green.  That's been the traditional EV indicator.  It's been noted, however, that there's nothing to prevent some other entity from incorporating Gibson Research Corporation in a different state of the United States and validly registering an EV certificate that shows the same name, which would lead anyone to believe, because we've been around a long time, that they're Gibson Research Corporation with an EV certificate, when in fact they are, unfortunately, with the same name, because I can't prevent somebody else from registering a name outside of California.  So that's what happens.



So I did inspect the dropdown of the security site I was visiting at the time.  I was on an Internet security site, and separately I was shocked by the number of unblocked trackers and third-party tracking cookies that Firefox was showing it was permitting.  So it turns out that I had not been keeping up with Firefox's changes of late.  Firefox has been making some content blocking changes, strengthening many of the defaults that it uses.  And in checking into what was happening, I realized that I should have been changing my settings along the way.  Since I imagine that others listening to this podcast may be in the same situation, I wanted to give everyone a heads-up.



Previously, to get the strongest security settings, it was necessary to use Firefox's custom tracking blocking options.  That's changed.  Under Firefox's main menu, it's like the little three bars in the upper right-hand corner of the browser chrome, under the second line item is content blocking.  I had earlier set it to "custom."  And as I said, that was the necessary setting at the time to enable the stronger protections which were not on by default.  But now they are on by default, and custom wasn't getting it done for me any longer.



So our users want to switch to "strict," which now and probably forever implements the strongest protection for users when they're visiting the 'Net by default.  And because it can, and Firefox warns of this, it can cause some sites to have a problem and to break, then you can reduce your security and privacy selectively by site if you choose.  But anyway, I immediately switched to "strict."  Everything seemed fine.  But I had not realized that in moving forward my previous setting of "custom" was no longer blocking everything as I would wish it to.  And so "strict" will do that.



And as you said, Jason, we've got three things.  The second is I'm 100% sure that those who have been pushing Firefox in the direction of increased security and privacy felt rewarded by the news that Germany's cybersecurity agency is now recommending Firefox as the most secure browser.  And that recommendation is within Germany for all of the corporate entities and other Germany agencies that are following this advice.  I cannot pronounce the name of this German Federal Office for Information Security.  It goes by the initials BSI, which stands for Bundesamt fr Sicherheit in der Informationstechnik.  I apologize to...



JASON:  Yeah, I think that was pretty good.  I think.



STEVE:  I apologize to the German speakers among us.



JASON:  From no authority whatsoever I think you did a great job.



STEVE:  We will refer to them henceforth as BSI. 



JASON:  All right.



STEVE:  Which is, you know - anyway, yes.  They tested Firefox, Chrome, IE, and Edge.  Of those four, Firefox was the only browser to pass all of the minimum requirements for mandatory security features.  Thus Firefox received their top marks during a recent audit which was performed by them.  So this BSI tested Firefox 68.  So that's the previous edition.  We're on 69 now, as I mentioned.  They tested Google's Chrome 76, IE11, which is also the current one, the current IE from Microsoft, and Edge 44.  Importantly, the tests did not include other browsers, namely Safari, Brave, Opera, or Vivaldi, which I do think is unfortunate since several of them are even more security and privacy-centric than Firefox.  But I have a sense, and our listeners will, after we look at what this BSI organization felt was important, that those browsers probably would have not made the grade either.



So the audit was carried out using rules detailed in a guide for modern secure browsers that the BSI updated last month, in September of 2019.  The BSI uses this guide to advise government agencies and companies from the private sector which browsers are safe to use.  The first edition was published two years ago, in 2017.  Then it was reviewed and updated, most recently this summer.  As a consequence, the BSI updated its guide to incorporate an awareness of the improved security measures over the last two years which have recently been added to our modern browsers.



So that includes things like, well, in fact HSTS is older than that, has been around for quite a while.  That's of course HTTP Strict Transport Security.  Also SRI, Sub-Resource Integrity.  That's the provision for verifying that something you are requesting from a remote server has not been changed.  CSP, Content Security Policy, that we've talked about extensively, which allows the web server to publish its policy which it wants the browser to enforce for the things that it subsequently requests.  Also telemetry handling and improved certificate-handling mechanisms.



So according to the BSI's new guide, to be considered secure, a modern browser must satisfy a set of minimum requirements.  And I'm going to tick off a 21-point list, but I think it's important and interesting to look at what this is.  A lot of these things won't matter to end users.  They sort of have an enterprise spin to them, which is again why I sort of wish that we had the other non-tested browsers placed into the same context.



But so here are the 21 things that the BSI felt was important:  Must support TLS.  Yeah, no kidding.  I don't think there's a browser that doesn't.  Must have a list of trusted certificates.  Must support extended validation, EV certificates.  Must verify loaded certificates against a certificate revocation list or an online certificate status protocol.  So either a CRL or an OCSP.  The browser must use icons or color highlights to show when communications to a remote server is encrypted or in plaintext.  Connections to remote websites running on expired certificates must be allowed only after specific user approval.



Must support HSTS.  Must support same-origin policy, and must support content security policy.  Must support sub-resource integrity.  Must support automatic updates, which is to say must support a separate update mechanism for crucial browser components and extensions.  Browser updates must be signed and verifiable.  Browser's password manager must store passwords in an encrypted form.  Access to the browser's built-in password vault must be allowed only after the user has entered a master password.  The user must be able to delete passwords from the browser's password manager.



Users must be able to block or delete cookie files.  Users must be able to block or delete auto-complete histories.  Users must be able to block or delete the browsing history.  Organization admins must be able to configure or block browsers from sending telemetry and usage data.  Browsers must support a mechanism to check for harmful content and URLs.  Browsers should let organizations run locally stored URL blacklists.  Must support a settings section where users can enable or disable plug-ins, extensions, or JavaScript.  Browsers must be able to import centrally created configuration settings, which is ideal for  wide-scale enterprise deployments.  Must allow admins to disable browser-based profile synchronization features.



Must run, after its initialization, with minimal rights in the  operating system.  Must support sandboxing.  All browser components must be isolated from each other and the operating system.  Communication between the isolated components may only take place via defined interfaces.  Direct access to resources of isolated components must not be possible.  Web pages need to be isolated from each other, ideally in the form of standalone processes.  Threat-level isolation is also allowed.



Browsers must be coded using programming languages that support stack and heap memory protections.  Browser vendor must provide security updates no longer than 21 days after the public disclosure of a security flaw.  If the primary browser vendor fails to provide a security update, organizations must move to a new browser.  And, finally, browsers must use OS memory protections like Address Space Layout Randomization or Data Execution Prevention.  Organization admins must be able to regulate or block the installation of unsanctioned add-ons and extensions.  Whew.



So, frankly, I'm kind of surprised that any browser was able to qualify against all of those.  And I'm proud and impressed that the browser I've chosen and am using was the one that did.  So according to the BSI, Firefox, as I said, is the only browser  to meet all of those requirements.  There were at least eight areas where the various other three browsers failed.  So to simplify things a little bit, I'll first note that IE failed all of these eight requirements.  And I don't know if that's that important.  It's still present, as we know, in Windows 10, though Edge is its formal successor and is today a far better browser.



So neither Chrome, Edge, or IE offer support for a master password.  IE has no built-in update mechanism.  Neither Chrome, Edge, nor IE offer an option to block telemetry collection.  IE alone is lacking in its support for same-origin policy, content security policy, and sub-resource integrity support.  And really, you know, in this day and age, if you can't support same-origin policy and content security policy, you're no longer in the game.  So IE is probably still present only for legacy support in enterprise instances where they've got things that will not run under Edge and only run under IE.  It really is time to move away from that.  Fifth, neither IE nor Edge offer support for browser profiles and different configurations.  And lastly, Chrome, Edge, and IE all lack a provision for what was called "organizational transparency."  Whatever that is, Firefox has it.



So it was the one that Germany chose, the only one they are now recommending going forward.  And again, since this is not a comprehensive list of all available browsers, I recognize that it's a bit skewed.  But on the other hand, reading through that list, I would be surprised if any of the other ones were also able to qualify.  It's sort of amazing that Firefox could because, wow, they really did set a pretty high bar.



JASON:  All right.  So, great.  We're still gushing about Firefox.  What else is awesome about it?



STEVE:  Yes, I get that.



JASON:  Meanwhile I'm over here, like I'm a Chrome user.  I'm like, okay, well, maybe I should switch over to Firefox.



STEVE:  Well, Chrome is the majority browser.



JASON:  It is.



STEVE:  More people are Chrome users than anything else.  And maybe Google will look at the things where it fell a little bit short and decide whether or not they care.  Let's see.  Master password.  I don't know why it doesn't have that.  Block telemetry collection, that would seem like a good thing, to offer it as a switch.  Leave it, you know, leave it off by default, but give it to the user if they want to.  And then organizational transparency, whatever the heck that is.  So provide some, and then Google could be recommended in Germany.



JASON:  Right.



STEVE:  So Mozilla has wisely recognized that its own internal HTML JavaScript-based browser pages could be subject to exploitation.  We're often talking about about:config, which is just this unbelievable list of line items of tweakable things that is inside of Firefox.  And in fact it's so big that you have to use the search bar in order to whittle it down to something manageable.  So anyway, we're always talking about that.  Mozilla's taken the wise precaution of preemptively blocking both inline and eval JavaScript on those pages to prevent any possibility of successful script injection attacks.  They recognize that mistakes can happen.  And they had to go through some hoops to rewrite those pages to be safe after, well, to still be functional after this block was put in place. 



It turns out, and I wasn't aware there were this many of them, they have 45 internal locally hosted about: pages.  About:config is the one we're always talking about.  There's about:downloads.  About:memory, which shows Firefox's memory usage.  About:newtab is the default page that you get with a new tab.  About:plugins - believe me, I'm not listing all 45, just the top few.  About:plugins is a list of all your plugins, as well as other useful information.  About:privatebrowsing opens a new private window.  That's a handy shortcut.  And about:networking displays various networking information.  And there's another 36 on top of that.



So since all of those pages are written in HTML and JavaScript and render within the essentially unlimited security context of the browser itself, they are prone to code injection attacks that, in the case of a vulnerability, could allow remote attackers to inject and execute arbitrary code on behalf of the user, i.e., in things like cross-site scripting attacks.  So to add a robust first line of defense against code injection attacks, even when there is a vulnerability, I mean, even in the face of a vulnerability, Mozilla took the preemptive act of blocking the execution of all inline scripts, which would then foreclose any injected scripts as part of that.



They implemented a strict content security policy to ensure that JavaScript code only executes when loaded from a packaged resource using the internal protocol.  That required them to, as I mentioned, to rewrite all inline event handlers and moved all inline JavaScript that had been present in those pages out of line into separate packaged files for all 45 of those about: pages.



In their blog posting about this last week, they wrote:  Not allowing any inline script in any of the about: pages limits the attack surface of arbitrary code execution and hence provides a strong first line of defense against code injection attacks.  So when attackers cannot inject script directly, they will fall back.  They'll tend to use the JavaScript eval function and similar regarded-as-dangerous functions to trick the target applications into converting text into an executable JavaScript script in order to achieve code injection.  So in addition to the inline scripts, Mozilla also removed and blocked all of the eval and similar functions which Mozilla feels is another dangerous tool.  They're not happy that it's in JavaScript, but they need to support it.



They wrote:  "If you run eval with a string that could be affected by a malicious party, you may end up running malicious code on the user's machine with the permissions of your webpage and extension."  And even Google feels strongly about this.  Google said:  "Eval is a dangerous function inside an extension because the code it executes has access to everything in the extension's high-permission environment."



So Mozilla rewrote all use of eval-like functions from system privileged contexts.  They removed them all, and from the parent process in its codebase in Firefox.  On top of this, Mozilla also added eval assertions that will disallow the use of eval functions and any of its relatives in system-privileged script contexts, and will inform the Mozilla Security Team of any unknown instances which the browser may encounter.  So what all of this means is that, for we Firefox users, Mozilla is being very proactive about the security on behalf of their users.  And of course in this day and age, as we know, it is very important for them to be proactive.



We talked about Microsoft's ElectionGuard open source election security system shortly after it was originally announced in May.  We didn't know much then.  There was a placeholder on GitHub with some aspirational text, not much more.  The good news is we're beginning to see some movement away from, in general, the whole environment of Diebold-style closed, failed, and proprietary election systems for-profit model.  That's the environment we've been in so far, where the election machines are purchased by states throughout the United States and even elsewhere.  India, with its huge democracy, has the same problem.  We're beginning to move towards open.



So in my opinion Microsoft is helping this hugely.  Last May 6th, during their Build Developer Conference, they announced for the first time their free open source SDK called ElectionGuard, which is aiming to enable end-to-end verification of voting.  All of the goods are now up on GitHub.  I have a link to them in the show notes.  I imagine if you just google "Microsoft ElectionGuard" - as one word, ElectionGuard - "SDK," you will be taken there.  It can be integrated into voting systems and has been designed to enable, in their words, end-to-end verification of elections, open results to third-party organizations for secure validation, and allow individual voters to confirm that their votes were correctly counted.  So we're talking, I mean, this is a next-generation in - it uses state-of-the-art homomorphic encryption in order to pull off some of this magic.  But that's where we need to go.



So it's back in the news because Microsoft has followed through by wrapping their bug bounty program in the major ElectionGuard modules.  Not all of them yet because I guess they're not feeling that they're all quite ready to be attacked by the hackers of the world.  But they are inviting security researchers from across the world to help with the discovery of high-impact vulnerabilities in this now-published ElectionGuard SDK.



In their blog posting announcing the bounty, they wrote:  "The ElectionGuard Bounty program invites security researchers to partner with Microsoft to secure ElectionGuard users, and is a part of Microsoft's broader commitment to preserving and protecting electoral processes under the Defending Democracy Program.  Researchers from across the globe, whether full-time cybersecurity professionals, part-time hobbyists, or students, are invited to discover high-impact vulnerabilities in targeted areas of the ElectionGuard SDK and share them with Microsoft under their Coordinated Vulnerability Disclosure program."



So ElectionGuard Bounty offers cybersecurity researchers a reward of up to $15,000 for eligible submissions with a clear and concise proof of concept to demonstrate how the discovered vulnerability could be exploited to achieve an in-scope security impact.  In other words, for these things that are covered, Microsoft believes they are ready.  The ElectionGuard components that are currently in scope for bug bounty awards include the ElectionGuard API SDK, the ElectionGuard specification and documentation, and the verifier reference implementation.  Microsoft indicated that it will update the ElectionGuard Bounty scope with additional components to award further research in the future.



So this is just very cool.  I can't think of anything more important than a respected entity that has been working with other cybersecurity organizations to develop a next-generation capability.  It's free.  It's open source.  Why wouldn't proprietary vendors incorporate this with the understanding that they need to open their systems?  There's nothing wrong with selling for profit an appliance that delivers the required technology.  But the technology itself has to be open to audit and verification.  That's just the way we have to go.  So this is a great step in that direction.  And I really -  I don't often congratulate and salute Microsoft for things they do.  Everyone on this podcast knows that.  But in this case I think this is just wonderful.



JASON:  Yeah, I completely agree.  Definitely an important thing for Microsoft to be helping out with.  But those behind these closed systems, like do they realize that it's important to open the systems?  And I guess that's the ultimate challenge, right, is to get them to [crosstalk].



STEVE:  I think that remains to be seen.  Yeah.  I think what's going to happen is that, while all systems were closed, the purchasers in states have had no choice but to choose among the best what they felt was a closed solution.  Now that we have open solutions - and there have been some rumblings that states would no longer purchase any closed solution.  So that's what we have to have.  As soon as one of the major vendors switches to ElectionGuard and offers a commercial implementation of Microsoft's open and auditable ElectionGuard system, then it'll be possible for the purchasing entities spread across the U.S. to say, okay, we're not buying it unless it is an open auditable ElectionGuard solution.



And the other cool thing is it's not just that it is open and auditable.  It offers features that voters want.  The idea that you are able to, after the fact, go home and take a receipt that you received from your voting experience and put it into a website and interactively, with cryptographic verification, know that your vote is among those counted and tallied, that's super cool.



JASON:  Absolutely.



STEVE:  I mean, you know, that's just neat.  That does away with all of the backroom shenanigans, all of the problems that we've had in Florida with hanging chad and all of this, I mean, it's a new game.  So it always seems that, in order to fix things, this country has to go through a period of intense pain and anguish.  And, you know, then we get things fixed.  So we've had that with our voting systems in the past, and it really does look like the consequence of that is a whole new dawn of voting integrity.  So yay.



JASON:  Right.  Yay, agreed.



STEVE:  Okay.  So of crucial importance to anyone, well, of crucial interest, I won't say "importance" because I don't want to overstate the actual impact of this, but the impact will be up to individuals to determine for themselves, given the set of facts.  And the set of facts are without controversy.  As we know, kernel driver flaws are always worrisome because they can be extremely potent, being in the kernel.  And flaws are even more problematic when they affect wireless interfaces.  And they are more problematic when they affect systems that are in their default configuration.  And they are still more problematic when they require no user interaction to be exploited.  And they are even more problematic when the flaw is a buffer overrun of a remote attacker-provided buffer of data.



This bug being tracked as CVE-2019-17 - and I love the fact that it ends in 666 - has all of those properties.  It was discovered by Nico Waisman, the principal security engineer at GitHub, last week while he was examining the handling of Notice of Absence protocol packets in Linux.  A patch to correct this is currently under revision, but has not yet been incorporated into the Linux kernel.  And I checked just before the podcast.  So hopefully, by the time people listen to this, that may change.  But so it's like it's very pregnant right now.  If successfully weaponized, and to our knowledge, to public knowledge that has not yet been done, if successfully weaponized, it would allow attackers to fully compromise vulnerable machines remotely in their default configuration without any action on behalf of the machine's user.



It is classified as "CRITICAL," in all caps, in severity, for all of those reasons.  It exists in the "rtlwifi" driver.  So it only affects Realtek WiFi-based systems.  The driver's been found to be vulnerable to a buffer overflow attack.  Obviously, systems lacking WiFi or with their WiFi disabled or with some other non-Realtek WiFi chip, will all be safe.  Otherwise, not so much.  The driver flaw can be triggered when an affected device is simply within radio range of a malicious attacker's device.  As long as the WiFi is turned on, it requires no interaction on the part of the end user.



The malicious device would exploit the vulnerability by using a power-saving feature known as Notice of Absence.  It's built into the Wi-Fi Direct protocol.  Wi-Fi Direct is a peer-to-peer standard that allows two devices to connect over WiFi without the need for a common access point.  The attack would work by adding vendor-specific information elements to WiFi beacons that, when received by a vulnerable device, would trigger the buffer overflow in the Linux kernel.  So once again, it only affects Linux devices using a Realtek chip when WiFi is enabled.  It cannot be triggered if it's turned off or if it uses a non-Realtek chip because that would have a different driver without this flaw.



Reporting does indicate that Android devices containing Realtek WiFi chips will also be affected.  So of course that's big because Android devices are inherently mobile.  They inherently have WiFi turned on.  And many of their kernels are unfortunately never going to be updated.  This has been...



JASON:  I've been there, yup.



STEVE:  Yeah.  This has been - I've seen some reports saying 2013, which would make it six years old, although more credible reports are saying four years old.  So at least for the last four years.  On the other hand, that is a ton of Android mobile devices that have been sold by vendors that are not updating their kernels ever.  So this is potentially a huge golden nugget opportunity for the bad guys, if they can figure out how to weaponize this.  So far, all we've seen disclosed publicly is a denial of service, which crashes the targeted device.  So we don't yet know for sure that anything more is possible.



However, looking at the source code - and it's easy to do.  Everybody, all the bad guys are looking at it right now.  It very much looks like a user-provided packet is the thing that will be made to overflow the buffer.  And that, I mean, that's the golden keys.  That's what you want.  The flaw was introduced in the driver innocuously.



The Linux gurus are all over the case.  Laura Abbott posted to the Linux Kernel Mailing List a note saying:  "Nicolas Waisman noticed that, even though" - and the variable is N-O-A, that's Notice of Absence - "noa_len," that is, the length - "is checked for a compatible length, it's still possible to overrun the buffers of p2pinfo, since there's no check on the upper bound of noa_num."  In other words, the noa_len is probably the length that is self-declared in the packet.  I didn't look, but that would make sense.  Whereas noa_num is the actual size of the packet, and there's no check on it.



So then she says:  "Bounds check noa_num against P2P_MAX_NOA_NUM," which essentially is like adding one simple test to the code.  In other words, it is trivial to fix this.  It's not going to take any time at all.  They're already, I mean, the fix is known and has been for a week.  The problem is it hasn't yet been pushed out; and, even when it is, how many systems with Realtek chips enabled of any sort are not going to get that patch?



So anyway, security-conscious Linux users with Realtek WiFi will want to be on the lookout for a Realtek driver update which should be coming very soon.  This is not a mysterious or difficult-to-fix patch.  I'm sure we will have it shortly.  If you are really security conscious, you might want to disable WiFi, if you don't know that you need it on.  There is no way to protect yourself currently with an unpatched Linux kernel and Realtek chips if you need WiFi on.  Even if you've established a link to a local access point, your chip will still promiscuously accept an incoming packet on this WiFi direct peer-to-peer protocol that would allow it to be taken over.



Again, this has just happened.  So there is no known - there's no publicly known takeover.  But I have a feeling, Jason, you and I in the next week or the week after or the week after may be talking about that having been developed.  And it'll be interesting to see if we get more news on the Android front because of course that's the big enchilada here because there are, you know, a lot of laptops have WiFi on.  Hopefully they will be able to get updated.  Linux-based servers probably don't have WiFi enabled.  They're all going to be safe.  But the big target are the Android devices, if they are known.  Do you have any sense for the presence of Realtek chips in WiFi in Android devices?



JASON:  Well, you know, as I was reading this earlier today, I was doing some Google searches to try and find some sort of list or some sort of indication what devices have this in there.  And I couldn't find any exact list that detailed devices, just that they exist in many.  And so there's the challenge is it's kind of obscure to even know that it's there.  Possibly there's a way to go into system settings and be able to look for a certain piece of information or detail in there that would show it on a device-by-device basis.  But I couldn't find any indication.



That's what really worries me is that if it's so obscure to know this, and that coupled with just the bad track record of updating the Android OS, at least on a Linux-based system you can apply an updated driver, and boom, you're done.  On Android it's really just like the manufacturer or the maker has to roll out a separate security update to the OS entirely.  And they're just not motivated to do that.  They're really not.



STEVE:  No.



JASON:  Especially on four-year-old devices.  So as with everything in Android security, when things like this happen, I've just over the years just, I think, become a little jaded and just realized, like, you know, here's yet another thing that's not going to be patched.  Google is going to maybe do some sort of update to address it.  But who's going to get that?  Not many.



STEVE:  Well, yeah.  In fact, our advice for quite some while after this pattern of lack of third-tier manufacturer updates was clear, was if you want an Android phone, get one from a  mainstream supplier, that is, like Google, Samsung, you know, one of the big guys that is taking the maintenance, the aftermarket maintenance seriously.  Because, you know, boy, it's necessary.



JASON:  Yeah.



STEVE:  And this sort of problem isn't found often.  But, boy, I mean, this ticks off every single checkbox of the worst possible nightmare.  It's no action on the user default configuration; no connectivity needed; kernel exploit, the driver is in the kernel so you have full access to everything in the machine.  It doesn't get any worse.



JASON:  Yeah, and I think you're right, like Pixel, Samsung phones.  Say what you will about Google's Pixel lineup, they're not as popular as so many other phones, but they're almost guaranteed to get the updates as soon as they're available.  And to that end, you know, they end up being the safer pick.  But do Pixels have this in them?  Do they have the Realtek WiFi?  That's a great question.  I guess we'll find out.  This is one of those things that we'll keep an eye on over the next couple weeks.



STEVE:  Yup.  Well, and we're glad to have you on the podcast for the next few weeks because you will be the guy to know.  



JASON:  We'll see, yes.  I'll help keep you posted.



STEVE:  So Yubico, the company our listeners know well because I've spoken of them from, like, before anyone knew of them, actually, they've been working toward a local Windows logon solution for quite a while.  It was originally made available in a non-official preview form, as long ago as last March.  It's been available for Mac and Linux machines.  And I am very pleased to announce that Yubico's solution for local hardware dongle-protected login to Windows machines, Win7 through 10, is now available.  In fact, I recently took a look at it.  It was a few months ago.  It was before I took my laptop out of the country for the SQRL World Tour.  It is very clean and simple.  It is now available.



So remember that, to be useful, the hard drive also needs to be encrypted with BitLocker or whatever your preferred encryption tool is, so that the drive cannot be removed from the system and mounted on another system and then just simply read.  So you want to encrypt it.  As long as you do, if you really want strong local security, that is, no active directory support, no enterprise stuff, it's just your Windows-based laptop, your Windows-based desktop, you can now use an inexpensive and very secure YubiKey to make that possible.  And it's a free download.  I have the link in the show notes, on page 10 here of the show notes, if anyone is interested.  But it's also easy to find it just by cruising around Yubico's website and grabbing it.



JASON:  Right on.



STEVE:  Two quick short random miscellany bits.  I am a huge longstanding fan of the Jason Bourne movies.  So I was very curious and hopeful to see what the USA Network had in store for us with last Tuesday's release of the first episode of their new "Treadstone" series, which is set in a time that seems well after the Jason Bourne adventures.  At some point they refer to him in the past tense.  This first season will have 10 episodes, each releasing on a Tuesday.



So this evening, since this is Tuesday, I will be checking out, well, I may actually rewatch the first one again, which I did see last week.  But it jumped around a lot, and it treated me as though I was smarter than I apparently am.  In other words, it left me feeling a bit confused.  But I'm going to watch it again, but I'm probably going to watch the second one.  So the jury's still out, but I wanted to bring it to the attention of any other Jason Bourne fans, since it does look like it may develop into something useful.



And speaking of useful, the long-awaited and very much anticipated second book in Peter Hamilton's newest Salvation trilogy releases one week from today, on October 29th.  As I mentioned, I was exchanging emails with John Slanina in the last couple days because he mentioned he had reread the first one.  I need to reread it, too, because it's been a while.  That's the only problem with Peter Hamilton stuff is that the books come out few and far between.  They are incredibly good, I mean, he is my absolute number one favorite science fiction author, period.  But there just isn't enough of him.



The timing is perfect since I just wrapped up a very enjoyable five-book series.  John, if you're listening, it was the Terran Fleet Command Saga by Tori Harris.  Five books.  They're not Hamilton grade.  But then in my opinion, as I said, nothing else is.  I would recommend the Terran Fleet Command Saga to anyone who enjoys a nice, well-written, slow burn, well-assembled, space opera combat mystery involving some interestingly enigmatic aliens.  The series had many great moments.



And of course all of this is set against the background of Ryk Brown's ambitious and ongoing 75-book Frontiers Saga series that I know John is also reading.  After finishing the Terran Fleet Command Saga, I quickly read book 12 of the second set of 15 which had recently dropped while I was reading the other series, to see how all of the people that we've come to know so well in Ryk Brown's series are faring.  So I'm of course following that.



You know, each of these authors has a very different flavor and style.  And if asked, I guess I would be unable to choose among them.  I've never encountered any other author with Peter Hamilton's storytelling and reality creation talent.  But for the sci-fi content-starved, Ryk Brown's Frontiers Saga, which just is producing wonderful pulp sci-fi, is just so much fun.  So anyway, just for those who are sci-fi enjoyers, and I know many of our listeners count themselves among that set, I wanted to make a note that the second book in the Salvation trilogy is a week from now.  I will start rereading the first one just because it's such an enjoyable read.  And then I'll know that number two is ready.  And then everybody is going to be frustrated while we wait for the conclusion, you know, what, another year probably.  But they're worth the wait.



And speaking of worth the wait, Jason, let's take our third break and then talk about the biometric mess that we find ourselves in.



JASON:  This episode brought to you by Steve Gibson.  We don't have a third sponsor today, Steve.  It's all good.



STEVE:  Oh.  That happened last time.



JASON:  Actually, how about this?  Twit.community.  If you didn't know we have a forum, go to twit.community, and you can talk with other TWiT fans who are on the forum, just basically talking about topics from the shows, talking about pitching in with the hosts of the shows, about the shows themselves.  So twit.community.  That's the ad.  That's the third.



STEVE:  You know, it's funny because Leo is so excited about this.



JASON:  It's really cool.



STEVE:  Well, GRC has had newsgroups forever.  I mean, the GRC newsgroups predate this podcast.  That's how old they are.  And they are.  They're NNTP text-only newsgroups.  But, I mean, so I'm well aware of the power of a true useful community.  And I've heard Leo remarking at how amazed he is at the quality of the people who are there.



JASON:  Oh, yeah.



STEVE:  And although the groups do require moderation because you want to keep bots and ads and despoilers out of them, it is so useful to have them.  All of the SQRL project was done in sight, and also with significant help from many other people in the newsgroup.  And really they're in the process of taking it out of my hands at this point, as they should, because I will soon be returning to SpinRite, and all of that development was being done and will be done, again, in plain sight in GRC's newsgroups.



And of course I recognize that there is a place for web forums, which is why SQRL also has its SQRL web forums at sqrl.grc.com, where sort of a different demographic of people prefer to hang out.  So it's definitely the case that there's a place for this.  So I'm really, really delighted that TWiT got some.  That's really very cool.



JASON:  Yeah, it's been a lot of fun kind of getting in there, and every once in a while I'll get an email, an autofire email from the community saying, hey, someone mentioned you.  It's just nice to have that open conversation, jump in there; and, you know, it's a good time.



STEVE:  Okay.  So the mess of biometrics.  We had two failures in biometric authentication which hit the news this week.  So as I mentioned at the top of the show, I thought this would be an opportune time for us to check in with biometrics to see how it's all going.  Sophos had a story about - well, many people did, but I liked their coverage - about the newly, well, actually it's not even newly released, is it.  It's like the Pixel 4 is supposed to start shipping Thursday.



JASON:  Yeah, it's not even out yet.



STEVE:  In two days.  So it's not even - exactly.  They opened with the rhetorical question, does it matter that Google's Pixel 4 Face Unlock works even if the owner has their eyes closed?  To which most of us, after thinking about it for a second, would answer, uh, yes.



JASON:  Yeah.



STEVE:  That does matter.  Turns out that the Pixel 4 is following in the footsteps of Apple's Face ID technology, similarly dropping fingerprint recognition in favor of optical face recognition.  But Chris Fox, a reporter for the BBC, actually he wasn't the first to - no, no, he was the first to bring this to light.  I was about to confuse this with fingerprint problems we'll get to in a second.  He discovered a potential issue, which is Face Unlock works when the user has his or her eyes closed, like when they're asleep.



It turns out it's not necessary to get Google to confirm this, since it's already on the Pixel 4's help pages.  Google wrote:  "Your phone can also be unlocked by someone else if it's held up to your face, even if your eyes are closed.  Keep your phone in a safe place, like your front pocket or handbag."  Now, what's interesting is that there were some leaked images of the Pixel 4, and I've got one here in the show notes.  It's been noted by the press that the leaked images show an option under Face Unlock which reads, and I'm reading it off of the show notes:  "Require eyes to be open."  And then underneath it says:  "To unlock the phone, your eyes must be open," and there's a switch.  Which, now, what's interesting is that in the leaked screenshot, the switch is off, which seems odd because this seems like an obvious good thing...



JASON:  Totally.



STEVE:  ...for Face Unlock to have.



JASON:  I don't know why you'd have this feature and don't just default to that always on.  Like don't even give the option on that one.  It's not necessary.



STEVE:  I agree.  It's like, okay, now, what is the use case for wanting to unlock your phone when your eyes are closed?  It's a little difficult.



JASON:  Maybe it's faster because it doesn't have to detect that your eyes are closed?  It's just like, oh, we've got all the - I don't know.  That's the only thing I can think of.  Like if you want it just slightly faster.  But who knows if that's true. 



STEVE:  Yeah.



JASON:  Who knows if that's true.



STEVE:  And so the obvious risk is that anybody could get hold of your phone - your kids, your spouse or your partner - and without your knowledge unlock it while you're sleeping or passed out or for whatever reason you're unconscious, and when your eyes are closed, and have access to your phone.  So what's interesting is that this appears to be unintended behavior because what's happened is the devices that were actually released to the press, both the BBC and the Verge, have preview devices, and that option has been removed.  The switch is no longer there.  And it will presumably not be there in two days.  In fact, Google has stated that it plans to fix the issue "within months."  But they have not been more specific.



In the meantime, anyone who is concerned about this, Google recommends using a PIN or an unlock pattern.  In other words, if you're worried that your phone, your brand new Google Pixel 4 with Face Unlock, will not pay attention to your eyes - and apparently the option is gone from the option screen, and I'm guessing it's because it didn't work very well.  That's the only reason I can imagine it was there.  Maybe they hoped to get it working by the time it was released, so it was there initially disabled, maybe, because it wasn't available yet, but they hoped it would be.  And then it was like, okay, we've got to ship this sucker, and it's still not robustly working, so let's take it off the option screen and just tell people not to use it if they're concerned.  So basically at ship, in two days, the phone will, we believe, not have this option.  Google confirms that now.  And if that's a problem, don't use it.



JASON:  Yeah.  And, I mean, this is not something that Google hasn't done before.  Like actually very recently, with the release of Android 10, when that software launched, people noticed right away that third-party launchers could not use the gesture navigation.  And Google had to admit, like, yes, you know, we really wanted gesture navigation to work with third-party launchers.  Lots of people use third-party launchers.  But we just ran out of time and had to go with this.  So at a future date that compatibility will be added.  And actually in the Pixel 4 that compatibility is added.  But this is something that Google will do.  They'll run out of time and - maybe lots of companies do this in different ways.  But Google definitely does.



STEVE:  Ship it anyway, yup.



JASON:  And in the case of security and something like this, you just, like, what?  I don't understand how you have this feature, and you don't have an eyes-open requirement.  At the same time, I mean, and I'm curious to hear your take on this, when it comes to biometrics, and like say I fall asleep, and my phone's next to me, someone could very easily take my phone and put it on my finger for the fingerprint capture, as well; right?



STEVE:  That's true.  That's true.  



JASON:  So that's kind of the same thing.



STEVE:  That's true, yeah.  Well, and speaking of fingerprints, it turns out that the - thank you for the segue, Jason.  It turns out that the sexy ultrasonic finger reader used by Samsung's flagship S10 and Note 10 smartphones can be spoofed with a $3 screen protector.



JASON:  Wah wah wah.



STEVE:  At least one that's not made by Samsung.  This recently came to light, although it was not the first time it was observed.  But it made news when a British woman claimed, and others including Samsung have since confirmed, that after fitting her new phone with a screen protector, she was then able to unlock her S10 using any of her fingerprints, including ones not enrolled in the phone's authentication system.  And since that made her curious, she reportedly then asked her husband to try the same thing, and his thumbprints worked, too.  As did the same trick on her sister's Samsung phone.  So obviously something was wrong.  Samsung's initial response when confronted with this was "We're investigating this internally.  We recommend all customers use Samsung-authorized accessories, specifically designed for Samsung products."



Then last week, in comments to Reuters, Samsung admitted the problem was real and said it would release a software patch.  Now, that's going to be interesting because this seems like a technology problem.  Anyway, they said:  "We are investigating this issue and will be deploying a software patch soon.  We encourage any customers with questions or who need support downloading the latest software to contact us directly."



So, okay.  The issue with the S10 and screen protectors was first noticed when the smartphone was launched back in February of this year, but the issue failed to acquire critical mass.  Unlike earlier designs, which used a dedicated sensor, the Qualcomm ultrasonic technology used by Samsung is embedded behind the screen and uses high-frequency sound, ultrasonics, which are modulated by the pressure of a user's finger against  the screen glass.  It was noticed, however, that covering the screen with a screen protector could in some instances create an air gap that could interfere with the integrity of the system's ultrasonics.  Samsung's advice, which still seems a bit flaky to me, is to use its brand screen protectors that use, they say, special adhesives that eliminate the possibility of any gap.  So to my mind, this whole technology seems sketchy at best.



JASON:  Yeah.



STEVE:  I would say that ultrasonic trans-glass thumbprint reading is not compatible with screen protectors.  That is to say, I mean, I think the technology is marginal.  And like when you put your fingerprint, right, you press your finger directly on the glass.  The idea that you can read an image through the display technology and all of its intervening electronics with some ultrasonic sensor behind there and get an image, to me that's already amazing.  Then the idea that you're going to introduce an additional uncontrollable layer of goo, which is to say adhesive, and then a plastic laminate in between the glass that is being read, to me, you know, I wouldn't be at all surprised if this update actually checks to see if they are able to get an image and refuses to operate if there is no image obtainable.



That is to say, if in fact there's an air gap, then you're not going to actually get anything that looks like a fingerprint.  And so probably the technology that existed just took whatever it was getting and said, okay, you've got a weird thumb, lady, but fine, and then just used it, when in fact it wasn't a thumb, it was an air bubble that it was resolving.  And so anybody else pressing on it got the same air bubble.  So maybe what they're going to do is be a lot more discriminating about does this actually look like a finger or not, and just say no.  And then say buy a real screen protector, or sorry, we're - maybe they'll just decide our technology is not compatible with screen protection.  Good luck.  Get a good case.  I don't know.



JASON:  Yeah, we'll see how that goes.



STEVE:  Yeah.  To me it feels like they've hit a bump, a big bump in their fundamental biometric technology.  And really this sort of speaks to the larger issue that I just sort of wanted to address, which is that we're always talking on this podcast about the tradeoff between convenience and security.  A long, impossible to memorize password is incredibly inconvenient, and incredibly secure, if you otherwise manage it correctly.  If it is really long, and full of gibberish, you know, if it's high entropy, uses all the available characters, that's a really, really - that's really strong protection.  And it can, although it isn't used to identify someone because we can't prevent them from using monkey123, if it actually were, like if it had 256 bits of entropy, it could be used to securely authenticate someone.  I mean, it's that good.



Whereas notice that the biometrics we're using today, they're not identifying someone.  They're confirming a presumed identification.  That's a far lower bar to meet.  For example, time-based tokens, the six digits that change every 30 seconds?  Similarly, they don't identify you.  They only confirm your presumed identity, which again is a much lower bar to meet.  So the biometric systems that we have today, they're making a go/no-go decision.  They assume who it is.  They've been trained on someone's face.  And so they're looking out at the person going, hmm, does this look like Granny or not, and making a go/no-go decision.



So what we're asking for is far lower determination, determinism, than having them look out and go, oh, that's John from down the street, and then loading his account or something.  No.  This is a simple yes/no decision being made.  And even so, it is very problematic.  Do you know, Jason - and I didn't track the technology in the Pixel 4.  We know, because I was following it when Apple came out with their Face ID, all this fancy 3D camera stuff, how they paint a dot grid, an IR dot grid out on the person's face in order to require it to be 3D, and then they do a full multiple stereo vision rendering thing.  Does the Pixel 4 have all of that?



JASON:  Yeah.  The Pixel 4 is the first Pixel to do that, actually.  And I think that was part of, I mean, from what I understand, that was part of the reason why Google got rid of the fingerprint sensor altogether, because they felt confident enough in that collection.  So it's very similar to Apple's approach.  It sounds like one of the main things that's missing from the Pixel 4 in the Apple approach is the eyes-open detection, which is a pretty [crosstalk].  So, yeah, it's similar.



STEVE:  Yeah.  And, you know, you would not think, given all the awesome technology that Google has in that phone, I mean, I watched their presentation.  It was like, holy crap.  I mean there's a lot of stuff in there.  You'd think it could tell whether someone's eyes are open or not.  I mean, a dog can.



JASON:  Well, yeah.  And what's interesting also, but maybe this doesn't have anything to do with the eyes, is there's another setting in Android settings called "screen aware" or something like that.  And it's a setting that, when it's activated, it can tell when you're looking at your phone in order to keep the display from timing out.  But now that I say that, like it might just be that you're facing the phone, not that your eyes are open.  If your eyes are open, then it's like, okay, you've kind of already got the detection going on.  



STEVE:  And that sounds like that was a pre-Pixel 4 thing, too; right?



JASON:  Yeah.  



STEVE:  The phones have had that for a while.



JASON:  Some phones have had that.  I don't know that the Pixel has had that, though.



STEVE:  Oh, okay.



JASON:  Yeah, I think that's a new feature of the Pixel.



STEVE:  Interesting, yeah.



JASON:  Yeah, I wouldn't be surprised if, in light of this, with the ultrasonic fingerprint sensor allowing anyone's fingerprint to let someone in based on the screen protector that's installed, if Samsung then ends up going, and maybe they're already working on this, going in the same route as Apple and the same route as Google with the Pixel 4, and this just becomes another reason why they then get rid of the fingerprint sensor and go for the face.



STEVE:  Do I remember Leo saying that he was not happy with the performance of the S10's sensor?  Does it take a while to operate?



JASON:  Yes.  Yeah, yeah.  I think Leo and I share the same opinion.  And it wasn't just the S10 in display.  Well, no, sorry, it was the Note 10 and the S10 both have the in-display fingerprint sensor.  And it's the ultrasonic, of course, different from what we've seen a lot of other in-display fingerprint sensors be, which was optical.  And like the OnePlus 7T and 7 Pro, they have an optical in-display fingerprint sensor that is super responsive and super quick and works almost every time.  The S10 and the Note 10, in my experience, and I think Leo would agree, is just really, like it works half the time.  It's hard to get it just right.  You do it, and it's like, press a little harder, press a little - it doesn't work nearly as effectively.



STEVE:  Interesting.



JASON:  Even though it's supposedly the better, more secure solution.



STEVE:  Yeah.  So you are able to do an in-screen optical sensor.  That's cool.  I didn't know, I didn't realize that there were those around.  I agree.  That would be great.  But again, everybody's got a camera already looking out the front of the phone.  The problem, of course, is that it's 2D, and you absolutely need to have 3D in order to up the ante and try to determine whether this is a live or static person.



JASON:  Right, right.



STEVE:  So everybody's being - and who knows.  I mean, we've already seen 3D modeling of faces and all kinds of other stuff that you're able to do once you've got 3D camera technology looking out at you.  So it just sort of seems like that's the direction everyone's going to go in.  And of course the other problem that we didn't talk about, but we have in the past when we're discussing biometrics, is a super, super complex long password, which can be used to uniquely identify you and is incredibly robust and secure, can be changed.  You cannot change your face.  You cannot change your fingerprints.  So on the one hand, your face and your fingerprint are something you always have with you.  The problem is they're something you always have with you.  And, like, for life, rather than just for your current excursion from the house.



JASON:  I always tend to fall back on multiple points of authentication.



STEVE:  Yes.



JASON:  Like I've always wondered why, if the fingerprint sensor on a phone is so ubiquitous - and now we're kind of heading out of that trend, it seems like, getting rid of it.  But if it's so ubiquitous, why can't I have it so that I'm using my fingerprint sensor when I enter my PIN, and those combine to say, hey, he knows the right digits, and his fingerprint is the right one, this is the right person, if people really want to be just that extra level secure.  Or fingerprint sensor along with face scanning.  Why can't they both combine?  And I feel like, now that I'm saying it, that we've talked about it before, and one of the points that maybe you mentioned is, if one of those things is out of whack, it makes it incredibly difficult for you to ever get into your device.  Maybe that was you.  I can't remember.  Maybe it was someone else.



STEVE:  Yeah.  In fact, I do remember you and I talking about that, and that's the other problem is that we've got multiple factors, and they're all a little bit soft.  So you have, on one hand, you don't want a false positive, so you don't want to allow somebody in who should not get in.  Nor do you want a false negative.  You don't want to prevent the authorized user from getting in when they want to get in.  I mean, that's very frustrating.  I remember, I think it was the very first - was it the very first Touch ID?  Something that Apple did just wasn't working that well when they first released it, and it was like [grunting].  I think it was the very first Touch ID.  Or maybe I later figured out how to over train it, and I think that's what it was.  I was able to over train it and then bring its reliability up where I wanted it to be.



But again, because it's a heuristic system that's inherently making a judgment call, does this look enough like the person I've seen before, because of course our faces are changing subtly over time.  Our hands pick up damage from the environment, cuts and scrapes and things sometimes.  So it's am I sure enough about who this is?  But if it was a super high-entropy password, it's not a matter of being sure enough.  It's like, holy crap, that's got to be the person because nobody else could get this...



JASON:  You either know it or you don't.



STEVE:  Yes, could get this right.



JASON:  Right.



STEVE:  And so we're dealing with the more soft decisions we make, the softer the overall compound decision becomes.  And that means that we become more likely to false positive or false negative; and that doesn't help anybody, either.  So really it seems very clear that biometrics is the future.  People want the convenience.  The only thing I would suggest to people, and we've said this on this podcast before, is give your phone to other people.  Stick your phone in other people's faces.  See if it unlocks for them.  Have other people challenge your fingerprint reader.  Develop some a priori real-world experience with how robust this is.  I mean, I don't think enough people do that.



JASON:  Most people don't.



STEVE:  Nobody asks someone else to, like, try guessing my password on my phone.  I mean, that's just, you know, we know that's not going to happen.  But it'd be really interesting to get some actual sense for whether you know other people whose faces will open your phone.



JASON:  Yeah, absolutely.  I completely agree.  That's a very interesting exercise.  I never thought to give my phone to someone, be like, hey, see if you can get into it.



STEVE:  We just assume.  We just assume.  We just assume.



JASON:  And all that was needed was a $3 screen protector, and your fingerprint could open my phone, too.  That's just creepy.



STEVE:  Yes, and in fact last April the Naked Security website reported that a Nokia 9's PureView fingerprint reader was fooled by a chewing gum packet.



JASON:  But you had to know which chewing gum packet.  [Crosstalk] security; right?



STEVE:  That's got to be Wrigley's double something.



JASON:  Got to be Wrigley's, yes.



STEVE:  Yes.



JASON:  Hey, before we go, I want to make sure that we get in here the chance to plug your SQRL event.  It's coming up here in the studio; right?



STEVE:  Oh, yes, yes, yes, that's a good point, because Leo's not back before that, and we do want people to know.  It'll be on Saturday, November 30th.  I'm going to be up because I want to just make one final recording of my - basically the SQRL story is what I call it, sort of what happened, what occurred to me, what ensued, what SQRL is in a lot of detail.  So it's not a user-facing presentation.  It's meant to explain SQRL's technology for our audience, for the audience of this podcast.  And so I've asked Leo if he would make the TWiT studio available for that purpose, and he and his staff have graciously agreed to do so.  So that'll be the afternoon of November 30th, in about four weeks.



JASON:  Awesome.  Yeah, and I think I have down here 2:15 p.m. Pacific.



STEVE:  Yup, yup.  Very cool.



JASON:  I'm sure you'll hear a lot more about that.  I'll make sure that we kind of get the plug in throughout the next month.



STEVE:  Yes.  And it is by appointment only.



JASON:  Okay.



STEVE:  So you absolutely - because there isn't that much room there.



JASON:  Right, no.



STEVE:  And so you've got to, you know, if you're sure you can come, please don't make an appointment and then be a no-show because that would just be annoying to all the people who would like to come, but weren't able to.  So if you're sure you can be there, then we'd love to have you.



JASON:  Cool.  Should they, I mean, normally we would send people to tickets@twit.tv.



STEVE:  Yes.



JASON:  Okay.



STEVE:  I think it's already been created.



JASON:  All right, great.  So November 30th, SQRL.  That's a Saturday, 2:15 p.m. Pacific.  Email tickets@twit.tv if you are certain you can make it for that.  That way we're not holding one of those seats from someone who could make it, and you end up not going, and everybody's sad.  So don't do that.  



STEVE:  Perfect.  Perfect.



JASON:  Don't make everybody sad.  Steve, awesome stuff.  What a great show, as always.  People who want to check in on everything that Steve is up to can go to GRC.com.  That's where you're going to find all the information about SpinRite, of course.  You can get your copy there.  Information about SQRL, if you want to find out a little bit more about what SQRL is all about leading up to November 30th, as well as audio and video of this show, can be found at GRC.com, and transcripts only found there.  So if you want a transcript of this show and all other episodes of Security Now!, you can find them:  GRC.com.



Our website is TWiT.tv/sn for Security Now!.  There you can also of course find all of our podcast episodes, audio and video; subscribe links for every single place you might want to go.  If you prefer YouTube or Pocket Casts or Apple Podcasts, wherever you get your podcasts, you'll find direct links out to those so you can subscribe very easily and have the show delivered to you automatically so you don't have to think about going to the site to play it from there.  And you can watch us record the show live every Tuesday, 1:30 p.m. Pacific, 4:30 p.m. Eastern, 20:30 UTC.  I don't think that's changed quite yet, but that's at TWiT.tv/live.



Steve, this was a lot of fun.  Thank you for having me along on this ride.  Really appreciate it.  And we'll see you next week on Security Now!.



STEVE:  Thanks, Jason.



Copyright (c) 2019 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.










GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#738

DATE:		October 29, 2019

TITLE:		A Foregone Conclusion

HOSTS:	Steve Gibson & Jason Howell

SOURCE:	https://media.grc.com/sn/sn-738.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we look at another collision created by third-party AV; a powerful new Windows Defender feature that's easy to have missed; a public database breach by someone who should know better; what's worse than having all your files encrypted?; a VERY nice-looking, fully encrypted and free email service engineered in privacy-respecting Germany; stats coming back from Firefox's newly enhanced tracking privacy protection; a new and very bad remote code execution vulnerability affecting Nginx web servers; and the planned introduction of RCS to replace SMS next year.  We also have a piece of SQRL news and some miscellany.  Then we look at the outcome of a recent appellate court decision which complicates the decision about whether using a password or a biometric is more "judgment proof."



SHOW TEASE:  It's time for Security Now! with Steve Gibson.  I'm Jason Howell, filling in for Leo Laporte one week more, and actually a couple more weeks coming up.  So I'm with you in the long haul.  Steve's got a bunch of stuff to talk about this week:  a new powerful feature in Windows Defender.  Asks the question, what's worse than having all your files encrypted?  Takes a look at the new replacement for SMS, that's RCS that's coming sometime next year.  And the big story, taking a deeper look at this thing we take for granted, that our PIN or our password cannot be compelled by a court of law.  It's a foregone conclusion that Steve details next on Security Now!.



JASON HOWELL:  This is Security Now! with Steve Gibson, Episode 738, recorded Tuesday, October 29th, 2019:  A Foregone Conclusion.



It's time for Security Now! with Steve Gibson.  I'm Jason, filling in for Leo, who is still out enjoying himself.  And I'm going to enjoy myself joining you, Steve.  How are you doing today?



STEVE GIBSON:  Great, Jason.  Great to be with you again.



JASON:  Great to be with you.



STEVE:  Happy to see that the lights are on in Petaluma.



JASON:  Yeah.



STEVE:  There's been some concern.  I guess our listeners away from California are aware that there have been, like, crazy major fires all over California.



JASON:  Yeah.



STEVE:  And not even only in the north.  Now there's something that's endangering the Getty, I guess, and a bunch of celebs had to get evacuated from their homes in Malibu over the night.



JASON:  The state is on fire, essentially.  California as a state is on fire.  Yeah, you know, and it's really - it's just kind of like when - like Petaluma, thankfully, is not part of all the evacuations that have happened, at least in the North Bay area.  But a lot of towns and cities north of us have had to evacuate over the past couple of days.  And it's just, you know, it's sad.  I go out traveling to another part of Petaluma, and I just see these areas that are normally empty, and they're full of RVs, people who are just kind of like camping out, waiting for the signal to be able to return to their homes.  It's just got to be so stressful, so bad.



STEVE:  Well, that sort of sounds like Palo Alto, where there's a major RV problem because you can't get housing in Palo Alto.  So they're just lining the streets with RVs.



JASON:  Right.



STEVE:  And of course then all of their waste disposal is a problem.  And, I mean, it's just created a mess.  Anyway, we are Episode 738 for this last podcast of October.  What is this, I guess it's two days before Halloween.



JASON:  This is our Halloween edition.  Lots of scary news today.



STEVE:  So, well, this one is called "A Foregone Conclusion," which is an interesting phrase that was used by the prosecutor in an Oregon case that wanted to compel a 20 - she was 27 or 29, I can't remember, but a young adult woman who got into a car accident while under the influence of methamphetamine - and it involved six other people in this accident, so it wasn't nothing - to compel her to unlock her iPhone.



And what's interesting is the logical reasoning behind the decision that the appellate court in Oregon upheld, which changes the game.  We've talked often about, just sort of from a theoretical standpoint, not that any of us listening to this podcast are actually going to have to worry about the police or law enforcement compelling us to unlock any of our devices.  But as a matter of curiosity, we've wondered in the past whether you're better off using your fingerprint or another biometric like your face, or that being something that you are versus something that you know.  So anyway, we're going to talk about that.  Thus the podcast is called "A Foregone Conclusion," which, again, is part of the legal reasoning behind this interesting decision.



We're also going to talk, however, first about another collision created by third-party antivirus software.  A powerful new Windows Defender feature that many of our listeners may have missed, and I wasn't aware of it until I encountered it, so I wanted to put it on everybody's radar.  We've got a new public database breach by an organization who should know better.  We sort of rhetorically ask the question, "What's worse than having all of your files encrypted?"  And this actually happened to Johannesburg, South Africa recently.



We've got a very nice-looking, fully encrypted, free email service engineered in the privacy-respecting Germany that I wanted to also make our listeners aware of.  And that actually was a result of something that came out of the Johannesburg attack.  We've got stats now coming back from Firefox's newly enhanced privacy, well, newly enhanced anti-tracking privacy protection.  So we're getting a sense for how effective that is.  A new and very bad remote code execution vulnerability affecting Nginx web servers.  That's one of the more popular new web servers on the Internet.  It's got 30-plus percent adoption, and new sites are often using it.  Anyway, it's got a big problem with the way it's invoking PHP.



We also have the planned introduction of RCS, which is slated maybe finally next year, in 2020, to finally replace SMS as sort of the base cellular provider, interphone, non-speech protocol and communications standard.  So we've got to talk about that.  We also have a piece of SQRL news; a bit of miscellany; and then, as I said, we're going to take a look at the outcome of this interesting recent appellate court decision which sort of changes the game a little bit in the decision, if I'm wanting to lock my phone for maximum protection, which course of action, which phone-locking technology do I use?  So I think another great podcast for our listeners.



JASON:  Absolutely.  And maybe the answer is it doesn't matter what you use to lock your phone.  There's always a way, apparently.  Even the things you thought were foolproof or whatever will not be.  Who the heck knows?  We're going to talk all about that.  All right.  Starting off with a little SpinRite, right at the top; right?



STEVE:  Yeah, but I first want to just talk about this.  I'm 100% bullish on the idea of synthetic credit cards.  We've talked about it a number of times on the podcast through the years.  There are a couple of card issuers that make that available.  None of mine do, unfortunately, or I would be an avid user of that service.  I just, and I think maybe once upon a time, maybe it was in the early days of PayPal, I remember once having access to it.  And I just, you know, I've had so many of my credit cards through the years compromised, where you get the security alert, and somebody in another state just bought luggage, and it's like, whoa, wait a minute, slow down.



And so as a consequence, one of the reasons I route so many of my purchases through Amazon, aside from the convenience, is that I'm not having to give random websites spread all over the Internet my credit card information.  And where I can, I will use PayPal, if a site offers that.  I mean, I will always use it because it's just been such a hassle.  In fact, right now I'm using a card  that is new relative to the one I had for years because I got fraudulent purchases or attempted charges on the card that I had had for years.



And I've told the story on the podcast a number of times that normally, back also sort of pre-Internet, or in the early days of the Internet, I worked with a travel agent who I would just use because it was more convenient, even after the Internet.  And I would annually go north for the holidays to visit my family.  And it became routine for my travel agent, who had gotten to know me, to start off our conversation with, "Okay, first of all, Steve, do you still have the same credit card that you did last year?"  The point was that I was, you know, often I wouldn't because there would have been a fraudulent charge, and I would have had to change the number, blah blah blah.



Anyway, so this is unsolicited endorsement of the idea of Privacy.com.  I'm not saying this.  Our listeners know because I've always been bullish on the idea of creating a synthetic credit card for a specific purpose.  And I am a little anti "sign up and get a subscription," like one more thing that I have to subscribe to.  So I like the idea that they have a free tier, which is probably where I would be because it sounds like it would meet - it's not like I'm having to do this all the time.  But there are occasions where I don't have a choice.  There's something I want.  They don't offer PayPal.  I can only get it from some random site.  And they're not using a major credit card clearing house.  They're just like, oh, like, trust us.  And it's like, I don't know.



So sounds like these guys are a perfect go-between, where you can say this is the vendor.  I'm going to give them this much money.  Let this one go through and then kill this number so it can't ever be used again.  So anyway, yay.  I'm delighted to have an outfit like this as a sponsor for the TWiT Network.  I just think it sounds great.  So there.



JASON:  I completely agree.  I love the idea, especially when you talk about kind of like comparing it to password protection online and having a different password for every site.  That way on the other side, if something is compromised, that one password, if it's ever discovered, it can't be applied to anything else.  It's totally kind of a direct kind of comparison to the idea of having a different card for every service that you sign up for.  I mean, they're very similar.  



STEVE:  Well, actually, I would argue that if - I think you said "single use."  And so that's more like having a one-time password.



JASON:  Yeah, true.



STEVE:  That's even stronger than a password per site.  You're saying I want you to authorize - you ask Privacy.com for a single-use card number.  That's, I think, where they're getting this 12-per-month deal.  And so you then give that to flaky XYZ site.



JASON:  Dot com.  



STEVE:  Dot com, exactly.  And they're able to, you know, process the transaction.  And, now, I don't know if there's like a per-card use fee.  I mean, they're doing something in order to - maybe they're just presuming you're going to want to go over the free limit of 12 per month.  Anyway, I'm going to find out because I'm definitely signing up for this.  This sounds like the kind of thing I would like to have available for those instances where it's a perfect match of my desire not to give out my cherished credit card, at the risk of losing it again, out to the wild.  



JASON:  Absolutely.



STEVE:  So anyway, yes, as you were kind enough to say, we have a Picture of the Week.



JASON:  Yes.



STEVE:  And this came to me, this was tweeted by a listener who said, well, I'm finally going to honor my longstanding promise to you to purchase a copy of SpinRite.  And, okay, I don't know exactly what he's referring to, but thank you.  And what's cool is he immediately got his money's worth.  I mean, this is like - this is the classic perfect case for the thing that SpinRite does that nothing else does, which is here is a drive which, I mean, it's a good thing he didn't wait any longer.  It is covered with those - and this is something that SpinRite users will recognize, the green R's which says there was a spot where the initial attempt to simply, lazily, casually read the data failed.  It came back no.



And so SpinRite said, huh, okay, and decided to, as it does, to give that spot on the hard drive its full laser-focus attention.  And it arranged to change the drive's mind, essentially, to perform a full perfect recovery of the data that was in that sector that previously refused to be read.  It got it all back.  And then it went on to the next one.  And as we can see, what is that [counting to 26].  Okay, so at least 27 sectors.  However, as is often the case, and you can sort of see how they're clumped up.  You often have regions which will be bad.  So there may well have been multiple actual sector recoveries which all collapsed to within a single one of those characters.  On one of the other SpinRite screens, you're able to get a complete detailed count.  And of course the SpinRite log logs each and every sector that it recovers and where it was on the drive.



So this sort of is a summary screen that gives you the 10,000-foot view of how the whole drive looks.  But anyway, this was just such a perfect example.  And I thank, first of all, our listener for following through with the commitment that he felt that he had with me.  It may have been someone that said he really desperately needed a copy some time ago, and he would buy it later once he had the money, and could he get one - I mean, I do that sort of thing, if someone is in real trouble or who knows what.  But in any event, I just thought this was a really nice example of, like, it doesn't get any better than that.  He definitely needed to run SpinRite.



JASON:  No kidding.



STEVE:  And it's probably good he didn't wait any longer.  



JASON:  No kidding.  Yeah, that's like the sign of success right there, all R's.  Love it.



STEVE:  Love it.



JASON:  And that's got to feel good from your perspective, to create something that is so capable of basically saving people's butts when it comes to their own data.  That's awesome.  Time and time again.



STEVE:  It does really feel good.  And, you know, and we've shared the stories of someone's wife was a listener, I mean, a guy was a listener, his wife retired and finally decided to pick up her long-on-the-backburner dissertation for her Ph.D.  She got it all done on a laptop, which then crashed.  And it was like years of work that of course had not been backed up, blah blah blah blah.  And so he whips out a copy of SpinRite and saves the day for his wife.  So, you know, those are just really fun, fun stories to share.  



JASON:  That's great.



STEVE:  Not so fun is the growing problem that we are seeing with third-party antivirus, at least on Windows.  And we've talked about the cause of this.  The cause is that today's AV, in order to continue to demonstrate its value to its customers, really has to hook its claws deeply into Windows.  Microsoft now is arguably competing.  We're seeing instances where third-party AV is causing Microsoft trouble.  It's always been sort of controversial because, in order to get the access that antivirus software needs, it needs essentially to kind of be a rootkit.  It needs to go in and make modifications to the underlying kernel level code in order to essentially insert taps into Windows for the things it wants to do.  Well, the problem is, you know, I said it was rootkit-like.  Well, Windows is trying to prevent rootkits.  So, I mean, there really is a fundamental  schism here which we're now increasingly reporting.  And it happened again last Tuesday, a week ago, in an interesting way.



Chrome, Google's Chrome, began updating itself to its most recent release, 78.  And there were a bunch of features there in addition to its 37 welcome security fixes, its experimental support for the increasingly controversial DoH, you know, DNS over HTTPS, which may be our topic for next week because, I mean, it's worth continuing to keep our eye on this because it offers benefits, but it is upsetting people.  Also they've added page tab mouse hover dropdowns and a bunch of other goodies that may require some manual enabling on the part of users.



In addition to all that, an increasingly growing subset of users immediately began receiving Chrome's equivalent of the Windows Blue Screen of Death.  In the case of Chrome, what you get is the "Aw, Snap!  Something went wrong while displaying this webpage" message.  I have a picture of it in the show notes.  I've encountered it.  Not often, but occasionally.  In this case, many people began reporting that they were experiencing this error whenever they started the browser.  I mean, basically something about Chrome's update to 78 caused it to stop functioning under Windows.  Well, the trouble was quickly tracked down to a collision between something that Chrome was doing and systems running an older version of Symantec's Endpoint Protection (SEP), which is the enterprise-oriented security suite that Symantec makes.



It turns out that an outdated version has an incompatibility with a newly added feature in this most recent release of Chrome.  And because Microsoft's own Chromium version of Microsoft Edge uses the same core, Edge had the same problem.  The collision was the result of Microsoft's new Code Integrity feature, which checks the drivers and system files for signs of unauthorized tampering.  And as I said, that's what AV is increasingly needing to do in order to continue to prove its relevance.



This new feature was added in Windows 10 1803, and it since has been rebranded WDAC, which is Windows Defender Application Control.  With that rebranding came an extension of its protection to include all applications running on Windows.  So essentially it's a whitelisting system, and the default is pretty much nothing is allowed to run.  I mean, it's breathtaking.  I mean, even Microsoft's own programs will not run unless they are explicitly whitelisted.  There is now you're able to back it off one level to accept all the things that ship with Windows plus some of Microsoft's optional things like Office that don't ship with Windows.



But, I mean, this thing is really squeezing down tight.  And as a consequence, it's a pain in the butt for many people to use.  I mean, like it's going off all the time because it turns out, gee, that people actually want to run code that Microsoft didn't write themselves and sign.  So anyway, it is a whitelisting system where enterprises that want strict control over what they permit their users to run have been asking for.  The flipside is be careful what you ask for because, boy, you know, it is really tough to use.



So it turns out that Chrome turned this on and began using it for their own protection, and Symantec's Endpoint Protection apparently ran afoul of it.  Server 2016 and Windows 10 RS1 are unable to have this fixed.  So those two Windows editions will need to wait until Tuesday, November 12th, when Symantec will be fixing this.  Existing Symantec Endpoint Protection customers not using Windows Server 2016 or Win10 RS1 can fix this by updating their outdated versions of Symantec's Endpoint Protection to the current version, if that's feasible.  One wonders why somebody with Symantec's Endpoint Protection hasn't been keeping current.  But whatever.  If you update, then you're okay.



The other workaround for people for whom it may be unfeasible to update Symantec's Endpoint Protection is to disable that feature in Chrome, which you can do.  If you edit the shortcut that you use to launch Chrome, you can add - and I have it in the show notes, the details.  I'm sure you could probably google at this point.  Well, actually I tried it, and it didn't come up very easily.  The switch is -disable-features=RendererCodeIntegrity.  Again, it's in the show notes.  But the problem is that would be, and that's been what's recommended on the 'Net.  The problem is that edits the link or the shortcut that you use to launch Chrome.  If you ever click a link to launch Chrome, then it's not going to be using that shortcut.



So what I would recommend, although do so at your own risk because it's a registry edit, I also have in the show notes a DWORD which can be added to the registry under HKLM\Software\Policies\Google\Chrome.  And then you add a DWORD.  You add a registry entry of DWORD, 32-bit DWORD type named RendererCodeIntegrityEnabled.  You thus set that to zero in order to globally cause Chrome to disable that feature.  Then it will be compatible with the outdated version of Symantec's Endpoint Protection, and you'll be able to continue to use Chrome.



Once again, none of this is a problem if you just use Windows Defender, which is now rated pretty highly and works well in Windows.  I get it that, if an enterprise has a Symantec Endpoint Protection license, that's expensive, and it's bought and paid for, and they want to use it, okay.  When Leo and I talk to people now we just say just use Windows Defender.  It's pretty good.



And recent comparisons of it versus other AVs, as I recall, the last time we talked about this and looked at it - it was a few months back - it tended to have a slightly higher false positive rate.  On the other hand, I've never had it give me a false positive.  And in fact, when I've let it look at a drive from the past, where I was deliberately having some viral samples off on a carefully labeled and sequestered directory, it's gone berserk.  So I know that it sees things that are problems.  I've never had it give me a problem when it shouldn't.  So I like it, and it's what I use.



And so I will mention a Windows Defender feature that I just discovered as a consequence of digging around.  It turns out Windows Defender has an Offline Scan feature which was one of these things where - it annoys me with Windows 10 that Microsoft is not leaving it alone because it's inherent in something this big that you're going to break things when you constantly mess with it.  And so Windows 10 is now never having a chance to stabilize.  It's never having a chance to settle because Microsoft's new idea is, oh, Windows as a Service.  And so we're going to keep making it better.  It's like, no, stop making it better.  Leave it alone.  Please, let it have a chance to calm down.  But no, that's not the model we're in today.  We're going to keep giving you new things, whether you want them or not, and in the meantime breaking everything that used to work.  And so we can patch them, and that'll, I don't know, keep you updating.



Anyway, the point is Windows Defender has always had its traditional Quick Scan, the Full Scan, and then the Custom Scan.  It recently received the Offline Scan as a fourth scanning mode.  The reason this is significant is, and we've talked about the insidious nature of rootkits, is that it is possible that once Windows is running, and a sufficiently clever rootkit has sunk its hooks deeply into Windows, that even Defender, trying as it might, would not see something malicious on the machine.



The whole point of a rootkit is that you are using the operating system to look at the contents of your hard drives.  Which is to say you are viewing your hard drives through the OS, through the API that the OS has given you.  So what a rootkit does is hook those APIs in the OS kernel itself so that when you say "Give me a list of all the files on the directory," the malicious ones are edited out of the list that is returned.  So then you go, great, I'm going to scan those.  Well, great, but you didn't scan the malicious ones because you couldn't see them.  You don't even know they're there.



So the point is Windows Defender now has an easily accessible Offline Scan.  You're able to access it through the regular UI, the Windows Defender UI.  There is now, I think it was, yeah, it's Scan Options which appears when you go through the Control Panel to Virus and Threat Detection.  There's Windows Defender, and there's a Scan button.  Below that it says Scan Options.  A new one has just appeared, Offline Scan.  And when you do that, I did it last night so that I could see for myself how it works, you are warned that the system is going to restart and shut down, and you need to save your work.  And so yeah, yeah, yeah.  And you do that.



And then the system reboots into the Windows Recovery environment, which lives on its own little partition.  It's sort of a minified Windows which is used for performing maintenance tasks.  And I have in the show notes a picture of the dialog that came up while it was running the offline scan.  It's minimal, and it's a little text window, says:  "Your PC is being scanned.  This might take a while.  When it's done, your PC will restart."  And so then it shows when I started it in the show notes.  It's at 5:35 p.m., and it had scanned 3420 items and was just about one quarter of the way through.  It didn't find anything, happily, on the VM version of Windows 10 that I had running.  I didn't expect it to.



But anyway, I just thought it was interesting.  And here I am, complaining on one hand that Microsoft is changing things all the time, and at the same time saying, oh, look, we got a really great new feature.  So, okay.  I guess this is a great new feature.  Certainly it can be.  And so I wanted to just bring it to our listeners' attention that, you know, it's not the kind of thing you want to do all the time, but I would argue that an occasional full offline scan with Windows Defender, you know, you want to make sure that it's got the updated signatures, so update the signatures first.  And then I would say, you know, maybe once a quarter?  I mean, the point is you would always expect it to find nothing.  If it ever did not find nothing, you would be really glad you ran that scan.



And the point is that the other thing that Microsoft talks about is they're less interested in talking about the fact that they may not be able to see something.  I think that's the big advantage.  But they also talk about how you may not be able to remove a rootkit while it's in place.  That is, you just - there's no way to get rid of it because the nature of the rootkit defends itself.  So if you are not booting Windows, if you are scanning a non-running Windows partition, then you find the rootkit before it's ever hooked into the OS because the OS isn't booted.  And it makes it much easier, first, to find it; but second, to kill it and remove it from the system so that it never has a chance to sink its hooks in in the first place.  So anyway, I do think it's a useful additional feature.



JASON:  Is there any way to schedule that?  I mean, I realize it doesn't have to happen very often.  But I know at least for myself, if I'm not scheduling something like that, especially like once in a quarter or whatever...



STEVE:  You know, that's a really good question, Jason.  I know that they're - and I don't know if I can bring up - let me see while we're talking about it because I probably have that on this version of Windows 10 that I'm talking to you on.



JASON:  Yeah, LawnDog in chat says it's not available as a schedule right now.



STEVE:  Yeah.  I guess I'd be surprised if it were.



JASON:  I just know for me, if I don't schedule things like that, my chances of remembering three months later are slim to none.  I suppose you can set yourself a reminder, but...



STEVE:  Yeah, and again, it's not the kind of thing that I think you need to worry about at that level.  It's sort of like, when it occurs to you, like hey, you know, I haven't done an offline scan for a while, and then sort of like, yeah, it's probably worth doing.



JASON:  Yeah, right, take the time.



STEVE:  So Scan Options.  I think that the traditional Defender UI did have a scheduling feature, but I'm probably thinking of it under Windows.  Check for Updates.  Ransomware Protection.  Allowed Threats.  Scan Options.  Yeah, there's Quick Scan, Full Scan, Custom Scan, and Windows Defender Offline Scan.  So, yeah.



JASON:  All right.



STEVE:  I'm not seeing - but again, it's not like it needs to be done on an absolute, oh, my god, don't forget to do this.  It's just sort of like, hey, when it occurs to you, and you're going to go have dinner or something, then start an offline scan while you're off doing something else and just give it a clean look at your system.  Seems like it's worth doing.



JASON:  Like it.



STEVE:  So I titled this "All Your Database Are Belong to Us," which will be reminiscent to those listeners of ours who remember "All your base are belong to us" that was this weird Internet meme that happened years ago.



JASON:  Oh, yes.



STEVE:  So even before I was skipping over the news, as I do now, of the ransomware attack du jour because I sort of like - we got a little crazy this summer with all of the pre-school startup ransomware attacks that were nuts pretty much through the Southeast of the country.  I had already been skipping over the news of the publicly exposed database du jour because that's another thing which actually preceded the ransomware attacks is that this or that company would be found to have their database publicly exposed on the Internet.  And it's like, what?  How does this happen?  I just - you've got to wonder.



And so, yes, we've talked about plenty of those in the past.  But even so, when it happens to Adobe, such a major player, it still seems worthy of dishonorable mention.  A data hunter whom we've spoken of in the past, by the name of Bob Diachenko, discovered a wide-open public and unsecured database belonging to Adobe, Saturday before last, on October 19th.  The database contained the email addresses and other Adobe-specific information of nearly 7.5 million customers of Adobe's Creative Cloud.  The database included the account creation date, the products that they're using, the status of their subscription, whether or not this is an Adobe employee, the member's ID, the country they are located in, how long it's been since their last login, and the payment status of their Creative Cloud account.



So it turns out that information affects approximately half of the believed size of Adobe's Creative Cloud customer database.  However, what was not exposed this time was any obviously compromising information such as their passwords or their credit card numbers, payment information.  On the other hand, what was exposed is exactly the sort of inside information - member IDs, products used, subscription status and so forth - that can be leveraged to make phishing emails far more believable and can therefore be used to induce users to download and run malware in a phishing campaign.



So once upon a time there was much less mischief that miscreants could get up to with this sort of data breach.  Those times are long gone, and we see successful phishing campaigns leveraging exactly this kind of disclosure.  And the bigger question we must ask here is how does this happen to an organization such as Adobe where security must be like an out-in-the-front requirement for any sort of work like this.  How does a database containing 7.5 million of Adobe's customers get placed by mistake out onto the public Internet?



In its security updated dated last Friday, October 25th, Adobe posted this.  They said of this breach:  "At Adobe we believe transparency with our customers is important.  As such, we wanted to share a security update," which is the way they couch this.  They said:  "Late last week, Adobe became aware of a vulnerability related to work on one of our prototype environments.  We promptly shut down the misconfigured environment" - and by the way, they did, like immediately, within hours of being notified, to their credit.  They said:  "...addressing the vulnerability.  The environment contained Creative Cloud customer information, including email addresses, but did not include any passwords or financial information.  This issue was not connected to, nor did it affect, the operation of any Adobe core products or services."  Okay, that's technically true.



"We are reviewing our development processes to help prevent a similar issue from occurring in the future.  Should you have any questions, we encourage you to contact us at," and then the generic helpx.adobe.com/contact.html.  And of course note that their transparency was not really optional since the news of this breach was widely published the same day.  So to their credit, Adobe did take the exposed data offline immediately upon being notified of its exposure on October 19th.  But they say that they're reviewing their development processes to help prevent similar issues in the future.



So one would have hoped that this was done after their very similar October 2013 breach, which impacted at least 38 million of their users, three million of whom had their encrypted credit cards and login credentials exposed.  So maybe the previous security review wore off, and it was time to have another one so that they can figure out, again, how to keep this from happening in the future.  I don't know.



But anyway, the problem is, as I said, these days, even though the information was not massively instantly exploitable, like login information, financial information and so forth, the big threat now is phishing attacks.  That's the way users are induced to clicking on email.  And in that database was a flag, if you were an Adobe employee.  So for a phisher, that says, ooh, let's send a very convincing-looking phishing email containing information that only we at Adobe would have to our own employee, inducing them to click something in order to, who knows what.  You could easily see that that kind of email would be far more likely to induce someone in Adobe to click a link and then get malware introduced into Adobe's network.  So, yeah, it's not at all clear that these are - that despite the fact that there isn't any instantly leverageable information, that it can't now be used for a modern attack because that's exactly the kind of thing we see.



And Jason, before we go on, let's take our second break.  And then we're going to talk about what is worse than having all of your servers encrypted with ransomware?  Turns out there is something.



JASON:  Uh-oh.  All right, yes.  We've got that coming up next.  All right.  So there is something worse, then.  We've got more to look forward to here.  More bad.



STEVE:  Yes.  When would you wish it was just ransomware?  And the answer is...



JASON:  I mean, this is the Halloween episode, so this makes sense.



STEVE:  Ah, yes.  When the attackers who got into your network stole all of your data, rather than encrypting it, and are threatening to expose it to the world unless you pay four bitcoins.  Now, okay.  Four bitcoins seems...



JASON:  How much is that?



STEVE:  ...kind of reasonable in today's extortion game.



JASON:  That's only $37,000 U.S.



STEVE:  Yeah, exactly, yeah.  For a city?  Yeah.  So that's what just happened to the city of Johannesburg, South Africa this past weekend.  They spent the weekend struggling to recover from its second cyberattack this year as key affected and infected services and systems were taken offline.  And of course that meant that services were not available to the citizens and people trying to use their city services.  The city first alerted users of the attack via Twitter on Thursday, October 24th.



And since then the note left by attackers, who call themselves the Shadow Kill Hackers, has been seen.  And it reads as follows.  The note reads:  "Your city has been hacked.  Hello, Joburg city!  Here are Shadow Kill Hackers speaking.  All of your servers and data have been hacked.  We have dozens of backdoors inside your city.  We have control of everything in your city.  We can shut off everything with a button.  We also compromised all passwords and sensitive data such as finance and personal population information.  Your city must pay us 4.0 bitcoins," and they say in parens...



JASON:  I love this part.



STEVE:  I know, yeah, I do, too, "...(that's a very small amount of money)..."



JASON:  Parentheses, yeah.



STEVE:  Just in case you weren't sure, "...to the following address," and then we have a bitcoin address, "until October 28, 17:00 p.m. your time."  So of course that was yesterday.  "If you don't pay on time, we will upload the whole data available to anyone in the Internet.  If you pay on time, we will destroy all the data we have, and we'll send your IT a full report about how we hacked your systems and your security.  Contact us for more information at shadowkill@tutanota.com.  Have a nice weekend.  Shadow Kill Hackers Group."



JASON:  Ah, they really do care, don't they.



STEVE:  They want the best for Johannesburg.



JASON:  This is tough love here, is what it is.



STEVE:  So I'll note that while the English used by this group suggests they're not native English speakers, they do have very good taste in secure encrypted email services.  Tutanota, a GDPR-compliant, German-engineered, client-side encrypted email service, with a useful free starter tier, does look very nice.  They are extremely privacy centric and ask for donation support.  So let's talk about them because we're going to wish Johannesburg good luck and hope that, I mean, I don't know what they chose to do.  When I was putting this together yesterday, it was noted in the security press that was following this that there had been no recent bitcoin payment activity to that bitcoin address, which of course, as we know, the bitcoin monitoring technology is now rather mature, so it's possible to monitor activity on a given address and determine whether anything has happened.  So it doesn't look like, as of the reporting that I saw yesterday, payment had been made.



JASON:  Yeah, I actually just found an article on CNN that says they are refusing to pay.  The date has elapsed.  They say, "The city will not concede to their demands for bitcoins.  We're confident that we will be able to restore systems to full functionality."



STEVE:  Well, and they must also be confident that maybe the hackers were bluffing.



JASON:  Right.



STEVE:  Maybe everything is encrypted enough that they're not worried about what could be posted publicly.  It'll be interesting to see whether anything does get posted; and, if so, if it's in fact damaging or not. 



JASON:  Right.



STEVE:  But let's talk about Tutanota because I've been aware of them in the past.  I don't think I ever took a close look at them.  But the fact that the Shadow Kill Hackers were using Tutanota as their email service sort of put it back on their radar.  And I have to say I am very impressed.  So, I mean, to the point where I could recommend these people without reservation, based on everything I've seen, to our listeners.  It's T-U-T-A-N-O - now, I had "nova" somewhere.  It's "nota," T-U-T-A-N-O-T-A, Tutanota.com.



So they say on their page:  "Join us in our fight for privacy.  In the future, Tutanota will be the privacy-respecting alternative for Google with a calendar, notes, cloud storage, everything encrypted by default.  This is our dream of the future Internet, and we are truly amazed by the feedback and the support we have received from you so far."  They said:  "We invite all of you to get in touch and to support our goal in bringing privacy and security to the world."



And I'm just going to quickly touch on some of the bullet points to share what it was that they said that so impressed me.  So they say:  "Secure email for everybody.  Tutanota is the world's most secure email service."  Okay, now I would argue that, as we know, there are several of these.  But these guys do look like they've got their hearts in the right place.  They said:  "Easy to use and private by design.  Sign up for free to take control of your mailbox.  Safe from attackers.  With end-to-end encryption and two-factor authentication, your emails have never been more secure.  The built-in encryption guarantees that your mailbox belongs to you.  Nobody can decrypt or read your data."



They say:  "We love open source.  Tutanota is open source so security experts can verify the code that protects your emails.  Our Android app is Google-free, making Tutanota the best open source email service."  They said:  "On your favorite device.  With apps for iOS and Android, your secure emails are available any time.  Our fast web client and fully featured apps make secure encrypted emails a pleasant experience.



"Free secure email without ads.  We take your email needs seriously by offering you a secure email service, free from advertisements.  We work around-the-clock to bring you an email service that lets you focus on the important things.  From our clear and minimalist design to white label customizations, from our free version for personal use to our fully featured secure business email for companies, we are committed to making sure Tutanota is the only email service you will ever need."



They say:  "We encrypt everything.  We guarantee that your data in Tutanota is always encrypted, whether you use our secure webmail client, our apps for Android and iOS, or our desktop clients for Windows, Linux, and macOS."  Now, that's interesting, a desktop client for Windows.  I'm going to take a look at that myself.  "Easily send a secure email with the knowledge that Tutanota encrypts subject, body, and all attachments for you.  Import all your contacts into Tutanota's encrypted address book and rest assured that nobody else can access your contacts' personal information.



"Anonymous email.  Privacy," they say, "is a human right.  Everybody has the right to privacy and security.  That's why the basic secure email account in Tutanota will always be free, with all security features included.  No personal information such as phone numbers is required to access your anonymous email account.  We do not log IP addresses and strip IP addresses from emails sent and received.  Your right to privacy is at the heart of Tutanota."



And then they conclude:  "Proudly engineered in Germany.  Our entire team is based in Hanover, Germany.  All your encrypted emails are stored on our own servers in highly secured data centers in Germany.  With its strict data protection laws and GDPR, Germany has some of the best laws in the world to protect your secure emails.  The German Constitution stresses the importance of freedom of expression and of the human right to privacy."



So anyway, I am impressed.  It's hard to argue with those principles, which I know many of our listeners here share.  So I wanted to put, as I said, Tutanota.com on everyone's radar.  And I guess we could thank Shadow Kill Hackers for bringing Tutanota back to our attention.  Either some listeners or I, I mean, it's not the first time I've heard of them, and they have been around for several years.  But they really are saying all the right things.  And I'm absolutely going to check out the Windows desktop app to see what it looks like and how it works.



I was just talking last week about - or maybe it was to Leo, so it must have been the week before - about the fact that I'm a Thunderbird user.  Oh, yeah, because it was in the context of Thunderbird, a future version coming, I think it was next summer, will be integrating PGP into Thunderbird natively to dramatically simplify its use for people who want to use PGP-style encryption.  And of course you do need to use an account at Tutanota if you want to use their encrypted email.  But, yeah, from time to time it's clear that there must be some means for sending an encrypted email to someone who is then able to view it through their web browser without themselves needing to have a Tutanota app or account.  So anyway, it looks like these guys have done everything right.  And I just wanted to let our listeners know.



JASON:  Tutanoted.



STEVE:  Duly Tutanoted.



JASON:  Duly Tutanoted.



STEVE:  So Firefox's Privacy Protection.  For just little old boring, "I never surf anywhere exciting" me, Firefox has blocked 3,080 web trackers since just September 4th of 2019, which is interesting because, again, really I'm not wandering around the 'Net very much.  But Mozilla just released the broader figures to demonstrate Firefox's anti-tracking effectiveness.  Their stats show that Firefox blocked 450 billion cross-site tracking requests since just the 2nd of July, shortly after enhanced tracking protection was first introduced for Firefox.  And since that time the rate of tracking protection blocking has risen to 10 billion blockings per day.  So what do you think, Jason?  Time, perhaps, to give Firefox another look?  You know Leo has switched back to Firefox; right?



JASON:  So, I mean, I installed it, actually, on my desktop the other day.



STEVE:  Since we talked about it last week?  Oh.



JASON:  Yeah, we talked about it last week.  Installed it again.  You know, it wasn't very long ago that I still was actually using Firefox.  But I wasn't using it isolated to itself.  I was using Chrome on one screen and Firefox on the other.  And I would log into one of my Google accounts on one browser, and the other one on the other, just because switching between the two accounts used to really suck.  



STEVE:  Oh, interesting.



JASON:  So having them siloed on their own was the way that I chose to do it.  And so I was still kind of in the Firefox realm.  But they've improved it, and so I've been using Chrome.  So now I have it installed, at least.  And so I'm going to challenge myself to kind of focus on it a little bit.  I think one side of that is that this is a Chrome OS laptop that I use.



STEVE:  Ah, yes, yes, yes.  So it does behoove you to stay with Chrome.  I completely understand that.



JASON:  There's a little bit of lock-in there, I think.  



STEVE:  So I'll also note that Firefox is continuing to move forward with what they call "Lockwise," which is their built-in password manager.  They were boasting that it can now generate a secure password.  It's like, okay, well, good.



JASON:  Welcome to the party.



STEVE:  When signing up for a new account.  I would argue that that's a minimum requirement for a password manager is that it be able to do that for you.



JASON:  Agreed.



STEVE:  And also that it can be used to replace a current weak password with a new, more secure one.  They also note that access to local browser-contained Lockwise database can be protected using Apple's Face ID and Android's Touch ID biometric recognition systems.  So that's nice.  And it does look like they did everything correctly.  They're using AES-256-GCM encryption, which is what you want to use.  That's what I chose for SQRL, for example, which is a tamper-resistant block ciphering technology.  They use the onepw protocol to sign into Firefox accounts to obtain encryption keys; PBKDF2 and HKDF with SHA-256 to create the encryption key from Firefox account's username and password.



So anyway, it looks like they've done everything right.  And I guess my only problem is that I'm, like you, actually, Jason, I am not browser monogamous.  I'm a bit fickle when it comes to browsers.  Some things cannot be beat under Firefox, like managing a large number of open tabs with the tabs sidebar.  I love having a bunch of tabs running down the side of my screen.  There's kind of a wannabe sidecar add-on for Chrome to put tabs on the side, but I've never liked it.  Firefox makes that, sort of builds it in.  So I would argue that Chrome has its place.  And I'm occasionally faced with IE or Edge.



So having a polygamous password manager such as LastPass allows me to keep all of those things that I'm using in a single archive, you know, all in one encrypted store, regardless of which browser I'm currently in the mood to use at the moment.  So I don't think Lockwise is going to get me because I really do want to be browser agnostic for password management.  But still, for those people who are only using Firefox, for what it's worth, Lockwise is continuing to move forward.



And I mentioned at the top of the show a very worrisome remote code execution affecting Nginx servers.  That's N-G-I-N-X, for those who don't know.  It appeared on the scene relatively recently, so it has the advantage of not dragging along Apache's  long history, which means a growing, some would argue bloated, codebase, which is having to keep, you know, sort of slows down and makes it a little more accident prone.  On the other hand, Nginx has had an accident.  So that can happen to even new servers.



Saturday, this past Saturday, a new and dangerous PHP flaw surfaced.  The very short version is, if your site or any site you are aware of and/or are responsible for, is Nginx web server based, and you're using PHP with the high-performance PHP-FPM feature in order to obtain optimal performance, you'll want to immediately, if not sooner, update to the latest versions of PHP.  Under v7.3, that's 7.3.11.  And under 7.2, that's 7.2.24.  The reason is your site is very likely susceptible to a serious remote code execution vulnerability.



This PHP-FPM that I mentioned, FPM stands for Fast CGI Process Manager.  It's an alternative means for sites to invoke the PHP script interpreter.  It brings increased efficiency through reduced per-instantiation overhead, which helps to keep busier PHP-based websites from becoming bogged down by PHP invocation overhead, which otherwise can happen.  If you're really busy with PHP, it can be the case that calling into PHP from the server has a substantial overhead relative to the amount of work that PHP's doing per call.  So the first mitigation was the switch from traditional CGI to Fast CGI.  And now we've gone an additional layer in to Fast CGI Process Manager, or FPM.  I use it myself for PHP over on Windows IIS.  So this vulnerability doesn't affect me.  But if I was telling people about Windows IIS server, rather than Nginx, then I would be a lot more worried.



So the problem is here the Nginx version of this module, its interaction with PHP contains a newly disclosed vulnerability that could allow unauthorized attackers to remotely hack a site's server.  The vulnerability, which is being tracked as CVE-2019-11043, affects websites with certain configurations of this PHP-FPM module, which is a common use in the wild because a proof of concept exploit for the flaw has been released publicly and has been shown to affect many sites in the public.  The primary vulnerability is in environment path info (env_path_info) underflow memory corruption in that module.  When chained together with several other issues, it allows attackers to remotely execute arbitrary code on vulnerable servers.



The vulnerability was spotted by Andrew Danau, a security researcher at Wallarm, while hunting for bugs in a Capture the Flag competition over the weekend.  He stumbled on the problem, and then two of his fellow researchers, Omar Ganiev and Emil Lerner, developed this into a fully working remote code execution exploit.  And of course we know that's the way these things tend to happen.  First you crash the machine, then you figure out how to get it to execute your own code.  Though the publicly released proof-of-concept exploit, which by the way is up on GitHub now, is designed to specifically target vulnerable servers running PHP 7 versions, the underflow bug also affects earlier PHP versions and could be weaponized in a different way.



So I've got the details in the show notes.  I'm not going to go over it at length here.  It involves the configuration of the invocation and the way the URL which is being fed to the FPM module has the script split off from the base URL.  It turns out that you can play some games by putting a new line in the URL that can create a zero-length component after the URL is split, which a hacker is able to use in order to get their own code to execute.  So even though this looks like it's very specific to this particular exploit, it turns out that it is the way PHP tends to be executed on Nginx machines.



For example, I had here in the show notes, and I'm quickly scanning - oh, yeah, here.  One affected web hosting provider is Nextcloud, who we've been also talking about recently relative to personal cloud hosting services.  They released an advisory yesterday warning their users that "the default Nextcloud Nginx configuration is vulnerable to this attack."  So it recommends system administrators take immediate actions.  A patch for the vulnerability was released just last Friday, and that was a month after researchers reported it to the PHP development team.  So the PHP folks have known of it for a month.  The proof of concept exploit, as I noted, is available, and the patch was just released.



So unfortunately we're seeing a window here where it's going to be very likely that lots of servers are not going to be updated in time.  And this thing has been weaponized.  It can execute the attacker's code remotely.  And it's very easy to scan systems to see whether they're vulnerable.  So once again, this feels like the kind of thing that we may unfortunately be talking about having been used to install cryptominers and malware, maybe yet even more ransomware, who knows.



But it's real, and if you are an Nginx-based PHP site, or you know of one that you have some contacts with, make sure that they've updated their version of PHP.  This is a fix to PHP.  It's not absolutely certain that every configuration would be vulnerable, but the most common ones are.  And it's believed now that, I mean, that I saw figures like many, many tens of thousands of websites on the 'Net are currently vulnerable to this.  So again, needs to get fixed fast.



So Jason, we have been living with SMS for quite a long time.



JASON:  Indeed.



STEVE:  Calling it the Simple Messaging Service...



JASON:  Little too simple.



STEVE:  ...probably is understatement, yes.  It is embarrassingly simple.



JASON:  Yes.



STEVE:  And of course, as we know, it's also been a source of many headaches for people through the years.  On the other hand, it's been better than nothing.  Imagine that you could not send an SMS message.  I mean, imagine if there was, like, nothing but voice.  That would not be good either.  So here's the story.  Maybe.  I mean, they're promising it.  We'll see.  Next year, in 2020, all four of the major U.S. mobile phone carriers - so that's AT&T, Verizon, T-Mobile, and Sprint - will, they are announcing, finally be joining forces to launch an initiative to finally replace SMS with the long-awaited RCS mobile messaging standard.  We potentially and hopefully care about this because the nationwide switch over to RCS would serve to finally, at long last, raise the incredibly low lowest common denominator for cellular content exchange well above where it currently sits with SMS.



As I said, it couldn't be any worse.  The bar couldn't be any lower than sending a limited size ASCII message that's not encrypted, not authenticated, can be hacked because the carriers, the global carriers are still using the SS7 switching system which is known to have flaws.  Well, the biggest among them is that there's no authentication in the SS7 protocol.  So, I mean, if you were designing something to be bad, you could not do a better job of designing something that was bad.  And that's what everyone's been using down at the SMS level.



So this initiative is also working with its carrier ownership group and other companies to develop and deploy this new RCS standard in a new text messaging app for Android phones that is expected to be launched next year in 2020.  However, because it got tired of waiting, earlier this year Google independently released RCS messaging for Android smartphones in two countries, the U.K. and France.  So that effort will presumably be integrated into the formal RCS, if and when it actually does happen next year.



So the goal of this joint venture, which is called CCMI, Cross-Carrier Messaging Initiative, because wouldn't it be nice to have it be cross-carrier, is finally to get the four major cellular carriers off their butts to deliver GSMA's Rich Communication Service, thus RCS, which is an industry standard, that is to say, an unadopted, unused, not yet deployed standard, to consumers and businesses, both in the U.S. and globally.  Although the RCS standard was developed more than a decade ago, it's never been adopted widely due to the quagmire of mobile carrier and phone maker politics.



And, you know, we have an example in front of us which is Apple, which has no particular interest in RCS because it's already offering more than that through iMessages.  On the other hand, they're going to have to, in the same way that they do allow SMS to interact with iMessage, thus the green balloon in iMessage, they'll have to allow RCS to interact with their system, as well.



JASON:  Certainly hope so.



STEVE:  Yeah.  Well, yeah, I'm sure they will.  And of course iMessage is, as we know, end-to-end encrypted, which RCS is not and cannot be.  A few carriers and services have struck out on their own so far to offer non-universal versions of this new messaging standard.  But those are, because they're non-universal, they're limited to the exchange of RCS-based messages to the subscribers of their own network.  So that hasn't been very useful.  I mean, okay.  You have to be conscious of whether - I guess it would be useful for, like, if you knew your friends were on the same carrier, and among family members if you were in the family plan on the same carrier.  But great.  It's got to be universal.



So what is it?  Unlike our now ancient SMS technology, RCS-based enhanced text messaging service supports high-resolution photo sharing, location sharing, group messaging, animated stickers, read receipts, and some of the other features which are all very reminiscent of Apple's iMessage.  So in that sense it does significantly raise the base level, lowest common denominator messaging functionality that comes installed on phones by default.  And we should remember that there is also a large lower tier feature phone market lying underneath the smartphone market.  Those will likely be the greatest feature recipients of this change, since smartphones have obviously had all these features for quite some while using their own protocols.



Speaking of which, unlike iMessage, Signal, WhatsApp, Threema, et cetera, as I noted, RCS messages are not end-to-end encrypted.  They do, however, contain message verification and whatever this is, they called it "brand certification mechanisms," to ensure that users are interacting with legitimate brands to protect them from fraudulent accounts, impersonators, or phishing.  Also in RCS's favor is that, whereas SMS communicates over the SMPP protocol, using as I mentioned the insecure SS7 switching system, all RCS traffic between the device and the network can be protected using SIP over TLS encryption.  So we will get endpoint authentication and communications privacy through encryption.



The news reporting on this notes that the CCMI project has not yet fully developed its RCS-based messaging standard.  Whereupon, reading that, I thought, really?  After more than a decade?  Okay.  So it's not yet clear at this moment if the major U.S. carriers will implement protections for their users' privacy from government surveillance. But I'm sure that many of us would be willing to take the bet that that won't happen at this late stage, where the government's campaign against the absolute privacy within its borders is so strong at this point.



There's just no way that cellular carriers are going to be able to stand up to the U.S. federal government and themselves incorporate secure end-to-end encryption, if we even believe they have the ability really to do so.  I mean, we know that Google can.  We know that systems like iMessage and Signal and WhatsApp can because they've been developed by serious security research and cryptography work.  There's no sign that AT&T and their ilk have any similar interest in doing that.



So I don't think we're looking at secure end-to-end encryption for the feature phone market from this.  However, the announcement does say that it will "enable an enhanced experience to privately send individual or group chats across carriers with high-quality pictures and videos."  So it's not clear what "enable an enhanced experience" means.  Sort of sounds like corporate speak written by a PR flack who is also unsure of exactly what the system would do.  But at least it sounds good.  So who knows.  We'll see how this rolls out.  We'll see next year what it brings.



What I think it means is that for the non-smartphone market the lowest common denominator which is already being used in feature phones, which is text, that's going to get a big boost.  So that's probably a good thing.  I don't think it'll have any impact on the higher end smartphone market, where users who can have encryption and want encryption already have it.  And we already have all of those other features - group chat and high-resolution photos and videos and FaceTime and everything.  So it's like, okay.



Still, it's useful.  At some point we're still using SMS for delivering one-time passwords; right?  I mean, we're trying to move away from that.  It's still being done.  So switching that to encrypted authenticated SMS messaging or SMS-equivalent messaging, well, that would be a good thing.  It would marginally increase the security.  Still not a good solution because you still have all the SIM swapping problems and all of that.  But bringing up the lowest common denominator certainly makes some sense.  So if it happens, we'll get that.



One bit of SQRL news and two bits of miscellany.  I learned this past week that there is a site called SQRLFor.net, S-Q-R-L-F-O-R dot net, as in SQRL for .NET.  And the site says:  "SQRLForNet is middleware for implementing SQRL in your ASP.NET Core solution. It offers a fully protocol-compliant server-side solution with a high degree of customization.  Our goal is to enable any and all ASP.NET Core sites to enable SQRL login for their visitors."  So in other words, yet another server-side package for SQRL.



And this actually, first of all, I was delighted to learn of this work.  And it does further demonstrate one of the aspects of SQRL that I've argued would really help its adoption, which is it is just not very difficult to bring up SQRL support on the server side.  Arguably, the large body of complexity exists in SQRL clients, but we only need a few of those.  We need a good one for iOS, and we have one.  We need a good one for Android, and we have one.  We need a good one as a browser extension, and we have one.  There is work on a native Linux client underway.



The point is that, once we've got the few clients that we need covered, then that handles the client side.  And the fact that it is relatively low effort, comparatively, to implement SQRL on the server suggests that adoption's going to be much easier.  And it's toolkits like this that reduce it to being a slam dunk.  For example, there's a plugin for WordPress, which means that more than half the sites on the 'Net that are WordPress-based can add SQRL with a single click over at WordPress.org.



So anyway, there's lots of ongoing news, and so I just wanted to keep our listeners in the loop with what's happening over there.  And Jason, we could take this opportunity to remind our listeners that I will be up in Petaluma in the TWiT studios on Saturday, the afternoon of Saturday, November 30th, to have the SQRL story formally recorded by all of the great TWiT producers over at your end.



JASON:  That's right.  Anyone, I think that there's still room for this, but...



STEVE:  Oh, how could there be.  Good, good.



JASON:  Regardless, either way, tickets@twit.tv.  There might not be.  Honestly, I haven't heard in the past couple of days where it's at.  But tickets@twit.tv is where you should email if you want to inquire and see if there's room in the studio.  If you happen to be in the area, and you want to be here for that, you can.  That's 2:15 in the afternoon Pacific time on November 30th.  It's a Saturday, like you said.  So tickets@twit.tv.



STEVE:  I think we're going to have some sort of hangout afterwards, I think, also.



JASON:  Right, like a meet-up of some sort.



STEVE:  Yeah, yeah.  And again, please don't sign up if you think you cannot make it.



JASON:  Right.



STEVE:  Only do so if you're absolutely committed to being there because we'll know who you are.



JASON:  Mm-hmm.



STEVE:  If you signed up and didn't show.



JASON:  That's right.  We got you.



STEVE:  Okay.  Two pieces of miscellany.  Tim Kington tweeted.  I got a kick out of this.  He says:  "You made a serendipitous mistake during the podcast this week" - meaning last week's podcast - "and said 'securious.'"  He said, "I loved it.  I think it should be the byline for the show:  'Security Now!, Podcast for the Securious.'"



JASON:  I like that.



STEVE:  So, yeah, I think that's pretty good.  Thank you for catching that.



JASON:  Has a nice ring to it.  That should be on a T-shirt.



STEVE:  I kind of remember that slip of the tongue.  Security Now!, the podcast for the securious.  And Dr. Flay's posting in GRC's Security Now! newsgroup.  He posted:  "The official Checkra1n domain" - remember we talked last week about the forthcoming CheckM8 publicly available exploit that we're expecting, which will allow anybody with a phone, from an iPhone 4S with the A5 chip up through and including the iPhone 10 with the A11 chip, it'll mean that it will be possible always to jailbreak any of those with a USB connection once that's developed.  Remember that I mentioned that it was weird that "Checkrain" spelled correctly, R-A-I-N dot com, had been immediately commandeered by an annoying download-and-install-for-pay site, sort of a scam site; whereas the Check R-A-1-N dot com was the authentic one.



Anyway, Dr. Flay noted that now Checkra1n.com have wisely registered - oh, first of all, it is live with a valid cert, and it gets an SSL Labs grade A certificate, and they have wisely registered other possible squatting options - checkra.in, checkra1n.io, checkra1n.dev, and checkra1n.net.  So anyway, he says - oh, and then he also said:  "I gave the scam site an appropriate review in WOT, though it will need more reviews before triggering any warnings in the reputation systems."  And then he put a link there to the mywot.com page, where our listeners who are interested could also go in and down-rate that site in order to hopefully begin, hurt its reputation so that browsers will begin warning users.  But anyway, thank you, Dr. Flay, for the update. 



JASON:  Yeah.  Super important.  A foregone conclusion.  What in the heck?  What have we got here?



STEVE:  So the subtitle for this discussion should be "Supreme Court, where art thou?" because the way our court system works, as we have seen and talked about a number of times in the past, lower court judges are judges, and they get to make their decisions sort of like based on what law they're able to find.  But when there isn't any clear law, they've got to rule based on what there is.  And of course that gets appealed.  It gets challenged.  Ultimately the Supreme Court is - obviously we call it the Supreme Court because that's the final arbiter of these things.



So I wanted to give a recently announced Oregon state appellate court decision, which addresses the question of whether forcing the disclosure of a memorized device unlock password is in fact tantamount to compelling self-incriminating testimony against one's interest, which thanks to the Fifth Amendment to the U.S. Constitution is a protected right.  The privilege against compelled self-incrimination is defined as "a constitutional right of a person to refuse to answer questions or otherwise give testimony against himself."



We've many times discussed this issue, since at this point in the development of the consumer product industry we broadly have two common ways of unlocking our devices:  as we discussed last week, biometrics, something you are, whether your face or your thumbprint or your knuckle print or whatever; and the more traditional password, a passcode or a swipe sequence, all of which can be lumped under a secret, you know, considered to be a secret, which is something you know.



In the U.S., as I mentioned, we have this problem where, when there is no ultra-clear, constitutional, black-letter law, nor any more recent ruling by the ultimate Supreme Court of our land, the lesser courts and judges are left with the freedom and the burden of attempting to interpret what has come before in order to create a foundation and a context for new cases which are brought before them.  So this has all boiled down to are we safer against forced device unlock using a password or a biometric?



And the upshot, as we've discussed in the past so far, has been that people who have memorized their passwords are safer because they're protected by the Fifth Amendment against being forced or coerced in any way to testify against themselves by divulging their devices' unlock password.  In other words, telling someone your password has been considered to be testimonial in nature.  And I would argue that, if it's your password that is your secret, then ultimately, whether or not the court rules for or against you withholding it, you still have the right to withhold it.  I mean, you could be held in contempt of court, right, if you don't give it up.  But you may decide that, well, that's better than letting them see what's in my phone.  So it seems very clear that using a password is the better solution.



And of course we should also note that there is sort of a hybrid here, right, because if you do the panic act on your iPhone, I think you click the power switch rapidly, like five times in a row or something, that immediately puts it into - it disables the biometrics, requiring that you use your password.  So you sort of can get the best of both worlds.  On the other hand, if something happens like you're in a car accident, and you're unable to get to your phone, and the authorities are able to get to it, then you don't have the ability to do that.



So anyway, the problem is this is not something, this whole issue of is your password something that can be compelled is not something that the U.S. Supreme Court has yet ruled on.  So the lower courts are flip-flopping around.  And another flip-flop happened last week.  This Oregon appeals court last Wednesday decided that a woman whose judgment was impaired by methamphetamine when she crashed into a tree, seriously injuring one adult and five children passengers - and actually a different adult, so I guess she injured herself, a different adult, and five children - they ruled that she can be forced to unlock her iPhone with something she has previously committed to memory.



The appellate court's decision states that in their opinion, which is their ruling on the matter, it is not a violation of her Fifth Amendment rights against self-incrimination because the fact that she knows her phone's passcode is a foregone conclusion.



Okay.  So that's interesting and new.  This was a 27-year-old woman who crashed her car into a tree in Salem, Oregon, injuring herself, her friend, and five children in the back seat.  She did not want to give police any help in building a criminal case against her.  They wanted to search the contents of an iPhone they found in - her name is Catrice Pittman, so found in Catrice Pittman's purse.  But she never confirmed whether it was hers and wasn't offering her passcode.  Her defense attorney argued that forcing her to do so would violate her rights against self-incrimination under the Fifth Amendment of the U.S. Constitution and Article I Section 12 of the Oregon state Constitution.



But a Marion County judge sided with police and prosecutors - and this was a clever prosecutor who came up with this - and ordered Pittman to enter her passcode.  The ruling was appealed.  Then Wednesday the Oregon Court of Appeals agreed with that ruling in a first-of-its-kind opinion for an appeals court in Oregon, and apparently anywhere.  The ruling will likely make it easier moving forward for Oregon police to compel access to the contents of suspects' cell phones and all of the massive amount of personal information they contain, you know, photos, videos, past Internet searches they've conducted, text messages, phone contact lists of everything they have been in contact with and when and so forth.



So the deputy public defender who represented Pittman on the appeal, an attorney named Sarah Laidlaw, she said:  "The upshot of this case is when police have a warrant to get information off a cell phone, and the government knows the phone is yours and you have the passcode, then the court can compel you to enter the passcode."



Ryan Scott, a criminal defense attorney in Portland who's not associated with this case, but closely follows appeals cases, said the ruling is an example of a continuing erosion of rights.  Ryan said:  "Our rights are a little less than they were yesterday.  But for those of us following this area of law, it's not a surprise."  He said that federal law has been leaning in this direction for the past few years.



Scott said the ruling won't affect many Oregon defendants whose phones are seized by police because police already have technology that allows them to crack into most of those phones.  But sometimes, as apparently in Pittman's phone, police cannot get in.  Scott said that the latest iPhones, more than other phones, have proven the most difficult.  Scott volunteered that, from his experience, for people who want their information private, he would recommend getting an iPhone.  And he added that:  "Apple is not paying me to say that."  And of course we know for many reasons that Apple puts a premium on trying to keep that stuff private.



JASON:  For sure.



STEVE:  Pittman had argued through her attorneys that punching in her passcode would amount to testifying against herself because doing so would effectively admit that the iPhone was hers, and that she knew the passcode.  But the Court of Appeals ruled that because police already had good reason to believe the phone was hers, given its location in her purse, the fact that she knew - and here's sort of the crux of this.  The fact that she knew its passcode was already a foregone conclusion.  In other words, she could be compelled to cooperate as an exception to her constitutional rights.



In doing some digging around additionally, I saw that this odd-seeming foregone conclusion standard has been coming up a lot recently in these compelled unlocking cases.  It allows prosecutors to bypass Fifth Amendment protections if the government can show that it knows that the - I've got too many "that's" - that it knows that the defendant knows the passcode to unlock a device.  And if I ever needed another reason to be a coder, rather than an attorney, this is it, since this makes my head hurt.



But the reason we need the Supreme Court is that, as we've covered in the past, not all courts, nor even all appellate courts, have ruled in the same direction on this decision.  So we need someone to make this final.  Of course we talked about this before.  An example of the decision that came out the other direction was the Florida Court of Appeal just recently, in November of 2018, regarding a case very similar to that of Pittman, it involved an intoxicated minor who crashed his car, leading to the injury or the death of some passengers, then refused to unlock his phone for police.



In Florida, the court refused a request from police that they be allowed to compel this underage driver to provide the passcode for his iPhone because of the "contents of his mind" argument that invoked the Fifth Amendment.  But the Florida court even went beyond that, saying that whereas the government in the past has only had to show that the defendant knows their passcode, with the evolution of encryption, the government needed to show that it also knew that specific evidence needed to prosecute the case was on the device - and in fact we talked about this aspect of that at the time - not just that there was a reasonable certainty the device could be unlocked by the person targeted by the other, which is to say they had to demonstrate they had some cause for knowing that there was evidence there.



If prosecutors already knew what was on the phone, and that it was the evidence needed to prosecute the case, they didn't prove it, the Florida court said at the time.  From the order to quash the passcode request, the court wrote:  "Because the state did not show, with any particularity, knowledge of the evidence within the phone, the trial court could not find that the contents of the phone were already known to the state and thus within the 'foregone conclusion' exception."



So here again, I mean, this invoked the "foregone conclusion" and said, okay, we know about that, but you can't show us that you have reason to know what it is that you expect to find, so no.  Like I said, this is a mess.  Oregon found differently.  Even looking at this prior Florida case law, the appellate court and the lower court both ruled that "foregone conclusion" applied.  So this is going to go back and forth until eventually one of these things makes its way to the Supreme Court and we get someone to decide for us one way or the other.



As I said, it looks to me like, if somebody were really concerned about this, then using a passcode is the right way to be safe in situations like this, rather than the convenience of a biometric.  And as we also talked about last week, it's not clear to me that a biometric provides and affords the degree of protection that we hope it does.  But it certainly is easier to use.



JASON:  How, in that situation, you've got a phone, you've got a PIN on it, and say you are in the position to say I don't remember my PIN, I mean, most of us would probably say that we know the PIN on our phone.  But it's not impossible for someone to not remember their PIN.



STEVE:  That's true.



JASON:  Maybe it's super long, and in that moment they don't.  So then in the case of this foregone conclusion, like, what then?  Because you legitimately can't remember, you're prosecuted against?



STEVE:  Yes.  So that's when they hold you in contempt, and they put you in a cell and give you some time to try to remember it.



JASON:  Go ahead.  Think about it real hard.



STEVE:  So, you know, we will - you're not going to like the food.



JASON:  Right.



STEVE:  You're probably not going to want to stay here for long.  So just do try to wrack your brain.



JASON:  Yeah.  Isn't it convenient that after all this you finally remembered.



STEVE:  Yes.  And so, let's see.  With a PIN, if you - I forgot how this works.  If you don't guess correctly, then you have to power cycle the phone to try again?  I mean, it can't lock you out forever; right?



JASON:  I think it depends on the phone.  If I remember, the  iPhone - and I've never been, like, a long-term iPhone user.  But if I remember on the iPhone, if you enter your PIN enough wrong times, every time you do it, it adds more time.  And, I mean, that could get so long that you just - it could be months.



STEVE:  Yeah.  I think something bad happens if you exceed some number of guesses.  I know it does.



JASON:  Yeah.  At a certain point it could erase all data, right now they're saying in the chat room.



STEVE:  Oh, that's what it is, that's what it is.  Yes, yes, yes.  It doesn't evaporate or teleport from your hand, but that's what it does.  It does the full erase of the phone to protect itself from - and actually, these days, since there's an on-the-fly cipher, an AES-256 cipher always in line between the iPhone's drive and the phone, all it has to do is wipe the key.  So it just - it instantly zeroes that key, its RAM-based storage of that key, and at that point you need to go back to Apple in order to you, you know, like if you do an iCloud backup and go through all those rigmaroles, or restore from home and so forth.



JASON:  Right, right.  



STEVE:  Very cool.



JASON:  Well, this show is a foregone conclusion.  I don't know.  I don't know if that makes any sense, but it is a conclusion because we've reached the end.



STEVE:  I just thought that was a real interesting legal argument.



JASON:  Yeah, it is.



STEVE:  You know, it's your phone.  It's in your possession.  We know you use it, so you know how to unlock it.  So it's a foregone conclusion that you have this information, and that means it's not testimonial.



JASON:  Right.



STEVE:  So it's like, okay, I guess Denise Howell can explain this to us in more detail.  But like I said, I'm glad I write code and not interpret the law because, ouch.



JASON:  Indeed.  I completely agree.



STEVE:  Computers are much more logical.



JASON:  To you, for sure.  This was excellent stuff.  I'm really happy also that you got to talk a little bit, shed some light on the RCS stuff.  That's been, yeah, I feel like that's been jumping back and forth between Google and the carriers for the last couple of years now.  And at least there's some sort of light at the end of tunnel as far as that's concerned.  I'm not sure if it's Google's preference as far the way it went down.  I think Google probably wanted the carriers to jump onto their solution.



But we recorded All About Android early last night, and Ron Richards on the show made the point that, like, no matter what, if it's implemented, and it improves things - kind of like you said.  Even if the baseline gets improved as a result, and they're all supporting it, it doesn't matter which direction it comes from.  Anything is probably better than SMS at this point.  And so I'm looking forward to it.



STEVE:  Does Google have a native end-to-end encrypted messaging?  Or is it always an add-on?  [Crosstalk] Signal and WhatsApp and, you know.



JASON:  Yeah.



STEVE:  But, like, iMessage is built into iOS.  Is there a built-in to Android?  I guess really it would have to be Android-centric; right?  So you wouldn't be able to send an iMessage.  No, you wouldn't be able to send an SMS to somebody over on Apple world.



JASON:  Yeah.  I mean, Google Hangouts was their messaging app for quite a long time.  And it does have encrypted conversations.  It doesn't use - I don't know enough about how it handles the encryption on Hangouts to know.



STEVE:  So, for example, it's not wrapped in Signal or one of the mainstream strong encryption...



JASON:  No, no, no, no.  It's not.



STEVE:  End-to-end encryption service.



JASON:  And, you know, Google's been moving to push Hangouts out the door, basically replacing it with Android Messages with RCS.  So I would say the answer is no.  Like if you really want truly encrypted messaging, you're not looking at Google for that right now.  



STEVE:  Or you add another, you know, your own layer.



JASON:  Sure, sure.  But Google isn't really putting out a solution right now that appeals to that, to that market.



STEVE:  Right, right.



JASON:  Honestly, Google has just dropped the ball repeatedly as far as messaging is concerned, and it's exhausting.  As an Android user, it's really exhausting.  You want something that works.  And I've just gotten used to using, like, 10 different messaging apps for different reasons with different people, and, yeah.



STEVE:  Oh, goodness.  So it's like, oh, wait a minute, she has this one, and he has that one.



JASON:  Right.



STEVE:  So you...



JASON:  Totally.



STEVE:  Whoa, yeah.



JASON:  And strangely, it makes sense, like I've done it long enough now that I know who's where and all that stuff.  But it shouldn't have to be that complicated.  And on iOS it's not.  And so that's what's really frustrating.  But anyways.



All right.  We've reached the end of this episode of Security Now!.  And Steve, awesome stuff, as always.  Anyone who wants to check in on what Steve is up to and inform yourself on everything Steve Gibson, go to GRC.com, find everything you expect to find there.  You're going to find SpinRite.  You're going to find information about SQRL.  Audio and video of this show is also there, as well as that's the only place that you'll find transcripts of the show.  So if you want to find a transcript, just go to GRC.com.



And our website that you're seeing right now is TWiT.tv/sn.  There you can also find all the audio and video, all the subscription links.  If you want to watch on YouTube, or you want to subscribe through Apple Podcasts, Google Podcasts, all the major ones are there, and you can just link right to it.  Or just watch them in the page.  That works, as well.  But we always recommend that you subscribe so that you don't have to think about it.  It just delivers to you like the podcast magic that it is.  If you want to watch us record this live each and every week, it's Tuesdays, 1:30 p.m. Pacific, 4:30 p.m. Eastern, 20:30 UTC.  I'm sure that's changing anytime now.  I know the times are about to change here.  And that's TWiT.tv/live.  And when you do that, you can be a part of the chatroom, and chat about all these topics while Steve is talking about them.



So Steve, great job today.  Thank you so much.  And we will see you next week on another episode of...  



STEVE:  And we will see you next week.  And the week after.  So thank you.



JASON:  That's right.  That's right, two more weeks.



STEVE:  Thank you, Jason.  It was a pleasure.



JASON:  Me, too.  Right on.  We'll talk to you next week on Security Now!.  Bye, Steve.



STEVE:  Bye.



Copyright (c) 2019 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.






GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#739

DATE:		November 5, 2019

TITLE:		BlueKeep and DoH

HOSTS:	Steve Gibson & Jason Howell

SOURCE:	https://media.grc.com/sn/sn-739.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we examine a widespread Windows breakage introduced by last month's Patch Tuesday.  We look at several things Google changed in their just-released Chrome 78, news from the Edge, the status of attacks on Intel chips, a new attack on publicly exposed QNAP NAS devices, the significant risk of trusting managed service providers, the downside of apps for autos, and worries over Chinese-made drones.  We then finish by coming back to look at news on two other fronts:  the escalating controversy over DNS-over-HTTPS (DoH) and the commencement of the long-awaited BlueKeep vulnerability attacks.



SHOW TEASE:  It's time for Security Now! with Steve Gibson.  I'm Jason Howell, filling in for Leo once again.  We've got news from the Edge, Microsoft Edge; an attack on Internet-connected QNAP NAS devices; why connecting to rental cars via apps might not be such a great idea after all; Steve checks in on the DNS-over-HTTPS controversy; and the BlueKeep attacks have begun.  Steve Gibson's going to break it all down for you next on Security Now!.



JASON HOWELL:  This is Security Now! with Steve Gibson, Episode 739, recorded Tuesday, November 5th, 2019:  DoH and BlueKeep.



It's time for Security Now! with Steve Gibson and myself, Jason  Howell, filling in for Leo, week three.  How you doing, Steve?



STEVE GIBSON:  Great.  Good to be with you for - this is three of our four; right?



JASON:  That's right.



STEVE:  We get you one more week, next week, and then - actually, and then I'm up there on Saturday, November 30th.



JASON:  That is it, indeed.



STEVE:  The fires are out, and things are calming down now.  The air quality has been restored.



JASON:  It's really interesting dealing with that scenario because there for, like, four or five, six days, it was everything that anyone was talking about, you know, locally.  It was just like fire was on the mind.  And then the second it dissipates it's like, quite literally, back to normal life.



STEVE:  That's right.



JASON:  It's just the kind of counterpoint between one extreme to the other is very interesting to me.  But yeah, we are here and ready to talk some security.  We actually have some folks sitting in the audience here today.



STEVE:  We have a guest audience.



JASON:  Yes, we do.



STEVE:  A packed house.



JASON:  Jim Willard and Tim Willard, both from Michigan, and Sharif Vallejo.  Where are you from, Sharif?  Oh, sorry, I thought that was your last name.  I was like, Sharif Vallejo, where are you from?  Sharif is from Vallejo.  So there we go.  He's here from Vallejo.  So we've got three guests sitting in for the show today.  So Steve, you've got to talk some serious security.



STEVE:  Very cool.



JASON:  It's got to be serious today.



STEVE:  So we have Episode 739 for November 5th.  And there were two things I want to sort of update us on.  One is DoH, and the other is BlueKeep.  DoH, of course, is DNS-over-HTTPS, which has turned out to be surprisingly controversial for everyone except users, who think it's a good idea.  But, oh, that's really causing some fur to fly.  So we're going to talk about that.  And then of course BlueKeep is the much-anticipated end of the Internet as we know it, which everyone keeps anticipating.  Well, finally we have attacks, but they are not what the Internet expected.  But they are what we were expecting on the podcast.  So we're going to talk about that.  We're going to end the show talking about that.



But there's a whole bunch of interesting stuff happened this week.  We've got a - I was affected by this - a widespread Windows breakage which was introduced by last month's Patch Tuesday.  It had been bothering me ever since the second Tuesday of October.  And in putting the news together for this show, I stumbled upon its explanation.  I implemented it, and it fixed the problem.  So I'm sure I'm not alone, and I wouldn't be surprised - let's see.  This is the first Tuesday of November.  I wouldn't be surprised if Microsoft unfixes or unbreaks what they broke next week.  And let's hope so because this has to be affecting lots of other people.  But we have a fix for it right now.



We also have, speaking of fixing what was just broken, Chrome 78 has made some important changes to what they did in Chrome 77.  So we'll discuss those.  We also have our new segment, News from the Edge, which is to say about Microsoft's Edge browser.  It's a little less dramatic than the name.



JASON:  I like that.  It's catchy.



STEVE:  News from the Edge.  We've also got the status, nearly two weeks downstream, the status of the attacks on Intel chips which were the whole Spectre and Meltdown thing.  And, you know, it does make you feel a little bit old when you realize that, okay, wait, Spectre and Meltdown, that was the beginning of 2018.  That's, like, wow.  Where did these two years go?



JASON:  It's kind of crazy.



STEVE:  It doesn't seem like it was that long ago.



JASON:  Unh-unh.



STEVE:  We also have an important new attack on publicly exposed QNAP Network Attached Storage boxes.  I know that our listeners are bullish on those because after I did my conversation about - we called it "The Joy of Sync" podcast a couple months ago, where I talked about my search for a really good way of synchronizing multiple locations and keeping directories synchronized.  QNAP has some of their own software.  And so I saw a lot of tweets from people who are using QNAP NAS devices.  They need to make sure that they update to the firmware that just came out on November 1st.  So we'll talk about that.



We also have an interesting report about the significant risk of trusting managed service providers and maybe the need to push back on the kind of access that they by default want to have into their clients' networks.  We've got the downside of apps for autos, and worries over Chinese-made drones.  So a whole bunch of fun stuff to talk about this week.  It's going to be another great podcast as we approach number 999, which of course we all know is the end of Security Now!.  And with that will be the end of all security problems on the Internet.  So, yeah.



JASON:  This is breaking news.



STEVE:  Well, I only have three digits, Jason.



JASON:  Well, it's breaking news to me that Episode 999 is the end of security issues.



STEVE:  My whole system is based on three digits.  And so it's like the Y2K of Security Now!, and it's lights out.  And Leo's been thinking, you know, that's about, okay, that's how long ago?  When's that going to be?  Okay, that kind of, you know, probably just works out.  He'll be off on a cruise when we wrap around from 999 to 000.  Actually, funny story.



JASON:  I can even clean up the mess while he's gone.  It's fine.



STEVE:  I was eating at - I had a favorite Chinese restaurant which had a computer-based system for managing, you know, that they put all their orders into and everything.  And this was a Y2K problem because I'm sure they never licensed the software.  And so when I first went in after the New Year, after Y2K, their systems had gone from 1999 to 19100.  So literally, so the 19 was hard-coded into the software, and the 99 went to 100.  So the checks came out, and they were five digits.  It's like, 19100.  So that was a bit of a problem.



JASON:  Whatever works for you, business owner.  Okay.



STEVE:  Yeah.  Anyway, they knew I was a computer guy.  They said, "What should we do?"  And I said, "Well, you know like how Leap Year works; right?"  And they go, "What?"  I said, "Okay, just go back, set everything back four years.  The year won't be right, but at least things will kind of line up correctly."



JASON:  Yes.



STEVE:  And anyway, so that's how they operated.  Yeah, 19100.



JASON:  And I'm curious to know if they're still dealing with that every four years.



STEVE:  Actually, they went out of business the summer of 9/11, the attacks of 9/11, because I don't know if you remember, but the restaurant business took a big hit...



JASON:  Yeah, it did, I remember that.



STEVE:  ...after the 9/11 attacks because everyone was a little freaked out and stayed home and ate popcorn and watched TV to see if anything was going to happen next.  So anyway, that kind of did them in, even though they were the best Chinese restaurant place in Southern California.



JASON:  Well, that's unfortunate. 



STEVE:  And in other trivia - no.



JASON:  Yeah, I know.  Where are we going to go next?  All right, so we're starting with, I guess - I guess this is the Picture of the Day, kind of?  Sort of?



STEVE:  Well, it is the Picture of the Week because it ties into our first story.  This was literally what I have been facing ever since my Windows 7 machine updated after the second Tuesday of October.  And there it shows me trying to bring up the Digikey.com site.  Digi-Key is one of my favorite and is actually my go-to site for electronic components, Mouser being second choice.  And here it shows down in the lower left, "Establishing secure connection."  And that's where it gets stuck.  And that's trying, after I hit Refresh, after we got the unhappy web page icon, and it says "This site can't be reached; www.digikey.com took too long to respond."  And then it gives you some generic problems.



So Windows broke something with last month's Patch Tuesday, which Microsoft has now acknowledged.  And my sense is that, since this is a core protocol thing, it can't be just me trying to get to one particular website that's been having problems.  So as we've been recently discussing, I often, like you, Jason, well, you're a stronger user of Chrome.  I'm fickle.  So I will be...



JASON:  You're a flip-flopper.



STEVE:  I'm a flip-flopper.



JASON:  Okay.



STEVE:  Exactly.  I'll often be using Firefox.  Normally I actually have Firefox open statically in my lower left-hand display.  That's just kind of where it lives.  Over on the right is Windows Explorer.  In the center is what I'm doing.  So I sort of have, like, reference stuff around me.  And then I have sort of a fixed layout for where things go, like what goes on which screen.  So it's been my habit to open up a Chrome session in the center screen when I want to browse around Digi-Key for some random electronics gizmo.  Except that Chrome stopped being able to display Digi-Key's site.  It would, you know, the thing would spin and eventually complain that it was unable to obtain a secure connection, or any at all.



And, I mean, this has really been an issue.  In the past few weeks I've hit F12 to open Chrome's developer window, to examine the exact error code that was returned.  I thought that perhaps Digi-Key might have somehow misconfigured their site.  Because, I mean, again, I've used these guys for years, and never have I had a problem.  So I remember I went over a couple times actually to Ivan Ristic's SSL Labs site and had them scan Digi-Key.  But it got top marks from Ivan.  So it didn't seem to be something there.



And one of the more curious things was that Firefox never had any problem.  While Chrome wouldn't open Digi-Key, Firefox - it just popped right up in Firefox.  But I wanted to use Chrome, and I couldn't.  So it turns out it wasn't just with Digi-Key.  Under Chrome over the past few weeks other random sites all over the Internet started having trouble.  And since I learned a long time ago that things that I'm experiencing are likely to be widespread, since I'm not doing anything weird, on a number of occasions over the last nearly a month I would Google "Chrome connection errors" or "Chrome connection timeout."



I mean, I figured other people would be, like, noticing this and having problems, thinking that Google must have broken something on Chrome that would therefore be affecting lots of people, very much like they did break something with Chrome 77.  Well, not actually they broke it, and we'll be talking about this in a minute, where we talked about how, if you had the un-updated version of Symantec's Endpoint Protection, people were getting the "Aw, Snap" error from Chrome.



But anyway, so as I said, as I was doing the research for today's news, I stumbled upon the cause.  And I'm telling everyone who's listening in case anyone else has been, like, pulling their hair out with the same sort of problem, not being able to use Chrome in certain instances.



So here's the story.  I don't know how Microsoft managed to do this, but they broke some, well, they added support that is enforcement for an enhanced anti-man-in-the-middle protection involving TLS handshakes, which immediately broke Windows' ability to connect as a client or as a server to a significant number of the Internet's websites.  And what was interesting, when I stumbled upon that, was it was like this "aha" because it perfectly explained why Firefox has continued to work because, as we know, Firefox does not use Windows' underlying security stack.  It brings along its own.  But Chrome does run on top of the Windows security protocol.  So if Microsoft had done something to Windows, Chrome would be affected; Firefox would not be.



So once I understood what was going on, I immediately disabled what had proven itself to be a not-yet-ready-for-primetime protocol handshake feature, rebooted my machine - though some reports suggest it isn't necessary to reboot - and Digi-Key's site popped up on Chrome with no problem.  So BleepingComputer - which is where I finally, it's the only place I saw this noted, like in reading everything over the last couple weeks, so hats off to them.



They said in their note:  "Microsoft has acknowledged a new issue affecting several Windows versions" - which is, yes, everything currently being updated, meaning 7 through 10 inclusive, that Microsoft says, or BleepingComputer reports of Microsoft - "could lead to Transport Layer Security and Secure Sockets Layer connections intermittently failing or getting timed out."  And in my experience, never getting made in the first place.



They said:  "This bug is caused by the security-related enforcement for the" - and then we have a CVE, it's 2019-1318 - "TLS spoofing vulnerability, which leads to Windows devices experiencing failures and timeouts during TLS DHE dot star," essentially.  That's all of the TLS cipher suites involving ephemeral Diffie-Hellman, which is what the DHE, Diffie-Hellman Ephemeral, DHE stands for.  "This happens only when the devices are trying to make TLS connections to devices without support for the Extended Master Secret (EMS) extension."



So what Microsoft wrote was:  "Connections between two devices running any supported version of Windows should not have this issue when fully updated."  In other words, says Microsoft, as long as you have Windows, fully updated Windows at each end, at the client and the server, well, what's the problem?  



JASON:  That'll solve the problem for everyone, won't it.



STEVE:  Earth to Microsoft.  News flash:  Not everyone is using your Windows operating systems at each end of the connection on the Internet, especially not on the server end, where IIS has been steadily losing steam and market share to Nginx, Cloudflare, and other providers through the recent years.  So Microsoft's notice of this is headlined:  "Transport Layer Security (TLS) connections might intermittently fail or time out when connecting."  Yeah.



And so I've got a link in the show notes to their notice.  Fortunately, there is a registry tweak which can end this trouble by redisabling Windows' new and apparently misguided enforcement of this Extended Master Secret handshake.  I have a picture that I took of my screenshot of Registry Editor, where I added two keys.  You add two keys under - and I'm not going to try to go over in detail, but they're easy to add for anyone who's comfortable playing with the registry, as most veteran Windows users have had to learn to be.  It's under Computer, HKEY_LOCAL_MACHINE\SYSTEM\CurrentControlSet\Control, and then SecurityProviders.  Under SecurityProviders is SCHANNEL in all caps.  And there you add two DWORDs, two 32-bit DWORDs for DisableClientExtendedMasterSecret and DisableServerExtendedMasterSecret.  And you set those DWORDs to "1," and that turns off Windows' failing attempts to negotiate an Extended Master Secret with the other end that doesn't know about that.  And, incredibly, Windows refuses the connection if it cannot obtain this Extended Master Secret negotiation.



So I just wanted to give everyone a heads-up.  I mean, my pain is over, thanks to having stumbled upon this.  So a huge and profound thank you to BleepingComputer for pointing this out.  And who knows how long I and doubtless many others would have been stuck without Chrome working on sites that would fail what is now an enforced handshake over on the client side, and presumably over on the server side, since there are options to disable either the initiator of the connection, thus the client, or the acceptor of the connection initiated by a client, so on the server side.



So again, it must be the case that Microsoft realizes, whoops, this is not what we wanted to have happen.  And I wouldn't be at all surprised if next Tuesday we don't have this problem resolved at their end.  But if anybody else has been beset by this, I mean, so the protection would be nice to have.  This does solve a connection interception man-in-the-middle spoofing problem which TLS can have with the ephemeral Diffie-Hellman key agreement negotiation.  So it would be nice to have it.  But despite the fact that this has been in the spec for at least four years, I saw references to 2015 around, I didn't bother to go into big detail, but it's one of those things where, yeah, it'd be nice to have it.  But it sure needs to fail gracefully.  And again, I had a lot of stuff to cover for the podcast, as was obvious at the beginning of the show, so I didn't spend time digging into this.



One of the problems with man-in-the-middle spoofing attack protections is they tend to be subject to downgrade attacks, meaning that the man in the middle can, since they're able to intercept the connection as part of the attack anyway, they're able to say, oh, we don't support Extended Master Secret.  Sorry.  Which is to say that, if the endpoints allowed Extended Master Secret to be soft supported, which is to say only supported when each endpoint agreed, then the first thing a man-in-the-middle attacker would do is claim not to support it, thus disabling it.  So it does make sense that Microsoft wants to force its enforcement, but it's breaking the Internet.  So unfortunately, we're not ready for, obviously, for its enforcement.



So I don't know what's going to happen, you know, because Microsoft tried to bring it up, and it broke a bunch of things for me.  And I can't be alone in this.  So as I said, it'll be interesting to see what happens next week.  And Microsoft knows they did this.  Their advice is, oh, well, update Windows at each end.  It's like, no.  That's just not practical.



JASON:  Yeah, not always an option.



STEVE:  Hello.  And less of an option moving forward as the world leaves IIS as the web server platform of choice.  So it doesn't look like that's going to happen any time soon.  And I was curious, too, because I did go back to SSL Labs.  I remembered that they showed what the server that Digi-Key had identified itself as using was, and now I don't remember.  But it was some cloud provider web hosting platform thing.  So it wasn't like Nginx.  It was something that maybe there isn't that large a market share and so not that many servers are now in a position where they don't support it.  But I couldn't get to a website that I want to get to using Chrome.  So that's not okay.  So anyway, I would imagine this will be of interest to some of our listeners.  So, hope so.



And speaking of Chrome and 77 and 78, 78 is just out, and several things have been fixed.  One is that the thing they did last week with Chrome 77, which turned out to have a problem with Symantec, which was because they were enforcing Microsoft's Windows Defender Code Integrity Checking, which would prevent things not signed by Microsoft from being loaded into the Chrome process, the Chrome renderer.  That turned out to break systems that were using Symantec's Endpoint Protection.  But it turned out that there were more things that it broke, not just Endpoint Protection, but also users were getting the "Aw, Snap" crashes if they used PC Matic, which is that cheesy, advertised on late night TV AV.  But also Palo Alto Networks traps security products, as well as something known as the Print Audit Infinite tool, which I've never heard of, but apparently it's used for tracking document printing on LANs.



So it wasn't just Symantec.  There were other apps which were also broken by this.  Which is to say, for some reason, these things were injecting their code into Chrome's process.  And this Code Integrity Checking allows Chrome to defend itself against that code injection.  And that's good for everybody.  We want that to be on.  But Google has decided to turn it off.  And so with Chrome 78, it is off.  They have said, however, that they intend to turn it back on in the middle of this month.  So those companies that have been alerted to the fact that their solutions are not compatible with Code Integrity Checking need to figure out how to fix that.  Apparently it's possible because we know that, if you update to the newer version, the current version of Symantec's Endpoint Protection, then this doesn't cause that problem.



So I guess this was probably necessary to clearly inform the companies that needed to that they've got to get their act together, or their users are going to be uninstalling their programs from their computers which are causing Chrome to break.  And I would imagine that Google would win this battle of incompatibility, rather than the Print Audit Infinite tool and PC Matic.  Anyway, so that has happened.



Also, last Thursday, on Halloween, which was when Google released Chrome, it's 78.0.3904.87.  And when I saw that I checked, and that's the one I had.  Not only to backpedal on, as I said, that enforcement of this Code Integrity Checking, but every bit as urgently to patch a new Chrome zero-day which they had just been informed of by Kaspersky.  And it was being exploited in the wild.  I have a link to the googleblog.com information about this.



They said:  "The stable channel has just been updated to 78.0.3904.87 for Windows, Mac, and Linux, which will roll out over the coming days and weeks."  Hopefully days.  They said:  "Note:  Access to bug details and links may be kept restricted until a majority of users are updated with a fix.  We will also retain restrictions if the bug exists in a third-party library that other projects similarly depend on, but haven't yet fixed.  This update includes two security fixes."  They said:  "Below, we highlight fixes that were contributed by external researchers.  Please see the Chrome Security Page for more information."



So there are two updates.  The second one was the one that was sent to them by the guys at Kaspersky Labs.  And to Google's credit, they received the information on Halloween, and they had 78 pushed out, or available in the stable channel, that same day.  So they understood this was important, and they fixed it immediately.  Kaspersky's disclosure, although Kaspersky pulls back, of course, from describing the details, Kaspersky's disclosure did have some wonderfully juicy details.



They said:  "Kaspersky's Exploit Prevention is a component part of Kaspersky products that has successfully detected a number of zero-day attacks in the past," and of course we've talked about those.  "Recently, it caught a new unknown exploit for Google's Chrome browser."  They said:  "We promptly reported this to the Google Chrome security team.  After reviewing the proof of concept we provided, Google confirmed there was a zero-day vulnerability and assigned it CVE-2019-13720.  Google has released Chrome version 78.0.3904.87 for Windows, Mac, and Linux; and we recommend all Chrome users update to this latest version as soon as possible."



And as it happens, I had fired up Chrome at the beginning of the work on the podcast yesterday.  And it wasn't till I went to Help About that that gave Chrome a little kick in the pants to update itself.  I was on 77 still, despite having just launched it.  So it's worth making sure the next time you launch Chrome that you've got 78.  So at this point it's very targeted.  It's unlikely that people who are not - I'm trying to think where it was.  Anyway, I'll get to it in a second.  But it looked like it was only a watering hole attack affecting a limited number of people.  But it was an unknown zero-day at the time, so nice that Kaspersky caught it.



They said:  "We're calling these attacks Operation WizardOpium."  They said:  "So far we've been unable to establish a definitive link with any known threat actors.  There are certain very weak code similarities with Lazarus attacks, although these could very well be a false flag.  The profile of the targeted website is more in line with earlier DarkHotel attacks that have recently deployed similar false flag attacks."  So these guys are at the top of their game.  They said:  "More details about this and recent DarkHotel false flag attacks are available to customers of Kaspersky Intelligence Reporting.  For more information, contact intelreports@kaspersky.com."



But the tech details, they said:  "The attack leverages a watering hole-style injection on a Korean language news portal."  Okay, that's what I meant when I said rather targeted.  Unless you happen to be visiting Korean language news portals - now, we don't know that's the only place this is being used.  This is where they found it.  They said:  "A malicious JavaScript code was inserted in the main page, which in turn loads a profiling script from a remote site.  The main index page hosted a small JavaScript tag that loaded a remote script from" - and then it's a code.jquery.cdn.behindcorona.com site.  "The script then loads another script named dot charlie dot," and then they redacted the dot js.



"This JavaScript checks if the victim's system can be infected by performing a comparison with the browser's user agent, which should run on any 64-bit version of Windows and not be a WOW64 process," meaning a 32-bit process running on a 64-bit OS.  "It also tries to get the browser's name and version.  The vulnerability tries to exploit the bug in Google Chrome browser, and the script checks if the version is greater or equal to 65."  The current Chrome version, of course, as we know, is now 78, which is immune to this.



"If the browser version checks out, the script starts performing a number of AJAX requests to the browser's controlled server at behindcorona.com, where a path name points to the argument that is passed to the script xxxxx.php."  Again, they redacted.  "The first request is necessary to obtain some information important for further use.  This information includes several hex-encoded strings that tell the script how many chunks of the actual exploit code should be downloaded from the server, as well as a URL to the image file that embeds a key for the final payload and the RC4 key to decrypt these chunks of exploit code.  After downloading all the chunks, the RC4 script decrypts and concatenates all the parts together, which gives the attacker a new JavaScript code containing the full browser exploit."



So these guys went through, I mean, they jumped through serious hoops in order to obscure and encrypt and protect from observation the JavaScript code for the exploit.  So remember, this thing is checking from Chrome 65, which presumably is where this problem began, anything greater than 65 at the time.  So that suggests this thing has been around and has been effective for some period of time.



"The analysis," they wrote, "we have provided here is deliberately brief due to vulnerability disclosure principles."  In other words, they don't want to give this away while browsers are still in the wild that could be affected by this.  They said:  "The exploit used a race condition bug between two threads due to missing proper synchronization between them.  It gives an attacker a use-after-free condition that is very dangerous because it can lead to remote execution scenarios, which is exactly what happens in our case," Kaspersky wrote.  



"The exploit first tries to trigger the use-after-free to perform an information leak about important 64-bit addresses."  So that's probably, as we know, address space layout randomization deobfuscation.  They said:  "This results in a few things.  If an address is leaked successfully, it means the exploit is working correctly.  A leaked address is used to know where the heap and stack is located, and that defeats the address space layout randomization" - yup, ASLR - "technique.  And, three, a few other useful pointers for further exploitation are then locatable by searching near this address.



"After that it tries to create a bunch of large objects using a recursive function.  This is done to make some deterministic heap layout" - we've seen that before, heap grooming, called "grooming the heap" - "which is important for successful exploitation.  At the same time, it attempts to utilize a heap spray technique that tries to reuse the same pointer that was freed earlier in the use-after-free part.  This trick could be used to cause confusion and give the attacker the ability to operate on two different objects from a JavaScript code perspective, though in reality they're located in the same memory region.



"The exploit attempts to perform numerous operations to allocate and free memory along with other techniques that eventually give the attackers an arbitrary read/write primitive.  This is used to craft a special object that can be used with WebAssembly and FileReader together to perform code execution for the embedded shellcode payload.  After decryption, the malware module is dropped as updata.exe to disk and executed.  For persistence, the malware installs tasks in Windows Task Scheduler."  That's a common means of getting something to run when the system boots or the user logs on.



So in short, from a user's standpoint, with Chrome before 78, and presumably back to 65, if a person were to visit a website, and without them clicking, doing anything, or knowing anything, their Chrome browser would download, install, and run a native application on that Windows machine, which is to say, this is as bad as it gets.  We don't know how long this has been in the wild.  We know apparently it's been effective since Chrome 65.  And it's clear, I wanted to give our listeners a good sense for how much industry went into the creation of this.  This was an expensive exploit to deploy.  It is effective against the world's number one browser on the Internet, which we now know Chrome is.  And it's very clear that anyone going to that particular news portal with Chrome of the appropriate versions, and Chrome is updating itself all the time so nobody would have had an older one, and that is until they update to 78, would get themselves infected.



So bravo for Google fixing this immediately.  Thread synchronization problems are among the most difficult problems to fix.  All the code is working.  Everything's fine.  The fact that they were blasting the heap and stack with allocations suggests that they were trying to obtain a pointer that would function with JavaScript still able to access the region of memory.  So they would have a means then of writing to memory through JavaScript, and then getting execution of that memory through essentially an outside of JavaScript pointer.



So, wow.  Again, as I said, this kind of synchronization problem, they are among the most difficult problems to find.  They are very subtle.  So first of all, somebody had to find this.  Then they clearly put together a heavy-duty campaign in order to leverage this into remote code execution and then set this up on some site.  You can use your imagination.  Who would want to be installing malware into visitors of Korean news portals?  So, wow.  That's where we are today.



JASON:  But at least you convinced me to use Firefox, so it doesn't matter anymore; right?



STEVE:  Very good point, yes.  And maybe some of us haven't been able to use Chrome, thanks to what Microsoft did to Windows last month.



JASON:  Right, exactly, all the signs are pointing away.



STEVE:  That's right.



JASON:  All right.  It's time we step over the Edge into this next segment.



STEVE:  So just a note to our listeners.  This is not a big piece of news, but I wanted to put it on everyone's radar.  The preview build for the new Chromium-based Microsoft Edge Stable version has been released on the Edge Insider site.  This is the preview of the Edge browser that will be made widely available in January.  Yesterday, as part of the kickoff at the start of Microsoft's five-day Ignite 2019 conference, the announcement was made that Microsoft Edge will become generally available and fully released, that is to say, the Chromium-based version, middle of January, on January 15th.



And so what's interesting is with this preview release, Microsoft Edge Stable will be at v79.0.309.7, which actually puts it one major version ahead of where Google's Chrome current stable version is.  As we know, it's at 78.  But on the other hand, Chrome will be past that point, especially if they rev Chrome in the middle of this month in order to turn Code Integrity Checking back on for probably Chrome 79.  So we expect them to do that.



So from an academic curiosity standpoint, it's going to be very interesting, I think, Jason, to see how the world settles out with a version of Edge that's internally identical to Chrome.  You know, same, I mean, they'll both be Chromium.  They'll be from different companies.  They'll have different UI Chrome.  But so will Edge take some of Chrome's market share?  Will anyone know or care what's under the hood of their browser?  I don't think they do now.



JASON:  I don't think they do now, yeah.



STEVE:  No.  So, you know, and so I guess that makes me wonder what's been moving people away from Edge in the first place.  I have no idea.  But it's going to be interesting to see how this develops.  In general, you know, I applaud this move by Microsoft.  It must have been so difficult to discard all of the work that was done on their brand new, very own new browser that replaced the old creaky IE engine.  That was the ChakraCore.  But this clearly was the sanest choice.  Why run independent projects for something that has become a commodity?  Just, I mean, this makes sense.  But do you think that people using Windows 10 with Edge, I mean, they must be using Chrome.  I mean, they must be downloading Chrome and choosing to use it instead of Edge for some reason.



JASON:  I mean, yeah, maybe that's just a force of habit.  If  they already happen to have all their Google accounts, Chrome makes it really, really appealing from the perspective of syncing all of your information, having that be another data point into your Google account.  So, yeah, I don't know, either.  I'm also admittedly not really much of a Windows user.  So I don't know through experience what that would be like.  I know that if I ended up on a Windows browser, like my habit would be to download the Chrome browser because it logs into the Google accounts and because that's just where I have all my data flowing through anyway.



STEVE:  Yeah.  I think that must be it.  And of course, if you are a user of Google Search, Google is actively promoting Chrome.  It's like, are you ready to switch?  Why haven't you switched yet?  Let's switch you right now.  Wouldn't switching be a good idea?  It's faster.  It's more stable.  It's Google.



JASON:  It's what you want.



STEVE:  And so I'm sure that people are like, oh, yeah, I don't want to use this stinky old Microsoft thing that works just fine.  I want the shiny Google version.  So, yeah.



JASON:  Right, right.



STEVE:  So we are two years downstream from our first encounter with microarchitectural data sampling vulnerabilities.  That's the fancy rollup catchall classification name that groups together all of those side channel attacks on Intel and, to a lesser, but still significant degree, AMD and other processors, which as we know started coming to light right from the start of 2018.  I mean, it was podcast one of January 2018 that was about Spectre and Meltdown, which puts it nearly two years ago.  We had, as we know, we first had Meltdown, and then Spectre.  And then they were followed by a few variants and extensions of them.  Then we were hit with PortSmash, then ZombieLoad, then RIDL, and then more recently Fallout.  So there have been a bunch.  And of course these are distinctive from the DRAM hammering attacks which have been demonstrated to be effective in the wild.



So here we are, though, nearly two years downstream of all that, and one glaring aspect remains true, which is despite there never having ever been any sign of any of these theoretical processor failings being leveraged into a practical attack, we have all nevertheless sacrificed around 20% of our previous system performance in order to prevent these attacks that have never materialized.  So of course we can't prove a negative.  There's no way to know whether if the industry had not responded as affirmatively as it collectively did, we would have eventually discovered these problems in the wild.  And we'll likely never know.  Hopefully Intel has learned a lot from all of this.



What they've done so far and what we've done, you know, what they've done with microcode patches and what the OS industry has done by implementing kernel-level embracement of those patches is to, as has been noted by people who have been doing benchmarks, it's now felt that we're paying about a 20% performance price, a penalty for this, just because we've turned off things that were all about optimizations at the  microarchitecture, which it turned out were not safe.



So at this point what we know is that Intel now has a profound understanding of these problems.  And remember, there was some mention of this theoretical possibility early on, which people just sort of ignored because we were ignoring all kinds of things in the early days, which we now soberly understand we cannot afford to ignore any longer.



So Intel knows what the problem is.  We know that their processor design pipeline is very long and deep, meaning which is to say that they start on a design years before it is in consumer silicon.  So it's certainly the case that they immediately scrapped the designs as far back as they could, or as close as they could, and are bringing a whole new architecture out.  But it's going to be years until we see those.



Probably at that time we will get everything we want.  We will recover the performance that we've lost.  We will have designs which are now fully aware of these problems and have new workarounds and probably a whole bunch of new patents filed by Intel and other processor manufacturers about how to not be susceptible to these microarchitectural data sampling problems while still developing for us the kind of performance that we were liking to become accustomed to.  With any luck, we'll have some new chips in a few years that are fast and safe, both.



Okay.  So QNAP.  As I mentioned at the top of the show, I know from many instances of feedback I've seen after the podcast about the file synchronization solutions that I found, that many of our listeners are very happy users of their QNAP brand of network-attached storage devices.  So I wanted to make sure that they and everyone were aware of the fact that many thousands of QNAP NAS devices are currently infected with malware which has been named QSnatch.  Over 7,000 infections have been reported in Germany alone, and the malware is still spreading.  Thousands more, many thousands more, are believed to be infected worldwide in what appears to be an ongoing outbreak.



And thank you.  On the screen right now is a map of known QNAP NAS device infections.  Clearly Germany is just buried.  But so are the coasts.  Doesn't look like there are many QNAP NASes in the more rustic regions of the U.S.  But, boy, in the Pacific Northwest and in Southern California, California in general, it's a lot of problems.



JASON:  Blanketed.  Saturated.



STEVE:  Blanketed, thank you.  Perfect word.  And also on the Northeast is also just you can't see the country underneath that cloud cover.



So information on how QSnatch works is still scant.  The only report comes from the National Cyber Security Centre in Finland, which was the first cybersecurity organization to spot the malware just last week.  NCSC-FI, that's the Finland members, have not yet discovered how this new threat spreads and infects QNAP NAS systems.  However, once it gains access to a device, QSnatch burrows into the firmware to gain reboot persistence.



So an analysis of the malware's code revealed the following capabilities.  We know that it modifies OS timed jobs and scripts, so cronjob and init scripts.  It prevents future firmware updates by overwriting update source URLs.  So it prevents the machine from updating itself.  It prevents the native QNAP MalwareRemover App from running.  It extracts and steals usernames and passwords for all NAS users.



So these features, while describing the malware's capabilities, don't reveal its intent.  So it's unclear if QSnatch was developed to carry out DDoS attacks, to perform cryptocurrency mining, or maybe just as a way to backdoor QNAP devices to steal sensitive files or host malware payloads for future operations.  We don't know yet.  One theory is that QSnatch operators are currently in the phase where they're building their botnet and will deploy other modules to it in the future.  The analysts confirmed that QSnatch has the ability to connect to a remote command-and-control server, to download and then run additional modules.



So this is definitely something you want out of your QNAP NAS, if you're unlucky enough to have had it infected.  And based on the patterns we're seeing, it looks like Europe is buried, and so is the U.S. wherever these things are.  So for the time being, the only confirmed method of removing QSnatch has been performing a full factory reset of the NAS device.  After performing a factory reset, users should install the latest QNAP NAS firmware update, which was last Friday, November 1st.  QNAP released a firmware update which incorporates specific QSnatch protections.  So QNAP themselves know what the problem is.  They've got a firmware update.



I don't own a QNAP device.  I bought a Drobo device back when Drobo was a sponsor of the show, and I have them at two locations, as our listeners know.  So I can't speak to whether the NASes would automatically update, or could, if they haven't yet been infected.  What we do know is that infections are aplenty, as that map showed.  And once infected, your QNAP will no longer update itself.  So I would recommend a manual factory restore and then update.  Because anybody who has a QNAP device will want to be current, and you're going to have to make sure, apparently by manual means, that you get any existing infection scraped out of your machine.



Once you've done that, you'll want to change all passwords for all accounts on the device, since they may have been compromised; remove any unknown user accounts from the device; make sure that the firmware, of course, is up to date, and also that your apps are up to date; remove anything you don't recognize, any unknown or unused apps.  Just as general proper behavior you would want to do that.  Also install the QNAP MalwareRemover application through the App Center functionality.  And, finally, set an access control list for the device so that you've locked this thing down.



We'll note that QSnatch is the fourth malware strain spotted this year that has targeted NAS devices.  There was a ransomware strain that was impacting Synology devices, and there were two previous ransomware strains that were infecting QNAP devices.  So QNAP has had two previous encounters this year.  It really is clear, if you've got your QNAP device exposed to the Internet.  And I should have started by mentioning that also.  I mean, from what our listeners were saying, it looks like they appreciate the functionality of having their NAS, their QNAP NAS on the 'Net.  That is, with a public-facing interface that allows them to access it remotely.  That's the danger, of course.



If your QNAP is only on your LAN and is not exposed to the public Internet, then you never had a problem.  You know, you're not one of those glowing orange circles on the map that we showed.  But for what it's worth, I know that a lot of our users are using these things.  Just take some time as soon as you can to make sure that you have not been infected, and do make sure that you update the latest firmware because this thing looks like it's serious, and we don't know what it's going to do next. 



JASON:  Yeah, sounds ugly.



STEVE:  Yeah.  I mentioned at the top of the show just sort of a - I wanted to put this on people's radar.  And that is we now have a report from Armor, the folks that have been watching and really doing a lot of forensics work on ransomware delivery.  They now have positively identified at least 13 Managed Service Providers who've been positively identified as being the means for having pushed ransomware out to their client companies.  The Managed Service Provider functions as essentially a large access hub for the ransomware.  And once hackers compromise an MSP's network, they then use its remote access tools to deploy ransomware to often hundreds of companies, and that means thousands of computers.



I have a link to the report in the show notes for anyone who's interested in the details.  And I'm not going to go through an enumeration of this.  But we've referred to Armor in the past.  This report adds six new identified managed service providers and cloud-based service providers to the existing list that they had already known of, which brings the total of publicly identified MSPs and cloud-based service providers of just during 2019 to a total of 13.



Chris Hinkley, the head of Armor's Threat Resistance Unit, said:  "This uptick in successful ransomware attacks against MSPs and Cloud-Based Service Providers," he said, "is a harsh reminder that organizations need to ensure that the third-party vendors they do business with are as equally protected against current and emerging cyber threats as they are.  This is especially true because, as we've seen, a successful ransomware attack against a single MSP or Cloud-Based Service Provider can be debilitating to all their customers, as well as to their own company, as the attack can quickly shut down key systems which the customers depend upon to run their organization."



And of course a ransomware attack against an MSP can be fatal, putting an MSP out of business, which appears to have happened with PM Consultants, which is an Oregon-based IT consulting and IT support provider to dental practices.  We talked about the dental practices that got hit.  After PM Consultants was hit by ransomware in early July, they just gave up.  They shut down their business later that month, saying that they were doing so in part due to a devastating event which had befallen their organization.  And we know that their clients were all targets and victims of ransomware.



So our takeaway here is that - and this is the point for our listeners that I think is important.  Any enterprise who is contracting for the services of any outside provider should give serious consideration to the sort of access that provider truly needs to have into their network.  It's entirely natural for any provider to have a full trust in their own capabilities and their own security.  And so they could in full good conscience ask for full and unfettered access into their clients' networks.  And based on the infections we're seeing, that's apparently what they're being given.  But any highly responsible IT manager who is on the receiving end of such an arrangement should give some serious consideration to exactly what sort of access the outside party needs, understanding that something entirely inadvertent on the other end could happen.



And, you know, it's not possible to offer any more concrete advice without knowing exactly what the relationship is and what form of access is really needed.  But we're seeing this new pattern where managed service providers are targets because it allows the malware providers to essentially explode into their entire customer base and get a huge multiplicative benefit from a single infection.  And so from my standpoint it's a little bit like the firewalls of yesteryear and today.  Initially, when you would hook your LAN to the Internet, you would just hook your LAN to the Internet.  It's like, yay, you know, we're a big friendly globe; right?  Well, of course, not any longer.  Now our firewalls are buttoned down tight.  And in fact we've talked about this before.  Original firewalls were default open, and then they would block specific ports they didn't want to give bad guys access to.  There isn't a single firewall around now that uses that logic.



JASON:  It's the opposite now.



STEVE:  Yes.  They are locked down.  They are default closed.  And then you punch little holes, hopefully little pinpricks through the firewall, only to allow a trickle of traffic, in the best case, from specific other sites and IPs that you want to trust, rather than everybody.  A website, of course, doesn't have that luxury.  They typically need to be open to the world.  But so if you have a relationship with a managed service provider, even allowing access from only their IP, that's not going to help you in this case because it's their IP that will be downloading malware onto your network.  You really need to look at the kind of access they need so that they're just not given carte blanche remote control and full software download capability into your network.



Again, without knowing in detail what's going on at, like, the nature of the service being provided, it's impossible to be more clear.  But really, I would urge enterprises that have relationships with third parties to look at what would happen if the third party were controlled by malicious agencies because that's happening.  And they represent a high-value target.  Bad guys are going to work hard to get a managed service provider infected.  And you don't want your MSPs, any of your MSPs, to be hosts of ransomware.  So just to add that caution to everyone's radar.



JASON:  I love this next story, by the way.



STEVE:  Oh, my god, yes.  It's such a great story.  So five months after returning his rental car, the brief renter of the car still has full remote control.  He can track the vehicle, remotely lock and unlock it, and start and stop its engine.  So the story is that when Masamba Sinclair rented a Ford Expedition from Enterprise last May, he was excited to connect with its FordPass.  The FordPass app allows drivers to use their phones to remotely start and stop the engine, lock and unlock the doors, and track the vehicle's precise location.  What could possibly go wrong?



So he's 34 years old, and he told the people, I think it was Ars who was doing the reporting on this, he said:  "I enjoyed it, and logged into FordPass to be able to access vehicle features from my phone such as locking and unlocking and starting the engine.  I liked the idea of it more than I found it useful."  He says:  "The UI looks good and works well."



So today Sinclair's opinion of mobile apps and rental cars is decidedly less favorable.  That's because, five months after he returned the vehicle on May 31st, his app continues to have control over the car.  Despite multiple other people having since rented the SUV in the intervening months, FordPass still allows Sinclair to track the location of the vehicle, lock and unlock it, start and stop its engine.  Sinclair has brought the  matter to Ford's attention, both through its website and multiple times on Twitter.  So far Ford has done nothing to end his access.



He said:  "All it took for me to initially connect was to download the app and enter the VIN" - that's, you know, the VIN number on the engine - "then confirming connectivity through the infotainment system.  There might be a way to disassociate my phone from the car itself," he says, "but that hasn't happened yet.  And it's crazy to put the onus on renters to have to do that anyway."  He says:  "I have had no problems at all and have even unlocked the doors and started the engine when I could see that the vehicle was at the Missoula Airport rental parking lot."  Wow.



So we have in the show notes three of his tweets, the first on June 4th.  He says to @Ford, he says:  "I can still track and unlock the Expedition that I rented last week via the FordPass app.  Huge safety concern for all future renters.  I submitted a solution via Ford New Ideas" - yeah, there's a new idea - "to solve this, and it was denied.  THIS NEEDS TO BE FIXED," he has in all caps.  And then he posts a pic.



The next day he tweets:  "It's day five since I returned my rental, and now someone else has rented it out.  Do I need to start remotely unlocking it until they also start to complain?  Please fix this!"



And it looks like a week later, June 14th:  "I returned this car" - he's tweeting to Ford.  "I returned this car two weeks ago, and you've shown no willingness to allow rental companies to remove my access to unlock it and start the engine.  Maybe I'll just start randomly unlocking it."  Hopefully he doesn't.



JASON:  Hopefully, yeah.  Don't do that.



STEVE:  Yeah, don't do that.  FordPass is offered, as we know, by the Ford Motor Company and is available for both iOS and Android devices.  It's one of several apps for connecting to Ford vehicles.  The less-than-intuitive means for unpairing a vehicle and phone, not to mention the difficulty in knowing a device remains connected, represents a serious security and privacy risk, not to mention to renters, but to people buying a vehicle secondhand.  Imagine you buy it from CarMax or something, and don't know or particularly care that the previous owner still has access.  While Ford said infotainment screens will indicate when a device is paired, it's obvious that multiple Enterprise employees and renters - okay, this was - remember this is, what, six months ago.  Multiple Enterprise employees and renters have continued to miss the warning.  Even now, after the reporter of this discussed the problem with both Enterprise and Ford representatives, Sinclair's access still has not been revoked.



Sinclair said:  "I've been opening the app and tracking the vehicle almost every day to see if my access is still there; and, sure enough, I can see exactly where my old rental, affectionately named 'The Beast,'" he wrote, "is at any given moment.  This means that I can not only find this rental car whenever I want, but I can also unlock the doors and help myself to anything that might be inside."



Since proximity - and this is me thinking now.  Since proximity to the infotainment system was required initially to complete the initial pairing authorization and authentication, and since occasional proximity would be an expected characteristic for any actual car owner, it would seem to me that a useful security tradeoff would be to have a device be forgotten if it hasn't been within physical proximity of the vehicle's infotainment system for, I don't know, what, a week?  Or two?  I mean, it sounds like repairing isn't that burdensome.



And, you know, a car owner, they're going to be in their car every day.  Or maybe they only drive it on the weekends.  Okay, so make it two weeks.  And after two weeks forget the thing.  Or maybe after one week, like, flash a warning and say, you know, "This device has not been near the car for the last week.  Please confirm you want to keep it paired."  So make it an affirmative statement of pairing endurance.  And if you don't do that, it unpairs.  I mean, it's nuts.



JASON:  Yeah, I'm curious about this FordPass thing because this actually reminds me of like a question I've had when I do rent a car, and they have the infotainment system, and I want to play  my music through the Bluetooth.  So that requires pairing.  So then as a renter you end up pairing to this thing.  And maybe you choose not to share your contacts with the system.



STEVE:  I hope.



JASON:  I usually don't share the contacts because that seems like a really bad idea.  But I'm sure a lot of people just say, yeah, whatever, connect it; and then they don't think to remove it.  So, I mean, I guess there should be some sort of mode in these things for rental car agencies for them to - on their turnaround.  Because even what you're talking about, Steve, is somebody rents it the next day, that still doesn't prevent this other third party from locking the doors, turning off the vehicle at the wrong time or whatever.  It's a liability.  There needs to be some easy way for them to be able to deactivate that, if that doesn't exist already.  Maybe it does, and they're just not using it.



STEVE:  Well, or in the rental mode it would be reasonable to turn down the "forget the device" delay to one day.



JASON:  Right.



STEVE:  Because, you know, anyone who's renting a car is in it, like, you know, a lot.  So, and if not, oh, boohoo if your device - I mean, besides, who wants to unlock their doors remotely?  I mean, okay, maybe.  Who wants to start the engine remotely?  Uh, okay.



JASON:  Maybe if it's cold outside.  I can understand that one.



STEVE:  These are all, like, gee whiz doohickey, you know, I have it because the...



JASON:  Yeah, not necessity; right.



STEVE:  Because I'm on the Internet, you know, my car is an IoT. Well, we already know what a bad idea that is.



JASON:  It's very, very interesting that it's taken this long and still nothing.



STEVE:  Wow, yeah.



JASON:  Just seems like the wires are crossed on that one because I would imagine if the right person at Ford knew of that...



STEVE:  Oh, the wires are short-circuited, Jason, no question. 



JASON:  Yup, indeed.



STEVE:  So Chinese-made drones in the U.S. are being grounded.  I have no way of independently assessing the danger that foreign-made drones could pose to U.S. domestic security.  But they are flying everywhere, and they are essentially flying video cameras.  And they're connected to the Internet.  And they're sending data back to China.



So what's in the news is that, pending a security review of these Chinese-made drones, the U.S. Department of the Interior, the DOI, announced last Wednesday that it is grounding all Chinese-made drones and drones with Chinese-made parts until it reviews its drone program.  And the Department of the Interior turns out to have quite a program.  But because it's simply not practical, that decision does not apply to drones, quote, "currently being utilized for emergency purposes such as fighting wildfires, search and rescue, and dealing with natural disasters that may threaten life or property."



So in recent years the Department of the Interior has enthusiastically embraced drones, publicizing the wide variety of ways it has deployed them.  In addition to being deployed for emergency rescues and disaster monitoring, which I think is a cool application, they're used in more expansive long-term projects such as geological surveys and wildlife population monitoring.  According to a 2018 report about its use of drones, the department owned at the time 531 drones and conducted more than 10,000 flights across 42 states and territories.  Okay.  So 365 days in a year, that's three a day.  So, I mean, they're in use.



Other more recent reports - wait, no, 30 a day.  I dropped a digit.  Yes, 10,000 flights on 365 days, that's 30 a day.  So, yeah.  Other more recent reports have updated that number to 800 operating drones.  So since their report in 2018 they've purchased another, what, nearly 300.  The report did not specify what percentage of those drones were Chinese-made.  However, the most popular and capable drones we know are made by the Chinese company DJI.  And in fact two years ago the U.S. Army discontinued the use of drones produced by DJI, who is the world's biggest manufacturer of drones, because of the risk of these vulnerabilities.



And last May the DHS, the U.S. Department of Homeland Security, warned companies that Chinese-made drones could potentially transmit sensitive footage or data to third parties.  And then last month a bipartisan group of lawmakers introduced legislation that would bar all federal agencies from operating drones manufactured or assembled in China.  Because the Interior Department uses drones to survey a variety of critical infrastructure, including mines and dams, as well as to study rapid response situations and emergency routes - and of course we know that these things have GPS in them also so they know what their location is - the information they collect has at least some potential for abuse.



So far to date, scant public evidence exists that Chinese drones have been involved in any large-scale cyberespionage, or that they have backdoors built in that would allow them to be exploited for surveillance.  For their part, DJI has pushed back and argued that opposition to its products is motivated primarily by political hostility toward Chinese companies.  But we all know that, just as with itty-bitty malicious chips hidden on motherboards, even if it isn't being done, it could be done.



So I suppose I'm glad that at least a sober awareness of the risks exist and that they're being taken seriously.  DJI has proposed assembling more of its drones in the United States as well as offering a version of its devices called the "Government Edition," with certain safeguards that would protect information captured and would prevent it from being transmitted wirelessly.  So, you know, just brainstorming a bit, what may develop would be a two-tier system where governments which can afford to pay for more costly, audited, and known clean drones will have that option.



But regular users like wedding photographers can save their money since recording Jack and Jill's nuptials from a height is unlikely to be of great interest to foreign spies.  And DJI could continue to make their lovely hardware, but allow an open source community to add the software, if that is even being done.  I know that there are a lot of drones that have been reverse engineered from a hardware standpoint and where there is now third-party open source software.  So, you know, it just means let's get responsible here as government purchasing agents.  If DJI wanted to salvage their reputation, Jason, they could just make it official, allow the government to take their lovely hardware and put their own firmware in it so they know exactly what it's doing, not make it a closed black box, or in this case a cream-colored four-armed shape.



JASON:  With deadly propellers on top.  Yes, indeed, indeed.



STEVE:  Yeah.  They are nice drones.



JASON:  All right.  Let's get into the controversy here, Steve.



STEVE:  So as I mentioned at the top of the show, DoH remains controversial, not so much with end users, who appreciate the idea that their ISPs and others who might wish to snoop on and possibly filter and block their DNS lookups, will be much less able to do so.  But within the industry, where those who are being blinded to DNS lookups are crying foul and running around stirring up a bunch of FUD - you know, Fear, Uncertainty, and Doubt - it's sort of breathtaking, like, how much kerfuffle there is.  For example, last week Motherboard carried a story with a headline:  "Comcast Is Lobbying Against Encryption That Could Prevent It From Learning Your Browsing History."



JASON:  I remember that, yeah.



STEVE:  So, yeah, so that's what Motherboard is saying.  Comcast is lobbying against encryption, that is to say, wants laws to prevent DNS encryption that would prevent it from obtaining its browser's history.  The subhead reads:  "Motherboard has obtained a leaked presentation which Internet service providers are using to try to lobby lawmakers against a form of encrypted browsing data."  They put that in there to simplify things.  They said, okay, so anyway, for our audience let's remember, as we've previously covered here, Mozilla has indicated that it plans to have Firefox route its DoH queries to Cloudflare's servers.



In response to the flack it's been getting, Mozilla has been explicit that "no money is being exchanged to route DNS requests to Cloudflare" as part of their DNS-over-HTTPS feature that's currently being gradually enabled for Firefox users in the U.S.  And, for example, they further indicated that the only reason they chose Cloudflare is that they are being rigorous about the requirements, the privacy requirements and the no-profit-from-this-data requirements, of anyone that they point their Firefox data to.  And as other DoH providers become willing to make the same pledge that Cloudflare has, then Firefox is happy to use those, too.  I mean, this is so, you know, it's important to understand we're just at the beginning of this.



So detractors have been saying that by using Cloudflare as the default DoH resolver for Firefox, Mozilla will help centralize a large chunk of DNS traffic on Cloudflare's service.  And that is indeed the case.  Let's also remember, as for Google, Google is planning to take a much more dynamic posture.  We've covered this previously.  Google will have Chrome test the user's currently configured DNS server.  And if that service also supports DoH, Chrome will switch over to using it.  This is, I think, a slick solution, since Chrome users may have already manually switched their non-encrypted DNS to some other provider who offers superior value-added services for DNS that they want.  So if that's the case, why not allow it to also be encrypted in addition to having those value-added services.



So, okay.  With that in mind, and Comcast knows all of this, let's look at what Comcast is alleging to lawmakers.  Motherboard wrote:  "The plan, which Google intends to implement soon, would enforce the encryption of DNS data made using Chrome, meaning the sites you visit.  Privacy advocates," writes Motherboard, "have praised Google's move.  But ISPs are pushing back as part of a wider lobbying effort against encrypted DNS, according to the presentation" which Motherboard obtained.  "Technologists and activists say this encryption would make it harder for ISPs to leverage data for things such as targeted advertising, as well as block some forms of censorship used by authoritarian regimes."



Mozilla, they write, which makes Firefox, is also planning a version of this encryption.  Marshall Erwin, senior director of trust and safety at Mozilla, told Motherboard in a phone call after reviewing sections of the Comcast slide deck that "the slides overall are extremely misleading and inaccurate.  And frankly," he said, "I would be somewhat embarrassed if my team had provided that slide deck to policymakers."  And he added, "We're trying to essentially shift the power to collect and monetize people's data away from ISPs and providing users with control and a set of default protections."



So they had one summary slide that I'll share.  And I'm just going to just read the headlines of the other slides.  I think there were 17 total.  So their summary that begins this says:  "Google has announced unilateral plans" - that's in all bold - "unilateral plans, along with Mozilla, which derives over 90% of its revenue from Google."  Now, I don't think that's true anymore.  Remember that that used to be the case, that Mozilla was - I think they defaulted to Google Search.  I mean, we know that historically Mozilla had a relationship with Google.  But I think I remember Leo telling me that that ended years ago.  So that seems, if that's correct, then wow, crazy to put that in there.  Anyway, unilateral plans to activate DoH in its Chrome browser as soon as October, that is, you know, now.



"Google also appears poised to activate DoT for devices using its Android mobile operating system in the near future."  And of course we covered that, as well, at the time.  "If activated, this feature would by default route all DNS traffic from Chrome and Android users to Google Public DNS."  Of course that's not true.  Thus, oh, we've got a big bold chunk here, "centralizing a majority of worldwide DNS data with Google."



And then they said "This charge would mark a" - here we are in bold - "fundamental shift in the decentralized nature of the Internet's architecture and give one provider control of Internet traffic routing and vast amounts of new data about" - one provider control of Internet traffic routing.  What?  "And vast amounts of new data about consumers and competitors."  Uh-huh.  Okay.  Except that Chrome is like, you know, there's like a list of nine non-Google Public DNS servers that they work with out of the box.



JASON:  Don't say that.  No one needs to know that.



STEVE:  Yeah.



JASON:  There's nothing to see here.



STEVE:  "The unilateral centralization," yeah, "of DNS raises serious policy issues relating to cybersecurity."  That's in bold.  "Privacy, antitrust" - oh, those are not bold.  Oh, "national security and law enforcement."  Those are in bold.  "Network performance and service quality, including 5G and other areas."  So, okay.  In the presentation Comcast paints this type of encryption as something that will fundamentally change the Internet and will centralize power under Google.



Comcast wrote in its presentation:  "The unilateral" - oh, this was in the slide that I just read.  Okay.  So I have a link to the PDF of the slide deck for anyone who really wants more pain.  But here are the 11 titles of slides.  The presentation itself is titled "Google's Proposed Public DNS Plans."  Slide one, or one of 11:  "Google's unilateral imposition of default centralized DNS encryption will harm key components of the Internet."  Okay, slide two.  "Google's plans will cause radical disruption."  Oh, geez.



Number three:  "Significant cybersecurity and national security risks."  Okay, what?  Four:  "Privacy risks increase as a single firm amasses more consumer data."  Wait.  Okay.  More than Comcast getting their hands on it?  Okay.  Let's remember that back in 2017 ISPs including Comcast aggressively lobbied Congress to make it possible to sell their users', their customers' browsing data without their consent or knowledge.  Okay.  Five:  "Antitrust and competition concerns."  Wait.  How is DNS a competitive territory?  It maps domain names to IPs.  Six:  "Law enforcement efforts may be compromised."  Okay.



Seven:  "CDN localization will likely suffer, and backbone costs will rise."  Okay, now, I would give them that one.  CDNs do rely on localization, that is, it can be the case that the IP you obtain varies.  But more often it's the CDN localization is performed at the router level, and the IP you get never changes.  So, sorry, number seven.  Number eight:  "Wireless performance, including 5G, will be undermined."  No, it won't.  Nine:  "Parental controls and content filtering may be disabled."  Okay, yes.  If the DNS provider you used was using DNS for parental control or content filtering, and you switched to a different DNS provider that doesn't offer that, then yeah, that will change.  But if your DNS provider offers DoH, Chrome will detect that automatically and just encrypt your queries so that your ISP can't see them, but your service is not otherwise affected.  Ten:  "ISPs and other enterprise services may be disrupted or broken."  What?



JASON:  How?



STEVE:  ISPs and other enterprise services may be, well, okay, there is the issue of enterprises.  But again, Chrome has already anticipated this and will not override non-public DNS queries, so enterprise services continue to be functional.  And 11, the last one:  "Congress should demand the Google pause and answer key questions."



So we've really got to ask ourselves why Comcast apparently places so much value upon their access to their customers' DNS data.  You know, they've got their non-DNS data.  They've got all their data.  So wow.  They really do seem to have their panties in a bunch over this thing.



JASON:  That data's as valuable to them in their eyes as it is to Google or anyone else, I suppose.



STEVE:  Yeah.



JASON:  Yeah, now would be the time to strike as far as that's concerned.



STEVE:  Well, they're doing that, yeah.



JASON:  Yeah, in many ways.



STEVE:  So, and speaking of - nice segue, Jason - time to strike, it took a long time, but the BlueKeep-based attacks have finally started.  And what we predicted on this podcast has finally happened.  The so-called BlueKeep vulnerability in Windows Remote Desktop is being exploited and, as we suspected, not by a worm.  ZDNet's headline was "BlueKeep attacks are happening, but it's not a worm.  Hackers are using BlueKeep to break into Windows systems and install a cryptocurrency miner."  Also what we expected.



The Hacker News says:  "First Cyber Attack 'Mass Exploiting' BlueKeep RDP Flaw Spotted in the Wild."  Threatpost:  "BlueKeep Attacks Have Arrived, Are Initially Underwhelming."  And BleepingComputer:  "Windows BlueKeep RDP Attacks Are Here, Infecting with Miners."  So some friends of ours are in the middle of this.  Over the weekend and across the world, security researchers' honeypots monitoring port 3389 - which is the RDP port, 3389, which is where Windows listens for incoming remote desktop protocol connections - the honeypots began lighting up, actually, and crashing as attempts were being made to leverage the BlueKeep vulnerability for the purpose of running cryptocurrency miners.



On Saturday Kevin Beaumont noticed that multiple honeypots in his EternalPot RDP honeypot network started to crash and reboot.  His pots have been active for almost a year, so they would have been scanned and catalogued by anyone who was planning an attack.  And during all that time, this is the first time they came down.



The first details about BlueKeep being the cause of these events came from our friend Marcus Hutchins (a.k.a. MalwareTech), who investigated the crash dumps from Kevin's machines.  He said that he "found BlueKeep artifacts in memory and shellcode to drop a Monero miner."  According to Marcus's analysis, an initial payload runs an encoded PowerShell command that downloads a second PowerShell script, which is also encoded.  Marcus says that the final payload is a cryptocurrency miner, likely Monero, currently detected by 25 out of 68 AV engines at VirusTotal.



Marcus said that the malware may not be a worm, but that it is mass-exploiting the BlueKeep bug.  This indicates that the cybercriminals are using a BlueKeep scanner to find vulnerable systems exposed on the 'Net and drop the cryptocurrency miner on them.  And in a later update Marcus said that analysis of its network traffic does not indicate self-propagation.



So the server doing the exploitation obtained its target IP addresses before the attack from a predefined list.  This is what we expected, since the exploit is straightforward once its details have been worked out.  And adding worm code just adds additional unnecessary complexity without returning any real value.  Once upon a time, when the Internet was a much more quiet place, a worm would have been launched as a means of obscuring the original attacker's origin.  But on today's Internet, where other machines are readily compromised and used to launch reflected attacks, hiding is far less necessary.  The first public BlueKeep exploit was added to Metasploit two months ago in September.  And Marcus's analysis has confirmed that the same code which was present in the open source Metasploit module is also present in the malware.



In other words, it looks very much as though whoever it was who was behind the attacks did not independently develop the attack, but is using publicly available code resources and made no effort to turn it into a wormable threat, as Kevin's honeypot crashes suggest.  It's not particularly reliable, either, or it wouldn't be crashing these things, it would be installing itself.  Kevin recently noted that there are presently more than 724,000 systems worldwide that are currently susceptible to BlueKeep exploitation.  Three quarters of a million machines now on the 'Net, you know, this thing got closed by Windows Update months ago.  These systems just never update themselves.  They've got RDP exposed and just waiting to get taken over.



So anyway, BlueKeep has happened.  It's going to be Monero.  Someone's going to be making money from Monero mining for them on all these machines that presumably no one is paying any attention to because they're sure not updating them with any useful code updates.



JASON:  There's just too much to pay attention to.  Your show is always just so full of joy and jolly, jolly insight into just how safe the Internet really is.  We're always talking about millions and millions of vulnerable systems.  It's very cheerful.



STEVE:  Get your tinfoil.



JASON:  Steve, excellent stuff here.  And always bringing so much knowledge on these topics.  Really appreciate the work that you do, and keeping people safe, helping keep people safe, anyways.  You mentioned it earlier.  We should mention it again.  The SQRL event is coming up Saturday, November 30th.  And now just a few weeks away because time is weird and going very fast.  So November 30th, 2:15 p.m. Pacific.



I'm not sure if there are any openings still for this.  I think last I checked it was pretty full, but there were a couple of open spaces.  So anybody who wants to sit in the studio for that recording is welcome to email tickets@twit.tv.  And only do that if you plan, if you're like 100% I will be there.  I will be sitting in the audience.  That way someone else who could make it isn't, you know, that opportunity isn't taken from them.  So we definitely want to make sure that people who sign up to be here can actually make it.  So if you can, tickets@twit.tv.  That's Saturday, November 30th, 2:15 p.m. Pacific for the SQRL event.



STEVE:  And I would just add that, if your plans change, and you cannot make it, but you did promise to be there, please also do the same courtesy and just let TWiT know that, whoops, you know, you really wish you could make it, but just, you know, can't get away.



JASON:  Yeah.  It's a great, great point.  Following up would be awesome in that regard.  GRC.com is where you can go for everything Steve, everything that Steve talks about on this show and offers.  Of course SpinRite, which you know all about.  SQRL, like we were talking about, if you want information about  SQRL, and you're like, hey, now that I've read up on it, now that I know a little more, I'll go to the event.  You can find that information at GRC.com.  Also audio and video of Security Now! can be found there, as well as the transcripts of this and other episodes can be found there, as well.  That's the only place that you can find the transcripts.  So if that's what you're looking for, GRC.com.



Otherwise, what you're seeing right now is our website, TWiT.tv/sn.  This is the Security Now! show page at TWiT.tv.  And here you can also find all the audio and video of every episode of Security Now!; ways to subscribe, which is really what you should do.  Just go there, click on the link to your podcatcher of choice, and you'll be subscribed.  You won't even have to seek out the episodes.  They get delivered to you through podcast magic.



We record this show every Tuesday, live, 1:30 p.m. Pacific, 4:30 p.m. Eastern, and we've had a time change this last Sunday.  So is it 20:30 UTC or 19:30 UTC?  I don't actually know.  I forgot to look into this before starting the show.  But check your clock is the best that I can do on short notice.



Thank you, Steve.  Always fun doing the show with you and appreciate the opportunity.



STEVE:  I appreciate it, and I had a great time, and we'll do it again next week, my friend.



JASON:  Absolutely.  Sounds good.  And also thanks to Jeff behind the board.  We'll see you next week on Security Now!.  Bye, everybody.



STEVE:  Bye.



Copyright (c) 2019 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.














GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#740

DATE:		November 12, 2019

TITLE:		Credential Delegation

HOSTS:	Steve Gibson & Jason Howell

SOURCE:	https://media.grc.com/sn/sn-740.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we check in on the developments of the long-term, now working, full consumer jailbreak of iOS devices from the iPhone 4S through the iPhone X.  We examine the strange case of the misbehaving transducer, catch up on the rapidly evolving exploitation of the BlueKeep vulnerability, check out Mozilla's rebuttal to Comcast's attack on DoH, examine the surprising state of web browser support for DoH, and remind Linux and BSD users to refresh their distros after an important flaw was disclosed in a widely used archive library.  Then we take a deep dive into the operation of a newly announced forthcoming solution and standard for significantly improving TLS website certificate security known as "TLS Credential Delegation."



SHOW TEASE:  Coming up on Security Now! with Steve Gibson, I'm Jason Howell, my final week until Leo gets back next week.  Steve's going to deep on TLS Credential Delegation.  Also there's a number of BlueKeep updates that Steve reveals, as well as some good progress on browser support of DoH.  And Checkra1n is now in beta, so you can check it out for yourself.  Steve Gibson breaks all that down, next on Security Now!.



JASON HOWELL:  This is Security Now! with Steve Gibson, Episode 740, recorded Tuesday, November 12th, 2019:  Credential Delegation.



It's time for Security Now! with Steve Gibson, Jason Howell filling in for Leo one last week in a row.  Leo gets back in a couple of days, Steve.  So next week you're going to be doing this show with Leo in the other room.  How's it going today?



STEVE GIBSON:  Who?



JASON:  Yeah, exactly, I know.  That's what I'm saying.



STEVE:  Huh?  Who?  What?  So, well, Leo has delegated the podcast the last four weeks to you.



JASON:  Yes.  I've been honored.



STEVE:  And today's topic is Credential Delegation for Security Now!.



JASON:  I see what you did there.



STEVE:  Yeah, huh.  Number 740 for November 12th.  This is interesting.  This jumps onto a - this is sort of an extension of one of my own long-running issues with secure connections.  All of our longtime listeners know how wound up I have gotten in the past about the failure of TLS, or back then SSL, certificate revocation.  So there's a new game that is beginning to emerge.  Actually it's already in Firefox, which is cool.  By the end of the podcast, our listeners are going to know how to turn that on and test it in their copies of Firefox.



So a lot of stuff happened this week.  We're going to check in on the developments of the long-term, now working, full consumer jailbreak of iOS devices from iPhone 4S through iPhone 10.  It's in its early beta, but it exists.  We're going to examine the strange case of the misbehaving transducer.  Catch up on the rapidly evolving exploitation of the BlueKeep vulnerability.  There's been a lot of news there in the past week.  We're going to check out Mozilla's rebuttal to Comcast's attack on DoH, examine the surprising state of web browser support for DoH.  I didn't realize they all have it already.  And I just stepped on the lead.  Remind Linux and BSD users to refresh their distros after an important flaw was discovered in the widely used archive library which allows remote code execution.



Then we're going to take a deep dive into, as I said at the top, the operation of a newly announced forthcoming solution and standard, it's in the IETF's hands now, for significantly improving the state of TLS website certificate security for a subset of all use cases.  This doesn't solve it for everybody, but it solves a class of a big problem known as TLS Credential Delegation.  So I think another great podcast for our listeners at 740.



JASON:  No surprise.  740, you've got this down.  You've finally got it down, Steve. 



STEVE:  Yeah, figured out how to do this.



JASON:  We were doing the math before the show, going like, 2007?  Was podcast even a term when this show launched?  Like you've been around a long time with this show.  And there's a reason why.  You pack in the knowledge.  So we are going to dive right into everything Steve said up there.



All right.  CheckM8 is official.  Like everyone?  Legit with a capital "L"?



STEVE:  Well, not quite everyone.  I don't know if it's ever going to be legit.  It might be sort of like the anti of legit.



JASON:  Lowercase "l," then.



STEVE:  But our Picture of the Week is a picture of the web page that we showed several weeks ago when we were cautioning our listeners about how to type in the domain name so they didn't go to Checkrain dot com by mistake and download some bad ware.  But what we're seeing now, our Picture of the Week, down there is a link that says "Get the beta now."



JASON:  Nice.



STEVE:  So we have moved into the first - actually, well, it's the first public betas because I checked again this morning.  We have three of them so far.  There was first v0.9, then there was 0.9.1, and now we're at 0.9.2.  Oh, and also it's worth noting that the official site is now Checkra, C-H-E-C-K-R-A dot I-N.  So that's the way they got - they got the Checkra in the .in top-level domain.  So that's where they're aiming people.  And if you were to go to the original Checkra1n where the "I" is a numeral "1" dot com, it just bounces you over to the proper location. 



JASON:  That makes way more sense.



STEVE:  So as we know, this is the public preview of the USB tethering iOS jailbreak, which is based on the unpatchable iOS CheckM8 bootrom exploit which can never be fixed, which Apple can never go back and fix on all the devices up until they found this last summer.  So up to 12 point - I guess up to and including, I think, because now I'm confused, because I did see some - when I was digging into this there was some comment about 12.3.2 and 12.3.3.  But maybe I'm misremembering it, and it was 12.2.  But anyway, it's the devices based on the A5 through the A11 chip, but not the A12 and 13.  And not yet everything.



So, for example, this 0.9.2 beta explains, it says:  "This release is an early beta preview, and as such should not be installed on a primary device.  We strongly recommend proceeding with caution."  Then under "What's new," to give our users a sense for what's going on here, they said:  "Fixed an issue where the Apple Watch would not receive notifications while jailbroken.  Improve reliability of entering DFU mode.  Fixed an issue where Checkra1n could not be used on macOS 10.10."  Oh, it's worth noting that right now it's strictly a Mac-based launch of this.  They'll be getting to other stuff, to Windows and Linux, subsequently.  But at the moment you have to have a Mac in order to do this, this early beta preview.



Then they said:  "This beta adds an option to boot into no-substrate mode.  To utilize this functionality, hold the volume button up when the Apple logo appears until the device finishes booting.  From there you'll be able to uninstall any tweaks causing you issues and reboot to get back into a normal jailbroken state."



Then, and here's the most important part at the moment, unsupported devices.  They said:  "Checkra1n will eventually support all devices between the iPhone 5S and the iPhone X; however, this beta lacks support for the following devices:  iPad Air 2, iPad 5th Gen, iPad Pro 1st Gen."  And then they said:  "Support for these devices will be added in a later release.  Support for the following devices is experimental and may require more attempts than usual:  iPhone 5S, iPad Mini 2, iPad Mini 3, iPad Air."  They said:  "Reliability on these devices will be improved in future releases."  So there are some where they're saying we got it; there are some where they're saying, yeah, we almost got it; and then there are some where, like, no, don't even try this yet.  We'll get you.  And they said:  "This beta only is available for macOS.  Work is ongoing to support Windows and Linux, which will be added in a later release."



Anyway, so just an update.  This is, I mean, for the security industry, this is just a windfall.  Apple has to keep the more recent of those devices current with all of their new patches and fixes.  This means that researchers, both wearing white hats and black hats, will be able to crack into any of these iOS devices which are supported in order to deeply, I mean, like completely, thoroughly reverse engineer what Apple has done.



So, you know, this is like the worst thing to happen for Apple's attempt to keep people out of what they've done.  We know that security through obscurity is not something you can rely on.  But if you can get obscurity in addition to security, why not get it?  Because reverse engineering does help bad guys discover problems that have not otherwise been discovered.  And this makes iOS, until at some far future point, the iPhone 10 goes out of update cycle, it makes it deeply reverse engineerable.



So the expectation is, I mean, this doesn't represent any kind of a security problem for your typical end user, except if someone were to take your phone into the back room, use this to jailbreak it, and then you were unaware that that had happened, it would be a way for border agents entering a hostile country to get deep access into an iOS device if you were unaware.  So certainly this says, because this cannot survive a restart, that if you are traveling into a country where you have lost control of your iOS devices even briefly, it's worth completely powering down and restarting it clean because what this has done is it has made those devices always jailbreakable with a USB connection to the device in order to make this happen.  And it's open.  It's going to be widespread.  I mean, again, this has been a real hit to Apple security, not something they're happy about, but not something they have any control over.



JASON:  Yeah, that was my follow-up question is what could they really do in this?  They've been pretty good at kind of keeping things like this at bay.  But this really just sounds like such a - blowing the lid wide open, you know.



STEVE:  Yeah.  Jailbreaking had been, I mean, it's the whole concept of secure boot, where the ROM verifies the signature of the kernel before it loads it into RAM.  Then it transfers control of the kernel.  The kernel verifies the signature of all the other stuff that it's loading piece by piece.  And nobody but Apple is able to sign any of this.  Well, that's gone now.  I mean, so Apple was able to use the secure boot chain to absolutely guarantee to its users, and we like that, that when we start our phone up - and the stuff that's running in our phone has been signed and has been verified all the way through startup.  And then we know that any apps that we are running are having their signatures verified also.  That's the part that breaks, which then allows - means that malware can be installed, can be implanted on our iOS devices by anybody who has access to the device.



So Apple doesn't like this, I mean, doesn't like that compromise of their security guarantee that until now they've been able to offer all of their users.  Nor do they like the fact that everybody, security researchers, can now easily sift through their code looking for as-yet-unfixed problems.  And we know there are as-yet-unfixed problems because Apple is constantly fixing problems which were unknown until they fixed them.  And in fact this also means that, in the same way that Windows patches have been, are being immediately reverse engineered and turned into relatively short-lived exploits, so too can Apple patches be.  When Apple fixes something, since Apple rather lazily rolls out these iOS updates, somebody can get it, figure out what changed, reverse engineer it, find the problems.  If it's useful to a bad guy, they can then exploit it until such time as the targets get themselves updated to newer versions of iOS.



So, yeah.  And it was only recently that we got the stronger iOS auto update feature in our iOSes.  So the older ones of iOS still covered by working devices may not be updated as quickly as the newer ones.  So, yeah.  And here it is.  It's gone from CheckM8 to now we actually have what we were expecting, which is the first working exploit, which anybody can download.  And the guys who are doing this are having a ball.  They're going to, just because they can, they're going to make it robust and bulletproof and reliable and working on all of the devices within the range.  So, very cool.



JASON:  Very interesting, yeah.



STEVE:  Very, very, yes.  Maybe not so cool as it is interesting.



JASON:  Yeah, yeah.  I'm really intrigued by this next story, as well, because we talked about this a little bit last week on This Week in Google.  And I'm curious to get your take on this.



STEVE:  You know, I saw that news also before last week's podcast, and I thought, eh.  But upon digging into it more, I thought, okay, this is kind of fun.  In one of the weirder pieces of research - and I titled this "The Case of the Misbehaving Transducer."  In one of the weirder cases of recent research, from a researcher with Japan's University of Electro-Communication - there is such a place - and four researchers with the University of Michigan, we have their paper titled "Light Commands:  Laser-Based Audio Injection Attacks on Voice-Controllable Systems."  It's like, wait a minute.  Laser-based audio injection?  Huh?



So, okay.  By definition, a transducer is any device that converts energy from one form to another.  That's the dictionary definition of a transducer.  Typically, it converts a signal in one form of energy to a signal in another.  But it doesn't have to.  But, for example, in the case of a signal, we all know that a speaker is a transducer that converts electrical current into movement of a cone, which then reproduces sound.  And going in the opposite direction, a microphone is a transducer that converts incoming sound into movement and then into electricity.  An example of a non-signal transducer is a light bulb, which converts electric current into light; and solar panels, going the opposite direction, converting light back into electric current.  So transducers.  One form of energy into another.



And in the past we have encountered some weird transduction behavior such as the famous case of shouting at an array of hard drives, which increased their read error rates and reduced their measured data throughput.  That way we had fun.  There was like a YouTube video of some guy who looked like he'd lost his mind, screaming at a RAID array.  But sure enough, it upset the hard drives.  They were functioning as audio transducers, even though they were certainly never meant to be that.  There was another instance where a whole datacenter crashed because the incredible loud fire siren whistle thing went off, and hard drives all failed in there because this thing was so loud, and hard drives have become so twitchy about any mechanical perturbation because their density's gotten so high that they've just, you know, the engineers have engineered out all the margin for error.



So now we have another oddball transducer failure.  As they describe it in their 17-page paper, they said:  "We have identified [what they described as] a semantic gap between the physics and specifications of MEMS" - which is the acronym for micro-electro-mechanical systems - "microphones, where such microphones unintentionally respond to light as if it were sound."  They said:  "Exploiting this effect, we can inject sound into microphones by modulating the amplitude of a laser light."  In other words, these microphones are light-sensitive as a weird side effect.  Who woulda thunk?  And you've got to wonder who even thought of trying this.  Let's aim a laser at our Amazon Echo and see what happens.  What?  So, okay.  So think about this.  Someone located hundreds of meters away could tell your Amazon or your Google voice assistant to unlock your front door or open your garage door.  That's not good.



So in their paper's Summary Abstract, they wrote:  "We propose a new class of signal injection attacks on microphones based on the photoacoustic effect, converting light to sound using a microphone.  We show how an attacker can inject arbitrary audio signals to the target microphone by aiming an amplitude-modulated light at the microphone's aperture.  We then proceed to show how this effect leads to a remote voice-command injection attack on voice-controllable systems.  Examining various products that use Amazon's [Echo], Apple's [you know who], Facebook's Portal, and Google Assistant, we show how to use light to obtain full control over these devices at distances up to 110 meters and from two separate buildings.



"Next, we show that user authentication on these devices is often lacking or nonexistent, allowing the attacker to use light-injected voice commands to unlock the target's smart lock-protected front doors; open garage doors; shop on ecommerce websites at the target's expense; or even locate, unlock, and start various vehicles" - and their examples, a Tesla and a Ford - "that are connected to the target's Google account.  Finally, we conclude with possible software and hardware defenses against our attacks."



Anyway, I'm not going to go into this in any greater detail since everyone gets the idea here.  But if we were to re-express this in more classic security terms, to understand why there's this problem, we would say that the lack of authentication that's present in today's voice command systems represents a rational tradeoff between convenience and security when used in the typical environment where it's not possible for a random stranger at a distance to inject their own authenticated voice commands into those devices.



So this lack of authentication is obviously not normally a problem, but it becomes one when voice commands can be injected at a distance.  So this falls into the category of it's unlikely to be a problem even today, really.  I mean, it's an interesting attack, but I don't think everyone needs to immediately throw their Google device under a black scarf, although might not be a bad idea.



JASON:  Or not in front of a window.



STEVE:  Right.  You make a good point.  But of course future devices whose microphones are inadvertently light sensitive might wish to add proactive illumination blocking, since that could presumably be easily done at no cost.  And our current crop of voice command devices currently obviously lack that.  So it's not something anyone considered.  It would be simple to spray a little black paint on the inside of the cavity where that microphone is located so that it could still, you know, still have an acoustic input, but would be blocked by light.  That could be done for no cost.  It's just not something anyone thought of before.  So I imagine, because it won't cost anything to do it in the next rev, I mean, the list of devices, I don't think there was one that wasn't attacked by these guys.  I mean, it's every one I'd ever heard of, and some I hadn't heard of.  So, you know, just - this is, as I said, oddball, but certainly intriguing.



JASON:  Oddball, but not entirely without at least some form of concern.  Like you point out, being able to give a voice command that unlocks your door, like that's actually a pretty big deal, I suppose.



STEVE:  That's not good.



JASON:  But then at the same time, who would really go through the trouble?  Like I saw the equipment that they used all splayed out onto the floor, and I was just fascinated by what they were able to do with the transducer and everything to kind of convert the signal and everything.  It's going to take a really motivated individual.  But maybe that's the case for a lot of security flaws.  It's not that it's difficult.  It's still possible, and therein lies the flaw, so you've got to do something about it.



STEVE:  Well, and of course, if someone really wants to get into someone's house, you just throw a brick through the window.  I mean, it's not...



JASON:  Well, there is that, too, yeah.



STEVE:  It's not like, you know, there's no other way to get into a house other than unlocking the front door.  So, you know.  And you might say, oh, well, but what about in a really secure facility?  Well, then, a really secure facility has no windows.  My Level 3 datacenter, where I've got GRC's servers, there's no windows.  I mean, it's a bunker.



JASON:  Yeah, that's very true, yeah.



STEVE:  Yeah.  So anyway, we have to remember that security is the weakest link, and someone's window is always the weakest link for a break-in.  Although the reason it's interesting, the reason we worry about these sorts of attacks over the network is it allows somebody in a foreign country to attack you at distance.  This doesn't allow that, either.  They can't get you from Russia with a laser beam.



JASON:  Yet.



STEVE:  Yet.  So anyway, it's certainly fun to look at the consequences, the actual security consequences of a transducer transducing, or transducting, a form of energy that it wasn't expected to, and how we presume that the transducer's only going to convert what we want.  And in this case, not so much.



JASON:  Maybe I'll move my Google Home out from the window in my kitchen, just to be safe.



STEVE:  Well, you don't want it to be in the heat.  I wouldn't want it to be in direct sunlight.  That's not good for electronics.



JASON:  I didn't even think about that.  So far so good.  It's been there a couple of years now.  It's doing okay.



STEVE:  Okay.  So last week's podcast was BlueKeep and DoH.  And we've got follow-ups on both of last week's headline topics.  Microsoft, their security team believes with good reason, and we'll get to that in a second, that more destructive BlueKeep attacks are on the horizon.  And they are once again, for the third time, urging users and enterprises to apply patches if they've been lagging.  I think we all know by now that if anyone was going to update any of those more than 700,000 Windows systems which are still right now exposing vulnerable RDP protocol that's susceptible to BlueKeep takeover, it would have happened months ago.  Those systems would have been updated.  Nobody's going to update those systems, apparently ever.



So I think that these repeated Microsoft warnings must be for political CYA cover, rather than for any practical purpose.  You know, they want to be able to say, oh, well, we warned everybody multiple times.  We told everybody.  We even patched old Windows XP.  We did everything we possibly could.  I mean, this has to be why they're doing this because they just know, they must know that nobody's listening who's in charge of these 700,000 Windows machines, amazing as that is.



So as we covered last week, we're only seeing BlueKeep leveraged to install cryptominers so far.  As ZDNet summed things up in their coverage, they said:  "Many security researchers considered the attacks underwhelming and not living up to the hype that was built around BlueKeep for the past six months.  This was because Microsoft said BlueKeep" - this is still ZDNet talking.  "This was because Microsoft said BlueKeep could be used to build wormable - self-spreading - malware.



"However," writes ZDNet, "the attacks that happened over the weekend" - that's what we talked about last week - "did not deploy malware that could spread on its own.  Instead, attackers scanned the Internet for vulnerable systems and attacked each unpatched system, one at a time, deploying a BlueKeep exploit, and then the cryptocurrency miner.  This was far," they write, "from the self-spreading malware outbreak that Microsoft said BlueKeep could trigger.  Furthermore, in many cases, the BlueKeep exploit failed to work" - well, we'll get to that later because that's been fixed - "crashing systems.  But Microsoft says this is just the beginning, and that attackers will eventually refine their attacks, and that the worst is yet to come."



So anyway, Microsoft said last Friday:  "While there have been no other verified attacks involving ransomware or other types of malware as of this writing, the BlueKeep exploit will likely be used to deliver payloads far more impactful and damaging than coin miners.  We cannot discount enhancements that will likely result in more effective attacks."  So, okay.  Microsoft tells us again that we all need to update.  All of us who can and heard this certainly did already.



So here's an interesting tidbit.  We've been hearing a lot about how the attempted exploitation of BlueKeep has been crashing systems.  And recall when we talked about this last week that it was when Kevin Beaumont first noticed the BlueKeep exploitation, well, he first noticed the BlueKeep exploitation because the servers of his honeypot farm were crashing.  Now we know why.  At this time the only public proof-of-concept exploit for the BlueKeep RDP vulnerability is a module for the Metasploit penetration testing framework.  That module was in turn assembled from a proof-of-concept code which was donated earlier this summer by RiskSense's security researcher Sean Dillon, Sean who tweets as @zerosum0x0.



So as we know, although the exploit code works, it has had the tendency to crash the systems it's targeting, rather than delivering a remote shell to the attacker, which is what it does when it doesn't crash.  And as I just mentioned, it was when 10 of Kevin's 11 RDP honeypots crashed that the world first became alert to this BlueKeep-based campaign.  Of course, we often quote Bruce Schneier's astute observation that attacks never get worse, they only ever get better.  And eventually, recent events with BlueKeep have followed that path.



This past weekend the BlueKeep Metasploit module was fixed to entirely eliminate the source of the BlueKeep Blue Screen of Death (BSOD) crashes.  You're not going to believe what it was that was originally tripping up BlueKeep.  It was Microsoft's patch for the Meltdown flaws in Intel's processors.  Those systems that were never patched for the Meltdown flaws did not crash.  Those that were patched, crashed.  And certainly lots of those have been patched, 10 out of Beaumont's 11.  Quoting from an interview that Sean Dillon gave over the weekend, he said:  "From looking at screenshots of the analysis Marcus Hutchins did" - of course we know Marcus as MalwareTech - "we know code execution was achieved, but that the honeypots were crashing because the exploit did not support kernels with the Meltdown patch installed," said Sean.



The original exploit that he created incorporated what he called a "KVA Shadow mitigation bypass."  Those with good memory may recall that KVA Shadow is the official name, Microsoft's official name for the patch for Meltdown.  But another researcher pointed out that there was a way to bypass the need for the system call hook that was causing the trouble in the first place.  So Sean updated the exploit, which has now been posted to GitHub and incorporated into the updated Metasploit framework which now changes the way this attack occurs so that a Meltdown bypass is no longer needed.  This raises the early BlueKeep patch to now a fully weaponized state.  It is 100% reliable and no longer triggers BSODs.



So, and I have a link in the show notes for anyone who is interested.  There's also a lot of discussion on the various developer forums about this.  So essentially in the course of one week, after we saw that this was being used and crashing these kernels, that's been fixed.  We now have a 100% reliable non-BSOD-triggering BlueKeep exploit.  So if systems were crashing as a result of the implementation of the first exploitation, they will no longer.



And Marcus Hutchins, again, MalwareTech, the guy who, as we know, famously stopped the WannaCry Internet worm in its tracks by registering that bizarre domain name when he noticed from reverse engineering of the worm that it was pinging that DNS, making a query for the DNS, he's weighed in on the issue of a BlueKeep-based worm.  He posted the following tweet thread last Friday, which I'm going to share in its entirety since he raises a number of very good points and because he obviously knows what he's talking about.



So his first tweet said:  "When the news broke about BlueKeep exploitation in the wild, most of the reactions were basically, 'It's not a worm, so it doesn't matter.'"  He said:  "I decided I'd do a thread on why that's wrong, and why a worm isn't even a worst-case scenario."



So he then generated a thread of tweets which read:  "There are two main purposes of a worm."  And he has in parens, "(self propagation).  First, dealing with cases when there are too many vulnerable systems to infect reliably with just scanning alone; and, two, dealing with a large disparity between the number of external- and internal-facing vulnerable systems."  He says:  "The WannaCry worm served both these purposes.  Firstly, there were just too many vulnerable systems to infect by scanning servers."  In other words, yes, it was a target-rich environment.  It was too rich.



"Secondly," he says, "if a network had SMB" - you know, Server Message Blocks, Windows file sharing - "exposed, then the chances that every single device on that internal network was vulnerable were very high."  In other words, if the worm got a foothold on an exposed border server, it could pivot to the LAN and scan internally in order to find probably all the internal systems, which was really what made this thing so destructive is WannaCry penetrated networks.



Then Marcus says:  "BlueKeep is different.  Not only is the number of externally facing vulnerable machines low enough to infect with a couple of servers, but also RDP is only enabled by default on Windows Server operating systems.  Because Windows clients don't expose RDP by default, unlike SMB, a BlueKeep worm wouldn't be able to pivot to systems within a network like WannaCry did.  Furthermore, I'd guess it's fairly likely that, if one of the network's RDP servers is exposed to the Internet, they all are.



"For all these reasons, a BlueKeep worm would not be hugely effective and not at all like WannaCry.  They might infect marginally, not exponentially, more systems; but the downsides are huge.  A worm would not only attract a lot of attention, but be technically challenging due to the limitations of BlueKeep.  The exploit" - and he's writing this on Friday before this got fixed.  "The exploit is both unstable and non-generic."  He says, parens:  "(The attacker would need to somehow fingerprint the OS and exploit accordingly)."



He says:  "Building a worm in a way that doesn't just repeatedly crash every BlueKeep vulnerable system would be challenging."  And of course we know that's been fixed now.  He says:  "And by no means worth the reward."  He says:  "I'm not really worried about a worm.  What I'm worried about is something that could already be happening.  Most BlueKeep vulnerable devices are servers.  Generally speaking, Windows servers have the ability to control devices on the network.  Either they're domain admin, have network management tools installed, or share the same local admin credentials with the rest of the network.  By compromising a network server, it's almost always extremely easy to use automated tooling to pivot internally."  He says:  "Example, have the server drop ransomware to every system on the network."



He says:  "The real risk with BlueKeep is not a worm.  A worm is pointless and noisy.  Once an attacker is on the network, they can do far more damage with standard automated tools than they could ever do with BlueKeep alone.  Remember all those news stories about entire networks being ransomwared.  That starts with a single system being hacked.  Not even a server.  A normal, non-admin, client system.  Attackers don't needs worms.  It was just convenient in the case of WannaCry/EternalBlue.  People need to stop worrying about worms and start worrying about basic network security.  Firewall your servers off from the Internet."  And maybe he meant Intranet.  He must have meant Intranet.  He said Internet.  Because of course a server on the Internet that's firewalled off from the Internet can't be attacked externally.  He says:  "Learn about credential hygiene.  Occasionally worms happen, but every day there are entire networks compromised using only standard tools."



JASON:  That's a good point.



STEVE:  So I obviously think he's exactly right.  We know that since the beginning I had said I didn't think BlueKeep made sense to use with a worm because Shodan already knows, it already has a list of all of the available RDP servers, which have already been scanned many times now to find the ones that are exposing RDP, that has not yet been patched and is vulnerable.  So the bad guys already have a list.  And, you know, once upon a time you would launch a worm like the original Morris worm when you wanted to hide your identity from the Internet.  But now people take over other people's machines in order to perform proxy-based scanning.  So you don't even need to hide anymore.  So, yeah.  I think he's right.



JASON:  I would agree.



STEVE:  So a bunch of great points.



JASON:  Absolutely.  Smart guy.



STEVE:  And Jason, we're going to talk about Mozilla and DoH versus Comcast.



JASON:  All right.  Circling back to DoH.  Got a few pieces of news here.  Lots of, like, themes in today's show.



STEVE:  Yeah, we're a theme show.  I did just want to - while you were delivering that, I was thinking that I just wanted to make sure that people understood what this means, that we now have a very reliable, non-crashing BlueKeep exploit which will allow a reverse shell to be obtained by anyone on the Internet who wants it for 700,000-plus Windows servers.  I mean, that's huge.



JASON:  Wow, yeah.



STEVE:  That's just, you know.  And as Marcus says, we don't need a worm.  Those machines are sitting there waiting to be commandeered and occupied by bad guys who then sit there and look around and see what they're connected to and what sort of goodies are on the LAN on the inside.  So this is, I mean, this is bad.  And so Microsoft is warning people, as I said, I think because they have to.  But it's been months now, and these systems are not getting themselves patched.  They may be running cryptominers today.  They're going to have bad guys setting up shop and looking around to see what other mischief they can get up to shortly.  It's just a matter of time.  So, wow.



You know, and I just realized there's another name for Comcast.  I should have - we should be calling them "Concast."



JASON:  Concast.  You can't be the first to come up with that one.  But it makes sense.



STEVE:  Mozilla versus Concast.  I was really glad to see Mozilla take the leaked slide deck which we discussed last week and push back hard with their own clarification and much more accurate letter to Congress.  Their letter was signed by Marshall Erwin, who's the Senior Director of Trust and Security for Mozilla.  I've got a link to it, if anyone is interested.  And I'm not going to go through the whole thing.  But I will share enough of the beginning of the letter so that our listeners get a sense for Mozilla's take and their position and because they really raised some very good points.



He wrote:  "Dear Chairs and Ranking Members."  And this was an open letter addressed to Congress.  "We are writing to express our concern about the privacy and security practices of Internet service providers (ISPs), particularly as they relate to the domain name services provided to American consumers.  Our recent experience in rolling out DNS-over-HTTPS (DoH), an important privacy and security protection for consumers, has raised questions about how ISPs collect and use sensitive user data in their gatekeeper role over Internet usage.



"With this in mind, a congressional examination of ISP practices may uncover valuable insights, educate the public, and help guide continuing efforts to draft consumer privacy legislation. During the last two years, Mozilla, in partnership with other industry stakeholders, has worked to develop, standardize, and deploy DoH, a critical security improvement to the underlying architecture of the Internet.  A complementary effort to our work to fight ubiquitous web tracking, DoH will make it harder to spy on or tamper with users' browsing activity and will protect users from DNS providers - including ISPs - that can monetize personal data.  We believe that such proactive measures have become necessary to protect users in light of the extensive record of ISP abuse of personal data, including the following incidents."  And then they highlight four goodies.



"One, providers sold the real-time location data of their mobile broadband customers to third parties without user knowledge or meaningful consent.  In one particular case, an intermediary was found to be selling particularly sensitive GPS data, which can pinpoint the location of users within a building for over five years.  Two, ISPs have repeatedly manipulated DNS to serve advertisements to consumers.  Comcast has previously injected ads to users connected to its public WiFi hotspots, potentially creating new security vulnerabilities in websites.  And last year, CenturyLink injected ads for its paid filtering software and disabled the Internet access of its users until they acknowledged the offer.



"Verizon tracked the Internet activity of over 100 million users without their consent through supercookies that could not be deleted or circumvented.  This allowed Verizon to closely monitor the sites that users visited and catalogue their interests without their knowledge.  And, finally, AT&T operated a program that required users to pay an extra $29 per month to opt out of the collection and monetization of their browsing history for targeted ads.  While the company ended the program after public criticism, it has considered reviving it in the current deregulated environment."



So they said:  "Unsurprisingly, our work on DoH has prompted a campaign to forestall these privacy and security protections, as demonstrated by the recent letter to Congress from major telecommunications associations.  That letter contained a number of factual inaccuracies.  These have been examined in detail by others and as such will not be given an in-depth treatment here. Nonetheless, it is important to highlight the underlying premise of that letter.  Telecommunications associations are explicitly arguing that ISPs need to be in a position to collect and monetize users' data.  This is inconsistent with arguments made just two years ago regarding whether privacy rules were needed to govern ISP data use.



"With the 2017 repeal of the Broadband Privacy Order, a substantial gap in consumer privacy protection was created.  That gap exists today.  ISPs are people's gateway to the Internet.  That gateway can serve as a data collection point, providing ISPs with unique access to sensitive browsing information.  That is why broadband privacy rules would have required ISPs to get the clear consent to use and share their subscribers' information.  However, those rules are no longer in place."



And I'll conclude my quote of this, although their letter went on:  "Our approach with DoH attempts to close part of this regulatory gap through technology and strong protections for user privacy.  Mozilla's policy establishes strict requirements for potential Firefox DNS resolvers, including requiring that data only be retained for as long as it is necessary to operate the resolver service, that data only be used for the purpose of operating that service, and that partners maintain a privacy notice specifically for the resolver that publicly attests to data collection and policies.  Unfortunately, ISPs do not maintain privacy notices for their DNS services.  As a result, their policies are opaque to users.  It is unclear what data is being retained, how it is being used, or who it is being shared with." 



So bravo to Mozilla for pushing back on the nonsense that we read in what Concast shared with Congress and in this campaign  that they are pushing for this kind of, well, just basically saying we don't want this to happen, and we're not happy, is what it boils down to.



JASON:  Now, lucky for Comcast, Concast is actually a business.  It exists.  So they're like, ha ha, you can't even get us with that one, Steve.



STEVE:  Although who would want that name?  Who'd want to call themselves Concast?  I don't know.  



JASON:  I know.



STEVE:  Not one I would go for.



JASON:  Yeah.  It's like - I'm trying to figure this out.  It's like construction solutions.  It's completely unrelated.  Anyways, I was very curious.  I was like, this has got to be out there somehow.



STEVE:  So what I found, somewhat to my surprise, is that despite ISP associations' hissy fits, DoH will be rolling out for all major browsers.  In the wake of all this DoH noise, ZDNet took it upon themselves to interview the product managers of all six major browsers.  In alphabetical order they are Brave, Chrome, Edge, Firefox, Opera, and Vivaldi.  And this was where I was a little surprised.  Although I guess in retrospect I shouldn't have been because so many of them are now Chromium based.  The good news is every single one of these top six browsers already has or soon will have DoH support.  All of them.



Brave, the Brave browser.  Tom Lowenthal, Product Manager at Brave for Privacy & Security, told ZDNet:  "We absolutely want to implement it.  Implementing DoH is far more than just the technical work, though.  We need to decide on sensible and protective defaults for the vast majority of people who don't think about their DNS configuration while making sure that we don't break things for the people and organizations who have carefully tuned their setup."  Because Brave is built on top of Chromium, which of course as we know, well, Chromium's open-source browser codebase, DoH support is available today.  However, the Brave team has not yet tweaked the feature so that it works exactly the way they want to, so it's not enabled by default.  But it's there.  You can go to brave://flags/#dns-over-https and enable it right now, if you're a Brave user, if you choose to.  So bravo for Brave.



Chrome, of course, as we know, does have it available.  It's not yet enabled for everyone by default since Google is currently running a limited experiment with a small number of users to see how DoH fares in a real-world test.  And as we've noted, they're taking an adaptive approach, first attempting to honor the user's existing DNS provider to see whether it supports DoH, and using it, if possible.  If not, then it follows a number of sort of heuristic rules of thumb to figure out what it should do.  And again, in Chrome, you say chrome://flags/#dns-over-https if you want to take control of it yourself and turn on DoH right now.



Similarly with Edge.  Even Microsoft told ZDNet that they were supportive of DoH, but they couldn't share their exact plans.  So, yeah, it sounds more like Microsoft.  However, like Brave, and as we know, the soon-to-go-mainstream, in mid-January, Chromium-based version of Edge already supports DoH.  And the preview versions do.  Same flag:  edge:// and then the rest.  And you can turn DoH on in Edge right now.  There are some additional tricks, command line switches and things, and I have links to a blog posting by one of the Edge developers, if anyone is a committed Edge user and is interested in getting going with DNS-over-HTTPS in Edge right now.



And of course, as we know, Firefox was the first out of the gate with DoH and as a consequence took some undeserved, in my opinion, arrows in their back for simply standardizing upon Cloudflare as their DoH provider.  No one really took the time to understand how rigorously Mozilla had vetted Cloudflare.  And many people who don't listen to this podcast might mistakenly believe that Cloudflare is just some other CDN.  But anyone who has the class to erect a large wall of lava lamps and use the video image of those lava lamps to generate true random numbers definitely stands out as an innovator in my book.  And of course we know that Cloudflare did that.  And they have met all of Mozilla's requirements that we noted above in their letter to Congress of really respecting user privacy and absolutely never using it for any sort of monetization scheme.  And since Firefox is officially supporting it, you can turn it on in the standard browser UI.



Opera has already rolled out DoH support.  It is disabled by default for all users, but it can be enabled at any time in the stable release.  And it works without going through any additional steps.  They achieved the "no additional steps" in the same way that Firefox did, just by standardizing on Cloudflare's 1.1.1.1 DoH resolver.  And I'll note, though, that as Opera users probably know, Opera builds in a popular VPN.  It and DoH, the VPN in DoH are not compatible with each other.  On the other hand, if you're using a VPN, you already have a privacy encrypting tunnel to route all of your browser's queries.  So assuming that DNS goes through the VPN tunnel, I hadn't thought of that until just now.  That would be worth checking to see whether Opera's DNS with the VPN up is going through the tunnel or not.  I don't know either way.  In Opera, it's opera://flags/opera-doh in order to get to its configuration.



Apple, with Safari, was completely mute.  ZDNet was unable to obtain any reply from Apple about Safari one way or the other.  But they are certainly privacy focused and user feature privacy focused.  So I think once the dust settles on DoH, Safari will probably follow suit in what will end up becoming probably the industry norm for browser-based DNS queries.



And lastly, Vivaldi.  It, too, is Chromium based.  So it also works like Chrome.  And its flags are like the other Chromium-based browsers:  vivaldi://flags/#dns-over-https.  So among many of Comcast's misassertions was their focus upon Google.  I guess Google makes a convenient soft target lately.  But as we know, and as this list just demonstrates, despite Comcast's assertions, it's not just Google, but the entire browser industry that is heading to DoH just as fast as development cycles and field trials will allow.



So I think it is very clear.  ISPs ought to be reading the handwriting on the wall, which is that browsers are going to be privacy protecting their DNS because they can.  And so I think what will happen is we'll have ISPs, I'm sure right now they're scurrying to bring up their own DoH support on their own DNS servers so that then they, too, will be able to be the DoH provider for browsers, which will return them to having an ability to see into their customers' DNS queries.  At which point hopefully browsers will make it easy to say, you know, I want to override my ISP's DNS.  I want to go to a trusted provider that is not going to be monetizing me without my knowledge or permission.  So I'll bet you that we see that before long.  And again, there are many providers of DoH beyond Cloudflare.  Mozilla just decided to settle on them because they were able to get the commitment from someone they trusted.  And I certainly trust Cloudflare, as well.  And I think our listeners could, too.



JASON:  Absolutely.  Absolutely.  And is there much that the ISPs could really do?  I mean, at this point, if all the browsers are implementing it, Comcast is just relying on their hopes that the government might somehow step in, in some way, shape, or form, and prevent this.



STEVE:  It's true.  There's nothing that they can do.  I mean, they could block it.  But that would be the end of life.  That would be, yeah.  That's not going to happen.  So they can't see into it.  I mean, it's worth noting that DNS was a simple way to see what people were looking up.  But people are still using the IP addresses they get from DNS to make the connections.  So it's more work for an ISP to use the IP addresses and reverse lookup those IP addresses to see who they're connecting to.  And it's not also, you know, many hosted sites will share many domains across a single IP.  So you can get some confusion.  There is some ambiguity if you use IPs.  But it's still something that ISPs will always have access to, unless their customers use a VPN, which then shields that, too.  But that seems unlikely.



So, yeah.  ISPs really can't do anything.  I think what we're going to see them do is, since Chrome is the majority browser, since it defers to using the preconfigured DNS resolver by default, ISPs are going to bring up their own DOH resolvers in order to once again capture this information, and sort of rendering this whole exercise moot.  But users who can override and choose to override their built-in DNS - and maybe Firefox will continue to say, hey, we're using Cloudflare.  We want to protect you from ISPs even after they bring up their own DoH services.  Which Firefox's strategy does; Google's strategy doesn't.



JASON:  Right.



STEVE:  So that'll be interesting.



JASON:  Yes.



STEVE:  And this little tidbit, this was just a throwaway.



JASON:  I know.  You're speaking to me with this one, by the way.



STEVE:  Oh, no kidding.  Interesting.  I noted to our listeners when I saw this appear.  We noted on this podcast when Microsoft decided to add the user's Downloads directory to their handy little Disk Cleanup applet in Windows 10.  And I specifically cautioned our listeners to be careful with that one, since on the one hand many people are in the habit of forgetting their Downloads folder and just allow it to grow without end.  I happened to note the other day that Lorrie's is like, I don't know how many gigs of stuff.  Everything she has ever downloaded is in that folder.  Which, I mean, I'm not kidding you.  And I keep thinking - I keep looking at it and thinking, oh, this hurts.  But I'm not going to mess with her machine.  And as a consequence it grows without end.



And many of today's downloads are massive things that just sit there and squat on space.  So I could see, as a consequence of that, I could see the logic of having Downloads in the Disk Cleanup applet.  And in this era of multi-terabyte storage, many people have taken to using their Downloads directory, kind of more or less deliberately, as an archive, a massive archive of everything they have ever downloaded.  And they assume that everything will always be there.  So Microsoft apparently discovered that too many people were selecting, simply going through and selecting all of the Disk Cleanup category checkboxes without looking and were, as a consequence, permanently and inadvertently deleting their download archives.  So that feature, which was first added into last year's ill-fated - and it then became infamous - October 2018 Windows 10 update, is now being removed.



So in the future, if you want to delete stuff from your Downloads directory, you'll need to do it yourself.  And really, I think that does make the most sense.  Disk Cleanup should be for those obscure OS-level things like in the list:  DirectX Shader Cache.  Okay.  Delivery optimization files, whatever that is.  System error memory dump files.  Good, get rid of it.  And Windows Update Cleanup.  I looked as I was putting my notes together last night.  I've got a gig of that on the machine that I was using last night.  So good riddance.  After I'm sure that I've updated and everything is stable and so forth, get rid of it.  That crap accumulates that no one needs.  Sort of like OS belly button lint.  So I can understand that being removed.  And so what happened with you, Jason?  You got bit?  



JASON:  Well, no, no.  It was just as I was reading through this I was thinking about how I use my Downloads folder, or rather how I don't...



STEVE:  It is convenient to go back and get something sometimes.



JASON:  I mean, yeah, it is.  I don't know if it's the most secure approach.  But, yeah, I kind of compile things in my Downloads folder.  I don't think of anything that's in my Downloads folder as permanent.  But I do keep things there indefinitely.  And, you know, it might be years down the line where I, like, do a search for something, and what do you know, it's already in my downloads folder instead of me having to go out and download it again.  So it comes in handy.



STEVE:  It's very much like when you go to Amazon to buy something, and it says, "You purchased this on such and such."  And I go, oh.



JASON:  Oh, did I?



STEVE:  I've got that around here.  I've got that around here somewhere.



JASON:  Or you download something, and then you realize the file that's been saved is a (2) or a (3).



STEVE:  Yes.



JASON:  And you're like, oh.  I guess I already did this.



STEVE:  Yup.



JASON:  But, yeah.  So I'm just - I never thought about how I use my Downloads folder, whether it's a good or a bad idea.  But, yeah, it's like a growing collection.  Every once in a while I'll jump in there, I'll be like - I'll get, like, creative or get a cleaning kind of a bone in my body and clean it up a little bit.  But I end up keeping things.  But then I don't think about it as permanent storage anyway.  So I don't know.  If software on my computer suddenly just assumed that my Downloads folder was a place for Disk Cleanup to automatically wipe away, I think I would be a little bit upset if I didn't realize that, and then suddenly it was empty.  I'd be wondering what I was missing.



STEVE:  And I'm going to stop making fun of Lorrie.  I just looked at mine:  3.576GB.



JASON:  Oh, really.  Interesting.



STEVE:  Okay.  Oh, who's talking, 3.576.  Yeah, thank goodness I don't have that checked on my machine.  Whew.



JASON:  But the question is, could you, without looking at the contents of your Downloads folder, empty it right now?  Would you be uncomfortable with that?



STEVE:  Uncomfortable, yes.



JASON:  Uncomfortable.  So would I.



STEVE:  There's definitely - I'm sure there's things.  Now, what I told her I thought we should do for her, and now I'm thinking, okay, Gibson, take your own advice, is just grab the whole thing and drag it over to the Drobo so it's there.



JASON:  Right.



STEVE:  And it's not on, I mean, like right now I'm making images of all that crap every single time I do an image of my whole system, which I have automated.  And that's dumb.



JASON:  Yeah.  Yeah, it's true, moving it over to some sort of archive or whatever.  I tend to collect files on my desktop, as well, at home.



STEVE:  Oh, I can't find my desktop.  I can't even see it.  It's just covered with little squares.



JASON:  I don't go that far.  But at some point it gets messy enough that I'm like, all right, I don't have the time to pull this stuff out now.  But I do want to save it.  So it's like, goes into a folder called "My Desktop" and the date.  And then that goes into some archive somewhere.  And then I keep it forever and never turn to it again.



STEVE:  Yes.  Jason, on the wall behind me there are books.  I actually - I have books.



JASON:  What are those?



STEVE:  You know, like "Programming the VGA."  And it actually turns out...



JASON:  When was the last time you pulled out that book?



STEVE:  Well, it actually was for SpinRite 6 I needed to do some VGA - because I had to program it myself.  But that's my point.



JASON:  Still, they look really nice.



STEVE:  Well, here's - I'm looking at a book on CGI.  Yeah.  And Microsoft AFC.  Yeah, I could really do with some housecleaning.



JASON:  That's okay.  No, keep it.  Keep it.  It helps your studio stay soundproof.



STEVE:  That's true.  That's true.



JASON:  That does help.  It does help.



STEVE:  It's not a hard surface so much.



JASON:  That's right.



STEVE:  So one last little tidbit for our Linux and BSD users.  The compression library included by default in Debian, Ubuntu, Gentoo, Arch Linux, FreeBSD, and NetBSD distros - and others, probably all of them - contains a vulnerability that can allow hackers to execute code on user machines.  It's not an across-the-network vulnerability, so it's not like a BlueKeep thing.  But last week, even though this thing's been known since June, last week details about this major bug impacting essentially Libarchive, which is the archiving library that all of these OSes use, went public.  But it was held embargoed until all the Linux and FreeBSD distros had rolled out updates containing patches for the version of Libarchive they had been shipping.



So of course we know a decompressor is an interpreter because it interprets the tokenized compressed representation of a file and reexpands it based on the tokens that it receives in the byte stream of the compressed contents.  It is extremely difficult to find every possible flaw, and decompressors especially tend to be even more difficult than other interpreters because they must inherently operate with fewer constraints.  They just sort of have to believe the data that they're being given.



So this bug, which is being tracked as CVE-2019-18408, not surprisingly allows an attacker to execute code on a user's machine if the user's machine attempts to decompress a deliberately malformed archive file.  So exploit scenarios would involve the reception and decompression, you know, like viewing something from email, receiving something in email, maybe downloading something from the 'Net one way or another, from a web or whatever.  It's not something that allows a remote attacker to get you.  But if you run something which has been deliberately created to exploit this flaw on a Linux distro or FreeBSD or any BSD that has not been updated, you could get owned.  So as I mentioned, it was originally discovered and patched back in June.  The vulnerability was identified by Google security researchers using some automated code testing tools, one they call, and I'm not kidding, ClusterFuzz and OSS-Fuzz.  It was patched in Libarchive 3.4.0.  I went over to Libarchive.org and noted that 3.4.0 was released in June, on the 13th of June.  So it has indeed been out there for a long time.



Our takeaway for everyone is that now would be a good time, if you haven't for a while, to resynchronize your OSes with the latest repository updates.  You may already have version 3.4.0.  You might just do a - presumably you could do a man Libarchive and see what version of Libarchive you have on your system.  As long as you're at 3.4.0, you're okay.  If not, you want to update because your system is vulnerable should you receive something.  And it's not going to be a widespread attack.  It's not going to be a spray.  There are no known exploits in the wild, though they would likely be targeted.  And so you don't want to be a victim of that kind of targeting.  So just make sure you have v3.4.0, which as I said has been around for a year, I mean since June 13th.  So you may well already have it.



JASON:  And now a deep dive on Credential Delegation.  Tell us what this is all about.



STEVE:  So as we know, web servers prove the domains they control by providing a certificate which has been previously signed by a certificate authority whom the client knows and trusts.  That certificate will contain an enumeration of one or more Internet domains and/or IP addresses and the server's previously assigned public key.  During the TLS handshake, the client challenges the server with a random nonce, which the server must sign using its matching secret private key.  The client uses the public key in the signed certificate authority certificate to verify the server signature of its message containing the nonce.  And if the signature verifies, the client can then be sure of one thing, that the server is indeed currently in possession of the private key that matches the public key in the certificate that was signed by the trusted certificate authority.



So that's how our certificate system works today.  But what if the server's private key is ever stolen?  That's, as we've often talked about it, that's the Achilles heel of this system.  And it's such a significant and intractable problem for our industry that I and this podcast have spent a great deal of time through the years exploring the problem and sharing the problem and its various solutions with our listeners.



And in terms of the scope of the problem, it's one thing for me to keep GRC's private key secret.  I have one 1U high rack-mounted server that's as well shielded from external network penetration as I've been able to make it.  That physical server is in a locked rack to which only I and Level 3 have access.  As I mentioned, Level 3's facility is a bunker.  Neither Sue nor Greg have ever had any reason to have access, so I'm the only one who knows it, who has access to it.  That server is in a locked, heavily monitored and guarded, super-secure facility, and there are no other copies of that private key anywhere in the world.  So it's about as secure as I can imagine.



But consider the situation for Facebook or Cloudflare or Google or any other massive Internet provider.  They have thousands of servers spread across the globe in hundreds of datacenters.  And they have hundreds of thousands of employees.  And sure, many fewer than all their employees need to have access or do have access to those servers.  But certainly still a great many.  We recently heard that Twitter had discovered a couple of spies in their midst.  So that's a problem.  And so, too, is keeping an absolute secret, which is what that private key must be kept, an absolute secret across thousands of networked servers around the world.  You know, mistakes happen.  We cover them often on this podcast.  This is, you know, it could be called "Security Mistakes" as well as "Security Now!."



And so we know that robust security is inherently always a best effort.  We make the best effort we can to eliminate all the bugs in our software.  When they are revealed, we fix them, hopefully, as quickly as we can to minimize the damage.  So the point is that resiliency in the face of a breach, which is to say recovering gracefully and quickly, is often the best we can do.  So when a server's private key escapes through whatever means and for whatever reason, the best thing that can be done is to immediately revoke its authority to represent the property for which it was signed so that it cannot be used to impersonate any of those properties moving forward.



All of our long-time listeners to this podcast will already know about revocation, this process of revoking a certificate's authority.  Revocation's utter failure and total implementation collapse has been one of my personal crusades and hobbyhorses for years.  Everyone will recall that I invested a great deal of time and trouble to demonstrate just how utterly broken - nonexistent, really - Google's Chrome browser's CRLSET revocation handling has always been.  I went so far as to deliberately create a revoked certificate and set it up on a revoked site to demonstrate that Chrome would honor it on every platform, despite the fact that it was clearly revoked and should never be honored.  What did Google do?  They added a special case exception for that particular revoked certificate.  So I changed the certificate, and they gave up because they realized I would just change the certificate again.  The point is Chrome has always been broken.  They don't have revocation.



Now, since all of these certificates, all of the certificates that are signed, as we know, have an expiration of two to three years for public domain TLS certificates.  They will eventually take themselves out of service by self-expiring.  So it's only the still valid by date certificates which we wish were no longer honored that we need to somehow suppress.  So the first solution, the first of many bad solutions, was CRLs, Certificate Revocation Lists.  Certificate authorities were supposed to publish a list of all the certificates they had issued which had been revoked, but were otherwise still valid.  And then anyone relying upon a certificate authority's certificate was supposed to check that list before trusting the certificate.



But as it turned out in practice, this didn't work very well.  Certificate authorities were slow to update their lists, when they even bothered to at all.  Those lists would become quite long and thus took a while to download.  So browsers didn't want to be doing that all the time.  And so the relying clients that were relying upon the certificates weren't much better about pulling and checking the lists.  So even when things were being checked, which wasn't really that often, so much so that it's just kind of broken completely, even in the best case there would be a significant gap from the time somebody, a CA would revoke a certificate, and it would actually be known to be revoked, if ever.



So the next thing that we came up with was OCSP, stands for Online Certificate Status Protocol.  That was a nice solution that was intended to close this large window of time between a certificate's revocation and a client's awareness of that having happened.  But that system, too, has flaws that we've discussed in the past.  Checking the status of every TLS certificate takes additional time that no one wants to spend.  So you go to a site.  You obtain a certificate from the browser.  Now the browser has to look at the certificate, find in the certificate's metadata the URL of that certificate, the issuing certificate authority's OCSP server, make a connection to the OCSP server, querying it with the OCSP protocol, for the updated status of the certificate that it's just received.



The reason you do this is that it might be a bad guy who has sent you the certificate.  It has since been revoked, but the certificate itself hasn't expired because it could last three years.  And so this gives you a real-time means of getting the current revoked status of the certificate.  But that's an extra connection.  Some CAs' OCSP servers, certainly in the beginning, were slow.  Sometimes they never replied.  They were offline.  And so then the browser doesn't get an affirmative okay.  What does it do?  Well, it's going to either fail open or closed.  That is, it's either going to fail and trust, if it can't get an affirmative denial, or it's going to fail closed and refuse a connection.  But the certificate might be valid.  It just can't verify that it's valid.



So if the browser chooses to fail in the trusting direction, that means that a bad guy can block the OCSP query.  I mean, presumably if a bad guy is in a position to spoof a remote server anyway, that sort of suggests they're in the position to intercept the user's traffic.  And so they could block the query to the OCSP server, knowing that the browser will fail in the trusting fashion, and then the bad guy wins.  So OCSP has its problems.



For all these reasons, the one correct and perfect solution is OCSP Stapling.  And when we last looked, Nginx had some flaky behavior with OCSP Stapling.  I love this solution.  Now what happens is the server which is sending the certificate staples  a recent OCSP assertion to their certificate and provides it to the client.  The client looks at the certificate, says oh, look, I have the OCSP assertion, which is fresh, that is, maybe as fresh as a few hours old, which has been signed by the certificate authority.



So what that does is it shifts the burden of providing a fresh assertion onto the site that you're already connecting to, getting the certificate from.  And that makes a lot of sense.  The server periodically goes out and updates its own OCSP assertion, which it sends out with all the certificates.  And, for example, GRC does that.  So my sense is, moving forward, once this is well supported by all the servers across the 'Net, it really represents a good solution.  But we're still not quite there, and there are some use cases that this doesn't fit perfectly.  And, well, for example, the one that we're about to address here.



Lastly, though, there's something that's happened since.  If the web server is going to be taking the trouble to periodically obtain a fresh assertion of its certificate's validity from its certificate authority, why not just allow the server to obtain a completely new and fresh certificate with a short lifespan?  And that of course is one of the features of the industry's latest innovation with the so-called ACME protocol, A-C-M-E, which stands for Automated Certificate Management Environment.  As we know, ACME first appeared with the Let's Encrypt movement, and they were followed by other CAs, including now my favorite one, DigiCert, that also offers ACME-based certificates.  ACME gives us a sort of workable compromise.



But there's a frightening tradeoff between safety and failure.  Let's Encrypt's certificates are valid for 90 days.  So that's a lot better.  Which is to say they have an expiration date which is when the certificate is issued, no further than 90 days in the future, the idea being that, since it's an automated system, it's easy to update the certificate before then.  But still, that's three months.  So it's a lot better than three years of validity, but it's still a large open window.  If a recently issued Let's Encrypt certificate were to be compromised, that compromised certificate will have an expiration date nearly 90 days away.  So during that period of a time a great deal of damage can be done.



If we were to automate the issuance of very short-duration certs, like hours instead of months, then the good side is, the upside, is that, if one of those were to be leaked, it's a very low value.  It's going to die almost before the bad guy is able to get set up and ready and try to use it.  But then, with very short duration certificates, we face the possibility of the dynamically issuing certificate authority suffering any sort of problem that could interfere with our refresh.  They might be DDoSed or suffer a network outage of some sort, during which all of everybody's short-duration certs would expire and could not be replaced, which would lead to a catastrophic loss of service.  So that's no good, either.



The solution to this problem is a new, in-the-works system formally known as Delegated Credentials for TLS.  It's been quietly in the works for the past three years by Facebook and Cloudflare, who have this problem, and Mozilla, who is over on the client side.  Cisco also had some involvement.  And this has just surfaced:  Delegated Credentials for TLS.



Now, because it requires support from our browsing clients, if you think about it, that's the one thing that Let's Encrypt doesn't require.  Let's Encrypt needed to be trusted in the beginning.  So as we know, they were using cross-signed certificates.  So I think it was Global Trust or somebody was signing, or Global Sign, somebody was signing their certs in the beginning until the Let's Encrypt root was readily available in all browser clients, as it is now.



The point is that none of those earlier solutions required client-side support.  This does.  This is an evolution of TLS v1.3 to add an extension to the protocol which does need awareness in the client.  And as I mentioned at the top of the show, Firefox has it now.  And our listeners can play with this immediately.  So because it requires web browsing client support, it's going to take some time to become fully deployed.  But it does offer a new and very useful tool that would be practical when dealing with these high-risk scenarios involving many thousands of servers that all need to keep a secret.



It wouldn't be of any use to me, for example, where I've got one server with one cert that is locked away as securely as I've been able to make it.  But in the case of Facebook or Cloudflare, where they've got to have certificates spread around in order to quickly terminate TLS connections, yet they're at risk of that facility being compromised or a server being compromised in any way, they need a way of offering short-duration attestations, or in this case delegations, of their longer lived credentials.  And that's what this system provides.



We already have the well-established concept of a certificate hierarchy, where the CA signs typically now an intermediate CA certificate, just so that their grand master can be locked away in deep freeze.  Typically an intermediate certificate is there.  It then signs the certificate which is being presented to clients by the web server.  And the idea is that this is a hierarchy because each certificate is signed by the one above it.



What this new delegated credential's extension to TLS 1.3 offers for the first time ever is the ability for the end certificate to delegate its credentials to a short-duration certificate.  That is, essentially the likes of Facebook and Cloudflare would be able to sign this credential delegation themselves and include in that the public key for a short-lived private key.  The browser would use the public key in that credential rather than the public key in the parent credential, which the delegated credential-using server would no longer honor.  It wouldn't even have the matching private key.  It only is able to have the public key.  It would sign that credential and offer it to the server.



So, okay.  So let me step back a little bit.  The web browser client needs to be updated.  Firefox is.  When you enable its credential delegation, all of its connections have this delegated credential acknowledgment added to the Client Hello packet, which is the first thing that goes off to the remote web server as it's doing its TLS handshake.  That tells the server that the client is up to speed and is aware of the operation of credential delegation.  So the server that it's connected to does not have the master credential.  It does not have the private key which the ultimate certificate authority issued.  Instead, it has a short-lived certificate which it received from a master server at Facebook or Cloudflare.



So it's sort of like a CA within the organization which is able to delegate its credential for a short period of time without ever giving out its private key.  That is, that private key never leaves headquarters.  Only the short-lived credentials which are signed by it leave.  And they contain a similarly short-lived private key.  So the credential contains a public key which the browser will use to complete its handshake, an expiration date for this credential, and the signature of the delegated credentials which were signed by the server's certificate which never leaves headquarters.



So this re-uses the proven strength and integrity of the existing certificate hierarchy by allowing content providers themselves to provide the last link in the certificate chain for the issuance of these short-duration TLS certificates.  And there are a couple interesting little twists here.  First of all, there is no provision for revocation.  They're not even trying anymore to revoke.  There's no provision for the revocation of these delegated credentials.



On the other hand, they are hard-limited to a maximum validity duration of seven days.  And any client must, as part of the protocol, must and will refuse any delegated credential that had a life longer than seven days.  So when the client gets it, you know how all certificates have a not-valid-before and not-valid-after dates, a date of first valid and last valid.  If that is any wider than seven days, the certificate is deemed invalid.  So no one will bother issuing those because no clients will accept them.



And so that means that essentially we've come up with a solution.  And who knows.  Maybe - it's not like seven days is what these guys will actually choose.  It's that they can.  That is, the spec says they could be up to seven days.  It may very well be, once the system is up and running and is proven, and they've got backups for their master, their grand master, and so forth, they may choose to set them for four hours.  I mean, they can be whatever they want them to be up to seven days.



So this is really cool because it solves the problem of the CA being offline, that is, the problem that Let's Encrypt might have of relying upon an external party over whose network you have no control by getting long-lived certificates from them, and then keeping it within the family, keeping it within your own local network, essentially, with a network you do have control of, where a single master server, which keeps the master key for one of these major content providers, is then able to reissue its own very short-lived delegations so that it's never exposing its master private key.  It's only exposed essentially a working key good for however long the provider chooses to have it be true for.



So this is already in Firefox.  I think it is a forthcoming IETF standard, been well designed.  I've got links to the standard in the show notes, and something our users can try.  If you are a  Firefox user or have access to Firefox, and you're curious, go to about:config.  We know that the result is an overwhelming list of tweaks and options and settings.  So you want to use the search bar and just put in "delegated," or "delegate."  That's enough.  I put in "dele," and it whittled it down to about six different hits.  But if you put in "delegated," you get one.  You will see there Firefox's support, right now, for this credential delegation.  Mine was turned off, so I double-clicked it to flip it to "true" to turn it on.  Then I went to "fb," as in Facebook, fbdelegatedcredentials.com.  That's a nice little test site which anyone can use:  www.fbdelegatedcredentials.com.  And if everything works, you'll just see a notice that acknowledges that the server and your browser have just successfully negotiated a TLS v1.3 connection successfully using delegated credentials.



So as I said, this doesn't make sense for single server scenarios where you already only have your key in one place.  It's not really buying you something.  I mean, I guess I could see you might put the key somewhere else, and have that even more secure, and have your server reach out for an updated delegated credential for it to then resend to all of the clients connecting to it for some period of time.  Yeah, maybe.



But, boy, this is a beautiful solution for what we've seen evolve on our globe, which is these massive content providers that are globe spanning, that want to have local points of presence for their service offerings in datacenters not far away from their customers, yet they've got this management problem of being a catastrophe to be issuing, to be using certificates with multiyear expirations against the danger of one of those private keys, any one of them anywhere, getting lose, and the revocation system being so broken that it might as well not even - no one should have bothered with it in the first place.  Which actually, comically, is the path that Chrome took.  It was like, well, it's broken, so we're not going to bother.  It's like, well, okay, but it has been useful.  Anyway, a very cool solution.  And now all of our listeners understand it.



JASON:  So and then, as you spelled out, Firefox already has support.  You can flag that on.  Chrome going to circle back around on this?



STEVE:  Oh, no question.



JASON:  No question, it's just a matter of when.



STEVE:  And as a consequence of the fact that it's pretty much Firefox and Chromium, what it means is that as soon as the Chromium code gets it, all the other browsers get it. 



JASON:  Yeah, that's true.



STEVE:  Edge, Opera, Brave, you know.  And I'm sure that Safari will do it.  I mean, and of course you do need a fallback for when a client doesn't support that.  Then it just doesn't know about delegated credentials.  It'll go, what the heck?  So there will be a phasing over time.  But here we are, I mean, it's not difficult for it to support it.  It's a matter of the client adding that to its Client Hello handshake.  You know, Firefox is all open source, too.  And the Chromium guys and Mozilla are always sharing stuff.



So, I mean, it's probably already in the works in some early build of Chrome, if it's not already there.  I don't know for sure either way.  But it's as simple as a little bit of logic to add the "I know about delegation" flag on the Client Hello packet.  And then, when you receive the response, you look in a different place.  You don't use the public key that is in the certificate that you received.  It will not be honored.  You use the public key which is in the delegated credential, which is signed by the public key, you know, the master public key.  And so you use that public key to verify the validity of the delegated credential.



So again, it's hard to describe in language.  I hope I've done so for our listeners.  But basically a relatively simple change for our clients.  The good news is in the industry we really only have three now:  Firefox, Safari, and Chromium, which is an umbrella for all of the others.  And then we're going to have this pretty quickly.  And as soon as it's ubiquitously present, then Facebook and Cloudflare and the others will be able to withdraw their private keys from all of their end node servers and instead start having them only serve very short-term credentials.  And what it really means is that they're not going to be attacked because they won't have a credential worth attacking.



JASON:  Right.



STEVE:  So it also lowers the pressure on getting one of those prized private keys out of one of those servers.  It's a very cool, like next-generation change.  So bravo to those guys who put this together.



JASON:  Yeah, it's definitely progressive.  I love it.  And someone in chat, there was a little bit of confusion.  It's fbdelegatedcredentials.com, not fb.delegatedcredentials.  So all one word.



STEVE:  Yes.  One long...



JASON:  And that should take you there and check it out later.



STEVE:  Yes, good, thank you, fbdelegatedcredentials.com.



JASON:  Right on.  Interesting stuff.  That's something to look forward to.  And, yeah, great show, Steve.



STEVE:  Well, our listeners have been wanting a little something meaty for a while.  So I figured, okay.  We'll wind them around that one.



JASON:  Love it.  Also, just a totally unrelated aside, I was noticing as we were reaching the end of that doc, I was looking at your SQRL logo, and then also on your mic.  It just kind of, you know, that's a really brilliant logo.  I have to say I really like it a lot because the longer I look at it, the more I realize what was probably obvious to so many people, how the tail, yes, looks like a lock, but also is in the shape of an S for SQRL.  And I don't know, whoever did your logo design for SQRL is doing a really a great job.



STEVE:  It was a really gifted, very temperamental Spaniard who - I used the LogoSauce.com site, which I've used them a number of times.  They're a great site.  But, boy, this guy, he just, I mean, I used the word "hissy fit" earlier.  And, you know, he would get all bent out of shape and go away, and he was worried that other people were stealing his ideas because I was trying to move the whole community in the right direction.  And anyway, he ended up, I ended up paying him for it, and I'm very happy for it.  And of course I put it in the public domain.  It's everyone's to use who is doing SQRL things.  But anyway, thank you.  I'm glad you like it.  I am just so pleased with it.



JASON:  It was just in that moment that I realized, like I've always seen the lock and the tail, but it was that moment that I realized, wait a minute, it's also an S.  That works for Steve.  That works for SQRL.  This is amazing.  And of course speaking of SQRL, we do have the SQRL event coming up November 30th.  It's just a couple of weekends away.  Saturday, November 30th, 2:15 p.m. Pacific is when that's going to take place.  And I'm sure that's going to be live streamed, of course, TWiT.tv/live, if you can't be here in-studio.  If you can be here in-studio, I'm super not aware of whether there are even seats left at this point.  But tickets@twit.tv.



If you know for certain that you can make it, email tickets@twit.tv so you're not taking that opportunity from someone else who could possibly do it.  And let them know.  Inquire within, and they'll let you know if there's still space.  I have not heard this past week whether that has filled up entirely or not.  But that's Saturday, November 30th, 2:15 p.m. Pacific if you want to hear Steve here at the studio talking all about SQRL and everything related.  So looking forward to that.



Go to GRC.com for everything that Steve is doing.  SpinRite, of course, the hard drive recovery and maintenance tool that's amazing, and you can get your copy there.  Information about SQRL, if you want to do some reading about that and research about that.  Audio and video of this show can be found there.  There you go.  There's SpinRite.  ShieldsUP!.  Audio and video of this show can be found there, as well as transcripts of this show.  If you rely on transcripts, you can find it only at GRC.com.



And then if you go to our site, it's TWiT.tv/sn.  There you're going to see Leo in the banner image, who will be back on this show next week.  But you'll also find audio and video of all of the episodes, all the ways to subscribe, all the information that you need just a click away by going to TWiT.tv/sn.  We record this show every Tuesday live at 1:30 p.m. Pacific, 4:30 p.m. Eastern.  And I checked the UTC today.  It's 21:30 UTC now after the time change here in the U.S.  So if you want to watch us live, you can, TWiT.tv/live.  And you can participate in the chat room, like so many folks today were chatting about the topics that Steve was talking about today.



Steve, excellent job, and I've really enjoyed doing the show with you this past month.  Thank you so much for bringing me along.



STEVE:  Jason, it's been a great four episodes in the past month.  And next time Leo wanders off to somewhere, I hope you'll come back for us.  



JASON:  Absolutely.  Any time.  I'm happy to do it.  I love hanging out with you.  Thanks, everyone, for watching this episode, watching or listening to this episode of Security Now! with Steve Gibson.  We'll see you all next week with Leo back in the chair.  Bye, everybody.



STEVE:  Bye.



Copyright (c) 2019 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.








GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#741

DATE:		November 19, 2019

TITLE:		TPM-FAIL

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-741.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we look back at November's Patch Tuesday while we count down to the impending end of patches for Windows 7 and Server 2008.  We check in with CheckM8 and Checkra.in as the iOS bootrom exploit continues to mature.  We look at GitHub's announcement launch of "GitHub Security Lab" to bring bounties and much stronger security focus to the open source community.  We discuss a recent court ruling regarding U.S. border entry device searches.  We cover yet another bad WhatsApp remote code execution vulnerability.  We examine the impact of version 2 of ZombieLoad, the formation of the Bytecode Alliance, and a bit of media miscellany.  Then we examine the impact of two Trusted Platform Module (TPM) failings, one which allows local key extraction, and a second that can be exploited remotely over a network.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  I am back in the saddle.  Well, the bouncy ball, anyway.  We've got a lot to talk about, including those Patch Tuesday updates last week.  We'll talk about CheckM8, the iOS jailbreak you don't have to worry about, and why the Trusted Platform Module is just a little bit less trusted these days.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 741, recorded Tuesday, November 19th, 2019:  TPM-FAIL.



It's time for Security Now! the show where we cover your privacy, your security, everything you need to know before you go out there in the Wild West of the Internet with this guy right here, Steve Gibson of the GRC.com.  Hi, Steve.



STEVE GIBSON:  Leo, welcome back.  Everyone recognizes your voice after four weeks with Jason.



LEO:  Yes.  I have returned.  Boy, I was gone a long time, wasn't I.



STEVE:  How is the recovery?  It always seems to be like the jet lag coming back is the tough one.  You're with us?



LEO:  Well, I thank you and your Healthy Sleep Formula because I used it both coming and going.  And the jetlag going there, even though it's as bad as you can get, by the way, because we were 12 hours difference...



STEVE:  Ooh, completely on the other side.



LEO:  Yeah.  It's 2:00 in the afternoon now.  It was 2:00 in the morning there.  And that's very bad.  But we were able to - we flew to Athens.  Of course we try to sleep on the flight.  We try to get in shape.  But your Healthy Sleep Formula meant when we arrived in the afternoon I walked around, I stared into the sun, you know, I got a lot of photons into my brain to let it know it's daytime, it's daytime, it's daytime.  And then at bedtime, stayed awake till bedtime, used the Healthy Sleep Formula.  And I used it the whole trip, and it really helped.  And though I have to say coming back, you're right, it's worse.  I'm a little, I don't know, loopy.  So I upped the dosage on the Healthy Sleep Formula because I had the nicotinamide into half, just half a pill.  But now I'm using the full horse pill.  And last night, great night's sleep, didn't wake up in the middle.  So I credit you.  I think that that is the secret cure...



STEVE:  Cool, for jet lag.



LEO:  ...for jet lag, yeah.



STEVE:  Maybe I'll take my own advice next time.



LEO:  You didn't do it last time?



STEVE:  It didn't even occur to me, no.  And I've heard you mention it many times.  And I just, you know, I was...



LEO:  Melatonin's in it, and that's a part - a lot of people have recommended that for a long time in jet lag.



STEVE:  Yes.  Although a lot of people overdose on that.



LEO:  They take too much.



STEVE:  We only make a tiny little couple milligrams is all that you want.



LEO:  Yeah.  I've been taking 300 micrograms, and that's what I took on the whole trip.



STEVE:  Yes, good.



LEO:  But I did the one milligram time release that you recommend, and that's what I did last night.  And, man, I slept great.



STEVE:  So I've been holding off on our Picture of the Week because it's actually a Video of the Week.



LEO:  Are you saving it for me?



STEVE:  I've been waiting for you because you and I are both programming language people.



LEO:  Oh, yeah.



STEVE:  And I didn't know about that with Jason.  So I thought, well, I'll just hold this one till Leo gets back.  But we're going to talk about a potentially significant new piece of research which has found fault in two popular Trusted Platform Module solutions.



LEO:  Ooh.



STEVE:  Yeah, one is part of Intel firmware.  The good news is it's pervasive, but it can be patched because it's firmware.  The other one is an STMicroelectronics extremely popular TPM.  And because it's in hardware, there's nothing you can do.  So we're going to talk about that stuff.  But there's a lot of good, fun, interesting news this week.  We've got November's Patch Tuesday, which occurred last Tuesday.  And we're counting down with not a very large count to the impending end of patches for Windows 7 and Server 2008, with some caveats for enterprise.



We're also going to check in with the CheckM8 and Checkra.in bootrom exploit to look at its maturation.  It's continuing to mature.  We saw the first public release last week.  It's come a long way in one week.  We're also going to look at GitHub's announcement launch of the GitHub Security Lab, which aims to bring bounties and much stronger security focus to the open source community.  So no longer will it just be the proprietary software, where you're saying, Microsoft, something seems wrong over here.  Or Apple, or whomever.



We also discuss a recent court ruling regarding U.S. border entry device searches.  We'll cover, yet again, another remote code execution vulnerability in WhatsApp.  Those just keep on coming.  We also have now ZombieLoad v2, which we'll take a look at, as well as the formation of the Bytecode Alliance, which is interesting.  Then a little bit of media miscellany that you and I are going to have fun talking about.



And then we'll wrap up, as I said, by looking at the impact of two Trusted Platform Module failings, which of course is important because it's very much like - it's related, actually, sort of to the Apple bootrom chain, boot chain problem because the only way we can secure our systems is if there is an anchor of trust which cannot be violated.  And that anchor then starts a chain of trust where each link is verified before it is trusted.  And so, as the name sounds, the Trusted Platform Module is supposed to be the thing on your motherboard which you can absolutely trust.  Except you can't, in a couple instances.



LEO:  Okay.  The video.  I've got it queued up here.  Tell us about it.



STEVE:  So I think because this is an audio podcast, you should probably narrate this like a horse race.



LEO:  Yeah.  You're bad.



STEVE:  Because that's what this is.  What this is, this is so cool.  Now, many people have seen it.  It's on YouTube, 2.6 million views.



LEO:  So what this is, and we've seen these before, these graphs that show over time the popularity or unpopularity or whatever of something.



STEVE:  Right.



LEO:  And how are they deriving the popularity?  Because this is programming languages.



STEVE:  Good question.  I didn't dig into it to figure out.



LEO:  I think the Stack Exchange, or there are some sites that do this.  GitHub does it.  So I'll check and see where the source is.  But here we are in 1966, and you probably guess, Fortran has almost half the market, followed by COBOL, ALGOL, Assembler.



STEVE:  Woohoo.



LEO:  Then down under 5% you've got APL, you got BASIC...



STEVE:  Yeah, I didn't even know anyone actually programmed in APL.



LEO:  Because you have to have a special keyboard.  It's crazy.  



STEVE:  It's got widgets and squirlicues and things, and yeah.



LEO:  But remember, this is 1966.  If you were a programmer...



STEVE:  And of course you need some really strong parentheses keys if you're going to do Lisp.  And that was at 1.48%.



LEO:  And that's because Lisp actually is one of the oldest languages, only one year younger than Fortran.  But it's going to - I have a feeling, I'm just going to guess, it's going to diminish a little bit as we go through the quarters.  Here we are in '67.  APL is starting to beat Assembler.  APL going over 5%.  Fortran, COBOL, ALGOL still in the lead.  APL fourth, Assembler fifth.  The growth in Assembler is starting to take over.  Assembler now becoming very popular.  Oh, wait a minute, here comes a newcomer, Pascal in 1970.  LISP is up to 3%.  It's growing, now 4%.  But Pascal's growing, too.



Fortran's still on top at only 24% of the total.  COBOL, BASIC, Lisp.  Lisp is up and coming.  This is what I like to see.  In 1973 the number third most popular language is Fortran.  COBOL, Lisp, then BASIC, ALGOL, Pascal now taking over for ALGOL as it well should.  And in 1975 for BASIC, wait a minute, it's beating Lisp now.  Pascal in the third position.  Fortran, COBOL, Pascal.  Coming around the clubhouse turn we've got - I can't keep doing this.  In 1977...



STEVE:  You're doing a fantastic job.



LEO:  It's Fortran, followed by Pascal in the second position.  Then COBOL drifting back behind BASIC, which is coming on strong.  Lisp is hanging in there at almost 10%.  But that's not going to last long as Pascal becomes the number one programming language in 1980.  Shrinking market share with Fortran.  BASIC hanging on at 20%.  Then there's C, a newer language, started in 1970, coming on.  It's now number two.  Pascal, C, Ada, brand new in 1983.  It's number three, then Lisp, Fortran, BASIC, Assembler, COBOL.  But here we go, 1984 and here comes C, coming on strong.  It's now the number one language.  But Ada, the language from the Pentagon, is suddenly taking over.  That won't last.



C takes over again in 1987, and Ada's going to shrink away to nothing.  Pascal's shrinking, too.  So is Lisp.  In 1988 it's C, Ada, Pascal, Lisp, Fortran, C++, a young, up-and-coming language that is soon to take over, I suspect.  Oh, yeah, watch C++ coming on strong.  Here we are in 1990.  C++ in fourth position after C, Ada, and Pascal.  Lisp, you know, you just can't kill Lisp.  It's still in there at about 10%.  C and C++, they're neck and neck.  C has 71% market share, but C++ has 20% in 1993, and on and on and on.  And we're only halfway through, and I'm going to completely lose it.  But I think, you know, it's funny is you can watch this, and you kind of know what's going to happen.  I mean, this is a story we know the ending of.



STEVE:  Yeah, I just, for me, for those who are listening and haven't seen it yet, I did...



LEO:  Here comes Paul Thurrott's Delphi, which took over for Pascal, is beating Pascal, because it is Turbo Pascal.



STEVE:  Yes, it is.



LEO:  So a lot of those Pascal users are kind of giving in to Delphi.  Oh, my god, Java just came out in the mid-'90s, and it is coming on strong.  C++ is actually dropping.  Here's PHP in the fourth position.  And JavaScript.  By 2000 it's Java, C, JavaScript, PHP, and C++.  You could see the impact that the web has had on programming languages because both JavaScript and PHP are really designed for web pages.  Java, too, really, and it's at 34% in 2004.  JavaScript 24%.  24% for PHP.  C and C++ kind of hanging in strong.  Lisp has completely fallen off, as Python, Perl, and Ruby start to climb the charts.  Visual Basic even has 6% of the market.  By 2007 it's Java, JavaScript, PHP, C, and C++.  MATLAB, which I don't know where MATLAB came from.  And Objective C comes up as the Apple starts to get popular.



And in 2010 Objective C actually is beating Visual Basic.  It's now Java in 2011; JavaScript; PHP; Python; C#, Microsoft's version of C++; followed by C++.  I'm surprised actually to see that C++ has such scant market share in 2013.  It's only 8%.  I would have thought it'd be easily second or third.  Python has come all the way up to third position now in 2015.  Growing fast, too.  In fact, I think we're getting close to the end of the line here in 2016 with Java, JavaScript, Python, and C#.  I wonder if Java's going to plummet, though, as we get closer to 2019.  Is it going to be a sudden loss for Java?  Or, no, it's hanging in there.  Well, it dropped a couple of places.



STEVE:  You were right, yeah.



LEO:  Python, JavaScript at the end in Q3 2019, 24% Python.  Wow.  Who woulda thunk it?



STEVE:  Yeah, yeah.



LEO:  Followed by...



STEVE:  Well, you did a beautiful job, yup.



LEO:  Sorry about that.  I hope people fast-forwarded.



STEVE:  Well, so grc.sc/languages.  Because I just think this is - it's so cool...



LEO:  It's worth seeing, yeah.



STEVE:  ...for anybody, yes.  And what our listeners couldn't appreciate is that the bars were changing their relative lengths and also swapping places.  So you really got to see the languages, like, move to the top, and then hang on for a while, and then lose out to somebody else, and then slip back down.  So again, I created a little - I used the GRC link shortener, grc.sc/languages.  And it's, what, it's like a few minutes of YouTube that I think our listeners will probably get a kick out of.



LEO:  This is a new - I'm seeing a lot of these on Reddit, this format.  It's a new way of showing data is these moving bar charts over time.  And I really like - for some things it's really useful.



STEVE:  Yeah, I think it's really compelling because, you know, you just get - and for those of us who are little bit long in the tooth, Leo, we've seen all of that happen.  So it's a little bit of a blast from the past.  It's like, oh, yes, I remember Fortran.  That's what I studied in college.



LEO:  Yeah.  Yeah.



STEVE:  And then, what?  For who?



LEO:  Yeah.  I'm sad to see Lisp drop off to nothing.  It's held its own, though, for quite a while, for three decades.  That's pretty good.  It'll be back.



STEVE:  You think?



LEO:  Along with APL, yeah.  Yeah, yeah.



STEVE:  That's right.  Just find an APL keyboard, if you can.  So last Tuesday was our Patch Tuesday for November.  It brought us 73 vulnerabilities resolved across Microsoft's whole product spectrum; 13 of the 73 were rated critical.  One of them was a zero-day.  That zero-day was found to be actively exploited in the wild.  It was a scripting engine vulnerability in IE, which was, oddly enough, independently reported by four different researchers.  So somehow it was sort of in the wind.



The vulnerability in IE allows an attacker to execute their own code if a victim were to be coaxed into visiting a malicious web page, or if they're tricked into opening an Office document.  There is a way of invoking this through Office.  But Microsoft wrote of this, they said:  "An attacker who successfully exploits the vulnerability could gain the same user rights as the current user.  If the current user is logged in with admin rights, an attacker could take control of an affected system."



And in the Office document scenario, an adversary might embed an ActiveX control marked as safe for initialization into an Office document.  If the ActiveX control were initialized, it could arrange to execute code from a malicious website, essentially to create a booby-trapped Office document.  So even though, like so even for people who are not using IE, you're using Edge or Chrome or Firefox, even so the IE library is still in Windows, and it can be invoked through Office.



So anyway, so that's worth having patched for last week certainly because it was found being used in the wild.  I don't remember if they said in targeted attacks, but that's what I would expect since sending somebody a Word doc and say, you know, here's the receipt for your traffic ticket or something, and people go, huh?, and then unfortunately make the mistake of opening it up because they're curious, and they get themselves owned.



So the other interesting thing that I had noted was that Intel, like Adobe, have now started synchronizing their own patches to coincide with our Patch Tuesday.  So, I mean, everybody's going to be synchronized here on the second Tuesday of the month.  In the same way that Adobe often, but not always, releases their updates on the second Tuesday of the month, so too now will Intel.  Also there were nine patches for Microsoft's Hyper-V virtualization.  And four of those, of the nine, were rated critical because they could potentially allow for remote code execution.  You don't want remote code execution in your virtualization system because of course we know that that's how bad guys get into a shared hosting machine and can get up to some mischief.  And then there were three critical flaws in the soon-to-be-retired original Edge browser, one also affected Exchange Server, and the remaining four were in various Windows components.



And then the final issue, which we'll be getting to at the end of the show, that Microsoft mentioned in this Patch Tuesday update, was the weakness discovered in the way STMicroelectronics Trusted Platform Modules implement the Elliptic Curve Digital Signature Algorithm, ECDSA, in version 2 of their hardware.  However, Microsoft did note that that's not one of the algorithms in the TPM that they make use of.  So that mitigates the concern.  Although other things running on the platform might well be making use of it.  So you definitely would want to know one way or the other.



But speaking of Patch Tuesday, we should note that only two more Patch Tuesdays remain for Windows 7 and Server 2008 systems.  So that's next month's December 2019, and then January 14th of 2020 is the final Patch Tuesday.  We're already in the extended whatever they call it, release phase of Windows 7.  I don't know why they even bother doing that.  They officially stopped releases years ago.  But of course everyone was still using it so they said, well, okay, we'll just call this the "extended service phase" or something.  But they promise that they're actually going to stop after January, so there will be no February Patch Tuesday unless you're an enterprise customer who's able to get into the extended security updates for eligible versions of Windows.



I found a graph that I thought was interesting, which we had talked previously about how we suspected that most end users had probably moved over to Windows 10.  For whatever reason they didn't stay on 7.  This is a chart - shoot, and I meant to note from where it was.  I know that it was in August, so it's relatively recent.  And it shows three classes of users:  customers, very small businesses, and small and medium-sized businesses and enterprises.  And what was interesting is, as of late summer, Windows 7 and 10, those red bars, they're neck and neck.  They're at 47% each.



So it's true that, overall, Windows 10 has taken a lead as we'd covered at the beginning of this year.  It finally pulled ahead.  But not surprisingly, the bias is in the end user who said, "Oh, new Windows?  Yay."  But the enterprise said, "New Windows?  Nay.  No, thank you."  So it's really going to be interesting to see now what enterprises are going to do.  Why is it that they haven't made the move?  And of course one of the problems I think is that Microsoft has demonstrated that they're not going to let Windows ever settle down.  I heard you, Leo, I think it had to have been since you were back, maybe it was on Sunday, just sort of bemoaning the fact that Microsoft is really having problems with Windows, with Windows 10.



LEO:  With 1909.  The latest version is causing all sorts of problems.  And I thought that this would not be happening after a while.  But no.



STEVE:  Well, right.  And so once upon a time it made sense for a rational Windows 7 user to hold off before adopting a new OS.  First of all, that would allow its newness to settle down and for most of the problems to be found and fixed.  But of course anyone who looked at Windows 8 and then 8.1 would have probably, who had any sense, would have thought, okay, I don't want that.  I'm going to keep waiting.  And so it was sort of roundly regarded as a bit of a catastrophe.  And what's now fully apparent, having been watching Windows 10 for some years, is that Microsoft deliberately has no plans to allow Windows 10 to ever, in air quotes, "settle down."



LEO:  Yeah.



STEVE:  I mean, it's just not going to happen.  So it's going to continue to be an unsettled work in progress, which is I think turning out to be not a wise move.  It's one thing to do this to end users.  But enterprise doesn't want a constantly moving target like this.  So it's going to be interesting to see.  I mean, what's odd to me is that I looked into - I have a link in the show notes, techcommunity.microsoft.com, here at the end of this story.  This is from Windows IT Pro blog.  And the link is how to get extended security updates for eligible Windows.



And I was curious; so I scanned through it.  What I saw was that you will be able to register your instance of Windows to continue to operate the way - that is, of Windows 7 Pro - to continue to operate the way it has been.  Which is to say it'll get automatic updates for, what is it, I think it's up to three years; right?  And so you pay a certain amount for the first year, then you pay more for the second year, and you pay more for the third year.



And Microsoft makes a point of saying you can't purchase partial years.  You have got to commit to the whole thing.  Which says their entire automatic security patch process is automated, and nothing but policy is causing them to refuse to provide fixes for things that are wrong with Windows 7.  That is, these are, I mean, this is the new bait-and-hook is that, oh, you don't want to continue running Windows 7, which works just fine, because it won't be getting any new security updates.  Except they're going to have them, and they're going to be selling them and offering them to their enterprise people.  But not to everybody else who still wants to be using Windows 7.



So, I mean, to me this feels like an ethical dilemma, that it's not like new features in Windows 7 that Microsoft isn't going to make available.  In fact, it's because people don't want new features that they've stayed with Windows 7.  No, it's the fixes to the security vulnerabilities due to the flaws in Windows 7 that Microsoft is now going to refuse to continue to provide to people, even though the entire system is in place for them to do so.  I don't know.  That just, to me, this seems like they've painted themselves into a corner.  They've produced an OS in Windows 10 that a lot of people have opted out from accepting despite the fact that it's free.  And now they're going to say, well, for three more years we'll provide people who insist on remaining with Windows 7 security updates.  But everybody else, sorry, good luck.  You know?  You're going to have vulnerable versions of Windows because we're not going to fix the bugs that we've left behind in it.



LEO:  They don't charge an insignificant amount to buy those updates, too.  And it doubles every year.  So they really don't - I think they just don't want people to do it.  And they acknowledge that some businesses have to.  But they just don't want to.  They want everyone on 10.  If they could get everybody on 10 with a snap of a finger, they would do it.



STEVE:  And I'm sympathetic to the idea that they don't want to continue to be responsible for an old code base.  I mean, I get that.  But if they're making it available to anybody, how do they deny it to people who need it and aren't enterprises?  I mean, an end user can't buy it.  You have to have the volume licensing agreement thing with Microsoft.  So it's not like everyone who's chosen to stay with Windows 7 could purchase the additional security patches if they wanted to.  They can't.



LEO:  Yeah, but that's the point is those security patches are there only because they have no choice.  There are some banks and X-ray machines and whatever.



STEVE:  Ah, okay.



LEO:  They want everybody on 10.  And by the way, there's a lot of good reasons.  I mean, there's businesses reasons for Microsoft.  But there's also developers want to have a platform that's consistent and uniform so they know what the features are.  And of course there's the security issue, as well.  So I understand there's a lot of reason why Microsoft would want to do this.  I'm not defending them.  I know what you're saying, Steve, and I don't disagree with you.  I don't.  I've just given up on Windows, to be honest.  Honestly, I have.



STEVE:  Please, I can't wait to join you.  I feel the same way.  It just feels like, boy, wrong.



LEO:  The good news is desktop Linux has gotten so good that, if people would just open their minds a little bit, there is a route.  And almost every PC that runs Windows will run Linux just fine.  There is a good exit route.  But people are very reluctant.  And I, you know, in a business I'm not going to make my employees use Linux.  I can't do it.  Maybe should charge them double if they don't.



STEVE:  So last week was the first emergence of Checkra.in.  And we noted that the official website, which as you and I talked about I think in the beginning, it was initially - it was Checkra1n.com.  And there was no security certificate there.  And bad guys grabbed Checkrain, R-A-I-N, dot com and put some...



LEO:  Oh, god.



STEVE:  Yeah, some malware there.



LEO:  Of course they did.



STEVE:  So it was like, okay, the hackers sort of did the wrong thing by doing a hackeresque domain name.  Well, now we know the final name is Checkra, C-H-E-C-K-R-A dot I-N.  So that, since there is a TLD .in, Checkra.in.  That's the official site.  So this has continued to evolve.  And of course aside from the fact that, as we covered last week, the exploit for the Windows BlueKeep vulnerability has now been perfected so that it no longer has a high incidence of BSODs, I mean, and that's something I'm sure we're going to be talking about in weeks to come.



Arguably the most significant thing going on at the moment is the continuing work to perfect Checkra.in.  Which as we know is the iOS CheckM8 bootrom vulnerability exploit.  And what makes it significant is, because it is actually a ROM bug, it's never going to get fixed.  It's going to persist in all the devices that Apple needs to continue supporting until they finally go out of support, which is probably going to be years.



So anyway, since it's a big deal, I did want to sort of check in with where it stands today.  When we talked about it last week, there were three betas:  0.9, 0.9.1. 0.9.2.  Now we are at 0.9.3.  We had also 0.9.3.2 and 0.9.5, which as of last night is where we are.  I didn't check yet this morning, I meant to, to see if we were yet at another one past that.  They are continuing to find and fix and squash bugs, and it is becoming increasingly stable.



Since I don't have any of my own first-hand experience with jailbreaking or with this particular emerging jailbreaking system, I went looking for some interesting and pertinent feedback from those who have been playing with it so far.  Dan Goodin, who covers security topics for Ars Technica, had done some digging.  And I've excerpted, and I paraphrase a little bit from what he found and reported.  It turns out those who have looked at it are finding that this Checkra.in exploit is surprisingly reliable and robust.  There are still platforms for which there is no support.  There are some platforms, which is to say, some iOS devices for which there is no support.  There are some for which it's still a little bit flakier.



Ryan Stortz, who's an iOS security expert and the principal security researcher at Trail of Bits, he said in his interview with Dan, he said:  "I expected it to be a little rougher around the edges for the first release."  And of course it's not even there.  It's at beta.  He says:  "It's really nice to be able to install a new developer beta on your development iPhone and have all your tooling work out of the box."  He says:  "It makes testing Apple's updates much, much easier."



And of course it promises, once it's finished, to work reliably on a wide array of hardware, meaning everything from the A5 through the A11 chip.  The iPhone 4S was an A5, but I'm not seeing it being discussed.  So it might be the 5S which is the first one.  It may be going no further back.  It may have been that the A5 is using on the iPhone 4 an earlier version of iOS, and they're not going that far back with iOS.  But in general, it is surprising people for its stability and the speed with which it's moving forward.



So there's a lot of controversy in the industry about its presence.  Not that that's going to do anything to dissuade it from happening.  The hackers who are putting it together are clearly having the time of their lives.  They recognize, unlike previous jailbreaks, that they're investing in something that has legs.  Inevitably, if a jailbreak ever becomes public, and it's only when it becomes public that it has an opportunity to acquire some user base, Apple will of course grab it and immediately close it, I mean, fix it in an emergency patch in order to keep this from happening.



So what we've historically seen is that Apple just squashes jailbreaks instantly, you know, at the first chance they get, demonstrating how unhappy they must be with the fact that there is now one that they're never going to be able to fix.  And in reading into the industry's reaction to this, there are people who consider it a bad thing because I guess they're concerned that end users will use this in order to do things like we've seen back in the Android era or in other closed ecosystems where users wanted to, like, use an unauthorized launcher for their apps or something.



I don't think that's going to happen because, first of all, as you were mentioning on MacBreak Weekly, our devices are being rebooted from time to time.  I'm often picking up my phone after I had used it the night before and being told I need to reenter my passcode, presumably because it did a self-restart overnight, and so it rebooted.  So this cannot persist across a reboot.  So I regard that as a good thing because I would never promote the use of this for persistent jailbreak on an end-user device.



I think this is interesting for the security community because it will give the security community, the white hat hackers, visibility into what Apple has been explicitly trying to keep away from everyone.  Of course the flipside is that the black hat hackers will also get visibility into what Apple is doing, and in both cases, if new problems are found, those could potentially be exploited in non-CheckM8 jailbroken phones in the same way that any new vulnerabilities found in iOS are being exploited during the brief window of opportunity that exists.



So anyway, I think this has been an interesting event.  There's nothing Apple can do about it.  We are soon going to have a robust, tetherable, USB jailbreak which has transient persistence, which will allow the security community to take a look into what Apple is doing in a way that they have never been able to before.  And I think that's cool.



LEO:  Just to be clear - because actually there was a little conversation about this in the TWiT Forums.



STEVE:  Oh, good.



LEO:  The jailbreak is transient, but you could do things to the phone that would not be transient.



STEVE:  It's not clear because...



LEO:  Okay.  Because couldn't you install some - so one of the reasons people do jailbreaks is so they can install software, sideload software.



STEVE:  Yes, but that would not be signed by Apple.



LEO:  Oh, and it would then be removed?



STEVE:  Yes, yes.



LEO:  Ah, okay.



STEVE:  So on a reboot, the first moment that Apple sniffed an app that did not have their signature, meaning it did not come from the Apple App Store, it's gone. 



LEO:  Interesting.  I mean, in theory somebody could be clever and figure out a way around that, perhaps, or rootkit or something.



STEVE:  Well, I don't think there is a way around that because you're going to then have the whole secure boot chain all the way up.



LEO:  Right.  And that's intact after reboot.



STEVE:  It is absolutely intact after a reboot.  So the vulnerability would be, if you lost control of your phone at a border - going into China, for example, or Russia, or pick on somebody - and they gave it back to you, it could have had Checkra.in used to install something transiently which would then have residence until you restarted your phone.  The comments have been made that the official Checkra.in app does place a Checkra.in app, and it puts a couple things on your home page, making it obvious that it's there.  But it's not clear that those could not be removed by a variant of Checkra.in, which certainly somebody else could create.



So what this does on older hardware is create a window of exploit between the time you lose your phone and somebody might compromise it until you next restart it.  So the advice has been twofold.  First, if you are someone who might be targeted, and your phone is ever out of your control, power it down completely, and then power it back up again, which will again let Apple purge anything that might have been done from the phone.  Or use a newer device.  Because, remember, nothing that's A12 or A13 is vulnerable to this.  So, like Leo, every time I see the back of your phone I think, oh, yup, Leo's got the latest and greatest.  He's got the three-lens monster.



LEO:  It's a giveaway, yeah.



STEVE:  So, yeah, not a problem for you ever.  And so anybody who's concerned, this would be a reason to update to something that is at least A12.  And if you're doing that, you might as well go to A13.



LEO:  Right.



STEVE:  And if, for whatever reason, you have an older device, if it's ever out of your possession, and you have any reason to suspect that you might be a target, just power cycle it, and then you're going to be okay.



LEO:  Good to know.



STEVE:  You know, is tethered, is USB.  So it's just in some ways it's sort of like a perfect flaw because it can't be fixed.  Apple needs to be providing updates for a long time to the A11 devices.  I have one, an iPhone 10.  And so I could do that to it.  So it's, well, I have an iPhone 6, for that matter, which is still being supported.  So I think a really, really interesting exploit.  Not the kind of thing...



LEO:  And relatively harmless.  Relatively benign.



STEVE:  Yes.  Yes.  And that's why I'm not, like, worried about it.  It is relatively benign.  Yet it does lead, I mean, I understand Apple's interest in not letting people poke around.  A year from now the poking around that white hats will have been able to do will probably have fixed a whole bunch of things that will benefit all iOS users.  At the same time, the people who want to see inside for nefarious purposes, well, they've been able to do that, too.  Although it might be that they were able to get in anyway, you know, through various ways.  So anyway...



LEO:  So you could make a strong case it's really only to the good.



STEVE:  I think so.  I just think, you know, what we've seen is the more people look at software, the more problems are found and fixed.  And Apple will fix them, if they are responsibly disclosed.  And so there's a huge community of people who see this as their first opportunity ever to roll up their sleeves and see what Apple has done.  And there will be improvements to iOS that arise from that.



LEO:  Yeah.  That's good.  Steve?



STEVE:  So GitHub.



LEO:  GitHub.



STEVE:  Has decided to launch a proactive Security Lab aimed at boosting open source software security.



LEO:  Oh.  Nice.



STEVE:  This was a, yeah, this is a very cool effort.  This was announced last Thursday at the GitHub Universe developer conference.  And in years past we've touched upon a few audits of important open source software.  Early on, Matthew Green was largely driving the effort to audit TrueCrypt, back before it became VeraCrypt.  There have been a couple of audits of OpenSSL.  I just noted that the audit of the new Unbound DNS server has recently been fully funded.



But I think we would all agree that it's probably long past time for the creation of a public effort to formally incentivize and reward those who discover and report problems in open source software.  All of the, you know, like the Zimperium that we've talked about, sort of the quasi-gray, we're not sure who they're selling their exploits to, but we do know that they're paying a lot for them, you know, those are all in proprietary closed systems.  So what GitHub is doing is saying, hey, let's put together, let's formalize an effort to fix open source software, as they put it, before it's able to do any harm.



Jamie Cool explained at the announcement, actually he wrote of GitHub Security Lab.  He said:  "We all share a collective responsibility to keep open source software secure.  None of us can do it alone.  Today at GitHub Universe we announced GitHub Security Lab to bring together security researchers, maintainers, and companies across the industry who share our belief that the security of open source is important for everyone.  We are excited to have an initial set of partners" - oh, and wait till you hear who they are.  I'll get to them in a second - "who have all committed to achieving this goal.  Together we're contributing tools, resources, bounties, and thousands of hours of security research to help secure the open source ecosystem.  As part of today's announcement, GitHub Security Lab is making CodeQL, which is like their main flagship gizmo, freely available for anyone to find vulnerabilities in open source code."  And of course, frankly, you could do it in your own source code.  Maybe that's violating the license, I don't know.



"CodeQL," he says, "is a tool many security research teams around the world use to perform semantic analysis of code, and we've used it ourselves to find over 100 reported CVEs in some of the most popular open source projects."  And I'll just note as an aside that GitHub obtained CodeQL from their purchase of Semmle, S-E-M-M-L-E.  Semmle are the people who developed this.  He said:  "We're also launching GitHub Advisory Database, a public database of advisories created on GitHub, plus additional data curated and mapped to packages tracked by the GitHub dependency graph."



He said:  "GitHub's approach to security addresses the whole open source security lifecycle.  GitHub Security Lab will help identify and report vulnerabilities in open source software, while maintainers and developers use GitHub to create fixes, coordinate disclosure, and update dependent projects to a fixed version."



He said:  "GitHub Security Lab's mission is to inspire and enable the global security research community to secure the world's code.  Our team will lead by example, dedicating full-time resources to finding and reporting vulnerabilities in critical open source projects.  The team has already had over 100 CVEs issued for security vulnerabilities it has found."  He noted that:  "Securing the world's open source software is a daunting task," he said.  "First, there's scale.  The JavaScript ecosystem alone has over one million open source packages."



LEO:  What?  How could that even be?



STEVE:  Yeah.  JavaScript, one million open source packages.  Then, oh, and of course it would be useful looking at the usage graph, you know, the distribution, because I'm sure Node.js is, like, way high.  It probably, you know, there are probably, what, maybe 50 or 60 that are in heavy use, and then it falls off quickly.  But if someone uses one of these obscure ones, and there's a problem in it, they can get their site compromised, and their users.  But then, he says:  "Then there's the shortage of security expertise.  Security professionals are outnumbered 500 to one by developers."



LEO:  [Laughing] Wow.



STEVE:  Uh-huh.  So, you know, everyone listening to this podcast...



LEO:  Everybody wants to be a coder.  Nobody wants to be a security pro, yeah.



STEVE:  That's right.  Who wants to fix those bugs?



LEO:  IT, boring.



STEVE:  "Finally," he says, "there's coordination.  The world's security experts are spread across thousands of companies.  GitHub Security Lab and CodeQL will help level the playing field."  He says:  "Joining us in this effort are the following companies, donating their time and expertise to find and report vulnerabilities in open source software.  Each have committed to contribute in a different way, and we hope others will join us in the future."  So those are, at launch, F5, Google, HackerOne, Intel, IOActive, J.P. Morgan, LinkedIn, Microsoft, Mozilla, the NCC Group, Okta, Trail of Bits, Uber, and VMware.  So an interesting group.



LEO:  Could be good.



STEVE:  Yeah.  And he says:  "To empower the research community, we're also making our state-of-the art code analysis engine, CodeQL, free to use on open source.  CodeQL" - get this, Leo, you'll find this interesting - "lets you query code as though it were data.  If you know of a coding mistake that caused a vulnerability, you can write a query to find all variants of that code, eradicating a whole class of vulnerabilities forever."  Which is really interesting, I think.  I have a link in the show notes.  To get started with CodeQL, it's https://securitylab.github.com/tools/codeql.



He says:  "If you're a security researcher or work in a security team, we want your help.  Securing the world's open source software will require the whole community to work together.  GitHub Security Lab will run events and share best practices to help everyone participate.  Follow" - and they have a Twitter account - "@GHSecurityLab account on Twitter for more details."



They said:  "As the world's security researchers uncover more vulnerabilities, maintainers and end users need better tools to handle them.  Today the process for addressing a new vulnerability is often ad hoc."  Get this:  "Forty percent of new vulnerabilities in open source don't have a CVE identifier when they're announced, meaning they're not included in any public database.  Seventy percent of critical vulnerabilities remain unpatched 30 days after developers have been notified."  And of course we've talked about this, how it's embarrassing, especially when a package hasn't been updated months after the maintainers of it were told of a problem.  And then of course the you-know-what hits the fan when it starts getting exploited, even though it's been known for months.



So here he said:  "Seventy percent of critical vulnerabilities remain unpatched 30 days after developers have been notified."  And of course we've talked about why, too.  The maintainers, this is a hen o at the usage graph, you know, the distribvoluntary effort for many of them.  They've got a day job.  They've got a bunch of stuff to get to, but they haven't gotten to it yet.  So one of the things that the Security Lab is working towards is bringing much more automation to this process.



He said:  "We're fixing that.  Maintainers and developers can now work together directly on GitHub to help ensure new vulnerabilities are only disclosed when maintainers are ready, and that developers can update to fixed versions quickly and easily."  He said:  "This will be accomplished through GitHub Security Advisories.  With Security Advisories, maintainers can work with security researchers on security fixes in a private space, apply for a CVE directly from GitHub, and specify structured details about the vulnerability.  Then, when they're ready to publish the Security Advisory, GitHub will send security alerts to all affected projects."



Anyway, this just sounds like a huge step forward for the open source community in general and for the security aspect of it specifically.  He notes that receiving a notification about vulnerable dependencies is helpful, but getting a pull request with a fix is even better.  So to help developers respond quickly to new vulnerabilities, GitHub creates automated security updates, which will be pull requests that update a vulnerable dependency to a fixed version automatically.  So automated security updates were launched in beta at GitHub Satellite 2019 and are now generally available and rolled out to every active repository with security alerts enabled.



And then finally he said:  "We've made all of the data that maintainers create in GitHub Security Advisories, plus additional data curated and mapped to packages tracked by the GitHub dependency graph, available for free.  You're able to explore the GitHub Advisory Database in your browser, link directly to records with CVE identifiers in comments, or access the data programmatically using the Security Advisory API."



So, for example, github.com/advisories is where you would go with a browser if you wanted to browse around and see what was going on.  And I have the link in the show notes to the API endpoint, if you wanted to be able to query it through your own automation.  So essentially what this looks like is we're bringing to what has traditionally been sort of a very ad hoc, you know, you could use the term "loosey-goosey" sort of process, you know, the kind of structure and automation which various corporate entities have had to bring to their own processes, but which had been lacking from the open source effort.



So I think this is just 100% good because it really seems clear to me.  You know, here's Microsoft scrapping years of development effort on a complete rewrite of their web browser.  They're like, eh.  We're just going to use Chromium.  It makes more sense to do that.  So, and Leo, you were just saying, Linux is ready for the desktop.  Well, all of this is open source.  All of the pieces of it are open source.



So we need the same sort of focus on security moving forward that we're used to receiving from Apple and from Microsoft and other commercial vendors in whose own interest keeping their product secure is.  There hasn't really been the same sort of economic drive, which is why it's been necessary to do fundraising campaigns to produce the money to then commission audits of important open source packages.  This is the next step in that maturity.  And so I think it's just - it's wonderful.



Speaking of wonderful, we've talked in the past about a number of different, really unfortunate instances of people being harassed at the border, and this whole issue of do you use a biometric, or do you use a password, when the border guards challenge you and demand to have access to your smartphone or laptop or whatever.



There was the case of a natural born U.S. citizen, Sidd Bikkannavar, who was at the time, and probably still is because it's only two years ago, a NASA engineer who was detained by U.S. Customs and Border Protection in 2017, pressured to hand over his NASA-issued phone and PIN so that the Border Protection agents could get into it.  This was for no cause and in spite of the fact that the work-issued phone could have contained sensitive information relating to his employment at the space agency, and in spite of the fact that NASA employees are obligated to protect all work-related information.  The CBP officer returned his phone half an hour later, saying it had been searched using "algorithms."  There was another instance of...



LEO:  Oh, well, those algorithms, you know how they are.



STEVE:  Yeah.  And I was thinking, well, what is that?  Is that supposed to mean that no people...



LEO:  Black magic.



STEVE:  ...people looked at it, you know, so we weren't looking at your photos, but algorithms were?  I don't know.  



LEO:  We don't know.



STEVE:  There was an artist, Aaron Gach, who - he's another natural born U.S. citizen who was forced to unlock his phone after returning from putting on a gallery installation in Brussels.  That particular installation focused on "mass incarceration, government control, and political dissent."  Is that why he had to turn his device over?  We don't know.  But he did, for no cause.



Diane Maye, a college professor and retired U.S. Air Force officer, was detained for two hours at Miami International Airport upon returning home from a vacation in Europe.  At the time that the lawsuit was filed in 2017, Maye said that the encounter left her feeling "humiliated and violated."  She explained that she worried that border officers, again without cause or reason, would read her email messages and texts, look through her photos and so forth.  She said that this was her life, and a border officer held it in the palm of his hand.  She joined the lawsuit, which was brought by the ACLU and the EFF, because she strongly believed the government should not have the unfettered power to invade our privacy without probable cause.



Okay.  So since this podcast has been underway, because it goes back to 2009, so for the past 10 years, that's when U.S. border agents obtained the right to legally search electronic devices of travelers at borders without any specific cause or suspicion, in the interest of protecting our borders and enforcing U.S. border security.  Then in 2016, three years ago, that law was revised to require "reasonable suspicion," unquote, for anything beyond basic searches.  But that still allowed agents to require travelers to unlock phones and gave them free rein to read messages and other basic information.  Again, without a probable cause.



So what is significant is that last Tuesday a federal court in Boston ruled that suspicion-free warrantless searches of travelers' electronic devices at U.S. border entry points are unconstitutional.  The decision comes from a lawsuit, which was  Alasaad v. McAleenan, filed against the Department of Homeland Security in 2017 by the ACLU, the American Civil Liberties Union, and the EFF, on behalf of 11 travelers.  Ten of them were legal residents of the U.S., and one was a lawful permanent resident, all of whom were forced into warrantless searches of their mobile phones and laptops at the border, including the three that I just mentioned.



Sophia Cope, a senior staff attorney with the EFF, the Electronic Frontier Foundation, said of this ruling:  "This is a great day for travelers who can now cross the international border without fear that the government will, in any absence of suspicion, ransack the extraordinarily sensitive information we all now carry in our electronic devices."  And reading into this a little bit more, and doing some digging, I noted that it does remain legal for border agents to look through the devices of travelers who get referred for a secondary inspection.  So during the primary inspection, travel documents and passports are reviewed.  That's what most of us experience.  I'm sure you just did, Leo, and I just did.



LEO:  Oh, yeah.  Oh, yeah.



STEVE:  Coming back into the U.S., where they just asked, like, where have you been, why were you gone, and so forth.



LEO:  What did you buy?



STEVE:  Yeah.  And then just kind of wave you through.



LEO:  Right.



STEVE:  If a secondary inspection is deemed to be needed, then it's the case that officers may still search phones, thumb drives, computers, and other electronic devices to determine whether they should let somebody into the country or to identify potential legal violations.  So having read that, I'm not quite sure what everyone's jumping up and down about because it looks like you could still be pulled off to the side and given that.  But, you know, this does seem like it's movement in the right direction.



According to the ACLU, the Boston district court's order puts an end to the authority that CBP and ICE had been granting themselves to search and seize travelers' devices for purposes beyond enforcing immigration and customs laws, which was the umbrella under which they were doing that.  At this point, border officers are required to demonstrate "individualized suspicion of contraband" before they can search a traveler's device.  So that does sound like they've raised the bar to a useful level.



And when I think back about 10 years ago, or before the era of smartphones, when you think about it, Leo, we used to just, before smartphones, we didn't have something that did contain a virtual record of our lives.  You know, if you're using Instagram, as I was when I was traveling, in order to sort of create a little travelogue for my friends and family who were here, you know, that's recording everything we do.  All of my text messages, I mean, they're boring, in my case, about when I'll be home for dinner.  But it is, I mean, if you read through the contents of someone's phone, you pretty much know a huge amount about them.  So the fact that we're now carrying a record of our life, why does that suddenly mean that border entry agents suddenly should have a right to what amounts to an electronic frisking?  And so it's looking like maybe we're going to, you know, we will be putting an end to that.  So that seems like a good thing.



LEO:  Yes.



STEVE:  I did want everyone to know that WhatsApp has been found to have a problem again.  And just so we don't consider these problems to be just another in a long line of theoretical vulnerabilities, which we are often talking about here, let's remember that Israel's NSO Group was recently found to have leveraged another recent WhatsApp vulnerability, this case in the VoIP calling, to successfully install Pegasus spyware on nearly 1,400 targeted Android and iOS devices worldwide.  So remote code execution flaws, especially in WhatsApp, which now is able to claim the title of the world's number one supposedly secure messaging app...



LEO:  Oh, everybody uses it.  Everywhere outside the world uses it.  It's universal.



STEVE:  So, you know, it won't go unexploited for long because of its high use.  So in October, a few months ago, we noted that WhatsApp was using an open source GIF image-parsing library which contained a double-free memory corruption bug, which could be leveraged for remote code execution in the context of the WhatsApp application.  So there was another instance of a problem.  That's two.  Now we have the third one recently.  This is just this last month, WhatsApp quietly patched another critical vulnerability that would, had it been used, and we don't know one way or the other, attackers to remotely compromise targeted devices to steal secure chat messages and the files stored on the devices, or whatever else they wanted.



The vulnerability, which was tracked as CVE-2019-11931, is a stack-based buffer overflow which resided in the way WhatsApp's parsing of the MP4 video stream metadata was being done, which would either crash WhatsApp or allow remote code execution attacks.  Of course we've often seen parsing stream metadata, especially for a compressed file format, is very difficult to get absolutely correct and to make robust.  It's so easy when decoding metadata to make the assumption that the sender was a valid codec rather than something malicious.



So to remotely exploit this vulnerability, all an attacker needs is the phone number of a targeted user and then to send them a maliciously crafted MP4 over WhatsApp.  It can then silently install a malicious backdoor or other spyware app on what is now a compromised device.  The vulnerability affects both consumers and enterprise apps on all major platforms.  So this was very widespread, both on Android, on iOS, and on Windows.  So this was patched.



So the takeaway is update your instance of WhatsApp.  Make sure that you're running the most recent version.  I won't enumerate them.  There's six of them that I have in the show notes, which are the ones which were affected.  And in every case it was versions before, up to, and including the one that I've listed.  So just make sure that you're current.



The attacks would have been targeted.  If you're somebody who might be a target of an attack, and if you can recall in the last few months having received an unsolicited WhatsApp message that crashed your WhatsApp, or tried to play a video and didn't, then you might want to dig a little further and see whether anything might have crawled into your device.  Again, it's unlikely that anyone would have been affected.  Facebook has claimed that no one was a victim of this.  But how would they know...



LEO:  No one, no one's a victim.  No one.



STEVE:  ...if somebody was, if it were a targeted attack, yeah.



LEO:  Oh, absolutely, no one.



STEVE:  And of course Facebook's reputation continues to decline.  So, yeah, just trust us, trust us.



LEO:  It's probably, though, a targeted attack because they'd have to have your phone number.



STEVE:  Yes, yes, yes.



LEO:  They have to know where they're going.



STEVE:  And so, you know, it's not something that would be sprayed.  Well, if it had been sprayed, then lots of people would have been affected by it.  And so Facebook would not be able to claim, oh, no, nobody we know.



LEO:  Nobody we know.  That's a better way to put it.  Nobody we know.



STEVE:  That's right.  So we had fun, Leo, months ago, with ZombieLoad.



LEO:  Oh, yeah.



STEVE:  Just saying it, saying it was fun.  And I won't spend much time on this.  I'll just note that there is version 2 of ZombieLoad.  And like all of these, these have ended up, I mean, and we thought this from the beginning.  They were interesting from an intellectual standpoint.  It's been two years now.  When we get to next January, it will be two years.  The first podcast that we did in 2018 was Spectre and Meltdown.



LEO:  Yeah.  It doesn't seem like it's been that long.  But geez.



STEVE:  I know.



LEO:  It's almost two years.



STEVE:  I know, two years.  And significantly, in all that time, as far as anyone knows, none of this stuff has ever been exploited.  So yes, it's a problem in theory.  If you were on a shared hosted VM in a cloud environment, if something was able to arrange to share the same hardware processor as you, they could reach across the VM boundary and maybe steal some information.  That's what got everybody up in arms and worried.



And as a consequence we had all these firmware updates.  We've had Windows now patching the firmware on the fly for some versions of Windows.  I mean, you know, basically two years of running around in circles because of Meltdown and Spectre and Foreshadow and Fallout and RIDL, you know, and we've got the so-called MDS, the Microarchitectural Data Sampling flaws.  Intel continues to be dogged by this.  I've seen reports of as much as 40% performance hit when all of this stuff is turned off.  And why?  Basically for what is nothing, it really has never been shown to be more than a theoretical attack.



Intel believed, and this is what's significant about ZombieLoad v2, they believed and claimed that their most recent architecture, the Cascade Lake architecture, released just in April, was completely protected against side-channel and speculative execution attacks in the hardware.  It is victim of ZombieLoad 2.  It turns out that this particular type of attack was always known by the attackers.  It wasn't until Intel produced a patch for it, which they have just made available, that the researchers behind this updated their original report in order to also disclose ZombieLoad v2.



Microsoft wrote:  "On November 12, 2019, Intel published a technical advisory around Intel Processor Machine Check Error vulnerability" - that's the official name - "that is assigned CVE" - get this - "2018-12207."  Meaning, uh-huh, they've known about it for quite a while.  I'm about to sneeze, I think.  Nope, maybe not.



LEO:  I hate that.



STEVE:  "Microsoft has released updates to help mitigate this vulnerability for guest virtual machines, but the protection is disabled by default.  Enabling this protection requires an action on the Hyper-V hosts running untrusted VMs.  Follow the guidance in the Registry Settings section to enable this protection on the Hyper-V hosts running untrusted VMs."



Anyway, so I didn't even put it in the show notes.  Somebody for whom this could have some effect can certainly track it down.  Given that not a single instance of any one of these low-yield theoretical attacks has ever made a usefully practical appearance in the wild, that's never been seen, and given that the only true vulnerability is in a shared hosting environment where a bad guy could arrange to co-reside on the same hardware, this is just not a concern for any end user, who certainly make up the bulk of this podcast's audience.  If something gets into your computer, you're already hosed.  I mean, Windows uses this messaging system which is incredibly insecure, and so a random app can query the global clipboard that all apps share.  So the instant you use it to copy a password that you've just created in order to paste it somewhere else, you're owned.



So anyway, as we know, personal workstations are not secure.  It's certainly the case that cloud environments want more security, need more security, and Intel's just going to have to continue to figure out a way to provide the kind of isolation that we need among processes without having to roll back the clock and give up on a decade of performance benefit because unfortunately the tricks that they were using were leaving, as we know, a footprint behind in the so-called microarchitecture of these chips that clever researchers were able to leverage into theoretical cross-process boundary information leakage.  Some of it worse than others, but none of it ever actually happened in practice.



And I think the last piece of news - is it the last piece of news before - yeah, it is, because we touch on a little bit of fun miscellany, Leo.  And this is important, I think, the announcement of something known as the Bytecode Alliance.  Mozilla, Fastly, Intel, and Red Hat are the cofounding launch members.  And the short take of this is WebAssembly is outgrowing its browser.  So as we know, and we've covered through the years, there's been some early competition among competing JavaScript replacements, with the general goal of improving client-side web application performance, come up with a way of making this stuff faster.  The winner is WebAssembly.



So the good news is we're not going to have some fractured environment where there are multiple solutions to the same problem.  WebAssembly is the standard, and it's now supported by all major web browsers.  And as it's continuing to mature, it has proven to be the right choice.  So the Bytecode Alliance is the next effort to generalize WebAssembly into a generic platform and to make it architecture-agnostic, creating a common runtime suitable for use on everything from extremely lean little IoT widgets through high-end CDN server farms.  In other words, completely outside the browser.



And if this sounds familiar, it's sort of the role that Sun had always hoped that Java might obtain; right?  Java, as we know, compiles to Java bytecode, and then you have the JVM, the Java Virtual Machine, as a runtime environment and interpreter of the bytecode.  What's different from Java is that WebAssembly has as its Internet-facing origin the web browser, where strong attack hardening had to be explicit from the beginning.  So for WebAssembly, security was built in from the start.  And I think that's what gives us hope, actually.



Lin Clark, who was writing from Mozilla, explained it this way.  He said:  "We have a vision of a WebAssembly ecosystem that is secure by default, fixing cracks in today's software foundations.  And based on advances rapidly emerging in the WebAssembly community, we believe we can make this vision real.  We're already putting these solutions to work on real world problems, and those are moving towards production.  But as an alliance, we're aiming for something even bigger.  As an industry, we're putting our users at risk more and more every day.  We're building massively modular applications, where 80% of the code base comes from package registries like npm, PyPI, and crates.io.  Making use of these flourishing ecosystems isn't bad; it's good.



"The problem is" - and we talk about instances of this all the time on the podcast - "the problem is that current software architectures are not built to make this safe.  And bad guys are taking advantage of that at a dramatically increasing rate.  What the bad guys are exploiting is that we've gotten our users to trust us.  When the user starts up your application, it's like the user's giving your code the keys to their house.  They're saying, 'I trust you.'



"But then you invite all your dependencies, giving each one of them that full set of keys to the house.  These dependencies are written by people you don't know and have no reason to trust.  As a community," he writes, "we have a choice.  The WebAssembly ecosystem could provide a solution here, at least if we choose to design it in a way that's secure by default.  If we don't, WebAssembly could make the problem even worse.  As the WebAssembly ecosystem grows, we need to solve this problem.  And it's a problem that's too big to solve alone.



"That's where the alliance comes in.  The Bytecode Alliance is a group of companies and individuals coming together to form an industry partnership.  Together, we're putting in solid, secure foundations that can make it safe to use untrusted code, no matter where you're running it, whether on the cloud, natively on someone's desktop, or even on a tiny IoT device.  With this, developers can be as productive as they are today, using open source in the same way, but without putting their users at risk."  He finishes:  "This common, reusable set of foundations can then be used on their own, or embedded in other libraries and applications."



So, I mean, I couldn't be more excited about the potential.  That's all it is at this point.  But they have - I think what happened is they recognized right now they have an opportunity.  WebAssembly is demonstrating to run very fast.  And due to the way it's been built, and because it had to be Internet-facing in the highest attack surface we have in the world, the web browser, and it is surviving, they said, okay, let's generalize it.  Let's broaden its application, but let's hold onto what we have at this point, which is this secure-by-default approach.



And of course how many times have our listeners heard me bemoan the way code is still being created today?  It is the Wild West.  It's buggy automatically because it is so difficult to create really secure code.  I mean, it's just near to impossible.  And so we'll just cross our fingers and keep our eye on this effort, the Bytecode Alliance, and see maybe if this time we can do it.  You know, Leo, I think it's been a matter of power.  We haven't had the processing power to spare because we're always pushing it to the limit.  We haven't had the bandwidth to spare and the storage space to spare.  But we've got gobs of all that now.



LEO:  Yup.  



STEVE:  So if we can just restrain ourselves from writing everything in C and say, wait, let's finally prioritize security, let's make that first, even if there's a little bit of cost in performance, isn't it worth it, now that we have more performance, finally have more than we need?



LEO:  Yeah, yeah.  I think WebAssem is amazing.  But, you know, you've got to be aware of the potential risks, obviously.



STEVE:  Yeah, yeah.  And I'm glad these guys are going to take it seriously.



LEO:  All right, Steve.  TPM.



STEVE:  Well, not quite.  As our listeners know, I reserve the right from time to time to share some things that I'm enthused about.



LEO:  Oh, I always look forward to this, actually.



STEVE:  Sometimes it's sci-fi books.  Actually most of the time it's sci-fi books.



LEO:  The new Peter Hamilton Salvation Part 2.  Have you read it?  Are you reading it?



STEVE:  Oh, I know.  No.  I'm rereading the first book because it's been several years.



LEO:  This is why I'm going to wait for Book 3.



STEVE:  You're right, you're right.  And he always does this to us.



LEO:  I know.



STEVE:  Remember "Pandora's Star," and how we were like, just dying?



LEO:  Oh, it was painful because he wrote one, and then we had to wait a year or two for the next one.  Oh, it was painful.



STEVE:  But I really like his writing.  And John's reread the first one.



LEO:  John's raving over it.



STEVE:  Yeah, and he says...



LEO:  I trust him and you, yeah.



STEVE:  Yeah, he's been saying that the second one is like really, really good.



LEO:  Oh, I can't wait.



STEVE:  So, okay.  So two pieces of just random feedback.  The second one, I couldn't wait for the podcast.  The first one, I was disappointed.  And I am fully prepared to have a minority opinion on this.  I think I probably do based on its IMDB rating.  But this is "The Mandalarian."



LEO:  "The Mandalorian," yeah.



STEVE:  "Mandalorian," right.  Oh, yeah, I misspelled it here.  Yeah, "Mandalorian."  I signed up for Disney+ on my Roku so that I could watch it.  Lorrie and I sat down with some excitement and anticipation.  Lord knows the trailers look just fantastic.  And about 50 minutes in I kind of was thinking, okay, this is a kiddie movie.



LEO:  Oh.



STEVE:  I mean, something about it, I don't know what it is, but it just didn't have the serious gravitas of "Star Wars" at all.  Okay, now, yes, Ewoks were a problem.  I recognize that.



LEO:  Oh, no.  Oh, no.  Oh, no.



STEVE:  But still, it just didn't do it for me.  So Lorrie refuses to watch the second one, even though it's only 30 minutes.



LEO:  Really.



STEVE:  I've heard it's action packed.



LEO:  I like to give a show three episodes, at least.



STEVE:  Yeah.  So anyway...



LEO:  Because they often get better.  The first one's the pilot.



STEVE:  Right.  And they will be released on Fridays from now on.  The first one came out, then the second one came out like a couple days later.  So maybe I'll watch the first three.  Anyway, I will be doing so solo.  Although I should tell you...



LEO:  Wow.  Wow.



STEVE:  ...it's not because - it's just, well, just because she's got other things to do.  I mean, she really did dislike it.  She just thought, oh, this is really dumb.  But we could not wait for, and we were not disappointed by, the movie we saw on opening night, which I rarely do, actually, last Friday.  We'd been waiting for it for a while.  And the reason I wanted to just mention it to our listeners is that it is very rare that I consider a movie to be perfect.  I mean, it can be about anything.  It doesn't have to be sci-fi.  It's possible to have a non-sci-fi perfect movie.  And I think "Ford v Ferrari" was a perfect movie.



LEO:  Now, this is a little shocker.  Now, it's funny because I just read an article, I think it was in The New York Times, saying it was the box office winner, even though there were movies everybody thought would easily best it.  It made $30 million in the first weekend.  So you're not alone.  There's something about this movie.



STEVE:  You'll know when you see it.  I mean, it is, I mean...



LEO:  Christian Bale I love.  Matt Damon...



STEVE:  Yeah.  You're rooting for, like, for what goes on.  There's maybe a little bit of nationalism because, you know, after all, "Ford v Ferrari."  And so, you know, I'm a patriot.



LEO:  And it's a true story.  This was the story of Ford trying to design a racecar.  It was going to acquire Ferrari, and it failed; right?  



STEVE:  Well, yeah, in fact Ferrari was a little foxy.  They used Ford's bid in order to get a better deal from Fiat.



LEO:  Oh, that pissed Ford off.



STEVE:  And also insulted Ford, talking about, I mean, like, "Go back to your ugly factory and make your ugly little cars."



LEO:  Ooh.



STEVE:  And so Ford Jr., Ford II, said, okay.  Anyway, I've said enough.  If any listeners have found my opinion to be useful in the past and think, I mean, and don't have some reason for thinking, oh, I'm never going to watch that, I will just say I don't think you will find your time to have been wasted.  It was really...



LEO:  Oh, interesting.  And I don't think you're a gear head.  I don't think you're a gear head, so...



STEVE:  Well, I was the top of my class at Bob Bondurant's Sears Point Racing.



LEO:  Oh, you are a race driver.  You're kidding.



STEVE:  And I very much liked my two - I had Fiat X1/9s because they were a mid-engine car and just handled.  They were absolutely neutral steering beautiful little sports cars.  So, yeah, I do love - I love to drive.



LEO:  Oh, that's awesome.



STEVE:  But you don't have to.  This isn't about that.  This is about people and gumption and spirit and courage and, you know.  And at one point I whispered to Lorrie, I leaned over, I said, "This is about art."  I mean, it's just - it's like it's the pursuit...



LEO:  The art of design and pursuit of perfection.  And I understand...



STEVE:  The pursuit of perfection, yeah.



LEO:  There isn't as much driving as you might think in a movie called "Ford v Ferrari," which doesn't bother me.  But Christian Bale in the cockpit, it's pretty gripping, I hear.  It's pretty amazing.  I am dying to see it now.



STEVE:  Yeah.  Don't miss it, Leo.  I know you will love it.  And I bet - I just wanted to make sure that it got onto our listeners' radar because, even if they don't watch it for two years until it streams on something the size of a walnut, it doesn't matter.  Still.  Although you'd really kind of miss it if you were looking at it on a small screen.  But then again, we all have big screens at home, pretty much, now.



LEO:  It was the Washington Post that was writing about it.  And what they're saying is that, in this era of streaming, they really didn't expect a movie like "Ford v Ferrari" to do very well in the theaters because...



STEVE:  It's easy to wait.



LEO:  It's easy to wait.  The headline was "Streaming was supposed to kill original theatric movies.  Don't tell 'Ford v Ferrari.'"  Thirty-one million in ticket sales - beating "Charlie's Angels," which only had nine million.



STEVE:  Well, okay.



LEO:  But, you know, you'd expect - that's a movie you'd expect people to go see in the theater.  Apparently it's terrible, so.



STEVE:  I am not surprised.



LEO:  Yeah, yeah.  Come on.  A reboot of "Charlie's Angels"?  What's not to love?



STEVE:  Oh, I definitely loved the first series, the original one.



LEO:  Yeah, with Farrah, yeah.  



STEVE:  That was a lot of fun.  But I was four at that time, so...



LEO:  What did you know?  What did you know?  So anyway, you're not alone.  The movie is doing quite well, which is - I think it's exciting. 



STEVE:  I'm glad to know it.  And we know that production companies look at the box office to decide what scripts they're going to pick up and what they're going to do.



LEO:  That's right.



STEVE:  So it just...



LEO:  If smart movies are back, I would love to see that.



STEVE:  And Lorrie is a gear head, so she was, I mean, she was down for this thing from the beginning.  The first time we saw a preview when we were seeing something else months ago, we all three of us, because we also went with my best friend, and it's like, okay, that's it.  Definitely going to see that.



LEO:  Oh, how exciting.



STEVE:  And it was better, I mean, again, it was a perfect movie.  It was perfect.



LEO:  Great.



STEVE:  So what's not perfect, it turns out...



LEO:  Uh-oh.



STEVE:  ...is a few implementations of something we need to trust, as you noted, the so-called Trusted Platform Module, the TPM.  And I got a kick out of this.  It has its own site, tpm.fail.  And when I realized that there was now a top level domain .fail, that's going to clearly skew the naming patterns of all subsequent vulnerabilities.



LEO:  Oh, yeah.  Oh, yeah.



STEVE:  There'll be, you know, we.fail; you.fail; he, she, and we.fail.  It's going to be...



LEO:  It's also doing a brisk business in companies like twit.fail, buying it proactively; right?  Leo.fail.  I'm going to go out and buy those all right now.



STEVE:  I wouldn't be surprised if it's expensive.  I know that .sucks is expensive.



LEO:  Same thing; right?



STEVE:  So I wouldn't be surprised if .fail is expensive.  But anyway, so we have tpm.fail.  The best summary, I'll just read from their short abstract at the beginning of their paper.  They said:  "Trusted Platform Module serves as a hardware-based root of trust that protects cryptographic keys from privileged system and physical adversaries.  In this work, we perform a black-box timing analysis" - and so all of our listeners who've been paying attention through the years know what this is about.  It means there's a side-channel attack because it's non-constant time based on its secrets.



"We perform a black-box timing analysis of TPM 2.0 devices" - and, by the way, that's where we are, that's the standard currently - "deployed on commodity computers.  Our analysis reveals that some of these devices feature secret-dependent execution times during signature generation based on elliptic curves.  In particular, we discovered timing leakage on an Intel firmware-based TPM, as well as a hardware TPM.  We show how this information allows an attacker to apply lattice techniques to recover 256-bit private keys for Elliptic Curve Digital Signature Algorithm (ECDSA) and EC-Schnorr (SCHNORR) signatures."  Yeah, it is funny.



"On Intel fTPM" - that's their firmware TPM - "our key" - and this is scary - "recovery succeeds after about 1,300 observations in less than two minutes.  Similarly, we extract the private ECDSA key from a hardware TPM manufactured by STMicroelectronics, which is certified at Common Criteria EAL 4+ level" - meaning that's the best you can get - "after fewer than 40,000 observations."  That suggests that the timing difference is subtler there, but they were able to see a skew and use it.



They said:  "We further highlight the impact of these vulnerabilities by demonstrating a remote attack against a strongSwan IPSec VPN that uses a TPM to generate the digital signatures for authentication.  In this attack, the remote client recovers the server's private authentication key by timing only 45,000 authentication handshakes via a network connection."  They said:  "The vulnerabilities we have uncovered emphasize the difficulty of correctly implementing known constant-time techniques, and show the importance of evolutionary testing and transparent evaluation of cryptographic implementations.  Even certified devices that claim resistance against attacks require additional scrutiny by the community and industry, as we learn more about these attacks."



So in the first place, unlike all of the previous Intel CPU leakage attacks that we were just talking about, these are practical to accomplish.  And as they demonstrated, they can be successfully performed remotely over a relatively high-speed, thus low-jitter, network.  And what I thought was interesting, in their discussion in their research paper they mentioned that Intel was initially not convinced of the attack's severity and gave it a rather low severity score.



LEO:  As usual.



STEVE:  So the researchers then demonstrated a successful attack over the network, meaning that it was a remote key recovery attack which really got Intel's attention.  Intel jacked the severity score up and then got busy fixing it.  The good news is this is a firmware implementation of the Trusted Platform Module within what Intel calls their Platform Trust Technology (PTT).  The bad news is that it's the easiest of the attacks to exploit.



The good news is that being implemented in firmware, it can be patched and updated, and Intel has last Tuesday released patches for the various chip firmware that supports PTT.  And it goes way back, I think at least to 2015, maybe earlier.  So on systems where that's the way the Trusted Platform Module, the TPM, is being used, it could be fixed and patched and then - but of course that requires some means of updating the firmware; Intel releasing the patch.  Unless Windows incorporates it into a security update, it would be incumbent upon the BIOS manufacturers of the affected motherboards to update their BIOSes.



And we know that there are companies that are maintaining their BIOSes going way back, like Dell has been very good at that.  And I think HP is, too.  So you'll want to be looking for a BIOS update for motherboards in the next couple months, and probably use something from Intel in order to determine whether you're vulnerable.  Oh, actually there is going to be - the researchers are going to be making available for open source a tool to test it.  So we'll see if Microsoft is going to patch this on the fly.  Linux is normally really good at doing this.  As we know, for all of the Spectre and Meltdown patches, shortly after Intel released the firmware, Linux had incorporated it into their boot.  So I think Linux-based systems, and for that matter servers that are running Linux, will be able to get themselves secured probably sooner than anybody else.



They did indicate that they, as I mentioned, that they intend to publish tools they used to analyze the vulnerable TPMs, along with proof-of-concept code on GitHub.  Of course, this will be a mixed blessing.  It will allow system admins to determine which TPMs they are using on their probably many various systems to determine whether they may be vulnerable.  But the proof of concept code, which should allow them to detect vulnerability, could also allow those devices to be attacked because the proof of concept would be able to be reverse engineered because it'll all be open source.  So as we know, that would mean that bad guys could take advantage of it.



On the hardware side, there are many manufacturers of Trusted Platform Modules.  The researchers tested all that they could find.  And, for example, those from Infineon and Nuvoton were found not to have secret-based timing influences.  But the very popular TPM by STMicroelectronics, those guys were found to alter its timing as a function of the secret it was manipulating.  So it's not operating in constant time.  The good news is the timing variations are so slight that many, many more tests were required.  On the other hand, they can be performed very quickly on a local system.  However, the network packet jitter inherent in any network communications renders this not attackable remotely.



So the good news there is it's not a remote attack.  The bad news is it's in the hardware.  So there's no fixing it.  I did not dig in enough to see whether it might be possible to stop using an STMicro-based TPM on a motherboard that has it and switch over to using the Intel PTT solution if it's available.  That might be an alternative, to use the patched Intel firmware-based solution, rather than the TPM that's on the motherboard.  I don't know either way whether that would be effective.  But it is important to note that it cannot be used remotely.  So STMicro has updated their device to fix the problem; but for all those many, many, many, many millions of devices that are out there, there is a vulnerability.



Now, the one good piece of news is that this was in the Elliptic Curve Digital Signing Algorithm, which I'm sure I read somewhere, I don't have it in my notes, that Microsoft indicated they don't use.  So Microsoft's normal use of the TPM does not invoke ECDSA.  On the other hand, third parties can be storing their keys in the TPM, like the VPN that I noted earlier, which does make them vulnerable, if not the native operating system.  



So that suggests that, for example, BitLocker, which relies on TPM to store its master secret, is probably safe from this.  And that's probably the highest use of the Trusted Platform Module that any Windows users have is just using it to secure BitLocker.  When I was traveling for the SQRL Tour a couple months ago, I encrypted my hard drive and used BitLocker in order to protect it and relied on the TPM to hide and keep its secrets secret.



So this is not a huge problem for Windows users.  Probably a larger concern in an environment sort of like the Intel, the previous Intel vulnerabilities, where you might have shared hardware, and so you might have something trying to get at the secrets that are stored globally in the TPM hardware.  But at least Intel has a patch ready.  It's because the patch is ready that we are now being informed of this problem.  And maybe it's possible to switch to a software-based solution, if you happen to have the hardware-based solution on your motherboard.  So TPM-Fail.



LEO:  Yeah.  Oh, well.  It doesn't, you know, I'm glad it doesn't sound like it's as bad as I was worried.



STEVE:  Yup.  I'm thinking it's not.



LEO:  Yeah.



STEVE:  So I wanted to put it into context for our listeners.



LEO:  Thank you, yes.  As always, the headlines are scary.  But once we get down to it - that's why we count on you, Steve, to give us the inside scoop.  You'll find Steve at GRC.com.  That's where his most fabulous program lives, his bread and butter, SpinRite, the world's greatest hard drive maintenance and recovery utility, GRC.com.  And I'm happy to say that soon work will commence, if it hasn't already - has it?



STEVE:  Work is commencing shortly.  I have a few last bits to iron out.  They're literally last bits, some status bit dialogue we've been having.  And then I need to update the documentation.  We had someone come into the group with a very critical eye and was very useful to the project because he attacked SQRL.



LEO:  Good, good.



STEVE:  And from the attack we got a lot of good, useful feedback, mostly to - what it turned out is that I had done things in my implementation that I hadn't fully articulated in the documentation.



LEO:  Of course.



STEVE:  So I will explain more clearly what it was, you know, how to do these things correctly where it is critical for them to be correct.  And then it's on to SpinRite 6.1.  And I am very excited.



LEO:  Very excited.  And we should mention that the SQRL, of course the unveiling has already occurred, but we're going to do our special November 30th, a Saturday afternoon, about 2:00 p.m. Pacific.  Are we, yeah, I guess we're streaming it.  So be a little after 2:00 p.m. Pacific, 5:00 p.m. Eastern time.  That's 22:00 UTC on TWiT.tv/live.  We will of course also package it up for distribution so you can all watch it at your leisure.  And we had mentioned that we were going to have a live studio audience.  The response has been phenomenal.  We are full.



However, we are creating a waiting list.  So if people have tickets to see the event in Petaluma on the 30th and won't make it, let us know so we can release your tickets to another worthy person.  It's no shame, of course.  If you're not going to be able to make it, that's great.  The shame would be not telling us because there are many people who'd like to go.  I should also say, for those of you who will be there, we're going to do an after-party little event at the Lagunitas Brewery next door.  We've got a room, and we're all going to be going over.



STEVE:  Gonna hang out.



LEO:  It's going to be a lot of fun, Steve and I and all of you.  And that's always been fun.  We had such a good time in Boston.  I'm looking forward to that.  So November 30th.  Don't forget to get your copy of SpinRite.  If they buy a copy today, Steve, do they automatically get 6.1?



STEVE:  Oh, yeah, yeah, yeah, yeah, yeah, yeah, yeah.  Everybody who has 6.0 and anybody who buys it before gets 6.1.  And in fact, owners of 6.0 will be able to start playing with it way before it's released.



LEO:  Oh, nice.



STEVE:  Very much like SQRL that has been working about a year and a half while all the final pieces got put together.  That's the way we were doing it before.  So owners will be able to use their serial number to look up their what we call the transaction code, and they'll be able to use that in order to download the versions well before its official launch.



LEO:  Nice, very nice.



STEVE:  So there will be some early benefit to those early adopters.



LEO:  Steve, while you're at the website, Steve has lots of great stuff.  We mentioned the Healthy Sleep Formula.  That's there.  We also mentioned, well, we didn't, but we should have, ShieldsUP! and all the other free tools.  They're all there, as well as SpinRite.  So it's a great site to get to and browse around:  GRC.com.  This show is there, too.  He has the only 16Kb versions of the show, as well as the 64Kb audio, and he has the only transcripts of the show.  So if you like to read along while you're listening, that's a great place to go, GRC.com.



We have audio and video at our website, TWiT.tv/sn.  Of course we always recommend you subscribe in your favorite podcast application.  That way you'll get it automatically, the minute it's available.  It helps us, too, because a lot of these apps note how many people subscribe and then in their discovery tab will show the podcast, and it helps us.  So please subscribe in your favorite podcast application, if you haven't done so already.  Steve, thanks so much.  Have a great week.



STEVE:  My friend, talk to you next week.



LEO:  See you next Tuesday on Security Now!.



STEVE:  Bye.



Copyright (c) 2019 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.










GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#742

DATE:		November 26, 2019

TITLE:		Pushing DoH

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-742.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we look at some interesting changes coming to Android and some inherent challenges presented by the nature of the Android ecosystem.  We examine some newly revealed troubles with the venerable VNC clients and servers.  We note a welcome change to Twitter and update on law enforcement's "foregone conclusion" strategy to force password divulgence.  We then look at a surprising pre-announcement from Microsoft about DNS, then dig more deeply into the details of the emerging DoH protocol and reveal a VERY interesting and surprising and unsuspected capability.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 742, recorded Tuesday, November 26th, 2019:  Pushing DoH.



It's time for Security Now!, the show where we protect you and your loved ones, virtually, online, your privacy, too, with this guy right here, Steve Gibson of the Gibson Research Corporation.  Hello, Steve.



STEVE GIBSON:  Leo, great to be with you again.  I'm actually going to be with you again.  



LEO:  Yes.



STEVE:  In a few more days.



LEO:  I'm so excited.



STEVE:  Weather allowing.



LEO:  It's pouring rain here right now.



STEVE:  Yeah.  Lorrie was thinking maybe we should drive, and I said, oh, you know, I don't think a little rain is going to ground the airplanes, so...



LEO:  No, they know how to fly in the rain.



STEVE:  Yeah.  And I figured that all of these cancellations we're hearing about are in the Northwest and also the Northeast, where it's snow and ice and sleet.



LEO:  Yeah, it's not rain, yeah.



STEVE:  Santa is thinking, maybe we can move Christmas to - how about March?



LEO:  Yeah, maybe, maybe.  Yeah, Saturday we're doing a special.  I should mention, so everybody knows, we're going to finally take the wraps off SQRL.



STEVE:  Well, in the official TWiT studio wraps, yes.  I've had a few rehearsals, and I'm ready for...



LEO:  All over the world.



STEVE:  ...the grand finale.



LEO:  Steve's done his world tour, and so it'll be me and Steve.  Is there anybody else you want?  There was some - I think somebody said Father Robert might be in town, if you want to have him on, too.  I do know we have 50 people in the studio.  We have a very full house.



STEVE:  Well, that's good.  And really what my deal is, is just sort of a one-man walkthrough of the - I call it "The SQRL Story," like what happened, what was the original spark, and then how it evolved.  And then why it took so long was all of the other details had to get resolved, like what happens if this, what happens if that, what happens if.  Anyway, so I basically explain, lay the entire thing out, explain the entire thing.  So we can certainly do, and it would be good to have some time for some Q&A at the end.  But it's really sort of a presentation, rather than interactive.  Otherwise we just get tied up in details and off the track.



LEO:  I agree.  I would just derail you.  So, good.  Well, there'll be plenty of people to ask questions.  So that's not going to be problem at all.  And then we're going to be getting together at Lagunitas afterwards.  We're going to have a little meet-up.



STEVE:  Yes.



LEO:  So that'll be fun, too.



STEVE:  Yes.  And it's fun to have just the crowd dynamic, instead of like talking into a dead camera.



LEO:  Oh, yeah.  Now, I should tell people, don't just show up.  Everybody who's coming has already RSVP'd and asked for tickets.  I should also mention there's a long wait list.  However, if you're not coming, but you got tickets, please email tickets@twit.tv and tell us because there are people on the wait list who would love to be in the studio audience.



STEVE:  Once upon a time I belonged to a video dating service.  And they had on the front of the VCR, it said, "Be kind, rewind."  And that sort of relates here.  Don't leave your seat empty.  If your seat's going to be empty, just let TWiT know so that we can fill it with somebody else's butt.



LEO:  There's plenty of people want to come to this event.  It's very, very exciting.  All right.  Now, what have you got for this episode?



STEVE:  This is a tease title because the title is "Pushing DoH," where of course "DoH" is DoH, which we know is DNS-over-HTTPS.  But the way I'm using the word is unexpected, so our techie listeners can say, hmm, what does he mean by that?



LEO:  What does he mean by that?  Hmm.  Hmm.



STEVE:  Pushing DoH.  We're going to find out.



LEO:  All right.



STEVE:  Because I was led into this further by a surprising preannouncement from a major OS vendor, which I thought, oh, that's odd.  Why are they preannouncing?  I have a sense that there's politics going on behind the scenes.  But that caused me to do a little more digging, and I discovered several things that we've never talked about before that sort of changed the territory.  And I'm guessing that the future will look different than we may think.  So we're going to wrap up with that by solving that mystery.



But we have some interesting changes coming to Android that we're going to talk about, some inherent challenges presented by the nature of the Android ecosystem which have come to light as a consequence of a recent report.  We've got some newly revealed troubles with the venerable VNC clients and servers.  I'm sure back in the day, Leo, I know I did, you probably were using VNC.



LEO:  Oh, not back in the day.  Still.  Oh, gosh, yeah.  We even recommend people continue to use it.



STEVE:  Yes.  And in fact it is heavily used, especially in the industrial enterprise.



LEO:  Oh, VNC, not VLC.  You're talking about remote access VNC.



STEVE:  Yes, yes, yes, yes, VNC.



LEO:  Yeah, my favorite VNC client was a Mac client called Chicken of the VNC.



STEVE:  Okay.



LEO:  So you're right, I haven't used that one in a while, yeah.



STEVE:  Well, over in Windows we got UltraVNC, TightVNC, and a whole bunch of other things.  Anyway, turns out a group took a look at VNC clients and servers; and, not surprisingly, it's time to update.



LEO:  Uh-oh.  Uh-oh.



STEVE:  If you haven't.  We also have a welcome change to authenticating with Twitter that I just wanted to make sure, because I know we have, I mean, I know from my own Twitter feed how many of our listeners are Twitter users.  And there's good news on the authentication front, and it only took Jack Dorsey being hacked to make this happen.  We've also got an update on law enforcement's foregone conclusion strategy.  That was something that Jason and I talked about that you missed while you were...



LEO:  Oh, I'm well aware of it.  It's doctrine, not a strategy, that allows them to get your password.



STEVE:  It does.  And so anyway, so there's been some update on that.  We also look at this, as I mentioned, a surprising preannouncement from a major OS vendor about DNS.  And then we're going to dig - that sort of led me to dig more deeply into some details of this emerging DoH protocol that we've been talking about because of its wide application in browsers.  I think our listeners are going to find it very interesting.



LEO:  Can't wait.



STEVE:  So I think another great podcast.



LEO:  I am very excited.



STEVE:  So our Picture of the Week I've had for a while.  One of our listeners processed some number of transcripts and did a word frequency analysis and then created a word cloud graphic.  And, I mean, and nothing really stands out.  Apparently "know," K-N-O-W, and "just," J-U-S-T, are right up there along with "like" and "that's."  So those are rather generic.



LEO:  But that makes sense.  The bigger the word in the word cloud, if you're watching the video, means it's used more frequently.



STEVE:  Yeah, yeah.  But we do have "Microsoft" down there at the very bottom.  We've got "machine."  There's "Leo," your name over there on the right.  "SpinRite."



LEO:  I see "SpinRite."  I see "password."



STEVE:  Yeah, and "Internet," oh, and "security," and "people."



LEO:  I know this is old.  Do you know how I know this is old?  There's no SQRLs.



STEVE:  Yeah, that's a very good point, yeah.



LEO:  We've got to redo this.



STEVE:  Yeah.  So I have had it for a while.  But anyway, I just thought it was kind of cool.  I had been sitting on it and hadn't shared it, so I figured, well, what the hell.  It's be fun just to look at it.



So there are some recent rumblings about Google's possible return to the stock Linux kernel for future Android-based smartphones.  And then there's also some different rumblings about an entirely different new OS that's being developed inside Google.  We've touched on it.  I remember mentioning it a couple years ago.  It's sort of been on a low simmer.  People are still working on it, apparently a hundred Googlers, and that's Project Fuchsia.  But it appears to be still some number of years away.



John Dunn, who was writing for Sophos, recently noted that the momentum appears to be building inside Google for what looks like it would be a radical overhaul of what he described as Android's tortured relationship with its precious Linux-based kernel.  He wrote:  "It's a big job and has been a long time coming, arguably since the mobile operating system was unveiled in 2007."  And I had to do a double-take.  It's like, 2007.  It's been 12 years that this has been going on.  But, yeah, we've had Android with us for quite a while.



So the company hasn't made any firm announcements.  But journalists recently noticed a low-key video posted to YouTube of a presentation given by the Android kernel team chief.  His name is Sandeep Patil.  And that occurred during September's Linux Plumbers Conference in Lisbon, Portugal.



So what's wrong with the way things are today?  We've talked around this a lot because Android security, keeping Android up to date is a real problem.  The development model that underpins how Android uses the Linux kernel, that has sort of evolved, inherently leads to a lot of complexity, which slows down updates, raises costs, and makes life difficult for both Google and for the device makers downstream in all sorts of ways.  The result is that the Linux kernel used by an Android device can be slightly different for every make and model and at different points in time.



The device makers start with the LTS, the Long Term Support kernel, before the device's so-called "out of tree" Android common kernel customizations are added to that original based Linux kernel.  And according to Patil, there are many of these.  They've been working to reduce them just because at one point it was really getting out of hand.  But post-reduction, as of February of 2018, 355 changes needed to be applied which required 32,266 code insertions and 1,546 code deletions on top of the base LTS v4.14.0.  And as I said, even that much tweaking was an improvement over the way things had been.  After that is done, then the system on chip companies like Qualcomm add a bunch of their own hardware-specific customizations.  Then manufacturers add even more of their own vendor and device-specific software.



So the consequence of this is understandably something which is very difficult to move.  And once you've got this built, you're really disinclined to start over, which of course is necessary when various types of problems, specifically vulnerabilities in the kernel, are found.  So what we end up with is multiple sequentially applied layers of customization which require that each device use a root kernel as its starting point, but then all of those changes flow downstream and affect everything else.



So of course we end up with Android devices that tend to have a limited shelf life.  But most problematically, it makes the application of anything we want to change, any patches, much more time consuming, labor intensive, error prone.  And, I mean, the human factor consequence is that it just doesn't happen as smoothly and easily as we know security updates have to be applied.



So what Google appears to be planning is essentially to scrap this whole approach, to return to the use of a base generic Linux kernel, with the goal of eliminating, if possible, the need for individual Android kernel modifications.  So what we would in effect have is an Android Linux kernel, which doesn't get changed, but then on top of which well-defined modules would be placed.  Essentially, sort of fixing the architecture.  You end up with the classic layered architecture with a well-defined interface among layers, rather than what has sort of evolved over the last 12 years, which is, oh, let's just go and apply a patch.  Oh, and let's apply one over here.  And, oh, now we need one over here.



Well, we ended up with 355 of those, which required all kinds of modifications.  And you can imagine when the source changes you have to reinspect all of the things that you're wanting to change and update them in order to correspond to what the source has done.  So anyway, it's looking like Google is in the process of completely changing the way this operates.  And this is good for us because, Leo, I'm sure you would know, the total Android OS base...



LEO:  Oh, it's got to be billions.



STEVE:  It's the largest OS in the world; right?



LEO:  Yeah, it's got to be, yeah.



STEVE:  I mean, because of all of the numbers of devices which are now Android-based.



LEO:  This won't affect 90% of them, though, because almost all of those devices don't get updated.  It's only the new devices that get updated.  So even if they change to a different kernel, only new devices will get that.



STEVE:  Well, the idea being, though...



LEO:  Oh, but from now on this will improve things.



STEVE:  Yes.  Yes.



LEO:  Absolutely.  Google's been working on that with Project Treble and other things.  They've really got to do that, yeah.



STEVE:  Well, yeah.  And in fact in my notes here I mention Project Treble, the Project Treble API which we talked about last year, whose goal was to speed up device patching.  And so this is in line with that.



LEO:  Basically, what Treble does is it puts patches in the store so that we no longer have gatekeepers.  The problem has been manufacturers and carriers are gatekeepers, and they block them.  But if you put it in the store, then the individual updates it, and it gets updated, yeah.  



STEVE:  Right, right.  And we don't know really where Fuchsia is.  I mean, the ambition appears to be big.  It looks like, if the rumors are true, it could replace Android for the smartphone platform.  It is expected to replace the Chrome OS, and they're looking at the desktop, as well, which is, you know, you and I have talked about the idea, for example, of China rolling out an OS from scratch on their own.  I just, I think, wow, that's - why would you do that in this day and age?



LEO:  On the other hand, I mean, when's the last time we had a new OS?  Android, 2007.  And it wasn't even really a new OS because it's Linux.  I think there's something to be said.  Look at all the problems we have now with Windows and iOS and macOS with bugs.  And I think part of it's the fact that these OSes have been around so long, they're just patches upon patches upon patches.  I think the idea of starting with a clean slate is not a bad idea.  We've learned a lot in the last 20 years.



STEVE:  Yes, and I would amplify that and say it's always a good idea.  I mean, the OS of today has a very different architecture.  I mean, for a while there was the whole jargon of the microkernel.  The idea was that you just had this tight little perfect microkernel.  And then instead of having a massive kernel, you would keep everything else out.  Well, that was even the original NT design.  But Microsoft could not help themselves.  They said, oh, but look at all those ring transitions that the GDI is making.  Oh, oh, oh, oh, please, please, please can't we just put GDI in the kernel?  It's like no, no.  But they did.



And as a consequence, now you're able to have a JPG take over your OS as a consequence of the fact that the rendering is being done down in ring 0 rather than the original, you know, the ivory tower concept was, oh, no, those will all be unprivileged processes that operate as services which the kernel calls.  And that way if anything goes evil in them, it can't do any damage.  Well, nice idea.  But it didn't survive because we're all too performance happy.  We just had to have the performance.



Well, now here we are again, and we sort of talked about this relative to WebAssem last week, where the WebAssem guys realize they've got so much performance with a what-has-not-yet-been-adulterated model that put security first that, oh, if we can please just not debauch it, then we could end up with a system which is secure by design, which we don't see around us anywhere.



LEO:  Right.



STEVE:  Because we always have to just cheat and say, oh, well, but gee, look at that performance over there that we're missing.  And people say, uh-huh.  But we did that on purpose; remember?  And it's like, oh, no, we fired him.



LEO:  Oh, no, unh-unh.



STEVE:  He kept storming into all of our meetings and screaming about how we can't...



LEO:  He was driving us nuts.



STEVE:  Yeah, well, we got rid of him, so now we're going to have more performance.



LEO:  Well, no, I think this is interesting and good, although as you point out it's a nontrivial thing to do, so...



STEVE:  It is really.  It's like, oh, boy.  So Kryptowire released a report where - and it was a self-serving report.  I mean, they have an app which scans firmware for flaws.  On the other hand, they scanned a whole bunch of Android devices.  And what they found does teach us a lesson.  So as we know, Android is the number one smartphone operating system.  And whereas Apple has the powerful advantage of having total control over relatively few different hardware platforms, and where related platforms differ, it's often in little details like the screen resolution.  They're not needing to deal, Apple isn't, with an essentially infinite number of hardware variations.  And there's only the one vendor, Apple.  That is to say, they are absolutely vertically integrated.  They're doing all of the hardware design.  It's operating exactly the way they want it to.  They're able to intimately integrate their software to it.  And it's all under their control.



Thank goodness for Google because we don't want to have a mono culture.  We want, I mean, look at what Android has brought to us by essentially creating a platform that allows far more diversity and also far more affordable solutions.  I mean, I look at people who are toting Android phones around.  And I know, Leo, you like them because of the freedom that they offer you, and great cameras, and lots of features.  But there's also, you know, so there's like the high end.  There's Samsung and Huawei and Xiaomi.  But then there's also obscure suppliers.  And I didn't even - they were in this report, so I thought, who's Bluboo and Leagoo and Ulefone and Walton?  Those are brands I'm not aware of, but they exist.



So what's been created, of course, is this very rich ecosystem.  But securing it is a problem.  Kryptowire, well, they start off their report, their first couple lines reads:  "Pre-installed apps and firmware" - that is, okay, pre-installed apps and firmware, that's really the key takeaway - "pose a risk due to vulnerabilities that can be pre-positioned on a device, rendering the device vulnerable on purchase.  To quantify the exposure of Android end-users to vulnerabilities residing within pre-installed apps and firmware, we analyzed a wide range of Android vendors and carriers using devices spanning from low-end to flagship.  Our primary focus was exposing [what they described as] pre-positioned threats on Android devices sold by United States (U.S.) carriers, although," they said, "our results affect devices worldwide."



I have then a page here in the show notes with two charts.  First of all, they found in the selection of devices 146 CVEs in brand new, like just take the wrapper off, phones across the spectrum.  41% of the problems they described as system properties modification.  Oh, I'm sorry, no, 41 of the problems, 41 of the 146, which represented 28.1%.  They found 26 CVEs, which was 17.8%, in wireless settings modification.  30 problems, 20.5%, which allowed command execution on the devices.  There was a little itty-bitty sliver that was AT command execution.  I remember, Leo, when you and I talked about that quite a while ago, we were surprised that AT commands were still in there, you know, Hayes will never die.



LEO:  It never goes away, yeah.



STEVE:  Never goes away.  There were also 34 problems, 23.3%, app installation flaws.  Eight problems were found to allow or affect audio recording.  And 4.1%, and there's no number there, so fewer than eight, probably looks like six or seven, dynamic code loading.  So sort of flaws across the spectrum.  The second chart is a histogram, a bar chart showing which classes of flaw.  There were two:  exploitable by a local app or exploitable by system or signature app.  And what was surprising was the company that had, out of the shrink wrap, highest number of CVEs found was Samsung with, like, 30, looks like maybe like 33 out of the 147 total.  Second was ASUS.  Third was Xiaomi, and Tecno, no, Lava looks like fourth, and Tecno was fifth.  Anyway, so, and I know, Leo, because I've heard you complain about one of the annoyances with the Android devices is all of the stuff, all of the so-called "value add" that is added garbage...



LEO:  Right.



STEVE:  That is added to these things in order to say, oh, look at all these extra bells and whistles we have.  Well...



LEO:  I wish they broke this up a little better because, as you mentioned, red is a local app.  Blue is either the system or a signature app.  And I'm guessing that's those Samsung apps.



STEVE:  Correct, yes.



LEO:  And so we don't know if it's a Samsung app or a problem with the operating system.



STEVE:  So, yeah, right.  What we do know from the report is that it's not problems with the base of the device.



LEO:  With the Google stuff, right.



STEVE:  Exactly.  And the fact that there are devices with so few problems, well, those are devices that are more bare, that they didn't bother to add any junk to them, any high-end customization, you know, gee-whiz features.  They just shipped a base Android device.



LEO:  This pie chart, though, not to throw you back in time, but I can't really figure out - okay.  So 41% system properties modification.  But then underneath it says 28, oh, no, 28.1%, sorry, 41 of them.  What does that mean?  Is that how the hack was?  I don't, yeah, I don't get what they're talking about.



STEVE:  Well, no, it's what it was that the hack was able to do.



LEO:  To do.  Oh, so this is the effect of the hack.



STEVE:  Yes.  So the vulnerability - exactly.  So it was able to modify properties of the system, which it shouldn't have been able to get to.



LEO:  Yeah, okay.



STEVE:  So anyway, they explained.  They said:  "Devices are shipped with pre-installed software.  The apps are not present on or vetted by official app stores.  Most of the functionality is built-in and cannot be disabled.  And the apps run with privilege and system access by default."



So what's happening here is that Google, at the base, is going through all of this effort to create a secure platform.  Then the vendor that wants to distinguish themselves and their device adds a bunch of custom stuff, and the custom stuff has bugs.  And the apps run with privilege and system access by default.  So essentially we've got a hardware vendor who is adding software that some guys wrote for them, you know, some of their software people said, oh, yeah, we'll write some apps.  But they're not professional security people.  They're "whip out the app under deadline and add it to the phone."



And it turns out that those are introducing vulnerabilities into the device that then is able to get in and undermine the security of the rest of the Android phone, as a consequence of the fact that a vendor in this rich Android ecosystem just decided to add some of their own stuff.  There is the potential for remote and local exploitation.  They found instances where backdoor functionality and data exfiltration was possible, all because essentially bells and whistles are added in order to make the device more attractive on the market.  But they're not being done with the kind of eye that we know security needs because it's hard to get that stuff right.



LEO:  Wow.  That's a real argument, then, for buying a Google phone over a third-party phone.



STEVE:  Yes.  And so I'm not familiar.  Do the Google phones, are they only the base phone?



LEO:  Yeah.  They're stock.



STEVE:  And then you add things from the Google Play Store.



LEO:  Yeah.  But remember, of course, every Android phone ships with a lot of Google software on it.  But that's the software you're getting on a Google phone, rather than Samsung's or LG's or anybody else's, or ASUS, or ASUS's [crosstalk].  



STEVE:  Right, right.  Well, and, I mean, we know how difficult it is to do professional secure software.  If someone just whips out some WYSIWYG fancy launcher for an off-brand phone, it's more likely than not going to have problems.



LEO:  Thing is, everybody installs - I have hundreds of apps, third-party apps installed.  I guess the difference is they don't have system access in the way that a Samsung app does.



STEVE:  Exactly, exactly.



LEO:  Plus Google scans all the third-party apps that you get in the app store and so forth.



STEVE:  Exactly.  And so these things are built in.  You can't remove them.  And they never went through any app store scan.



LEO:  That's what I really hate, yeah.  I just really hate that.



STEVE:  Yeah.



LEO:  So that's - I've always been prejudiced against third-party Android.  I really like Google Android.



STEVE:  Yeah.  So VNC users.  Time to update.  Kaspersky's researchers found 37 vulnerabilities, looking at four VNC implementations:  LibVNC; UltraVNC; TightVNC, and they noted 1.x; and TurboVNC.  Kaspersky did responsible disclosure, notified the developers, and most but not all of those 37 identified problems were fixed.  As our listeners know, being a Microsoft user myself, I use Remote Desktop, although only with it safely ensconced behind several layers of traffic filtering firewalls.  You will not find a listening remote desktop port anywhere within GRC's IP space.  That just - it doesn't exist.  But in years past I've played with UltraVNC and TightVNC.



I remember that - you remember Bob Basaraba, my friend, my Canadian friend.  He was just - he had TightVNC installed on every computer of all of the people - I would call them his victims; he would call them his clients - who he was managing.  And I remember that one of the things about VNC that was cool is that with Remote Desktop the client is always connecting to the server that shares its desktop or that provides a virtual desktop environment.  With the VNC tools, you can invert that relationship so that it's possible for the user who is the client to call to the server that is listening, and then they are sharing their computer.  So it's more like the remote service model, which of course is the reason that Bob was doing that.



Anyway, they're still around and going strong.  And as I mentioned, they have a very strong following in the industrial controls environment, where you are typically VNCing into some sort of a user interface for managing some equipment.  And I've played with them in years past.  They have their appeal.  They're typically small and lightweight.  Now, we know that, until recently, just having been fixed, they've got some problems.  And of course I know that this podcast's following has a way above average technical orientation.  So I wouldn't be at all surprised if among our listeners there are people who are using VNC.



So mostly I wanted to make sure that everyone knew, because I don't know how tight the various VNC versions update communication is with its users.  I mean, these things have been around for so long, they way predate any notion of auto updates.  And maybe that's been fixed, or added later, or you're on a mailing list, whatever.  But Shodan reveals more than 600,000 VNC servers currently accessible online.  And Kaspersky believes the actual figure is likely to be significantly higher, probably because it's possible to get the servers to listen on nonstandard ports, and Shodan scans the expected port, verifies the protocol that is listening and answering, and says, oh, yeah, that's VNC.  And so it adds it to its searchable database.



So the Kaspersky guys closely examined four common open source VNC implementations.  LibVNC, as its name suggests, is a library of code on top of which developers can create fully functional customized apps.  And LibVNC is used in systems that allow remote connections to virtual machines, as well as iOS and Android mobile devices.  So it tends to be used.  The problem is that, as is often the case with open source software, if the library gets updated, and this one needed updating, it's not always clear how those changes filter out into other applications that once imported that code base and may now have heavily customized it.  So it could be that there are other applications using LibVNC that, whereas the library itself is going to get updated, it's not clear that all of the descendants of it will.



The second one was TightVNC 1.x, which it's the application recommended by vendors of industrial automation systems for connecting to their human machine interface systems.  TurboVNC specializes in enabling remote work with high bandwidth things:  graphics, 3D, and video objects.  And then UltraVNC is the VNC variant which was specifically built for Windows, also widely used in industrial settings for connecting to human-machine interfaces.  So across those offerings, one problem was found in TurboVNC; four were found in TightVNC; 10 in LibVNC; and by far the most in the Windows version, UltraVNC had 22 problems.  And of course being a networked client and server solution, you need a VNC at each end, and the server will be listening.



The good news is what they found was the bulk of the problems were on the client side rather than on the server side.  So that's good.  They did note that server-side vulnerabilities were significantly less common than the client-side.  On the other hand, the flaws that existed in the server side could allow authentication bypass, which of course we know in the now-infamous Windows BlueKeep RDP flaw is the reason it is such a problem.  So they said that some of the attacks would be impossible without authorization.



So one of the things you absolutely want to do, if you've got VNC up and listening, is to make sure that you've got a strong password.  We know that background automated password, so-called "credential stuffing" attacks are on the rise.  So all the bugs involved incorrect memory usage which led to malfunctions and crashes.  And as we know, that's where exploit work begins.  If you can crash something with some sort of a buffer overflow memory use problem, the advantage that the attacker has is that all of this is open source.  So they don't have to reverse engineer.  They're not having to probe blindly at the server.  If they see that they've been able to crash something and can identify what the server is, then they're able to go into the open source community, get the source, figure out what it is they did to make the thing crash, and see whether they could take advantage of it.



Anyway, so the good news is all of the developers were responsive except the creators of TightVNC, who said that since they no longer support the first version of their system, the 1.x branch, they're not patching the vulnerabilities which were found even though there is a substantial body of TightVNC v1.x on the Internet.  So that might be any of our listeners.  If you're using TightVNC 1.x, know that its updates have been abandoned, and that updates are known, and that some of them are critical.  How many did I say were in TightVNC?  Four in TightVNC, which the maintainers are now aware of, but have said they're not going to fix.  The good news is they've moved on past the 1.x branch.  So if possible, you want to move yourself over to an updated version of that.



So we can hope that the problems in LibVNC will get propagated out to any of the other projects which are using that library in order to take advantage of this virtual network computing technology, which is now really pretty mature.  I took a look at it.  I had some occasion to look at it a few months ago.  And I was impressed that the things were still around.  In general, we've seen a move over towards commercialization of these things.  There were a lot of them that were rather feature lean until you signed up for the VNC as a service plan, which didn't fit the application that I had.  But anyway, I've got the link to a detailed breakdown that Kaspersky provides of all of the vulnerabilities that were found.



And of course the standard advice, if you're going to run one of these things on the Internet, is if you can, if you always know you're going to be connecting from one or some low number of IPs, then filter.  There's no reason to leave any server that doesn't need to be publicly accessible, to leave it publicly accessible.  That is the number one thing I would recommend.  For example, if you're always connecting from one office to another, and your office IP is not changing, just put up a firewall rule so that that port is only accessible from the known IP.  It's a pain, if that IP does ever change.  But it's a matter of just updating the firewall.  And in the meantime, you're not open to the rest of the world.  Period.



And you also want to make sure that you are running the latest version of VNC, if you know that you are a shop that has VNC in there.  And as I said, make sure that you're using as strong a password as possible.  These days, you know, nobody needs to type in a password.  You can just cut and copy and paste.  And I would say that's the kind of password that you want to be using in every instance.



So anyway, just a heads up.  I'm just so glad that we've got an industry now where we're really beginning to take security seriously, and we've got so many security-related shops that are deciding, hey, let's go take a look at this and see what we can find.  They responsibly disclose, these patches get applied to the software, and then hopefully this is the last link in the chain is that the word gets out that it's kind of important to update this software.  



LEO:  Remote access is hard because RDP has problems.  There must be something about it.  It's hard to do.



STEVE:  Yeah.  Well, it is you know, in the case of RDP, and we sort of - we've touched on this a few times.  Generally there's a protocol that is running over that connection.  And a protocol  probably means an interpreter.  And, you know, one of our themes of the podcast is interpreting is difficult.  You just, when you're writing the interpreter, you just assume that the data you're receiving is from the thing on the other end, the other automation, which produced the proper protocol.  Because we had a hard enough time getting it to produce the proper protocol, so you're happy that it is.  So your guard is down.  You're not thinking of, like, oh, but what if somebody deliberately produced the wrong protocol?  Well, you know, that tends to be the way all of these things happen, get exploited.



LEO:  Yeah, yeah.



STEVE:  So Twitter finally allows SMS-based two-factor authentication to be disabled.



LEO:  Turn it off.



STEVE:  Oh, my goodness.  They announced last Thursday that its users will finally be able to disable SMS-based two-factor authentication for their accounts and use an alternative method only, such as the ones we like, the mobile so-called, you know, the one-time authenticator, the OTP app-based approaches.  Or if you're, like, super secure oriented, a hardware security key.  But until last Thursday, believe it or not, it was impossible.  If users wanted to use any form of two-factor authentication for their Twitter account, they had to first register a phone number and enable SMS-based two-factor authentication, whether or not they wished to use SMS at all.  And if you wanted to use a one-time password mobile authenticator app, or a hardware security key, you had to first enable SMS-based authentication in order to get that, and you could not disable it.



So the good news is apparently it took Jack Dorsey being hacked using a SIM swap attack.  And we talked about this.  I can't remember now what it was.  I think it was when I was setting up my Hover account.  I'd been at Network Solutions forever.  I was moving all my domains over to Hover, which is now where I have all of my domains.  And I was very happy to see that they offered two-factor authentication.  And I remember coming on the podcast after that, specifically talking about why I was not using SMS-based messaging to second factor.  I was delighted that Hover allowed me to use an authenticator app.  And this was years ago.  And of course now we've seen example after example of high-profile identification impersonation as a consequence of the fact that the telephone system, we've talked about how the Switching System 7, SS7 system, does not incorporate intersystem authentication.  It doesn't even exist in the No. 7 protocol.  It's just missing.  It was never put in there.



So anyway, I just wanted to give our listeners a heads-up that you are now able to disable SMS-based second-factor authentication, keep one-time password authenticator app-based authentication, and you can also remove your phone number from your account, which also was not possible until recently.  So yay.  I'm not happy that Jack got himself compromised, but it certainly did bring to his and his security team's attention what we've all known for a long time.  And I'm wondering, Leo, do you know any of the back story?



LEO:  Oh, yeah.



STEVE:  Was it because tweeting started off as SMS?



LEO:  Yes.  So for a long time you could tweet via 40404, their short code.  Until very recently you could.  And there was a tool that would - I think it was a - I don't know if it was a Twitter tool.  I think it was third-party tool, Twitter might have acquired them, that allowed you to tweet via your phone.  Maybe you could turn it on.  In any event, it didn't do any authentication.  It just said, well, that's your phone number, so it must be you.  And so the SIM swap meant that all they did was they got Jack Dorsey's SIM, they got his phone number, and then they could tweet as him.  There was no further authentication after that, using 40404.  So what they've really done is they've disabled 40404.  So you can't - I don't think you can tweet via SMS anymore.



I'm a little confused by this story because I thought the same thing.  In fact, I had experienced that, and I wanted to turn off SMS.  For a while I couldn't.  Somebody messaged me, said no, you can.  What you have to do is, as you described, turn on SMS first, set up TOTP, then you could disable SMS.  And I did that some months ago.  So I'm not sure exactly [crosstalk].  Well, it may be that they hadn't ruled it out globally.  Because I do remember a time when you couldn't do that.  So maybe I was lucky enough.  Maybe because I have a blue check.  Maybe because they thought, oh, well, we've got to give this to some people right away.  But I guess it's globally available now.  And it's crazy that you couldn't do that, that you had to have an SMS verification.  But that's not how Dorsey was attacked, to be clear.  



STEVE:  Well, I mean, and tweeting was originally, as we know...



LEO:  Was text.  Well, sort of like text.



STEVE:  Well, it was 140 characters.  It was because an SMS message was 160.  And so you had 20 characters for the addressing, and then 140 for the message.



LEO:  But that was the SIM dark age.  That was 2006.



STEVE:  And so maybe it's just old...



LEO:  2007.  I mean, but no, for a long time they kept 40404 around.  In fact, it's in my address book as Mr. Tweet.  So if I text Mr. Tweet, it would tweet it, yeah.  And that's how they got Jack because, you know, there was no authentication on that system.



STEVE: Yeah.  Well, and we've often talked here about how often legacy stuff, which you're no longer using... 



LEO:  Oh, yeah.



STEVE:  ...but it still around, and it's still active, can come back and bite you in the butt.



LEO:  I'm guessing some people still used it.  That's why they didn't want to turn it off.  



STEVE:  I think that's exactly right.  It probably was like people with a feature phone who were like literally sending SMS messages in order to tweet.



LEO:  Right.  Yeah, because Twitter in a way was kind of an alternative text messaging service, really.



STEVE:  Yeah.



LEO:  They finally arrested the guy who did it.  One of the Chuckling Squad guys got arrested.



STEVE:  Ah.  Good.  Well, maybe that will cool off anybody else who wants to get up to such hijinks.



LEO:  I hope so.



STEVE:  It's good to have some accountability.  Speaking of accountability, I'm glad you know about the foregone conclusion.  Jason and I talked about it when the foregone conclusion...



LEO:  I'll tell you that story.  This is a child pornographer.  And they had his hard drive.  There has been a court doctrine in the past that, if law enforcement knows what they're going to get when the safe is opened - we know the gun's in there, the guy has even said the gun's in there, but he won't give us the safe code - it's been okay to say no, you have to give us the code.  It's a foregone conclusion what we're going to get.



STEVE:  Yes.



LEO:  So it's not testimonial.  The whole issue is testimonial.  You're protected from self-incrimination by the Fifth Amendment of the Constitution.



STEVE:  Right.  Yeah, so, and in the instance that Jason and I discussed, there was a woman in a car accident...



LEO:  Oh, this is different.  This is a different case.



STEVE:  ...who did not want to unlock her phone.



LEO:  Oh.



STEVE:  And so the prosecutor said we know it's your phone because it was in your pocket.  We know you use your phone because we have some records that demonstrated you were just using it.  So it is a foregone conclusion that you are able to unlock your phone.  Therefore you must do so.  So it was...



LEO:  No, I don't think that's it.



STEVE:  Well, no, [crosstalk].



LEO:  It's not that we know you can unlock it, it's that the evidence on the phone is a foregone conclusion.



STEVE:  Yeah, well, in this case...



LEO:  They know you can unlock your phone.  They know you know your password.



STEVE:  Right.  And so but that was, in the case that Jason and I discussed, that was, I mean, this was stretched in order to do that.



LEO: I see.  This is not the case, however, that was just recently decided.



STEVE:  Correct.  So now what has happened is there was an instance where the foregone conclusion was attempted to be used to compel the disclosure of a password.  And so once again testimonial.  That passed the lower court.  But what just happened was that the Pennsylvania Supreme Court ruled that the Fifth Amendment to the U.S. Constitution, again, as we know, against self-incrimination, protecting someone from being compelled against incriminating themselves, did hold, and that the foregone conclusion approach that the prosecutors were taking did not hold.



And so it came down, though, it was a closely divided decision, a 4-3 ruling.  They overturned the lower court order.  And this was a child pornography case where a 64-character password was being used to protect the plaintiff's computer.  And he said, essentially, he said no effing way I'm going to give it to you because we all know what's on the computer, and it's going to be bad for me.



LEO:  That's the foregone conclusion right there.



STEVE:  Exactly.  And so that was the logic that the prosecutors attempted to apply.  It was appealed, and it was overturned on appeal.  And I always like the carefully worded logic of these things, so I thought I would share exactly what the court's thinking was.  They said:  "Based upon these cases rendered by the United States Supreme Court regarding the scope of the Fifth Amendment, we [the Pennsylvania Supreme Court] conclude that compelling the disclosure of a password to a computer, that is, the act of production, is testimonial.  Distilled to its essence, the revealing of a computer password is a verbal communication, not merely a physical act that would be non-testimonial in nature."  And so maybe the unlocking the safe, maybe that's a physical act that's non-testimonial?



LEO:  No, it's testimonial.  That's the same thing.  It's something in your brain.



STEVE:  Oh, okay, same thing.



LEO:  So they can't demand the password.  They can unlock the safe in other ways, just like, you know, they can physically open the safe.



STEVE:  Right, break it, right.



LEO:  But they can't demand the contents of your brain.  And I was looking at the Florida case you were talking about.  You're absolutely right.  The problem in the Florida case was they were trying to redefine "foregone conclusion" to be, well, we know she knows the passphrase.  The court said no, no, no, that's not what it means.  It means you have to know that once you get in the phone, there is the evidence you want.  And you know it from other sources.  And this is in the case of the child pornographer.  They know what's on that hard drive.  He's even said it.  And he's even said, but I'm not going to let you see it because then you put me in jail.



STEVE:  Yes.  Yes.



LEO:  It is a foregone conclusion, but that's not sufficient.  Anyway, continue.  I'm sorry.



STEVE:  Yeah.  So the court said:  "There is no physical manifestation of a password, unlike a handwriting sample, blood draw, or a voice exemplar."  They said:  "As a passcode is necessarily memorized, one cannot reveal a passcode without revealing the contents of one's mind.  Indeed, a password to a computer is, by its nature, intentionally personalized and so unique as to accomplish its intended purpose - keeping information contained therein confidential and insulated from discovery."



They said:  "Here, under United States Supreme Court precedent, we find that the Commonwealth is seeking the electronic equivalent to a combination to a wall safe - the passcode to unlock Appellant's computer.  The Commonwealth is seeking the password, not as an end, but as a pathway to the files being withheld.  As such, the compelled production of the computer's password demands the recall of the contents of Appellant's mind, and the act of production carries with it the implied factual assertions that will be used to incriminate him.  Thus we hold that compelling Appellant to reveal a password to a computer is testimonial in nature.



"We acknowledge that, at times, constitutional privileges are an impediment to the Commonwealth.  Requiring the Commonwealth to do the heavy lifting, indeed, to shoulder the entire load, in building and bringing a criminal case without a defendant's assistance may be inconvenient and even difficult."



LEO:  But it's right.



STEVE:  Yeah, yeah, exactly.  "Yet to apply the foregone conclusion rationale in these circumstances would allow the exception to swallow the constitutional privilege.  Nevertheless, this constitutional right is firmly grounded in the realization that the privilege, while sometimes a shelter to the guilty, is often a protection to the innocent.  Moreover, there are serious questions about applying the foregone conclusion exception to information that manifests through the usage of one's mind."



LEO:  This does not settle it, of course.



STEVE:  No.



LEO:  It's just the Supreme Court of Pennsylvania, and there's many other courts.  Part of it is - here, get this.  Part of the foregone conclusion doctrine that they're trying to bend it into is, well, we know that a decrypted version of the hard drive exists.  So, somewhere.



STEVE:  Somewhere.  In theory, yeah.



LEO:  In theory.  So for that reason we should have access to it.  I'm hoping, but again, this is up in the air, other courts have not held the same way, so far it looks like the trend is going in the right direction, that that is considered testimonial, and you can't testify against yourself.



STEVE:  Yeah, yeah.  



LEO:  Let's hope so.



STEVE:  And of course we've discussed this because the question is do I lock my phone with a password or a biometric?  Because one is testimonial, and one is a physical manifestation.



LEO:  Right.



STEVE:  You know, like handwriting recognition or something.



LEO:  Yeah.



STEVE:  Yeah.  So I wanted to reverse myself, Leo.



LEO:  Uh-oh.



STEVE:  Because I do want to find out what happens to Baby Yoda.



LEO:  Did you at some point say you didn't?  You're going to get Disney+, is that what you're saying?



STEVE:  Well, the bill, I saw the email because a week had lapsed, and so my seven-day trial was up.



LEO:  And you said you hated "The Mandalorian."



STEVE:  Well, yeah, I did.  And I'm going to watch it.



LEO:  Oh.  See, I have still not paid for Disney+.  I haven't even started the one week because I just don't know.  So it's getting good now?



STEVE:  Well, it's not bad.  And...



LEO:  And Baby Yoda?  Is Baby Yoda in it?  Is that the thing?



STEVE:  Yeah.



LEO:  Ah.



STEVE:  And but not THE Yoda.  And that was, I mean, I heard some discussion at the beginning of MacBreak Weekly about this.



LEO:  It's a meme now.  "Boomer, okay."  Yeah.



STEVE:  And so, you know, Yoda, I don't know if we ever knew the name of the creature.  I mean, the species.  Because Yoda was the name of that green guy with the ears; right?



LEO:  I think he's a rubber puppet species, but I'm not sure.  He seems very rubber puppet-like.



STEVE:  Well, I did - I think I'd only seen the first episode when I talked to you last week.  I said, ohhh.



LEO:  Right.  You couldn't even get through it, I think, yeah.



STEVE:  Barely made it.  But then little Baby Yoda manifested, and I thought, okay.  I'm, you know.  And Lorrie looks at me, "Really?  You're going to make me watch this?"  I said, "No, honey, you can read or something."  But I've got to find out what happens.



LEO:  I respect your right to do that.  My opinion changed of the Apple TV Plus "Morning Show."  I hated the first couple of episodes, and it started to grow on me.  I think it's kind of, honestly, a Stockholm Syndrome thing, that after a while, if a TV show's especially bad, you've spent so much time, you've invested so much time in it, that you start to love it.  You just say, well, I'm going to keep watching because it's...



STEVE:  Okay, now, "The Leftovers" was like that for me.



LEO:  Oh, god, I hated the freaking "Leftovers."



STEVE:  Oh, my god.



LEO:  And I hate-watched that one.  You're right.



STEVE:  We were told that it was, by the end, it was the best television ever made.



LEO:  No.  Not even close.



STEVE:  By somebody whose opinion I sort of used to respect.  And so Lorrie and I kept waiting for it to get good.



LEO:  Never gets good.



STEVE:  For three seasons.



LEO:  Yes.  Oh, you watched more than I did.



STEVE:  And oh, my god.



LEO:  Yeah.  "The Affair," same thing.  I thought, this should get good.  These are good actors.  This should be a good - this should be better.



STEVE:  Oh, and it's well regarded, too.



LEO:  And it's highly reviewed.



STEVE:  Yeah.



LEO:  Yeah.  And somehow this affair's gone on for four years, four seasons.  I don't know.  You know what, there's so much good TV, Steve, you don't need to watch the stuff that's only mediocre.



STEVE:  Well, in fact I heard you mention "The Crown."  And that's...



LEO:  Oh, that's a good show.



STEVE:  Yeah?  Good.



LEO:  Do you like that stuff?



STEVE:  I don't know.  I'm going to try.  Lorrie will definitely.



LEO:  She'll like it, yeah.



STEVE:  And so I'll...



LEO:  Did you watch "Downton Abbey"?  Did you like "Downton Abbey"?



STEVE:  No, didn't try that. 



LEO:  Yeah.  See, I think these are more for Lorrie than you.  It's funny because in our house I'm the girl, and I love...



STEVE:  I heard about you crying at the end of the Apple commercial.  So, yeah.



LEO:  Lisa says, "Are you crying at a commercial?"  And I said, "Yes."



STEVE:  And actually I get what it was that had to have happened, even though.  



LEO:  Oh.



STEVE:  And so I can understand.  I can understand how moving it would be.



LEO:  It was beautiful.  It was beautiful.



STEVE:  Now we have to apologize to our listeners for the last 10 minutes of their lives.



LEO:  I'm so sorry, wasted.  That's all right, you can hate-listen to this, too.  No, no.



STEVE:  Okay.  So I'm going to do the first half of Pushing DoH, which doesn't give - which is sort of a good introduction.  It's the reason I got into this, which is that Microsoft has not only just jumped into the DNS-over-HTTPS world, but has in one fell swoop instantly legitimized the various web browsers' efforts to protect their users' DNS queries from the prying eyes of their ISPs.  So sorry, Concast.



One of the other things that Jason and I talked about, Leo, was that Comcast put together a really, it should be embarrassing, set of arguments to Congress about Google.  They singled out Google because of course Google's the whipping boy du jour, hiding DNS queries from them, subverting the Internet.  Well, and of course we talked about how the U.K., what was it they called Mozilla?



LEO:  Enemy of the people or something, yeah.



STEVE:  It was awful, yeah.  And then they backtracked on that after they got a lot of pushback.  It seems clear that we are on the brink of evolving DNS away from UDP, if only, and apparently only, because remember this only is the first hop of DNS.  It's not DNSSEC where it absolutely protects it from hacking.  And it's not the deep DNS which will still be moving across the Internet via UDP.  It's just keeping your ISP's hands off of it is all this really does because it's only creating an encrypted tunnel from your browser to wherever you have pointed your DoH client to get DoH resolution.  But what's interesting is Microsoft has formally stated that they're going to be adding DoH to Win10, to Windows 10.  It's going to get native support.



Here's what they said.  They said:  "Here in Windows Core Networking we're interested in keeping your traffic as private as possible, as well as fast and reliable."  This is Microsoft speaking.  "While there are many ways we can and do approach user privacy on the wire, today we'd like to talk about encrypted DNS.  Why?  Basically, because supporting encrypted DNS queries in Windows will close one of the last remaining plaintext domain name transmissions in common web traffic.



"Providing encrypted DNS support without breaking existing Windows device admin configuration won't be easy.  However, at Microsoft we believe that 'we have to treat privacy as a human right.  We have to have end-to-end cybersecurity built into technology.'  We also believe," they said, "Windows adoption of encrypted DNS" - that is, Windows' own native adoption of encrypted DNS - "will help make the overall Internet ecosystem healthier.  There's an assumption by many that DNS encryption requires DNS centralization.  This is only true if encrypted DNS adoption is not universal.  To keep the DNS decentralized, it will be important for client operating systems (such as Windows) and Internet service providers alike to widely adopt encrypted DNS.



"With the decision made to build support for encrypted DNS, the next step is to figure out what kind of DNS encryption Windows will support, and how it will be configured.  Here are our team's guiding principles on making those decisions."  There are four of them.  First:  "Windows DNS needs to be as private and functional as possible by default without the need for user or admin configuration because Windows DNS traffic represents a snapshot of the user's browsing history.  To Windows users, this means their experience will be made as private as possible by Windows out of the box.  For Microsoft, this means we will look for opportunities to encrypt Windows DNS traffic without changing the configured DNS resolvers set by users and system administrators."



Two:  "Privacy-minded Windows users and administrators need to be guided to DNS settings even if they don't know what DNS is yet.  Many users are interested in controlling their privacy and go looking for privacy-centric settings such as app permissions to camera and location, but may not be aware of or know about DNS settings or understand why they matter, and may not look for them in the device settings."  In other words, yeah, people just don't think about DNS as being a privacy issue.  They don't even know what it is, often; it's so behind the scenes.



Three:  "Windows users and administrators need to be able to improve their DNS configuration with as few simple actions as possible.  We must ensure we don't require specialized knowledge or effort on the part of Windows users to benefit from encrypted DNS.  Enterprise policies and UI actions both should be something you only have to do once rather than need to maintain."  And, finally:  "Windows users and administrators need to explicitly allow fallback from encrypted DNS once configured.  Once Windows has been configured to use encrypted DNS, if it gets no other instructions from Windows users or administrators, it should assume fallback to unencrypted DNS is forbidden."



So anyway, what's interesting to me is that they're not doing this soon.  They're just saying we're making a statement that we concur with the needs to encrypt DNS.  So they said:  "Based on these principles, we're making plans to adopt DNS over HTTPS (or DoH) in the Windows DNS client.  As a platform, Windows Core Networking seeks to enable users to use whatever protocols they need, so we're open to having other options such as DNS over TLS in the future.  For now, we're prioritizing DoH support as the most likely to provide immediate value to everyone."  Well, and of course the reason is the browsers are tending to drive servers to do DoH, so those are the servers that'll be available for Windows to natively talk to, so DoH it is.



They said:  "For example, DoH allows us to reuse our existing HTTPS infrastructure."  And that actually is the trick that I'll be talking about in a minute.  They said:  "For our first milestone, we'll start with a simple change.  Use DoH for DNS servers Windows is already configured to use.  There are now several public DNS servers that support DoH; and, if a Windows user or device admin configures one of them today, Windows will just use classic DNS, without encryption, to that server.  However, since these servers and their DoH configurations are well known, Windows can automatically upgrade to DoH while using the same server.  We feel this milestone has the following benefits."



And I'm going to skip the enumeration because we know what they are.  That is to say, if you have configured Windows to use some whatever it is, DNS servers, Windows in the future will test them to see whether they support DoH and, if so, will automatically preferentially switch to DoH rather than traditional DNS over UDP.



So they said:  "In future milestones, we'll need to create more privacy-friendly ways for our users to discover their DNS settings in Windows as well as to make those settings DoH-aware."  Meaning, for example, a user would have the ability to say I'm pointing you at a DoH DNS resolver.  I only want DoH.  Do not fall back if something is blocked to DNS over UDP.  And of course we know about downgrade attacks where it would be possible to block DoH queries to a configured server in order to cause the person to use DNS, and then you could play DNS games with them.



So they said:  "Why announce our intentions in advance of DoH being available to Windows Insiders?  With encrypted DNS gaining more attention, we felt it was important to make our intentions clear as early as possible.  We don't want our customers wondering if their trusted platform will adopt modern privacy standards or not."  Okay.  Which seems odd to me.  I have to believe that some sort of lobbying at the political level was going on behind the scenes with the browser vendors.  I mean, for one thing, Microsoft is now all Chromium for Edge.  That's where they're moving.  Chromium does support DoH.  So presumably Edge, well, actually we know that Edge does because we talked about that a couple weeks ago, that you could already turn DoH support on in every major browser except Safari.  Opera has it.  Chrome has it.  Firefox has it.  Vivaldi has it.  And of course Vivaldi is Chromium based, so that's where it got it.  So all the browsers can support that currently.



And I just look at this very strong letter that Comcast wrote to Congress, and the fact that Mozilla then rebutted it, just like took it apart point by point.  And then here's Microsoft saying, you know, we believe in DoH.  We're building it in.  Yet you can't get it today.  I just sort of think, okay, they were sending a signal maybe that will be useful to the rest of the industry, that this looks like the direction we want to go.



So it turns out that there's a bit more to DoH than may at first be obvious.  And I wanted to share what I learned after doing some additional digging.  So the first thing to observe is that our goal is encryption to get privacy and certificate verification to know who we're connected to for authentication.  But that means that we have both DoH, DNS-over-HTTPS, and DoT, which you heard Microsoft refer to.  That's DNS-over-TLS.  Either one would give us encryption for privacy and certificate verification for authentication.  And since DoH is DNS-over-HTTPS, and HTTPS is HTTP-over-TLS, my immediate instinct was to prefer the simpler of the two solutions, which would simply be DNS-over-TLS with no HTTPS involvement at all.  So I've been sort of wondering, okay, why DoH?  DoT is simpler, leaner, cleaner, and so forth.



Well, it turns out that DNS could very nicely leverage some of the advances we've previously discussed in the HTTP protocol as we moved from HTTP 1.1, where we've been forever, to HTTP/2.  And we did a podcast about HTTP/2 (SN-495), all the new features that it is bringing to us.  And I'll be referring to some of those, so you don't have to go back.  But okay.



So in the first place, rather than create a new URI label such as dns://, you know, in the same way we have, like, mailto and ftp and the other URI labels.  The designers of DoH chose to reuse all of the external HTTP protocol, so it's just https://, and instead to identify DNS queries using standard HTTP query headers.  So the path of the query would, for example, be "/dns-query."  So that would be the root of the query would be /dns-query.  And then the accept types header, which indicates to the server what type of material you're willing to receive in response to the query.  There's a new application type, application/dns-message.  And the content type header would also be, and that's essentially the format of what it is that you're going to be receiving would be similar, application/dns-message.



So these headers clearly identify and flag the query as, even though it's going over HTTPS, as being a DNS query and wanting a DNS reply.  And it turns out that both the GET and the POST-style queries are supported.  They both carry, in terms of the format of the query, by default the exact same binary identical query data on the wire as DNS itself uses.  So the payload of a DNS UDP packet is, for example, in a POST query, the body of the post would be the binary identical DNS query.  And in the case of a GET, that binary DNS query data would be base URL encoded to turn it into ASCII so that it could be sent as the query tail of a GET query.  And in both cases the replies to these queries are exactly the same binary DNS data as would have been returned over the wire via traditional UDP query.



So, you know, we know that HTTP is already capable of bringing binary data.  That's what our various image formats are, and arguably even the compressed texts that we get in Gzip compression.  That's non-ASCII, it's binary, so there's no problem receiving the result from the HTTP tunneled query in binary.  There is one little variation on this, which is the IETF, which is working to standardize the use of DNS on the wire format, does allow JSON encoding.  So the accept header can specify application/dns-json, J-S-O-N, in which case there is an RFC 8427 which standardizes the JSON format if for some reason you want a pure textual, both a transmission and a reception of DNS using JSON instead of binary.



Okay.  So the point is that in every way, DoH is a DNS query that fits entirely within HTTP and is completely compatible with HTTP.  The fact that you're getting binary is fine with HTTP.  All of our images come over the line that way.  And it's only the query headers which indicate to the servers that that's what this is.  This happens to be a DNS query rather than give me the next page or give me some content, give me an MP3 or whatever.



So the way this was defined, DNS becomes just another HTTP query.  So we've often talked about the crazy mess that a typical website or a web page has become, where the browser pulls down and parses the base page, which to a crazy degree these days then refers to scores of other domains from which it's pulling ads, it's pulling JavaScript scripts from every direction, I mean, it's just this incredible conglomeration of assets which the browser needs to aggregate in order to pull this page together.



And of course so it gets the first page.  It parses the page.  It finds all of the URLs.  It finds all the domain names for the URLs.  Now it has to go and generate a blast of DNS queries in order to turn the domain names into IP addresses, wait for all the queries to come back to get the IP addresses, then initiate connections out to all of those servers in order to ask them for whatever asset it is that the original source page it downloaded has told the browser it needs to get.



So that's a lot of work.  And if we've seen anything over the past decade of work on the web, it's been web browsers and their protocols being revised and updated and massaged to provide the fastest possible experience for their visitors.  Google has spearheaded a bunch of improvements which were initially experimental, and then they worked, and now they've moved themselves into, sometimes merged and sometimes whole, into mainstream web browsing protocol.  So as I've said before, hats off to Google for pushing a lot of these innovations.



We talked about HTTP/2 is inherently a multiplex protocol, whereas the original HTTP was a single request and a query, after which the server would disconnect.  Then we said, okay, that's dumb because now we have to connect again if we want to ask the same server for something else.  So for a while the browser was setting up lots of connections in parallel.  Then we switched it around to maintaining a persistent connection.  And so after the reply to the first query came back, it was possible to ask the connection to be kept alive so that the browser could then send another query over the same connection and get a second thing back.



HTTP/2 dramatically changed that so that now we have essentially a meta connection where we establish a connection, and then we have a multiplex protocol where individual queries are numbered and tagged and can all be sent out in a blast.  And the data that is coming back, the answers to the queries are in packets which are tagged, which allows the browser to essentially simultaneously receive the results of its queries in any order that the server wants to send them.  And then the browser reassembles them on its end.



And as we've discussed, HTTP/2 even allows for the web server to anticipate the content that it knows the web browsing client will eventually be needing even before it asks for it.  After all, it's sending the page.  It knows all of the assets which eventually the web page is going to need in order to be displayed.  So why send it and sit there idly waiting for the client to read the page, parse the page, and start asking for things?  It might as well use that time by just sending ahead, pushing the content that it knows the web browser client will eventually be needing.  And of course it does that.



And if anybody is interested and is confused by any of this HTTP/2 stuff, we did talk about it in the past.  And all of our podcasts are available, as we know, online historically.  So you could go back and brush up on HTTP/2 if you were curious.  And so what happens is the browser will simply find the asset already in its cache, as if it had been pushed by the server to the browser ahead of time.



Okay.  So with that background, imagine a future where the HTTPS website that's being connected to also offers DNS resolution services via HTTPS.  In other words, the website that is offering web pages is also a resolving, a recursive DoH server.  It's already serving HTTPS.  It's already got a certificate.  You've already established a connection.  DoH is just an HTTP query.  So it completely folds into all of the existing protocol that the client and server are operating on, including the benefits of HTTP/2.



So nothing has to change.  In this future, the website knows all of the large number of domains being referenced by the page the browser has requested.  And DNS is inherently a caching protocol and system.  So the DNS resolving web server can have previously looked up and cached the most recent IP addresses for all the domains it will be asking the browser to go and fetch assets from.  And using HTTP/2's built-in send-ahead push technology - thus the title of this podcast, "Pushing DoH" - that web server could prepopulate the client's local DNS cache with fresh and valid DNS lookup addresses, thus completely short-circuiting all of the time required for the client to do DNS lookups for all the domain names in the URLs which appear on the page.



Now, yes, for this to be safe we would probably want the website's DNS replies to be DNSSEC.  So when DNSSEC matures and  becomes available, the replies can be verified secure coming from the resolving site.  On the other hand, think about it.  We're already trusting the web server we're connecting to, to provide us with a whole bunch of content that our web browser is going to go and get.  I mean, it's already providing the URLs containing the domain names that we go look up.  So it's not clear that there's a lot of danger inherent in accepting the IP address for the domain that we were going to look up and get an IP address for anyway from the web server.  But when you think about it, this change of model is so much more efficient overall.



In the original, completely distributed DNS model, every client of an active website which is going to get web pages is needing to perform its own DNS lookups.  And, like so for everybody that gets that page, they're having to do DNS lookups for all of the assets on that page.  And yes, it's broadly distributed.  That's what DNS is.  But why not have the web server do the DNS lookups and then pipe those ahead to the client while the client is receiving the web page and figuring out what it is that it's got to do so that it already has the DNS lookups done ahead of time.



And this is not fantasy.  It turns out these are the plans for where DoH is headed in the future.  And that explains why DoH is what people have jumped on, rather than DoT, DNS-over-TLS.  Even though that's a lot leaner, it doesn't have the advantage of being able to share the existing connection, which already exists, and take advantage of all the advances that we've made in HTTP in order to create a multiplex protocol.  And what this potentially does is dramatically improve the speed at which web pages are able to load.  So, way cool.  Way cool.  



LEO:  Nice.  Nice side effect, yeah.



STEVE:  Yeah.  Yeah.



LEO:  Very nice.



STEVE:  And the user does nothing.  We do nothing.  The web browser will be able to check, generate a query, see if a DoH query is handled.  Or it'll just be receiving DoH replies for lookups it hasn't even gotten around to asking for yet.  And those will then contain non-expired DNS queries which the web server that it connected to will have recently looked up and cached.  And of course DNS, as I said, is a caching protocol.  It's got expiration times and timeouts.  So it just beautifully flows.  And it's just very, very cool.



LEO:  Sometimes doing the right thing has a benefit.  Nice.  You might do it for one reason and get some of the other effects.  So I guess DoH is coming.  It's too bad that DNSSEC did not take off.  Then we wouldn't need this; right?



STEVE:  Well, no.  DNSSEC provides protection for the records, but no privacy.



LEO:  Oh.



STEVE:  So DNSSEC is not an encrypted protocol.  It's a signature protocol.  So it signs the records and prevents them from being forged.



LEO:  Forged, okay.



STEVE:  But it does not give you privacy.



LEO:  Interesting.



STEVE:  But we still need this in order to not be eavesdropped on.  And that, boy, that really does seem to be the main driver for this is that - and it's interesting, too, that ISPs are kicking up such a fuss.  They really don't want to lose that information. 



LEO:  Is there any logical other reason like, well, but then we can't optimize or something?  I mean, is there any other, I mean, is there any conceivable justification for them saying that we want this?



STEVE:  I just think, I mean, so they can recapture the information by themselves supporting DoH.



LEO:  Right.



STEVE:  So, I mean, it's not like - it's not a mystery.  If they wanted, you know, right now they're doing...



LEO:  Good point.  That's actually a good point.



STEVE:  Yeah.  Right now they're doing the DNS for their clients, typically, unless the client has manually overridden DNS. 



LEO:  Right.  



STEVE:  So all they have to do is just bring up, I mean - and if DoH happens, then all the DNS servers are going to offer it anyway. 



LEO:  Yeah.  That's the one problem right now.  At least with Firefox's you only have Cloudflare.



STEVE:  It's very - yes.



LEO:  And really, if you're not - do you trust Cloudflare over your ISP?  



STEVE:  Well, Cloudflare, Google...



LEO:  Google now has one.



STEVE:  OpenDNS offers it.



LEO:  Oh, okay.  But the default in Firefox, there's only one choice.  That's Cloudflare.



STEVE:  That's true.



LEO:  You have to manually enter in - you'd have to find and enter in the address for the others.



STEVE:  That is true.  Although the reason it's only been Cloudflare is that Mozilla...



LEO:  Trusts them.



STEVE:  Really, well, trusts them, and I think we do, too.



LEO:  Yeah, right.



STEVE:  And we can, true.  And they extracted a very powerful pledge about not keeping any query data beyond the time necessary to service the request.  So it's very much like the ExpressVPN guys who are absolutely not doing any logging.  They're like, we don't want to have the information.



LEO:  Right.  And I imagine Cloudflare uses the transit information to help optimize their own Cloudflare services.  So there's a benefit to Cloudflare.  They're not doing it completely for nothing.



STEVE:  Yeah.



LEO:  But not an invasion of privacy, and that's the key.



STEVE:  Right.



LEO:  Yeah.  Thank you, Steve.  Hope you enjoy this show.  And if you do, I hope you'll make a point of coming by every Tuesday around about 1:30 Pacific, 4:30 Eastern time, 21:30 UTC.  That's the easy one, Leo.  21:30 UTC.  We do the live show on a stream so you can watch or listen live at TWiT.tv/live.  You're basically watching us make the show.  Of course then the editors get to it, clean it up, polish it up, put some stuff at the beginning and the end, and put it up on the website.  Steve has a 16Kb audio version of it, as well as a 64Kb audio version, at his site, GRC.com.  He also has something unique:  transcripts.  So if you want the transcripts, GRC.com.



While you're there, take a look at SpinRite, the world's best hard drive maintenance and recovery utility.  Get it now, and you'll be getting 6.1 the minute it's ready.  You'll also be able to use lots of free services - Steve's very generous - ShieldsUP! and so forth.  And of course you can find out more about SQRL, GRC.com.  He's on the Twitter at @SGgrc, and that's a good way to reach him.  You can direct message him, anybody can, @SGgrc.  We have audio and video of the show.  If for some reason you want to watch it, you can at TWiT.tv/sn for Security Now!.



I do encourage everybody lately on the shows to subscribe.  There's a couple of advantages.  For you, the advantage is you don't have to think about it.  You'll just have it the minute it's available.  That's the key to a podcast.  But the advantage to us is, by subscribing in your favorite podcast app - and I don't care which one you use, Apple iTunes or Stitcher or Slack or Overcast or Pocket Casts, whatever, Podcast Republic, whatever you use.  Subscribing tells them, oh, people listen to this show, and maybe they'll feature us from time to time, which we would sure like.  That helps us with discovery.  So subscribe; okay?  Awesome.  Thanks for being here.  And we'll see you next Tuesday on Security Now!.  Bye, Steve.



STEVE:  Bye, Leo.



Copyright (c) 2019 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#743

DATE:		December 3, 2019

TITLE:		Android StrandHogg

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-743.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we revisit free upgrades from Win7 or 8 to 10, which can still be done; an alert for users of HP SSDs; the complications which arise with international privacy treaties when end-to-end encryption might be threatened; the U.S. government's formal permission to hack; a quick look at a particularly devastating ransomware attack; more anti-tracking privacy happiness coming soon, by default, to Firefox; the never-ending headaches caused by Windows DLLs; an update on my "Joy of Sync" determinations; and a look at the way some Android multitasking features can and are being actively abused - with Google's knowledge.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  We'll talk about upgrading to Windows 10.  It may not cost you a penny.  A wonderfully written new memo from the Department of Homeland Security revealing a new vulnerability disclosure policy, and I think the best programmer comment I've ever seen.  That and StrandHogg.  It's all coming up next.  You know what "StrandHogg" means?  You'll find out on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 743, recorded Tuesday, December 3rd, 2019:  Android StrandHogg.



It's time for Security Now!, the show where we cover your security and privacy and safety and how computers work and whatever, and science fiction, whatever else this man here wants to talk about because he is the star, and I mean star of our show, Steve Gibson.  Hello, Steve.



STEVE GIBSON:  Leo, great to be with you again.  And we had a great time on Saturday doing our...



LEO:  Man, you had this audience just rapt, watching every move.  One guy told me, though, "I lost him at prime numbers."  But for most - so Steve did a great SQRL presentation, like really detailed SQRL presentation.  Not just how it works, but the genesis of the idea, the deep guts of how it works, really explained it to everybody.  And we had a wonderful studio audience, about 45 people.  All went over to Lagunitas afterwards.  That was fun, too.  Had a great time.



STEVE:  It was.  And we had one listener flew in from, believe it or not, from Japan for this.  And someone else from Dallas.



LEO:  Another guy who was at the Boston event came in from Austin.  Yeah.



STEVE:  Yeah, Austin, right.



LEO:  So Boston and Austin.  So, yeah, you have some hardcore fans.  Took a lot of selfies again.



STEVE:  So we're helping people's frequent flyer miles in every way we can.



LEO:  I love it.  Anyway, it was a lot of fun.  If you haven't seen it, it's the first episode on our new Events feed at TWiT.tv/events.



STEVE:  Yeah, I loved that.  I didn't understand why I got number one.  But it's like...



LEO:  You're number one.



STEVE:  Event number one, yay!



LEO:  Number two, three, four five, all the way up 100 will be CES.  And then that's where we're going to put stuff that is out of the ordinary that we do, TWiT events.  And then the news feeds will be the breaking news things we do like the Apple, Google, and Microsoft announcements, stuff like that.



STEVE:  Right.



LEO:  So what's on the agenda this week?



STEVE:  So this is interesting.  The title of today's podcast, #743, is Android StrandHogg, with two G's, H-O-G-G, which is a Norwegian word referring to mistreatment by Vikings.  And one may wonder how that became our show title.  It's because of something really interesting.  And it's going to be interesting to see how this evolves.  Four years ago, and we talked about this at the time, there was some research out of Penn State and FireEye, the guys at FireEye Security, noticing that there were some features in Android's multitasking system by design which they demonstrated could be abused.  And Google said, okay, yeah, we don't really think that's a problem.



Well, it's now been found in the wild.  And it's not good.  So anyway, we're going to wrap up by talking about StrandHogg, which is the name that was given to the guys who first found it when banks were reporting that their customers' accounts were being drained as a consequence of what Google deliberately put into Android.



LEO:  Wow.



STEVE:  So makes for an interesting story.  But as we approach the end of patching for Windows 7, I wanted to revisit the free upgrade from 7 or 8 to 10, which it turns out can still be done.  Also we have a very important alert for users of HP's SSDs because they're all going to die.  We also have complications that arise with international privacy treaties, which I found sort of interesting, when end-to-end encryption might be threatened by one of the nations in the treaty.  And what really brought me to this is that apparently they're still rattling their sabers at the DoJ about this and whether they're going to legislate us out of encryption, which, you know, 2020 may be really interesting.



We've also got the U.S. government's formal permission to hack them, that is, within the executive branch, believe it or not.  We'll see how that goes over.  We also have a quick look at a particularly devastating ransomware attack.  I'm trying to downplay ransomware, but it's still there; and this one was just like, yikes, ouch.  We also have more anti-tracking privacy happiness coming soon by default to Firefox.  The never-ending headaches caused by Windows DLLs.  An update, I wanted to check back in to see how my "Joy of Sync" recommendations and self-determinations have been fitting.



LEO:  Oh, good.  I've been waiting for an update.  That's great.



STEVE:  Yeah.  And then we're going to wrap up by talking about some, well, some questionable multitasking features that Google deliberately has in Android that are now looking as though they're not quite so well advised.  So I think another great podcast for our listeners.



LEO:  Uh-oh.  Yeah.  You know what, you're getting good at the teases because now I just have to listen.  I just have to find out. 



STEVE:  Yup.  So our Picture of the Week is one I've been holding onto for a while showing a snippet of comment code.



LEO:  I love this.  I love this.



STEVE:  At the top of some code.  And we've often talked about how some comments you're writing for yourself, some comments you're writing for whoever it is that gets stuck with the task of maintaining the code that you wrote.  And of course ideally people who come along later will augment the comments that they find with more to keep them current.  Anyway, so this is a comment block that reads:  "Dear programmer:  When I wrote this code, only god and I knew how it worked.  Now only god knows."  It says:  "Therefore, if you are trying to optimize this routine, and it fails (most surely), please increase this counter as a warning for the next person."



LEO:  Oh, my god.



STEVE:  And then the next line shows as a variable "total_hours_wasted_here =," and it's currently set to 254.  So anyway, I just thought that was a hoot.  So, yes.  If you encounter this, and the count is large, maybe you should consider not bothering to optimize...



LEO:  Do we know where this came from?  This is awesome.



STEVE:  No, I don't.  Isn't it great?  I love it.



LEO:  Oh, man.  I love it.



STEVE:  So, okay.  It turns out today, even though Windows 10  free upgrade ended on the 29th of July 2016, you can still upgrade Windows 10 for free.  And so I just wanted to mention that because I imagine maybe this month, maybe next month, because after all, as we know, middle of next month, middle of January 2020 is the last security update which Microsoft will be providing for Windows 7.



Now, we say that assuming that they're going stick to, and it seems likely that they will, stick to their determination to force anybody who wants more updates into the paid plan for the following three years.  At the same time, we've seen them reach back even to Windows XP if something really horrific like BlueKeep, in this example, had happened.  Because of course they did go back and patch even Windows XP because the BlueKeep vulnerability in the desktop server was so bad that they wanted to go back and fix it.



So presumably updates stop after the middle of next month, January 2020.  And so I don't recall exactly what it was, or I did know as I was starting to do this research, but I remembered that there was some kind of, I don't know, skeezy way that you could still get an update that didn't really seem copacetic that Microsoft was making available.  And I didn't know, if that was still available, what was going on.



And of course it never really was clear to me why this was time limited in the first place, why they were, after all this massive push - remember there was the, well, of course my own freeware, Never10.  And then there was also the GWX, Get Windows 10, that was unwanted software that was being downloaded that was really pushing people.  And for a while you could push it off, and then it sort of stopped giving you a choice.  You had to hit the X button to close the dialog because you then were only left with a choice of upgrade now or upgrade tonight.  And it's like, wait, what happened to "No, thanks"?



So after all that, then they suddenly said, okay, we're going to give you a deadline.  And I guess the point was to give people the impression that this was really it.  And then if you didn't get it now, then you're never going to be able to get it for free.  And in fact, if you want Windows 10 Pro, oddly enough, the Download Windows 10 takes you to this page where, first of all, they're trying to sell you their Surface hardware because you fill out a questionnaire about whether Windows is fast or slow, whether you run one thing or more than one thing at a time.  It sort of does this weird profiling of you.  Basically  the answer always comes up, oh, you should upgrade to a Windows Surface thing.  But if you answer the questions in some way,  then it will also give you the choice of, well, okay, we guess if you just upgrade to Windows 10 you'll be happy enough.  So it turns out that...



LEO:  By the way, I don't know what page you're going to.  But I don't see that on this page.



STEVE:  Yeah.  If you go down...



LEO:  Windows 10 confirm.



STEVE:  No, I think if you scroll down further because there was something about having to get the license, to qualify for a license or purchase the license.



LEO:  You don't even really need to do that because you don't need a license, is the point.



STEVE:  Correct.  Correct.  And that ends up being the case.



LEO:  So I didn't have to do any of that.  I'm just going to download right here the ISO, 64-bit.  The key is in the past we've always thought, well, if you do that, you're going to have to enter a serial number.



STEVE:  Correct.  And they say you're going to.



LEO:  And they say you're going to.  But you don't.



STEVE:  Yes.  And are you running this from Windows 7?



LEO:  No, this is on a Mac.



STEVE:  Oh, okay.  So maybe it's different if you're running it from Windows 7.



LEO:  I've not seen - there may be other entry points.  I just googled "Windows Media Creator Tool" and went right to that page.  There may be other entry points that you've gone through.



STEVE:  Well, so I created a shortcut for our listeners, grc.sc/win10.  So that takes you to - that's just a shortcut to the official Windows 10 software download, which does eventually  take you to the Media Creators Tool.  That's where it gets to.



LEO:  Well, yeah.  So don't follow your link.  Just google "Media Creation Tool."  It takes me right there.  This is the number one link, and I didn't have any of that crap.  It's a simple click.  So I think you're maybe going in through a sales portal of some kind.



STEVE:  Create Windows 10 installation media.



LEO:  Maybe you're right.  Maybe if you're on Windows 7 it senses.  It has a sense.  But in any event, you don't need to have a key.  It'll download it for you.  Their theory is, well, you're going to download a trial version.  Eventually you'll need a key.  You'll have to activate it in, I think, 90 days.  But apparently not.



STEVE:  Yes.  And in fact Ed Bott, who we know over at ZDNet, he was curious about all this.  He wrote that he had decommissioned a machine back in 2017, a little Intel small form factor PC.  And he was curious about the whole upgrade process.  And so he went through this process and was told that he was going to have to have, like, purchase a license to Windows 10.  But to his surprise, as he wrote, once he went through this upgrade process, he was greeted with a screen that I have in the show notes just saying "Windows 10 Pro.  Windows is activated with a digital license."



You do have to go to the activation service.  But it just moves you along.  So you and I have both told our listeners what I wanted to, which is, even today, even though they sort of say you need to purchase a license - and last night when I was doing it from a Windows 7 machine, I was following through the Download Windows 10, and it took me through this purchase process.



LEO:  Yeah.  I clicked your link.  And for me on this Mac it did the same, the proper page without any upsell.  So I think it's because you were on Windows 7.



STEVE:  That would make sense.



LEO:  Now, there should be a caveat issued here because nobody - Ed Bott, Paul Thurrott - nobody has been able to verify that this is Microsoft policy.  What's not clear - and I just want to say it's not guaranteed to work because Microsoft says it doesn't work.



STEVE:  Right.



LEO:  It seems to work with everybody we talk to.  



STEVE:  Right.



LEO:  But here's the reason I say that.  If it doesn't work, you're now going to have installed Windows 10 on top of your Windows 7, and that may be a pain in the butt for you.  There is a rollback; right?



STEVE:  Yes, yes.  You are able to...



LEO:  Okay.  So you could theoretically roll back.



STEVE:  Yes, you are able to back out.



LEO:  I'd still back it up before you do this would be my advice.



STEVE:  And I did also pursue the state of the online dialogue about this.  And the only problem that anyone reported was the typical, like the upgrade hung because of some random hard drive or the USB, I mean, like there are various reasons why Windows 10 upgrade just fails.  And so when it succeeds... 



LEO:  Microsoft hasn't fixed that part yet.



STEVE:  No.  So when it succeeds, you seem to be golden.  But as a consequence of the fact that it sometimes fails, all of that rollback stuff is there.



LEO:  And you'll be glad if you have an image, too.  So just for safety's sake.



STEVE:  Yeah.  That would be a good thing to do.  Make an image.  But I just wanted our listeners to know, if they're feeling like, oh, boy, I sure do wish I had gotten it when I could, well, I don't think it'll ever not be available.



LEO:  I did see one post on Reddit from somebody who says he worked in the Windows division, and that this is always - you probably saw it, too.  This has always been the policy.  They don't announce it because they want to make money if they can because it is a $130 upgrade if you pay for it.  But he said it comes from Terry Myerson, who's been gone for about a year from Windows, who really was trying to get the upgrade numbers, the percentage of upgrades high, didn't really care.  Revenue, Windows revenue is no longer that important to Microsoft.  Well, I don't - it's not fully credible that Microsoft would turn its back on $130 from every upgrader.  They may be a little bit more nervous about having a lot of Windows 7s out in the wild.  I would be.



STEVE:  Well, yeah.  And remember we covered it at the time.  They were pushing to their shareholders or their stockholders like all of the, what was it, the monetization opportunities which would be made possible by all the things in Windows 10 that all of us dislike.  So it's like, okay, well, that's all still there.  So, yeah.



LEO:  The other thing that's a question mark is whether you need to install on top of an existing Windows 7, or you can do a clean install.  And most of the people I've seen say best to just install on top of.



STEVE:  Yes.  I would also do that because it might not know.  I mean, so you have the option.  You will be presented with the option of creating installation media or just upgrading this system that you're running on.  And for the sake of preserving and essentially promoting your Win7 or Win8 license to Win10, I would say just upgrade over.  But yes, make an image first.



LEO:  Some people have done clean installs and it's worked.



STEVE:  Oh, good.



LEO:  The other thing I would say that's important for everybody to know is the way activation now works, once Windows 10 is activated on a machine, at that point you can wipe it, you can do anything you want.  You have what Microsoft calls an "entitlement" to Windows 10.  That machine for thereafter, as long as you don't change major system components, will be activated for Windows 10.  So a clean install after the fact is fine.



STEVE:  Yeah.  And you and I have always been believers of clean install.  I don't think I have, in fact I am sure I have never once in my life done a major Windows version upgrade.  It's like, it's not worth it.  Just start over.



LEO:  If you have a version of 7 installed, go ahead and install on top of it.  If you don't, go ahead and try because people in the chatroom are saying, no, no, it works with a clean install, too.  So this is what's so strange.  Microsoft has been dead silent on all this.



STEVE:  Yes.



LEO:  And who knows?  After they hear this show, they may flip off the switch; right?



STEVE:  Whoops, we forgot about that page, yeah.



LEO:  But for now it works.  Now, here's the real question, Steve.  Should you upgrade to Windows 10?



STEVE:  Hmmm.  That is a question.  I mean, I'm liking...



LEO:  Would you?



STEVE:  Not immediately, as we know.  I mean, I'd still be on XP if my XP machine hadn't just finally fallen off a cliff.  It just decided, okay, I'm tired.



LEO:  But you're a security expert.  I wouldn't recommend most people run XP online; right?



STEVE:  No, no, no.  No, no.  And I absolutely agree.  I've made peace with Win10.  I have it on a bunch of systems.  I'm comfortable with it.  I have access, thanks to my MSDN subscription, to the long-term servicing channel, so I'm able to install a machine that doesn't have any of the cruft on it.  And I have decruftified a number of standard Win10 Pro machines to get the Candy Crush Soda Saga and all that other nonsense off of it.



LEO:  There's a PowerShell one-liner that will delete all of those apps.



STEVE:  Yes.



LEO:  That's what I always do.  I mean, complaints some people have had, I know you've had, is that you can't really fully turn off the telemetry, the phoning home that Windows does.  But I don't know.  If you're going to use Windows, you might as well live with that.



STEVE:  Yeah.  And of course we know that there are some small percentage of people who have real problems with it.  But again, it's a small percentage.  They're loud.  And Paul and Mary Jo cover those sorts of things when they happen.



LEO:  Oh, yeah.  We hear from them.



STEVE:  And unfortunately it does create reputation damage for Windows 10 because people hear that, and they go, I don't want any of that.



LEO:  I honestly, personally, I think it's as good as Windows 7, which I think was the best version of Windows Microsoft ever made.  I think 10 is fine.  I'm not a Windows fan.  But 10 is no worse than any other Windows, and it's better than most.  How about that?



STEVE:  No.  Yeah, I agree.  And the only thing I wish is that this whole rolling Windows forward thing, which continually creates instability, I wish they'd just let it alone.  And I heard Mary Jo saying that last week.



LEO:  Oh, yeah.



STEVE:  Just leave it alone for a year instead of continuing to fuss with it.



LEO:  They kind of did this.  The most recent update, 1909, was barely a feature update.  It was really more just a cumulative  update of bugs.  And I think that that - everybody loved that.  Everybody said, oh, thank you.  To me...



STEVE:  Did you see that they've announced now formally what the next one is going to be?



LEO:  2004, yeah.



STEVE:  I said to Lorrie, yeah, I said to Lorrie last night, now, this is not going to confuse anybody.



LEO:  No.



STEVE:  Because we're going to have Windows 10 2004.  It's like wait, what?



LEO:  Huh?



STEVE:  In 2020 we're going to get the 2004 version of Windows 10?  What?  



LEO:  Yeah, Paul and Mary Jo don't like it either, yeah.



STEVE:  What are they thinking?  Okay.  Well, speaking of numbers, we are now counting down to 32768, which the programmers among us know is 15 bits in binary.  65536 is the full 16 bits.  So half that is 32768.  Unfortunately, due to a mistake in the firmware running a large set of Hewlett Packard Enterprise Class SSDs, the instant the total power-on running time of any of those SSDs crosses 32,768 hours, 32768, which is three years, 270 days, and eight hours, all of those drives will simultaneously become totally and unrecoverably offline, taking all of their stored data with it.



LEO:  Oh, my god.  That's terrible.



STEVE:  It's a catastrophe.  So HP's customer bulletin, I have a link in the show notes, says:  "Bulletin:  HPE SAS Solid State Drives - Critical Firmware Upgrade Required for Certain HPE SAS Solid State Drive Models to Prevent Drive Failure at 32,768 Hours of Operation."



LEO:  That's a really concrete NTBF.  They all fail at 32768.  All of them.



STEVE:  Yes.  In fact, it even suggests that, if the drives were simultaneously commissioned into a fault-tolerant RAID, they would probably all fail at the same time.



LEO:  Now, these are enterprise drives.  So I'm going to guess they're probably not in the kinds of PCs our listeners are buying, unless they...



STEVE:  Well, we've got, as we know, we have a lot of high-end listeners.



LEO:  Oh, yeah.



STEVE:  Yeah.



LEO:  But not like in your HP laptop.  That's even a different company these days.



STEVE:  Right, right.  So they said:  "IMPORTANT:  This HPD8 firmware" - that's what you want to get, you want the HPD8 firmware - "is considered a critical fix and is required to address the issue detailed below.  HPE (HP Enterprise) strongly recommends immediate application..."



LEO:  Yeah.  Yeah.



STEVE:  Yeah, that'd be handy - "of this critical fix.  Neglecting to update to SSD Firmware Version HPD8 will result in drive failure and data loss at 32,768 hours of operation and require restoration of data from backup in non-fault tolerance, such as RAID 0 and in fault tolerance RAID mode if more drives fail than what is supported by the fault tolerance RAID mode logical drive.  By disregarding this notification and not performing the recommended resolution" - get this - "the customer accepts the risk of incurring future related errors."  They said:  "HP was notified by a Solid State Drive manufacturer of a firmware defect affecting certain SAS SSD models..."



LEO:  Oh, so it wasn't HP software.  Their supplier.



STEVE:  Yeah, exactly, "...used in a number of HP server and storage products."  And now here they are:  the HPE ProLiant; the Synergy; the Apollo; the D3000, D6000, and D6020 disk enclosures; MSA Storage; StoreEasy 1000 Storage; StoreVirtual 4335 Hybrid Storage and StoreVirtual 3000 Storage are affected.  Then they said:  "The following platforms are not affected by this issue:  the HPE 3PAR StoreServ Storage, the D8000 Disk Enclosure, the Nimble Storage, Primera Storage, StoreOnce Systems, XP Storage, and SimpliVity," which is hard to say.



LEO:  That sounds like a Simpsons name.



STEVE:  SimpliVity.  SimpliVity.  SimpliVity.  There it is, SimpliVity, yes, so well named, SimpliVity.  And they finally said:  "The issue affects SSDs with an HP firmware prior to HPD8 that results in SSD failure" at that time.  "After the SSD failure occurs, neither the SSD nor the data can be recovered."  In other words, it bricks itself.  It hard bricks itself at 32K hours.  "In addition, SSDs which were put into service at the same time will likely fail nearly simultaneously."  Yikes.



So in the disclosure they list the 20 SSD model numbers.  So anyone who's listening who worries they may be affected, I would take this seriously.  And they do have firmware update software for Linux, Windows, VMware ESXi, provided from links at that.  So wow.  And it's not clear that everybody who has these is on the mailing list and is going to see this announcement.  So I do hope that this information gets picked up and covered enough that people aren't going to be hurt.  



LEO:  I'll say it on all our shows through the rest of the week because there's people need to hear it, yeah.



STEVE:  Yeah, because it's belly-up.



LEO:  No matter what we do, though.  There's going to be people lose data.  It's going to happen.



STEVE:  Yeah, yeah.  For example, as we know, how many BlueKeep vulnerable RDP servers are currently exposed?  Well, we know it's about half a million, despite the fact that the news couldn't have been any more well covered than it has been.



LEO:  Can you believe there are people who don't listen to Security Now!?



STEVE:  Doesn't matter.



LEO:  What is wrong with them?



STEVE:  Well, apparently fewer every day, Leo.  So that's good.



LEO:  We're going to take over the world at this rate in the year 32768.



STEVE:  Yeah, wow.  So it turns out the EU is not happy about a possible U.S. encryption ban.  And when I read that headline, I thought, wait, wait, what possible U.S. encryption ban?  Well, okay.  So it's nothing that our listeners haven't heard about.  Although it turns out that the ongoing battle over end-to-end encryption took a bit of turn last week when EU officials warned that they may not take kindly to a U.S. encryption ban or the insertion of a crypto backdoor in the U.S.-based technologies.  And again, as I said, 2020, it really does promise to be an interesting year.



Back in June of this year, it turns out that there was a little-publicized meeting where senior U.S. government officials quietly met to discuss whether they could legislate tech companies into not using unbreakable encryption.



LEO:  Whoa.  I don't remember that.



STEVE:  Unh-unh.  It was not covered.  There was apparently a story in Politico that covered the NSC, the National Security Council...



LEO:  Oh, I do remember that.  But I just dismissed it as political posturing.  



STEVE:  Right.



LEO:  I do remember this story, yeah.



STEVE:  Right.  Because they were wondering whether to ask Congress to outlaw - outlaw - end-to-end encryption.  So of course let's just punt this one.  So U.S. officials did not reach a decision one way or the other on the issue.  But news of the conversation spooked some enough to ask the EU some formal questions which were picked up by somebody at Techdirt, Glyn Moody at Techdirt.  The person asking the EU was someone named Koerner, who asked whether the Commission would consider a similar ban on encryption in the EU.  He also asked what a U.S. ban on would mean for existing data exchange agreements between the EU and the U.S.



And it turns out that that actually does affect our various treaties.  So the question was posed, would a ban on encryption in the USA render data transfers to the U.S. illegal in light of the requirement of the EU GDPR for built-in data protection?  So at the moment, our two regions, the EU and the U.S., enjoy an agreement known as EU-U.S. Privacy Shield, which they introduced after the European Court of Justice invalidated a previous agreement which was called the International Safe Harbor Privacy Principles.



So today's Privacy Shield, which is what we have now, is a voluntary certification scheme for U.S. businesses.  By certifying under the scheme, U.S. companies prove their adequacy to transfer and process data on EU citizens.  It shows that the companies have made some effort to follow Europe's strict privacy principles in the absence of any cohesive federal privacy law in the U.S.  So it's sort of a stopgap, let's keep doing things while we figure out what direction the U.S. is going to take.



On November 20th, just a couple weeks ago, European Commission officials replied with their answers, confirming that they would not consider a ban on encryption in the region and pointing out that the General Data Protection Regulation (GDPR) explicitly refers to encryption as a "privacy protection measure."  But the answer to the next question was a bit more contentious.  They said:  "If the U.S. were to enact new legislation in this area, the Commission would carefully assess its impact on the adequacy finding for the EU-U.S. Privacy Shield" - which as we just said is that framework which the Commission has found to provide a level of data protection that's essentially equivalent to the level of protection in the EU, thus allowing for the transfer of personal data from the EU to participating companies in the U.S. without any further restrictions.



So the jury's still out on how the EU would react to cross-Atlantic data transfers if the U.S. were to implement crypto backdoors, which it seems unlikely that end-to-end encryption itself would be outlawed.  But we keep having U.S. law enforcement being unhappy about the going dark problem and their inability to serve a warrant, at least, which would compel the decryption of specific conversations.



So there's an attorney, Ashley Winton, who is a U.K.-based specialist in privacy law, who explained that a split between the two territories on data exchange could have serious consequences.  He said:  "We know that under the GDPR personal data must be held securely; so legislating against strong encryption or introducing legal backdoors is not going to be good for the safe passage of European Personal Data, howsoever it gets to the U.S.  Unlike the annual review of Privacy Shield, in theory, if the European Court were to determine that the transfer of Personal Data to the U.S. was no longer safe, all affected transfers could be halted immediately."  Well, that's not going to happen.  But apparently it's going to cause some sort of kerfuffle.



So anyway, I just thought this was interesting, that we're sort of sitting here worrying about what the U.S. is going to do and what's going to happen with our technology.  It hadn't really occurred to me that Europe is looking at the strength of their GDPR and essentially requiring that, to the degree that U.S. companies end up with EU citizen data, our U.S. companies need to be offering a similar level of protection.  And it may well be that law enforcement requiring the ability, depending upon the way it's implemented, I mean, again, "backdoor," as we know, is a heavily freighted term.  But it may cause some ruffling of treaties between the U.S. and the rest of Europe.  So it'll be interesting to see how that develops.



And Leo, let's take our second break.  And then we're going to talk about this interesting thing that happened in Washington, where agencies of the government are being encouraged to allow themselves to be hacked.



LEO:  Okay.  Yeah, I remember when the NSC considered it.  And I think we may have reported on it.  But given that by the time the story came out in Politico, they'd already declined to do it.  And given the composition of the NSC at the time, I didn't judge it to be a serious interest in doing this.  And you could see why.  Somebody might have said, what if we did this?  And then somebody maybe more astute said, if we do that, we can't do business with the EU anymore.  Might have just killed it; right?  I mean, I don't know how strong the interest is or was in doing it.  I hope it wasn't strong.  And we know that the law enforcement wants to do it.



STEVE:  Well, but Leo, we know it hasn't gone away.



LEO:  No.  The FBI director asked for it, and a number of people have asked for it.  Let's just hope saner heads prevail.



STEVE:  As people say, you may ask.



LEO:  You can ask.  I think also the fact that the EU said this might have been also a little bit of a, hey, guys, over here.  If you do that, just be ready because it's going to cause some waves.  That may be the whole point of the EU saying this; right?



STEVE:  Right, right, right.  



LEO:  Just a little reminder that it affects everything.



STEVE:  It's not all about you guys, yes.  It's not all about you Yanks.



LEO:  Yeah.  I think that's probably what's going on there.  Just in case anybody thought this was a good idea.  Back to Steverino.



STEVE:  So under the heading of this should be interesting, the Cybersecurity and Infrastructure Agency (CISA), which we run across that acronym from time to time, they're active and doing things.  It's part of the Department of Homeland Security.  They've just published what they call the VDP, the Vulnerability Directive Policy, requiring executive branch federal agencies to be welcoming and responsive to cybersecurity bug reports from the general public.



I have a link in the show notes to the CISA.gov page.  And they said:  "A VDP directive and you."  They said:  "Today we're issuing a draft binding operational directive, BOD 20-01, which will require federal civilian executive branch agencies to publish a Vulnerability Disclosure Policy.  A VDP allows people who have 'seen something' to 'say something' to those who can fix it.  It makes clear that an agency welcomes and authorizes good faith security research on specific Internet-accessible systems.



"In preparing this directive, we worked with several agencies that have VDPs and made an effort to align the directive with federal guidance, international standards, and good practices.  But this directive is slightly different from others we've issued, where agencies are directed to take an action, and then CISA verifies the action has taken place.  Here, while agencies must maintain VDPs that are the beneficiaries of vulnerability reports, it's the public that will provide those reports and will be the true beneficiaries of vulnerability remediation.  That's why we're doing something we've never done before with our directives:  seeking public feedback before issuance.



"We want to hear from people with personal or institutional expertise in vulnerability disclosure.  We also want to hear from organizations that have a VDP and manage coordinated vulnerability disclosures.  In seeking public comment, we're also nodding to the fact that, to our knowledge, a requirement for individual enterprises to maintain a vulnerability disclosure policy has never been done before, and certainly not on this scale.



"So what does the draft directive do?  It lights a fire.  Each agency must publish a VDP and maintain handling procedures, and the directive outlines a set of required elements for both.  It draws a line in the sand.  Systems 'born' after publication of a VDP must be included in scope of an agency's VDP.  It expands the circle.  Until everything is included, at least one new system or service must be added every 90 days to the scope of an agency's VDP.  It starts the clock.  There's an upper bound - two years from issuance, in this draft - for when all Internet-accessible systems must be within scope.  All are welcome.  Anyone that finds a problem must be able to report it to an agency.  No 'catch and keep.'  An agency may only request a reasonably time-limited restriction against outside disclosure to comply with their VDP.  And defense, not offense.  Submissions are for defensive purposes.  They don't go to the Vulnerability Equities Process."



And then they had two points for what it doesn't do:  "It does not establish a 'federal bug bounty.'  A bug bounty is a program that pays researchers" - as we know - "for valid and impactful findings.  Nothing in this directive prevents individual agencies from establishing a bug bounty of their own, though.  And does not create a 'national VDP.'  The directive is an executive branch policy instruction that requires federal civilian executive branch agencies to have a VDP.  The difference might appear slight, but they're very different things."



And then it says:  "Why isn't this a national VDP?  We think a single universal vulnerability disclosure policy for the executive branch is a good goal.  It makes sense particularly when each agency has all Internet-accessible systems in scope, but we expect that goal to be an unrealistic starting place for most agencies.  Instead, the directive supports a phase-based approach to widening scope over time, allowing each enterprise - comprised of the humans and their organizational tools, norms, and culture - to level up incrementally."



Then they finish with:  "Doing good things together.  We believe that if you make good things easier to do, more people will do them.  With this directive, we want to take steps that diminish complexity and make expectations plain.  In support of that, we're also sharing draft implementation guidance on the directive, as well as a draft VDP template."  They said:  "We welcome your feedback and perspective on all these documents, as well as any comments on our approach.  Public comment will take place on GitHub and last until December 27th," so basically for one month.  I've got links to the details in the show notes, and I think this represents a welcome and really forward-looking and insightful step forward for the U.S. government.



I'm sometimes surprised, but I'm really pleased to see it, basically encouraging, explicitly encouraging the publication of formal guidance for people who find problems with government systems.  You're not risking, probably, given that you provide the information to the relative agency in a responsible fashion, you're not going to have the feds knocking on your door, hauling you away to jail, given that you've disclosed responsibly.  And the idea that it's going to be requiring, every 90 days, something else needs to be put in scope; and, regardless, everything must be within scope in two years.  It means that two years from now everything that this VDP policy covers will be subject to scrutiny from the outside world, and the nature of responsible disclosure explicitly laid out.  So bravo, surprisingly so.



LEO:  I also want to say bravo to Jeanette Manfra, who - I don't know who that is.  She wrote the memo.  It is the best written bureaucratic memo, I mean, it has "pineapple pizza" in it.  She is a good writer.  And maybe that's...



STEVE:  I agree with you.  I had this - yup.



LEO:  Yeah.  She's assistant - she's not some flack hired to write these, either.  She's the Assistant Director for Cybersecurity for the Department of Homeland Security.  So she is a fairly high up bureaucrat.  But man, can she write.



STEVE:  I agree.  I thought it was really well written.



LEO:  Oh, she's a former communications specialist and military intelligence office.  Wow.  They're doing something right there, I'll tell you.



STEVE:  Yeah, nice.



LEO:  Have to get her on one of our shows.  Do you think she's be able to talk about stuff?  This is a really interesting...



STEVE:  Yeah.  Oh, like without being bound by crippling non-disclosures.



LEO:  Yeah.  She says:  "At CISA we work to do good things.  Some are easy, like eating pineapple on pizza."  And she has a link, by the way.  "Some are hard, like managing risks in 5G.  Yet we know if it's hard to do good things, most people won't do them.  And reporting a vulnerability on a government system shouldn't be so hard."



STEVE:  Yes.



LEO:  It's really well written.  Anyway, sorry, I just - I threw that in because I...



STEVE:  No, I'm glad you did because that helps...



LEO:  You don't usually see that.



STEVE:  Yes.



LEO:  By the way, the pineapple pizza takes you to an article on understanding foreign interference, "The War on Pineapple."



STEVE:  Okay.



LEO:  This is interesting.  Targeting divisive issues, like pineapple on pizza.  Moving accounts into place.  This is actually a really good slide show on how foreign interference works.  "Pineapple Pizza Controversy Rocks the U.S."



STEVE:  Oh, right, fake news.



LEO:  Fake news.



STEVE:  Yup.



LEO:  Wow.  I'm impressed.  Wow.  I'm sorry.  Didn't mean to derail you.



STEVE:  No, no.  I'm glad.  I'm glad for that.  



LEO:  She deserves some credit.



STEVE:  So as our listeners know, last summer was the summer, sort of before the school year, the summer where I had a hard time, I think ransomware was the overwhelming topic of the podcast for three or four weeks running because there was just so much of it happening.  And I promised I wouldn't do that anymore.  But sometimes something that's particularly egregious happens, and so it's necessary to just take a moment and mention it.



In this instance we have - this is like the big win, unfortunately, for ransomware guys, is when, and we've talked about this also, the managed service provider, the idea that there is a single organization that is providing managed services to some large network of their customers and clients.  And something happens at the managed service provider to let bad guys get in.  They then set up shop; they figure out like where they are; and they say, "Holy crap, look what we have access to."  And then they of course are able to essentially create a devastating attack.



In this case, this is a healthcare managed service provider that was responsible for providing services for 110 nursing homes.  So think about that, 110.  And also some acute care facilities.  All of them have been simultaneously crippled by a ransomware attack on their common IT provider whose name is Virtual Care Provider, Inc. (VCPI).  They're a Wisconsin-based managed care provider.  They provide data management and records hosting, security and access management to nursing homes across the U.S.



Brian Krebs picked up on this last Monday while the attack was still underway.  He said that it involves our old friend Ryuk.  And as we know, the hackers who were driving Ryuk are known to calculate how much ransom they feel the victimized organizations can be forced to pay based on the size and perceived value of the destruction that they have wrought.



But we've also seen instances where the crooks get it wrong.  And that appears to be the case here.  VCPI's chief executive and owner, Karen Christianson, told Brian that her company simply cannot afford to pay the $14 million bitcoin ransom.  I mean, you know, this is - yes, they're a managed care provider for 114 healthcare facilities.  But it's a competitive industry.  They don't have $14 million.  And apparently no insurance.  She said they can't begin to pay the $14 million ransom.  As it is, it turns out, VCPI's own employees have been asking when they'll get paid because it's their own salary infrastructure was also hit.  But she said the top priority is to wrestle back access to the lost electronic medical records.



The attack successfully affected all of the firm's core offerings - Internet service, email, access to patient records, client billing, and phone systems - and even the internal payroll operations that VCPI uses to pay its own workforce of nearly 150 employees.  Regaining access to electronic health records is the top priority because, without that access, the lives of the seniors and others who reside in critical-care facilities are at stake.



Christianson said to Brian:  "We have some facilities where the nurses can't get the drugs updated and the order put in so the drugs can arrive on time.  In another example we have an assisted living facility that is just a single unit that connects to billing.  If they don't get their billing in to Medicaid by December 5th" - that's two days from now - "they close their doors.  It's over, and seniors who have no family to go to are then suddenly homeless," Karen said.



LEO:  Oh, my god.  These people, these ransomware guys are jerks.



STEVE:  I know.  They said:  "We have many clients right now who are demanding, 'Just give us our data,'" she said, "but we can't.  We don't have it."  So imagine how devastated she must be.



A report from Vanderbilt University's Owen Graduate School of Management noted that the corrective actions being taken to harden and secure facilities and systems can, at least temporarily, worsen the problem.  They wrote:  "Corrective actions are intended to remedy the deficiencies in privacy and security of protected health information.  However, enhanced security measures may introduce usability" - which they said "we define as the ease of use" - "problems.  New security procedures typically alter how clinicians access and use clinical information in health information systems and may disrupt the provision of care as providers require additional time to learn and use the new and modified systems."  In other words, yeah.  Additional layers of security can be annoying.  But of course they can prevent this.



But anyway, obviously, the group behind Ryuk are serious.  And in another similar attack there were hundreds of veterinary hospitals were similarly affected.  So I just wanted to point out that these sorts of cyber attacks that are of this nature, I mean, are having devastating effect in the real world.  And as you said, Leo, these guys are just real creeps.



LEO:  They're also stupid because they're asking for more than the company can pay.  They're never going to get money.  So it probably is a 14 year old, I mean, with no real life experience.  I mean, that's too bad.  That's the problem with Ryuk is a script kiddie can do it now.  Anybody can do it.  



STEVE:  Yeah, exactly.  And Brian reported that the attack was unleashed inside VCPI's networks around 1:00 p.m. Central time on the 17th of November.  So it's been happening for some time.  It could have been lying in wait for a while as the intruders mapped out the internal networks and compromised the resources and data backup systems in preparation for the ultimate attack.  Because of course they want to nuke the backups also, if they have any opportunity to do that, because that renders the victims utterly helpless.  And so apparently these people were acting as responsibly as they could, but the bad guys were in there long enough to be able to find and compromise their backup systems, as well.  So we just can't say, well, yeah, you deserved it because you weren't doing the right thing.



And this takes me back to the point I've been making about any of our listeners who may be in enterprises using managed services.  What tends to happen is that the managed service provider just says, well, give us Remote Desktop access into your network.  We talked about last week how most managed service providers have admin-level access into the networks of the clients that they're providing services to.  Well, yeah.  That means if something gets into the managed service provider, they have admin access into the networks of their clients.  You don't want that to be you.  Generally, that level of access isn't needed.



So everyone wants it because it's easy, in the same way that we used to all just want to run as admin because, oh, look, I can do anything I want to without having to switch users.  And we've learned the lesson of how that's not safe.  So, boy, be really careful.  It's not just your own network that you need to be careful about.  You really need, essentially, as IT, you're indirectly taking responsibility for the security of anybody who you let in.  And managed service providers are getting knocked over.  So I just wanted to drive that point home again.



Firefox is seriously pushing back on tracking signal leakage.  As we know, cookies are not only the most obvious, sanctioned, and most easily controllable aspect of web browser-based Internet tracking.  They are what people use by default.  But as we've talked about, "fingerprinting" makes use of many subtle signals the browsers may send in an attempt to lock onto a user who is deliberately attempting to thwart cookie-based tracking.



And as it turned out, when I was putting this together last night, I realized, oh, yeah, me, too.  I was sitting in front of a very wide screen.  It's 3840x1600 screen.  And so it's impractically, I think it's probably the one that Lisa has, Leo, because I know you have mentioned, I've heard you say that Lisa has also a really wide...



LEO:  Oh, it's so amazing.  Every time I see it, I say, "I want this."



STEVE:  And so you don't run that.  You don't run a browser full screen on a screen that wide.  It doesn't need it.  It's ridiculous.



LEO:  Right.  No, no.



STEVE:  So I realized that I had the browser that I was using as I was pulling the show together last evening, it was on some random arbitrary slice of the screen, meaning that its x-coordinate and its horizontal width were random, well, they weren't random numbers, but they were just some, you know, they were not standard numbers.  It wasn't X of 0 and width of 1600, which is like a standard resolution.  I was at some particular 1326 X and 1842 in width.  Which is something that script running in my browser, that is, script running in an ad is able to see.  Which means, so long as I don't move my window around, and generally when I relaunch my browser it comes back where I last used it.  So that tends to be kind of a generally sticky little bit of beacon information that I'm sending out as I move around the web.



And there are, as we know, all kinds of similar soft signals which, when they're merged together, can serve to profile us.  Things like what time zone we're in, the exact pixel layout of our browser, which fonts we have installed in our system.  And so just deleting cookies no longer tells the whole story.  We talked, in fact, about how there was a navigator.getBattery function which was allowing advertising scripts to determine the exact battery charge of the machines their users were using, and note that it tends to change predictably over time.  I mean, we tend to think, oh, my lord, nobody is going to go to the trouble of, like, monitoring that.  It turns out people are.



So anyway, that's the backdrop against which Firefox has been very vocal about introducing explicit anti-fingerprinting code into the base browser.  It's there now, but it is not turned on by default.  And we've talked about various of these things over time, like it's even possible for an ad to draw something in, like draw something with vector art and then snap the pixels that result, and look at subtle variations in the way lines are drawn and rendered into digital pixels.



Turns out those things vary subtly from OS to OS and from version, even version of operating system, and browser to browser.  So there's that.  Absolute screen coordinates would be obscured.  The image extraction would be blocked.  They're considering rounding window dimensions to multiples of 200x100.  Only specific system fonts would be enumerated.  The time precision would be reduced to 100ms, and 100ms of deliberate  jitter would be added.  The keyboard layout would be spoofed.  The locale would be spoofed to simply English and U.S.  The date input field and the date picker panels would be spoofed.



I mean, clearly the bad guys have gone to so much effort in order to pick up on signals.  The time zone that we're in would not be accurate.  It would be spoofed to UTC so that everything would be uniform.  And all device sensors would be disabled.  Turns out you can read things like position of the device that the user is holding.  So of course the downside to all of this is that any websites that were making legitimate and beneficial use of those details, like we've talked about how could you use battery level, well, maybe if you were a particularly conscientious gaming app you might back off on the amount of high-energy GPU usage if you saw that a laptop's battery was getting lower, or you might proactively say to the user, hey, you'd better save your game, looks like things are getting dangerously close to low.



So there are useful things this can be done for, or useful applications for these things.  But unfortunately it turns out the actual users of these, as instrumentation has shown, are the bad guys rather than the good guys.  So despite the fact that all of these features have been added to our browsers over time, they're being used against us more than for us.  So the good news is, as we know, there's a tyranny of the default.  Most users are running Firefox just out of the box.  Our listeners probably already have fingerprinting turned on.  I know I do.



The good news is we're now at v70.  Two major versions from now, at Firefox 72, if everything continues to go smoothly, that fingerprinting default will be switched to on.  So as Firefox is updated across the board to v72, this kind of long-term fingerprint tracking will, at least for Firefox users, become increasingly difficult and, arguably, soft to a point that it no longer provides sufficient precision for the bad guys to even bother with it, which will be a nice thing.



And our last little bit of news for the week, as you noted on the Sunday show, Leo, it was a slow news week because the hackers in our case took Thanksgiving off.  But there was one more piece of news that I thought was interesting, which is new problems with Windows DLLs, or I guess I should say "continuing problems with Windows DLLs."  As we know, some ideas in computer science are fundamentally fraught with problems.  An example would be the oh-so-convenient, though horrifically insecure, practice of allocating temporary communications buffers on a stack that is shared with code execution history and pointers.  What could possibly go wrong?



Another historically bad idea has been Windows dynamically linked libraries.  We know that Windows designers had their hearts in the right place.  Back in the 1980s when Windows was attempting to run in a system that had a 10MB hard drive, actually I guess the early Windows ran off of floppies, and 512 kbytes of RAM, there was not a single byte to waste.  So there was tremendous pressure to share any common functional code among the system's components and applications.



The idea of the DLL was clever in that not only could there only need to be one DLL stored on the hard drive; but, even more importantly, only one copy would ever be physically loaded into physical memory.  When another program was run and wished to use some of the functions from one of the shareable DLLs, the image of an already loaded instance of that DLL would be mapped into the virtual memory space of the requesting app by the Windows loader so that even apps that needed the same things would not be causing a second copy to occupy RAM.  This allowed much more to be accomplished with a small disk and a small RAM footprint.



But this technology did not age well.  Over time, there was a divergence of DLL versions and capabilities.  DLLs could use other DLLs.  And that created a confusing network of inter-DLL dependencies.  Such a mess arose that Microsoft was finally forced to step in and attempt to fix this problem.



Quoting from the Wikipedia entry on what's known as "side-by-side assembly," Wikipedia says:  "Side-by-side assembly (SxS, or WinSxS on Microsoft Windows) technology is a standard for executable files in Windows 98 Second Edition" - all the way back then - "Windows 2000, and later versions of Windows that attempts to alleviate problems, collectively known as 'DLL Hell,' that arise from the use of dynamic link libraries in Microsoft Windows.  Such problems," Wikipedia writes, "include version conflicts, missing DLLs, duplicate DLLs, and incorrect or missing registration.  In side-by-side, Windows stores multiple versions of a DLL in the WinSxS subdirectory of the Windows directory, and loads them on demand.  This reduces dependency problems for applications that include a side-by-side manifest."



They said:  "Microsoft Visual C++ 2005 and 2008 employ side-by-side with all C runtime libraries.  However, runtime libraries in Visual C++ 2010 no longer use this technology; instead, they include the version number of a DLL in its filename, which means that different versions of one DLL will technically be completely different DLLs now."  I mean, this has just been a disaster.



So as I said, Microsoft designers were doing what they needed to at the time.  And if anything, with the unfair benefit of 2020 hindsight, Microsoft might now be criticized for not completely killing off DLLs a long time ago.  But one thing that Microsoft has always provided has been rigorous backward compatibility, pretty much at any cost.  And, boy, trying to corral and manage what has happened with DLLs is the price that they are now paying.



So this brings us to today, or rather yesterday, when researchers with SafeBreach Labs published a trio of security advisories which described DLL-related bugs occurring in Autodesk, Trend Micro, and Kaspersky software.  All of the problems were responsibly disclosed to their respective software publishers before their public disclosure, which was made yesterday.



And interestingly, all of these problems still relate to the original way that DLLs work.  We've talked about some security problems in the past where, for example, if an app references a DLL, the Windows loader looks around the system for the DLL.  Typically it looks first in the app's own directory, which thank goodness allows apps to bring a copy of the DLL with them.  One of the many problems which DLLs have had has been newer apps would overwrite the DLLs with the same name, but a different version number, and that DLL would act in some different fashion, which would cause - remember those days, Leo, where you would install some new software on Windows, and suddenly something that used to work before would break?



LEO:  Oh, yeah.



STEVE:  It was - oh, my god.



LEO:  The worst offender was the C++ libraries.  And if you look in your Windows install even today, you probably have multiple C++ DLLs from a different variety of versions.  And it can really screw with you.  It is DLL Hell.



STEVE:  Yeah.  It's been a complete collapse of that model.



LEO:  The idea is great.  These are dynamic linked libraries.  The idea is the application doesn't have to have everything in the application.  It can link to libraries.  But somehow it just got out of control.



STEVE:  Just, yeah.  Even Windows 98 Second Edition was already  beginning to do some sequestering of DLLs because it was like, oh, ugh.  Now we have lots of memory.  We've got lots of drive space.  We just wish everyone would bring their own code with them and not rely on anybody else because, oh, my god.



LEO:  Or Microsoft should standardize on a single C++ runtime and deprecate all the rest.  And if you try to install an older one, I don't know, seems like it's Microsoft's fault somewhat, but maybe not.  Maybe not.



STEVE:  Yeah, no, I agree.  It was a good idea.  But really, you know, Apple has a reputation for killing things.



LEO:  Yeah.  This doesn't happen on Apples, yeah.



STEVE:  Right.  But the flipside is Apple does get criticism for killing off things.  Like they said, okay, we're no longer going to support such-and-such.



LEO:  Also Apple, if you look at it, the way you package apps on Apple, that monolithic icon hides a huge number of stuff, including all the libraries.  They keep it all in the library, in the icon.  And so they're very large apps.  I just downloaded an app that was half a gigabyte.



STEVE:  Ooh.



LEO:  For something that didn't do anything.  Because honestly, otherwise you'd have 18 copies of Electron in all different system folders.  Instead, you have 18 copies of Electron, but they're all within the package icon.  So maybe that's - I don't know if it's better, but it at least eliminates DLL Hell.



STEVE:  Well, so one of the things that Windows did, because the concept was an app didn't have to have its own DLLs, it would be able to use them if they were in the system.  So that allowed the Windows loader to go looking around for them.  And that's been a cause of problems in the past.  Turns out, believe it or not, even now that hasn't gone away.  So in the case of Trend Micro Maximum Security versions below v16.0.1221, there's a vulnerability that just got fixed.  The researchers found that the service known as "coreServiceShell.exe," it loads a DLL, paCoreProductAdaptor.dll.  However, in the case of a missing DLL, the lack of safe DLL loading, and the lack of DLL signature validation, hackers are able to exploit this as a security hole to cause their own unsigned malicious replacement DLLs to be loaded by the Trend Micro maximum security service every time that service runs.



And the service runs with the NT AUTHORITY\SYSTEM permission, which is the highest set of permissions in the system.  Basically it's like kernel root-level permission.  So it turns out that it is extremely easy to evade all the other provisions that Trend Micro put into place in order to prevent their system from being hacked because they weren't checking the signature of the DLL that they were loading.  Windows would just go find a DLL of the proper name.  And all the bad guys have to do is arrange to get that DLL in the Windows search path upstream of where Trend Micro put the real one, and that would cause a persistent, highly privileged load of a malicious DLL.  So as I said, this whole DLL thing, not only do things not work, but it has been a serious security problem moving forward.



The second vulnerability in Kaspersky Secure Connection is a VPN client, part of Kaspersky's Internet Security solutions, which is used - the VPN is used to create a secure connection back to the vendor's servers for transferring updates and information.  Like Trend Micro, their VPN versions below 4.0 does not check the signatures of the DLL it loads.  And it also runs that DLL with maximum system permissions.  And because it's a DLL, it's possible for the bad guys to drop a malicious one elsewhere in the system, in the Windows DLL search path, which Kaspersky will cause to have loaded and executed preferentially to their own, thanks to Windows.



And Autodesk, not very much different.  In this case, all of these things are loading with the NT AUTHORITY\SYSTEM permission, and Autodesk was found not to be checking the signature of its DLLs.  The researchers wrote:  "After an attacker gains access to a computer, they might have privileges limiting them to access only certain files and data.  But the Autodesk service provides them with the ability to operate as NT AUTHORITY\SYSTEM, which is the most powerful user in Windows, so they can then access almost every file and process which belongs to the user on the computer."



So what we've seen is that the DLL design pattern and methodology is so deeply ingrained into Windows that there really is nothing Microsoft can do about it today.  The fact that that Windows side-by-side mess doesn't directly address DLL security, it only is there so that modern Windows has any chance of running at all these days.  I was initially a bit surprised to learn that processes running with maximum permission were allowed to have unsigned DLLs loaded into their process space.  But upon reflection, changing that at this point would also break too many longstanding assumptions.  So I suppose the best we can do is to keep patching and be glad that there are folks like SafeBreach Labs, these guys, who take it upon themselves to find and responsibly disclose those problems when they are found.



And I wanted to take a moment just to sort of touch base on the Joy of Sync solutions that I found.  It occurred to me because I stumbled upon some of the notes that I had made, and I realized that in all the time that I have been using it, I have never once had a problem.  It has never once failed me.  That is, it is essentially the reason I went looking for something else when I was having problems with Dropbox's sync.  So I don't know what the magic potion is, the magic programming dust that Sync.com has, but they have it.



And so I just, you know, as we know, it's often the case that we'll start to use something, and then something will happen, and we'll go, oh, well, and stop using it and fail to let everybody know that our recommendation changed.  So I just wanted to remind people about it.  They make 5GB free for anyone who is interested.  I have a link in the show notes which will give you an extra gig free if you want to give it a try.



So you start off with 6GB free.  It's just grc.sc/sync.  And if you use that link, it expands to a link which actually both of us, you and I, get a gig free.  I already have 3TB, so I'm fine.  But I figured, hey, difference between five and six gigs, that's more significant.  So anyway, I am every bit as happy and bullish with Sync.com as I was.  I don't know if - I think I did the Joy of Sync podcast before I left for my tour.



LEO:  Yeah, you did, yeah.



STEVE:  Because I think I told you, Leo, that I had forgotten to  put the presentation on my laptop.  And it was like, oh, crap.  But then I realized, I mean, we were literally on the tarmac, and I said to Lorrie, I said, "You know, I think I forgot to put the presentation on the laptop."  And she looked at me with alarm.  And I said, "But it's okay because I do all of my work now underneath my Sync folder."  And so I knew that I would be able to get it wherever I was.



So anyway, grc.sc, that's for shortcut, grc.sc/sync.  And again, my recommendation stands.  I think these guys have nailed it.  And I'm also, for other applications where I want to do my own peer-to-peer, Syncthing.  I have been using it also extensively for other purposes where it doesn't make sense or there's no need to sync to the cloud.  And Syncthing has never let me down.  And I have it running on my Drobos.  It's keeping my two Drobos in sync, as well.  So anyway, I just wanted to touch base.



LEO:  You mentioned that Sync.com - I wanted to use Sync.com because it's end-to-end encrypted, which is awesome.



STEVE:  Yes. 



LEO:  But they don't...



STEVE:  Yes, in fact...



LEO:  Go ahead.



STEVE:  Go ahead.



LEO:  Well, they don't have a Linux client.



STEVE:  Yes, yes.  That is the one problem.  They don't have Linux.  They understood after we talked about it on the podcast, and a lot of our listeners said, hey, thanks, we'd love to use it, but without Linux - and so anyway, they did receive that message.



LEO:  I hope they make one.  I, because of that, ended up using  pCloud, which is not end-to-end encrypted, but they have an end-to-end encrypted folder.  So you can say this folder I want always end-to-end encrypted.  Which for my purposes was actually sufficient.  I'm not too worried about it.  They also had, and I like this, a lifetime price.  I think it was like 200 bucks, and that's it, for 2TB.  So I just bought the 2TB for life.  And I just use it as - in some ways I'm wondering if people really need NASes much anymore.  As storage gets cheaper in the cloud...



STEVE:  I agree.



LEO:  For me it ends up being much more convenient than my Synology NAS.  I just keep everything - I still run the Synology NAS so I have a local backup.



STEVE:  I agree.  Yup.



LEO:  But, you know, I think it's a pretty nice solution.



STEVE:  Well, and remember that at least the Sync.com also has full versioning.  You can go back to any prior...



LEO:  Right, as does pCloud, yeah.



STEVE:  Okay, yeah, go back to any...



LEO:  But Dropbox does that now, too.



STEVE:  Yeah.



LEO:  Now, I love that because, if you screw up, you can go back in time.  I'm actually pretty happy with pCloud.  And they do have a Linux client.  And that's, for me, half of my computing's on Linux, so I kind of have to have that.



STEVE:  So I found the reference.  



LEO:  StrandHogg.



STEVE:  Yes.  Promon, who we'll be talking about, calls this vulnerability StrandHogg, which is Old Norse, referring to the Viking tactic of raiding coastal areas to plunder and hold people for ransom.



LEO:  Oh, that's the Strand, probably.



STEVE:  So I guess they weren't - I guess those Vikings weren't all just glamorous and...



LEO:  Oh, it wasn't all mead and beer in Viking horns.  No, no.



STEVE:  So these guys, Promon security researchers, have found proof of a dangerous Android vulnerability which, as we know, they call StrandHogg, that allows real-life malware to pose as legitimate apps, with users unaware they're being targeted.  They are a partner with the security firm Lookout, whom we've talked about several times.  Lookout confirmed that they've identified 36 malicious apps exploiting the vulnerability.  And among them were variants of the BankBot banking trojan, which was first seen in 2017.  During testing, Promon researchers found that all of the 500 most popular Android apps, as ranked by the app intelligence company 42 Matters, are vulnerable to StrandHogg.  All versions of Android are affected, including Android 10.



LEO:  Is this their logo, or did you make this up?



STEVE:  Yes, yes.  No, that's their logo.  In fact, there's one on their page that has that mask on top of one of the Android bots.



LEO:  Oh, my.



STEVE:  There's two Android bots on either side, and then that one in the middle is the evil Android bot.  Anyway, BankBot - that's the malware that is being used - is one of the most widespread banking trojans around, with a ton of variants, and new ones appearing continually.  BankBot attacks have been detected around the world, in the U.S., Latin America, Europe, and the Asia Pacific region.  StrandHogg was first detected when banks reported that their customers were reporting missing funds from their accounts.



So in their little top-of-the-page summary, they summarize what's the impact of their finding.  All versions of Android are affected, including Android 10.  All top 500 most popular Android apps can be abused.  Real-life malware is exploiting the vulnerability.  That is, this is in the wild, doing this now to people.  Thirty-six malicious apps exploiting the vulnerability have been identified.  Oh, wow.



LEO:  Bless you.



STEVE:  Excuse me.  Yeah, I don't think I've ever sneezed before.  I've come close a couple times, but there it is.  The vulnerability can be exploited without root access.  So all it takes to do this is to get a piece of malicious software into an Android device.  And unfortunately, Google is always removing malicious code from the Google Play Store.  When this is exploited by hackers, they can listen to the user through the microphone.  They can take photos through the camera.  They can send and receive SMS messages, make and/or record phone conversations, phish login credentials, get access to all private photos and files on the device, get location and GPS information, get access to the contacts list, access the user's phone logs, I mean, it's like they were somehow able to root your device, but it turns out that's not necessary.



So what's going on here?  How does this happen?  It turns out it's the harvesting of dangerous permissions.  And I have a graphic from their report in the show notes.  They show the user clicking an app icon of a legitimate app in order to launch it.  But instead of seeing the legitimate app, the malware is displayed, impersonating the app, which then asks for any permission while pretending to be the legitimate app, like allow to send and view SMS messages.  Which, depending upon the app, might be something, a permission you would expect to give the app.



But unfortunately, as a consequence of leveraging this - and here's the key - designed-in aspect of Android, you're actually giving the permission to the malware, not to the legitimate app.  Which is then allowed to run and take advantage of the victim.  So the vulnerability makes it possible for a malicious app to ask for permissions while pretending to be the legitimate app, and an attacker can ask for any permissions they want:  SMS, photos, microphone, GPS, text messages, view photos, eavesdrop, track the victim's movements, whatever.  The attack can be designed to request permissions which would be natural for different targeted apps to request to reduce the suspicion on the part of the victims; right?  So that would be natural in a little bit of a social engineering hack.  Users are unaware that they are giving permission to the hacker and not the authentic app they believe they're using.



But wait, there's more.  Powerful credential stealing apps are also enabled by the presentation of faked login prompts.  You click on an app where you are expecting to need to log in.  A malicious login page is displayed on the victim's screen instead of the legitimate app.  Then those credentials are supplied to the actual app in order to log in.  Basically, it's a man-in-the-middle attack on your device.



LEO:  Is this the screen overwrite permissions that we've talked about before?



STEVE:  Yes.  It is an extension of those longstanding screen overwrite permissions.



LEO:  It's always been a problem.  But Google seems loathe to take them out, I think because apps like Facebook use it for their Messenger pop-up bubbles and things like that.



STEVE:  That's exactly right.  So what's going on?  What makes StrandHogg unique is that it enables these sophisticated attacks without the need for a device to be rooted.  It leverages what are arguably weaknesses inherent in Android's multitasking system to enable powerful attacks that allow malicious apps to masquerade as any other app on the device.  The exploit uses an Android feature called Task Affinity, which allows any app, including malicious ones, to freely assume any identity within the multitasking system they desire.  And as I mentioned at the top, Promon's research found that all of the top 500 most popular Android apps are vulnerable to this attack across every version of Android.



So what's interesting here is that their research significantly expands upon research which was carried out by Penn State University and the guys at FireEye in 2015.  Back then, the researchers theoretically described certain aspects of the vulnerability.  At the time, Google dismissed the vulnerability's severity.  But Promon now has clear evidence that hackers are exploiting StrandHogg to gain access to devices and apps.  The specific malware sample that Promon analyzed did not reside on Google Play, but it was installed through several dropper apps and hostile downloaders distributed on Google Play.



These apps have since been removed.  But in spite of Google Play's Protect Security Suite, dropper apps continue to be published and frequently slip under the radar, with some being downloaded millions of times before being spotted and deleted.  And just due to the nature of the way Google Play works, Google is necessarily reactive.  Of course they will remove anything that they become aware of.  But this means that some people are downloading and being hurt by malicious apps in the interim.  For example, there was one CamScanner which was a PDF creator that contained a malicious module.  It had been downloaded 100 million times before Google realized that it was malicious and pulled it.



So Promon's CTO, their Chief Technology Officer, Tom Lysemose Hansen, he commented:  "We have tangible proof that attackers are exploiting StrandHogg in order to steal confidential information.  The potential impact of this could be unprecedented in terms of scale and the amount of damage caused because most apps are vulnerable by default and all Android versions are affected."



So I wanted to dig a little bit deeper into this.  I was curious about what are the details.  So I went back, and I found the original security research which was presented at the 2015 USENIX Security Symposium back in August of 2015.  The researchers titled their paper "Towards Discovering and Understanding Task Hijacking in Android."



And in their abstract they wrote:  "Android multitasking provides rich features to enhance user experience and offers great flexibility for app developers to promote app personalization.  However, the security implication of Android multitasking remains under-investigated.  With a systemic study of the complex tasks dynamics, we find design flaws in Android multitasking which make all recent versions of Android vulnerable to task hijacking attacks.  We demonstrate proof-of-concept examples utilizing the task hijacking attack surface to implement UI spoofing, denial of service, and user monitoring attacks.  Attackers may steal login credentials, implement ransomware. and spy on users' activities.



"We've collected and analyzed over 6.8 million apps from various Android markets.  Our analysis shows that the task hijacking risk is prevalent.  Since many apps depend on current multitasking design, defeating task hijacking is not easy.  We've notified the Android team about these issues, and we discuss possible mitigation techniques in the paper."



So this is not news to anybody at Google working on Android.  And then I dug a little deeper.  I was worrying about this "allowTaskReparenting" attribute.  So it's in the Android 1.6 SDK.  And it's called allowTaskReparenting.  And as an example, they said:  "If an activity" - and this is from AndroidCookbook.info.  "If an activity has its allowTaskReparenting attribute set to 'true,' it can move from the task it starts in to the task it has an affinity for when that task comes to the fore."  And as you can hear, that sounds like exactly what this malware has been designed to do.  It creates an affinity bound to some task it wants to impersonate, turns on allowTaskReparenting, and then automatically gets switched into that task's process.



They said:  "For example" - and this was the example they give - "suppose that an activity that reports weather conditions in selected cities is defined as part of a travel application.  It has the same affinity as other activities in the same application, the default affinity, and it allows reparenting.  One of your activities starts the weather reporter, so it initially belongs to the same task as your activity.  However, when the travel application next comes forward, the weather reporter will be reassigned to and displayed with that task."



So in other words, this has been designed to do this.  There is something designed into Google which, if malware is allowed to be installed in someone's device without itself needing any permissions, without itself needing any deep access, is able to look around the phone, see what's there, create these affinities, and then impersonate by doing a UI layer, exactly as you said, Leo, a UI layer over the real app in order to obtain  the permissions for itself.  And that is now being actively exploited in the Google Play Store for any new malware which is installed and does this, until it is identified as doing so and then removed.  And then of course another instance of it is installed under some other name.



So I don't know what Google's going to do about this.  They know about it.  Maybe the fact that this report is now public as of yesterday, and this is now getting some real scrutiny, maybe they have some means in the works to fix it, but haven't implemented it yet.  But this if nothing else will turn up the heat on them, and that's just all to the good.



LEO:  We've seen this before, and they didn't face it last time, as you mentioned, four years ago.  I think it's because there are major apps like Facebook Messenger, which is you don't use Android, but there's a chat bubble feature which I really like.  If you get a Messenger chat, a little head will show up in a bubble on top of the screen that you can move around, you can tap, you can interact with.  So it is part of its multitasking capability.  And I suspect they don't want to turn it off because they don't want to get Facebook angry.



STEVE:  Yeah.



LEO:  I notice that's not available on Apple.  You can't overwrite the screen.



STEVE:  Nope.



LEO:  Nope.  There's the peril involved, though.  I'm always, you know, I love Android, and I'm very happy with my orange Pixel 4.  I just don't download apps at random; you know?  I think that's the best thing to do.



STEVE:  Correct.  That is all the user can - yes.  That is our takeaway advice, not for the first time on this podcast, is just, if you can, stick with well-known mainstream - and don't just download everything that's available because it's free.



LEO:  Right.  You might be tempted by that PDF scanning app.  But get the one from Google.  Don't get the one from [crosstalk] or whatever it is.



STEVE:  Right.  And the fact that 100 million people have downloaded it doesn't mean that it's safe.



LEO:  That's amazing.  Wow, that's amazing; isn't it?



STEVE:  Yeah.



LEO:  Terrifying.  Now, you'd have to be targeted in this attack.  Or no?



STEVE:  Well, no.  No, no, no.  And if this thing doesn't know who you are, it will find out who you are.



LEO:  And then contact the servers and say, hey, I have access to Leo's phone.  Would you like to see what he's doing right now?



STEVE:  Yep, exactly.



LEO:  Okay.  Geez, Louise.  Steve Gibson does it again.  And all I can say is, if you don't listen to this show, your hard drive could die.  That's all.  I'm just saying.  You've got to listen every week to Security Now!.  You don't have to listen live, although we do do it live every Tuesday, 1:30 Pacific, 4:30 Eastern, 21:30 UTC.  Figure out what your time is.  And go to TWiT.tv/live.  There's audio and video so you can listen while you work.  Boss will never know.  It's good, actually the boss should encourage it.  It's a good way to keep up on all the perils, risks, and inside and outside baseball stuff that has to do with technology.



You can also, if you are listening live, chat with us at irc.twit.tv, always a very active chatroom during the show.  You can get on demand versions of the show.  Steve's got kind of two unique versions.  He has a 64Kb audio file, but he also has a 16Kb audio file.  So if you're on a bandwidth-limited connection, that's a great choice.  It's a little scratchier, but at least you get it.  He's also got the smallest version of all, which is the fantastic transcripts that Elaine Farris does for him.  So you can download and read along as you listen.  Those are all available at GRC.com.



While you're there, Steve didn't mention it this week, but pick up a copy of SpinRite.  That's his bread-and-butter and the world's finest hard drive maintenance and recovery utility.  SQRL is official.  You can find out more about it, learn about the API, how you would write a SQRL - I want somebody to work on a SQRL authentication plugin for our TWiT Forums at Twit.community.  It's a Discourse server.  And I think that Discourse is widely used.  You already have it for XenForo, and people that use your forums love it.



STEVE:  And also for WordPress.  There is a WordPress plugin.



LEO:  Yeah, yeah.



STEVE:  So we need Disqus.



LEO:  Yeah.  So Disqus would be great.  I use Disqus.  I would use it on my WordPress blog.  But I use Disqus as the commenting engine on it.



STEVE:  Right, right.



LEO:  And maybe I should just go to native WordPress commenting, and then I could do it.  I just don't want to lose all the existing comments.  I'll look tonight and see if I can export the Disqus stuff into WordPress because I don't want to use Disqus anyway.  It's another person watching what you're doing and all of that.



STEVE:  Yeah.



LEO:  Yeah.  Besides SQRL and SpinRite, there's all sorts of other stuff like ShieldsUP!, lots of free software.  It's a rathole that you'll spend many happy hours perusing:  GRC.com.  He's on Twitter at @SGgrc.  And that would be a good place if you have a question or a comment or want to leave a message for Steve.  If you've got some inside dope, you could direct message him at @SGgrc on Twitter.  He does not yet have his own Minecraft server.  That's just a matter of time, though.  No.  It's never going to happen.  We have on-demand audio and video of every show, as well.  Video, yeah.  So you can see us as well as listen.  And that's available at TWiT.tv/sn.  TWiT.tv/sn.



Best thing to do, though, honestly, subscribe.  Then you don't even have to think about it.  You'll just get it automatically, audio or video, the minute it's done.  And if you are subscribing, it helps us because that way we kind of rise through the ranks in the various podcast applications.  We're more likely to show up in the discovery tab.  So whichever podcast application you use, please do subscribe to Security Now!.



Steve, you have said this off the air, but I'm going to say it on the air.  In a couple of weeks, the Christmas Eve show.



STEVE:  Ah, yes.



LEO:  I can't wait.  This is going to be a special episode.  You want to tell us about it?



STEVE:  Well, I'm noticing that we're starting into a new decade with 2020.  So I thought it would be fun to do - I know.



LEO:  Hard to believe.



STEVE:  I thought it would be fun to do a decade of hacks, looking back over the last 10 years of hacks and attacks, and just sort of do a little retrospective of everything that we've covered.  So we'll deal quickly with any news of the week, and then spend most of our time looking back over the past 10 years.



LEO:  What a decade it has been for security.  That'll be the December 24th edition.  We are recording it the day before.  We didn't want do it on Christmas Eve, so we're recording that at our usual time, but on Monday the 23rd, just in case you watch live.  And then the following week the New Year's Eve Security Now! will be a best-of episode from all the episodes all year long.  And we love those best-of episodes.  They're always a lot of fun.  So Steve, we'll see you right back here next Tuesday.



STEVE:  Okay, my friend.  Till then, thanks.  Bye.



Copyright (c) 2019 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#744

DATE:		December 10, 2019

TITLE:		VPN-geddon Denied

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-744.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we look at Microsoft's force-feeding of Windows 10 feature updates, the creation of a tool to keep Win7 and 8 updates freely flowing for free, the continuing evolution of a new highly secure programming language, an update to Microsoft's RDP client for iOS, Avast and AVG in the doghouse, some VERY severe authentication bypasses in OpenBSD, and a note about the WireGuard VPN.  Then we take a look at the report which every security website breathlessly covered - and got wrong.



SHOW TEASE:  It's time for Security Now!.  Steve is here to talk about forced Microsoft Windows updates.  We're going to talk about Microsoft turning to Rust for systems programming, a brand new VPN Steve likes an awful lot, and why you might have read some headlines about VPN-geddon you don't have to worry about.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 744, recorded Tuesday, December 10th, 2019:  VPN-geddon Denied.



It's time for Security Now!, the show where we talk about your security and privacy online with our good friend, Steve Gibson.  Hello, Steverino.



STEVE GIBSON:  Yo, Leo.  Great to be with you again.



LEO:  Good to see you.



STEVE:  There was a piece of news that not a single tech press publication failed to cover.



LEO:  Incorrectly, no doubt, yeah.



STEVE:  Incorrectly.



LEO:  Uh-oh.  Okay.



STEVE:  Involving what was claimed to be a horrible, critical, end-of-the-world, VPN-agnostic problem in Linux and all of the *Nixes and the *BSDs and iOS and Android, I mean, it just covered the territory.  So I titled our episode "VPN-geddon Denied" because this is not VPN-geddon.  But we'll have fun talking about it.  It's interesting from a technology standpoint.  I don't mean to get on the case of the tech pubs.  They're not developers.  But it's such an interesting thing that happened that I think it would make a great topic for today.



But we're going to take a look at Microsoft's beginning to force-feed Windows 10 feature updates onto people who, well, after more than year presumably don't want them, but they're going to get them anyway.  We have the creation, and you could have seen this coming, of a new tool to keep Windows 7 and 8 updates freely flowing for free, after Microsoft attempts to cut them off after January.



We have the continuing evolution of a new, highly secure programming language.  An update to Microsoft's RDP client for iOS, of all things.  Avast and AVG are in the doghouse.  We have, now, in this case, some actually truly severe authentication bypasses in OpenBSD such that anybody listening to this, if you have OpenBSD exposed to the Internet, you need to make sure you've updated it recently.  We also have a note about a VPN known as WireGuard that I ran across earlier this summer.  And it's been on my radar.  I'm planning to do a deep dive for our listeners into it.  But it popped up actually on my radar for today.  So we'll talk about that.  And there was sort of an interesting coincidence that involves it when I was in Sweden that I'll briefly talk about.



And then we're going to take a look at this report of, well, every security website breathlessly covered and got wrong.  And speaking of insecure programming languages, we're talking about one that is a language written for security, with security foremost, is evolving.  But our Picture of the Week is such a perfect example of just how anti-secure a language, in this case PHP, can be.  So I think another great podcast for our listeners.



LEO:  Sounds very exciting.  Very exciting.  Steve?



STEVE:  So Leo, our Picture of the Week is just such a perfect example of how programmers can so easily get themselves in trouble and get tripped up when a language they're using really wasn't designed for the application it's being used for.  In this instance, for those who don't have video, this is an example of an equation testing for the equality of two hashes.



And so there's a string, 240610708, which is hashed with MD5.  And the fact that it's MD5, that's not significant here.  We know that that's been deprecated, but that's not the point.  And then there's an equality sign joining it to another hash of a completely different string, QNKCDZO.  And we know that these two different strings should, with an extremely high probability, hash to different results.  So they should not be seen as equal.



But automatic languages, languages which jump in and do things on behalf of the programmer to make the programmer's job easier, often do something known as "type mangling," where it will see that you're trying to compare two things, and it'll go, oh, okay.  Well, MD5 produces a string of its output, so we're comparing two strings.  Then, unfortunately, PHP looks at the strings, and it sees something that it thinks is a number.  The kicker here is that both of these hashes begin with the characters 0e.  And that looks like, to PHP, it looks like scientific notation, that is, 0e followed by an exponent.



Well, anything, zero raised to any power, any exponent of zero is going to be zero.  So this equality sees that you're comparing zero versus zero, even though the actual hashes and the strings are completely different.  They just happen to collide with the first two characters, which as a consequence of PHP's overly  helpful type conversion, or mangling, says, oh, look, I don't even have to do the math here.  "0e," that means anything that follows it is going to not matter.



LEO:  I would blame the programmer, not the language in this case because you just didn't use the right type for these hashes, that's all.



STEVE:  Correct.



LEO:  It's just a string converted to a number.  You don't have to - and that's the programmer's fault, honestly.  Admittedly, PHP's defaults are probably not security forward.  But still.



STEVE:  Well, and that's the point is the defaults are to be helpful and to say, oh, look how easy this is to use.  You can learn this in an afternoon.  And, yeah, but don't write...



LEO:  Yeah.  You might be tempted to say, oh, just going to cast a string to a number, and it'll be fine.  But any, I mean, I think a decent programmer is going to know better, I would hope.



STEVE:  Well, and the problem is that these languages allow people who don't really have the chops to write code that they should not be writing.



LEO:  Yes.  Like security code.



STEVE:  Like, oh, I could write some.  Yeah, I'll check the hashes to make sure they're the same.  I mean, you can see this.  You can imagine this happening on some web page that someone wrote in PHP.  And it's like, oh, look.  Looks great to me.  It's like, yeah.  Except here's a little glitch.  Anyway, this is sort of subtle, but I thought it was a perfect example of a way that a language that is trying to help you can actually hurt you.  And the fact is we see these sorts of things all the time.  You really need to understand what your language is doing for you in order to write code that always behaves the way you expect it to.



LEO:  Yes.  And I think this is a case where a better designed language would make it harder for you to make that mistake.



STEVE:  Yes.



LEO:  That's why we like statically typed languages, for one thing; right?



STEVE:  Yes.  Or strongly typed.  I'm a real fan...



LEO:  Strongly typed, yes.



STEVE:  ...of strongly typed language because there, if you're declaring what everything is, then the compiler can help you to find your own mistakes.  You're trying to assign a string to an integer, and it's like, whoa.  Whereas PHP says, yeah.  We know how.



LEO:  Yeah.  I'll be glad to do that.



STEVE:  I'll convert that for you.  And it's like, whoa.  So anyway, I just...



LEO:  Another argument for not writing your own security code, but using libraries that are well designed, too; right?  I mean, a good library's not likely to do that.



STEVE:  Yes, exactly.  Good, well-designed libraries written by people who have an expertise in that thing can then produce for you an API which encapsulates that security knowledge and language-specific knowledge so that that work is being done for you.  And so, for example, if there was an API that was to compare two hash buffers to make sure that they're the same, it would have done the right thing if it were written by somebody who should write such a library.



LEO:  Right.



STEVE:  So Microsoft has started forcing feature updates on people who apparently don't want them.  Windows 10 version 1809 - not to be confused with 1909.  1809, as we know was first, well, ostensibly made available more than a year ago on November 13th of 2018.  That was the one that just kept having so many problems that they ended up not releasing it until several months into this year, 2019.  But that release for Windows Home, Pro, Pro Education, and Pro for Workstations, its end-of-service date is next May 12th of 2020.  So it's still a ways away.



But after May 12th of 2020, any systems still running what was the October 2018 update will no longer receive any future quality and security updates.  Therefore, in a deliberately planned preemptive move, since after all it has been more than a year since its original intended release, which was November 13th of 2018, Microsoft last Thursday announced that they were going to start pushing people forward.



Last Thursday they said:  "Beginning today, we will slowly start the phased process of automatically initiating a feature update for devices running the October 2018 Update Home and Pro editions, keeping those devices supported and receiving the monthly updates that are critical to device security and ecosystem health."  They said:  "We're starting this rollout process several months in advance [five or six] of the end-of-service date to provide adequate time for a smooth update process."



And you know, you and I, Leo, we kind of go around about this.  We're in agreement.  I mean, we understand Microsoft's need to drag their legacy stuff kicking and screaming forward, and it makes sense.  They're wanting to have pretty much a single build that they are then continually rolling forward.  And we talked about it at the time, when they would allow an individual under the advanced update settings of the versions of Windows that allow that to go in and forestall a feature update, not the security update, but the feature update by up to 12 months.  And so basically people have been able to push this off for that length of time.



LEO:  But time's up, Charlie.



STEVE:  Yeah.  And given all the trouble that that particular one had, anybody would be understood, especially if they don't want any feature updates, if they're like, yeah, I'm happy with what I have.  Leave me alone.  Well, you've got a year.  Now, as you said, time's up.  So Windows will no longer accept the "Thanks anyway, but not yet" response to whether they want to receive a feature update.  And again, basically it looks like six months ahead Microsoft is saying, okay, you've had more than a year.  We're not really lowering the boom until May of 2020, but we're just going to start moving things forward.



So anyway, if anybody sort of has a machine that's been stable and settled down, and then suddenly they get one of those, hold on a second, we're improving your Windows experience or updating, and you get the little rollercoaster balls for, like, way longer than usual, it's probably because you fell into this, okay, no more waiting.



LEO:  Yeah.  And I think if you're an enterprise license person, you have much more flexibility on that.  It's just the regular users that have to do the upgrade.



STEVE:  Well, my favorite is the Long Term Servicing Channel.  You're completely released from all of this if you're on the LTSC bandwagon.  If you're an enterprise user, and you're like, no, we really do want to stay with what we have, then Microsoft says, yeah, okay, fine.



And in a bit of "we should definitely have seen this coming" news, a group of Windows enthusiasts who hang out over in the My Digital Life forum have come up with something they call "BypassESU" utility, where ESU stands for Extended Security Updates.  I have the link, if anyone is curious, in the show notes.  So two months before it'll even be needed, we already have a hack to trick Windows 7 and 8 into continuing to receive another three years of security updates after the otherwise final January 14th Patch Tuesday, next month.



So the story goes, last month Microsoft released a test Windows 7 ESU update that was KB4528069.  It allowed IT administrators to start the verification process that their systems would be compatible with the upcoming ESU, the Extended Security Updates process.  And I have also a link in the show notes to that update.



And so there are a couple interesting points in here.  In their summary describing the update, they said:  "This optional non-security update will help you verify that your eligible Windows 7 SP1 and Server 2008 R2 SP1 devices can continue to get Extended Security Updates after the end-of-support date of January 14th, 2020."  So let's see.  They said no security content.  "This update is a test package we recommend that you deploy in your test environment.  Install this update on your on-premise devices that are eligible for Extended Security Updates."



This update can be installed on x64-based architecture for Windows Server 2008 R2 and also either x86 and x64-based for Windows 7 SP1.  They said:  "This update is not applicable for Windows 7 Virtual Desktop (WVD) and Windows 7 Embedded.  Installing this update has no impact on getting security updates between now and January 14."  So it's only for continuation.  Under installation instructions, install the update from one of the release channels such as Microsoft Update Catalog, meaning that anybody can do that.



And then they said for prerequisites:  "You must have the following installed on your on-premise device before you apply this update."  They said:  "Install the following SHA-2 code-signing support and servicing stack update."  And then there's a link to that.  Of course we've talked about that.  That's where some months ago Microsoft stopped dual signing the updates, I think it was this summer, so that the newer updates are only being signed with SHA-2, not also SHA-1.  So obviously you would need that in order to, I mean, and everybody probably already has that.  And that creates the SHA-2 code-signing support.  You're supposed to have the servicing stack update, and there's another knowledge base article from March 12th, and be up to date with the monthly rollup.  And then the last one is install and activate the ESU key.  For information about how to install and activate the ESU key, see "How to get Extended Security Updates for eligible Windows devices" blog on the Microsoft Tech Community website.



So what happened was some guys went in, and they figured out basically how to short-circuit this ESU key requirement.  So just to be clear, doing this violates licenses.  It's technically against the law.  So that hasn't kept people, for example, for decades of figuring out how to create hacked Windows OS installation key checks.  That's a hack that's been around forever.  And also this is the kind of thing that Microsoft will likely respond to.



So I wouldn't expect this particular bypass ESU tool crack, is what essentially it is, to work forever.  I expect that, I mean, and it'll be interesting to see with how much fervor Microsoft pushes back on this; whether they're going to say, wait a minute, we don't want hackers to be continuing to get Windows 7/Windows 8 updates.  Basically, if you're not an enterprise who's paying for them, you need to do that.  So anyway, we'll keep our eye on this.  Again, I just sort of chuckled when I saw, okay, well, yeah, it figures that some hobbyists would figure out how to do this.  And so we'll see if this is going to be a cat-and-mouse game.  Microsoft will patch it, somebody will come up with a way around it again, blah blah blah.



And I did note sort of the irony of the fact that Microsoft, if they were to defeat this ESU bypass, would have to do it with an update, which of course is what the ESU bypass allows you to receive.  So if you've got the ESU bypass installed, then you're able to get the update from Microsoft which probably breaks it and causes you to no longer be able to get any more updates.  So maybe the thing to do is just to say, okay, fine, I'll upgrade to Windows 10.  We talked about how that can still be done for free last week.



As I've been talking about recently, on recent podcasts, we're at the point now where - and Leo, I had so much fun listening to your Mac Pro coverage.



LEO:  Isn't that wild?



STEVE:  I mean, the size of the RAM.



LEO:  My god.



STEVE:  Was it $69,000?  Did I hear that right?



LEO:  I think we maxed it out, I think at $61,000.



STEVE:  For a single machine.



LEO:  That might have included the $6,000 monitor.  I don't remember.  It's over $50,000, yeah.



STEVE:  Wow.  Anyway, so talk about having...



LEO:  But honestly, I mean, going back in time, I remember buying a DEC PC for $10,000.



STEVE:  I think my first IBM XT was 10-plus.  I wanted both monitors.  I needed the 10MB hard drive.



LEO:  Exactly.



STEVE:  I think the hard drive was $5,000.



LEO:  Exactly.



STEVE:  It was 5K to not have to use floppies.



LEO:  So we're talking a lot of money, but we're also talking a lot of computer.  I mean, it's a supercomputer.  It really is.



STEVE:  Well, yes, it is.  Which sort of brings us to the point that I've been talking about recently is that we have, I mean, there's like a 64-core Threadripper is now the term that's being used?



LEO:  Yeah, that's next for AMD's Ryzen.



STEVE:  So the history of programming languages has been one of trying to ring as much performance as possible out of the system.



LEO:  Not anymore.



STEVE:  And there certainly are places where you're doing high-end 3D modeling or massive matrix work, or you're modeling atmospheric perturbations, I mean, that are like incredibly number-crunchy.  But many of us are sitting in front of a word processor which we would like to not have hackable if we make the mistake of opening a JPEG in the word processor.  That doesn't take a lot of processing power, to get spellcheck that works with reasonable performance, given the engine which is powering our machines these days.



So as I've been talking about, it is really making sense for us to sort of stand back and say, okay, the problem we have today is no longer that we're not able to type into our word processor at a speed which it can keep up with.  The problem we have today is that all of our crap keeps getting hacked.  And so let's fix that.  Let's fix these buffer overruns.  And, I mean, like, really, let's come up with a language which no longer allows those mistakes to be made, a language where you don't give the programmer - and this is where the programmers chafe a little bit - you don't give the programmer the ability to get themselves in trouble.  And that PHP example is a perfect instance of an anti-secure language.



So Microsofties are talking more and more about something called Project Verona.  It's the codename for their increasingly customized, tightened, and hardening language which is reminiscent of and has strongly apparently been influenced by the Rust programming language.  We've talked about Rust a little bit.  There was an interesting blog post last week by Adam Burch, actually it was a little older than that, but he's a software engineer in the Hyper-V team.  And he shares his experience.  The tags on the post are memory safety, Rust, safe systems programming languages, and secure development.



He wrote:  "This Saturday, 9th of November, there will be a keynote from Microsoft engineers Ryan Levick and Sebastian Fernandez at RustFest Barcelona.  They'll be talking about why Microsoft is exploring Rust adoption, some of the challenges we've faced in this process, and the future of Rust adoption at Microsoft.  If you want to talk with some of the people working on how Microsoft is evolving its code practices for better security, be sure to attend the keynote and talk to Ryan and Sebastian afterwards.



"This blog describes part of the story of Rust adoption at Microsoft.  Recently," he wrote, "I've been tasked with an experimental rewrite of a low-level system component of the Windows codebase," and he said, "(sorry, we can't say which one yet)."  He said:  "Instead of rewriting the code in C++, I was asked to use Rust, a memory-safe alternative.  Though the project is not yet finished, I can say that my experience with Rust has been generally positive.  It's a good choice for those looking to avoid common mistakes that often lead to security vulnerabilities in C++ code bases."



And he said, under the heading "Great Dev Experience," he said:  "For C++ developers used to writing complex systems, using Rust as a developer is a breath of fresh air.  The memory and data safety guarantees made by the compiler" - and this is the key, by the compiler, not the programmer - "give the developer much greater confidence that compiling code will be correct beyond memory safety vulnerabilities.  Less time is spent debugging trivial issues or frustrating race conditions.  The compiler warning and error messages are extremely well written, allowing novice Rust programmers to quickly identify and resolve issues in their code.  Visual Studio Code already has a helpful extension (RLS) which provides IntelliSense suggestions and syntax highlighting.  Additionally, the Cargo build tool offers very helpful features around testing, documentation generation, and auto formatting."



As for the learning curve, he writes:  "Thanks to a plethora of online documentation and very helpful compiler error messages, Rust has a pretty easy learning curve for someone like me who has used C++ for the majority of my career.  There are tutorials aimed specifically at C and C++ systems engineers.  In his talk at RustConf 2019, Jeremy Fitzhardinge at Facebook noted that he saw experienced C and C++ devs become comfortable with Rust in around four weeks and pretty fluent in eight.  This aligns with my own experience.



"I participated in the annual Microsoft internal One Week Hackathon with one experienced Rust developer and one complete novice.  Within three days, the novice Rust developer had written more than 1,000 lines of idiomatic Rust code.  In addition to the great documentation, there are helpful tools like Clippy" - I don't want to know what that is - "which allow experienced C++ developers to jump right into coding Rust without" - oh, is it going to jump out and ask, ooh, looks like you're trying to sort a buffer?  Anyway, "...jump right into coding Rust without much direct assistance from those experienced with Rust."



He said:  "As we expand the use of Rust inside Microsoft, I believe it will be prudent to start a Rust Reviewers group for any PRs that include Rust code.  This will help novices in diverse teams get feedback from Rust experts, regardless of the specific problem domain.  In general, new components or existing components with clean interfaces will be the easiest to port to Rust.  The component I've been rewriting has been challenging, as there are many abstractions leaked from one layer to the next, requiring some preliminary refactoring before progress could be made."  Which is to say there were approaches and idioms from whatever it was written in before, probably C, that don't map one-to-one onto Rust because of the different approach that Rust takes.  So it required some reconceptualization of what the C code was doing in order to express it the way Rust wants things expressed.



But he's finishing, he says:  "Keeping It Safe.  To obtain the desired safety guarantees from Rust, strict guidelines must be placed around the use of the unsafe keyword."  And I don't know Rust, so it's hard to say...



LEO:  It's very C-like, yeah.



STEVE:  Yeah.



LEO:  Unsafe says this is - yeah.  Well, you can guess what that is.



STEVE:  Yeah.  Well, what it must do is it must be explicitly turning off some of the things that the compiler would do in order for you to do something which you're acknowledging is unsafe.  And so the compiler says, okay, well, I was about to  slap you, but as long as you use that responsibly.  Anyway, so he says:  "Any calls to a Foreign Function Interface should occur in a wrapper function that provides a safe abstraction around it."  So in other words, you're sequestering the dirty bits that you have to have in order to talk to other parts of the system that haven't yet had interfaces defined that were safe.



Anyway, I had a little bit more here, but all of our listeners have the gist of it.  He ended up being very bullish.  And he noted somewhere, I didn't want to skip this, he noted going back to C++, he said:  "In general, using Rust has been a really great experience.  I look forward to seeing more developers at Microsoft working on the language and working with the wider community on making the language an even better fit for some of the things we do here at Microsoft."  So Microsoft is actively giving back to the Rust community.  There are things that he talks about where, because of the size of the projects they're doing, they need more enterprise-grade source and module management than Rust has in a way that is useful for them.  So they're actively working in an open sense to evolve Rust to be capable of being used within their own environments.



LEO:  It's kind of interesting.  They have, of course, TypeScript and other kind of safer functional languages they've developed, including, what is it, F++?  F?  So it's interesting they're using this open source solution.  Rust is well loved as a systems language.  And I think a lot of C programmers should be very comfortable with it.



STEVE:  Yes.  Well, yes.  And it is turning out, and this was some of the feedback from Facebook, it is holding its own in performance.



LEO:  Oh, yeah.  Rust is amazing.



STEVE:  So you're not taking a big...



LEO:  No.



STEVE:  Yeah, it's not like dropping into a scripting language.  I mean, you're getting the same level of performance.  I found the line that I thought was really interesting in here.  He said:  "After writing Rust code, I find writing C++ much more frustrating..."



LEO:  Oh, yeah.



STEVE:  "...since I can't rely on the compiler to ensure memory safety."  So anyway, I wanted to sort of just put another pin in this notion that it's time now.  As I said, lord knows our email clients are fast enough.  So let's write them in Rust so they're not sources of ridiculous security flaws as they continue to be.



LEO:  Nice.



STEVE:  Or some other truly safe language.  So Microsoft's RDP client, which...



LEO:  Beloved RDP client.



STEVE:  Yes, beloved RDP client for iOS is back.



LEO:  Oh.



STEVE:  I don't think I was aware that Microsoft had a remote desktop protocol client for iOS.  That might be sort of cool, though.  So their RDP client for iOS had not been updated for more than a year.  Then a couple of weeks ago they updated the client to version 10, put it up on the Apple App Store, then almost immediately pulled it down without saying anything.  It just disappeared.  And I saw that news, and I thought, well, okay.  That's interesting.  I didn't know there was an RDP client for iOS.  And how would that work, exactly?



But based upon some hearsay from users, the short-lived version 10.0 Remote Desktop client would wipe all of the user's saved Remote Desktop settings when it was installed, which apparently is undesired behavior.  One user posted:  " It's because it wipes all your saved config inside the app."



LEO:  Handy.



STEVE:  "It resets it to new."  He wrote:  "I managed to update before they pulled it."  And then another user also encountered a similar issue.  So there have been a couple people who concur.  But apparently after force-closing the app and restarting it, the existing profiles were still present.



Anyway, last Wednesday the iOS RDP client reappeared as version 10.0.1 with that problem solved.  And I have a link in the show notes to the App Store entry for it.  And I was curious because, well, combining it and a VPN would be an interesting solution for remotely managing Windows systems.



So they say:  "Microsoft Remote Desktop.  Get work done from anywhere.  By Microsoft Corporation.  Use Microsoft Remote Desktop for iOS to connect to a remote PC or virtual apps and desktops made available by your admin.  With Microsoft Remote Desktop, you can be productive no matter where" - you know.  We all know what the features are going to be.



One thing that caught my eye, though, was that it said, under What's New for 3rd of December - so maybe that was the first release.  Anyway, it's the version 10, which now is 10.0.1, does now have support for the Windows Virtual Desktop service.  There's a brand new connection center UI, brand new in-session UI for switching between connected PCs and apps, a new layout for an auxiliary onscreen keyboard, and then this is - oh, improved external keyboard support.  And then support for SwiftPoint Bluetooth mice.  And I said, what?  Mice?



Support for microphone redirection.  Support for local storage redirection.  Support for camera redirection.  Support for new iPhone and iPad devices.  Dark and light theme support.  Control whether your phone can lock when connected to a remote PC or app.  Collapse the in-session connection bar with a long press on the remote desktop logo.  Then they finished, I guess feeling a little sheepish that they hadn't been doing anything for a year:  "We're committed to making this app the best Remote Desktop client for iOS, and value your feedback.  If you encounter any errors, you can contact us via Settings/Report an Issue."



So I just sort of wanted to put this on everybody's radar.  I'm going to have to make some time to give this a try and play with it.  I didn't realize that one of the features in iPad OS v13, they sort of snuck in mouse support.  You get to it, and I found it, by going into the Control Panel.  Then under Assistive Support you turn that on.  And then a ways down is Devices.  And you open that, and there's mice in there.  So either a wired or a Bluetooth mouse can be used with your iPad, which I think would be kind of cool.  I mean, I love my iPads.  I've been loving them ever since version one.



LEO:  Well, also the big screen would be nice for remote access; right?  I mean, especially if you use iPad Pro.



STEVE:  Yes, iPad Pro, big screen.  I mean, maybe you can do it just by touch.  But anyway, I thought that was really cool.  So just wanted to let people know that exists.  I mean, if you ever have, I mean, you certainly - we know we don't want to have RDP exposed to the public Internet.  There's just no way to do that.  So you'd want to combine it with a VPN.  But given that you VPN for security, for secure access to the RDP server, being able to do something like emergency reboot or something with your phone, that would be cool.



Okay.  Enough Microsoft stuff.  Avast and AVG are in the doghouse.



LEO:  [Growling]



STEVE:  Uh-huh.  Yeah.  The Avast Online Security, Avast SafePrice, AVG Online Security, and AVG SafePrice extensions have all been pulled from the Mozilla add-on repository after  they were found to be silently collecting far more user-identifiable data than was required to do their jobs.  This story begins with a German web researcher and developer by the name of Wladimir - it's "W" rather than "V," so it's not Vladimir, it's Wladimir, I guess - Palant.  And so we might first ask, does Wladimir have any street cred?  I'd say so.



The first two paragraphs of his bio read:  "My name is Wladimir Palant, and developing software is both my day job and a hobby.  I became fascinated," he writes, "by Firefox extension development in 2003, and my Adblock Plus extension became so popular that I eventually co-founded Eyeo GmbH to continue its development."  So this is the Adblock Plus, the guy who originated Adblock Plus.



He says:  "By now Adblock Plus isn't available just for Firefox, but for Chrome, Opera, Safari, Android, and Internet Explorer, as well."  He says:  "My other notable Firefox extensions are Google/Yandex search link fix, as well as developer tools JavaScript Deobfuscator and Extension Auto-Installer."  And he goes on talking about he has an interest in web application security and so on.  So this guy knows what he's talking about.  He was poking around the Avast and AVG extensions and was shocked by what he found.  His announcement and takedown of these guys is titled "Avast Online Security and Avast Secure Browser Are Spying on You."  I've got the link in the show notes for anyone who wants more detail.



He asks, rhetorically:  "Are you one of the allegedly 400 million users of Avast AV products?  Then I have bad news for you:  You are likely being spied upon.  The culprit is the Avast Online Security extension that these products urge you to install in your browser for maximum protection.  But even if you didn't install Avast Online Security yourself, it doesn't mean that you aren't affected.  This isn't obvious, but the Avast Secure Browser has Avast Online Security installed by default.  It is hidden from the extension listing and cannot be uninstalled by regular means.  Its functionality is apparently considered an integral part of the browser.  Avast products promote this browser heavily, and it will be used automatically in banking mode.  Given that Avast bought AVG a few years ago, there's also a mostly identical AVG secure browser with the built-in AVG Online Security Extension."



So his summary of findings.  He says:  "When Avast Online Security extension is active, it will request information about your visited websites from an Avast server.  In the process, it will transmit data that allows reconstructing your entire web browsing history and much of your browsing behavior.  The amount of data being sent goes far beyond what's necessary for the extension to function, especially if you compare to competing solutions such as Google's Safe Browsing.  Avast's Privacy Policy covers this functionality and claims that it is necessary to provide the service.  Storing the data is considered unproblematic due to anonymization" - he says, "(I disagree)" -  "and Avast doesn't make any statements explaining how long it holds onto it."  And actually some additional research that he did later explains probably why they're holding onto it.



So he says:  "What's happening exactly?  Using browsers' developer tools, you can look at an extension's network traffic.  If you do it with Avast Online Security, you'll see a request to" - and it's a URL, uib.ff.avast.com, and that "ff" is probably Firefox, dot avast.com/v5/urlinfo.  He says, "...whenever a new page loads in a tab."  And he shows an example here of a screenshot, I have it in the show notes for anyone who's interested, of a snapshot of the data which is being sent.



He said:  "So the extension sends some binary data, and in return gets information on whether the page is malicious or not."  That is, it sends this data out, pinging the Avast server for is this page safe to allow the user to visit or not.  The response is then translated into the extension icon to be displayed for the page.  He says:  "You can clearly see the full address of the page in the binary data, including query part and anchor.  The rest of the data is somewhat harder to interpret.  I'll get to it soon.  This request isn't merely sent when you navigate to a page.  It also happens whenever you switch tabs.  And there's an additional request if you are on a search page."  Get this.  "This one will send every single link found on this page, be it a search result or an internal link of the search engine."



LEO:  Well, that's not nice.  I understand them checking the page before you log in.  I mean, that's, by the way, why I don't like antiviruses.  I've never recommended AVG or Avast for this reason.  It was free.  It's free for a reason.



STEVE:  Yes.  I was going to say that.  And I encounter it installed on, I mean, I don't want to say "just about every."



LEO:  All the time, yeah.



STEVE:  But it's all over the place.



LEO:  Now, they claim that they're anonymizing this, and it's not - but still.



STEVE:  Well, and he addresses the fact that studies have shown what degree of deanonymizing is possible, given enough data.  And, okay.  So he says:  "The binary UrlInfoRequest data structure used here can be seen in the extension source code.  It is rather extensive, however, with a number of fields being nested types.  Also, some fields appear to be unused, and the purpose of others isn't obvious.  Finally, there are 'custom values' there, as well, which are a completely arbitrary key/value collection."  He says:  "That's why I decided to stop the extension in the debugger and have a look at the data before it's turned into binary.  If you want to do it yourself, you need to find this.message call in scripts/background," blah blah blah.  And he explains how to do it.



So what's there?  He says the interesting fields were the URI, the full address of the page you're on; the title, the page title, if available; the referer; the windowNum or tabNum, which is the identifier of the window and tab that the page loaded into.  Then there's an initiating user action window event, how exactly you got to the page, for example, by entering the address directly, by using a bookmark, or by clicking a link.  Then, whether you visited this page before, a "visited" field.  The locale, your country code.  A user ID, a unique user identifier generated by the extension, so that's going to be globally static for you for the life of your use of this.  Again, completely unnecessary for them to have that.  Except for the way they're monetizing, which we'll get to in a second.



They say:  "For some reason this one wasn't set for me when Avast AV was installed," although he has that shown in his notes.  There's a plugin GUID, a globally unique ID.  Seems to be another unique user identifier, the one starting with "ceda" in the screenshot above.  "Also not set for me when Avast AV was installed."  A browser type and browser version; an OS and OS version.  And he says:  "That's merely the fields which were set.  The data structure also contains fields for your IP address and a hardware identifier, but in my tests these stayed unused.  It also seems that, for paying Avast customers, the identifier of the Avast account would be transmitted, as well."



So he says:  "What does this data tell about you?  The data collected here goes far beyond merely exposing the sites you visit and your search history.  Tracking tab and window identifiers, as well as your actions, allows Avast to create a nearly precise reconstruction of your browsing behavior:  how many tabs do you have open, what websites do you visit and when, how much time do you spend reading/watching the contents, what do you click there, and when do you switch to another tab.  All that is connected to a number of attributes allowing Avast to recognize you reliably, even a unique user identifier.



"If you now think, 'But they still don't know who I am,' think again.  Even assuming that none of the website addresses you visited expose your identity directly, you likely have a social media account.  There have been a number of publications showing that, given a browsing history, the corresponding social media account can be identified in most cases."



So, finally, "Isn't this necessary for an extension to do its job?  No.  The data collection is definitely unnecessary to this extent.  You can see this by looking at how Google Safe Browsing works, the current approach being largely unchanged compared to how it was integrated in Firefox 2.0 back in 2006.  Rather than asking a web server for each and every website, Safe Browsing downloads lists regularly so that malicious websites can be recognized locally."  And then he cites a little blurb from Firefox:  "No information about you or the sites you visit is communicated during list updates.  Before blocking the site, Firefox will request a double-check to ensure that the reported site has not been removed from the list since your last update.  This request does not include the address of the visited site.  It only contains partial information derived from the address."



So Firefox demonstrates, and Google has demonstrated, it's entirely possible to offer this kind of service without, well, first of all, with local caching of known problematic websites, and it's merely a confirmation to double check.  So he writes:  "I've seen a bunch of similar extensions by antivirus vendors, and so far all of them provided this functionality" - that is, similar to Firefox - "by asking the AV app itself," not an online server.  "Presumably, the antivirus has all the required data locally and doesn't need to consult the web service every time."



Then he later made an edit.  He said:  "I got a hint that Avast acquired Jumpshot a bunch of years ago.  And if you take a look at the Jumpshot website, they list 'Incredibly detailed clickstream data from 100 million global online shoppers and 20 million global app users.  Analyze it however you want.  Track what users searched for, how they interacted with a particular brand or product, and what they bought.  Look into any category, country, or domain."  And on and on.  He says:  "So now you have a pretty good guess as to where your data is going."  And again, Leo, I agree with you.  This is free.  You get what you pay for.  You and I would argue that Windows Defender is now doing as good a job as this.



LEO:  It's always done better than AVG and Avast.



STEVE:  Yes, exactly.



LEO:  I mean, yeah.



STEVE:  Exactly, exactly.



LEO:  The real irony of this is that these are companies that in theory make antimalware tools that one could argue are creating, by putting it on your machine, you're installing spyware.



STEVE:  Yes.  I mean, this is extensive, I mean, admittedly it's a GUID.  If you're a paying customer, they may know who you are.  And I'm not suggesting that anybody who is taking advantage of this Jumpshot data - but, boy, Leo, if you go over to Jumpshot, it's chilling.



LEO:  Oh, boy.



STEVE:  It's one of those deep research, learn who's searching for what.  And so basically AVG and Avast users are being monetized, without their knowledge or permission, by feeding all of their browser activity, tab switching, and how they got to the page, and what they clicked on once they were there, their entire clickstream is being fed back into a database for monetization.



LEO:  Yeah, I mean, just the fact that every link on every page I visited is also sent back.



STEVE:  Yes.



LEO:  There's no security reason for that, so obviously they're gleaning, intentionally gleaning information from the pages I'm on.  That's not okay.



STEVE:  Yeah.  He concludes:  "Avast Online Security collecting personal data of their users is not an oversight and not necessary for the extension functionality either.  The extension attempts to collect as much context data as possible, and it does so on purpose.  The Avast privacy policy shows that Avast is aware of the privacy implications here.  However, they do not provide any clear retention policy for this data.  They rather appear to hold onto the data forever, feeling that they can do anything with it as long as the data is anonymized.  The fact that browsing data can usually be deanonymized doesn't instill much confidence, however.



"This is rather ironic, given that all modern browsers have phishing and malware protection built in that does essentially the same thing, but with a much smaller privacy impact.  In principle, Avast Secure Browser has this feature as well, it being Chromium based.  However, all Google services have been disabled and removed from the settings page.  The browser won't let you send any data to Google, sending way more data to Avast instead."



So he reported his findings to Mozilla, who agreed, and immediately yanked all of Avast and AVG extensions from the Firefox extension repository.  Avast have stated that they are working to correct this and will have an acceptable extension, which collects far less information, available as soon as possible.



LEO:  Can you still use a Chrome extension?  Is there a Chrome extension?



STEVE:  Yes.  Google has been notified; and, as of this reporting, has not taken the same action.  Google had not pulled the extensions from the Chrome Web Store.  And my annoyance with this behavior is that companies are essentially doing everything they can get away with.  This is not the end of the world for any of Avast's 400 million users.  As we say, you know, they're sort of like, well, you know, you're getting what you paid for, which is you didn't pay anything.



But they appear to have been collecting all of this data just because they could.  No one said no.  No one had looked.  And it's just not possible for every piece of communicating software to be examined and vetted ahead of time.  I mean, Mozilla, they wouldn't have allowed this.  But they just said, oh, you know, it looks like, you know, like click the button if you abide by our guidelines.  And so Avast said, oh, yeah, we do.  Well, when someone said, uh, take a look at this, Mozilla said, holy crap, and immediately yanked them all.



Unfortunately, it needs to be preemptive, rather than after the fact.  And it'll be interesting to see how, if the clickstream is really curtailed, like they're clearly going to work to maintain as much of this as they can while still working to please Mozilla because they're clearly monetizing all of their customers' browsing habits.  I mean, again.  And if it were right upfront, if they were being really clear to their users that we're giving you free antiviral, and in return we're using anonymous collection of your browsing history and selling that somewhere else, it's like, then if the user says yeah, okay, fine, then okay.  And I think most users probably would.  Not our listeners.



LEO:  Yeah.



STEVE:  Something that I ran across last summer and was very interested in and have on my list to get to in early 2020, is a new VPN, and by that I mean protocol, not like another version of OpenVPN.  This thing's called WireGuard.  And it pinged onto my radar this week because of an interesting announcement regarding it.  So I have not yet made time to discuss it in depth.  I will be.  And I am not alone in believing that this is the future of VPNs.  As I said, I first became aware of it last summer.  After reading into it a bit, I bookmarked the project for follow-up, to tell our listeners.  And I will be doing so.



LEO:  We should make it clear, this is not a client you're talking about.  It's a server.  This would be a replacement for OpenVPN or...



STEVE:  Correct.  It's not a client or server.  It is, like, both ends.  It is a...



LEO:  Oh, okay, it's both.  It's a protocol, so you'd have to have a WireGuard compatible client to use it.



STEVE:  Correct.  It was everything I loved, a complete reconceptualization and a rewrite from scratch of a Virtual Private Network, very much as I was planning to do with CryptoLink, as our longtime listeners will recall.  It's a simple take on the VPN.  In their own words, they say:  "WireGuard is an extremely simple, yet fast and modern VPN that utilizes state-of-the-art cryptography.  It aims to be faster, simpler, leaner, and more useful than IPSec, while avoiding the massive headache.  It intends to be considerably more performant than OpenVPN.  WireGuard is designed as a general purpose VPN for running on embedded interfaces and super computers alike, fit for many different circumstances.  Initially released for the Linux kernel, it is now cross-platform - Windows, macOS, BSD, iOS, and Android - and widely deployable.  It is currently under heavy development, but already it might be regarded as the most secure, easiest to use, and simplest VPN solution in the industry."



So sometime after that, while doing research for the podcast, I encountered an independent set of benchmarks which compared WireGuard with OpenVPN, and it wasn't even close.  WireGuard blew the packets off OpenVPN.  So I have not yet switched over to it myself, only because I've got a bit too much on my plate as it is.  But it goes a long way toward solving one of the longstanding annoyances with a VPN, which is, as we know, it tends to be a little slower than a non-VPN connection.



So the next time I encountered WireGuard was a total coincidence.  The sponsor of my OWASP SQRL presentation in Sweden was a company named Mullvad.  I think their whole name is Mullvad VPN, which I'd never heard of at the time.  And anyone who's seen the SQRL Sweden presentation video may have noticed some of the signage down there.  Anyway, they took everybody to dinner the night before; and I got to meet the president and founder, and a bunch of the techies there.  And I was really interested because they exclusively use WireGuard.



And they've got a really interesting model.  When you sign up on their website, you get a serial number.  That's it.  No username, no email address, just a serial number.  Then you somehow arrange to send them money.  And they told some rather funny stories about receiving boxes of coins from anonymous people saying here's my money, and then here's the serial number.  And so they fund that serial number, and you get to use WireGuard with multiple endpoints and so forth.  Anyway, I ended up being very impressed with the company, met the owner, and they seemed like neat people.



So what happened this week is that Steven Vaughan-Nichols, writing for ZDNet, asked rhetorically:  "How much are people looking forward to WireGuard, the new in-kernel Linux virtual private network?"  Linus Torvalds recently said:  "Can I just once again state my love for it and hope it gets merged soon?"  He said:  "Maybe the code isn't perfect.  But I've skimmed it; and, compared to the horrors that are OpenVPN and IPSec, it's a work of art."  And then Steven Vaughan-Nichols wrote:  "That's not really damning with faint praise because, for Linus, this is high praise."



He said:  "WireGuard has now been committed to the mainline Linux kernel.  While there are still tests to be made and hoops to be jumped through, it should be released in the next major Linux kernel release, 5.6, in the first or second quarter of 2020."  He said:  "It's been in development for some time.  It's a layer 3 secure VPN.  Unlike its older rivals, which it's meant to replace, its code is much cleaner and simple.  The result is a fast, easy-to-deploy VPN.  While it started as a Linux project, WireGuard code is now cross-platform and is now available on Windows, macOS, BSD, iOS, and Android."  And then I have some more details in the show notes for anybody who's interested.



He says - I skipped a bunch.  "WireGuard works by securely encapsulating IP packets over UDP.  Its authentication and interface design has more to do with Secure Shell than with other VPNs.  You simply configure the WireGuard interface with your private key and your peers' public keys, and you're ready to securely talk."  So that's all there is to it.  He says:  "When it arrives, I expect WireGuard to quickly become the new standard for Linux VPNs.  With its tiny code size, high-speed cryptographic primitives, and in-kernel design, it should be faster than all other existing VPN technologies."  WireGuard's not just fast, it's secure, as well.



With its support of state-of-the-art cryptography technologies such as the Noise protocol framework; Curve25519, which of course I'm a fan of, that's what SQRL uses; ChaCha20, Poly1305, BLAKE2, those are all Bernstein's works; and other stuff.  Oh, and then I was surprised he mentioned, he said:  "And this is why some companies - like Mullvad VPN - adopted WireGuard long before it was incorporated into Linux.  As Mullvad co-founder Fredrik Strmberg wrote two-years ago:  'We find WireGuard beneficial for a number of reasons.  Its simplistic design in few lines of code makes it easier for sysadmins and developers to integrate it correctly, and harder for them to get it wrong.'"



He says:  "Thus, WireGuard will move the world one step closer to our own vision of making mass surveillance ineffective."  So anyway, I think we've got a new VPN here on the horizon, and I will be taking our listeners into a deep dive of it early next year.



LEO:  That's cool.



STEVE:  Which brings us to VPN-geddon.  Once again, we have breathless headlines.  I have five of them.



LEO:  It's nonstop.



STEVE:  "New Linux Vulnerability Lets Attackers Hijack VPN Connections."  "Linux Bug Opens Most VPNs to Hijacking."  "New Vulnerability Lets Attackers Sniff or Hijack VPN Connections."  "New Linux Bug Lets Attackers Hijack Encrypted VPN Connections."  And, finally, "Networking Attack Gives Hijackers VPN Access."  None of which is true.



LEO:  Hmm.



STEVE:  They all sound really bad.  And again, I guess partly the problem is the guys who posted the advisory did make this sound like the end of the world.  They kind of overhyped what it was they had done.  But for our more technically oriented listeners, I think everybody will find this sort of interesting.  But first, not to pick on any one of the publications, but every one of the publications that I routinely monitor for security news had this as one of their headlines and took the opportunity to say something about it.



So, for example, one of them in the press that people would be reading says:  "A team of cybersecurity researchers has disclosed a new severe vulnerability affecting most Linux and Unix-like operating systems - FreeBSD, OpenBSD, macOS, iOS, and Android - that could allow remote network-adjacent attackers to spy on and tamper with encrypted VPN connections.  The vulnerability, tracked as" - and we have a CVE number - "resides in the networking stack of various operating systems that can be exploited against both IPv4 and IPv6 TCP streams.  Since the vulnerability does not rely on the VPN technology, the attack works against widely implemented Virtual Private Network protocols like OpenVPN, WireGuard, IKEv2/IPSec and more, the researchers confirmed.



"This vulnerability can be exploited by a network attacker controlling an access point or connected to the victim's network just by sending unsolicited network packets to a targeted device and observing replies, even if they are encrypted.  As explained by the researchers, though there are variations for each of the impacted operating systems, the vulnerability allows attackers to" - and we have four bullet points - "determine the virtual IP address of a victim assigned by the VPN server; determine if there is an active connection to a given website; determine the exact sequence and acknowledge numbers by counting encrypted packets and/or examining their size; and inject data into the TCP stream and hijack connections."



Okay, now, nobody would be blamed, if you were to read that, for thinking, holy crap, this is really bad.  Except it's not true.  If it were, it would sound like VPN-geddon.  But it's not.  So that was mostly a repeat of the announcement of these so-called "security guys."  I mean, they did some stuff.  But they didn't really do what they claimed to have done.  I've skipped the beginning of their opening disclosure because it was pretty much what I just read.



They said:  "There are three steps to this attack:  Determining the VPN client's virtual IP address."  So oftentimes you connect up to a remote OpenVPN server, and you'll get a 10.8.0.something virtual IP because the VPN server is giving you a different IP that routes through your network stack to get to the virtual interface that the VPN client has established on your local machine.  You talk to that, using that IP, and it goes to the virtual VPN interface, which then encrypts it and sends it to the physical interface, which then transits it out over the public Internet, like to the VPN, which then decrypts it and lets it loose on the Internet.



So step one, determining the VPN client's virtual IP address, not their physical IP address.  Two, using the virtual IP address to make inferences about active connections.  And then, three, using the encrypted replies to unsolicited packets to determine the sequence and acknowledgment numbers of the active connection to hijack the TCP session.



Okay.  So they said then in their disclosure there are four components to the reproduction:  the victim device connected to an access point; the access point, controlled by the attacker, meaning that all of the victim's traffic is transiting through the access point.  So it's a man in the middle, basically.  The VPN server, that is remote, probably, not controlled by an attacker.  And then, although they do have the VPN server at 10.8.0.1, so they're apparently looking at the traffic going to the VPN server.  And then a web server not controlled by the attacker, with a public IP out in the real world.



So they say:  "The victim device connects to the access point, which for most of our testing was a laptop running create_ap.  The victim device then establishes a connection with their VPN provider."  And, okay, so this is sort of a special case because they apparently have access to the VPN server at 10.8.0.1, so they're also seeing the virtual traffic transiting the access point.  So that's sort of a bit of a special case, but okay.



Then they said the access point, meaning the man in the middle, can then determine the virtual IP of the victim by sending SYN-ACK packets to the victim device across the entire virtual IP space.  And they say the default for OpenVPN is 10.8.0.0/24.  So it's 10.8.0.x, right, where "x" is between 1 and 254.  So they said:  "When a SYN-ACK is sent to the correct virtual IP on the victim device, the device responds with a reset packet.  When the SYN-ACK is sent to the incorrect virtual IP, nothing is received by the attacker."



So, okay.  So first of all, this is the definition of a kludge.  So they're determining the virtual IP of the interface that the victim is using by essentially port scanning it by sending SYN-ACKs to some random port, probably doesn't matter, but at all of the different IPs available because they saw that, when they hit the right one, it will object with a reset, as it should, over TCP protocol.  And if a SYN-ACK comes to just a random IP that nobody's listening on, it goes nowhere, so nothing happens.



Okay.  So they're saying from their position they can get the virtual IP that the VPN server assigned to the client, who is the victim.  It's like, okay.  So then they say, and here it really gets a bit of a stretch:  "Similarly, to test if there is an active connection for any given website, such as" - and they list as an example 64.106.46.56.  Don't know what that is.  "For example, we send SYN or SYN-ACKs from 64.106.46.56 on port 80 or 443 to the virtual IP of the victim across the entire ephemeral port space of the victim."



Okay.  Now, the ephemeral port space starts at 1024.  Remember that the service port space is defined as 1 to 1023.  Those are the ones you need to be privileged.  You need to be root in order to open a listening socket on those low port numbers.  Apps like OpenVPN are able to do it in the high port space.  And outgoing connections from like a web browser automatically walk up through that ephemeral port space.  So they're sending SYNs or SYN-ACKs from a public IP to port 80 or 443 that they assume the victim has traffic with that public server on, again noticing a slight difference in behavior.



So they used the term "four-tuple," which we remember from when we used to talk about the TCP protocol.  The four-tuple is the source IP, the source port, the destination IP, and the destination port.  You have to have all four of those correct, and that defines the endpoints of a TCP connection.  So they said:  "The correct four-tuple will elicit no more than two challenge ACKs per second from the victim, whereas the victim will respond to an incorrect four-tuple with a reset for each packet sent to it."



So this doesn't let you determine the site that somebody is talking to.  And remember there are four billion public IPs, give or take.  But if you have to use the first scan to figure out the virtual IP of the person, and you wanted to see whether they were talking to a given public server, you would send - basically you would DoS this victim with TCP SYNs or SYN-ACKs on the port that you need to know also, that you assume they're connecting to this remote server over.  So once upon a time 80, more likely 443, or who knows what if it's a different type of server.



So you send all these SYNs and SYN-ACKs as if they were from that public IP and port 80 or 443, to all of the ephemeral ports because you don't know what the outbound port number on the virtual IP is that the victim is currently using.  So you spray them all.  And you get a reset if you guess wrong, or no more than two challenge ACKs per second if you got the four-tuple right in order to confirm that this person in fact has an open TCP connection to the remote server.  Notice that we're not able to see into the traffic; right?  It's encrypted.  The VPN is intact.  It's still there.  It's working.  So what we're doing is, from this privileged man-in-the-middle position, we're sort of spraying the victim with stuff, TCP garbage, to learn what we can of their connection.



And then they say here:  "Finally, once the attacker determined that the user has an active TCP connection to an external server, we will attempt to infer the exact next sequence number and in-window acknowledgment number needed to inject forged packets into the connection."  Now, I'll just note that they can't get the private key which is being used to encrypt these packets.  So you can't forge a packet.  I mean, you can send nonsense to the victim.  Okay.  But so what?



Anyway, so then they say:  "To find the appropriate sequence and ACK numbers, we will trigger responses from the client" - we're going to send more crap to it - "responses from the client in the encrypted connection found in part two."  So they're acknowledging that it's an encrypted connection.  So we're going to spray the encrypted connection with some more crap and see what bounces off of it.  They said:  "The attacker will continually spoof reset packets into the inferred connection until it sniffs challenge ACKs.  The attacker can reliably determine if the packets flowing from the client to the VPN server are challenge ACKs by looking at the size and the timing of the encrypted responses."  Again, they can't read them, but they're going, ooh, look, it's tiny, so it must be an ACK.



"In relation," they say, "to the attacker's spoofed packets.  The victim's device will trigger a TCP challenge ACK on each reset it receives that has an in-window sequence number for an existing connection.  For example, if the client is using OpenVPN to exchange encrypted packets with a VPN server, then the client will always respond with an SSL packet of length 79" - that is, 79 bytes - "when a challenge ACK is triggered."



They say:  "The attacker must spoof resets to different blocks across the entire sequence number space until one triggers an encrypted challenge ACK."  Now, as we recall from TCP, the sequence number and ACKs are 32-bit numbers.  And so, and we don't know how large the window is, so we're having to spray the possible sequence number space, the 32-bit space, with this junk, looking for one that triggers an encrypted challenge ACK.  And again, it's encrypted.  They said:  "The size of the spoof block plays a significant role in how long the sequence inference takes" - yeah, no kidding, because it's 34 billion - "but should be conservative" - uh-huh.  You need to do a lot of them.  You can't assume the blocks are too big - "but should be conservative as to not skip over the receive window of the client."  You've got to hit the window in order to get an ACK back.



"In practice, when the attacker thinks it sniffs an encrypted challenge ACK, it can verify this is true by spoofing X packets with the same sequence number.  If there are X encrypted responses with size 79 triggered, then the attacker knows for certain it is triggering challenge ACKs," and he says, "at most two packets of size 79 per second."  Anyway, I'm not going to continue with this nonsense.  Everyone gets the idea.  This is just such a crock.



The first parts of this so-called "attack" attempt to simply probe the state of the TCP stack by flooding its IP and port space with packets designed to elicit a reply.  Then they spoof packets from an assumed public IP space designed to, again, elicit a reply.  And all they're doing is they're seeing that basically they're annoying the virtual interface which is encrypted by whatever VPN is in use.  They're annoying it, and the TCP stack is responding as TCP stacks do to their poking at it with encrypted results that they can't read.  And the real downfall is that then they say that they are able to - what was the term?  I can't even - my brain won't remember it, it's such nonsense.  Oh, that they're able to hijack the connection.  Except it's encrypted.



And so, what?  I mean, it's true that a TCP connection can be hijacked if it's not encrypted because the attacker can see the sequence numbers going back and forth.  And we talked a long time ago about how BGP routers, routers that were using Border Gateway Protocol in order to keep their routing tables synchronized, there were attacks on their long-term persistent TCP connections by people who from outside were blasting them with nonsense and guessing what the sequence numbers were, and in fact being able to hijack the connection.  Which works if it's not encrypted.  But if it's encrypted, you can't hijack the connection.  I mean, again, all you can do is piss off the TCP/IP stack on the virtual interface and cause it to emit a bunch of encrypted annoyed ACK packets.  But that's it.



So anyway, I just thought this was interesting because it was an example of a complex attack which sounds really bad, but it's probably like the people who know better are like, yeah, okay.  So?  And admittedly, it does breach some of the privacy  guarantees that you would like your VPN to be permitting, but only if you establish a man-in-the-middle position, and you have the ability to also see the virtual IP traffic going to the VPN server.  So that's a bunch of necessary preconditions.  And even so, you are never able to see into the traffic.  And you can't even see the remote server.  It's talking to you.  You have to guess from the four billion public IP space and send probes at the poor virtual interface and see if it annoys it or not, and in which way it annoys it.  Anyway, I just thought, wow, okay.  Game over.



LEO:  It feels like the authors of this article were a little disingenuous, like they didn't have anything here.



STEVE:  Yeah.  They really didn't have anything.



LEO:  So what was the point?



STEVE:  They really overplayed this.  I just - they were amateurs who thought, ooh, wow.



LEO:  Look what I can do, yeah.



STEVE:  Look what I can do, yeah.  And it's like, yeah, okay.



LEO:  So what?



STEVE:  Unfortunately, yeah, the idea of hijacking a VPN connection, that has, I mean, those words have meaning.  And they've abused the meaning.



LEO:  Can they break the connection?



STEVE:  Yeah, they could probably bring it down, yeah.  Maybe.



LEO:  Okay, yeah.



STEVE:  Yeah, I mean, if they got lucky, well, see, you'd have to send a reset.  And you can't send a reset because you don't have any idea what the encryption is because you'd have to send an encrypted reset, and they can't generate any valid encrypted packets.  So I don't really - I'm not even sure, I mean, maybe if you flooded it with enough nonsense you could just annoy the...  



LEO:  That's just a DDoS.  Yeah, that's not - yeah.



STEVE:  Yeah, exactly, just a DDoS.  And so that's not special.  Yeah, I mean, they said - that fourth point, I'm looking at it.  Inject data into the TCP stream and hijack connections.



LEO:  But you can't.



STEVE:  Okay, well, right.  Not meaningful data.  You could just inject nonsense.



LEO:  And I wouldn't see it anyway because it doesn't fit the schema; right?



STEVE:  Right, right.  It bounces off the leading edge of the TCP/IP stack.  So it's just like, nah, just going to be ignored.



LEO:  Oh, yeah.  Maybe they just - maybe they weren't very sophisticated themselves, and they thought that [crosstalk].



STEVE:  I think they probably weren't very sophisticated.



LEO:  Didn't have any impact at all.



STEVE:  Got a lot of press.  Got a lot of press.



LEO:  Yeah, well, and you know, to be fair, most of the press, even the tech press isn't technically that technical.  And so they're just going, oh, oh, oh.  So it's good we've got you.  That's all I can say.



STEVE:  Well, and that's why I didn't read the beginning of their disclosure, because it was echoed by the tech press.



LEO:  Right.



STEVE:  Who just copied this alarming notice and said, "Oh, no."



LEO:  Right. 



STEVE:  Anyway, "Oh, no" is right.



LEO:  Oh, no.  No.  No geddon of any kind.  Steve Gibson is our man.  He is at GRC.com.  That's the Gibson Research Corporation.  SpinRite lives there, the world's best hard drive recovery and maintenance utility.  He's working on the new one.  And given a little time, I think he will come up with something pretty special.  But you can get the existing one right now and get a free upgrade when it's out, 6.1 is out.



You can also go there to get the show.  He's got 64Kb audio, just like we do.  But he also has 16Kb audio for the bandwidth-impaired and a full human-written transcription, so you can read along as you listen, all at GRC.com.  So check that out.  He's on the Twitter at @SGgrc.  And you can direct message him there if you've got comments, questions, suggestions:  @SGgrc.



We have audio and video of the show.  Holy cow.  Why would you want video?  I don't know.  Because there's pretty pictures.  All you've got to do is go to TWiT.tv/sn.  You can also watch on YouTube.  And do, if you will, subscribe.  That way you'll get a copy the minute it's available of a Tuesday afternoon.  We do the show usually 1:30 Pacific, 4:30 Eastern, 21:30 UTC, if you want to watch live, TWiT.tv/live.  You can chat live, too, at irc.twit.tv.



Steverino, thank you so much.  I'll see you next week on Security Now! 



STEVE:  Right-o.



Copyright (c) 2019 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.






GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#745

DATE:		December 17, 2019

TITLE:		Plundervolt

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-745.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we start with a reminder about Google's still operating Sensorvault.  We look inside Google's new "Verified SMS Messages" feature.  We examine another salvo in the end-to-end encryption war, note a nice authentication feature added to iOS v13.3, and deliver some Patch Tuesday news.  We discuss a startling discovery about the weaknesses of RSA at scale, a collection of quick bits about last Friday the 13th, Mozilla 2FA for add-on developers, the surprising hard out for Microsoft's Security Essentials, and two bits about Chrome 79.  We have a clarification about last week's "VPN-geddon Denied" discussion, a significant announcement about my new focus, and some SQRL news.  We conclude with a look at yet another interesting new way of compromising Intel processors known as "Plundervolt."



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here with lots to talk about.  Is Google watching you?  Yes, they are, and what you might want to do about it.  RSA certificates are broken, at least on some IoT devices.  This is another thing you'll want to be aware about.  And Patch Tuesday.  What did we get?  What are we getting?  Coming up next, you're getting Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 745, recorded Tuesday, December 17th, 2019:  Plundervolt.



It's time for Security Now!, the show where we cover the latest in security, privacy, ransomware, and Mr. Steven Gibson's mustache.  Happy holidays, Steve.



STEVE GIBSON:  As it gets ever whiter.  Ah, yes, happy holiday, likewise.



LEO:  Likewise.



STEVE:  So today's episode is titled "Plundervolt."



LEO:  Plundervolt?



STEVE:  It is, believe it or not, the domain was available, so they grabbed it, and they gave it a logo.



LEO:  Oh, my.



STEVE:  And we're going to talk about it for Episode 745, this lovely December 17th.  It's lovely in Southern California.  I assume it's nice up where you are.



LEO:  It's a little gloomy, but it's not rainy.  It's been cold.  Has it been cold in Southern California?



STEVE:  Yeah, it's been, yeah.  And we're so spoiled here in California.  It's like, oh, darn, you know.



LEO:  Cold for us is low 50s.  Be clear.  I was talking to somebody in Winnipeg who said that, yeah, it's 37 below at night.  37 below at night.



STEVE:  Below, yes.



LEO:  That's cold.



STEVE:  So we have a reminder about Google still operating Sensorvault.  We're going to take a look inside Google's new verified SMS messages feature.  We've got another salvo in the end-to-end encryption war, went back and forth last week.  A nice authentication feature which has just been added to iOS 13.3.  We've got some Patch Tuesday news.  A startling discovery about weaknesses of RSA at scale, that is, at Internet scale.  Really interesting sort of a techie topic I think our listeners are going to enjoy.



Then we've got a collection, since there was so much of that, a collection of quick bits about last Friday the 13th; Mozilla requiring two-factor authentication for add-on developers; a surprising hard out for Microsoft's Security Essentials coming to everyone next month; two bits about Chrome 97, which is just out.  And then we have a clarification about last week's "VPN-geddon Denied" discussion; a significant announcement about my new focus; some SQRL news; and then we conclude with a look at yet another interesting new way of compromising Intel processors known as "Plundervolt."



LEO:  Plundervolt.



STEVE:  Plundervolt.  Wonderful.



LEO:  Wunderbar.



STEVE:  So I think it'll be another interesting podcast for our listeners.



LEO:  All right, Steve.  I've got your picture ready to go here.



STEVE:  So I actually had the logo for Plundervolt as our Picture of the Week because nothing else had occurred to me when I ran across this little tidbit that I thought, okay, this gets the Picture of the Week.  The news is that coming soon to a Windows 7 system near you will be a full-screen, I mean, I guess it's when you boot the system up.  It's not really clear when you're going to be assaulted with this.  But it's not a little dialog anymore, it looks like Blue Screen of Death.  And in some senses it is.  It says:  "Your Windows 7 PC is out of support."



So presumably that begins on January 15th.  And I was a little curious by some of the wording.  So anyway, it doesn't have very much on there because it knows people are like, what?  Like someone doesn't know.  Maybe they somehow missed all of the previous announcements of this.  They said:  "As of January 14, 2020, support for Windows 7 has come to an end.  Your PC is more vulnerable to viruses and malware due to," colon, then three things - no security updates, no software updates, no tech support.  And then here's the part that I thought, well, this is interesting.  "Microsoft strongly recommends using Windows 10."  That's no surprise.  But then they go on to say, "On a new PC."



LEO:  Yeah.  Buy a new PC.  



STEVE:  And it's like, what?  "For the latest security features and protection against malicious software."  And then there's a "Learn more," there's a "Remind me later," and then thank goodness in the lower left is a "Don't remind me again."



LEO:  Never tell me again.



STEVE:  Yeah.  So I got the message.



LEO:  I have to say this is an improvement because in the past Microsoft's made it difficult.  Remember, that's why you had Never10, because they put these pop-ups up, and you'd never get rid of them.  And yes, "Learn more" is the default.  "Remind me later" is also a big button.  But at least they put over here "Don't remind me again."  Go away.  And that will, according to Mary Jo, that will make it go away.



STEVE:  Yeah.  And I did find it interesting, when I was doing the digging around Microsoft site for the detailed information on the continuing availability of an essentially or effectively free Windows 10 upgrade, I did find it interesting that, if you sort of followed the normal path, you ended up at Microsoft trying to sell you hardware for Windows 10.



LEO:  Yeah, yeah.  They want you to buy a new PC.  That's, you know, that's a sop to their OEMs, yeah.



STEVE:  Yeah.



LEO:  And it's also an acknowledgment that most people never upgrade Windows.  They just buy a new computer.  That's how most people get Windows in the first place; right?



STEVE:  And, well, and actually, if you have Windows 7 on your PC, because everything has only shipped with Windows 10 now for so long, it is either you somehow deliberately arranged to struggle not to get Windows 10, or your machine is old; you know?  Or your machine is old.



LEO:  Right.



STEVE:  Because anything you would have bought in the last few years, you don't have a choice.



LEO:  It's more than five years old, anyway, yeah.



STEVE:  Yeah.  So it's probably got a smaller, slower drive.  It doesn't have as much RAM as Windows 10 would like to take over and squat on.  So, yeah.  Might be actually time to get a new machine.  And I'm sure our listeners know how to choose those. 



There was a bit of news that Forbes' Thomas Brewster covered that I just want to kind of remind our listeners that this was still going on.  And that is that Google's once-secret Sensorvault technology was back in the news this past week after some reporting by Thomas Brewster, who covers cybercrime, privacy, security, and surveillance for Forbes.  As we know, last year - well, actually we don't know.  We know about Sensorvault.  But the news is that last year and this year substantial damage was done to Milwaukee, Wisconsin property by some arsonists.  To help locate the crime's perpetrators, the Bureau of Alcohol, Tobacco, and Firearms - actually and Explosives, the ATF, I guess they decided not to add an "E" to the end, so it's not ATFE.



Anyway, they demanded that Google supply records of users, Android users' devices, which were located in the respective locations surrounding the times that the arsons took place.  And as Brewster reports, although federal agents had used, as we know, the Sensorvault technology before, apparently they got way more results than they were expecting.  Two search warrants requested any consumer information in Google's possession covering, as the reporting had it, 29,387 square meters, and this is the so-called geofencing range, during a course of nine hours for four separate incidents.



And as we know, if Android users or even iOS users running some Google apps have not turned off the defaulted on location history option, their physical whereabouts is being continually tracked and archived within Google's giant database called Sensorvault.  And when we first covered this we learned that, as I think I recall, that data went back 14 years?  I mean, so it was just - it was from the beginning of Google time, like all of the location information that they had been able to accrue, they were.  And there's never really been a clear, as far as I know, a clear indication of why this is going on.



In this case, Google found just shy of 1,500 qualifying device identifiers.  So I guess this was a high-traffic area.  It was 1,494 qualifying device identifiers which were found in Sensorvault and were sent to the ATF, pursuant to the subpoena, for the ATF to comb through.  So anyway, I wanted to, as I said, briefly revisit the topic with our listeners, just to remind everyone that unless they have explicitly turned it off, it's on.  And we talked about this back at the time.  Google did confirm that, if location history is turned off, then they will wipe it from the Sensorvault vault so that you're able to do a firm hard opt-out.



But at the time of this original reporting, I think it was in The New York Times, Congress stirred and sent Google a letter asking a bunch of questions about this.  And I never saw any reporting of Google's official reply.  Maybe that didn't make the news, or there were other issues happening at the time.  But what we learn about this kind of data collection is that, at the moment, if a company has something that they're collecting for some purposes, these secrets are difficult to keep.  And if they get out, and the company is issued a subpoena, they can be induced to turn these things over.  So a privacy respecting company, because it's really impossible, apparently, to keep these secrets, will not be keeping data that they may not want to be forced to turn over.



And unfortunately at this point Google has this massive archive, and for purposes that have never been made clear.  I guess it's their right to collect it if the license agreement for the devices states that's being done in the fine print.  But it certainly doesn't make people happy, and it sort of demonstrates that Google is not really respecting our privacy if it's up to us to opt out in order to be removed from this kind of database.  So anyway, I just wanted to sort of remind our listeners.  It's been some time since this came up.  And it's, as far as we know, still going on.



LEO:  I'm more upset with law enforcement because I feel like courts should say you can't issue a subpoena for everybody within 20,000 square meters of an event.  That's way too broad.  I mean, the police say, well, look, we will look at videos of a bank robbery.  Lots of innocent people in the bank are included in those videos.  But this is a much broader sweep of - it's a fishing expedition.  And I just feel like that's...



STEVE:  Yeah.  Well, and as we talked about at the time, there have been false arrests as a consequence of this.



LEO:  Right, right.



STEVE:  Because, again, you're not actually operating from direct evidence.  You're working from inferential evidence.  And so you're needing to say, well, we know it's not Sarah Jane and Suzy Q because they were somewhere else, or we know them, or who knows what.  But in the case that we covered at the time, they whittled it down to some guy and put him in jail for a week.  And he's still not happy.  He's still threatening to sue.



LEO:  The story I saw on this was they have two suspects as a result of this out of the 1,500 that they rounded up.



STEVE:  Ah.



LEO:  But they haven't filed charges against those two suspects.  So I think it's the similar case where, yeah, you get a lot of information about innocent people, and it doesn't necessarily narrow it down.



STEVE:  No.



LEO:  And bad guys, don't carry your cell phone with you next time.  You know?  I mean, this is something easily avoided by bad guys.



STEVE:  Yes, yes.  And in fact you can use that data on your behalf.  You can leave your cell phone at home.  And if you are  brought in as a suspect, you say, "No, I was at home, go ask Google Sensorvault."



LEO:  There you go.  There you go.  And they don't know you didn't have it, yeah.  I have it turned on.  I like it.  It's nice to know my - and you would use it maybe like when you went to Europe.  You would then have a track of all the places you visited.  It's kind of cool to have.  If you take pictures...



STEVE:  Oh, we used it like crazy.  In fact, even when I was up two weeks ago or a couple weeks ago, with you, I didn't mention it, but the night before we hung out for dinner with one of my old high school buddies who's up in the wine country, begins with an "H," I forgot the name of it.



LEO:  Healdsburg.



STEVE:  Healdsburg, yes, of course.  And we couldn't have found him.  He was out at the end of some long windy road through the grapevines.  And it's like, okay.  But the map knew right where he was.  So thank you very much.  



LEO:  Well, and a nice thing is Apple of late, and Android, too, will tell you these apps are looking at your location.  You want to only do that when the app is running.  And I think more and more people are aware of this.  What you forget is that Android does it all the time in the background, that Google's always keeping track.  So, yeah, clear your location history and turn it off unless you have a use for it.  I use it, so I don't mind.  And I'm not setting fires in Milwaukee, either.



STEVE:  So we got a new feature last week in Android's Messenger app.  It's just been enhanced to offer its users what they call "Verified SMS."  That's with a capital "V," so I think that's the official name.  And at this point - okay.  So it's kind of a cool feature, if a company wants to conduct business over SMS, which pretty much everyone now knows is just as much spam as it is real.



So Google said, okay, we're going to offer SMS verification.  And at this point the list of supporting companies, because it's something that companies have to sign up for with Google, is rather short:  1-800-Flowers, Banco Bradesco, Kayak, Payback, and SoFi.  That's it.  With any luck, the service will grow in time.  But their explanation was very light on details.  But there was just enough for me to sort of read through the watered-down explanation and figure out what must be going on.  So Google has effectively implemented an entirely separate authentication layer separate from SMS messages, which are just SMS messages.  So they go through the regular SMS system.  They're limited in size.  And they're non-encrypted.



What Google wanted to do was to arrange to authenticate them, to add a separate authentication layer.  So to do that, both ends of the SMS message connection also need to have data connections, that is, Internet connections back to Google for the exchange of metadata.  Companies must, as I noted, register with Google and securely establish their identities.  In Google's coverage of this, that's all they said.  We don't know how that's done.  Probably the way certificate authorities verify people.  Maybe, you know, it's never been very robust.  It's call you at the number that D&B has you registered as or something.  But somehow the point is Google is convinced that you are, you the company, are who you are claiming to be.  At which time the company generates a public/private key pair and provides their public key to Google for Google's redistribution of that company's public key.



Then, also, each instance of Android's Messenger also generates a similar public/private key pair and provides their public key to Google.  Now, that might be done ahead of time.  That might be done on the fly.  Either way would work.  And again, Google was very scant on facts here.  So when a company wants to send a "Verified by Google" SMS message, they obtain the recipient's public key from Google.  Okay, so I guess I answered my question.  That does presume that each Android device has preregistered, the Android Messenger has preregistered itself and its public key with Google so that the company wanting to send a verified message can get their public key.



So the company takes their private key and their recipient's public key and uses some key agreement protocol such as Diffie-Hellman Key Agreement (DHKA) which will allow them to synthesize what will end up being a shared secret key.  This key is used as the key for an HMAC through which they run the SMS message.  So basically they create a Message Authentication Code, a MAC of the message using the shared key which they derive from their private key and the recipient's public key.  They send that HMAC back to Google and send the SMS message directly to the Android user over the standard SMS channel.



When the Android user receives the SMS claiming to be from the company, they have the company's apparent phone number, that is, the originator of the message.  So Messenger asks Google for the private key matching that phone number.  After Google returns that to the Messenger app, just as happened on the sending end, the Android client uses its private key and the obtained public key with the same key agreement, such as Diffie-Hellman.  This, thanks to the magic of Diffie-Hellman Key Agreement, will yield the same key that the sender obtained.



So the recipient uses that to key the HMAC, runs the received SMS message through that, sends the message's HMAC back to Google.  And if the two MACs match, the one from the sender and the one that's been received back from the recipient, Google then sends back a flag that turns on verified sender notification, along with a bunch of other information about the company to allow you to do a little bit of digging into who this verified sender is.  So that's what they've done.  Basically they added an entirely separate authentication mechanism to SMS.



And as I was thinking about this, it sure seemed like a lot of work to go through.  But it does allow senders to send SMS messages to any Android recipients in a way that authenticates them as the sending company, and also absolutely prevents any tampering with the unencrypted SMS message while it's en route.  So no man in the middle can get it.  You can't spoof being the authenticated company.  You can't change the message in any way, or the MACs won't match.  And I did notice that U.S. law enforcement would have no complaint about this, since the system never encrypts the communications.  It's only providing robust authentication of the sender and preventing any tampering.



So in Google's coverage of this, they wrote:  "As part of this feature, Google attempts to verify all messages that appear to be sent by a business that is registered with Verified SMS. If the authenticity codes don't match, and Google can't verify the message, the Messages app displays:  'Message could not be verified.'  Because verification requires a data connection, if you have a weak data connection, the Messages app may display 'Verifying sender...,'" presumably like waiting to get enough data or a robust enough connection to actually perform the verification.



And Google said:  "If you have no data connection, the Messages app displays 'Waiting for connection to verify sender.'"  And they said:  "Until the sender of a message has been verified, Google doesn't recommend replying with sensitive info or opening links that you aren't sure you trust."  So anyway, as I said, I sort of wanted to cover it because I thought it was interesting from a cryptographic standpoint.  They're creating something new that we've never had before.  We certainly have secure encrypted authenticated messaging out of the SMS band.  This doesn't encrypt SMS.  It just adds, using metadata, it adds an authentication layer to it, which is another new feature for the Internet and for Google's messaging Android users.  Which I thought was sort of interesting.



So I originally had titled this "The Other Shoe Dropped," but I changed it to "Another Shoe Dropped" because I think we still have several more shoes to go.  While we were busy recording last week's Security Now! podcast, representatives from Apple and Facebook were testifying to a Senate Judiciary Committee hearing, attempting yet again to explain the value of encryption that has not been deliberately weakened.  In return, those on the committee informed Apple and Facebook, in increasingly less uncertain terms, that they had better put backdoors into their end-to-end encryption, or laws would be passed forcing tech companies to do so.  It's getting just more and more bear, or should I say "Barr."



The chairman of the Senate Judiciary Committee, our dear Senator Lindsey Graham said, and I quote:  "You're going to find a way to do this, or we're going to do this for you.  We are not" - this is still Lindsey talking.  "We're not going to live in a world" - and you know what's coming - "where a bunch of child abusers have a safe haven to practice their craft."  That's a craft?  All right.  "Period," he says.  "End of discussion."  And as anyone who's been following politics may know, Lindsey has been seeming a bit drunk on power lately.  But that's beside the point.



So this is the latest shot fired in the ongoing war over encryption.  The most recent salvos have been launched following the privacy manifesto that Zuckerberg published last March.  As we've been noting here, Zuck framed the company's new stance as a major strategy shift that involves developing a highly secure private communications platform based on Facebook's Messenger, Instagram, and WhatsApp services.  Facebook's stated plan is to leave the three chat services as standalone apps, but to merge their technical infrastructure so that users of different apps can talk cross-app to each other more easily.



And in digging into this a little bit more, I learned that that merging would include adding the WhatsApp Signal-based end-to-end encryption into Messenger and Instagram to make them compatible and similarly attack proof.  At the moment, Facebook's Messenger supports end-to-end encryption in the so-called "secure connections" mode, which is a mode that's off by default and must be enabled individually for every chat.  And Instagram has no end-to-end encryption on its chats at all.  So all of this stands to change once the three are merged into a single Signal-derived messaging triumvirate.



This past October, as we mentioned a couple months ago, the three governments - the U.K., the U.S., and Australia - explicitly warned Facebook that they had better end or at least pause that plan.  And in an open letter that we talked about at the time, calling on Facebook to back off of its so-called "encrypting everything" plan, U.S. Attorney General William Barr threatened them, essentially.



So Monday of last week, Facebook released an open letter responding to Bill Barr's letter.  In that letter, the WhatsApp and Messenger heads, Will Cathcart and Stan Chudnovsky, said that any backdoor access into Facebook's products created for law enforcement would weaken security and let in bad actors who would exploit the access.  They said:  "That's why Facebook has no intention of complying with Barr's request that the company make its products more accessible."



They said in their note:  "The 'backdoor' access you are demanding for law enforcement would be a gift to criminals, hackers, and repressive regimes, creating a way for them to enter our systems and leaving every person on our platforms more vulnerable to real-life harm.  People's private messages would be less secure, and the real winners would be anyone seeking to take advantage of that weakened security.  That is not something we are prepared to do."  And in his opening statement on the day that followed, last Tuesday, Lindsey Graham told Apple and Facebook representatives who he was facing in this committee meeting, "The fact that people cannot hack into my phone..."



LEO:  What a jerk.



STEVE:  He says he appreciates.  He says:  "I appreciate the fact that people cannot hack into my phone."  But he said:  "But encrypted devices and messaging create a" - and here it is again - "safe haven for criminals and child exploitation."



LEO:  B.S.  Oh, my god.



STEVE:  I know.  So I'm trying to not go on too long here and skip this.  In Facebook's letter, Cathcart and Chudnovsky pointed out that cybersecurity experts have repeatedly shown that weakening any part of an encrypted system means that it's weakened "for everyone, everywhere."  They said it's impossible to create a backdoor just for law enforcement that others wouldn't try to open.  Oh, and they said they're not alone in that belief.  More than 100 organizations, including the Center for Democracy and Technology and Privacy International, responded to Barr's letter to share their views on why creating backdoors jeopardizes everyone's safety.  Facebook's letter also quoted Bruce Schneier from comments he made earlier this year, saying:  "You have to make a choice.  Either everyone gets to spy, or no one gets to spy.  You can't have 'We get to spy; you don't.'"  He said:  "That's not the way the tech works."



And I'll say again for the record, because I know that my stance on this confuses some people, I don't want backdoors either.  And we have a real problem with jargon because "backdoor" is a heavily weighted word that everyone would agree is not good.  But too many lawmakers I'm seeing are taking, I mean, even Dianne Feinstein, who's a California Democrat, they're taking Lindsey Graham's and Bill Barr's position.  So if we're going to be forced to provide law enforcement with the equivalent of 21st-century lawfully warranted wiretaps, I would like those taps to be secure.  Which is to say, I would like the legislation to be carefully written and not written stupidly with a broad brush.



As we know, Apple already provides secure group chat with iMessage.  So it's utterly obviously possible for an additional silent listener to be added to Apple's existing iMessage chats.  The technology is already there.  It already exists.  And it's secure.  I'm not saying I want that to happen.  But saying that it's not possible is not correct.  Anywhere you have multiparty chat technology with the system and its keys managed by the provider, as they all are, it's obviously possible to add an additional listening party in a secure fashion under lawful warrant.  Companies don't want to do that.  I get that.  They would rather be able to take the stance that for their customers' sake, that you have absolute privacy.



But in the U.S., the right to privacy is not an absolute.  When a court has been convinced that a lawful suppression of an individual's or company's privacy serves the greater public good, privacy can be lawfully breached.  The math of cryptography empowers absolute privacy in a way that the U.S. constitutional framers could have never foreseen and did not intend.  I mean, it's not a protection that we have under the Constitution.



So anyway, of course our dear Attorney General was unable to resist marching out the kiddie porn during a Wall Street Journal event last Tuesday following these hearings.  He granted that, yes, there are benefits to encryption, such as to secure communications with a bank was the example Bill Barr used.  He says:  "A financial institution that will and can give investigators what they need when served with a warrant.  But he said that the growth of consumer apps with, as he put it, warrant-repellent end-to-end encryption like WhatsApp and Signal have aided terrorist organizations, drug cartels, child molesting rings, and kiddie porn type rings."



So anyway, unfortunately, whereas the U.S. Congress currently and otherwise seems to be more divided than it's been in quite some time over political policy matters, in this they are much more united.  And so I just think it's - I think 2020 is the probably the year that we're going to see some laws created, and I just hope they're good ones because U.S. operating companies are subject to the laws of the U.S., and other countries are not any more happy about all of this than we are, than our law enforcement is.  So interesting times.



Apple's iOS v13.3 last week added support for the first time for hardware key dongle authentication to Safari.  With last week's update to v13.3, Safari on our iDevices obtained access to FIDO2-compatible authentication hardware such as Yubico's YubiKeys or Google's Titan, where they have the proper interface hardware.  All three hardware modes can be used:  NFC, USB, and Lightning.



So after the update to v13.3, users who have proper hardware can authenticate by using, for example, the YubiKey 5 NFC or Security Key NFC by tapping on the top of an iPhone from the iPhone 7 on.  You can also do physical authentication, for example, with the YubiKey 5Ci by plugging it into the Lightning port of an iPhone or iPad.  So that's a cool addition for those who are looking to get that kind of hardware dongle-based protection from their iDevices.



And last Tuesday turned out to be an important Tuesday to patch because it foreclosed upon an elevation of privilege vulnerability in Windows that was seeing widespread and active exploitation in the wild.  It's now been patched in Windows, and it is a vulnerability that had been patched earlier in Chrome because it used a problem in Chrome, coupled with a problem in Windows.  It needed them both in order to get exploited.  Last week's December Patch Tuesday fixed a total of 36 vulnerabilities.  Seven were critical, 27 were important, and one was moderate.  And that left one that was low in severity.



The important one to patch turned out to be the one rated as one of those that was important.  It was another flaw, and we've had a bunch of those this year, in the Win32k module - which, once again, also again because a lot of them have been this, enabled privilege escalation.  When used with Chrome, it facilitated an escape from Chrome's sandbox, which is never a good thing to have happen.  Google had addressed their side of the flaw in Chrome 78.0.3904.87, which was pushed out as an emergency update last month after Kaspersky disclosed it to them and to Apple.  However, hackers were still targeting that flaw for any Chrome browsers that had not been updated.



And that's interesting.  I don't know why they wouldn't be, although I have seen Chrome be a little lazy with updating as iOS often is for me.  So I guess there was still a window of opportunity, and the benefit from achieving this exploit was great enough, and Chrome is now the most used browser on the Internet, so it was still being attacked.  Anyway, it was a use-after-free exploit that was chained together with the now-patched exploit in Win32k component of Windows OS, which was handling objects in memory.  And it's been patched.



Okay, everybody.  If you have propeller cap beanies...



LEO:  Beanies on.



STEVE:  Get them out and spin up your prop.



LEO:  Uh-oh.



STEVE:  We know that an RSA private key is just a very large random prime number.  It's hidden inside its matching RSA public key by multiplying it with another very large prime.  And the private key is able to remain hidden because none of this planet's best math magicians have ever been able to figure out any means for breaking those two primes apart once they've been multiplied.  But of course, if either of those primes did not have sufficient entropy, if either of them could be guessed, the hidden private key inside would be discoverable.  It turns out that's kind of obvious.



But there's a more powerful weakness.  It's been known for a while, but there hasn't been a survey done until recently.  Last Saturday, during the first IEEE Conference on Trust, Privacy, and Security in Intelligent Systems and Applications, which was held down here in Southern California, in Los Angeles, a team of researchers from Keyfactor - interesting name - presented their findings into the current security posture of digital certificates on the Internet.  Their paper carried the chilling title:  "Factoring RSA Keys in the IoT Era."  Okay.  "Factoring RSA Keys in the IoT Era."



LEO:  Mm-hmm.



STEVE:  So I'll share more about the details in a second.  The super short version of their findings is that the certificates being generated by relatively entropy-starved IoT devices turn out to be lacking.  The devices lack super high-quality random number generators, and they are revealing weekly randomized primes, which leads to a failure of the fundamental guarantee offered by RSA public key crypto, which asserts you can't factor this.



LEO:  You can't factor this. 



STEVE:  You can't factor this.  Exactly.  As a result of the widespread deployment of IoT devices today, these Keyfactor researchers claim that one in every 172 RSA certificates in active use today is vulnerable to attack.



LEO:  One in 20, that's a lot.



STEVE:  No, one in 172.



LEO:  Ooh, wow.



STEVE:  But even that, one in 172.  Okay.  So here's the abstract from their paper:  "RSA keys are at risk of compromise when using improper random number generation."  Okay, that's not a surprise to anybody.



LEO:  Yeah.



STEVE:  "Many weak keys can be effectively discovered and subsequently compromised by finding reused prime factors in a large dataset."  And I'll get into the details of that in a second.  That's where our propeller beanies come in.



In this paper they said:  "We collect and analyze 75 million RSA certificates from the Internet, and find that one in 172 certificates have keys that share a factor with another.  In contrast, only five of 100 million certificates found in a sample from Certificate Transparency logs are compromised by the same technique."



And I should pause here just to note that - so what they're saying is that, when they looked at all certificates on the Internet, one in 172 were found to have a prime factor that collided with another certificate somewhere on the Internet.  But if they looked at 100 million certificates found in the Certificate Transparency logs, only five of 100 million certificates had a collision.  The point here is that Certificate Transparency logs are published by Certificate Authorities, so their primes are being generated, not by IoT devices, but by high-quality random number generators.  So those are of higher quality is the point.



So they said:  "The discrepancy in rates of compromise is overwhelmingly due to IoT devices exposed to the Internet, which may be subject to design constraints and limited entropy.  The widespread susceptibility of these IoT devices poses a potential risk to the public due to their presence in sensitive settings."  Yeah, like all the IoT crap that we have in our houses these days.  They said:  "We conclude that device manufacturers must ensure their devices have access to sufficient entropy and adhere to best practices in cryptography to protect consumers."



Okay.  So what exactly is going on?  What I learned, and this is really interesting, which is why our listeners are going to find it interesting, too, I believe, it turns out that there's sort of a weakness in our cherished and beloved RSA public key system.  The result of this weakness - and Leo, you're going to love this from a math standpoint, too.  The result of this weakness is that the entire system, when taken at Internet scale, is exquisitely sensitive to the quality of every single prime in use, and that in turn makes RSA somewhat vulnerable and brittle.



So here's how these guys describe the attack.  And I've added a few words here and there to clarify from their - I've added a little editorial to clarify things.  So they said:  "RSA is used in the process of encrypting data to send across a network.  The server transmits its RSA public key to the client as a part of an SSL or TLS handshake."  We know all that.  "Part of the RSA public key contains the modulus "n = p * q," where p and q are two randomly chosen primes of similar size.  Primes p and q are kept secret, as knowing these values allows the private key to be calculated."  Right?  So p and q are primes, and they are multiplied to create this modulus n.  And the whole point is you can't factor that.



They said:  "Ensuring that p and q are selected with sufficient randomness" - that is, that are sufficiently random primes - "is a crucial component of keeping the public key secure."  Now, again, by that they mean the public key is public, but it carries the private key.  So when they say "Keeping the public key secure," they mean actually keeping the embedded private key which is in the public key secure.  So they said:  "Factoring a large modulus n to obtain p and q is not feasible under normal circumstances.  However, if keys are generated with poor randomness, then it becomes a concern that two public keys anywhere on the Internet may share a prime factor once enough keys are generated."



So here's the counterintuitive loophole bit of RSA.  If two RSA moduli, for example n1 and n2, in two different public keys, so if two of those where the p and q separately obtained primes, if they share precisely one prime factor p, then computing the Greatest Common Divisor (GCD) of those two public keys n1 and n2 will reveal the value of p.  So again, you have two certificates containing public keys on the Internet.  If by chance they share a random prime, then it is possible, it turns out, relatively easily to compute their Greatest Common Divisor.  And their Greatest Common Divisor will be the prime that they share.



Then they said:  "The GCD computation is significantly easier than straightforward factoring" - which of course is, as we know, effectively impossible - "and can easily be performed in practice.  The other factors of n1 and n2 can then be trivially found by the simple calculation of n1 over p" - that would return q for that certificate - "and n2 over p" - that would return q for the second certificate.  And they said:  "Respectively, fully compromising both keys.  This GCD computation can be scaled to analyze all pairs of keys in sub-quadratic time," meaning that it doesn't explode with the number of keys, sub-quadratic time in the number of keys.



They said:  "Selecting the prime factors of appropriate size and uniform randomness should prevent two moduli from ever sharing a factor in practice.  However, if there is a flaw in the random number generation when choosing primes, a collision is likely with a sufficiently large dataset.  Attackers can use this knowledge to collect a large number of RSA public keys and then look for Greatest Common Divisors between their moduli to search for factors shared by any pair.  And when found, both keys are rendered insecure.  The private key component hidden in the public keys is completely revealed."



So what this means, there's a great many of the public keys circulating around the Internet are gathered into a massive dataset.  This is no longer difficult today.  It was.  It was impossible when RSA was invented.  Today we've got cloud everything:  massive storage, massive computation.  So you gather the huge dataset of public keys that are themselves not vulnerable.  You then compute the Greatest Common Divisor against each pair of keys.  And that's, of course, if the number of keys is n, then there are n times n-1 pairs of keys.  So it's a very big number.  But these days we deal with very big data all the time.



And in fact, these guys, I think they used an AWS, some AWS resources, and they did that.  They took 75 million public keys that they had gathered, and they ran this GCD operation which has been now made very efficient against every combination of two keys in the dataset.  And that's where they found that surprisingly high collision rate.  Among high-quality keys generated with good entropy, it was only five in 100 million.  But in the keys actually in use on the Internet, thanks to all the IoT devices we now have, that collision rate rose to the high level of one out of every 172 pairs.



So suddenly that makes attacking the keys being produced by IoT devices essentially practical.  You'd have to hope that a particular device that had synthesized its key was in the dataset, but this turns this into a potentially practical attack because these IoT devices are just not generating high-entropy primes for the certificates that they're making for themselves.  So I just thought that was incredibly cool.  And of course it further enforces our use of the term "IoT" standing for Installation of Trojans.  



LEO:  So it's because IoT devices don't have much processor power; right?



STEVE:  Yeah.  Although it's also because I don't think this problem was sufficiently well appreciated.



LEO:  Anticipated, yeah.



STEVE:  Yes, anticipated.  So, for example, nothing would prevent a camera that you're installing from establishing a secure link to the mothership and asking it for...



LEO:  Give me a key, yeah.



STEVE:  Yeah, exactly.  Give me a certificate.  But they don't do that now.  They go, eh, we got enough entropy here.  We'll make our own, and we'll just assert our own certificate.  They shouldn't do that.  They should go somewhere and get a good one.  And probably, if this research gets enough air and becomes enough well known, then all of our Alexas and our Echoes and our everythings, all of these higher end devices could certainly, I mean, they're able to create secure connections back to the mothership.  So they ought to just ask for a private key and start using it, or ask for a certificate, rather, a certificate and a private key.  They would need both.



Okay.  Some quick bits here.  Friday the 13th came true, unfortunately, for New Orleans, which was hit by a ransomware attack.  I know.  The extent of the attack was not known at the time of its announcement which - get this, Leo - was announced over the loudspeaker system in City Hall.



LEO:  Turn off your computer.  Turn...



STEVE:  Step away from the keyboard.



LEO:  They even said to unplug it.



STEVE:  It did, yes.  It told all government workers to immediately turn off their computers and unplug them.  City websites were down.  A spokesman for the mayor said the attack started sometime after 11:00 in the morning on Friday morning.  New Orleans activated their Emergency Operations Center and contacted officials from the Louisiana State Police, the FBI, the state National Guard, and the Secret Service for assistance, according to a tweet from the city's Department of Homeland Security and Emergency Preparedness.



So now we're living in a world where you might receive emergency orders to immediately turn off and unplug your computer over the  public address system.  Amazing.



LEO:  Did we ever find out really, though, what had happened?  Because they said we didn't lose any data.  We weren't compromised.  It felt like there was more to this story.  In other words, there was no encryption, as far as I know.



STEVE:  Oh, so I haven't seen any follow-up.



LEO:  Yeah, there has been a follow-up.



STEVE:  Maybe it was just a cyber attack, and they said "oh crap" and shut everything down to be careful.



LEO:  What it sounded like is that they were being bombarded with spear phishing attacks.



STEVE:  Oh, my lord.



LEO:  And rather than take the chance that somebody would open one of those emails, they just said everybody shut your computer off now.



STEVE:  Right. 



LEO:  And then they went all around, and they looked to see if they could find anything, and they couldn't.



STEVE:  That's actually kind of smart.



LEO:  They said, based on what we were told by federal law enforcement...



STEVE:  This is what we should do.



LEO:  This is what we should do, yeah.



STEVE:  Wow.



LEO:  So as far as I can tell, they lost nothing, and nothing was encrypted.  But it was just shut down for that period of time because...



STEVE:  Well, and it's a nice holiday message for all of the...



LEO:  Yes, every city in the U.S.



STEVE:  Yeah, and even for their employees.  I mean, you're going to start taking it seriously if this happens, and then somehow there's like a meeting of like, okay, we did this because all kinds of email was coming in, and we couldn't take the risk that any of you boneheads would say, oh, what did my Aunt Mary send me?



LEO:  Exactly.  They had just had training in what not to do, how not to get phished.  So it might well be that they avoided - this was actually the right way to handle it.



STEVE:  Yeah, yeah.  Another little quick bit.  Mozilla - I thought this was interesting - will be requiring all Firefox add-on developers to sign into their accounts using two-factor authentication next year.



LEO:  Good.



STEVE:  Yeah.  Their goal is to help prevent supply chain attacks where malefactors compromise an add-on authors authentication in order to inject bogus malware into an otherwise popular and believed to be safe add-on.  So I think that's all for the best.  Also, the primary reason I would have considered obtaining Windows 7 extended security updates - if I had been eligible, which I'm not, being a corporation, or not being a big enterprise - would have been for the continuing updates to Microsoft's built-in Security Essentials AV.



But interestingly, as it turns out, everyone is going to be cut off starting January 15th, even corporations who opt to purchase the extended security updates.  Microsoft is not going to be providing them.  Which I just don't understand that.  I guess, I mean, they have to do the research for Windows 10.  But maybe it's additional incentive to move people off of Windows 7?  Because, again, it's nice to have Defender or Security Essentials, which is what I've got running in my Win7 machine, protecting you, you know, keeping an eye on things.



This seems nutso to me because it will drive corporate users who have decided they want to keep Windows 7 to adopt third-party AV at the enterprise level.  It's going to be a windfall for AV publishers.  But I verified Microsoft intends to say, nope, sorry, we're not going to offer any security updates, even for people, that is, any of our AV Security Essentials updates, even for people, like enterprise customers, who do opt to continue having support for updates.  Crazy.  I don't get it.



With the release of Chrome 79, the option to force the browser, Chrome, to continue displaying what Google has formally stated they consider to be trivial - that is their word, "trivial" - the leading www appearing in URLs such as www.grc.com has been removed.  The www is gone for good.



LEO:  Hallelujah.



STEVE:  And is not coming back in Chrome.  Okay.  I mean, I do think I made a mistake years ago.  There was a big discussion in the newsgroups, in GRC's newsgroups, about whether I should standardize on www or not.  The problem was I allowed either to be used, and the way you came in is the way you stayed.  That is, I wasn't pushing anyone over one way or the other.  The problem was that Google was then duplicating all the links.  And  so I wasn't getting the benefit of the traffic aggregation under one URL.  They were being spread out.  Many were under www.grc.com, and others were under grc.com.



And so I thought, okay, I've got to make a decision here.  Do I want to redirect everybody who does www.grc.com over to grc.com?  Or people who come in as grc.com over to www.grc.com?  I needed SSL certs either way because in order to do a redirect you need to first have a connection.  So it wasn't a matter of saving money on TLS certs.  It was coalescing all of Google's links into a single agreement.  And I made the wrong decision.  I said, well, you know, www is what web browsers are for.  That's the World Wide Web.



LEO:  Right, right.



STEVE:  So that's what we should do.  And I'll take some pleasure in the fact that Amazon is the same way.  It's www.amazon.com, not amazon.com.



LEO:  Right.



STEVE:  But lots of other people have said, huh?  That's dumb.



LEO:  In the early days, you had to distinguish web traffic from other kinds of traffic, and servers, web servers from other kinds of servers.  So that preceding www told you this is a web server you're contacting.  But you could be...



STEVE:  Do you really have to?



LEO:  You don't anymore.  No, no, no, it was a convention.  It was a convention.  I don't think it was required.



STEVE:  That was before my time.



LEO:  Yeah.



STEVE:  Because we used the port numbers to perform that.



LEO:  Yeah.  Since you open up the conversation, you know.  But by convention.



STEVE:  Like port 80 is nonsecure, and 443 is TLS.



LEO:  Right, yeah.  I think it was more so for humans.  So but that convention, even that convention is useless at this point.  I mean, who - I kind of don't like browsers, though, that take the full URL out, as Safari does.



STEVE:  I agree.



LEO:  I want to see where I am.



STEVE:  And the problem, really, Leo, is magnified by the fact that w, that's a long - in English, w.  It's like, www.



LEO:  It's nine syllables for no reason.



STEVE:  Is anyone still listening?



LEO:  Yeah.  No, that's - way back in Tech TV days, this is how long ago, 1998, I insisted that we have the convention that, when we published web URLs, we didn't do and we didn't say w.  They were still saying https://.  So I said we don't say that, and we don't say dub dub dub.  It's assumed.  The problem is, if you haven't configured your DNS properly, on some sites, if you don't enter the dub dub dub, you don't get the full site.  So but that's just a configuration mistake, I think.  It's not...



STEVE:  And I think the other really interesting thing that's going to happen is when - it'll be interesting to see when browsers start assuming https.



LEO:  Right.



STEVE:  They're still not doing that.



LEO:  Right.



STEVE:  And so my front gate bounces people not only from grc.com, now I wish it didn't, but over to www dot are you still awake dot grc.com.  



LEO:  Https://.



STEVE:  And, yes, also moves them from http over to https in the process.  So, agh.  Anyway, but as you said, I mean, when Google first did this, they took a lot of flack.  And so they first backed off.  Then they gave us a means of overriding that for us old sticklers.  And now in Chrome 97 the override is gone.  It's like, okay, just suck it up.  Get used to it, that www is history.  And I kind of wonder maybe if I ought to just change my redirect and train Google over to grc.com because this ship seems to have sailed, or in my case sunk.



Also, speaking of Chrome 79, as planned, Google reenabled with Chrome 79 their browser's new code integrity protection feature.  But not as planned, the unrecoverable "Aw snap" browser crashes immediately resumed as before.



LEO:  Oh.



STEVE:  The culprits are many.  But they include CylancePROTECT; even updated versions of Symantec Endpoint Protection, again; Webroot security solution; and others.  So it is possible, if you are affected by this, your Chrome just won't work.  It's possible to force the feature off when necessary by launching Chrome 79 with the switch.  It's --disable-features=RendererCodeIntegrity on the command line.



I don't know what Google's going to do about this.  I think at this point they're probably just going to tell people, well, in fact I did see some of their what-to-do, and it's like, tough.  Fix your AV.  Because what's happening is bogus AVs are reaching in and trying to muck around with Chrome.  And by Chrome turning this on, they're preventing anybody who doesn't have something signed by Microsoft from loading stuff into Chrome or Google or Microsoft, any third parties, from sticking their fingers into Chrome.  Which is, I think, correct.  So unfortunately, companies should not do that.



A little bit of clarification from our topic last week, the "VPN-geddon Denied."  Somebody posted in the - well, I know who somebody, his name is Grant Taylor - over in the Security Now! newsgroup.  In response to my discussion, he said - and he is a heavy-duty Linux guy.  He said:  "The VPN in any form is a path for the traffic to the victim to take."  He said:  "It happens to be an encrypted path.  But it's not the only path.  The same traffic can be sent unencrypted" - or other traffic, some traffic, any traffic - "can be send unencrypted to the LAN interface's MAC address destined to the VPN interface's IP address."



He said:  "By default, many operating systems will accept the traffic on the wrong interface and hand it to the TCP/IP stack, which will hand it off to applications.  The traffic does not need to come into the system through the encrypted VPN."  And he's right.  I didn't make that as clear as I should have.



I replied to Grant:  Right.  The virtual interface is a full interface inasmuch as it can receive data from other parties with local access to the network.  I should have made that more clear since, as we know, that's the only way the attacker, who lacks the VPN's working encryption key, could possibly have injected, as I was talking about last week, SYNs or SYN-ACKs with test sequence numbers.  They would need to come in unencrypted and hit the interface for it to respond.  Otherwise there's no way to synthesize test SYNs.  



And now, Leo, my big announcement.



LEO:  Oh, I'm ready.  I'm listening.



STEVE:  My change of life announcement.



LEO:  Are you getting married?



STEVE:  No.



LEO:  Oh.  You're too old for that.



STEVE:  It's bigger than that.



LEO:  It's bigger than that?



STEVE:  Bigger than that.  Posted to the grc.sqrl newsgroup.  The subject line, this was posted yesterday, Monday, December 16th:  "Turning SQRL over to you guys to tend."



LEO:  Ah, nice.



STEVE:  It was message number - get this - 23,606.



LEO:  Wow.



STEVE:  In the SQRL newsgroup.  That's how much traffic there was over the last six years.



LEO:  Wow.



STEVE:  And the message reads:  "Everyone.  That was a momentous subject line to write, but I believe it's finally time to do just that.  SpinRite is now the more urgent need.



"As you saw over this past weekend, I made an intense push to wrap things up with SQRL, and I did.  The published SQRL docs reflect the local Word doc copies I maintain, and nearly 41,000 copies of the 'SQRL Explained' PDF have been downloaded.  As we know, anyone who reads that will deeply understand SQRL, just like the Google cloud security guys did.  The sqrl.grc.com server is also updated with the latest functional code, so it's back up to spec.



"I have a list of tweaks for GRC's SQRL client for Windows, but thanks to everyone's deep pre-release testing, the release number one client doesn't have anything wrong that needs fixing right away.  Though some UI bits can be improved, I think that's probably always going to be true, and I do not want to delay my switch to SpinRite any longer.



"One thing I absolutely know because I've been watching carefully, and I've seen it for myself for some time now, is that I'm leaving SQRL in the hands of an extremely competent and capable group of developers who understand it every bit as well as I do.  And GitHub has exploded with a wonderful array of SQRL-related clients, servers, and middleware.  SQRL no longer needs me.  SpinRite does.



"Over the next week or two I'll be spending some time at Level 3 maintaining the Windows servers that have been neglected for far too long.  And I'll deploy a new Unix server to replace the aging hardware that has been hosting these newsgroups and our DNS from the start.  Then I'll switch over to our spinrite.dev newsgroup here and return to generating test releases of new low-level SpinRite code as we develop the technology that v6.1 will need, and work to bring SpinRite fully back to life."  So that day has come.



LEO:  I think SQRL is done, basically; right?  That's what you're saying.



STEVE:  It's been done for quite a while.



LEO:  It's out of your hands.



STEVE:  Yes, it is.  And speaking of which, after that posting, there was another posting that appeared in the GRC newsgroup from a Jose C. Gomez.  His subject was "OAuth 2 Provider for SQRL."  And he wrote:  "Hi, all.  Over the past couple of weeks I've been working on a functioning OAuth 2 provider that works exclusively with SQRL.  This should, in my opinion, allow millions of sites, if they choose to, to adopt SQRL without having to change much on the backend.  I am finally in a pre-alpha release stage and wanted to share it with everyone here and get some input and thoughts.  Following the SQRL motto, I've made it so you can remain pretty anonymous and still use the service.  And of course there are really no secrets to keep."



He said:  "It currently implements the basic Authorization Code grant flow and works fairly well.  I'm planning on releasing it in beta sometime this week to let whomever wants to try it, play with it."  He says:  "But I run a Discourse forum like Leo, so I've made sure it will work with Discourse out of the box so the community at TWiT should be able to start using it, if Leo chooses to, pretty easily."



LEO:  Thank you, Jose.  I will.  Yeah, good.  That's great.



STEVE:  He says:  "Anyway, here's a quick demo of it in my Discourse instance."  He said:  "Again, this is still in alpha/pre-alpha.  So if you go poking around, things may blow up, LOL.  But feel free to."  He says - oh.  He says:  "It uses the Ask facility" - and people who know SQRL may remember that Ask is the one thing I built in which isn't about authentication, but it allows the server the SQRL client is authenticating to, to send you an out-of-browser, that is to say, out-of-band question, which is more secure than anything that could be done in the browser.  And I didn't know why it would be necessary, but it was the sort of thing where I thought, well, you know, if it's not there, it'll never be there.  And if it is there from the beginning, maybe somebody will have a use for it.



Well, it turns out he's using it.  So Jose said:  "It uses the Ask facility to act as the Permissions Granting Screen of OAuth."  He said:  "I thought it was a pretty neat way of putting the entire permissions structure in SQRL.  We also have the ability, if we want to" - and note that normally what you get with OAuth is the site you go to, you need to interact with the site in order for it to ask you if you want to grant the site you are coming from permissions under its authentication.  The point is you don't have to do that.  SQRL solves that problem also, thanks to the Ask facility.  So it's all kept within SQRL, making actually the whole process much more transparent.



Anyway, he said:  "I have to give a big thanks to TechLiam and Jeff Arthur, who have been my sounding board over in Slack" - okay, we also have a Slack channel - "while I slugged through the protocols and fought with the specs.  Also, a zillion thanks to Paul F., who let me use some of his tools like SQRLView and his command line SQRLClient for troubleshooting."  Like I've been saying, I mean, there's been an explosion of this stuff.  So there's just like we're rapidly developing a rich ecosystem of SQRL stuff.  And then he wrote:  "Seriously, SQRLView is an amazing piece of software, and it should be shouted from the rooftops for anyone writing or dealing with SQRL.  Liam's .NET Core middleware is also a great piece of open source engineering, and it keeps getting better. Cheers, guys, and thanks again.  I look forward to some feedback."



And then in the show notes I have a - he posted a link to a GIF which shows - it just shows a few quick pictures of his SQRL authentication in process.  And he has sqrloauth.com is the site, S-Q-R-L-O-A-U-T-H dot com.  So anyway, that served as like a perfect punctuation on my announcement that I am switching to SpinRite because SQRL is done, and it is in good hands now.  So a huge thanks to everybody who has been interested, to all the developers who have jumped in and helped to bring this alive.  And we know that it is coming to the attention of some big-time players.  There's a lot going on behind the scenes that are at the whisper level at this point.  But we've given it every possible chance of success.



LEO:  I presume somewhere he has the secret keys and all that stuff for what I need to do.



STEVE:  Yeah.  And in fact I asked him if he wanted to have a forum over on sqrl.grc.com, and he immediately jumped on it.  So I did that this morning.  So if you go to sqrl.grc.com, you'll find that he's got his own forum, of which he's a moderator.  So he'll be managing that, and he'll be able to post links and keys and announcements and so forth.  So Leo, you could probably aim your webmaster or your web guy over there.



LEO:  Oh, I'm the web guy.



STEVE:  Oh, cool, cool.



LEO:  But I'll have - yeah, Jose, if you want to email leo@leoville.com because I'll need an account, obviously, on his OAuth server to do this.  But our Discourse already has the OAuth 2 plugin.  I just need a custom server.  And looks like I need a few - I have a client ID.  I need a client secret authorization URL, token URL, that kind of thing.



STEVE:  Cool.



LEO:  Callback ID, callback path, all of that stuff.  But he can help me out with - I don't know exactly what his implementation involves.  I'd need an account on his server, I think.  But Jose, I'm listening.  And I will do it the minute I get the information.  We'll set it up on my Discourse.  That'll be awesome.  Well, our users would love it.  I mean, they've been wanting that for a while.  Good.  All right.  Let's talk about Plunderbuss.  Whatever it's called.



STEVE:  Okay.  So Intel must just be so tired of having their once-believed solid chips just hacked and hewed and...



LEO:  Is AMD subject to this, too?  I mean, they use speculative execution, but I never hear that it is.  



STEVE:  I don't know why researchers don't go after AMD as much as Intel.  I guess market share or accessibility.



LEO:  The early stuff, you know, the Spectre and the - they were to some degree.  My sense is the latest stuff, no.



STEVE:  Well, get a load of this one.  Here's what their abstract reads:  "Dynamic frequency and voltage scaling features have been introduced to manage ever-growing heat and power consumption in modern processors.  Design restrictions ensure frequency and voltage are adjusted as a pair, based on the current load, because for each frequency there is only a certain voltage range where the processor can operate correctly.  For this purpose, many processors, including the widespread Intel Core series, expose privileged software interfaces to dynamically regulate processor frequency and operating voltage."



Okay.  So essentially what that means is that die sizes are getting bigger.  We know how much power these systems are using in order to create the processing power they have.  Whereas, you know, we have people with water-cooled GPUs and things.  The point is that there is power being burned.  So you can reduce power consumption if you switch the voltage between zero and one through a lower change because it actually - essentially there's distributed capacitance throughout all of this silicon.  And so in order to move something up to a different voltage, to switch it from a zero to a one, you need to essentially fill the capacitance which is present with some number of electrons.  And they're continually making those capacitances smaller, but they still exist.  So if the voltage for a one is less, then you need fewer electrons to be squirted in there in order to bring it to a one because of its lower capacity for holding electrons.



So one of the things that you do is you lower the voltage as much as you can so that less current has to be pushed and pulled in order to go through those voltage excursions.  And then of course you also run the chip as slow as possible when it's not needing to be working hard so that you are pushing the current through those excursions less often.  So you control those two things.  The problem is, if you want to run faster, you also need to provide more voltage because voltage is pressure.  And so that's why it's necessary for speed and voltage to be moved in concert.  And so the Intel chips do that.  Anybody who's messed with tweaking voltages and speed in their BIOS knows they have, like, way too much power and way too much control over that stuff.  You can really run your hardware right at the - literally at the leaking edge.



Okay.  So they said:  "In this paper, we demonstrate that these privileged interfaces can be" - get this.  These privileged interfaces, that is, the previous paragraph ended with "Intel Core series expose privileged software interfaces to dynamically regulate processor frequency and operating voltage.  In this paper, we demonstrate that these privileged interfaces can be reliably exploited to undermine the system's security.  We present the Plundervolt attack" - "volt" now we know as in voltage, and "plunder" as in, you know, give us your women - "in which a privileged software adversary abuses an undocumented Intel Core voltage scaling interface to corrupt the integrity of Intel SGX enclave computations."



They said:  "Plundervolt carefully controls the processor's supply voltage during an enclave computation, inducing predictable faults within the processor package.  Consequently, even Intel SGX's memory encryption and authentication technology cannot protect against Plundervolt.  In multiple case studies, we show how the induced faults in enclave computations can be leveraged in real-world attacks to recover keys from cryptographic algorithms, including the AES-NI" - that's the New Instructions - "instruction set extension or to induce memory safety vulnerabilities into bug-free enclave code.  We finally discuss why mitigating Plundervolt is not trivial, requiring trusted computing base recovery through microcode updates or hardware changes."



So, okay.  So this is not a processor working the way Intel intended it, which was of course all of the last two years of speculative execution exploit.  This is deliberately making the processor go out of spec, making it fail by sneaking the voltage down until it fails, and then turning that failure into an exploit, believe it or not.  It's like, okay.  Is anything possible?  Apparently.



Okay.  So what's cool is a couple things.  First of all, this affects Intel 6th, 7th, 8th, 9th, and 10th Generation Core Processors, also the Xeon E3 v5 and v6, and the Xeon E-2100 and E-2200 Families.  So that's everything from Skylake on is affected by this.  In the show notes I have a picture of the thing that they reverse-engineered, this undocumented undervolting model-specific register.  These are registers where - this is where things like the number of cycles or the number of branches not taken versus the number of branches taken, that's where all of that extra microcode-level wisdom accrues.



So a register in there, it's a 64-bit register, and it's got three bits that they call the "plane index" that's 40, 41, and 42.  And they allow you to select which plane, as their term is, like which area of the processor core you want to tweak the voltage.  Index 1 is the CPU core itself.  Index 2 is the GPU, which you can separately control.  Index 2 is the cache in the core.  Index 3 is the cache out of the core.  And 4 is the analog I/O subsystem, if any.  And then lower down in there is an 11-bit signed voltage offset.  So it's 10 bits plus sign, which are in units of 1/1024 of a volt.  So that's essentially a millivolt.  So basically this allows you to push up or down by one volt in essentially one, just slightly less than one-millivolt steps the supply voltage individually of any of those cores.



So the way they find, the way they got into this, their original proof of concept is a cool little bit of code that I also have here in the show notes.  What I'm showing here in the show notes is a little bit of C.  They have a 64-bit multiplier which is just 1122334455667788.  And then they have a variable, which they define as the hex deadbeef, D-E-A-D-B-E-E-F in hex, times the multiplier.  Okay, so that's going to create the initial value for var.  Then they have, while var equals that same expression, the deadbeef hex times the multiplier, while that's true, then they have it recompute var in the same way.  They set var equal to deadbeef, then var * = multiplier, which will multiply var by the multiplier.



So this is an infinite loop; right?  Because you are testing var as the product of deadbeef times multiplier.  Then you're reperforming the multiplication, setting var again to deadbeef times multiplier in a while loop controlled by that original test.  It's going to run forever, never going to come out of that loop, unless the multiply fails.  If the multiply produces the wrong result, we drop out of the loop.  And then var is XORed with the proper value of deadbeef times multiplier, which turns var into which bit or bits were incorrect in the final in-loop multiplication.



So they said:  "This code is placed into the secure enclave and clearly should never terminate.  But our experiments revealed that undervolting the CPU just before switching into the enclave leads to a bit-flip in var" - I know, Leo, it's unbelievable - "typically in byte 3, counting from the least significant byte as byte 0.  This causes the enclave program to terminate.  The  code outputs the XOR of the erroneous value with the expected value, to highlight only the faulty bit or bits.  And they consistently observe that in this specific configuration the output is always 0x04 00 00 00."  So it's the four bit, the hex four bit in the fourth byte from the bottom is the one that  gets set when they sneak the voltage down.  That's where the multiply fails.  And as we have so often seen, what starts off as a benign but unexpected fault in a computer system can often eventually be developed into a full working exploit.



LEO:  This is almost certainly Intel-specific.  There's no way this is going to be the exploit.  This is Intel's mistake.



STEVE:  But what's bizarre is it's across their whole processor family.



LEO:  Yeah.



STEVE:  Probably because they reuse the same silicon for that.  And they've successfully done that here.  Their page goes on to great depth showing how they managed to leverage their undervoltage tweaks into a complete breach of the sequestered processing, which is supposed to be hidden inside Intel's secure enclave.



The result of this is Intel's statement:  "When SGX is enabled on a system, a privileged user may be able to mount an attack through the control of CPU voltage settings with the potential to impact the confidentiality and integrity of software assets.  Intel has worked with system vendors to develop a microcode update that mitigates the issue by locking voltage to the default settings."  So basically this was something Intel never imagined could be abused, or was hidden because it was undocumented, and nobody would ever find it.



Well, these guys did, and they figured out how to turn it into essentially an export of secret keys, of private keys from the secure enclave.  And a lesson we've learned during the past two years, since the tip of the iceberg which was known as Spectre and Meltdown, is that choosing a PC vendor who is actively keeping their past product offerings current with a flow of BIOS updates containing the microcode patches now being produced by Intel on a relatively ongoing basis, is one more factor to consider when selecting your vendor.



I would say, you know, I would have never worried.  I didn't worry 10 years ago.  Well, three years ago, actually, about whether the vendor I bought my motherboards from was producing BIOS updates.  It's like, yeah, the BIOS is the BIOS.  Who cares?  As long as it boots the OS, we're not using it once it gets booted.  Well, it turns out BIOS updates are important because they are updating the microcode on the fly, as we know.  And in so doing, they're fixing problems.  So anyway, I just thought that was so cool, that sneaking the voltage down to the point where the processor starts to fail, and it fails in a way that's predictable, and that predictability allows that fault to be turned into an exploit.  Wow.



LEO:  It said "privileged user."  What would that take, to be a privileged user?  Who is the user?



STEVE:  Yes, that's a very good point.  And that's one of the mitigating factors.  You have to have access to those model-specific registers, and that requires kernel-level access.  So ring 3 you cannot access those registers.



LEO:  The user app can't do this.



STEVE:  Yeah.



LEO:  Maybe an antivirus could, though, and they usually have ring 0.



STEVE:  An AV could.  And of course things get down into the kernel, unfortunately, all the time.



LEO:  Right.



STEVE:  Those Win32k elevation of privilege, those all get you down into ring 0.  So it's not like that's impossible to get to.  There was something I wrote years ago.  It was that thing that everyone was using to figure out if they had 64-bit capability.  It was not intended for that, but it was incredibly popular.  In fact, it still is.  It's like people use it all the time.  It's not coming to mind.  But I had to write a separate DLL that ran in the kernel with kernel privileges in order to query some of the model-specific registers in order to determine what was going on.  So again, somebody running at ring 3, you don't have access to those.  You need to get down at ring 0 and have the required privilege because you could do great damage.



LEO:  Yeah.  Microsoft's been trying very hard to keep people out of ring 0.  But the problem is it breaks stuff.  And I think they tried in Windows, I want to say in Windows 7, didn't they say, okay, no more kernel extensions?  No more kernel access?  But then everybody complains, and they say, well, we have to have it.  



STEVE:  Yup.  And the big hassle, I've hit this recently, is  now all drivers have to be signed.  And in Windows 10 you have to go through a really annoying, I mean, basically you upload your driver to Microsoft because your driver has to carry their signature.



LEO:  Well, but that's how you protect the kernel.



STEVE:  That's right.  That's right.



LEO:  Securable is the name of the program.



STEVE:  Oh, Securable.  You're right.



LEO:  Bill in Michigan knew it.



STEVE:  Yeah.



LEO:  Steve has so many programs, so many children, he's forgotten many of their names.  But you know what, you can now forget SQRL.  It's been handed off.



STEVE:  It has a life of its own.  It has a life of its own.



LEO:  And soon as I hear from Jose I will do whatever it takes to...



STEVE:  Oh, that'd be very cool.



LEO:  I will.  I've wanted that for some time.  And, yeah, that's one of the native plugins on Discourse's OAuth, too.  So that's a big deal.  I think having OAuth 2 capabilities, I guess we'll have to go through his server, though, for the time being.



STEVE:  Yeah, I was thinking about that, too.  It's making me think that, well, I don't want the responsibility because if GRC was DDoSed...



LEO:  Who runs them?  LinkedIn runs them.  There are a lot of places where OAuth 2 servers live.  Anytime I use the single sign-on, click the Facebook or the Google link, that's OAuth 2.



STEVE:  You're passing through there, yes, yes.  And you're bouncing through their service.  So they need to be up, like always.



LEO:  All the time, yeah.



STEVE:  So it would be good once Jose has this figured out to figure out where he could put it so that...



LEO:  It's something Cloudflare should do.  



STEVE:  Oh, I like that, yeah.



LEO:  John.  Oh, John.  Be great if Cloudflare did it.



STEVE:  Yeah, yeah, yeah.  That's exactly the right profile.



LEO:  Yeah, yeah.



STEVE:  Because if they don't, they will by tomorrow.



LEO:  They can.



STEVE:  They're amazing. 



LEO:  Isn't it?  Yeah, let's keep that.  Let's keep kissing them, and maybe they will.  It'd be really nice.  Steve is at GRC.com.  Expect more good things there now that he's freed up to get to work and focus on other stuff.  GRC is his website, the Gibson Research Corporation; but it's also where this show lives, 16 and 64Kb audio versions you can download from there.  Also transcripts.  Elaine Farris writes those out with great care, so they're really well done.  A few days after the show comes out they'll be posted at GRC.com.  And naturally, while you're there, you should absolutely get a copy of SpinRite, the world's best hard drive maintenance and recovery utility because you're guaranteed an upgrade if you buy it now.  And Steve's working on that right now.



STEVE:  Soon to be new and improved.  And it is my commitment, because I don't want to be rushed, and everybody knows nothing I've ever done is fast - although I did crank out Never10 in a couple days and a few other things I was able to do quickly.  But product scale things take time.  I'm going to be producing pre-release versions which anybody who owns SpinRite will be able to use their serial number or their transaction code in order to download.



LEO:  Oh, nice.  So keep that handy.  That's great.



STEVE:  I'm going to be pushing one out shortly after I return to work.



LEO:  Oh, man.  Well, quick, go to GRC and buy SpinRite so you can get that.  You still - you going to fire up the MASM and the Brief, and you're ready to go?



STEVE:  You know, I've given up on Brief.  I did make the switch to Visual Studio.  I didn't like it at all at first.



LEO:  VS Code?  Have you looked at VS Code?  That's the lightweight editor that Microsoft makes.  It's not Visual Studio.



STEVE:  Yeah, well, I have Visual Studio, although of course true to form I'm using Visual Studio 2008.  So, you know.



LEO:  Look at VS Code because that's a simple code editor.  It's Electron, sorry.  But it does do, it will do, I'm pretty sure it will do MASM.  And, you know, there's other good text editors out there, I'm sure.



STEVE:  Well, I'm not looking for an editor.  I really - I don't spend much time in a debugger.  And in fact I'm debugging on DOS.



LEO:  Of course.  



STEVE:  And so I may be back using SoftICE, which was the really cool soft in-circuit emulator that I developed all of my DOS stuff on.



LEO:  Wait a minute.  You don't use an editor at all?  What, do you just - what are you doing?  



STEVE:  No, no, no.  I need more than an editor.  So Visual Studio...



LEO:  Yeah, yeah, you need a debugger.  I see what you're saying.



STEVE:  ...is an editor and a compiler and debugger.  And that's the way I ended up, ever since I switched to Windows 7 in the middle of the SQRL project, I've had to, like, bite the bullet and leave Brief.  So I'm not going back to Brief.  I should mention, too, and I meant to earlier, I am probably going to switch to Windows 10 here early next year.



LEO:  You buried the lead.  That's a big story.



STEVE:  Well, the big problem was giving up 16-bit code.  And that's why I had to give up Brief.  But having done that, I've survived, I'm still here, I'm still able to develop stuff.  So it's like, okay, moving to 10, I mean, I will literally strip the crap out of it, and I do mean crap, in order to tame it.  But I think, you know, and I have a bunch of Windows 10 machines around now.  And it's like, yeah, okay, there are some things about it that annoy me.  But I've sort of made peace with it.  So I think that's probably what I'm going to do is to make a snapshot, make copies, virtualize this one, and then so I can always fall back to it.  And I've got to move all my license stuff over, which is annoying.  But once that's done, it's done.  I think it makes sense.



LEO:  Look at VS Code.  It allows you to run a debugger for most languages.  Again, I'm not familiar with its ASM capabilities.  But I bet it does.  And compile and all of that stuff.  The idea is - the problem with what you're - Visual Studio is really a .NET development tool.  It's crazy heavy.  And so I think VS Code might be more to your liking.  It's just an editor with built-in - for instance, I can get a REPL with Lisp so I can run the code and debug it and all of that stuff.  So I bet you you could do that with VS Code.  There's probably other stuff out there, too, but VS Code's free, and I like it.



STEVE:  I'll take a look.



LEO:  A lot of coders use it, yeah.  Okay.  GRC.com.  Go get that stuff.  On the Twitter he's @SGgrc.  You can leave him messages, comments, suggestions.  You can also get the show from our site, TWiT.tv/sn for Security Now!.  And you can even subscribe in your favorite podcast application.  In fact, that'd be the best thing to do.  That way you'll get it automatically, the minute it's available.  Normally we do the show on Tuesdays at 1:30 Pacific, 4:30 Eastern, 21:30 UTC.  Not next week because  Tuesday's Christmas Eve.



STEVE:  Yes, it is.



LEO:  So we're moving the show, all the Tuesday shows to Monday.  So next week, if you want to watch live, we will be on December 23rd, 1:30 Pacific, 4:30 Eastern, 21:30 UTC.



STEVE:  Yes.



LEO:  The nice thing about the download...



STEVE:  With our special podcast, "A Decade of Hacking."



LEO:  Oh, is that what you're going to do?



STEVE:  We're going to do the retrospective last 10 years.



LEO:  Whew.  Yeah, we did that TWiT Special.  We just did the decade's big stories and gadgets.  I can't wait.  That'll be fun.



STEVE:  Yeah.



LEO:  And then the following week it's a "best of" on December 31st, New Year's Eve, a "best of."



STEVE:  Perfect.  So one week off for us.



LEO:  Yep.  Thank you, Steve.



STEVE:  Okay, buddy.



LEO:  And I'll see you next time on Security Now!.



Copyright (c) 2019 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.






GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#746

DATE:		December 23, 2019

TITLE:		A Decade of Hacks

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-746.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we stumble into Microsoft's own confusion about whether or not Microsoft's Security Essentials will continue receiving updates after January 14th.  We look briefly at the year when ransomware happened.  We revisit the Avast and AVG Mozilla extensions to see how they're doing.  We look at the just-announced big news for Apple's and Google's bug bounty programs for 2020, and also at Mozilla's addition of another very appealing DoH provider (which Leo apparently likes).  We provide a nudge to Drupal site masters to update their Drupal Cores RIGHT NOW.  And then we conclude by revisiting this past decade - spanning 2010 to 2019 - and the many hacks we've explored during these previous 10 years.



SHOW TEASE:  It's time for Security Now!, our last show of the year, but also our last show of the decade.  Steve's got security news and then a look back at what a crazy decade it has been, next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 746, recorded Monday, December 23rd, 2019:  A Decade of Hacks.



It's time for Security Now!, the show where we can - whoo, I'm in a New Year's mood, I can tell you that - the show where we cover your security and privacy online with this guy right here, Steve Gibson.  In years past he's been known to dance with Captain Kirk on New Year's, but that won't be happening this year.



STEVE GIBSON:  Without any alcohol, either.  I don't know what the heck was going on.



LEO:  That was awesome.  That was awesome.  That was a New Year's Eve marathon episode that we did a few years ago.  Hello, Steve.  Merry Christmas.  Happy New Year.



STEVE:  Ho ho ho, Leo.  I think you're happy because we're all getting a week off.



LEO:  Yeah.



STEVE:  While we're celebrating our holidays.  And we're going to have "best ofs."  But we're going to end this decade, this being the last podcast of the decade, with a retrospective focus on - wow, it was really fun for me to put this together.  I used some research that the ZDNet folks had done, and then of course added a lot more of the techie details and took out some things that were unnecessary to say, but pulled together a really nice sort of decade of hacks, which is today's topic.  Although we do have a bunch of news of the week, which I'm going to spend less time on than I normally do because there was just so much that happened in the last 10 years.  I mean, and so many of these you're going to, I mean, they'll just put a big smile on your face when we run across like Diginotar and things like that.



LEO:  Oh, yeah.  Oh, yeah.  The Hong Kong Post Office will probably figure in this, yes.



STEVE:  So we have a great podcast for our listeners.  And this is also a commercial-free podcast,  so we will make use of the time that is saved.  And, oh, such a fun Picture of the Week that we will get to.  So we're going to talk about stumbling along with Microsoft over their own confusion about whether or not Microsoft Security Essentials will continue receiving updates after January 14th.  We're going to look briefly at the year when ransomware happened.  And actually we come back to that at the end of this podcast because of course that was this year.  We revisit Avast and AVG's Mozilla extensions to see how they're doing, which remember when they got yanked a week or two ago.  We look at the just-announced big news for Apple's and Google's bug bounty programs for 2020; also at Mozilla's addition of a very appealing DoH provider, which apparently you like, Leo, because they've got your endorsement on their home page.



LEO:  Oh, boy.  I hope it's somebody I like. 



STEVE:  We provide a nudge to Drupal site masters to update their Drupal cores right now.  And we'll then conclude with revisiting the past decade of all the things that we've talked about on this podcast, which is pretty much everything that happened in the past 10 years.



LEO:  Holy moly.  I don't know how you're going to fit all that in.



STEVE:  I have no idea.



LEO:  Well, it's good, because this show has to serve for a couple of weeks.



STEVE:  Yes.  Our listeners could take it in two pieces, if they wanted to, knowing that there will be a "best of" episode as the official Security Now! podcast next week.



LEO:  Yes.



STEVE:  But when I happened to be on the SQRL forums, I saw a posting by Paul Holder, who is one of the team of moderators that I have - the SQRL forums are moderated in order to keep all the crap off of them.  It just is necessary to do that these days.  And so I've got - I asked from the SQRL newsgroup, as I was setting up the SQRL web forums, for a show of hands, who wants to be a moderator.  And I got about 15 or 16 people who'd been long-term contributors to the SQRL effort, so I sort of knew them by their participation.  And so we've got a group of people who are pretty much always there, which is nice because it means that people who are posting don't have to wait long for someone, a moderator to come along and say, oh, yeah, that one's fine, and to let them in.



Anyway, Paul is one of those people.  And his posting on Thursday said:  "I just converted my TWiT.community account to SQRL login via Jose's OAuth 2 provider."  He said:  "It's pretty sweet.  Time to add it as one of the sites supported on a list somewhere."  So actually that spurred me to agree that we need to now have over in the SQRL forums, in GRC's SQRL forums, we need to have a place where people can post SQRL sightings where they've seen things.  And of course then I followed the - he posted a link to a thread over in your community, Leo, where you were very active apparently in the past week, bringing SQRL online on your forums.



LEO:  Well, and we talked about this last week.  As you know, Jose Gomez had written an OAuth plugin for his Discourse forum.  We use the Discourse software.  It has an OAuth 2 plugin as part of its installation.  So it was actually fairly simple.  Jose runs a SQRL authorization server.  I take it that somebody has to run that.  And I signed up with my SQRL account and modified the settings in the TWiT Community, and there you go.  You see, you can login with email or login with SQRL.  And if you have a SQRL account, which you could set up in a variety of places - are there a lot of OAuth providers for SQRL?



STEVE:  No.  As far as I know, Jose's is it.



LEO:  Jose's the first, okay.



STEVE:  Yes, yes.



LEO:  So I'll give you his address, it's sqrloauth.com, sqrloauth.com.  It's free to set up an account.  Once you set that up, you'll have a SQRL login, and then you can use that to join our TWiT Community forum.



STEVE:  Very, very cool.



LEO:  One disadvantage, it's a little unfortunate, it's not your fault, SQRL's fault, or I don't even think it's Jose's fault.  You can't use two-factor if you're using SQRL.  I know you don't need it with SQRL.  But SQRL does allow you, and SQRL OAuth allows you, to have both, to tie your regular login to your SQRL login.  And if you had two-factor set up with your original password-based TWiT Community, which we certainly recommend you do, you have to disable that in order to use SQRL.  And so that somewhat reduces your security because there still is that password login, no longer with two-factor.



STEVE:  Oh, I see what you mean.  Yeah, yeah, yeah.  Right, right.  Because you had to take two-factor off of your password-based login.  So I get it.  Right, right, right.



LEO:  So I don't, I mean, SQRL is effectively as secure as two-factor because of course you have to have somebody's SQRL private key in order to use his SQRL account, and presumably people are pretty careful with distributing that.



STEVE:  The SQRL private key and the password that you use to authenticate your private key.



LEO:  That was one thing that kind of surprised me.  Is it always the case that I have to use the first few letters of my SQRL password to authenticate?



STEVE:  Yeah.  The problem is essentially you're giving SQRL the permission to be you on the Internet, to impersonate you.  So somebody could walk along, if your laptop was left unattended and go, oh.



LEO:  Oh, yeah, use your SQRL, yeah.



STEVE:  Yeah.  Now, in the case of biometrics we don't have that.  So if you had a face or a thumbprint or something, then that serves as your "Yes, it's me."  But even then, per authentication we just ask you to look at your phone or put your thumb on it because we don't want someone else to come along.



LEO:  So it is two-factor.  You do actually have two factors.



STEVE:  Yeah, yeah.  There still is that, yes.



LEO:  Yeah.  So I guess what you'd probably want to do, I don't know how you would do this, is delete your password login to that site and just use your SQRL login.



STEVE:  Well, and we have anticipated this.  There is a setting in all SQRL clients saying, "Please disable all non-SQRL authentication."  So in instances where - and, see, this doesn't help you because you've got a problem because you're using OAuth and a SQRL provider.  But at some point somebody will implement SQRL natively for Discourse.  And when that happens a user can turn that checkbox on, and it solves this problem because we always have this problem of security being about the weakest link.



And so, yeah, it's really great that you've got the security of SQRL.  But if you still have username and password, that could still be hacked.  So the idea is, after SQRL users become comfortable with SQRL, they can just turn that setting on in their client.  And as they visit sites and use SQRL there, the site sees, oh, I've been asked to disable everything but SQRL.  And at site's decision, we have no way of forcing sites to do that, but there are a lot of things that I put in there because they're the sort of things where it's impossible to add them later because not everyone will support them.  So it's better to have them in at the beginning, even if no one supports them, because then they might.  But it's part of the spec.  So, yeah, that has been anticipated.



LEO:  Nice.  Well, now you can use SQRL to log into TWiT.community, I'm happy to say.



STEVE:  Very cool.  Thank you.



LEO:  Thank you, Jose.  Well, thank you Jose.  He's really the one who did all the heavy lifting.



STEVE:  Yeah.  Well, it's going to help - it helps the spread.



LEO:  Yup.



STEVE:  So in apparently a reversal, it now looks like Microsoft's Security Essentials - which we just talked about last week, bizarrely enough, being discontinued, even for enterprises that chose to pay for ongoing Windows 7 security updates.  Apparently Microsoft has reversed themselves.  I mean, as far as we know.  We know this thanks to Computerworld's Woody Leonhard, who posted the question to Microsoft.  According to an official post, the company will continue to ship updates to Microsoft Security Essentials after Windows 7 demise.  Now, what we still don't know is if that applies to all Windows 7 users or only the extended security update users.



So what Woody explained was, in his Computerworld posting, he said:  "Late last week, I talked about a discrepancy in Microsoft's promised handling of Microsoft Security Essentials as Windows 7 reaches end of support.  An internally inconsistent official announcement seemed to say that MSE signature file updates would stop, even for those who have paid for extended security updates," he says, "which is absurd.  Why would Microsoft stop updating its antivirus program even for people who are paying to continue receiving monthly rollup patches?" he said.



So then last Tuesday Microsoft held an "Ask Me Anything" session, as Woody termed it, for the Win7 forlorn, on the Microsoft Tech Community Forum.  Woody asked:  "Can you confirm that Microsoft will really, for sure, cut off Microsoft Security Essentials malware signature updates after January 14, even if you're paying for extended support?"  Microsoft engineer Mike Cure provided an official response:  "MSE will continue to receive signature updates after January 14th."  And he cited a Windows 7 Support FAQ which says:  "Microsoft Security Essentials (MSE) will continue to receive signature updates after January 14th, 2020.  However, the MSE platform will no longer be updated."



And so, again, that makes me now ask, okay, wait a minute.  So if the platform isn't updated, will the signature updates go to everyone?  Okay.  Then also during the same Ask Me Anything, someone, @Brian, responded by referring to the Extended Security Update FAQ which asks the question:  "Will Microsoft Security Essentials continue to protect my PC after end of support?"  The answer:  "No, your Windows 7 PC will not be protected by Microsoft Security Essentials (MSE) after January 14, 2020.  This product is unique to Windows 7 and follows the same lifecycle dates for support."  So Woody wrote:  "That's an obfuscating piece of bafflegab, subject to whimsical..."



LEO:  I like that word.



STEVE:  "Bafflegab."  Yeah, I had to do a double-take on that.



LEO:  I'm using that from now on.



STEVE:  I like bafflegab.



LEO:  It's bafflegab.



STEVE:  Bafflegab, embaffling me, it baffles - "...subject to whimsical interpretation, as I described in the Computerworld article last week."  That's Woody speaking.  Mike Cure then clarified the situation by promising:  "I'll get the ESU FAQ corrected as soon as possible."  And then Woody concluded by noting that:  "As of early last Wednesday morning" - when he had posted - "nothing's been corrected."  And he says:  "Those of us who actually like and rely on MSE are still hanging on a limb."



So we don't know.  I don't think even Microsoft knows.  They don't seem to know what the other hand is doing.  And of course no one has addressed whether those who don't pay for Security Essentials - so what Microsoft is doing is they're separating the signatures from the platform.  They're saying that the platform, the MSE platform itself won't be updated - not that I know that's being updated that often - but that the signatures are separate from the platform.  So maybe we'll all keep getting them, or maybe not.  I don't know.



So we'll have to wait till next month, and then we'll find out.  Actually month after next, I guess, February.  It'll be mid-February, the February Patch Tuesday.  And also next month is going to have a late Tuesday.  Actually we already know that because the first podcast of Security Now! is the 7th, which is the latest it can ever be for the date on a month, which means that of course the 14th is a week later, which is the final, the second Tuesday of January.  So it'll be February that we find out whether anybody got updated who still has Windows 7 and isn't paying for the updates.



The second piece I had really should go at the end, the very, very, very end because it's just sort of about - it's an update from Armor, who's been following ransomware stuff, about just what a problem this has been.  Eleven more school districts have been hit by ransomware since October something - I have it in the notes here - which is when they updated their cutoffs.



So as a consequence, standing back and looking at 2019 overall, a total of 72 school districts or individual educational institutions have publicly reported being a victim of ransomware which impacted 1,039 individual schools.  In the show notes I have a map that's showing a lot, and looks like the upper East was really badly hit.  That may have been a consequence of a stronger bias toward managed service providers, MSPs, since we noticed the pattern that anyone who used managed service providers, and we do know several instances where school districts were, that was just like the site of contagion for many of these ransomware attacks.  If somebody could get into a managed service provider, they could end up bankrupting the managed service provider because they would lose so many of their clients when all of their clients got zapped as a consequence of all being serviced by a single entity.



And the other thing we saw this year is that the security provisions really were lacking.  That is, the MSP gets infected, and then somehow they've got access, like as we have seen, admin rights into the networks of their clients.  Which is just a bad idea.  If there's a takeaway for our listeners this year, it would be, if your company is using a managed service provider, figure out how to throttle their rights because, unless they really, really need to have admin privilege, you really want to put a throttle on that.



We talked about how Avast and AVG had been removed from Mozilla's store, or Mozilla's add-in repository for Firefox, after Wladimir Palant, the creator of Adblock Plus, took the time to reverse engineer the backhaul communications of those two plugins and also a couple other, like is this the best price extensions that they offer, and found that they were sending way  more data back to the mothership than was accounted for by the service they were offering of letting you know that a URL you were visiting should not be trusted.  The extensions are back, and they reportedly behave themselves far better than they had been.  We haven't yet heard back from Wladimir.  But others have looked and reported that just the URL and a much smaller bit of binary data is being sent back.  Still don't know what that is.  But it presumably was good enough to satisfy Mozilla.



And as I recall, Mozilla was requiring people to now provide the source code for their add-ins, which would mean that Mozilla would have been able to take a look at what that binary data being sent back actually is.  Avast had a standard CYA, you know, our privacy is our number one mission for our listeners, blah blah blah.  Okay.  Still seems like a bad idea to have somebody, your AV system sending back the URL of everywhere you visit to a provider, especially when we know that they purchased a data broker that is selling in anonymous detailed tracking information.



Basically, all AVG and Avast users are the source of anonymous blow-by-blow tracking of some commercial entity that is offering this for sale.  And we know because other providers do this that it's not necessary to take this approach.  You don't actually have to send back the click trail of all of your customers.  You can do it locally.  And as we talked about, Firefox does it, Google does it, where they have a local store of known baddies which they update periodically.  And before declaring a page bad, then they do a proactive check just to make sure it's still bad, which seems like a much better thing to do than send back every single click you make, every tab you click on, when the tab loses focus and regains focus.  And as we saw, it's just like way too much.



And the revised extension, it puts up a page, I have it here in the show notes, says getting proactive permission, it says - this is Avast online security saying:  "Allow us to protect you by scanning web addresses.  If you agree, we'll look at the addresses of the websites you visit so we can tell you if those sites are safe.  See our Privacy Policy."  And a big green, "Yes, Scan Web Addresses."  Or, dimmed out, "No Thanks."  They're not being really explicit about the fact that it's not your local installation of AVG which is doing the looking, but rather sending back to home base everything you do.  So, okay, fine.  Anyway, Leo, you and I are just about as down or negative on Avast and AVG as we could be.



LEO:  Yeah.



STEVE:  So enough said. 



LEO:  You don't need it.  You don't want it.



STEVE:  Yep.  As for Google and Apple, who have both done a revamp for 2020, just recently announced their bug bounty awards.  On the Google side, last Wednesday Google announced their plans to revamp their six-year-old patch rewards program started in 2013.  At the time, Google announced that it would provide financial aid to open source projects if and when and after, significantly, they implemented security features.  The project maintainers had to apply, provide a plan for the feature they wanted to implement, and Google would commit to a financial reward after they had actually implemented the features.



But starting the first of the year, Google will be changing how this program works to provide financial support upfront, even before projects implement the security features to which they commit.  The rationale behind the change is the recognition that many open source project maintainers prioritize features that will be implemented based upon the sponsorships they receive.  Such sponsorship is prevalent, as we know, in the FOSS, the Free Open Source Software community.



So, for example, if a company needed a particular feature added to an open source project, the company usually donates to the project with the condition that the maintainers implement the feature which the company is essentially paying them for with a higher priority before other features.  Therefore, by Google providing its funds now upfront, Google is able to provide project maintainers a way to fund their work while prioritizing security features at the same time, rather than relying upon the largesse of wealthy corporate entities whose needs for project features may be more self-serving and less security focused.



So Google will continue, is going to continue to push security, but they're saying we're going to give you money as a sponsor to sponsor the development of the feature ahead of time.  Certainly Google can afford this.  There are two different fundings that project maintainers can request via this Patch Rewards program.  The smaller is $5,000, meant to motivate and reward a project for fixing a small number of security issues like improvements to privilege, separation, or sandboxing; cleanup of integer math operations; or more generally fixing vulnerabilities identified in open source software by other bug bounty programs.  So that's $5,000.



The larger, $30,000, meant to incentivize a larger project to invest heavily in its own security, for example, providing support to find additional developers or implement a significant new security feature like full-blown compiler mitigations, that sort of thing.  So much more of a salary-ish sort of thing when a much bigger chunk of work wants to be done.  And I'm sure that Google will keep an eye on it and verify that it gets done.  But they're not requiring it upfront.  So that's cool.  I have a form for anybody who's interested in applying in the show notes.  You fill out the form to apply for the grant.  And if Google decides that your project looks like it's worthy, you can get it.



Apple, they've opened their previously closed bug bounty program to all security researchers and have now published their official rules.  They talked about this last August, that they were going to be doing that.  Well, that has happened.  On Friday, Apple formally opened its bug bounty program to all researchers.  They had announced it, as I mentioned in August, during Black Hat.  There's a big splashy page on the developer site at Apple, headlined "Apple Security Bounty."  Up until this time, the bug bounty program was strictly by invitation only, and only accepted iOS security bugs.  Now Apple will accept vulnerability reports for a wide array of products, including iPadOS, macOS, tvOS, watchOS, and iCloud.  So basically the iOS platform in all of its variations, plus Mac and iCloud.



And the company has increased its maximum bug bounty from $200,000 to, wait for it, $1.5 million.  Actually, there's even a way to, I think - oh, no.  That would be if you get the extra 50% bump, I'll explain in a second, so depending upon the exploit chain's complexity and severity.  However, Apple's not handing out such high rewards casually.  The rules are strict, and they've set what those in the industry regard as a high bar for earning the top rewards.  On the other hand, it's $1.5 million.  So that would be good.



To be eligible for the top prizes and various bonuses, researchers must submit clear reports which include a detailed description of the issues being reported, any prerequisites and steps to get the system into an impactable state, a reasonably reliable exploit for the issue being reported, and enough information for Apple to be able to reasonably reproduce the issue.  So of course that's pretty much the Pwn2Own requirements; right?  I mean, that's the set of things that anybody winning Pwn2Own has been able to do in the past.  And so now we're talking about some substantial reward rather than just say, hey, look, I got a laptop.



So those bugs which are novel, affect multiple platforms, work on the latest hardware and software, and impact sensitive components are more likely to net the top $1.5 million reward.  And vulnerabilities discovered and reported in beta releases will also be highly prized, with Apple willing to add a 50% bonus on top of the regular payout for any bug reported in a beta release.  And as we talked about this at the time, because we did get a sense that this was going to happen, this seems entirely rational since it would incentivize researchers not to wait until something is actually out and shipped, but to get onboard and help Apple find these problems before they ever ship them in the first place, which it makes sense that Apple would and should be willing to pay for.



The 50% bonus is for regression bugs where older and previously fixed problems return for an encore, as well as bugs that are found during beta.  Also, since takeover bugs requiring no user involvement are the most sought after by the likes of Zerodium, the discovery and reporting of those will also bring researchers top money from Apple.  So long as the researcher is able to provide a fully working exploit chain for these types of submissions, they stand to qualify.



Under "Eligibility," Apple says the party must be the first party to report the issue to Apple Product Security, provide a clear report which includes a working exploit, not disclose the issue publicly before Apple releases the security advisory for the report, which of course is generally released along with the updates.  Issues that are unknown to Apple and are unique to designated developer betas and public betas, including regressions, can result in that 50% bonus payment.



And so Apple's looking for security issues introduced in certain designated developer beta or public beta releases.  And they're saying that not all developer or public betas are eligible for the additional bonus.  And then they're very interested in refinding anything that they broke.  Regressions of previously resolved issues would also get payment.



So, for example, I have in the show notes - and Leo, you were showing it a second ago - sort of the breakdown, the menu of payment schedule.  iCloud:  Unauthorized access to iCloud data on Apple servers gets someone who finds a way to do that $100,000.  A device attack through physical access, if it's a lockscreen bypass, 100,000.  If it also allows user data extraction, that's a quarter million.  A device attack via a user-installed app, so if an app can be installed and obtain unauthorized access to sensitive data, $100,000.  If the app is installed and obtained kernel code execution, $150,000.



A CPU side-channel attack gets a quarter million.  Then we have network attack with user interaction, one-click unauthorized access to sensitive data over the network, $150,000.  One-click kernel code execution through a network connection, a quarter million.  And the final category, where the big money is, network attack without user interaction, a zero-click access to the kernel with physical proximity, and I don't know what they mean by that, "with physical proximity."



LEO:  That would mean like a Bluetooth or WiFi exploit, maybe?



STEVE:  Yeah, maybe.  Network attack without user interaction, zero-click, oh, radio to kernel with physical proximity.  So I think you're right, Bluetooth or WiFi, quarter million dollars.  Zero-click unauthorized access to sensitive data, half a million dollars.  And, finally, zero-click kernel code execution with persistence and kernel protection bypass, that brings you the million dollars.  And if to that you found that in a regression - or what was the other thing?  It was - or in a beta.  So either you found something that they broke, or you find a zero-click kernel code execution with persistence and kernel protection bypass, that's where you get - and that's in beta software - $1.5 million for the big payout.



So anyway, it's nice that this is happening.  As we've said, the problem has been that the commercial entities have not been able to outbid the likes of Zerodium, so a hacker who's trying to make a career from finding security flaws by being an ethical bug hunter - and as we've talked about, it's entirely possible now to do that - they're going to look out for themselves.  And so the tendency would be to sell into the gray market like Zerodium rather than help Apple find their own problems.  So it's very cool that this is now something that Apple's doing.



And Leo is endorsing a new DoH provider offering.



LEO:  This I've got to find out about.  It's probably a sponsor.



STEVE:  If you scroll the show notes down, you'll see your endorsement.  Mozilla has expanded its DoH provider offering.  Last Tuesday Mozilla announced the addition of a second DoH server provider to its previous list of one, which of course we know was Cloudflare.  Users can now choose between Cloudflare and NextDNS.



LEO:  Huh.  They might be quoting me about DoH as opposed to NextDNS.



STEVE:  Ah, that's a possibility.  Yeah, that would, you're right, that could very well be.  So Mozilla said:  "Firefox announces new partner in delivering private and secure DNS services to users."



LEO:  I guess I should use it and see.



STEVE:  Actually, it sounds really cool.  "NextDNS joins Firefox's Trusted Recursive Resolver (TRR) program committed to Data Retention and Transparency Requirements that respect user privacy.  By adding a second provider, NextDNS, a relative newcomer startup founded just this past May, Mozilla has not only added an alternative, but has lifted its promised Trusted Recursive Resolver program off the ground."  Mozilla says that its Trusted Recursive Resolver program matters because:  "DoH's ability to encrypt DNS data addresses is only half the problem we're trying to solve.  The second half is requiring that companies with the ability to see and store your browsing history change their data handling practices."  And now of course...



LEO:  We're sending them a takedown notice because I've never recommended them.  I never even heard of them.



STEVE:  Ah, okay.



LEO:  Although it sounds pretty good; right?  It's okay?



STEVE:  It does sound pretty good.



LEO:  Okay.



STEVE:  Yeah.



LEO:  I don't know why they're including me in this.  Geez.



STEVE:  Well, I went there, and I said, oh, there's Leo.



LEO:  That's the problem.  I mean, you can't just put my name on something.



STEVE:  Yeah, yeah.  Well, and it's a younger Leo.  But I think that's a picture of your...



LEO:  That's my standard headshot, but I just...



STEVE:  Floating around the Internet, yeah.



LEO:  Yeah, it's probably from - I don't know where they got that.



STEVE:  Anyway, so as we know, it is likely that ISPs are going to respond to DoH by bringing up their own DoH servers so they can once again capture their customers' DNS.  And I think this is why Mozilla is making this push, this notion of a Trusted Recursive Resolver program.  Although it doesn't really ring.  It doesn't fall off the tip of the tongue very easily.  Trusted Recursive Resolver.  Besides, most people don't know what a recursive resolver is, let alone why they need to trust one.



But in any event, so the requirements to qualify for Mozilla's  Trusted Recursive Resolver program are only collect data, for example IP addresses, for the purposes of running the service, and don't keep it for longer than 24 hours.  Publish a privacy policy explaining this.  Do not block, modify, or sensor websites unless required to by law.  So NextDNS may be apparently not endorsed by Leo.  And NextDNS, you're bad people.  You should not have done this.



LEO:  Uh, yeah.  By the way, you pay for this.  It's $2 a month.



STEVE:  Ah, okay.  May particularly appeal to more capable tinkerers like Security Now! listeners.  So what's the appeal?  Control and visibility.  Users who sign up for an account are given an inordinate amount of control over what gets blocked and what doesn't, including being able to create domain allow/block lists, and sign up for a range of public advertising and tracking and filtering lists.  Techie users can even block specific applications, as well as view their traffic logs, all providing a level of control and visibility into DNS that is very unusual for DNS providers of any type.



So of course the reason, as we know, that so many ISPs are chafing at the pending loss of access to their customers' DNS, is the reason why putting the power of DNS management into the hands of those who want it and know what to do with it makes sense.  So anyway...



LEO:  They're just going to replace my name with your name now.



STEVE:  Oh, no.  Agh, you're right.



LEO:  Well, the quote that they have on the page does not refer to them.



STEVE:  Right, it's very generic.



LEO:  The other two do mention them by name, but I don't think I've ever heard of them.



STEVE:  Well, and Leo, they put you first.  You're on the left.



LEO:  Geez.



STEVE:  Of the three, so...



LEO:  Holy cow.



STEVE:  Yeah.



LEO:  Well, maybe if anybody remembers me ever saying anything about them, let me know.  Otherwise we're sending them a cease-and-desist.



STEVE:  Well, and they've not been around.  They're only been around since May, so a little over six months.  



LEO:  But, hey, at least they're a good company.



STEVE:  They do sound like a good company.  And, I mean, the fact that Mozilla is vouching for them, that says a lot.



LEO:  Well, are they really?  



STEVE:  Yes.  Oh, no, no, they are.  I mean, Mozilla has added them officially to their Trusted Recursive Resolver list, meaning that these people have agreed to Mozilla's rather strict requirements over the lack of logging and tracking that this company will do.



LEO:  They're not even following me on Twitter.  I don't know how they even found me.  The founder was a director at Netflix and co-founder of Dailymotion, Olivier Poitrey.  It's French.  It's interesting.  Hmm.  I think they're French.



STEVE:  Well, I imagine your face will be disappearing from their website before long.



LEO:  Yeah, the founders of Dailymotion, the two of them have founded it, which is kind of the French YouTube or European YouTube.  Huh.



STEVE:  So Drupal users, the Drupal admins within the sound of my voice need to update their Drupal Core immediately, if they haven't done so in the last week.



LEO:  Oh, god.



STEVE:  The Drupal Core team have released four critical, well, one critical and three moderately critical patches to the core.  So that's just a heads-up.  The latest release of Drupal 7.69, 8.7.11, or 8.8.1 will prevent remote hackers from compromising web servers.  So this is why it's critical.  There's a bug in the Archive_Tar library that Drupal Core uses for creating, listing, extracting, and adding files to tar archives.  If a site in no way invokes Archive_Tar, then you're okay because that's, for this critical patch, that's where the problem lies.  And Drupal Core by default does use this Archive_Tar library.  There's a problem in it.  So if somebody were to arrange to upload a .tar, a .tar.gz, or .bz2, or .tlz files, that can take over your server remotely.  So time to update, if you haven't.



Okay.  I blew through the news so that we would have time to play with our decade of hacks.  2010, Leo, the year is 2010, and we had fun with Stuxnet.



LEO:  Oh, yeah.  And by the way, this show was, what, five years old in 2010, something like that; yeah?



STEVE:  I think, 10 years ago, no, so nine years ago, yeah, not quite five years.  I think it was - was it two years old?



LEO:  Oh, okay.



STEVE:  Because we're in year 12 now.



LEO:  We started in 2008?  Yeah.



STEVE:  I think that sounds right.



LEO:  That sounds right.



STEVE:  But still, so it was fresh.  It was a fresh podcast.



LEO:  Stuxnet, our first really big story.



STEVE:  It was a big story.  Yeah, because really the Honey Monkeys, eh, you know.  But, yes, Stuxnet.  We are now pretty certain that the Stuxnet worm was co-developed by the U.S. and Israeli intelligence services as their means to sabotage Iran's nuclear weapons program, which had been ramping up in the late 2000s and was, like, beginning to be a worry in 2010.  As we covered at the time, Stuxnet was cleverly designed to ride on thumb drives as a means of jumping the air gap to non-networked computers.  It was specifically designed to destroy SCADA, that's Supervisory Control And Data Acquisition (SCADA) equipment, and in this case the centrifuges which were being used by the Iranian government for its nuclear fuel enrichment process.



And it did.  It successfully destroyed equipment in several locations in Iran.  Though there were other cyberattacks carried out by nation-states against one another before 2010, Stuxnet was the first incident that grabbed international headlines and really sort of marked the entry into a new phase of cyber war, moving from simple data theft and information gathering into the actual physical destruction of equipment.  So, yikes.



Also 2010, Operation Aurora was the hack that changed Google.  Although these attacks by the Chinese government's military were actually conducted in the 2000s, their efforts to compromise U.S. properties in this so-called Operation Aurora included Adobe, Rackspace, Juniper, Yahoo, Symantec, Northrop Grumman, Morgan Stanley, and Google.  All these attacks came to light in early 2010.  So it was called Operation Aurora.  And it marked a turning point for Google.



After Google discovered and publicly disclosed the attacks against its infrastructure, they decided to stop working with the Chinese government in censoring the search results for Google.cn.  And Google eventually, as we know, shut down their operations in China.  In explaining their decision, Google specifically mentioned Operation Aurora and the attack against their infrastructure by China as one of the factors behind their decision.  And then, Leo...



LEO:  Yes?



STEVE:  Wow, it does bring you back; doesn't it?  The Sony PlayStation hack. 



LEO:  Oh, yeah.  That was a big deal, yeah.



STEVE:  Yep.  The Sony PlayStation attack, the spring of 2011.



LEO:  Not the only one of the decade, either.



STEVE:  Yeah, announced that a hacker had stolen details for 77 million PlayStation Network users, including personally identifiable information and financial details.  It's interesting that by today's measure such a breach would be a bit of a yawn.  Oh, another massive breach of personal information.  But at the time, and for many years afterwards, this breach stood alone as a biggie and really did impact them.  It was catastrophic for Sony.  They were forced, as a consequence of this hack, to shut down their cash cow, the Sony PlayStation Network, for 23 days.  Remember it was off for, like, nearly a month.



LEO:  Yeah.



STEVE:  While their IT people addressed the security breach.  And it remains today the longest outage in the PlayStation Network's history.  They not only lost profit directly due to the outage, but then more so over class-action lawsuits filed by users after some started noticing credit card fraud against the information that had been leaked during this breach.  And then they still lost more money when they were forced to give users a bunch of free PlayStation 3 games as an incentive to bring them back.  What the industry learned in a big way was the degree of damage that a successful network attack could cause when a company fails to invest in proper security.  And of course we'll be talking about the Sony Pictures attack here in a little bit.  There was another problem with Sony.



So this event also stands out because corporate lawyers woke up and started a trend of companies adding CYA clauses to their Terms of Service requiring that users relinquish any rights to lawsuits following security breaches.  That wasn't in typical Terms of Services before the Sony breach.  But now that is a standard of rights that are waived when users use websites and network services and so forth.  It's like, well, breaches may happen.  They're acts of god.  So oops, you know, you have to hold us harmless for any consequences of those.



And in the latter half of 2011 was Diginotar.



LEO:  Oh, Diginotar.  



STEVE:  Just fun to say "Diginotar."  That was when we first learned of the Iranian government's successful hack of the then-popular in that neck of the woods Dutch Certificate Authority, Diginotar.  In a very nice retrospective summary which Threatpost wrote a year later - so that was in, I think it was in late 2012.  Looking back on the event, Threatpost wrote:  "The attacker who penetrated the Dutch Certificate Authority Diginotar last year had complete control over all eight of the company's certificate-issuing servers during the operation, and he may have also issued some rogue certificates that have not yet been identified.  The final report from a security company commissioned to investigate the Diginotar attack shows that the compromise of the now-bankrupt certificate authority was much deeper than previously thought.



"In August 2011 indications began to emerge of a major compromise of a Certificate Authority in the Netherlands, and the details quickly revealed that the attack would have serious ramifications.  The first public acknowledgement of the attack was the discovery of a large-scale man-in-the-middle attack waged against Gmail users in Iran.  Researchers investigating that attack discovered that the operation was using a valid wildcard certificate issued by Diginotar for *.google.com," giving the attacker the ability to impersonate Google to any browser that trusted Diginotar's certificate, as they all did at the time.



LEO:  What fun.



STEVE:  What?



LEO:  Oh, just laughing.  What fun.  You can all be Google.  You be Google, and you be.  By the way, I did recommend NextDNS back in May.  Somebody found a clip from This Week in Google.  So I don't know how I found them, and I don't remember the conversation at all.  But it was a Pick of the Week.  It was one of those things where that must have just come out, and I thought, oh, here's something really cool.



STEVE:  It would have because they launched in May.



LEO:  Yeah.



STEVE:  So you probably found a new DNS provider.  And again, the fact that Mozilla has added them to their list of two, Cloudflare being the other one, it does both, yeah.



LEO:  It's a good recommendation, yeah.  So I take it back.  I take it back, NextDNS.  I did recommend you way back then.



STEVE:  So it quickly emerged as we looked at, I mean, the whole industry went, "Holy sh*t."  Remember that, like *.google.com?  So what we discovered was that attackers had also obtained valid certificates for a number of other high-value domains, including Yahoo, Mozilla, and others.  Browser manufacturers immediately revoked the trust in the compromised certificates and reassured their users that the Internet was not broken.



Now, the final report from Fox-IT, which is the Dutch company that was brought in at the time of the attack in 2011 to find the root cause and determine the extent of the damage, says in its final report that the attack was a wide-ranging one that likely started more than a month before the CA themselves discovered it.  And of course it was discovered when, as we recall, somebody used Google.  Somebody used Chrome.  Google has always been good about deliberately pinning its own certs.  And so it was when someone used Gmail through Chrome that an alarm went off in Palo Alto headquarters saying, "A browser of ours just received a valid cert that we didn't produce from some outfit called Diginotar.  What the heck is going on?"  So that was the beginning of that drama.



And that ends 2012.  How many times since 2013 have I referred to on this podcast Edward Snowden, and the fundamental way the Snowden revelations changed the world's entire security landscape?  I think it would not be an overstatement to conclude that the Snowden leaks were probably the single most important cybersecurity event of this decade.  They exposed, as we know now and didn't before, a global surveillance network that the U.S. and its Five Eyes partners had set up after the 9/11 attacks.  And cybersecurity has been forever changed.



An unfortunate event, or another consequence of Edward's revelations, is that that drove the repressive countries like China, Russia, and Iran, to ramp up their own surveillance operations and to increase their foreign intelligence gathering efforts, which has led to an overall increase, an escalation in cyberespionage as a whole.  And it was interesting.  I did a little bit of browsing around.  Wikipedia has really extensive coverage on the downstream impact of Snowden's leaks.  But, boy, you know, the Snowden revelations, well, I mean, as I said, and I have been saying, fundamentally changed.



LEO:  If you had to pick one security event in the decade, that's the one.



STEVE:  Yes.



LEO:  By far.



STEVE:  Yes.  I mean, there were other big hacks.  But this changed everything.  I mean, just it changed the way we operate.



LEO:  It was ironic because it wasn't anything we didn't already know.  It just confirmed in the most dramatic way how much state surveillance was going on in the U.S.



STEVE:  Yeah.  Yeah, exactly.  And we had then visions of an - they were found, these network TAP rooms, where at the major exchange points on the Internet there was a dark closet, and all the fiber was passing through it.  It was like, what? 



LEO:  And we knew that already from a whistleblower about AT&T.  But again, somehow Snowden crystallized everything.  And because the information was from the NSA, it had dramatic impact.



STEVE:  Yeah.  And then at the end of 2013 Target admitted that malware planted on its stores' systems had enabled hackers to collect payment card details for 40 million of its previous shoppers.



LEO:  That was that long ago.



STEVE:  Yeah.



LEO:  That's still the prototypical breach.



STEVE:  It is.  That's exactly right.  And as a consequence, much of the world was introduced to the concept of point-of-sale malware, which wasn't something that we were really aware of at the time.  And of course that meant that now consumers understood that purchasing something at an infected retailer could put their credit or their purchasing information at risk.  There had been previous smaller incidents of point-of-sale malware.  But this was the first time that a major retailer suffered a breach of such proportion.  And we know that many retailers have fallen since.  But exactly as you said, Leo, Target was the first biggie.  And it still sort of remains like, ooh, you know, the Target breach is like, whoa, yeah.



Then in November of 2013 we had the big Adobe hack, where Adobe admitted that hackers had stolen the data of more than 153 million of their users.  The data was dumped online, and their users' passwords were almost immediately reversed back to their plaintext versions because back then, 2013, heavy salting and much better PBKDFs were not in use at that point, or at least not at Adobe.  And for many years after the Adobe breach, it continued to be used as a cautionary warning about the use of weak and easily guessed passwords.  And it's worth noting that it was the Adobe breach that brings me to another major event, which actually wasn't until a little bit later.  And that is, but it's significant, and that's Troy Hunt's creation of "Have I Been Pwned?"  Troy we know is an Australian security researcher.



As a consequence of the Adobe breach, he created - and since unfortunately there was 153 million records of Adobe users - he created a simple way for users to go to Have I Been Pwned? website and learn, without disclosing any information because it was done client-side, whether their password matched the hash of a password that had been leaked.  Anyway, Have I Been Pwned? marks another major milestone in this past decade.  Today it's been such a success that Troy's site includes data breach databases - get this, Leo, this is an indication of the trouble - of over 410 hacked sites who have lost their data.



That's one of the reasons why the truth of SQRL being it gives no websites any secrets to keep is significant.  The SQRL identity at a website doesn't do any other website or anybody else any good.  So SQRL gives sites no secrets to keep, unlike these 410-plus hacked sites whose information totals more than nine billion hacked accounts is now available through hashing access at Troy's Have I Been Pwned? site.



Also, 2013 was the year that Silk Road, which was that Tor-based dark web marketplace for selling illegal content, basically drugs, people, everything, it got taken down.  Silk Road's discovery and takedown for the first time showed the world, and certainly the security world, since we did a podcast before that about The Onion Router, TOR, the TOR network, and what a strong technology it had for protecting people's identity and privacy.  But we now know, we learned with the Silk Road takedown, it isn't absolute; that it is not possible to provide perfect anonymity and security.



And 2014, the hack of Sony Pictures.  It was in 2014 that we learned that North Korea had some competent hackers of their own.



LEO:  Oh, yeah.



STEVE:  They initially called themselves the Guardians of Peace, and then later the Lazarus Squad.  And they were eventually linked to North Korea's intelligence apparatus.  As we'll recall, the motivation behind the hack was an attempt to force Sony to abandon its planned release of "The Interview," which was a comedy about an assassination plot against North Korea's then-leader Kim Jong Un.  When Sony refused to be intimidated, the hackers damaged their network and leaked studio data and private emails online.  And as our listeners will recall, this also was the first widespread use of the term APT, Advanced Persistent Threat, since we learned that Sony's breachers had been lurking inside their network for quite some time before.  And a couple other events that we are about to get to will be reminding us of the notion of APT.



Also 2014 was the hack, the big hack of Mt. Gox.  It was not the first cryptocurrency exchange to get hacked, but it remains to this day the biggest cyber heist of the cryptocurrency ecosystem.  The hack, which is still shrouded in some mystery, occurred in early 2014.  And Leo, are you sitting down?  Are you centered over your ball?



LEO:  I'm on my ball, yeah, centered, yeah, balancing, yeah.



STEVE:  The hackers made off with 850,000 bitcoins currently worth more than $6.3 billion.



LEO:  Holy moly.



STEVE:  And we know...



LEO:  Then they forgot their password.  It's so sad.



STEVE:  Isn't that a tragedy.  They're busy trying them all.



LEO:  Yeah.  We don't know who it was, though, do we.



STEVE:  No, we still don't know what happened.



LEO:  Have those coins ever been cashed in?  Or are they just sitting somewhere?



STEVE:  That's a good question.  I have not looked.



LEO:  Because you could look on the, you know, the thing about bitcoin is there's a registry.



STEVE:  Yes, there is an audit trail, yeah.



LEO:  Yeah, and every single transaction's in there, and everybody, by the way, who has a bitcoin wallet has all of those transactions.  So you could search for that number.



STEVE:  Incredible.



LEO:  Wow.



STEVE:  Incredible.  And at the time bitcoin wasn't as liquid as it is now.  Now it's a freely exchanged commodity.



LEO:  People would notice, though, if you sold $6 billion worth of bitcoin all of a sudden.



STEVE:  Yeah, actually the market would notice, yeah.



LEO:  Yeah.



STEVE:  That would be an event.  So unfortunately, since then, just as the infamous bank robber Willie Sutton explained that he robs banks because that's where the money is, hackers have since realized that stealing virtual currency is a lot easier than making it the old-fashioned way.  So now cryptocurrency exchanges have become a frequent target of attack because that's where the virtual money is.



LEO:  And Mt. Gox is long gone after that.



STEVE:  Yup.  That's over.  And no review of 2014 would be complete without reminding everyone of Heartbleed - that probably established the need to have a website and a kickass logo to go with your security vulnerability announcement, since that's now become de rigueur - a not-just-theoretical remote data extraction vulnerability that rocked the Internet at the time.  Oh, baby, did we get some mileage out of that on this podcast, Leo.  Heartbleed.  It really did enable the discovery of a server's private keys with a low, but non-zero, probability.



There were a number of people who famously thought, well, okay, yeah, maybe.  It was a buffer overrun, remember.  We got a chunk of RAM out of the server.  And the question was, okay, is there going to be anything useful in that RAM?  And it turned out, yeah.  Most of the time no, but maybe.  And you could definitely get a buffer and start scrutinizing it.  The promise of compromise was so juicy that it also began what is now the common practice of jumping immediately on top of newly announced vulnerabilities before they can be patched.



Heartbleed was exploited almost immediately after being publicly disclosed and led to a long string of attacks during 2014 and after.  And even today some server operators fail to patch their OpenSSL instances, despite repeated warnings.  At the time when it was publicly disclosed, it was believed that about half a million Internet servers were vulnerable.  And that's a number which took many years to decline.  And, boy, 2014 was a busy year for us.



LEO:  By the way, you're having the same problem we had on TWiT on Sunday, where the decade seems to be weighted heavily towards the first half.



STEVE:  Yeah, it does, yeah.



LEO:  You know?  I don't know why that is.  But yeah, we were stuck.  It took us two hours to get to 2014.



STEVE:  Yeah.



LEO:  I'm giving you a little pause so you can caffeinate.  You can take a breath.  We're not even halfway.  Well, I guess we're sort of halfway there.  What a decade this has been.



STEVE:  It has been, indeed.  June 24th, 2014.  The paper carried the innocent academic title, "Flipping Bits in Memory Without Accessing Them."



LEO:  Oh, boy.  I now know where we're going.



STEVE:  "An Experimental Study of DRAM Disturbance Errors."  And the result was Rowhammer, another authentic, not just theoretical, attack.  And of course we had Double Rowhammer, and we had Drammer and Rowdrammer and, I mean, it went on and on and on.  As we know today, it would prove to be just the first of the many that we would be seeing in subsequent years, the many attacks against the computing hardware that we had naively believed up till then to be bulletproof.  Eh, not so much.



And then, on the social engineering juicy side, 2015 was the year of the Ashley Madison dating website data breach.



LEO:  Oh, yeah.



STEVE:  Uh-huh.



LEO:  That one struck terror into the hearts of quite a few people.



STEVE:  It did.  It was in July of 2015 that a hacking group calling themselves the Impact Team, and that was actually well named, released the internal database of Ashley Madison, which was a website aimed at those wishing to have a relationship outside of their marriage, an "affair," as they're called.  Whereas most breaches today may expose our username and a hashed password for websites we may not even recall visiting, the Ashley Madison breach exposed people's real-world lives; and, sadly, a few committed suicide after being publicly outed as having an account on the site.  So there was a breach that had some very real-world consequences.



LEO:  Yeah.  We've since learned that most of the women on Ashley Madison were just bots.  It was almost all guys.



STEVE:  Yeah.  Surprise.



LEO:  Yeah, what a shock, huh?



STEVE:  So the practice known as SIM swapping, where hackers contact a mobile provider and trick the providers' personnel into transferring a victim phone number to a SIM card controlled by the attacker, first surfaced also in 2015.  The initial SIM swapping attacks were linked to incidents where hackers reset passwords on social media accounts or grabbed a highly sought-after username.  But as with Willie Sutton, once hackers realized that they could also use the technique to gain access to cryptocurrency or people's bank accounts from where they could steal large sums of money, the practice became much more of a nuisance, and there was a lot more pressure to authenticate that kind of attack.



It turns out that one of the problems is that our U.S. mobile carriers would prefer to do this online or over the phone, rather than requiring an in-person visit, as is typically required outside the U.S.



LEO:  Really.



STEVE:  That's one of the - yes.



LEO:  Oh, that's interesting.



STEVE:  That's one of the reasons that it was much more prevalent here is that you do some weak proof of identity using information which can also be hacked, and then you can escalate your attack.



LEO:  They come to your house to deliver the SIM?  Or you probably have to go to the store.



STEVE:  Yeah, you have to go in, in person, and show identity and so forth.



LEO:  Sure.  That's so much - why don't...



STEVE:  I know.  I know.



LEO:  We should really be doing that.



STEVE:  It's like banks these days.  No one wants to see you.



LEO:  Yeah.



STEVE:  It requires money to have a teller standing behind a counter somewhere.  Also, in December of 2015, that's when the cyberattack on Ukraine's power grid occurred, causing power outages across Western Ukraine, making it the first successful attack on a power grid's control network that had been recorded.  Although Stuxnet and Shamoon - that was a related attack - were the first cyberattacks against industrial targets, the Ukraine incident was the first one impacting the general public.  It opened everyone's eyes to the dangers that cyberattacks could pose to a country's critical infrastructure.  And as we know, the threat continues to loom today.  We often hear of the idea of our own electrical grid being taken offline, and the presumption is, yeah, if bad guys wanted to do it, it's just not that well protected, unfortunately.



2016, it was the spring of 2016 that the Democratic National Committee, the DNC, admitted that it had suffered a security breach after a hacker going by the name of Guccifer 2.0 started publishing emails and documents from the organization's servers.  It was later determined that the DNC had been hacked, not by one, but by two Russian bears, cyberespionage groups Fancy Bear (APT28) and Cozy Bear (APT29).  The data that was stolen during the hack was used, as we know, in a carefully staged intelligence operation with the intent of influencing the upcoming U.S. presidential election.  So that DNC hack certainly ranks as one of the bigger events, a cybersecurity event for the U.S., at least, that occurred during the decade because it's continued to have repercussions in U.S. politics.



It was also in 2016 that Yahoo admitted that it had suffered two data breaches in the span of four months, including one that would turn out to be the largest breach in the history of the Internet.  Whoopsie.



The Shadow Brokers.  Although to this day we still don't know who they are, boy, did they have an impact.  It was between August 2016 and April 2017 that the group calling themselves the Shadow Brokers first teased, then auctioned, and then leaked the hacking tools developed by the Equation Group, which we know is the codename for the U.S. National Security Agency, our NSA.  These tools were top-quality hacking tools which made an immediate impact.  A month after the final Shadow Brokers leak, one of those tools, EternalBlue, gave the WannaCry worm the teeth it used to wreak havoc across the global Internet.



It was a blog post in early September of 2016 which introduced the world to Mirai, a strain of Linux malware designed to work on routers and smart Internet of Things devices.  During the 90 days that followed, after being used to launch some of the largest DDoS attacks ever seen, Mirai would become one of the most well-known malware strains in the world.  And after its source code was released online in an attempt by its author to disavow its own authorship, it became one of today's most  widespread malware families, with its code being the foundation of most IoT/DDoS botnets in use today.



This podcast has used two slogans as the result:  "The 'S' in IoT stands for security," and "IoT is short for 'Installation of Trojan.'"  And unfortunately IoT is going to be a challenge moving forward.  And Leo, we should pause just for a second.  I didn't pick up on the news of Apple and Google teaming up to produce an IoT security standard?



LEO:  Isn't that wild?



STEVE:  Did you talk about that on...



LEO:  Oh, we talked about it on TWiG, and we talked about it on MacBreak Weekly today.



STEVE:  Somehow I missed that.



LEO:  Yeah, it's, well, you know how standards bodies are.  We'll see.



STEVE:  It's an initiative.  It's a committee.



LEO:  And security is going to be a big part of it, by the way.  But I think really what it is, is an acknowledgment that the IoT space can't really take off if you have...



STEVE:  It's too fragmented?



LEO:  Yeah, so many protocols.



STEVE:  Ah.



LEO:  So even Z-Wave in response announced they're going to open source Z-Wave.



STEVE:  And so interoperability is the goal.



LEO:  Yes, exactly.



STEVE:  Ah, okay.  Well, that'll be really nice, too.



LEO:  I hope, if that happens.  But I have my doubts.  I think you get these big companies sitting down at the table, they're going to say, "Well, I want this."  "Well, no, you can't have that.  I want this."  We'll see if they actually ever agree on anything.  We'll see.  I hope so.



STEVE:  So it was May 2017 that WannaCry first swept across the Internet, which was as we know fueled by the Shadow Brokers leak of the NSA's EternalBlue exploit against Windows file sharing.  We now know that WannaCry was also developed by North Korean hackers looking to infect companies and extort ransom payments as part of an operation to raise funds for the sanctioned Pyongyang regime.  So okay, way to make money, guys.  Yeah, wow.



Vault 7 was WikiLeaks' last good leak.  It was a trove of documentation files describing the CIA's cyber weapons, sort of the equivalent on the CIA side of the Shadow Brokers leak over on the NSA side.  No source code was ever included; however, the leak provided a look into the CIA's technical capabilities, some of which included tools to hack iPhones, all the major desktop operating systems, the major browsers, and even smart TVs.  At the time, WikiLeaks said it received the Vault 7 data trove from a whistleblower.  Later he was identified as Joshua Adam Schulte.  So that was also in '17.



And then the MongoDB.  Although incautious sysadmins had been leaving databases exposed online without any password for quite some while - after all, iCloud, the cloud in general had already happened - 2017 was the year when hackers finally turned their attention to this previously untapped Internet resource.  It began in the tail end of 2016 and really picked up steam by January of 2017 with hackers accessing databases, deleting their content, and leaving ransom notes asking for cryptocurrency to return the nonexistent and previously deleted data.



Although the first wave of attacks targeted what was essentially low-hanging fruit of MongoDB servers, hackers later expanded their reach to encompass other database technologies including MySQL, Cassandra, Hadoop, Elasticsearch, and others.  Although widespread attacks died out by the end of 2017, they served to highlight the significant dangers of publicly exposed misconfigured databases.



LEO:  You aren't used to talking for so long without a break, are you.



STEVE:  No.



LEO:  You want me to do a phony ad?



STEVE:  No, it's good.  Got a piece of phlegm.



LEO:  Oh, yeah.  I know how that feels.



STEVE:  Yeah.  Anyway, so I'm still talking about what a problem these databases are.  By the end of the year, a new category of researchers known as "breach hunters" had been born.  These were individuals who looked for open databases and then contacted companies to let them know they're exposing sensitive information online.  Oh, by the way, I just thought you'd like to know.  During 2018 and 2019 most security breaches and data exposures are now being discovered by breach hunters, rather than hackers dumping a company's data online after an intrusion.



LEO:  That's good.



STEVE:  So it's nice that we have breach hunters.  It's dumb that all these databases just keep being left exposed online.  It's like, dummies.  Yeah.



And 2017 was also the year of the Equifax hack, in which the personal details of more than 145.5 million American, British, and Canadian citizens were stolen from the company's systems.  We know that the breach was the fault of Equifax failing to patch the Apache Struts vulnerability for about 90 days.  I think the breach occurred about 90 days after Apache Struts' vulnerability was announced.  And they just hadn't gotten around to it.  We still don't know who was behind the intrusion, nor what their motives may have been.  But it was a big lesson about the need to patch because this was of course extremely expensive in terms of reputation damage and real-world lawsuits against Equifax.



And then what would 2017 be, Leo, without Coinhive?  It was the latter half of 2017 that Coinhive appeared, and the term "cryptojacking" was coined.  The Coinhive service, as we know, enabled hackers to make money by mining the Monero cryptocurrency on other people's computers after somehow arranging to load a snippet of JavaScript into a victim's browser.  If nothing else, just putting an ad up, just buying an ad that had this JavaScript in it.  When the ad was on someone's browser, their machine would be busily part of a mining pool and generate a little incremental piece of a Monero coin.



And for a while during that crazy, like, bitcoin was at $20,000 per coin, during that crazy peak it made sense.  Unfortunately, nobody liked having other people using their computers to mine coin, so the browsers actively pushed back on the practice.  And then after the collapse of cryptocurrency valuations, it rendered the whole scheme much less profitable.  And of course, as we know, it was later in 2018, I think it was 2018, that Coinhive shut their doors.  And then, as we covered at the time, it wasn't long before somebody else jumped in to offer Monero mining services to the bad guys.



Coming up with the big security event of 2018 is a no-brainer when you have the likes of Meltdown, Spectre, and the myriad other attacks which were subsequently discovered to be theoretically effective against our modern processors' speculative executing microarchitectures.  Even though not a single one of these was ever found to be exploited in the wild, by the time we had learned that, given time, well, by the time we had learned - I got my phraseology here in what I wrote doesn't make sense to me.



Even though not a single one of these was ever found to be exploited in the wild - well, I still don't know what I'm trying to say here.  By the time we had learned that - oh, I see.  By the time we learned that, given time and sufficient motivation, bad guys will arrange to leverage any - oh, I see.  By this time we had learned that, given time and sufficient motivation, bad guys will arrange to leverage any perceived weakness into a working exploit.  So every single one of these fundamental microarchitectural mistakes needed to be, and have been so far, cleaned up.  Future CPU design has been hugely impacted as a result, at the cost of some hopefully short-term performance.



And Leo, you've often said this, and I concur completely.  It is a little bemusing that, yeah, all of this hay has been made of Spectre and Meltdown and all their ilk during the last really full two years, 2018 and 2019.  Yet as far as we know, not a single instance of these vulnerabilities has ever actually been used against anyone.  Maybe it would have been an attack that allowed VM boundaries to be breached, though even then it was only theoretical.  I mean, you know, it was demonstrated.  Proof of concepts were demonstrated.  So, yeah, you don't want that to be in a cloud system where people who you don't trust are able to run code on your shared hardware.  But it certainly never had any impact on end users, despite the fact that end users have suffered a performance hit.  So anyway, clearly the event, the security event that stands out for 2018 was Meltdown, Spectre, and all of the other speculative execution attacks.



And this was also when we learned of the Starwood/Marriott data breach.  Though not as big as Yahoo's three billion figure, the Starwood/Marriott data breaches deserve a mention, if for no other reason due to its sheer size and the fact that a lot of personal information, like passport information, was lost.  The breach, which was disclosed in November of 2018, impacted around 383 million of the hotel chain's guests.  At the time, the first estimate you'll remember was 500 million.  But after further forensic investigation they said more like 383 million.  Forensic investigators dug into the Starwood reservation system, and they discovered a RAT, a Remote - they found a rat in the system - a Remote Access Trojan and the Mimikatz tool, which is used to sniff usernames and passwords.



There's never been any public disclosure of the route into the Starwood systems.  So it's entirely possible that only the attackers know how they originally got in.  But this was, again, another instance of an APT, an Advanced Persistent Threat, because those guys were there for quite a while, exfiltrating all of this data.  And in fact remember it was only when an auditing tool happened to be on the network that saw an odd query of the database that tripped a filter that said "here's an odd query."  And then when it was looked into more deeply, the username of the query was not one that was online or present or something at the time.  So then they dug deep, and they ended up saying, oh, crap.  Not only do we have a big problem, we've had a big problem for quite some time.



And I will wrap this up with 2019, just declaring it the year of the ransomware.  I think that has been really the overarching story of the year because bad guys have figured out that they can encrypt data on exposed systems.  There's now huge pressure to get into systems, to encrypt data, and make that happen.  We also have this new sort of multilevel marketing model that GandCrab pioneered where the bad guys syndicate the attack, essentially, by giving the syndicatees, or syndicators, I guess, yeah, the syndicators - would it be "ees" or "ors"?  The person who syndicates is a syndicator.



LEO:  The content provider is the syndicator.  The content receiver is the syndicatee?  I don't know.



STEVE:  Yes.  Yeah, I think so.  I think that's right.



LEO:  I think neither word makes any sense.  It sounds like gobbledygook to me.



STEVE:  Anyway, the point is that, as we know because we've talked about it a lot, the idea is the bad guys behind GandCrab who initiated this get a whole bunch of minions to go install the GandCrab malware; get it into other people's systems; and, very smartly, give them the lion's share of the ransom, which keeps the minions incented to infect other people's systems with this malware.  Meanwhile, the GandCrab guys deal with all of the backend.  They negotiate...



LEO:  It's more like a franchisor and a franchisee.



STEVE:  Yeah, I guess you're right.  It's a franchise, yes.



LEO:  Yes, a malware franchise.  Be the first on your block.  



STEVE:  And that's 2019 in a nutshell.  As a consequence of that, we saw a huge number of attacks throughout the year, ransomware attacks.  And the problem is, since much of the way in is social engineering, and that's unfortunately not something that technology can completely solve,  I have a feeling we're going to be seeing more of that in 2020.



LEO:  Yeah.



STEVE:  And there, Leo, is our decade in review.



LEO:  That's the decade.  You heard it here.  All 10 years in review.  What do you think 2020's going to look like?  It's definitely ransomware.



STEVE:  I think it's ransomware, and I think it's IoT.  I think that after Christmas - this is going to be the IoT Christmas.  And we're going to...



LEO:  Oh, you're right.



STEVE:  And it's going to be the IoT death in January.



LEO:  Oh, you're right.  Very interesting.  One thing that, you know, if I look at the decade from the overview, that most impresses me is the ingenuity of hackers and their willingness, like water, to find whatever cracks there are.  They just flow; you know?



STEVE:  And Leo, some of these attacks are phenomenally impressive.



LEO:  Pretty sophisticated.  Yeah, yeah.



STEVE:  Oh, my god.  I mean, if I, like, ever read every science fiction book so that there were no more, and the Internet - well, okay, wait, it can't go away because then I wouldn't have it to - the point is, way down on my list is having the time to invest in reverse engineering other people's work in order to find flaws.  Because, I mean, I like to read.  I like science fiction.  And there's TV, and there's movies, and there's podcasts.  It's like, you know, all this other stuff.  But, boy, I am so impressed with the work that these hackers and attackers do.  It is, I mean, if they pointed their skills to being productive, rather than being in the dark underbelly, we'd have, like, IoT security and things that would be really useful.  But no.



LEO:  But no.  Steve, what a year it has been.  What a decade it has been.  It is always a pleasure as we head into our third decade of the shows.  I always look forward to our Tuesday afternoons together, 1:30 Pacific, 4:30 Eastern, 21:30 UTC.  That's when we do Security Now!.  You can watch us do it live at TWiT.tv/live.  Today's show we did a day early because we didn't  want to work on Christmas Eve.



STEVE:  Better than a day late.



LEO:  Yeah.  And next week's show is New Year's Eve, and we won't be doing a show then.  We'll be doing a "best of."  And we've got the highlights of 2019 all put together by our producers and our editors.  But you and I will reconvene in the new year.  Actually, I won't be here in the first Tuesday of the new year, which is...



STEVE:  Really.



LEO:  Yeah, I'm going to be in Vegas for the Consumer Electronics Show.



STEVE:  Oh, CES, baby, CES.



LEO:  So we'll be back January 7th, but it'll be probably Jason  Howell.  And then I'll be back with you on the 14th.



STEVE:  Well, you know what I'm watching, Leo.



LEO:  What are you watching?



STEVE:  I'm watching the count of Security Now! episodes because we're now at 746.  In four more we'll be at 750.



LEO:  Yeah?



STEVE:  And then we start the final quarter.



LEO:  It's the final countdown.



STEVE:  The wind down, the final - although it's not like it's going to be happening fast, the final 250 episodes.



LEO:  Take a few years.



STEVE:  Yeah, that's right, another five years, that ought to do it.  And by that time IoT will be secure.  Ransom will have been cured.  Really, we'll be just like, you know, we'll have - well, you'll still have hair.



LEO:  By that time it'll take two people, three people, maybe four people to do the show just to keep up.



STEVE:  Perfect.  Then I will happily hand it off to them.  But not to worry.  Not for another five years, everybody.



LEO:  Whew.  Steve, always a pleasure and an honor to work with you.  It's been a great year.  Thank you for the - I know you put so much work into this, and I really - we're all very grateful that you do.  It's one of our most popular shows, and people just love hearing what's going on from you.  I have a fellow in the studio, he came here for the show today.  He works in security at a major social network.  Actually, not exactly security.  He's a developer.  But security is one of your issues, absolutely.



STEVE:  Has to be these days, yeah.



LEO:  He listens to the show to make sure that he's up on everything going on.



STEVE:  Very cool.



LEO:  Well, have a wonderful holiday.



STEVE:  Okay, my friend.  Have a great holiday and a New Year.  And try to not walk too much during CES.  I know those are exhausting shows.



LEO:  I'll try to come back healthy.  That's going to be the biggest trick.



STEVE:  And I love 2020.  I just like the shape of that.



LEO:  Isn't that a great year?  Yeah.



STEVE:  2020 looks like a great year.



LEO:  Feels like the future.



STEVE:  With any luck that'll be the kind of vision we have:  2020.



LEO:  Nice.  Thank you, Steve.  GRC.com.  That's his website.  Get the show there or at TWiT.tv/sn.  Subscribe.  And don't forget SpinRite, the world's best hard drive recovery and maintenance utility.



STEVE:  And just for our listeners, those who've hung on here to the very end, I am deep into bringing up the new Unix server and moving - I'm now in the challenge of moving the newsgroups over to the new hardware.



LEO:  XenForo runs on Windows only; right?



STEVE:  No, it's a PHP.



LEO:  Okay, so anywhere.



STEVE:  But these are not the SQRL forums.



LEO:  Oh, okay.



STEVE:  This is the old-school NNTP.  You've got to have a news reader-style newsgroup.  That's where all the real work happens at GRC is really deep under.



LEO:  That's good.  It keeps the plebeians out.



STEVE:  It does.  It was funny, we used to have - I had a browser, I do have a read-only browser interface.  And it used to be read-write.  And we had these problems with just all this crap being posted in the newsgroups.  And I don't think I can take credit for it, but somebody noted that all the crap was coming in through the browser interface.  And I said, oh, problem solved.  I just set it to read-only, and it was much better.



LEO:  Much better.



STEVE:  Much better.



LEO:  Much mo' betta.



STEVE:  Okay, my friend.  Next year.



LEO:  Thank you, Steve.  See you next year.



STEVE:  We'll do it again.  Bye.



Copyright (c) 2019 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#748

DATE:		January 7, 2020

TITLE:		Our Malware Lexicon

HOSTS:	Steve Gibson & Jason Howell

SOURCE:	https://media.grc.com/sn/sn-748.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This first podcast of 2020 we look at a proposed standard for creating machine-readable warrant canaries.  We also take a precautionary lesson from a big Xiaomi blunder, examine Microsoft's research into brute forcing RDP, look at the continuing problem at the point of sale, follow up on Russia's plan to disconnect from the Internet, consider the end of life of Python 2.7, review the top 20 HackerOne bounty payers, warn of some bad new SQLite security vulnerabilities, and cover a bit of sci-fi, SQRL, and SpinRite miscellany.  Then we group all malware into a seven-member lexicon and quickly run through each one.



SHOW TEASE:  This week on Security Now! - should I say "this decade"?  We'll start the show with a little decade talk.  Steve breaks down the lexicon, the terminology associated with malware.  Super interesting stuff there.  Also standards around warrant canaries; how Xiaomi cameras actually sent random private video feeds to Nest Hub users, not a good look; HackerOne's top 20 bug bounty programs; problems with point-of-sale systems; and a whole lot more, coming up next with Steve Gibson on Security Now!.



JASON HOWELL:  This is Security Now! with Steve Gibson, Episode 748, recorded Tuesday, January 7th, 2020:  Our Malware Lexicon.



It's time for Security Now!, the '20s edition.  I'm Jason Howell, filling in for Leo Laporte, who's in Vegas.  Joining me as always, Steve Gibson, the amazing Steve Gibson.  How are you doing, Steve?



STEVE GIBSON:  The indefatigable Steve Gibson.



JASON:  Why aren't you in Vegas for CES?



STEVE:  You know, this is one of the first things that - okay.  So first of all, I don't think I ever missed a COMDEX back in the day.



JASON:  Ah, I remember COMDEX, yeah.



STEVE:  That was the big computer dealer expo.  That was the PC industry show.  And I did a CES when I was with Koala, after selling them the light pen.  They did the KoalaPad back in the early days, an early touch stylus.  And I've been to a bunch of CESes.  That used to be a thing.  But for me, what the Internet does is free you from the need to go to all these things.



JASON:  True.



STEVE:  And I hear people saying, "Oh, Gibson's not a real security guy.  He never goes to conferences."  It's like, no.  I mean, yeah, I get it that it's nice to meet people and shake hands and hobnob and, you know.  And if I were still in my 20s and 30s I'd probably think that was really cool.  But we have a complete seat to CES just by looking at what everybody is posting about CES.  You'll end up with a much better sense for the show than if you tried to do it on foot.  And besides, I'm busy.  People would much rather have me finishing up the work I'm doing on revamping our servers at Level 3, preparing to get into the return to work on SpinRite, than wandering around and ending up with sore feet at the end of a long day of CES.  So priorities.



JASON:  Completely understand.  Completely agree.  I've had the years of going to CES year after year after year and running on that treadmill endlessly.  And then I've had of course in the last however many years here at TWiT, we've really taken many of these years kind of the "sit back and view it" approach.  And you're right.  It's kind of like the perfect example of seeing the forest for the trees; right?  Like when you're at CES, you're seeing all the trees, and it's really cool.  It's good for one thing because you can have an up-close experience with some of these things that we hear about.  But standing back, there's benefit there, too, because you just get this perspective of everything on equal footing.  Some of the noise is gone, so it's kind of like you see it for what it is.  I don't know, there's benefits to both sides.  I suppose it really depends.



STEVE:  Well, I also don't subscribe any longer to lots of paper magazines.  Popular Electronics, Popular Science, Scientific American, I mean, I used to do that, too.  But then the Internet happened, and like, hello, this is better.  So, yeah.  I'm glad they're doing the shows.  I'm glad Leo and a camera crew are there, and TWiT is going to bring us CES.



JASON:  That's right, that's right.  They're churning out a whole bunch of content to find.  And of course TWiT.tv you'll find all the content published on the site here.



STEVE:  So, yeah, of course.  So anyway, I thought for the first episode of this, whatever it - okay.  You and I were talking before the show that technically this is not the beginning of a new decade.  This is like the "are you counting from zero or one" problem that bites so many programmers in the butt throughout their entire programming history.



JOHN SLANINA:  I'm sorry, Steve.  I'm sorry, Steve, I have to interrupt.  I really feel strongly about this.



STEVE:  Okay.



JOHN:  A decade is 10 years.  You can start a decade at any point in time.



JASON:  That is true.



JOHN:  What you cannot say is this is the 201st decade.  That is the only thing you cannot say.



JASON:  Ah, okay.



JOHN:  It's not the beginning of the 201st decade until next year.  But any other - you can start a decade any time you want. 



JASON:  We can start a decade right now.



STEVE:  We are, for example, in the second decade of this podcast.



JASON:  Woohoo!  Yes.  That makes sense.  That is true.



STEVE:  So anyway, I think it was Sophos who did sort of an interesting - they called it a virus, and they're not really viruses.  It's more generically malware.  But they divided all of the malware that we know into seven categories.  And so I thought it would be interesting at the end, after catching up with a bunch of interesting news of the week, as we always do, to just sort of scan through this division.  I called it "the lexicon," so this podcast is titled "Our Malware Lexicon."  That is, the common sort of aggregation of malware that has now sort of settled upon us.  So that's how we're going to end things.



But we're going to first take a look at a really interesting proposed standard for creating machine-readable warrant canaries.  And we'll talk a little bit about what warrant canaries are and how they happened, also, just to give our readers a little bit of background.  We also have a precautionary lesson about a big Xiaomi blunder.  We're going to examine Microsoft's research into brute-forcing remote desktop protocol and why they still don't have it right.



We're going to look at the continuing problem at the point of sale, follow up on Russia's plan that we've been watching and following for the last few years to disconnect itself or be able, at least, to disconnect itself from the Internet.  We're going to look at the end of life of the beloved Python v2.7; review the top 20 HackerOne bounty payers; warn our listeners, as we often do, there's like one definite oops, pay attention to this one, bad new SQLite security vulnerabilities; cover just quickly a little bit of sci-fi, SQRL, and SpinRite miscellany; and then take a look at our malware lexicon.  So buckle up.  I think we're going to have a good podcast.



JASON:  Buckle up.  If today is any indication...



STEVE:  As we kick off whatever this is, the decade or the who knows what.



JASON:  I think we now know it is the decade.  It is the decade that we have defined because anything could be a decade.



STEVE:  So speaks John from his booth of authority, yes.



JASON:  And I love that he did that because as far as I'm concerned, JammerB had the right answer for that question.  That's how I feel.  And I also appreciate that we can now say it's "the 20s" because for all of my life "the 20s" has felt old-timey.



STEVE:  The Roaring Twenties.



JASON:  And now it's like the future.  So that's kind of weird.  All right.  Do we start with this picture?  We've got a little picture here.  It's a pretty picture.



STEVE:  Yes, we do.  I just sort of grabbed it.  I thought it was kind of a cute logo which was part of this - it's called the CanaryTail on GitHub.  It's their logo for the page to deal with this proposed machine-readable warrant canary.  So I just thought that was a great Picture of the Week since it ties in with our first story.  Of course the term "canary," as we know, comes from the idea of taking a canary down into a coal mine as a low-tech oxygen and noxious gas monitor.



Wiktionary defines "canary in a coal mine" as "something whose sensitivity to adverse conditions makes it a useful indicator of such conditions, something which warns of the coming of danger or trouble by a deterioration in its health or welfare."  And I guess actually for some people bunions are supposed to be able to tell you of an oncoming storm or something; right?  Anyway, I don't know if that quite qualifies as a warrant canary.  But okay.



So the trouble was that government warrants compelling the release of information were typically accompanied by gag orders enjoining the party served with the warrant from making any disclosure about being in receipt of such warrant.  So it was you must provide us with the following information, and you cannot tell anyone that we asked you and you provided.  This was passed in 2001 as part of the U.S. Patriot Act, and it enabled authorities for the first time to ask for personal information stored by service providers about U.S. citizens and also allowed them to issue gag orders that would prevent the organizations from telling anyone about it.  So it meant that government could essentially access an individual's private information without that person knowing.



And, you know, you can understand why the government would want that.  That was, for me, that was like sort of the first writing on the wall that encryption was in trouble because we can pass whatever laws we want to in this country, and then that's the law, as they say.  And, boy, I mean, Congress is sure dragging their heels.  Law enforcement - well, anyway, I won't get onto the encryption debate.  We certainly spent a lot of time covering that in 2019.



In this case, privacy-respecting companies like, well, anybody, ISPs, any kind of service provider, anybody who holds people's data and wants them to be able to trust it, want their users to know whether the government is asking for the information that they've promised not to provide.  So the idea of a warrant canary, I did a little bit of background research because I was curious, was first conceived by a guy named Steve Schear in 2002 after the Patriot Act came into effect.  And his idea, which I think is sheer brilliance, was to provide a passive way, rather than an active way, which was illegal, for warning people that an organization holding their data has received a subpoena.



So instead of telling people that they had been served with a subpoena, which became illegal under the Patriot Act, his clever idea was to reverse that and stop telling them, instead, that they had not been served with a subpoena, which apparently was not illegal because I don't think you could compel someone to say something.  You could just tell them what they can't say.  So it was very cute.



Anyway, so to do this, the organization would typically display a public statement online that it would only change if the authorities were to serve it with a warrant.  And so as long as the statement stays unchanged, individuals checking it would know that their information was safe.  So if a "nothing to see here, move along" statement were to change or disappear, then people could infer that not all is well, so the organization doesn't have to explicitly say so.



So that idea is clever, but until now it has not scaled well.  Hopefully that may be about to change.  But there can be problems.  A classic example is what happened about a year and a half ago with SpiderOak, that is, a high-reputation neat provider of cloud-based encryption services, during the summer of 2018.  They essentially were forced to walk people back from the ledge after concerns were raised because they changed their warrant canary.



On August 18th of 2018 they posted on Medium hopefully a clarification.  They said:  "Over the weekend there has been chatter on the Internet about the change at SpiderOak from a Warrant Canary to a Transparency Report.  We understand," they wrote, "that some people are concerned that this is a signal that we have in some way been compromised."  So they said:  "To be completely clear, nothing has changed other than the way we report our interactions with the government from the first time we posted a canary" - in this case it was four years ago - "in August of 2014."



Then they said separately, on a separate line, "We have received zero search warrants, zero subpoenas, zero court orders, and zero national security letters.  Even better for our customers, we could not hand over their data even if we were asked to.  The No Knowledge approach that SpiderOak uses" - that's the so-called TNO, Trust No One approach of pre-encryption technology - "means that we don't have the keys to decrypt the data you trust us to store for you."  And they finished:  "Thank you for choosing SpiderOak and deciding to trust in cryptography instead of promises."  And of course we know that's my philosophy, as well.



But anyway, that highlights the problem of there not being a standard.  And what happens even with a well-meaning change can be like, oh my god, oh my god; you know?  Does that mean the canary died?  It's like, okay, no.  Okay.  So there is now a proposed standard on GitHub.  It was put up by a GitHub user using the moniker "carrotcypher," C-A-R-R-O-T-C-Y-P-H-E-R.  And it was inspired, that is, this proposed standard, inspired by the work of organizations like the Calyx Institute, which is a technology nonprofit that develops free privacy software and what is now a defunct Canary Watch project from the EFF, the Electronic Frontier Foundation.  Also the Freedom of Press Foundation, NYU Law, Calyx, and the Berkman Center.



Anyway, so Canary Watch listed and tracked warrant canaries.  When Canary Watch was shut down, the EFF explained:  "In our time working with Canary Watch we have seen many canaries go away and come back, fail to be updated, or disappear altogether along with the website that was hosting it.  Until the gag orders accompanying national security requests are struck down as unconstitutional" - which the EFF believes they are - "there is no way to know for certain whether a canary change is a true indicator.  Instead the reader is forced to rely on speculation and circumstantial evidence to decide what the meaning of a missing or changed canary is."  Just like we saw in the case of SpiderOak.



So the hope of this CanaryTail is to change this.  As it explains on its GitHub readme.md page, they said:  "We seek to resolve those issues through a proper standardized model of generation, administration, and distribution, with variance allowed only inside the boundaries of the defined protocol."



So instead of some arbitrary language on a website, the warrant canary standard would be a well-defined JSON - that's J-S-O-N JSON, not J-A-S-O-N Jason.



JASON:  It gets me every time.



STEVE:  I know.  Your ears perk up - a JSON-formatted text file which is convenient since it's readable both by people and by machines.  The CanaryTail file would include 11 codes with a value of zero for false or one for true.  These codes would include things like W-A-R for warrants, G-A-G for gag orders, and T-R-A-P for trap and trace orders, along with another code for subpoenas and so forth, all having specific legal meanings for an organization and its users.  Naturally, a value of zero adjacent to any of these keys would mean that none of the warnings had been triggered.  If the code changes to one, it's a cause for concern.



And of course the beauty of this is presumably that these files would have a common name, would be in the root directory of the domain hosting the canary, and this would allow for automated verification.  And if this really did take off as a standard, you can imagine, for example, our web browsers might be - or certainly an extension.  I guarantee you that there would be an extension for our web browsers which would poke at the canary and bring up some sort of a warning with an icon jumping up and down if a non-zero item was found on one of these files.  So it's a cool idea.



The file also contains some interesting codes including DURESS, which would indicate that the organization is somehow, in some way, being coerced; along with codes indicating that they've been raided.  There's also a special code - I love this - indicating a seppuku pledge.  I first encountered the term on "Serenity," that wonderful movie that followed the "Firefly" series.  Seppuku, as in the ancient Japanese suicide ritual.  In this case the seppuku pledge that would be contained in the warrant canary file would be a pledge that an organization will shut down and destroy all of its data if a malicious entity were to somehow take control of it.  In other words, it would kill itself before it would allow itself to be abused, and the knowledge, the information that it contains.



And of course because these days you can't have anything good without somehow tying in Bitcoin's blockchain into the whole thing, the proposed standard must also be cryptographically signed, including a public key, with an expiration date, and must include a block hash from the Bitcoin blockchain to verify the freshness of the digital signature.  As another safeguard, it would include a panic key field with another public key.  If the file is signed with that key, people could interpret that as a kill switch, causing the warrant canary to fail immediately.  That would be useful if an organization were to suddenly get raided and can't afford to wait until the current canary file expires.  So anyway, a lot of thought obviously put into this idea.  And who knows?  We may end up with a standardized warrant canary which our browsers could be checking for us whenever we go to a website that has one, which might be kind of cool.



JASON:  So if our browsers are checking for it, then I would imagine - so let's say an organization has to switch one of these flags so that people kind of know if something has happened, law enforcement or whatever the cause is of that.  They know, as well; right?  Like the public knows because the browser is identifying this.  But it's no secret to them, either; right?



STEVE:  Right.



JASON:  They're going to know this, as well.  So, I mean, is there - I guess I don't know what my question is.  Pardon me, I'm a little foggy.  I've got a cold.  But, I mean, that kind of takes some of the secrecy out of it.  I guess when I've always thought of the canary, the warrant canary, I've always thought of this, like, this mysterious hidden kind of signal to everyone else that's in the know that, oh, we know how to crack the code.  We now know that there's something that's happening to this site or this organization that maybe they don't know that we know.



STEVE:  Yeah.



JASON:  Does that make sense?



STEVE:  So, yeah, it does.  And I think where your previous understanding differs from the way it's been applied is that the canary is useful because people have a way of discovering it.  That is, maybe in their privacy statement they will say, oh, and just as a verification, keep an eye on our warrant canary.  I mean, so the idea that a site would be hosting a warrant canary, that itself is not secret.  And so, for example, that's why when SpiderOak changed theirs from explicitly calling it "here's our warrant canary" to "here's our transparency report," people all freaked out.  Well, of course they were only able to freak out because they knew that it had been previously called the SpiderOak warrant canary.



So the idea was that it was supposed to be something that was discoverable by anybody who was interested, and it would be up to them to keep an eye on it.  Or like, for example, maybe before they would go back to update their software with a new version they might, if they were of the tinfoil hat-wearing ilk, go see if the site has a warrant canary attesting to the fact sort of passively that, yup, you know, everything is still fine.  Nothing to see here.  Move along.  Anyway, so it was never really meant, you know, they weren't meant to be secret.  They were meant to be discoverable by somebody who wanted to poke around in their privacy statements and see what they could find.  Or be also passed from person to person.  Like hey, you know, make sure you check out the site's warrant canary before you trust them overly.



JASON:  Right, yeah.  Okay, that makes more sense.



STEVE:  So this is one I'm glad...



JASON:  I love this one.  I'm so happy you put this in here, thank you.



STEVE:  I'm glad you knew of it and you were hoping that we would talk about it.  This puts the "wow" in Xiaomi.



JASON:  Yes, it does.



STEVE:  As we know, recently a number of, well, what we're finding out is that a number of users of Xiaomi's smart, or now it seems apparently not so smart, IP-connected IoT cameras - get a load of this - have been receiving and watching other Xiaomi users' video feeds.  Whoops.



JASON:  Ouch.



STEVE:  For whatever reason, what is clearly an unintended bug appears to only affect Xiaomi IP cameras when they're streaming through Google's Nest Hub.  The problem first came to light when a Reddit user claimed that his Google Nest Hub was apparently pulling random feeds from other users instead of his own Xiaomi Mijia, M-I-J-I-A, cameras.  He shared some photos showing other people's homes, an older adult sleeping on a chair, and a baby sleeping in its crib, all which appeared on his Nest Hub's screen, unbidden.  So the problem does not appear to lie in anything that Google did in its implementation, but rather is something about the way Xiaomi implemented their connection between their smart cameras and Google's Nest Hub.



In a statement Google made which was provided to the publication "Android Police," Google said it's aware of the issue and is in contact with Xiaomi to fix the problem, although Xiaomi had not yet responded to Google, which I think was a mistake because, in the meantime, as a precaution to protect its Nest Hub users, Google has temporarily disabled all Xiaomi devices that are attempting to get to the Internet through the Google Nest Hub.  So that will certainly get the attention of Xiaomi's users and no doubt get the attention of Xiaomi, as well.  The exact details surrounding the bug are still unknown, but it does not appear to be affecting all Xiaomi cameras which have been integrated with the Google Nest Hub.  So it's a mystery.  Who knows what the story is.



I would suggest and recommend that, until patches arrive, that users unlink their cameras from Google Nest Hub - that is, Xiaomi users, only Xiaomi users, unlink their Xiaomi IP cameras from Google Nest Hub, if that's the thing that you're using to tie them to the Internet - and wait, as I said, for patches.  And of course the takeaway for our podcast is always to keep in mind that in the past, in 2019, and moving forward into 2020, any connection to the cloud truly does remain a high-risk business.  It just is.  Everything that we see inherently suggests that.



Most organizations want to offload the burden of using the cloud from their users, which is why, for example, Dropbox and Microsoft, while they encrypt their customers' data at rest, they have the keys.  Dropbox, their terms of service specifically say "Our employees may need to look at the contents of what you're storing in your Dropbox in order to verify it meets the terms of service."  So they can obviously see it.  So we're still in the early stages, it feels to me.  Perhaps we will eventually, as a society, as an IT environment, we will eventually learn how to secure the cloud in a way that's useful.



But we're about to talk about Microsoft's Remote Desktop Protocol and find that they still haven't figured it out, which is amazing to me.  But it's very clear that we're not there yet, and that products and services are being launched and offered with inadequate attention paid to security even now.  So whether you're outsourcing your management functions to a third party who then becomes a conduit to carry destructive ransomware into and throughout your organization, or you're using cloud-connected surveillance cameras and lord only knows who can also see whatever the cameras are aimed at, there's just no arguing that, while there are absolutely significant upside benefits, so far they don't come without some significantly balancing downside risk.



So at this point it's the Wild West, and the technology does cool stuff.  You know, it's neat that you're able to look out from your front door of your home while you're at the office.  There are secure ways to do that, that don't involve the cloud, that would arrange to only allow you and your office to see your video.  But it takes a lot more configuration; and it's not the "oh, look, I just plugged it in, and suddenly it all works" experience that users are used to now.  The flipside of that is, yeah, other people may be getting your video because of a bug.  So, whoops.



JASON:  And, I mean, also important to point out this was a small number of users.  Xiaomi, I think, or someone revealed it was about a thousand cameras or something like that, or a thousand users.  But, I mean, that number is kind of pointless in the perspective of people who are considering bringing smart technology into their home, but are worried about whatever the inherent risk is.  And when you're talking about bringing cameras into your home, of course one of the biggest worries is how do I know that the right person - i.e., me or my family - is going to see that video feed and no one else is, especially depending on where you put these security cameras.  This just ends up being like the perfect storm example of, like, oh, shoot, like that's a worst-case scenario, and yet some random person had this other random person's video feed from inside their home.  That could be really bad.  It's not good.



STEVE:  Yeah, and if anyone has been receiving that spam recently that talks about seeing you doing something that you don't want anyone to see you doing, so pay them some bitcoin or they're going to release it publicly, well, this puts a little more veracity.  I mean, that's just nonsense.  That's not happening.  No one should believe that spam.  But it's like, ooh, you see this in the news, and you can imagine if the local news picks up a little blurb that, gee, the wrong people are seeing other people's video feeds.  It's like, what?  What?  That's not good.



JASON:  It's perfect, perfect fodder.



STEVE:  So, yes.  So you make the point, I mean, it is, yes, it's really cool and neat, and we would wish that mistakes were not being made.  But mistakes are being made.



JASON:  There's humans involved here.  We aren't going to be able to create 100% perfect technology.  As for Xiaomi, their devices are back online except for this one particular camera.  So apparently Xiaomi has isolated it to this camera, and the services there are not active yet.  So I don't know when that'll happen.  Obviously Xiaomi's looking into it.  But, yeah.



STEVE:  Well, so here we have now our next case of what is not a mistake.  And that's just what - this is the part that boggles my mind.  This is not a mistake.  This is the way Microsoft designed it.  On December 18th they posted, the security group posted a blog titled - very high-falutin' title - "Data science for cybersecurity:  A probabilistic time series model for detecting RDP inbound brute force attacks."  And I have a link in the show notes for anyone who wants to glaze over more than that title already did.  But I'll just share the introduction.  It's a long, long posting.  But you can see what the intent was from the introduction.



They wrote:  "Computers with Remote Desktop Protocol (RDP) exposed to the Internet are an attractive target for adversaries because they present a simple and effective way to gain access to a network."  Yeah, no kidding.  "Brute forcing RDP" - then Microsoft says, and I got a kick out of this, too - "a secure network communications protocol that provides remote access over port 3389" - okay, well, stop right there.  It isn't a secure  network access protocol.  Last year they were forced to go back and patch Windows XP because it was not secure.  And half a million Windows machines right now still don't have that patch applied.  So they would like it to be, they wish it were a secure network communications protocol.  And I would argue that so long as you have a brute-forcible authentication that doesn't keep people from brute forcing it, it can't be called a secure network communications protocol.  But okay.



They go on.  They say:  "Brute forcing RDP, a secure network communications protocol that provides remote access over port 3389, does not require a high level of expertise or the use of exploits."  Right, because it's not secure.  Instead, they said:  "Attackers can utilize many off-the-shelf tools" - many - "to scan the Internet for potential victims and leverage similar such tools for conducting the brute force attack."  So off the shelf, wonderful.



"Attackers," they write, "target RDP servers that use weak passwords and are without multifactor authentication" - because it doesn't offer that - "virtual private networks, and other security protections.  Through RDP brute force, threat actor groups can gain access to target machines and conduct many follow-on activities like ransomware and coin mining operations."



They said:  "In a brute force attack, adversaries attempt to sign into an account by effectively using one or more trial-and-error methods.  Many failed sign-ins occurring over very short time frequencies, typically minutes or even seconds, are usually associated with these attacks."  What do you know?  Imagine that.  They said:  "A brute force attack might also involve adversaries attempting to access one or more accounts using valid usernames that were obtained from credential theft or using common usernames like 'administrator.'  The same holds for password combinations.  In detecting RDP brute force attacks, we [Microsoft] focus on the source IP address and username, as password data is not available.



"In the Windows operating system, whenever an attempted sign-in fails for a local machine, Event Tracing for Windows (ETW) registers Event ID 4625 with the associated username.  Meanwhile, source IP addresses connected to RDP can be accessed.  This information is very useful in assessing if a machine is under brute force attack.  Using this information in combination with Event ID 4624" - which actually means success - "for non-server Windows machines can shed light on which sign-in sessions were successfully created and can help further in detecting" - yeah, after the fact - "if a local machine has been compromised.



"In this blog we'll present a study and a detection logic that uses these signals.  This data science-driven approach to" - they're not fixing the protocol, but they're going to analyze its failure.  "This data science-driven approach to detecting RDP brute force attacks has proven valuable in detecting human adversary activity through Microsoft Threat Experts, the managed threat hunting service in Microsoft Defender Advanced Threat Protection.  This work is an example of how the close collaboration between data scientists and threat hunters results in protection for customers against real-world threats."  Again, okay, so we're not going to fix this.  We're just going to analyze it. 



So here's what we know now as a consequence of this report - I read the rest of it so that we don't have to on the podcast - that we didn't know before.  Microsoft learned that approximately 0.08% of RDP brute force attacks are successful and that brute force attacks last two to three days on average.  For the study, Microsoft collected data on RDP login-related events from more than 45,000 workstations running Microsoft's Defender Advanced Threat Protection.  And this is the instrumentation that we are always talking about, these machines phoning home and providing useful information to Microsoft.  Anyway, that Advanced Threat Protection is the commercial version of the free built-in Windows Defender AV app.



The data was gathered across several months and involved collecting details about both failed and successful RDP login events, reflected by Windows log events with the IDs I mentioned before, 4265 and 4264, which respectively provide the usernames of a user or an attacker based on whether they succeeded or failed.  As we know, RDP is often deployed in enterprise environments to allow Windows sysadmins to manage servers and workstations in remote locations and can also be used by employees to access their machines when they're away traveling.



So we also know - horrible authentication bypass mistakes in RDP notwithstanding - that attackers mount attacks against Windows systems with open RDP ports.  These attackers employ standard credential-stuffing attacks to brute force cycle through many username/password combinations in an attempt to guess the target computer's RDP login credentials.  These attacks typically use combinations of usernames and passwords, maybe that have been leaked online after breaches of online services.  And we know that massive lists of usernames and passwords are easily obtainable these days.  They are available for easy download, and they're often used.



Microsoft says that their recently observed RDP brute force attacks, as I mentioned, typically last two to three days on average, with around 90% of cases lasting one week or less, and fewer than 5% lasting two weeks or more.  The attacks tended to last for days rather than hours because attackers are deliberately throttling their access in an attempt to avoid having their IPs detected as malicious and therefore banned by defensive third-party firewalls.  Consequently, rather than blasting an open RDP port with thousands of failing attempts at once, attackers try only a few combinations to literally slip under the radar.



As I noted earlier, Microsoft said that:  "Out of the thousands of machines with RDP brute force attacks detected in our analysis," they wrote, "we found that about .08% were compromised."  So I did the math; and, at a success rate of .08%, that's 36 machines out of 45,000.  So, okay.  That's not many.  But we're talking about a bad guy remotely getting into your system and being able to sit there just trickling login attempts over some length of time.  And of course we know that the use of botnets expands the rate at which brute forcing can be done because a botnet can be used in order to provide a mass of IPs so that a single IP cannot be blacklisted and shut down all attempts to get in.



So the Microsoft research team added that:  "Across all enterprises analyzed over several months, on average about one machine was detected with high probability of being compromised resulting from an RDP brute force attack every three to four days."  They noted that:  "A key takeaway from our analysis is that successful brute force attempts are not uncommon."  They said:  "Therefore it's critical to monitor at least the suspicious connections and unusual failed sign-ins that result in authenticated sign-in events."



And I don't know.  As our listeners know, I manage myself a number of Windows servers remotely.  I find the use of Remote Desktop Protocol very convenient.  But you won't find a single instance of RDP listening on any publicly exposed port within any of GRC's IPs.  It should now be clear as day to anybody that there is no safe way to have RDP publicly exposed ever.  You know?  Tuck it safely behind a VPN with multifactor login, including a time-based token and a client credential certificate required, and then the entire problem just goes away.



But instead, perversely, Microsoft concludes their otherwise useful and interesting research report by recommending that systems admins combine and monitor multiple signals for detecting inbound RDP brute force traffic on a machine.  In their report they finished, they concluded that the signals that you should watch were hour of day and day of week of failed sign-in of RDP connections; timing of successful sign-in following failed attempts; Event ID 4625 login type, filtered to network and remote interactive logins; Event ID 4625 failure reason, filtered to include subtypes 2308, 2312, and 2313; cumulative count of distinct usernames that failed to sign in without success; count and cumulative count of failed sign-ins; count and cumulative count of RDP inbound external IP; count of other machines having RDP inbound connections from one or more of the same IP.



I mean, again, they're trying to fix something that's completely broken.  And I was trying to think of an analogy.  It would be like saying to the vendor of a truly impenetrable security door, "Oh, I know that your doors are impenetrable and inexpensive, and that the door we have here is kind of fragile, flimsy, and falling apart.  But we've decided that we're just going to ask our receptionist to keep an eye on it to be sure that no one she doesn't know comes through."  And then when the vendor says, "What receptionist?" he says, "Oh, yeah, you didn't see our receptionist when you came in?  No, she doesn't always feel well on Mondays."



So again, here's all of this effort being focused on how to determine if your publicly exposed, fundamentally insecure, and insecurable Remote Desktop Protocol server is being brute forced.  Okay, first of all, Shodan lists them all.  I mean, you can get a list of them all.  So, yes, your publicly exposed RDP server is going to be brute forced.  So what good does watching that do?  Fix the problem.  Anyway.



JASON:  Well, at least we know.  At least we saw it happen.



STEVE:  Yeah, yeah.  I guess that would be justification for buying the really good impenetrable door.



JASON:  Yeah, sure.



STEVE:  Or what's the problem with a VPN?  They're free.  Get one.  They don't cost anything.  Put a pfSense box for 150 bucks in front of it.  I mean, it's like, okay, problem solved.  Amazing.



JASON:  All right.  So what do we have next?  Point of sale.



STEVE:  Well, here we are at the beginning of 2020, and point-of-sale systems continue to be a huge problem.  They're one of the earliest problems, and we just don't seem able to solve these problems.  The hacks, I guess, are maybe less gee-whiz, a little less technically interesting.  And of course credit card theft typically represents a widely distributed, though individually recoverable pain, as I well know, actually, having had more than my share of credit cards canceled and reissued after incidents of online fraud.  I've told the story a number of times of back when I was using a travel agent.  Every year Judy would say, "Okay, Steve, are we changing our card again?"  It's like, yes, Judy.  That would happen because I got a fraudulent charge back in those days.  Now I'm able to aggregate a lot more of my purchases through a few vendors, or PayPal, if sites allow a purchase through PayPal.  So the problem seems less.



But point-of-sale terminals continue to be a problem.  In our Decade of Hacks retrospective two weeks ago, Leo and I remembered the history-making attack on the Target chain of retail stores.  That was a huge point-of-sale attack that really sort of put the whole idea on people's radar.  So I just wanted to remind everyone as a consequence of a couple events that have just happened, that these attacks have not stopped or even really slowed down.  In this case another major restaurant chain - or chain owner, actually, Landry's - recently revealed a longstanding attack on its POS, point-of-sale systems.



When I heard "Landry's," I didn't think much of it for myself since I've never eaten at a Landry's restaurant, or so I thought.  When I looked over the chains that they own, however, and operate, I recognized many of them.  It turns out that Landry's owns and operates more than 600 bars, restaurants, hotels, casinos, food and beverage outlets, with over 63 different brands under the Landry's umbrella.  They do have a seafood restaurant.  There's the Chart House, Saltgrass Steak House, Claim Jumper, Morton's Steakhouse, Mastro's Restaurants, Rainforest Caf, just, I mean, and this list goes on and on.  So it's likely that, if you are a diner, that you've eaten at one of these places. 



So according to their own breach notification which they published this week, the malware was designed to search for and steal sensitive customer credit card data.  It's funny, too, because when I read this, "malware is designed to search for and steal," when we're talking about our lexicon, we'll be talking about data scraping malware that does exactly that.  It lives to search out and find stuff that it's been targeted for.  Anyway, credit card numbers, expiration dates, verification codes and, in some cases, cardholder names.



This point-of-sale malware infected point-of-sale terminals at all Landry's-owned locations.  So it was pervasive.  They said, and this puzzled the tech press that was covering this, that the end-to-end encryption technology used by their terminals prevented attackers from stealing payment card data from card swipes at its restaurants.  But then they said that their outlets also use "order entry systems with a card reader attached for wait staff to enter kitchen and bar orders and to swipe Landry's Select Club reward cards."  So apparently somehow that allowed the wait staff to mistakenly swipe payment cards, which were not end-to-end encrypted, and allowed the data to escape.



So no one's really clear how many, what it meant.  Landry's not being forthcoming.  They did, however, say that this malware was actively scanning their systems between the 13th of March in 2019 and the 17th of October in 2019.  So for, what, more than half a year this stuff was in at all of their locations.  And they also said it may have been installed as early as the 18th of January.  So for what it's worth, they're clearly trying to at least disclose the nature of the problem.  And they said that during the investigation they had removed the malware and implemented enhanced security measures and are providing additional training to wait staff.  So maybe it was a human factors problem, as well.  Who knows?



Also, however, another major U.S. convenience store chain, Wawa, W-A-W-A, also said last month that it had recently discovered malware that was skimming customers' payment card data at just about all of its 850 stores.  In this case the infection began rolling out to the stores' payment card processing system on March 4th and was not discovered until December 10th.  So again, nine months of active slurping up of anyone who used a credit card or debit card, I guess, at Wawa.  They said it took, after the discovery, two more days for the ransomware to be fully contained.  Most locations' point-of-sale systems were infected by April 22nd, apparently, although the advisory said that some locations may not have been affected at all.



And they said that the malware collected payment card numbers, expiration dates, cardholder names from payment cards used at potentially all Wawa in-store payment terminals and fuel dispensers.  It did not disclose how many customers or cards were affected.  They said the malware did not access debit card PINs, credit card CVV numbers, you know, the little authentication numbers on the back of our cards, or driver's license data, which might have been used to verify age-restricted  purchases, you know, presumably alcohol.  They said information processed by in-store ATMs was also not affected, so that was clearly an independent network.  They said they'd hired an independent forensics firm to investigate the infection.



So anyway, I just wanted to mention that point-of-sale problems are not gone.  They tend to be, you know, they hit the news when they're huge.  But they're generally, I mean, for example, I'm seeing them and not bothering to bring our podcast listeners' attention to them constantly because again, as I said, they're sort of low tech.  They're not that interesting.  They tend to be regional when they're a small local chain, so they're not affecting everybody.  So it probably hits the local news.



And in general everyone using a credit card online would be well advised to scan their statement.  Certainly if huge payments appear, that will tend to stick out like a sore thumb.  But a lot of these things are deliberately small dings against a credit card, specifically to keep it under the radar.  When these cards are being aggregated, they're then sold off in the black market to bad guys for whom these cards are very valuable.  So anyway, certainly keep an eye on, you know, just scan through and make sure that you recognize all the payments on your card.  It just makes sense to do that.



I titled this next piece "Be careful when you flip that switch" because, according to the Russian government, their recent Runet disconnection test was successful.  Monday of Christmas week the Russian government announced that it had concluded a series of tests during which it successfully disconnected the entire country from the worldwide Internet.  The tests were carried out over multiple days, starting the previous week, and involved Russian government agencies, local Internet service providers, and local Russian Internet companies.



The goal of the tests, which we've talked about previously, was to determine and verify whether the country's entire national Internet infrastructure, known inside Russia as Runet, could function without access to the global DNS system and the external Internet.  And of course this disconnect, especially DNS, which immediately came to everyone's attention as, oh, how are you going to do that, you know, dropping off the Internet is not easy.  After they pulled the switch, all Internet traffic was rerouted internally, effectively making Russia's Runet the world's largest Intranet, that is, a big network that is not connected to the rest of the Internet, although Intranets normally have gateways that hook them to the Internet.  It became a private network.



The government was not interested in revealing any technical details about the tests and what exactly they consisted of.  It only said that the government tested several disconnection scenarios, including a scenario that simulated a hostile cyberattack from a foreign country.  I'm not sure that would work that well, but what the heck.  A cyberattack would normally have some sort of technical agency within the environment that it's bound to attack, and it would probably work autonomously, especially now that Russia has just announced to the whole world that, hey, we're able to pull the switch.  So if you really did want to attack Russia, you'd arrange to be switch-resistant.  And we know there are lots of ways to do that.



But in any event, this was all cited by multiple Russian news agencies.  And Alexei Sokolov, who's the deputy head of the Ministry of Digital Development, Communications, and Mass Media, said:  "It turned out that, in general, both authorities and telecom operators are ready to effectively respond to possible risks and threats and ensure the functioning of the Internet and the unified telecommunication network in Russia."  He said the results of the tests would be presented to President Putin in 2020.  And of course we've discussed, as I mentioned, this plan several times in the past.  So we already understand its outline.



The tests were the result of many years of planning and included some required lawmaking by the Russian government, as well as physical modifications to Russia's internal Internet infrastructure.  The tests had originally been scheduled for April of 2019, but ended up being delayed until this week of before and during Christmas to give the Kremlin time to pass their new so-called "Internet Sovereignty Law" which grants the Russian government the power to disconnect the country from the rest of the Internet at will, which it apparently didn't have before that, and with little explanation, on the grounds of - yes, you can just imagine the next phrase - "national security."



So to enable this the law mandates that all local Internet service providers be able to reroute all Internet traffic through strategic chokepoints under the management of Russia's Communications Ministry.  So as I understand it, basically there's been a big rewiring which has occurred in order to make this possible.  And ISPs are now running their traffic through these chokepoints.  And these chokepoints thus serve as a gigantic kill switch for Russia's connection to the external Internet.  And it's been noted that it also provides a powerful opportunity for Internet surveillance, not unlike China's Great Firewall technology.  That is, when you've got all Internet traffic transiting a few chokepoints, well, that's where you put your big monitoring technology if you want to see what's going on through those chokepoints.



So as I mentioned, it's truly difficult in this day and age to consider - I can't imagine having the U.S. being disconnected from the entire rest of the world.  I mean, who knows what the consequences would be, what things, what critical infrastructure things would break when they lost contact with things outside of our geographic boundary?  It's a little bit like the Y2K problem. 



JASON:  Exactly.



STEVE:  It really is something that you need to consider very carefully.  And frankly, I have to say I'm a little envious of Russia that they have the ability to do this.  It's a challenge, and it did take them, even with as much control as their government has, it took them years to be able to pull this off from just a technology standpoint.



JASON:  I don't know.  I can't imagine much harm would happen if the U.S. was cut off from the rest of the world.  I mean, I'm an optimist.  So maybe that's my flaw.  But I don't see how anything could possibly go wrong.



STEVE:  Okay.



JASON:  Steve's like, "There, there.  There, there."



STEVE:  Okay, Jason, yeah.  Speaking of things not going wrong, Python 2.7 has reached its end of life after 20 years.  How quickly 20 years passes.  Python 2.7 was released in the year 2000.  Wow, you know, like back when Windows 2000 server was like the thing.  Wow.  So Python 3 released six years later than 2.7, in '06.  But as we so often see with technology, if it's not broke, leave it alone.  So Python 2.7 was working quite nicely, thank you very much.  So why would we want to move to 3?  Even though now we've had 3 for, what, 14 years.  And sure enough, due to 2.7's incredible popularity, even though it had its original sunset retirement planned for 2015, the Python developers decided, oh, what the heck, we will continue supporting both development branches.



But I'm sure it hasn't escaped anyone's attention by now that it is, as we've said already, year 2020.  And yes, after a good 20-year run, Python 2.7's retirement day finally arrived six days ago.  Python 2.7 was officially at end of life at the beginning of this year.  And as we know, painful as this will be, this will ultimately be a good thing.  Dropping 2.7 will allow the Python team to focus all of their attention on Python 3 with an eye toward increasing its speed and patching any bugs and just doing a better job, not having to continually split and back port things that they're doing and caring about on 3 back to 2.7, only because everybody else is using it.  So they finally said, okay, enough already.  We're done.



In their announcement they wrote:  "We are volunteers who make and care for the Python programming language.  We have decided that January 1, 2020 will be the day that we" - well, and in fact it turned out not to be, but we'll get there in a second - "will be the day that we sunset Python 2.  That means we will not provide any more after that day, even if someone finds a security problem in it.  You should upgrade to Python 3 as soon as you can."



Now, unfortunately, having said that, the team does plan one last release of Python 2.7 in April.  That final release will include any final bug and security fixes that may have been hanging over from 2019 and, frankly, any more up until that final release date that they have the opportunity to fit in in time.  They said:  "We want to leave Python as solid and stable as possible."



But it's clearly time to move on.  Bleeping Computer noted that for those who really do still require, and will going forward, Python 2.7 compatibility, and for whom for whatever reason moving to 3 is a problem, an option would be to move from Python 2.7 to PyPy.  That's P-Y-P-Y.  It's an alternative reimplementation of Python written around a JIT, a just-in-time compiler, rather than an interpreter.  As a result, Python programs often execute faster under PyPy, or when compiled under PyPy, than interpreted under Python.  Also PyPy-compiled programs also tend to require less memory to execute.  So it's worth considering.



On the other hand, 2 is going away.  Most Linux distros are working right now to migrate their dependencies and their libraries from 2.7 to 3.  Debian, Ubuntu, and Fedora have begun the process of updating Python 2 libraries to their Python 3 equivalents.  Debian's so-called "Buster," Debian's Buster 10x release and Ubuntu 18.04 LTS will both be using Python 3 as their default, but Python 2.7 will still remain available for those wishing to install it deliberately.  In the current release of Fedora 31, Python 3.6 is already the default version installed.



Red Hat has stated that even though the Python Software Foundation has retired Python 2.7, they will continue to support it through the normal RHEL lifecycle, so that continues it until 2024, so for another four years.  So anyway, if you are a Python user, you need to consider 2.7 and 3 and see if you're able to move away from 2.7 because it's going to be getting kind of scarce.



It's fun to check in on HackerOne.  As our listeners know, we're quite bullish about the emergence of this new category of employment, security vulnerability hunting and reporting for profit.  This part- or full-time career is enabled by companies such as HackerOne, my personal favorite, since unlike Zerodium, HackerOne does not hoard and then turn around and secretly sell their submissions into the gray market for potentially malign use by national law enforcement, intelligence, and cybercrime entities.  Rather, HackerOne simply manages the bounty programs offered by legitimate firms who have grown up and understand that putting new eyes on their code only makes sense, and that incentivizing such scrutiny requires bounty payouts.  And so that's what they're doing.  So it's instructive to check in every so often to see what the top 20 HackerOne clients are up to.



I have a link in the show notes to the extensive PDF covering a complete detailing of it.  I'll spare our listeners that.  But to give you a sense for it, ranked downward from the most money paid down, the top 20 companies are Verizon Media in the number one spot, then Uber, PayPal, Shopify, Twitter, Intel, Airbnb, Ubiquiti Networks, Valve, GitLab, GitHub, Slack, Starbucks, Mail.ru, Grab, Coinbase, Snapchat, HackerOne themselves - that's in the 18th place - Dropbox in 19th, and VK, which is a social media domain, are the top 20.



Verizon's spot is solidly held.  They have paid out more than $4 million since launching their program back in February of 2014.  So they are number one in all-time bounties paid and in the most hackers thanked and in the most reports resolved.  Number one in all three.  Now, part of that is early participation.  2014 compared to some of these other newcomers is a long time.  So they've had time to accrue a high total bounty.  As noted by number two, that's Uber, who has less than, but not much less, than half the total bounties paid at 1.8 million, whereas remember Verizon is at 4 million.  So Uber is at half that, or a little less than half that, at 1.8.



But whereas Verizon's top bounty, top single bounty ever paid was $6,000, Uber has paid one of $15,000.  And also Verizon's 4 million are spread across a total of 5,269 reports, whereas Uber's only slightly less than half, 1.8 million, is only spread over 1,172 reports.  So it does appear that Uber is paying bigger bounties on average.  And I don't know if, like for example in Verizon's case, maybe Verizon's more recent bounties are greater because there's been an inflation in and competition in bounty payout such that in the same more recent timeframe it might be that Verizon is paying more expensive bounties to be more at parity to what Uber is.  We really can't tell from the data.



PayPal, in the number three slot, takes the award for the single largest bounty paid through HackerOne, $30,000.  And they're also clearly paying more per bounty since their all-time total payout is one bounty of $1.25 million.  Oh, no, wait.  No, no, no, no.  I got that wrong, sorry.  They're clearly paying more - right.  I'm sorry.  The largest single bounty I just said was $30,000.  But they're also clearly paying more per bounty since their all-time total payout is $1.25 million, spread across only 430 reports.  So that's by far a larger per-report bounty.  And they've only been a participant in HackerOne since 2018.  So despite that, they are in the top five in fastest response time, that is, in terms of paying bounties after the reports.  So they clearly see this as an important and significant security win for them.



So anyway, I'm not going to keep enumerating every detail of the next 17.  Again, all of those details are available through the link in the show notes.  But I will note that Shopify boasted the fastest time to bounty payout of only two days and is among the top five in most reports resolved and the largest single bounty paid.  Valve is also among the top five paying the single largest bounty, and it has company with GitHub, Coinbase, Snapchat, and Dropbox.



So just another reminder that it's possible to pay the bills while truly doing good, actively and objectively improving the  overall security of this industry.  And, you know, if you're still living at home, and you're a hacker, maybe you can hack your way into your own space and give your Mom and Dad a break.



JASON:  Right.  No, I promise, Mom and Dad.  My hacking is for good.  By the way, here's a multiple thousand, $30,000 payout.  That's not bad.  But there's probably tons and tons of people going after this.  So I don't know.  Your odds, I'm not sure what that...



STEVE:  There is certainly competition, and it is a function of being good.  But a lot of people think they're good.



JASON:  Everybody.



STEVE:  And this could be - yes, exactly.  And this could also be something where, as they say, "Don't quit your day job."



JASON:  Right.



STEVE:  But which you literally, rather than sitting mesmerized in front of a ridiculous videogame, like all evening every evening, get creative.  Sharpen up your hacking skills.  And if you find that you can start earning some bounties, then maybe your employer needs to give you a raise.



JASON:  Not bad.  Not bad at all.



STEVE:  We'll finish up before our last break and getting into our fun stuff with this week's possible action item for some of our listeners.  As we often do, we have one potentially significant and widespread new vulnerability that we need to inform our listeners of.  The prolific Tencent Blade Team, that's the Chinese hacking team that is very good, they recently disclosed another batch of serious SQLite (S-Q-L-I-T-E) vulnerabilities which they have named Magellan 2.0.  These newly discovered vulnerabilities occur in obviously the extremely popular and widespread SQLite database, which I see installed everywhere.  I mean, it's sort of an embedded application-scale database which allows an app to store data locally for whatever purpose and spares the app developers the burden of doing it themselves.



So it's a very compact, relational database management system that's used, I mean, I see it all over the place.  It is, significantly, in Google Chrome, Mozilla Firefox, Windows 10, and all over the place.  It's been nearly a year since the Tencent Blade Team disclosed their original batch, which they called Magellan 1.0 SQLite vulnerabilities.  These new revelations affect all programs - get this - all programs that utilize SQLite as a component in their software.  And the vulnerability manifests if they allow external SQL queries.  Now, that's the good news.  It is probably a tiny percentage of apps where their use of SQLite also exposes SQLite to the public.  Most of the library use is just internal database purposes like, well, you can imagine.  I would be surprised if Thunderbird, the email client that I use, doesn't keep all of its stuff in SQLite.  That's just what you do these days.



What's significant, and Chrome has already fixed this, is that one of the biggest targets was Google's Chrome browser.  What they found was that, as a consequence of the way Chrome operated, if the browser had Web SQL enabled, it was possible to go through a web page and get to SQLite and turn this into a remotely exploitable remote code execution vulnerability.  They said:  "These vulnerabilities were found by Tencent Blade Team and verified to be able to exploit remote code execution in the Chromium render process."  The advisory continues:  "As a well-known database, SQLite is widely used in all modern mainstream operating systems and software, so this vulnerability has a wide range of influence.  SQLite and Google had confirmed and fixed these vulnerabilities.  They were disclosed responsibly."  They said:  "We will not disclose any details of the vulnerability at this time, and we are pushing other vendors to fix this vulnerability as quickly as possible."



So anyway, using these vulnerabilities, Tencent was able to remotely execute commands in Google Chrome as long as, as I mentioned, this Web SQL was enabled in the browser.  So this critical vulnerability was closed by Google.  The SQLite team were notified of this round as they were almost a year ago of the Magellan 1.0 problems.  SQLite was patched for these only on the 13th of December, so not even a month ago yet.  So, and of course it itself does support remote queries.  If anything you're using, like for example is using UPnP to map a port open so that some component of something that is SQLite based has public access, that would mean you are in serious danger.  So I wanted to bring it to everyone's attention.



There were five CVEs issued.  They've all been fixed and patched.  But of course it doesn't do any good unless SQLite itself gets patched.  The other problem is there could well be ways of locally exploiting SQLite to execute code which malware might start being clever about using.  As I said, the problem is it is so widely used that, when you discover a problem like this, it's going to be everywhere.  And because it's an embedded component of something you may have had in your machine for who knows how long, that may not update itself, and you certainly wouldn't even be aware that SQLite was in there as an embedded component, it really does represent a problem potentially moving forward.  2020 may be seeing some SQLite-related exploits going forward.  It wouldn't surprise me.



JASON:  All right, got some sci-fi right here.  I like this.



STEVE:  So I tweeted out to all of my Twitter followers when I was clued in.  It wasn't until, well, sometime between Christmas and New Year's that Netflix had dropped the second 10-episode season of "Lost in Space."  And that's now behind me.  And I enjoyed it, although I have to say that Dr. Smith, oh, she is deliberately designed to be so annoying, and she's fulfilling her role well.  Also "The Mandalorian," I finally caught up and finished all of that.  And so I will be resigning from Disney because there's nothing else that I want to watch there except, well, I've seen all the Star Wars stuff.



JASON:  So you signed up for the trial?  Or did you do like a month?  Because, I mean, "The Mandalorian" has been going now since it launched, I guess what is that, a month and a half?



STEVE:  Yeah.  I did sign up.  I thought it's only fair to do the subscription.  And it goes, for me, it goes through the 24th.  So I will resign, you know, it's $7 a month, so it's not a big deal.  But again, I think that's sort of generally what people are doing now is they sign up for something that has a specific thing they're interested in watching, they watch it, and then they go, okay, well, there's nothing else here.  I'll come back next year for the next installment.



JASON:  Totally.  Yeah, I'll be curious to hear because I wasn't aware that "The Mandalorian," the releases were wrapping up.  Like I've heard nothing but great things about it.  At some point I'll watch it, along with everything else that I've said that about.  But I would be really curious to know with Disney Plus how many people do exactly what you did; right?  Like we signed up because we knew "The Mandalorian" was a thing that we wanted to see.  And Disney's super happy that everybody did that.  And then what is going to be the rate of people dropping it and pausing it until the next season? 



STEVE:  Well, and sadly, I think what the outcome of this will be, if this becomes like a thing that people do, is they'll, I mean, nothing requires them to drop them all at once and allow people to binge on them in one week and then resign.  They could certainly dribble this out once a week.  And, well, in fact, no, wait, "The Mandalorian" was dribbled out once a week.



JASON:  Oh, yeah, that's how they did it, yeah.



STEVE:  Yeah, it was "Lost in Space" where it was all available from Netflix.  And thank you, Netflix.  I'm keeping my subscription because there's lots of other goodies there that I want to watch.



JASON:  Sure.



STEVE:  So I think they're probably safe from that.  But so I see what you mean.  Yes.  Once a week for 10 weeks, and now goodbye, Disney.



JASON:  Right.  Until the next "Mandalorian."



STEVE:  Yeah.



JASON:  Yeah, it's interesting that they both, between Netflix and Disney Plus, they have the different release approach.  And I'm not sure which is better for...



STEVE:  Yeah, and I think if they tried to stretch it out over a year, like if they did one a month, that's not going to work, either.



JASON:  Right.  I don't think that would work.



STEVE:  People would go, "Screw this.  I'm not, you know, I'm not going to wait for this."



JASON:  Absolutely.  Or you'd wait for the year and then sign up and watch and then binge on them, which people could totally do after the 10 weeks of episodes of "Mandalorian."



STEVE:  So I do have a neat announcement.  As SQRL continues to spread, we have now SQRL support, native SQRL support for Drupal.  A contributor of ours, I guess I pronounce his name Jrgen?



JASON:  Jrgen.



STEVE:  Jrgen Haas.  He wrote:  "Happy to announce that I was finally able to finish off the SQRL integration into Drupal 8 and the forthcoming version 9.  It is feature complete and supports all of the SQRL protocol version 1."  Then he has links to a test server.  He says the source code with the Drupal module is available from, and we have a link to that.  He said:  "The installation on any existing Drupal 8 site is as simple as" - and then he has the command - "composer require drupal/sqrl," followed by, and then it's like "drush en sqrl," D-R-U-S-H E-N S-Q-R-L, he says, and that's all to get it up and running.  And then he said:  "Maybe a new forum section on sqrl.grc.com might be worth having."  And he says:  "To me this is a great start into 2020, and I wish everyone around a Happy New Year."



Anyway, so I thanked Jrgen for his terrific work, told him that I'd be mentioning this, as here I am on this week's Security Now!, and that I would immediately set up a forum at sqrl.grc.com for him to moderate for the public management of this great work.  And that forum has been in existence now ever since then, like a week plus.  And he's there, and a bunch of people are hanging out and playing with it.  And in fact shortly after that a Brian of London chimed in, posting, he said:  "And as one of the first testers, I can confirm it was ridiculously easy to fire up a SQRL client," he said, parens, "(can't even remember which client I used), sign in for the first time, and then set up an avatar and so on."  He says:  "I haven't given it an email or password.  None needed."  So very cool continuing progress on the SQRL front.



And as I mentioned last time, when I formally turned the SQRL project over to its terrifically capable community, this past holiday period was my work time, well, actually it was time I was giving myself to work on some long-needed attention to GRC's servers at Level 3.  So that has all happened.  I'm not quite finished yet.  Our long-running FreeBSD v5 Unix DNS and news server hardware I finally replaced after 15 years of flawless nonstop service.  There's FreeBSD Unix for you.  And nothing was wrong with it.  It was working just fine, even after all that time.  But I wanted to move us from a 32-bit architecture to 64.



We're now running FreeBSD 12.1, which is the latest.  That allowed me to update to a current version of BIND.  We're running BIND 9 for serving DNS.  And, you know, new versions of OpenSSL and so forth.  So anyway, I'm still wrapping up the re-addition of some of the deep and useful customizations that I had made to the old news server code.  You know, it's all written in C, and I speak C.  So there are a number of - we have a number of enhancements to the standard news server that provides everyone who's over there a bunch of special and useful features.  So I'm in the process of reimplementing them from 15 years ago.  And then I'm just getting, you know, sort of clearing the decks to get back to SpinRite 6.1, which is coming soon, I'm happy to say.



JASON:  Nice.  You've got a lot going on.



STEVE:  Never a dull moment.



JASON:  Never a dull moment in the '20s.



STEVE:  Yes.  Our Malware Lexicon.  So, you know, we're always talking about this or that sort of threat.  And just like with biological viruses, malware evolves over time.  Some malware is highly opportunistic.  It rises briefly to capture our attention while it capitalizes on a short-term opportunity.  Then, when that opportunity disappears, so does it.  But many others have evolved to exploit more fundamental flaws and problems that we have not yet as an industry managed to resolve.  For example, the ongoing troubles with point-of-sale systems that we were just talking about earlier are a good example of an early problem that persists.  And then of course the new and clearly justified worries over the security of IoT are a good example of a relatively recent new threat.  So some things are old, and they're enduring; other things are hanging around.



So I ran across a nice summary breakdown of sort of a malware methodology by Sophos.  I thought it was an appropriate way to ring in this New Year to sort of quickly run through the lexicon of the terminology that we now use to describe the various sorts of enduring threats which continue to evolve and mature.  They're the Deadly Seven.  There are seven of them.



Number one, keyloggers.  They're surprisingly simple and can be implemented in many different ways.  They hook into the stream of data coming from our keyboards which allow them to capture everything we type.  As anyone who has ever seen one of my SQRL presentations knows, I often use the example of being in Las Vegas and needing to log into my Southwest Airlines account to get a boarding pass at a Las Vegas hotel's business center, and being horrified by the idea of entering my username and password into the widely shared PC there.  And the reason I use it in a SQRL presentation is that SQRL of course allows for, it provides a zero keystroke login to untrusted PCs.  From the PC's standpoint, and from the standpoint of any malware that is in it, I'm just suddenly logged in without doing anything.  So there's no keystrokes for anybody to capture.



And of course the keyloggers, as a consequence of the way our keyboards work, keyboards actually transmit a key cap down/key cap up individual messages.  So they don't merely know that we type F.  They get enough detail to tell that the user pressed the left Shift key down, then pressed F, then released F, then let go of the Shift key.  So that means they can even keep track of keystrokes that don't produce any visible output such as Function keys, backspaces, and other key combinations that might be used to turn, you know, Alt and Control things that turn options on and off.



And of course, as we also know, they don't need to be implemented in software at the operating system level.  They can be used without root power and hook themselves into the keystroke stream data up at the application level.  Anyone who's installed a keystroke macro recorder and playback tool, or an on-the-fly spellchecker - I have one running on my system - will realize that the application sitting in the system tray down there is monitoring everything they are typing.  So it's not rocket science.  It turns out also JavaScript running in our browser can monitor - and could alter, if it had some reason to, the text we're typing in, such as maybe delaying our own login - the flow of keystrokes as we browse.  That means that rogue JavaScript injected into a login page could recognize and steal usernames and passwords.



And let's recall that the integrity of our HTTPS connections is now being actively subverted by anything we allow to intercept and inspect our data, whether it's a corporate middlebox or locally installed AV that is wanting to help us by doing this.  Those connections are no longer private.  And of course banking trojans commonly include a keylogger module so they can try to capture our passwords when they recognize that we're logging into a bank that they have been set up to recognize and subvert.



And finally, as we know, keyloggers can be hardware.  They can be a little device that you wouldn't even notice, a little lump in the cord connecting the physical keyboard to the back of the computer, a little what looks like an adapter at the back of the computer.  In fact, I have some that are like, they are legitimate PS/2 to USB adapters.  But I'm sure that isn't a keystroke logger.  But such things could look just like a legitimate adapter.  And we've actually also seen instances where the keystroke logger is molded into the USB connector at the end of a keyboard.  So it's a nefarious little bit of malware or mal tech.



Second, data stealers.  That's the term given to sort of a generic group of malware that gets into our machine and then, for their own purposes, goes hunting around our hard drives.  Perhaps even around our whole network, if they're able to escape the confines of our machine.  And those data stealers look for specific types of data that is worth something to the crooks that launched it.



Once upon a time, yes, those quaint vintage days of computing before the Internet, we mostly had true computer viruses which spread by themselves, often infecting floppies in order to get from one machine to another.  They would typically, when they got into a machine, spew out emails containing an infected attachment in order to spread themselves.  Back then, many viruses would search through qualifying files on our computers looking for text strings that matched a specific pattern, such as an email address.  And then, by harvesting email addresses from everywhere they could find it, not only our own email client's address books, they were able to send things out to people that we'd never even contacted, but whose addresses appeared in documents, marketing material, saved on web pages or whatever.  Those days are long gone.  Times have changed.



But on the other hand, contemporary data stealers are much more sophisticated these days, and their interests are wide-ranging.  Now they're searching out and looking for bank account details, ID numbers, passport data, credit cards and account passwords.  They know how to recognize special files by names and by internal structure.  Sometimes if files are weakly encrypted they're able even to decrypt them in order to get into them.  So that would give them access to poorly constructed password vaults that contain our credentials and browser databases and may have authentication tokens and browser history.  So several other classes of malware, such as bots and banking trojans that we'll be talking about in a second, also include sophisticated data stealing modules in order to get the data that they care about.  So these data stealers are in the lexicon because they are a class of malware.



Then we have RAM scrapers because malware can't always find what it wants in the files on our computer, even if the malware has admin or root access.  That's because some useful data, you might even argue some of THE most useful data, might only ever exist temporarily in RAM before it is deliberately scrubbed without ever being written to disk.  Permanent storage of some data is now prohibited under regulation, such as the PCI-DSS, which is the Payment Card Industry's Data Security Standard, and Europe's GDPR, their General Data Protection Regulations.  Those regulations prohibit the permanent storage of some information such as the CVV number used to authorize credit card payments.  And these regulations are naturally bad news for attackers because it means they can't easily get hold of those codes and the transactions that have already occurred.



However, with RAM scraping malware, which is able to keep an eye out for data as it is stored temporarily in memory, attackers may be able to spot what is critically useful data to them, such as CVV numbers and perhaps the accompanying full credit card information, and suck it right out of RAM, essentially scraping it.  And as we know, computers must have, for a moment at least, a private key in RAM in workable decrypted state in order to perform decryption.  So valuable secret data must transiently exist in RAM, even if only briefly.  So things such as decryption keys, plaintext passwords, website authentication tokens, are being watched for by RAM scrapers.  That's number three on our hit parade in the malware lexicon.



Then we have number four, bots.  A bot, of course, is the term the industry has given to malware which sets up shop in a machine, then phones home to its bot master to report in.  It requests and awaits for further instructions.  And of course it's not alone.  It's typically in a large network.  It essentially establishes a semi-permanent backdoor through a computer so that the bad guys can later send commands from wherever they are.



And of course, as I said, we call a collection of such bots a "botnet."  The other popular term for bot is "zombie" because they can also act like sleeper agents.  The commands bots understand, including sending boatloads of spam from your IP address in order to avoid IP-based spam filters, searching for local files, sniffing for passwords, blasting other machines on the Internet, or rather blasting them off the Internet with floods of traffic, and even clicking on online ads to generate pay-per-click revenue span the range of what we have found bots doing.  And of course they're also capable of receiving updates to themselves to allow them to be updated and to download new and improved modules.



So we've recently talked about the evolving strategies bots are using to sneak these updates into a network right under the noses of watchful AV screeners.  And since bots are initiating connections outward to their bot masters, just like any IoT devices we've talked about where you just set them up and, oh, look, they're magically connected, these things work the same way.  They don't need ports open for them.  And our otherwise protective NAT routers don't protect us here because outbound connections by default are allowed, whereas NAT routers, as we know, drop inbound connections.  So number four in the malware lexicon was bots.



Number five, kind of related, but much more of the specialist, are banking trojans.  And they are more prevalent than this podcast would have you believe.  I guess I don't cover them much because they're not that exciting.  But they're definitely in existence.  They deserve their own subclass of malware because of their specialization.  They exclusively target their victims' online banking information.



And for what it's worth, I watch them grow in capability, where they're always adding more banks and lending institutions to their repertoire.  As we might imagine, banking trojans typically include a keylogger component, which is used to sniff out the passwords as the user is using them to log into the bank.  They often incorporate a data stealer component to trawl through likely files such as browser databases and password vaults in the hopes of finding unencrypted passwords or account details.



And a trick widely used by banking trojans is web form injection.  We haven't talked about this for years, but that occurs where the malware adds additional data fields to forms displayed in the browser.  By doing this, they hope to trick their victims into entering additional unnecessary data such as a credit card number for "verification," or a date of birth.  The typical computer user has given up any hope of actually understanding what's going on when they use the computer.  They just do what the computer tells them to.  So they just hope for the best.



So when their bank suddenly appears to be asking for a credit card number for some reason, and/or their date of birth, they think, well, okay, the bank knows those things, so it's okay, I guess.  Well, in the process they've just filled it into a form that the banking trojan added and populated on their login form, and it's collected information that it wanted that it wouldn't have otherwise been able to see them typing in because they weren't being asked.  Anyway, banking trojans are nefarious, and they're also a real big deal as number five in the malware lexicon.



Second to last at number six is RATs.  I wish it had a more distinctive acronym because it's never clear when I'm talking about a RAT, like, what I'm talking about.  But it stands, of course, for Remote Access Trojan.  It's got a lot in common with a bot, but it differs in that it's not usually part of a massive campaign simply to see how many bots can be corralled and managed for mass attack events.  Instead, a RAT enables much more targeted and potentially much more damaging intrusion.



Normally a RAT is placed deliberately into a target machine, maybe through social engineering, a phishing attack, some other sort of intrusion.  But here it's not simply intending to use the machine as a point of attack, but rather looking inward rather than outward, seeing what's there and essentially, being a Remote Access Trojan, giving the person who is operating it a persistent connection, able to sit there, look around, see what's going on.



They're able to take screenshots, listen to the audio in our rooms by turning on our computer's microphone, and then of course turning on the webcams, which always makes people nervous about whether or not it's possible for the cam to be on while the little webcam light is off.  And we do know that there are software systems that allow the light to be turned off, even when the cam has continued to be on, which is why this podcast has long recommended using some sort of physical shutter, even if it's only a little corner of a Post-it note over the camera hole when you aren't deliberately sitting in front of it and having a conversation with someone.



And, finally, last but certainly not least, number seven in the malware lexicon is ransomware.  After 2019, it would likely be no exaggeration to state that ransomware is probably the most feared type of malware.  Whereas many people might not know exactly what a bot or a RAT are, pretty much everyone at this point has heard the horrifying stories of entire municipalities, companies, or healthcare providers being shut down by ransomware and then having to pay a lot of money in order to regain access.  So your typical user may not know exactly what it is, but they know it's a current problem, and it's one they don't want to have.



As we've observed last year, the enabling factor for ransomware, and actually this is just my own theory, I don't think I've read it anywhere else, I think the enabling factor for ransomware was the rise of readily exchangeable cryptocurrency because it used to be the way you caught the bad guys was you followed the money.  Cryptocurrency makes that much, much more difficult.  So until there was a safe and untraceable means for receiving payment, the ransomware business, such as it is, wasn't really able to take off.



But today there are even, as we know, companies specializing in being the go-between between a ransomware victim and the extortionists who are demanding payment.  So this has solved the problem of I don't know how to send anyone bitcoins.  Now there are helpful people that probably take a little piece of the action, but they know how to do that.



Ransomware attacks can be so lucrative that they are the inverse of the scattershot bot model.  Attackers can afford to sit inside a victim's network, take their time to maximize the damage done.  After finding their way into the network, they can set up and get ready to scramble hundreds or even thousands of computers at the same time.  And if they discover that online backups exist, those have the opportunity to be compromised.



And what we've seen is, even if offline backups exist, if a large enterprise has done everything right, still the time and labor required to reimage and restore thousands of computers has been shown to be hugely damaging all by itself.  No organization that is making responsible offline image backups expects to need to deploy all of them at once.  They just figure, well, if a given machine suddenly explodes, we'll be able to get a replacement back up in no time.  But much more difficult when ransomware takes all of the systems that it was able to find off and encrypts them at the same time.



And because the payout might be significant, the attackers can afford to spend the time to research the entire cybersecurity countermeasure situation to sidestep or disable anything that might halt or limit the ransomware's reach and effect.  It doesn't get a lot of attention, but the security forensics firms that are brought in often see effective defenses were in place and got sidestepped anyway.  So it's just, you know, ransomware is the scourge of probably 2019 and promises no sign of letting up in 2020.



So this first podcast of 2020 launches us into what is certain to be a very interesting year of cybersecurity events, beginning with next week's final Windows 7 patch update, the last Patch Tuesday that Windows 7 and Windows Server 2008 R2 will be getting.  So that will be something we I'm sure mention next week.  Stay tuned.



JASON:  Also bring in someone playing a sad violin song to go along with it, serenade out on that one.  Good stuff.  I love taking a look at some of these things that we hear the terminology, kind of don't think twice about it.  It's nice to kind of get reintroduced to them.



STEVE:  Yeah.  And it's weird, too, that, I mean, we really do have well-established classifications.  There is some cross use, whereas like banking trojans are a thing, and they take advantage of keystroke loggers and data stealers, whereas those also exist just for their own sake.  But, yeah, it's interesting.  I mean, what we've seen is well-defined strategies for abusing our systems as a function of, ultimately, the way they're designed. 



JASON:  Sure, yeah.  And what you said about cryptocurrency tied with ransomware, I completely agree with that.  That really changed the game.



STEVE:  Yup.  I remember once upon a time the way Russian bad guys would demand payment was you'd have to send them Western Union.  You needed Western Union in order to wire the money to them.  That was because that gave them some modicum of protection.  But now, boy.



JASON:  No.



STEVE:  With exchangeable cyber currency, unfortunately, that really did open up a new market.



JASON:  Yup.  Game has changed.  Right on, Steve.  Always good stuff.  If anyone wants to go check out all that Steve is up to, you can do that, GRC.com.  You'll find everything you need to find there.  SpinRite, of course, Steve's hard drive recovery and maintenance tool.  You can get a copy there.  Also information about SQRL, growing information about SQRL to be found there.  Audio and video of this show, as well as transcripts, which can only be found there.  Of course we host audio and video of this show on our site.  But if you want transcripts of the show, go to GRC.com, and you can find that there.



Our site is TWiT.tv/sn.  That's the show page for Security Now!, the official show page here at TWiT.  There you can find everything that you need to know about this show minus the transcripts.  Audio and video, play it in the page or subscribe.  All the links are there for you to subscribe in a number of different ways, a number of different places.  And you'll also find there the kind of posting days, every Tuesday.



We record live every Tuesday at 1:30 p.m. Pacific, 4:30 p.m. Eastern, 21:30 UTC.  But if you're subscribed, it kind of doesn't matter.  You don't need to catch it live.  Most people get it in their feeds automatically.  That's the beauty of podcasts.  So go to TWiT.tv/sn and subscribe, if you haven't already.  And then you don't even have to think about it.  Steve appears like magic, as he should, into your ears.



Steve, thank you so much for letting me bomb into your show once again.  I really appreciate it. 



STEVE:  Jason, glad to have you while Leo is in Vegas.  And we'll see you next vacation, I'm sure.



JASON:  That's right.  We'll see you next time on Security Now!.  Bye until next week.



STEVE:  Bye.



Copyright (c) 2020 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#749

DATE:		January 14, 2020

TITLE:		Windows 7 - R.I.P.

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-749.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week's Security Now! podcast is titled "Windows 7 - R.I.P.," not because there's much that we haven't already said about the fact, but that it happens TODAY; and that, given the still massive install base of Windows 7, it's significant that all of those machines will now be going without any clearly needed security updates.  So the big news for this week WAS to be the event of the first successful preimage attack on the SHA-1 hash.  But that news was preempted at the last minute by the much more immediately significant news of the remotely exploitable "Cable Haunt" vulnerability that's present in most of the world's cable modems right now!  So we'll be talking about that after we look at the FBI's recent request to have Apple unlock another terrorist's iPhone; update on the Checkrain jailbreak solution; examine the challenge of checking for illegal images while preserving privacy; look at some deeply worrying research into just how easy it is for bad guys to get SIMs swapped; examine the consequences of not patching a bad VPN flaw; deal with a bit of miscellany; and then, finally, look at the new "Cable Haunt" vulnerability.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here with a fond farewell to Windows 7.  Today's the last update.  And, man, it's a good thing, too, because there's some major issues with all versions of Windows that Microsoft is fixing today.  And then he's going to terrify us with Cable Haunt, an exploit that is going to affect cable modems all over the world and is very difficult to fix.  In fact, you can't fix it.  Only your Internet service provider can.  Details coming up next on Security Now!.



LEO LAPORTE:  This is Security Now!, Episode 749, recorded Tuesday, January 14th, 2020:  Windows 7 - R.I.P.



It's time for Security Now!, the show where we cover your privacy, security online with this guy right here, Steve Gibson.  Hi, Steve.  Haven't been here since the New Year, but it's nice to see you.



STEVE GIBSON:  Leo, likewise.  Great to see you, always.



LEO:  Did you have a good New Year?



STEVE:  Yeah.  Just a quiet time.



LEO:  No dancing with Captain Kirk or anything?



STEVE:  My sort of favorite kind of New Year.  Well, I must say I do miss the old days...



LEO:  I do, too.



STEVE:  ...of TWiT craziness and butt tattoos, and you just never know what's going to happen.



LEO:  You know what I found out?  Those tattoos never go away.  My hair grew back, but the - still there.



STEVE:  Yeah.  The good news is this came as no surprise to your wife.  So it's not like one day you were walking away in the buff, and she said, what?  What is that little...



LEO:  Yeah, she was present at the inception, as we say.



STEVE:  Yeah.  Is that an AND gate with a personality, or what is that?



LEO:  Instead of doing that, we went to CES, had a lot of fun at the Consumer Electronics Show, formerly known as the Consumer Electronics Show.  And I want to thank Jason Howell for filling in last week.  But I don't plan to go anywhere for at least six months.  I'm here.



STEVE:  Whoa, what?  Oh, come on.  Not till summertime?



LEO:  That SQRL logo would make an awfully nice tattoo.  I'm just looking at...



STEVE:  Okay, no.



LEO:  It's kind of made to put on your butt.



STEVE:  So speaking of butts, this podcast is titled "Windows 7 - R.I.P."



LEO:  Oh, boy.



STEVE:  Not because there's much that we haven't already said about the fact.  We've pretty much beaten that topic to death.  But just because that happens exactly today, and I just couldn't pass up the opportunity to say, well, I mean, it is significant because there's still a massive install base of Windows 7.  We're going to talk about it just briefly at the end because it turns out it's a little more than one in four workstations, the most recent survey is, are still running Windows 7.  Despite the fact that you could argue Microsoft hasn't quite got it fixed yet, though they've been trying since, what, since '08 was it, I think, that it came out?



LEO:  This is Patch Tuesday, so today's the last Patch Tuesday for Windows 7, yeah.



STEVE:  Yes.  Yes, it is.



LEO:  And just in the nick of time, too, by the way.



STEVE:  Well, I'll talk a little bit about my plans because over the last five years I've warmed up to Windows 10.  I don't like it nearly as much as 7, but I don't hate it.  I've sort of - I've capitulated like the rest of the world has.  So that's the title of the show.  The big news for this week was to be the event of the first successful chosen prefix collision on the SHA-1 hash, which really marks its demise.



But big as that news was, it was preempted by the last-minute revelation of a remotely exploitable, kind of, but I'll explain, exploit against probably all cable modems.  And there are a lot of those.  The team that did this is based in Europe, and they found 200 million cable modems that were exploitable.  Probably all of those in the U.S. are, too.  So they named this "Cable Haunt."  And as one does these days, they grabbed the domain name, and someone came up with a good logo for it.  And because it's an issue now, and there is something that - well, I learned a lot, actually.  I found out that I have a browser-accessible spectrum analyzer in my cable modem.  And so do you, Leo.



LEO:  What?



STEVE:  Didn't know.  It's really cool.  You can dynamically see the spectrum of data coming in through your cable modem.



LEO:  Wow.



STEVE:  So it's like, what?  Anyway, and it turns out, though, that's kind of a mixed blessing because there's a problem with it.



LEO:  No.



STEVE:  Which could be exploited.  But anyway, we're going to talk about that.  We're going to take a look - and I just heard you mentioning it toward the end of MacBreak Weekly, talking about the FBI's recent request to have Apple unlock another terrorist's iPhone and what the downstream consequences may be of that.  We're going to check in on the Checkrain jailbreak solutions, speaking of iPhones, and see how that's coming along; examine the challenge of checking for illegal images while preserving privacy; look at some deeply worrying research into just how easy it is for bad guys to get their SIMs swapped, that is, it's shockingly easy.  So found a group of four researchers from Princeton.



We're going to examine the consequences of not patching a really bad VPN flaw for the last nine months, and then deal with a little bit of miscellany.  I made the mistake, Leo, of going to a 4DX movie.  I've never walked out of a Star Wars movie in my life until I did on Friday.  So I thought it'd be fun to chat with you a little bit about that horrifying...



LEO:  Wait, is there a new Star Wars movie I didn't know about?



STEVE:  And then, so we're going to deal with some miscellany, and then finally wrap up by explaining what Cable Haunt is and why it actually could be a problem.  So, wow.



LEO:  Did you see the stuff about the Crypt32.dll problem with Windows, too?



STEVE:  No.



LEO:  I'll give you a chance to look at that because it apparently affects all versions of Windows.  Brian Krebs...



STEVE:  And that's what you meant when you said "just in the nick of time."



LEO:  Yeah.



STEVE:  Do we know if it's being fixed today?  Or is this going to be a, oh, gee, don't you really wish you had Windows 10 because then you would get the fix.



LEO:  Microsoft has quietly shipped a patch to branches of the U.S. military and other high-value customers/targets that manage key Internet infrastructure.  But Patch Tuesday apparently is the day that we're going to see a fix for Crypt32.dll.



STEVE:  Okay, yup.



LEO:  That's the cryptographic function in Windows that is apparently broken and a serious security vulnerability.



STEVE:  Oh, just imagine that.  Thank goodness that now, after today, all of the problems in Windows 7 have been resolved.



LEO:  That's it.  No more.



STEVE:  And there just is no more.



LEO:  No more.



STEVE:  We're not getting patches because they finally got it all fixed.



LEO:  They fixed it.



STEVE:  Yeah, right up at the finish line.



LEO:  The biggest issue Krebs talks about is it could be used to spoof certificates and digital signatures on software.  So malware could pretend to be something legit with a spoofed certificate because of this flaw.  So it might be a good day to apply your patches.  I'm going to do that right now, actually.  Okay, Steve.  Let's haunt my cable.  Here it is.



STEVE:  So the FBI has, well, yeah, we do have our Picture of the Week, which is the fun logo that the Cable Haunt guys created.  And we will be getting to that at the end because it's horrifying.



LEO:  Yeah, no kidding.



STEVE:  The FBI has asked Apple to unlock another iPhone.



LEO:  Oh, yeah.



STEVE:  And so who knows what this is going to mean.  What did you guys conclude over on MacBreak Weekly?



LEO:  Well, Apple was incensed because Attorney General Barr had the temerity to say, oh, Apple's not helping us at all, and they listed all the things they've done to help the FBI, including the iCloud account, account information and so forth.  But they said, and I think quite rightly so, at the last paragraph of their statement, it's a bad idea to have a backdoor in any encryption.  It's just going to give it to the bad guys, as well as the FBI; and we just don't think it's a good idea.  We don't want to do it.  So we have - they were at great pains to say, no, no, we're helping the FBI.  We're helping them in every way we can.  But we can't give them that encrypted information.  We have no way of breaking in, and we don't think it's a good idea to put something in there that can't.



STEVE:  And so for what it's worth, I wanted to sort of - I know I've confused some people when I've talked about how it's obviously possible for Apple to add a silent listener to an iMessage group.  That's very different from the FBI coming to them and saying we want you to decrypt this phone.  In this current case, this is the guy who shot up the military base, what, six weeks ago down in Florida, who actually - he had two iPhones, and he shot one of them.  One of them has a bullet through it.  So that may be a little more tricky to recover, if Apple even could.



But the point is, since the Syed Farook event that we had in 2016 - I guess actually the shooting was in 2015, and then the whole issue happened, kind of boiled over in 2016.  Since then Apple has further strengthened the security of the iPhone system.  And I am absolutely sure that they're telling us the truth, and the FBI, when they say we designed a system that we cannot get into.  So, I mean, there is the iCloud backup stuff, which sort of creates a little bit of softness to the whole thing.  But in terms of the phone itself, I'm sure that there is absolutely nothing Apple can do.



So sort of like the question, the other issue that we have of whether an individual can be compelled to disclose something that they know, like a password, in order to unlock their phone, is that testimonial or not?  And we know courts have been going back and forth on this.  And ultimately that's the kind of question that may end up getting pushed up to the highest court that we have in the U.S. for a decision.  But this issue, if the FBI and Apple are going to face off, this is not something that the Supreme Court can rule on because it may very well be that Apple has, in fact I'm sure it is, they've produced a system they cannot decrypt.



And so what that says is that what would be required is that the U.S. law would end up being pushed, being changed to make it unlawful for commercial encryption systems to be sold in the U.S. that don't have some means for law enforcement to get in.  And on that count, I am 100% aligned with all of the crypto experts who say there's no way to do that without weakening the system.  And so, again, I wanted to make the point that that's very different from an Apple negotiated real-time, essentially a wiretap on iMessage, which they could do if they wanted to.  In this situation I'm sure we've got phones that, notwithstanding the various other corner cases that we've talked about, that Apple cannot get into.



And it'll be really interesting to see if such legislation happens.  I'm skeptical that that is going to happen because, I mean, the fact is it really does weaken crypto in a way that I don't think it's clear you can fix.  And Apple is sort of a special case.  It's also probably not a broad enough perspective to keep talking about them because they really have an iron grip on our phones.  I mean, we're sort of renting them from Apple because Apple absolutely decides what the devices will do and won't do, which of course is why some people like to jailbreak their phones in order to get back some of the control that Apple has designed out of the system that we're using sort of with their permission because, I mean, they really are tightly tethered to Apple.



So anyway, I just sort of wanted to touch on that.  It'll be interesting to see where this goes.  I mean, as far as I know, this issue is pending.  The FBI general counsel wrote a letter to the Apple general counsel saying we need help decrypting these phones.  The general counsel replied we'd like to help you help you, we really would.  We help you in all these other kinds of ways.  But the phones are designed with our customers' privacy paramount, and we're allowed to do that, so we have.  And the question will be, is that ever going to change, that commercial companies would not be allowed to do that?  And we know the arguments are there's lots of noncommercial solutions for encrypting stuff, if that happens.  And that would be, you know, a good point to be raised.  So, boy, it'll be interesting to see whether this ends up getting legislated in 2020.



LEO:  The only way I could see that this would happen is if they decided to criminalize encryption to the point that, if you had it, if you used it, you would go to jail.  Because you're right.  They can't stop somebody from using Signal or some other open source solution.  The math is out there.  But they could say it's a violation, it's against the law to use something that has crypto.  If we can't see it, then you're breaking the law.  And that...



STEVE:  So they could say that it is against the law for a U.S. citizen to withhold information that would allow law enforcement to decrypt their phone. 



LEO:  No, no, no.  I'm saying they could say to you, Steve Gibson, you may not use encryption; that that in and of itself is a crime.  Criminalize encryption.



STEVE:  Yes.  I knew that's what you meant.  But that seemed like way far out there.



LEO:  It's the only way you could do it because, even if they said to Apple, Samsung, everybody, okay, you know, and okay, my phone is no longer encrypted, I could still use Signal or some other...



STEVE:  Right.



LEO:  Or write my own.  I could still use crypto because crypto's well understood.  It's out there.



STEVE:  Right.  And so that's why I had backed away from that to the position of it's against the law for a citizen to refuse to provide the information they have to law enforcement to see into their crypto.



LEO:  Including your own.  That's right, yes.



STEVE:  Exactly.  And of course the one glitch there is, in this particular case, we have a dead terrorist who is unable to divulge the information.  And of course users could still choose to go to prison rather than divulge what is in their mind, their ability to decrypt their device, if what's in there is really so bad that prison is the better outcome.  I mean, we're really in a mess.  This is really a mess.  And it will only be if somehow the decision is finally made that companies have the right to encrypt in a way that the government cannot see.  And, boy, that's difficult to square.  I mean, it's just hard to imagine a world where, not just the U.S., but the U.K., that's even more aggressive about seeing into their citizens' data, yeah, it's just really, for me, fascinating.



LEO:  There is some evidence, and I'll have to read more on this, that the cryptographic vulnerability we were talking about in Windows and Crypt32.dll is because it's straightforward to create a second private key without the knowledge of the original signing private key.  And you have to wonder if Microsoft might have thought that would be a useful thing to put in their crypto library, a backdoor.



STEVE:  Huh.



LEO:  I mean, I have to read a lot more with it.  But this is an interesting question.  There may be more to this.



STEVE:  So it looks like it may not be a bug, it may be a feature.



LEO:  Well, we saw this before with the vulnerability with WMF.



STEVE:  That's exactly where I was going to go, Leo, was I got really taken to the cleaners by people who said, oh, Gibson, you don't know what you're talking about.  And I said, look, I'm looking at the code.  I can imagine a Windows - this was when Windows was the only thing that ran Windows metafiles.  And I could totally see somebody saying, well, we're going to have all this interpretive stuff.  But wouldn't it be cool if you could execute code in a Windows metafile?  And it was like, back then, before networking and before the Internet and before all this concern, it would have been a reasonable thing to do.  And the problem is it stayed in there for decades until someone stumbled on it and said, uh, this looks really bad.  And to me it looked like an understandable cool backdoor to allow code to be executed in a Windows metafile.  So, yes, Leo, you just hit on exactly the analogy I was thinking of.



LEO:  Yeah, well, we'll learn more about it.  This is just breaking now.  But the NSA has just put out a warning saying patch it.



STEVE:  Well, and doesn't this make you wonder, I mean, if this isn't - do we know that it's patched today?  Is it in all of the Windows OS Patch Tuesdays today?



LEO:  It's my understanding that it is, but I haven't seen anything from Microsoft.



STEVE:  Because this is another example, I mean, we saw XP updated recently when this horrific problem in RDP was found, in Remote Desktop Protocol.  So one really wonders how well Microsoft is going to be able to adhere to their refusal to update Windows 7.  I mean, imagine that this hadn't happened until after today?  Are we saying that Microsoft is going to allow a huge security vulnerability in all Windows 7s?  And people who are paying for them still get them.  But those who aren't paying, sorry, you have no recourse.



LEO:  I just think also there's a certain irony in the fact that it's the NSA warning us about this cryptographic vulnerability when at the same time other members of the administration are saying we want more cryptographic vulnerabilities.



STEVE:  Yes, good point.



LEO:  And the NSA has always had that kind of duality where they're trying to protect us at the same times they're trying to snoop at us.



STEVE:  I know.  There was that dual random number, DRBG, which, like...



LEO:  Right.  Didn't they get the RSA to weaken encryption?



STEVE:  Yes.  I mean, it really looks like, you know, there were three different random bit sequence generators, and they paid RSA to choose arguably the worst of the three, which came from an unclear...



LEO:  So we can crack it.



STEVE:  Yeah, which sort of just seemed really, again, you don't know for sure.  But if it walks like a duck and quacks like a duck, then maybe it is a duck.  Anyway.



LEO:  I don't want to go all conspiracy theory, but it's fascinating.  It's just...



STEVE:  So Checkrain is the unfixable, actually in the boot ROM.  And, boy, I was thinking of that also today because the thing we'll be talking about with our cable modems can be fixed with a firmware upgrade, but end users are unable to upgrade their own cable modem firmware.  That has to be something that's pushed from your provider.  So but my point is thank goodness that's not in ROM, or it would really be a problem.  And it's a big problem as it is.  But still, in the case of Checkrain I wanted just to kind of check back in.



It looks like they've now solidly got it working across all iPhones.  It's still in beta at 0.9.1.  And the most recent beta, bringing us to 0.9.1, fixes multiple bugs, including, they said, an issue where the loader app would crash when installing Cydia on iPads; a crash when the macOS language was set to anything other than English; an issue where iPad Minis would not work with the GUI; an issue with the scp - that's secure file copy - binary not working as expected.  And they ended up saying, under "unsupported devices," all there are now are some iPads - iPad Air 2, iPad 5th Gen, iPad Pro 1st Gen.  Oh, there is an iPhone 5s, so there's one iPhone that they still - they say it can work, but it may require more attempts than usual.  And then the iPad Mini 2 and Mini 3 and the Air.



So basically they've got it.  They've got the persistent jailbreak, persistent as in Apple cannot fix it.  But remember that it is a boot time thing.  So rebooting your device will flush it out of RAM.  Still, they're continuing to make inroads, and it is now available on macOS and Windows.  And they're still working to bring it up under Linux.  So that little bit of soreness continues marching forward.  



While we're on the subject of Apple and the privacy of their users, I thought it was interesting that during last week's CES, the Consumer Electronics Show, their Chief Privacy Officer Jane Horvath confirmed that Apple is automatically and continuously scanning images being backed up to iCloud in order to ferret out any instances of child abuse.  She explained that this was the way they were working to help fight child exploitation, as opposed to breaking encryption.



And it is the case that there are some technologies, Microsoft has something called PhotoDNA, which is sort of a soft hashing process which makes this sort of thing possible.  We know, for example, that hashing is useful for determining the exact duplication of something without revealing what it is.  And, for example, everybody, all responsible websites, are now using that for the password-based login.  When the user first logs in, creates their account, establishes a password or changes the password, their password is hashed very strongly, and the strength keeps going up, the number of iterations.  And there are of course memory-hard ways of hashing that makes it much more difficult to accelerate hashing.



But the point is that password is turned into something that is derived from the password, and that's what's stored.  So that in order to check when the person relogs in, the same thing is done, and the results are checked.  So you can imagine this being done with images, where you would hash an image, and then you could check for a duplicate image if the hashes of the images matched.  And it turns out that for the past 12 years the National Center for Missing and Exploited Children, the NCMEC, has been making available a large file of hashed values for known child sexual abuse images.



So one thing that lots of responsible large companies are doing is, when images are uploaded, they hash them and then check them against this hash list of known abusive images to see if they get an exact match.  Of course the problem is, as we know about hashes, even if one pixel of an image were one shade darker, it would result in a hash having about half of its bits on average inverted.  It's like, you know, one bit changes, and half the bits in the hash have a 50% probability of changing.  That's what's cool about hashes.  So you can't hash images in a useful fashion.



What Microsoft came up with, with the so-called "PhotoDNA," is that the images are resized to a standard size.  Then color is pushed out, so they're turned into black and white.  Then it's broken into a grid, and some technology scans each region of pixels within each grid cell and generates - uses an algorithm to generate a fingerprint for the edges that are seen within that cell.  That ends up producing something which is intended to survive cropping and resizing and recompressing and so forth, so that basically it produces a - it's still using a hash-like approach.  Nothing of the actual content of the image survives.



But it does produce a fingerprint such that sufficiently similar images should produce a collision and raise a red flag, and then probably that pops them to someone's attention who would then verify whether or not it looks like it's exploitive.  And that's technology which lots of large companies - Microsoft, Google, Verizon, Twitter, Facebook, Yahoo, and so forth - are using in order to basically make sure that they're not hosting abusive images inadvertently.  So it is possible for Apple to be, as was said last week, making sure that they're not hosting such images while preserving the privacy of their users.  So, you know, I just think it's a cool technology.



I mentioned four researchers at Princeton, one of them being Jonathan Mayer.  I hadn't seen his name for a while.  We've spoken of him many times and the work he's done in the past.  We know that robustly authenticating identity in an online world is difficult.  We also know I just invested six years of my life working to create a robust solution for network-oriented or network-based online identity authentication.  The standard fallback, unfortunately, is for the agency wishing to identify us to send a text message to the mobile phone associated with our account, the account that we have with them.  The problem is this assumes that our identity is tightly bound to our phone, yet of course we are not our phone.



Four days ago, on January 10th, a group of researchers, these four guys at Princeton, published a whitepaper detailing their explicit research, I mean, like they deliberately experimented with how hard this would be to spoof mobile carriers.  If their paper weren't coming from Princeton, it might have been titled something like, "Holy Crap, You're Not Going to Believe What We Just Did."  But instead their paper carries the sufficiently dry and academic title, "An Empirical Study of Wireless Carrier Authentication for SIM Swaps."



The abstract of the paper, which is perfect, it reads:  "We examined the authentication procedures used by five prepaid wireless carriers when a customer attempted to change their SIM card.  These procedures are an important line of defense against attackers who seek to hijack victims' phone numbers by posing as the victim and calling the carrier to request that service be transferred to a SIM card the attacker possesses.  We found that all five carriers used insecure authentication challenges that could be easily subverted by attackers.  We also found that attackers generally only needed to target the most vulnerable authentication challenges because the rest could be bypassed.



"In an anecdotal evaluation of postpaid, as opposed to prepaid accounts at three carriers, presented in Appendix A of this report" - and, by the way, I have a link to their whole report in the show notes.  They said:  "We also found, very tentatively, that some carriers may have implemented stronger authentication for postpaid accounts than for prepaid accounts.  To quantify the downstream effects of these vulnerabilities, we reverse-engineered the authentication policies of over 140 websites that offer phone-based authentication.  We rated the level of vulnerability of users of each website to a SIM swap attack, and we plan to publish our findings as an annotated dataset.  Notably, we found 17 websites on which user accounts can be compromised based on a SIM swap alone, in other words, without a password compromise."



So first of all, so as not to keep anyone in suspense, the bad news is that the five mobile carriers found to be vulnerable were AT&T, T-Mobile, Tracfone, US Mobile, and Verizon.  In all five cases, the authentication procedures were found to be vulnerable and allowed attackers to readily conduct a SIM-swapping attack.



We should pause for a second, Leo.  You and I were just talking about this, what, a couple podcasts ago, how because of the assumption that having the phone in your possession authenticates you, that you don't - and what we learned was that in the U.S., unlike in Europe, in the U.S. SIM exchange can be done by telephone, requiring no physical presence in a store.  So you're not having to go to a carrier's physical location, present yourself, show your driver's license or student ID or anything, you know, photo ID, in order to demonstrate who you are.  Rather, this is just done over the phone.



And so in the show notes I have a picture of the interaction between a customer service representative and an adversary.  So they call up.  They phone one of these carriers in this particular flow, claim to be the victim.  They request a SIM swap on the account.  The customer service representative says, okay, fine.  What's the PIN number on the account?  The adversary intentionally provides an incorrect PIN.  Well, or a PIN.  Maybe they get lucky, but probably not.  So the customer service representative says, "I'm sorry, sir, that's not the proper PIN."  And so they notify the adversary of this authentication failure.  And the adversary says, "Oh, crap, maybe I wrote it down wrong.  Okay, I guess I don't have my PIN."



So the customer service representative says, "That's quite all right, sir.  We would like to know two recently dialed numbers on that phone."  So the adversary correctly provides two recently dialed numbers.  The customer service representative looks those up and says, oh, yeah, there they are.  Perfect.  We will fulfill your SIM swap request.  And in doing so, the attacker's SIM is then associated with the victim's phone number.  All subsequent text messages and phone calls come to the attacker.



And of course then we know, I mean, one of the other things they found was that 17 websites out of the handful that they chose, that's all you need is, oh, sorry you can't log in.  We'll send a text message to your phone.  What is it?  And then of course the text message then goes to the attacker, who types it in.  Ah, well, welcome back.  What do you want to do?



So here's what they found.  There are three key findings.  The first is mobile carriers use insecure methods for authenticating SIM swaps.  Specifically, one of the things they will ask for is the last payment that was made.  They wrote:  "We found that authenticating customers via recent payment information is easily exploitable.  AT&T, T-Mobile, Tracfone, and Verizon use payment systems that do not require authentication when using a refill card.  An attacker could purchase a refill card at a retail store, submit a refill on the victim's account, then request a SIM swap using the known refill amount as the authentication."



LEO:  And that's why this is easier on a prepaid account than a postpaid account.



STEVE:  Right.



LEO:  You do buy those cards, yeah.



STEVE:  Right.  So again, a little bit of cleverness.  And, frankly, as I was reading these things, these are so bad, but also kind of clever, that I was wishing this hadn't been made public because, you know...



LEO:  Why, you want to use them yourself?  What?  Oh, because anybody could do it, yeah.



STEVE:  Yeah.  I don't want to be a victim, and this sort of makes it like, okay, here's your guide to SIM swapping.



LEO:  Yeah.



STEVE:  Also, second way of verifying somebody, recent numbers.  They said:  "We also found that using information about recent calls for authentication is exploitable.  Typically, CSRs" - and I meant to look up what that is.



LEO:  Customer service reps.



STEVE:  Thank you.  "Customer service reps requested information about outgoing calls."  They said:  "Consider the hypothetical following attack scenario.  Using only the victim's name and phone number, our simulated adversary could call the victim and leave a missed call or message that would prompt the victim into returning the call to a number known to the attacker.  This call would then appear on the outgoing call log, and the attacker could use it for authentication."



LEO:  Which is why I never call anybody on my phone, ever.



STEVE:  I know.  "Customer service reps appeared to also have the discretion to allow authentication with incoming call information, as this occurred..."



LEO:  Oh, that's even worse.



STEVE:  I know, "...four times between AT&T, T-Mobile, and Verizon."



LEO:  Don't they know anybody could call a phone?



STEVE:  Exactly.



LEO:  It's crazy.



STEVE:  I mean, that's how bad this is, Leo.



LEO:  Oh, I know, my mom called me.  Here's her number.  And that would work.



STEVE:  Yes.  "An attacker can trivially generate incoming call records by calling the victim."  Duh.



Personal information.  They said:  "We found that Tracfone and US Mobile allowed personal information to be used for authentication.  While our attacker did not use this information, it would likely be readily available to real attackers, for example, data aggregators, and is often public, so it offers little guarantee of the caller's identity.  We note that for over a decade FCC rules have prohibited using 'readily available biographical information' to authenticate a customer requesting 'call detail information.'"



LEO:  Like mother's maiden name.



STEVE:  Yeah, exactly.  Doesn't matter.  Finally - or, no, there's three more.  Account information, they said:  "We found that AT&T, US Mobile, and Verizon allowed authentication using account information.  As with personal information, this information would often be readily available to an adversary.  Receipts, whether physical or electronic, for example, routinely include the last four digits of a payment card number.  We note that PCI DSS, the industry standard for protecting payment card information, does not designate the last four digits of a payment card as 'cardholder data' or 'sensitive authentication data' subject to security requirements.



"As for the activation date associated with an account, that information can be readily available from business records via a data aggregator, inferable by website or mobile app logs via user-agent logs, or inferable via mobile app API access."  They say:  "We note that FCC rules also prohibit using 'account information' to authenticate a customer requesting 'call detail information.'"  But once again, AT&T, US Mobile, and Verizon all do it.



Device information:  "We found that all carriers except T-Mobile use device information for authentication.  These authentication methods include the customer's IMEI (device serial number) and the ICCID (SIM serial number).  Both the IMEI and ICCID are available to malicious Android apps, and IMEIs are also available to adversaries having any radio equipment."  So it goes over the air.



And, finally, security questions:  "We found that Tracfone used security questions for authentication.  We also found that T-Mobile, Tracfone, and Verizon prompted users to set security questions upon sign-up.  Recent research" - as we know - "has demonstrated that security questions are an insecure means of authentication because answers that are memorable are also frequently guessable by attacker."



And then, believe it or not, the second major finding:  "Some carriers allow SIM swaps without authentication."



LEO:  At all?



STEVE:  Yes.  "Tracfone and US Mobile did not offer any challenges that our simulated attacker could answer correctly.  However, customer support representatives at these carriers allowed us to SIM swap without ever correctly authenticating - six times at Tracfone, three times at US Mobile."



LEO:  Well, that's more than a coincidence.



STEVE:  And I have to say, because I'm approaching 65, I recently needed to sign up for Medicare because I'll be qualifying for that in a few months.  That led me on an interesting journey to unblock my credit reports that I'll be talking about in our Miscellany section.  And I discovered that just getting mad was all that was required in some cases in order to cause a representative to say, oh.



LEO:  Oh, he's mad.



STEVE:  Okay, sir.  I mean, I really wasn't, but it was like, I can't understand what you're saying.  What are you asking me?  And after doing that a few times, they said, okay, well, never mind.  It's fine.  And I was just like, what?



LEO:  You're old.  Whatever.



STEVE:  Oh, my lord.  So, finally:  "Some carriers disclose personal information without authentication, including answers to authentication challenges.  AT&T, in one instance, disclosed the month of the activation and last payment date and allowed multiple tries at guessing the day.  They also guided us in our guess by indicating whether we were getting closer or further from the correct date."



LEO:  You're getting warm.  You're warm.



STEVE:  You can't make this up.  Oh, no, sorry, you're going further away.  Unbelievable.  "Tracfone, in one instance, disclosed the service activation and expiration dates.  Neither are used for customer authentication at Tracfone."  Because in, what was it, three instances, they didn't require any authentication.



LEO:  Oh, god.



STEVE:  "US Mobile, in three instances, disclosed the billing address on the account prior to authentication.  In one instance, a portion of the address was leaked.  In one, part of the email address was disclosed."  Oh, my goodness.  "And in three instances the representative disclosed portions of both the billing address and the email."  Just to kind of make it easier because, oh, you know, we really do want to help you with this.



LEO:  Unbelievable.



STEVE:  So we're kind of in some deep trouble here, Leo, where as an industry we are clearly relying so much on the possession of our phone, whose communication can rather easily be commandeered and rerouted to an adversary.  It's just unbelievable.



LEO:  I'm updating Windows.  Cumulative update, Patch Tuesday.  Retry.  There were some problems, it says.  Uh-oh.  Got to have your Windows update today.  This is no day to put it off.



STEVE:  Actually, I didn't restart this machine.  The machine I use Skype with you on Leo is Win10.  And because I had time, I let it find the updates.  And it gave an error the first time.  It wanted me to restart.  And I thought no, no, no, no.  I'm not restarting.  So I just did it, I did a retry, and then it was able to go through the second time.



LEO:  That's what I'm doing, then.  All right.



STEVE:  Yeah.  Because, you know, I can't restart the same day that I'm doing a podcast with you.



LEO:  You don't know how long it'll take to get up.



STEVE:  Oh, my god.  A disaster.  So speaking of disasters, this is another one of those, if you, your organization, or anyone you know or care about is using a VPN by the name of Pulse Secure, stop listening right now, yes, and verify that they have patched their VPN server endpoint anytime since last April.



LEO:  Yikes



STEVE:  Of 2019, when an emergency patch to close a serious, let's just say as bad as it gets, remote access vulnerability was patched.  So as a consequence, as of the time of this story, nearly 15,000 Pulse Secure VPN server endpoints were not patched.  It's heavily used in corporate and government situations.  So we're talking nine months after the patch was made available, 15,000 were still vulnerable.



So the story begins, as I said, in April 2019, when Pulse Secure issued an advisory for their apparently aptly named "Zero Trust" VPN product, though I'm sure that's not the way they meant the name to be taken.  At the time, they warned organizations of an out-of-cycle patch which fixed a vulnerability in their product known as Pulse Connect Secure.  With organizations being as negligent as we know they are with the application even of Windows OS patches, where virtually all the work is done for them, you can imagine how often patching occurs outside of the Windows ecosystem.  It's rare.  In other words, next to never.



This particular vulnerability is as bad as it gets for a VPN.  Our listeners could probably write the next few sentences themselves.  It allows people without valid usernames and passwords to remotely connect to the corporate network and turn off multifactor authentication controls, remotely view logs and the cached passwords in plaintext, including Active Directory account passwords in enterprise environments.  So it is a full remote authentication bypass for what is apparently a widely deployed corporate and government VPN.



So that was in April.  Four months go by.  On the 14th of August 2019, someone posted an exploit for the issue on Kevin Beaumont's forum, OpenSecurity.global.  Then, a few short days later, a public exploit was dropped by Justin Wagner and Alyssa Herrera.  On August 22nd, with the help of BinaryEdge.io, which is a sort of commercial version of Shodan, which we know Shodan.  We often talk about that Internet scanning service.  BinaryEdge.io is the same sort of thing.



Kevin determined that someone was actively scanning the Internet for the Pulse Security vulnerability.  Kevin said that he sent up a flare that organizations needed to urgently patch this at the time four-month-old critical vulnerability.  On August 25th, Bad Packets scanned the Internet and found nearly 15,000 endpoints across the world still having the issue directly exploitable.  5,010 were located in the U.S.; 1,511 in Japan.  The U.K. had some, Germany had some, France, the Netherlands, Israel, Switzerland, Canada, South Korea, and then a total of all other countries, 4,052.  So all those others were between 800 and 300.  So a widespread authentication bypass on a VPN that lets people into the networks behind them.



So those results included networks at governments across the world and many incredibly sensitive organizations, so wrote Kevin, and what was essentially a list of the world's largest companies.  So it became clear organizations were not patching.  Since then, Bad Packets has been working with the various CERTs around the world, and other security-related groups, to try to get governments to wake up to this and get themselves secured.  Kevin works in corporate security, so he follows the ransomware scene and general cyberattacks because he needs to know what attacks are going on and what the attackers are up to.



He had seen the correlation between companies being successfully attacked, that is, with ransomware, and those who used Pulse Secure VPN.  And he saw from Internet vulnerability scanning that many of the organizations either had not patched their Pulse Secure system at the time of the incident, or had only patched recently.  It turns out that as part of the vulnerability, it's possible to install a backdoor in Pulse Secure systems and gain access subsequently, even after the VPN itself has been patched.



And finally, last week, Kevin spotted two notable incidents where recently breached companies had a strong reason to believe that Pulse Secure was the entry point into their networks for the breach.  And in fact it was the Sodinokibi, also known as REvil, ransomware that was installed.  In both cases, the organizations had unpatched Pulse Secure systems, and the footprint was the same.  Access was gained to the network.  Domain admin was then gained.  VNC was used to move around the network.  They installed VNC via PsExec, masquerading as Java.exe.  Endpoint security tools were disabled, and the Sodinokibi ransomware was then pushed to all systems using PsExec.  Which, by the way, is the very popular, one of the very popular Sysinternals tools.



Kevin wrote that he had now seen an incident where they can prove Pulse Secure was used to gain access to the network.  Today there are still more than a thousand unpatched, wide-open systems on the Internet, despite nine months of pressure to get these things closed.  And in Kevin's final update, he added that he had just learned that Travelex - and I had seen this independently - had been hit by a Sodinokibi attack.  He noted that Travelex had seven unpatched Pulse Secure VPN servers running.



So I think it's clear that as an industry we still have not solved the problem of communicating the urgency of patching and getting systems patched, especially things like a VPN that are by design exposed to the public Internet.  I mean, I've been saying don't let RDP be exposed.  Put it behind a VPN.  Yeah, but don't put it behind Pulse Secure.  Or do put it behind it after being patched.  Just, you know, here, 15,000 instances four months after this was made known.



So I don't know how we solve the problem.  I mean, and you could imagine each of these is going to have some different story.  The guy who was in charge of this got laid off, and the Pulse Secure account had his email address that was then discontinued because it was gone.  And so the notifications that they may have sent, I don't know if they did, but assuming that they were as responsible as they could be, they would have sent out notifications to everybody that they knew were using their VPN.  Well, it just bounced.  So what could they do?  Somehow this problem needs to get solved.  It's clearly still a big problem for our industry.



Oh, and speaking of patching right away, I will have a little bit of good news about Firefox in a minute.  In this case, it was a brief hiccup.  China's Qihoo 360, the security company that we've often mentioned, spotted a serious, as in CRITICAL in all caps, type confusion bug in Firefox's IonMonkey JavaScript JIT - we know that stands for just-in-time - compiler which was being abused in the wild.  And as we know, that makes it a zero-day vulnerability.  Just two days after Mozilla released Firefox 72, they issued an immediate update to patch and resolve this critical zero-day flaw.



As it happens, I was using Firefox over the weekend, and my Firefox said, hey, give us permission.  We need to restart right now.  And I said, oh, okay, and did.  And I got 72.0.1, which is the now updated with this just-in-time compiler zero-day flaw fixed.  And what's interesting about this is that attacking a just-in-time compiler is both clever and common because, as we know, compilers like interpreters tend to be a bit finicky.



But also, in the special case of a just-in-time compiler, it's their job to create new executable instructions on the fly.  That means they cannot be constrained by the usual safeguards provided by DEP, Data Execution Protection, which would otherwise prevent the execution of data, because executing freshly compiled data as instructions is precisely what JIT compilers must do.  So it's sort of an interesting aspect of just-in-time compilation that, by their nature, they're not able to have the same level of rigorous protection that statically compiled code is able to provide.



Qihoo 360 notified Mozilla of the trouble, indicating that they discovered the flaw after observing targeted attacks occurring in the wild.  Mozilla instantly fixed it and pushed out a fix.  So anyway, everybody should be using 72.0.1.  And, you know, I've seen Firefox waiting to be asked.  Maybe they're not because this is a critical problem, and so it'll be a little more aggressive.  But I've often gone to the Help>About Firefox to see what I'm running, and that woke it up to the availability of an update, which I then of course immediately allowed it to apply.  So it might be worth, if you're a Firefox user, just going under Help>About Firefox and making sure that you have 72.0.1.  And if not, you'll get it right then.



LEO:  Oh, it won't download it and then just have it, and then if you restart?  Because I know it can't obviously apply it when it's open.  If you quit it and open it - see, most people never quit their browser.  That's part of the problem.



STEVE:  I know, that's the way now I'm just - it lives over on my lower left screen.  It's like my portal to the Internet, yeah.  And so I don't think...



LEO:  Restarting your machine isn't a bad idea either once in a while; right?



STEVE:  That's a good thing to do, too.



LEO:  The whole thing; right?  Yeah.



STEVE:  It does, it would flush any little RAM critters out that might have crawled in.



LEO:  RAM critters.  Hate that when it happens.



STEVE:  Speaking of crawling in, I had to crawl out, Leo.



LEO:  Uh-oh.



STEVE:  Of a movie theatre on Friday.



LEO:  Yeah, so you didn't like, what is it, DX?  You went to see what DX?  4DX?



STEVE:  4DX.



LEO:  What's that?



STEVE:  I didn't realize what I was getting myself into.  And I would never have imagined that I would walk out of a Star Wars movie.  Now, okay.  Bar Bar Jinks or whatever...



LEO:  Oh, I know what this is.  Jar Jar, yeah, yeah.  No, no.



STEVE:  Jar Jar, god.



LEO:  This is with the moving chair.



STEVE:  Oh, my god.



LEO:  We have these in Petaluma.  They don't call it 4DX.  They have another name for it.



STEVE:  Oh, well, that - oh.



LEO:  This one looks like it moves more than ours does.  Ours is just a little bit of movement.



STEVE:  My first indication that perhaps I'd made a serious mistake in choosing the theatre was when I spotted the control on the arm rest to turn off being sprayed with water.



LEO:  Okay.



STEVE:  I'm not kidding.



LEO:  Yeah, ours doesn't do that.



STEVE:  There's a button on the arm rest whether you want water spray or not.



LEO:  Oh, my god.



STEVE:  And that was - I was a little concerned.  The chairs were in sets of four.  And there was all this extraneous stuff lining the edge of the theatre on both sides.



LEO:  Including apparently water spritzers.



STEVE:  It was unbelievably wrong.



LEO:  Even the guy in the ad doesn't look that happy about it.



STEVE:  No.  He's gripping his cup holders on either side for dear life.



LEO:  Help me.  Help me.



STEVE:  Lorrie ended up having to kind of sit forward because the seat was just throwing us around so much.  And, I mean, she's no weenie.  She, like, her comment was, "The ones at Disney are done right," where if you're going on the roller coaster over the edge, you dip forward.  I mean, for example, we're looking at the Millennium Falcon dodging and jumping and things.  It's in the distance, and we're being thrown around in our seat.



LEO:  Uh-oh, yeah.



STEVE:  Even though it's not like we're in the Millennium Falcon and we're seeing the star field out the front.  We're, like, passive observers.  And there's strobes flashing on the side.  There's clouds of mist coming up from both sides of the screen.  It's the most - it is so ridiculous and wrong.



LEO:  Didn't you suspect something when you saw that the ticket price was $26.70 each?



STEVE:  That should have been my clue.  The good news is we left after about 30 minutes.  Finally my buddy Mark kind of looked at me, and he said, "Are you kidding me?"  And I said, "Let's just get out of here."  And so...



LEO:  It's a shame because it spoiled the movie for you.  I mean, it's...



STEVE:  Oh, Leo, it threw you, almost literally, out of the experience.  I mean, you couldn't pay attention.  And I do have to say that what I saw of the movie wasn't that impressive.  It looked like a whole bunch of frenetic action just for its own sake.  On the other hand, this had to be an incredibly expensive installation.  Mark was commenting, he said, "Did you see the people trying to get into their seats?"  Apparently someone showed up late, and they were like, their popcorn was flying in the air because they were trying to sit down, and this thing was jumping around all over the place.  Oh, my god.



So, oh, and the other thing, the other problem was it was in 3D on top of it being in, like, 4D.  And I looked, I couldn't believe the glasses were red/green tint.  They weren't the RealD 3D that we talked about years ago which use a really cool clockwise and counterclockwise polarization.  This was red and green 3D.  I thought, what year is this?  Anyway, enough said.  Just a little heads-up for any listeners who want to go see Star Wars.  And Lorrie said, you know, this is for 13 year olds.  Maybe.



LEO:  Well, Star Wars is for 13 year olds, too.  But we have - you went to 4DX.  We have D-Box in Petaluma.  But it doesn't spit at you.  It just does a little bit of this.



STEVE:  Apparently there are six different smells that it can also...



LEO:  Oh, I don't want smells.  No, thanks.



STEVE:  I'm not kidding you.  I'm not kidding.



LEO:  The real problem is, as you can tell, the moviemakers don't make the effects.  The 4DX guys make the effects, right, after the fact.



STEVE:  And they're trying to sell how wonderful it is that you could just, you know, you bring a martini, it'll be shaken and not stirred.



LEO:  They should have really said you have to be this young to ride this ride.  I don't think they should have let you in.



STEVE:  The good news is there was no complaint about getting our money back.  I said, "Come on, this is ridiculous."  And this sounded like it's not the first time that it happened, either.



LEO:  Yeah.  Yeah, they knew.  But do go, you know what, I like the movie.  You can wait till it gets, you know, watch it at home.  But I liked it, only because it completes the adventure.  It's the end of the nine-movie series.



STEVE:  Yes.  And I do have to see it.  We did enjoy "The Mandalorian."  And because my Disney subscription is up on the 24th, last night I just sort of browsed through to see if there was anything else I wanted to see there, and we watched the first half of the making of the trilogy.  Which I really - we ended up stopping halfway because it was bedtime.  But we're going to absolutely finish it.  It was really fun.  It's two hours long, and it's the inside story of how the original trilogy got made and almost didn't get made, and all the things that went wrong.  And anyway, so if anyone hasn't seen it, I would say it's probably worth finding somewhere. 



LEO:  I have Disney Plus.  I've yet to watch one thing on it.  So okay, now I'm going to watch that, I guess.



STEVE:  Well, and the only thing that Lorrie thinks "The Mandalorian" was worth was Baby Yoda.



LEO:  Yeah.



STEVE:  You know, everybody thinks is wonderful.



LEO:  Hit with the kids.



STEVE:  So I don't think I'm going to continue.



LEO:  Yeah.



STEVE:  And I mentioned applying for the first time for - I needed an account at Social Security in order to sign up for Medicare.  And I immediately got blocked when I was trying to create my Social Security account, and I was told to call a toll-free number.  So I thought, okay.  And so I called.  The first question I was asked was "Have you locked your credit bureaus?"  And I said, "Yes, I have."  And she said, "Well, that's why we cannot process you electronically.  Here's a code.  Go take it to your local Social Security office in order to proceed."  So I did that.  And that was all fine.  But so, and the point was, I said to her, I said, "Well, yeah, I locked it to prevent identity theft."  And she says, "Yes, well, it's working."



LEO:  Yes, it is.  Yes, it is.



STEVE:  And I said, "Oh, that's cool."  So but the other thing that...



LEO:  I have two-factor on my Social Security account.  But guess what the second factor is?  A text to my phone.



STEVE:  Uh-huh.  Yeah.  Yeah.



LEO:  I guess, you know, what are you going to do?



STEVE:  So the other thing I decided to pull the trigger on, since basically everything I buy is through Amazon, is the Amazon store card because, as a Prime member, you get 5% off across the board.  And it's nuts that I'm not getting 5% off of pretty much everything I buy.



LEO:  True.



STEVE:  So in order to qualify, because my credit is locked - and I'm telling everybody this because long ago we talked about locking Equifax, Experian, and TransUnion.  I just wanted to share my experience, which was that I was able to unlock all three.  There is no charge for doing it.  And all three now allow you to do a transient unlock, a temporary unlock.



LEO:  That's great.



STEVE:  Which is what I - yeah, it's very nice.  I was able to say that I wanted my queries to my reports allowed until January 10th, and that allowed me a few weeks of time for Amazon to pursue checking my accounts and so forth.  And it all worked.  So each of them - I have in the show notes, if anyone is interested, I have three toll-free numbers for Equifax, Experian, and TransUnion, which are what you call if you want to do a temporary unlock of your credit.  And you're able to give them a date where you want it then to automatically relock.  And there's no charge for this.  So things have improved since the days that we first talked about this.



LEO:  They improved because Congress made them.



STEVE:  Yes, I know.



LEO:  They didn't improve on their own.



STEVE:  And I'm thankful, thankful for that.



LEO:  But they used to charge for locks.  They used to charge for unlocks.  They used to charge in some states as much as $40 to lock and unlock, and Congress finally said, no, you can't charge a thing to do either.



STEVE:  That is so wrong.



LEO:  And so, yeah, it was terrible, yeah.  So lock.  There's no penalty to locking.  And I like the transient unlock.  I think that's great.



STEVE:  Yes.  And I wanted to tell our listeners again, I mean, I'm sure everybody has heard the horror stories about identity theft.  Identify theft occurs when somebody typically applies for credit in your name.  And then the credit grantor checks your credit in order to grant the credit.  Now somebody who's not you has the ability to incur debt that is essentially yours.



Anyway, so the point is, if you're a person who's actively needing to get credit, then it's probably an inconvenience to lock things down.  But if you're like a lot of the listeners of this podcast, and you and I, Leo, where we're sort of past the age where we're applying for credit, you know, we're not newlyweds with kids and buying things...



LEO:  Only because you never asked me, Steve.



STEVE:  I really think locking one's credit is a slam dunk.  It's a no-brainer.



LEO:  Just understand that that means you can't get credit.  You can't buy a car.  You can't, you know, you can't get a car loan.  You can't get a house loan.  You can't get a credit card.  That's what you want.  And it's nice that they give you this transient unlock so that you can do those things and then go back to a secure state.  I think it's really good, yeah.



STEVE:  So I was in the process of sort of looking at my domain names, GRC's domain names, because I've got a lot of things that are something.grc.com - www.grc, news.grc, SQRL.grc, blog.grc and so forth.  And EV certificates cannot have wildcards.  And as we've been talking about recently, EV certs are kind of like, eh, okay, well, the browsers aren't showing them anymore, so what's the point?  And given that you have to have a bunch of EV certs or a multidomain EV cert, that is, EV certs don't allow you to use wildcards, I'm seriously considering switching back to, you know, I do not want to do Let's Encrypt.  It's not often talked about, but Let's Encrypt and any of the ACME-based automated cert issuers are really being used and abused now to the point where I wouldn't be surprised if at some point browsers put up a little indicator to indicate that, yes, you have a secure connection, but this is from an automated certificate, not one that has had a human in the loop.



I'm going to keep using DigiCert because they're my Certificate Authority, and I have no problem with satisfying them that I am me in return for having a certificate that a bot did not issue.  I mean, I get it.  There's places for bots.  But I don't think that's the case for certainly a security-oriented ongoing enterprise.



Anyway, the point is grc.sc is one of the domains.  That's my little shortcut domain.  And something, one of the other domains I have is grctech.com.  I have grctech.com, which I created in order to have a third-party domain not under GRC, in order to check cookie handling in browsers.  And this is years ago.  And so I was thinking, yeah, you know, maybe it's just time to shut that whole thing down, that I don't have to bother with grctech.com.  I thought, I'll just take a look in on the cookie-handling stuff.



So I went to - and I'm going to recommend our users do, because what I found was surprising.  I created a new shortcut, grc.sc/cookies.  I first ran it in Firefox because that's my default go-to browser.  And it went through the test.  Grc.sc/cookies does an extremely comprehensive test of your browser's current cookie-handling settings.  Came up perfectly in Firefox.



And I'm not sure what caused me to check it in Chrome.  But I did.  And Chrome has a problem with cookie handling, which I never knew.  It does not properly handle first-party persistent cookies.  And the page that came up, my own page, demonstrated the fact, and it explains it in English, what's going on.  The page explained this browser's exchange of first-party persistent cookies is enabled, and some cookies are being freshly exchanged.  Some anomalous cookies are also present, so please see the additional points below.



And then down below I explain:  "The first-party persistent cookies shown above as stale, in orange, was previously accepted by your browser.  An updated fresh cookie was just offered to this browser, as indicated by the cookie's label being black, but this browser ignored the updated cookie and instead returned the stale one it already had."  And I happen to know because it shows that was 182 days, 21 minutes, and 7 seconds old at this time.



Anyway, Chrome has a problem with the first-party persistent handling of cookies set on icons, which may or may not be a problem.  Anyway, I just thought it was interesting.  I wanted to bring it to our listeners' attention.  Grc.sc/cookies will take you to GRC's Cookie Forensics page.  And I think I'm going to leave it up and leave it running and renew the grctech.com domain because it looks like it's still useful.  I had assumed all browsers had fixed this.  Back then, when I initially created it, there were all kinds of problems with browsers and handling cookies wrong for different types of content.  They'd been fixed, I thought.  But not so for Chrome.



LEO:  You know, it's funny because it's today the story came out that Google says it's going to phase out support for third-party cookies in Chrome within two years.  They won't have it at all.



STEVE:  No kidding.



LEO:  It won't even be in the browser.



STEVE:  Whoa.



LEO:  Yeah.



STEVE:  I did not hear that.  Hallelujah.



LEO:  Yeah.



STEVE:  Wow, that's going to change the world.



LEO:  They're also, weirdly, phasing out support for user-agent strings.  I don't know...



STEVE:  They're just not going to do it?



LEO:  Apparently.  They're going to do something called "client hints."  This is all - in fact, I'd love to get at some point a show about this - part of their new privacy sandbox project.  And so they're doing a lot of things to - which is weird because Google, of course, an advertising company.  But I think they see the writing on the wall.  So if they're not to lose market share to Firefox and Brave and everybody else, Edge, they're going to have to do something about Chrome.  And so they're changing a lot of things - user-agent strings and third-party cookies.



STEVE:  Good for them.



LEO:  Yeah, yeah.  It's interesting.  I don't, you know, I think that, well, I'm always skeptical when Google says, oh, we're going to protect your privacy.  But in this case it kind of sounds like they're going to.  Oh, yeah.  We're all about privacy, yeah.



STEVE:  So before we take our final break, I'll just note, as we said at the top of the show, that today is the final day for Windows updates.



LEO:  Get them.  Windows 7 updates.  Yeah, get them.



STEVE:  Sorry, Windows 7 updates. And presumably, well, and we know, as you said, Leo, we believe this Crypt32 API in Windows  needs to be fixed.  So it's being fixed, so that's good.  I've pretty much made peace with Windows 10.  That's what Lorrie's running.  I'm about to rebuild my "A" system, which I've just been using a closed Lenovo, the Carbon X, which I like a lot.  But my tech support guy, Greg, has been saying that his laptop is really old.  So I thought, you know, I never really use it.  I'm going to give it to him.  So I'm going to rebuild a system for myself based on an Intel NUC, which I really like those.  That's what I built for Lorrie, and I'm really happy with it.  And I'm going to set up Windows 10.



So Windows 10 will be one of my main go-to platforms.  You know, you have to strip all the crap out of it and spend some time making it usable.  There's no way I'm having Candy Crush Soda Saga on my menu.  But it can be fixed.  So I just sort of wanted to set the record or update the record for where I stand.  I'm sitting in front of a Windows 7 machine which will stop getting updates after today.  And it's such a pain in the butt to set up a new system from scratch that, since I'm having to do it in one case already, I guess I could clone that.



LEO:  Yeah.



STEVE:  Maybe I'll do that.



LEO:  Except that it is pretty specific to the hardware.



STEVE:  Yeah.  Well, no, I would clone it with another NUC.  I actually have two identical.  I deliberately got the older NUC, the one that could still run Windows 7, because the newer one won't at all, when I thought I was going to end up using Windows 7 on it.  But I'm going to end up using Windows 10.  Because, again, I don't want to be stuck in the mud forever.  It doesn't make any sense.  Oh, and when I was looking at grc.sc, I'm seeing that the Windows 10 upgrade shortcut that I created is still getting a lot of use.  So I thought I would remind our listeners, grc.sc/win10 - W-I-N-1-0.  Yup, grc.sc/win10.  That just bounces you over to the link at Microsoft where you can download Windows 10 and install it on top of your Windows 7 system, and then you'll be getting upgrades or updates next month.



LEO:  Good.  All right.  Let's talk about Cable Haunt.



STEVE:  So some Dutch researchers discovered this.  As a consequence they checked Europe and found more than 200 million Broadcom chipset-based cable modems vulnerable to what they found.  I would imagine that probably means worldwide, what, half a billion?  That is to say, more than 500 million?  The website is CableHaunt, C-A-B-L-E-H-A-U-N-T, dot com.  And I have that link and a link to the report, their full PDF report in the show notes.



So let me explain what new horror we have here, and what it means.  Cable Haunt is the name given to a new critical vulnerability found in cable modems from various manufacturers around the world.  The reason so many various brands share the same problem is the very worrisome tendency we're seeing toward a monoculture.  Broadcom is by far the dominant supplier of cable modem core technology.  And Broadcom published some reference firmware which everyone copied.  Remember we saw this in the Universal Plug and Play, UPnP, where Intel published something that wasn't meant to be used, and everyone just said, oh, it works.



LEO:  This is an example, folks.  This is how you would do it maybe sort of.  But don't forget you've got to put error checking in.



STEVE:  Yeah, wouldn't that be nice.  So this vulnerability ultimately enables remote attackers to execute arbitrary code on vulnerable cable modems, and pretty much everyone's cable modem is vulnerable.  At this point it looks pretty much like all modems are vulnerable.  There are some, I'll mention some, that look like they're older, that are based on a TI chipset.  But, I mean, like my cable modem is vulnerable.  Looks like they all are, with exceptions.  So this exploitation is accomplished indirectly through an endpoint on the internal local network.



LEO:  So they have to be inside the house.



STEVE:  Well, they have to be on your browser.  Your browser can do it.



LEO:  Ah.  So malware can do it, yeah.



STEVE:  Malware could do it.  A bad ad could do it.  A compromised IoT camera.  Anything on your LAN is able to do this.  Through this malicious communication a buffer overflow could be exploited to gain control over the cable modem.  So the researchers write:  "There are an estimated 200 million cable modems in Europe alone.  With almost no cable modem tested being secure without a firmware update, the number of modems initially vulnerable in Europe is estimated to be close to 200 million."



LEO:  Wow.



STEVE:  "However, it is difficult to give a precise estimate of the reach of Cable Haunt."  And of course everybody's busy testing their modems.  There is a Linux script, a Linux Python script available, so Linux users can test.  Unfortunately, it doesn't look like there's an easy way to do it in Windows.  I'm hoping, you know, it's the kind of thing I would normally do one of my jiffy quickie things.  But everyone wants me to get back to SpinRite 6.1 - and I do, too - so I'm not going to do it.  And the other thing is it's just not that difficult.  I'm sure this time next week I will be talking about some freeware that has been created to allow people to check their environments.  There are some things you can do immediately that are cool, that I will share during this podcast.



So the reason, they said:  "The reason for this is that the vulnerability originated in reference software, which has seemingly been copied by different cable modem manufacturers when creating their cable modem firmware.  This means," they're saying, "that we have not been able to track the exact spread of the vulnerability and that it might present itself in slightly different ways for different manufacturers.  We have contacted as many of the largest ISPs" - and they're talking Europe - "and manufacturers as we could ahead of time, to give them time to fix the issue, but with varying success.  Some of the contacted ISPs have informed us that they have or are rolling out firmware updates.  However, we're still missing updates from several, and some have wished not to be listed on this website.  The ISPs that have confirmed their modems are secure can be found below."  And this is in their report.  And actually their website has a bunch of FAQ expandable tab things down at the bottom where there's a lot of additional information.



"The affected component is the cable modem's core OS, which is eCos" - e-C-o-s, which I had seen before.  It's a widely popular embedded multithreaded real-time OS.  Being an embedded OS, it's meant to be small and fast and lightweight.  And since it also only runs its own trusted compiled-in code, it completely lacks any of the now common anti-malware preventions such as address space layout randomization, protections against stack smashing or execution and other mitigations.  Thus it freely allows execution of code on the stack.  That is, it never expected to have a problem, so it doesn't check for it.



LEO:  I've just checked.  All three of my cable - four of my cable modems are vulnerable.  My Arris and my Netgear, yeah.



STEVE:  I know.



LEO:  But I run that myself.  My ISP doesn't.  I got my own.  So does that mean - my ISP can't fix it.  I have to fix it; right?



STEVE:  No.  I also got my own.  I have a Netgear CM1000, a DOCSIS...



LEO:  That's what I have, yup.



STEVE:  Yup, a very nice DOCSIS 3.0.  I know because I have a friend at Cox, thanks to this podcast, who updated my firmware on that.  And it is not the case that a subscriber is able to apply firmware themselves.  Only the WAN side, only your ISP is able to update the cable modem.  They refuse to have foreign firmware attached to their network is the way they think of it.



LEO:  But if you bought it yourself, it's still foreign firmware if you update it.  Only the cable company can do it.



STEVE:  Yes.  You're unable to update the firmware on your cable modem.



LEO:  Now, some ISPs are doing an update.  Eric in our chatroom lives in Sweden.  His Swedish ISP, Com Hem, did a security patch for Cable Haunt.  It took out 300,000 customers for an hour.



STEVE:  Yes.  That's going to happen.



LEO:  Oh, my god.



STEVE:  But on the other hand, that's good news because, well, just wait till you hear how bad this is.  So there's no protection.  The result is that exploits are unusually easy to write, to implement, and to run with high reliability.  Code is always at a fixed known address since there's no randomization.  And the OS never performs any stack sanity checking, making it entirely vulnerable to buffer overflow attacks.  This made finding the Cable Haunt exploit easier for them and makes it that much easier to be exploited.



LEO:  The irony is that code was probably written in the room I am sitting in right now.  This used to be a Broadcom facility.



STEVE:  Now, get this.  Our cable modems all have, and I never knew this, I ran it this morning, a built-in spectrum analyzer which can be used to identify connection troubles with the cable system.  Although the port which exposes the spectrum analyzer may vary by modem make and model, it is readily discoverable with any port mapping tool such as Nmap.  And the proof-of-concept Python script performs this port scan to locate the spectrum analyzer. 



Now, first of all, there is a very cool port scanner which I used this morning that I will recommend.  I used GRC's shortcut.  It's grc.sc/aps - advanced port scanner - grc.sc/aps.  It's free.  Does not require installation.  You can install it if you want to.  Nice-looking gal on the website home page.  They produce advanced-port-scanner.com and also advanced-ip-scanner.com.  Both are free.  Both are cool.  The IP scanner, I ran it, and it found all of the systems that I have installed on my LAN across the entire IP space.  



LEO:  It's Windows only, unfortunately.



STEVE:  It is.  But there are, you know, LAN port scanners are a dime a dozen now.  You could easily find one for Linux and for Mac.  So if you just google, like macOS port scanner, you know, or LAN port scanner, I'm sure you'll find a good one.



LEO:  Or you can even do it from a command line; right?  I mean...



STEVE:  Yes.  Yeah, yeah.  This one showed me, when I ran it, that port 80 was open on my cable modem, and port 8080.  But I'm getting ahead of myself.  So request to the port scanner on - and the IP address for my cable modem is 192.168.100.1. 



LEO:  Macintosh comes with a network utility app that has a port scanner built in.



STEVE:  Ah, very cool.



LEO:  So you don't have to use Nmap.  Nmap will do it, but, yeah, it's built into macOS; yeah.



STEVE:  Yeah, nice.  So my cable modem is at 192.168.100.1.  And it turns out it's listening for its normal web connection on port 80.  But it has a previously, unknown to me, spectrum analyzer that you can bring up with your web browser by going to that IP:8080.



LEO:  How do you - that's not your router?  That's your cable modem?  It has its own address?



STEVE:  Yeah, yeah.



LEO:  Your router also has its own address.  Don't confuse the two, obviously.



STEVE:  Correct.  Yeah, the router will be normally 192.168.0.1 or .1.1 is generally where today's routers live.  This thing is .100.1. 



LEO:  Got it, okay.



STEVE:  And if you put that address into your web browser, you generally go to that router.  And it'll say, you know, log in.  And you're able to look at some normal stuff like the number of corrected and uncorrected packets, how many errors it has.  You're able to spot - it's sort of interesting, if you've never done it before.



Requests to port 8080 are sent JSON formatted through a WebSocket.  A WebSocket is a JavaScript-y way of initiating a TCP or UDP connection over the Internet.  However, the JSON deserializer inside the cable modem allocates - get this, Leo - a predefined amount of memory for each JSON parameter.



LEO:  Oh, I can see the problem already.



STEVE:  Uh-huh.  It will keep reading input parameters.



LEO:  Sure, as long as you give them to it.



STEVE:  Until a comma is reached in the input stream.  Not surprisingly, this can be easily exploited by a malicious request.  In their example, and I show them here in the show notes, the fStartHz, you know, the frequency, the starting frequency for the spectrum analysis, fStartHz parameter, has a larger value than was allocated in memory and will therefore overflow and overwrite the registers.  They said:  "To validate this, a JSON package with 200 A's as the fStartHz parameter can be sent through the serial connection to the cable modem.  This will crash the modem, and all register values will be displayed, showing that the program counter has changed to 0x41414141," which we all know is four capital A's in hex.  And so you can see in the show notes a JSON-formatted query and where they changed fStartHz to all capital A's.



The eCos OS saves the caller's registers, when a subroutine is called, saves the caller's registers on the stack and restores these before returning.  Therefore, if the variable registers S0 through S7 are overwritten, and the return address register is saved on the stack, as is the case, it's trivial to run any existing code in the system with the attacker's desired input variables.  They said in the report, although they did not bother to engineer the execution of their own code, they used Return Oriented Programming (ROP) to essentially execute any existing code on the system in what they describe as a Turing-complete manner, meaning they were able to use existing code just before return instructions to get anything done that they needed to get done, manipulating the system extensively.



This, then, they used to open a telnet server for external root access to the cable modem, allowing remote access to the entire system using the cable modem as a telnet server out to the public Internet.  Through this telnet connection they were then able to access a range of methods, including reading and writing arbitrary memory addresses, executing code from any memory address, including ones just written to.  They noted that the last steps vary from modem to modem, but they provide a complete example in Appendix B of their report.



So the attack can be executed by having the victim run malicious JavaScript.  They said a common avenue of attack would be a link that is opened in a browser, but could for example also be done through ads on a trusted website or an email client.  And as I noted, Leo, anything on your network has access out to the cable modem on your perimeter, so if a webcam got compromised, or any IoT device.



Anyway, so it turns out that the JavaScript running in the browser establishes a WebSocket connection directly to the modem through the local IP address.  Normally the WebSocket access would be security restricted, but it's up to the server to enforce the restriction, and they didn't implement that in the cable modem because we're all friendly.  This is on the LAN side.  Nothing malicious would ever happen.  And besides, who cares if you see a spectrum analysis?



What they have verified is that it is possible to change the default DNS server; to conduct remote man-in-the-middle attacks; to hot swap code or even entire firmware; to upload, flash, and upgrade the firmware silently; to disable the ISP's subsequent ability to remotely upgrade firmware in the cable modem; to change any config file and settings; to change associated MAC addresses; to change serial numbers; and to host a botnet.



So as we mentioned, unlike our routers, consumers do not update our own cable modems.  This is only done from the broadband WAN side.  And as I mentioned, thanks to this podcast I have a friend at Cox in Atlanta who's deep into the technology.  And in the past he has pushed for more updates to my Cox-connected modems.  So I'm sure it can be done.  It does cause an outage while the modem receives the firmware, shuts down, reboots, then relocks to the network.  But I think it's entirely foreseeable that pretty much everyone who has a cable modem is going to be seeing a brief outage.



LEO:  You can see why, though, an ISP is going to be reluctant to do that.  I mean...



STEVE:  Oh, lord, yes.  Because who knows...



LEO:  The calls they're going to get, it's going to cost them a lot of money.  I mean, Comcast is so huge, they're going to get a million support calls.



STEVE:  Yup.



LEO:  My cable went out.  My modem went out.



STEVE:  Well, and of course they've been pushing phone.  So now phone service will be out.



LEO:  Oh, that's right.  Anything Internet.



STEVE:  TV service, yes, TV service will go out.



LEO:  Not cable TV, but anything over your modem will.



STEVE:  Oh, you're right, you're right.  Cable modem, right.  So the digital phone service will go out.



LEO:  If you're watching Netflix, bye-bye.



STEVE:  Yup.



LEO:  Not that Comcast minds about that.



STEVE:  And I'm sure they'll do it at 3:00 a.m. so as to minimize the disruption to everyone.  But still, a lot of people have stuff, I mean, who knows, you know, streaming things, and they're just assuming that they've got their Internet up.  And when they find out it goes down, they'll have no idea why because there's no way for Comcast to notify anybody.



LEO:  Right.



STEVE:  So Threatpost updated their earlier coverage this morning by adding:  "As far as U.S. ISPs are concerned, a Cox spokesperson told Threatpost, 'We are rapidly testing all our in-home broadband equipment, determining any vulnerability and the best steps to mitigate as needed.'"  And a spokesperson with Charter told Threatpost that Charter is "currently working with each of our vendors to determine if their equipment is vulnerable and when we could expect to see a firmware upgrade."



Anyway, so as I mentioned, I ran the Advanced Port Scanner, grc.sc/aps, or just google "advanced port scanner."  It's a nice little bit of freeware with a good reputation.



LEO:  And as I mentioned, Mac people have something built in, in the network utility.  And at least my install, which is based on Ubuntu, and I bet you all Linux installs, netcat is installed.  So you probably already have that in Linux, as well.



STEVE:  Nice.



LEO:  Netcat.



STEVE:  So do that against the IP for your cable modem.



LEO:  How do you figure out what that is?



STEVE:  That's a good question.  I knew that mine was 192.168.100.1.  It is in the manual for your cable modem because there is an admin interface that wants you to change from the default.



LEO:  Oh, okay.



STEVE:  In some cases that will prevent the attack, although I believe, and this is spelled out in the website, for the modems they know of, it looks like there is a ready authentication bypass, as well.  But Leo, scroll down.  Look what I found.  I discovered this in my cable modem.  At :8080 - oh, what was really interesting, it was cool that - go a little bit further down.  When I went there under Firefox, what I got was "Spectrum Analyzer not supported in this browser.  Please use Safari or Chrome."  Which again, remember that the people who discovered the attack realized that using Firefox made you safe.  Firefox cannot be used as the jumping-off point for this.  On the other hand, remember that anything in your network can be.  So this needs to get fixed.



Anyway, there's a spectrum analyzer.  And it was, like, updating in real-time.  All this was like going "blump blump blump" and changing.  It was just amazing.  I had no idea that was in there.  So because I use pfSense, and I have a firewall between my LAN and my router, I installed a rule to block access to port 8080 at that IP.  So until Cox gets around to updating me, I'm secure.  And so that is certainly a short-term workaround that users who have the ability to add some firewall rules to their router could employ in order to...



LEO:  Say that again.  So if you block port 8080 inbound, that will...



STEVE:  8080 for me.  They say that the port... 



LEO:  Whatever your spectrum analyzer port is.



STEVE:  Yes, exactly.  And that's why you need the port scanner in order to for sure locate which is the port where that's operating.



LEO:  Somebody in the chatroom says, am I not vulnerable if I don't have a spectrum analyzer on my cable modem?



STEVE:  That's a good question.  You're not vulnerable if you don't have Broadcom.  And apparently all Broadcom modems do have the spectrum analyzer.



LEO:  Have the spectrum analyzer, okay, all right.



STEVE:  Yeah.



LEO:  And I was looking at the list.  It's all the commonly used modems.



STEVE:  I know. 



LEO:  And presumably Comcast has to be able to ping my modem and say what are you so it can apply the appropriate patch.  They can do that?  DOCSIS 3 will let you do that?



STEVE:  Yes.



LEO:  Yeah, okay.



STEVE:  Yes.  They absolutely have the ability.  In fact, my friend in Atlanta was able to put my cable modem under observation for a while.  In fact, this all happened, Leo, when we were having those dropouts on my connection.  I was able to get some amazing service, thanks to the podcast.



LEO:  By the way, it's happening right now.



STEVE:  Oh.



LEO:  And if you're doing a podcast, do not patch your modem in the middle of a podcast, also.



STEVE:  Also, Tom's Hardware has some coverage.  They said the Lyrebirds researchers say models known to be vulnerable include the Arris Surfboard...



LEO:  That's the one I have.



STEVE:  ...CM8200A, Arris Surfboard SB6183...



LEO:  That's the one I have, yeah.



STEVE:  ...Arris Surfboard SB8200, COMPAL 8284E, COMPAL 8486E, Humax HGB10R-02, Netgear CS3250EMR, Netgear CG3700EMR, Netgear CM1000.



LEO:  That's what I've got.



STEVE:  You and I both have those also.  That's what I have.  The Sagemcom F@st 3686, the Sagemcom F@st 3890, Technicolor TC4400, Technicolor TC7230, and Technicolor TC7300, although some firmware versions of those models may not be at risk.  And they talked about, oh, it said:  "We discovered that our aging Arris Surfboard SB6141 uses a TI chipset, so we're out of the woods.  But two later Arris models, the Surfboard SB6183 and 8200, do use Broadcom chipsets, and the latter is on the list of known models vulnerable to Cable Haunt."  So essentially, recent modems, and pretty much every modem.



So again, the reason I think this is worth thinking about is, I mean, like paying some attention to for our listeners, is that this is, like, hundreds of millions of cable modems.  The good news is they can be responsibly patched without end users needing to do anything.  The bad news is it's going to take a while for, I mean, and I don't know how long.  Weeks?  I mean, they're going to be in a hurry.  The cable companies are going to certainly be on this.  Broadcom has been asked about this.  They said that - this might have been Tom's Hardware.  Yeah, it was:  "We've reached out to Broadcom for comment, and a company spokesperson gave us this statement:  'We made the relevant fix to the reference code, and this fix was made available to customers in May of 2019.'"  So there's another thing that's annoying. 



LEO:  What?  What?



STEVE:  Yes.  This has been known and been available to all of our ISPs since May of last year.  And now, only because it came to light, they're all running around and scrambling.  So once again it's like, oh, well, maybe it's not a problem.  Uh-huh.



LEO:  Well, I know why they don't want to do anything about it.  This is going to be a big hassle for them.  They're knocking people's Internet out.



STEVE:  Yup.



LEO:  For at least some time, the time it takes to reboot.



STEVE:  Yup.



LEO:  There is a list also of vulnerable modems on the Cable Haunt site, too.  It looks like it's pretty similar to the Tom's Hardware list.



STEVE:  That's probably where they got it, yeah.



LEO:  Yeah.  Oh.



STEVE:  Yeah.



LEO:  And the thing that's frustrating is there's nothing you can do about it.  You have to just get your ISP to do something about it.



STEVE:  Correct.  The end user is unable to update their firmware.  It's only doable from the WAN side.



LEO:  Although there is this kind of weird fix where you block the spectrum analyzer port.  If you could figure out with a port scan which port it is, you might then use a firewall rule to block that port.  Where would the firewall, though, wouldn't that be on the router inside the cable modem?  Where would you - does the cable modem have a firewall, too?



STEVE:  Well, so if you have a cable modem, normally that goes then...



LEO:  Oh, I see.  I get it.  Because it's coming from inside the house.



STEVE:  Exactly.  



LEO:  So your firewall rule is not from outbound traffic coming into your cable modem.  It's from your computer going to your cable modem.



STEVE:  Right.  Right.



LEO:  So you want to block outbound traffic on that port.



STEVE:  Yes.  You want to block your router sending something to what it sees as the WAN, to 192.168.100.1, port whatever it is.



LEO:  So that's a little bit of research for most of us because we have to figure out, A, what our cable modem address is, and then what port the spectrum analyzer uses.



STEVE:  Right, right.



LEO:  Wow.  What a mess.



STEVE:  So some homework for all of our listeners.



LEO:  Yes.



STEVE:  It is a real mess.  It is, again, it's this monoculture is a problem.  If something big like this is discovered, and everybody is using the same one, I mean, it's potentially devastating.  Whereas if we had a much more heterogeneous environment, it'd be like, well, yeah, some people.  But my point is that there's no way that hackers are not on this right now.  I mean, there's just no way that they're not working to come up with an ad that they will stick into the ad stream that will be delivered to people's browsers, most of which are now Chrome, and will then attempt to access their cable modem, and can right now.



LEO:  Nice.



STEVE:  Yeah.



LEO:  I bet you it's not fixed for some time.  I'm just guessing.



STEVE:  That's my feeling, too.  It just feels like, you know, they've had it since May.



LEO:  Since May.



STEVE:  Yup.



LEO:  Criminently.  Well, thank goodness you listen to this show.  That's all I can say.  One more reason that you cannot miss a Security Now! episode ever.  Okay?  And tell your friends about this.  And now of course I'm responsible for my family's cable modems, not just mine.  So my mom and my sister, I've got to figure out how to fix it for them.  She's on Cox, so she probably has a - and she also has Cox phone service.  So it's going to take out her phone service.  That's a big issue.  I bet you there's FCC rules about that, taking people's phone service out?



STEVE:  Yeah.  For example, I know that when my neighborhood has a planned power failure, there are Cox trucks with generators running on all the little substations in order to keep the phones up.



LEO:  Terrible idea.  My mom said, "You told me to go do that."  I said, "No, I didn't.  I said the exact opposite.  Do not use your cable company for your phone service."



STEVE:  Yup.



LEO:  Okay.



STEVE:  I just abandoned my landlines, as a matter of fact, in the last few days.  All they were was just generating constant telemarketing calls.



LEO:  Right, who needs them?



STEVE:  And I thought, okay.  I mean, and I wanted to send a message, not that anyone cares.  But it's like, look, folks, you could have fixed this.  You chose not to.  I mean, I like a good solid wired copper connection, but sorry.  This is just ridiculous.  I don't even answer it anymore.



LEO:  Well, we'll keep our eye on this one, and you keep listening to Security Now! for this kind of very valuable information.  We do Security Now! on Tuesdays, 1:30 Pacific, 4:30 Eastern.  That's 21:30 UTC, part of a long day.  So sometimes it gets pushed, often it gets pushed back a little bit.  But just, you know, watch all day.  You know?  Just watch all day.  You can see or listen to our live stream at TWiT.tv/live.



There's also, of course, Steve's site, GRC.com, where you can get SpinRite, the world's best hard drive recovery and maintenance utility.  Steve, you haven't been plugging it lately, but I'm going to plug it for you.  Everybody needs a copy of SpinRite, if you've got a hard drive.  Especially these new - I now have a 16TB hard drive, two of them.



STEVE:  I heard you say that.  It's like, okay.



LEO:  450 bucks, 16TB, Steve.  It's amazing.  But that's a drive you really need SpinRite on before you use it.



STEVE:  Well, the new SpinRite.  The new SpinRite.  You can't even start on the old SpinRite now.



LEO:  Well, hurry up and finish SpinRite.  I got a bunch of those.



STEVE:  Yup, I'm on it.  



LEO:  Can I do an 8TB drive on it?



STEVE:  Well, I mean, you can run it, but it'll take...



LEO:  Forever.



STEVE:  The problem is it's - yeah.  Now, the new one runs half a terabyte per hour.  But that was actually the lower density drives.  Essentially the new SpinRite, the forthcoming SpinRite, will run at the drive speed.  That is, the maximum speed the drive can go.



LEO:  Nice.  Nice.



STEVE:  So the way we have gotten 16TB is the density has gone up so high. 



LEO:  Yeah.



STEVE:  So the density goes up.



LEO:  So, and those helium drives, they don't even have air inside them.  I mean, it's crazy.



STEVE:  Yeah, yeah, yeah, yeah.  So anyway, that means that the throughput should go up.  So we may be able to do a terabyte an hour.  And so it'd be feasible to run SpinRite before you put it in.  Even occasionally.



LEO:  My Synology, when you add the 16TB drive, does a parity check all the way through, and it took it days.



STEVE:  Oh, my god, as it would.



LEO:  Now I want to put another one in, but I don't know.  Oh, what a world.  We live in an interesting world, and this is the place you can find out more about it.  GRC.com.  He has 16Kb audio, which is great for bandwidth impaired.  He has actually the smallest version of the show, which is the human written transcript.  Elaine Farris does a great transcript.  That's a quick download.  A lot of people like to read along while they listen, and then use it for later study, underline it, annotate it, put it in a book, put it on the shelf.  That's all at GRC.com, along with SpinRite and a lot of other great free stuff, including what was that address for the cookie checker?  Grc.sc, which is his shortcut, grc.sc/cookies.



STEVE:  Cookies, plural.



LEO:  Cookies plural, okay.



STEVE:  Cookies.



LEO:  A lot of great - and then /windows10?  Win10, lowercase.



STEVE:  Yes.



LEO:  There's a lot of great stuff.  Steve's just - he's full of it.  Great stuff, that is.



STEVE:  I'm full of it.  All right.



LEO:  GRC.com.  We have copies of the show, 64Kb audio.  We've got video, too, at TWiT.tv/sn.  It's also on YouTube.  Best thing to do, get a podcast application.  Subscribe.  That way you'll get it the minute it's available each and every week.  You do not want to miss an episode of this show.  It's kind of "must" listening for anybody who's concerned about security online.  Steve, go enjoy the second half of your "Making of Star Wars" documentary.



STEVE:  Yay.  I'm going to, yes.



LEO:  And I will see you next week, right here. 



STEVE:  Okay, buddy.  Thanks.  Bye.



Copyright (c) 2020 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#750

DATE:		January 21, 2020

TITLE:		The Crypto CurveBall

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-750.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we look at Google's addition of iOS devices as full Google account logon hardware security keys, an update on Apple versus Attorney General Barr, a serious new Internet Explorer zero-day and how the vulnerability can be mitigated, the release of Microsoft's Chromium-based Edge browser, the FBI's reaction to the Pulse Secure VPN vulnerability, another new and CRITICAL RDP remote code execution vulnerability that has slipped under the radar, a bit of miscellany, and then we examine the headline-grabbing CryptoAPI vulnerability that's been dubbed "CurveBall."



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  Lots to talk about.  We're going to talk about Apple versus the FBI, Patch Tuesday, and that massive security flaw Microsoft did patch in Windows 10 and Server 2016.  However, there's now a new security flaw in Internet Explorer.  It's still not patched, but Steve has the fix.  That and a lot more, all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 750, recorded Tuesday, January 21st, 2020:  The CurveBall CryptoAPI.



It's time for Security Now!, the show that everyone has to listen to each and every week.  Because if you don't, god knows...



STEVE GIBSON:  You balls may get curved, Leo.



LEO:  Steve Gibson.



STEVE:  You never know.



LEO:  There's a reason he said that.  He's not just out of the blue.  Steve Gibson from the Gibson Research Corporation, our security guru.  Many of us have been taught that there's no one better than Steve Gibson, over the years, day in, day out, to keep up on what's going on in the land of security.  And you're right on top of this new CurveBall exploit, too, this year.



STEVE:  Well, actually I wasn't.  I think it was you who referred me to it when we were talking about it last week.



LEO:  It had just come out, yeah.



STEVE:  It had just happened.  It was part of the January Patch Tuesday patches.



LEO:  So new that it didn't have a catchy name yet.



STEVE:  It was known as the CryptoAPI flaw.  Now CurveBall.



LEO:  Now it's got a catchy name.



STEVE:  Yes.  And there's also a snarky something about crypto chains.  We'll get to it.  There are two proof-of-exploits now on GitHub.  But so what I had intended to talk about before whatever it was I talked about last week, was the failure of SHA-1 hashing.  And it turns out that gets, I mean, horrible as that is, even that gets pushed back again - hopefully I'll be able to get to it next week - because too much happened.  We have the addition of Google's iOS device family to their secure logon FIDO2 security key family.



LEO:  Yeah.  I saw this.  I want to ask you what it means.  I immediately took advantage of it, but I want to know what it means.



STEVE:  Yeah.  So it's a good thing.  



LEO:  It's a good thing.



STEVE:  We've also got a little bit of more back-and-forth between Apple and our Attorney General Barr, which I wanted to touch on.  We do have, and there's a takeaway for our listeners, a relatively serious new IE zero-day vulnerability which is, being a zero-day, it was discovered being exploited in the wild.  And despite the fact that it's IE, it's possible for sites to deliberately invoke IE, if that's what they want specifically.  And it's bad.  It's a remote code execution vulnerability.  So we've got to talk about that.  There's something, I did it immediately, that our listeners can also do because it uses sort of a fringe DLL that isn't in the mainstream, so you can just sort of say, no, we're just going to disable it to keep it from being abused.



We've got the release of Microsoft's Chromium-based Edge browser.  The FBI's official reaction to the Pulse Secure VPN vulnerability after that all hit the news.  I guess that's what we talked about last week because it was so potentially worrisome.  The FBI has told us some of what they know about it.  We've got, believe it or not, this thing slipped under the radar because of the CryptoAPI flaw that had everybody all worked up, a new critical Remote Desktop Protocol remote code execution vulnerability, reminiscent of BlueKeep.  We've got a bit of miscellany, and then we're going to examine the headline-grabbing CryptoAPI that, as we already said, has been given the name CurveBall.



LEO:  That comes from the fact that it's elliptic curve crypto that is broken.



STEVE:  Yes.  Well, it's an interesting flaw.  It was introduced five years ago, in July of 2015, just sort of in an update.  So, for example, Windows 7 wasn't ever vulnerable to it because it only affects 10, Server 2016 and Server 2019.  So it's things that have been, well, it's things that have been updated recently in this newer evolution.  Also Firefox is not vulnerable because they carry their own NSS.  They've always had their own security suite.



LEO:  Oh, interesting.



STEVE:  Google Chrome was, but they quickly updated Chrome in order to check for it.  But the vulnerability is pervasive.  And I'm sure we're going to see some exploits of it.  So anyway, a lot of fun stuff to talk about.  



So our Picture of the Week is just kind of fun.  It's not really apropos of anything except that it plays off of the crazy world that we're in now.  It just shows the front of a classroom with a teacher standing on one side of the blackboard, and a little kid looking at her, saying, "Before I write my name on the board, I'll need to know how you're planning to use that data."



LEO:  I love it.



STEVE:  That's right.  What are your plans?



LEO:  It's true.  Everybody's aware these days.



STEVE:  Yeah, yeah.  Unfortunately, we're all aware because of the abuses of that in the past.  And so we're trying to negotiate a compromise between the economic model that has evolved on the Internet of using this data because they can, and arguably individuals' more threatened rights to privacy than we've ever had before.



LEO:  Yeah.



STEVE:  Anyway, iPhone has joined Android in being a qualified Google account security key.  We talked about this last spring, I think it was last April, that Google allowed Android devices, essentially their devices, to double as a hardware security dongle, a physical token for use in providing multifactor authentication to Google's services.  Now that feature is being rolled out to users of iOS v10 and subsequent devices.



LEO:  So this looks like the single sign-on approval thing.  Microsoft does this with their Authenticator, too.  So when I log onto Windows, it pushes something out to Authenticator that then on my device, on my phone says, hey, is that you logging in at IP address blah blah blah.



STEVE:  Yup.



LEO:  And you say yes or no.  Which is much better, I mean, from a point of view of the user, much easier than typing in a six-figure TOTP.  Is it better, more secure than TOTP?  It's probably the same; right?  It's a push.  Is it a push?



STEVE:  I think it's probably - yes.  Yeah.  So it's probably comparable security.  What's interesting is that it does use Bluetooth, and Bluetooth must be active on both devices, both on the phone and the device you are wishing to authenticate on.  But they do not need to be paired, so it does not require a pairing relationship.  It just uses a little brief beacon-style communication.



LEO:  See, that worries me because isn't Bluetooth somewhat insecure?



STEVE:  Well, Bluetooth is radio.  On the other hand, your one-time token you're typing in on your keyboard; and we know that, I mean, there are all kinds of ways of hacking anything that involves your keyboard.  So you can imagine something which is intercepting your keyboard is it captures your one-time password when you type it in and then sends it somewhere for them to authenticate before you're able to authenticate.  I mean, so the whole issue of authentication at a distance is fraught with trouble.



LEO:  So this is different than I was saying.  So that push notification is just like a text message or an auth code where you're saying yes.  What you're saying is this is actually - my phone is physically an authentication device that, using Bluetooth LE, tells the computer, oh, yeah, that's him.



STEVE:  Yes.



LEO:  Does it involve any interactivity on my part?  Do I have to press a button on the phone or anything?



STEVE:  Yes.  Yeah, you do have to explain.  You have to confirm physically that...



LEO:  I have to unlock it.



STEVE:  So your phone has to be unlocked.



LEO:  I have to unlock it.



STEVE:  Exactly.  It's got to be unlocked, and then you've got to say, yes, that's me.



LEO:  So is it better than a YubiKey, for instance?



STEVE:  No.  I would say the advantage is it is similar to a YubiKey, but it's probably one that you already have at home.



LEO:  You already have it, yeah.



STEVE:  So now you've got both Android and iOS devices are part of this slowly expanding, I'm sure, carefully expanding range of devices like the YubiKey, like the Titan, which is Google's version of that.  So you download and install this Google Smart Lock app.  So there is an app that you need to install on your phone.  For anyone who's interested, I've got the link in the show notes.  But I'm sure if you just put in - I'm not sure because iOS or the App Store is so bad about finding things.  But Google Smart Lock application is what it's called.  You do need to turn on two-step verification or Advanced Protection on your Google account.  So you've got to go to your - in your browser, you sign into Google.  You'll be asked to...



LEO:  When they did this, by the way, I then decided to turn Advanced Protection back on because it made it more convenient for me.



STEVE:  Right, right.  Otherwise it was just - it was a pain in the butt to have to, like, oh, crap, okay, now I've got to go do that.  So then you'll be asked to sign in again to reprove that you're doing this, that it's you doing this, even though you've already signed in.  Then you scroll down.  I was a little puzzled, but it's there.



You scroll down to find "add security key" because essentially, even though it's not what we have thought of in the past as a security key, we've thought of that as a YubiKey, this is turning your phone, when it's equipped with this Google Smart Lock app, into a physical security key.  And among your options there you'll find iPhone.  You'll say, yeah, you want to add your iPhone.  So you click on "add."  Then you enable iPhone security key by tapping "Yes, I'm in," when prompted to in the Google Smart Lock app.  And so that sort of sets it up.



And from that point on, as long as you've got Bluetooth active on both devices, you sign in on a browser using Chrome OS or a browser running iOS, macOS, or Windows, or I'm sure Android.  And then in this case you would check your iPhone Smart Lock app where it should be saying "Is this you," and you verify by tapping "yes." 



LEO:  So let me do this, then, because I haven't done it yet.  You can see I've already used my Google phone that way.



STEVE:  Right, right.



LEO:  And I have a bunch of YubiKeys.  I like having more than one YubiKey in case I lose one, and I keep the other ones in a safe secure location.



STEVE:  And I should note that, even Google says, once you've done this, you should create another backup security key.  And I don't know what that means.  But the problem would be if anything happened to your phone, you didn't have it with you, and you needed to sign in, well, I mean, this is always the problem with tightening authentication is that it keeps the bad guys out, but it also keeps you out unless you're able to prove...



LEO:  I can't use both my Google Pixel and my iPhone.



STEVE:  Whoa.  That's interesting.



LEO:  So it says you may only have one built-in security key on your account.  That makes sense.  You don't want to have too many of these, either.  Since I have backup with my YubiKeys, I guess I'm going to go for that.



STEVE:  That's interesting.



LEO:  And for purposes of demonstration.  And then it just, well, you know, it didn't - maybe because I have Smart Lock open, it didn't do anything on Smart Lock.  It just added it.  But I guess it says, well, you've already verified.  By the way, it's not my 10s because - but I guess - this is a little concerning.  10s is an old phone.  This is now an 11.  Named 11.  It doesn't say 10s anywhere.  But I think Google still thinks it's my 10s.  So I'm just going to live with that.  That is not really reassuring.  But anyway, okay.  Google knows what it's doing.



STEVE:  Well, and this does, I mean, again, for example, I'm using a one-time password for my Google access, and I have long been doing so.  Google is good about allowing my accounts to be sticky.  They're good about notifying me if I have signed in on a machine, on something I have that they haven't seen before.



LEO:  Yeah, yeah.



STEVE:  So that's all good.  And I do have, you know, I'm using OTP Auth.  That's my favorite app on the iPhone.  I've talked about it before.  I like it because it gives me some flexibility.  I can create folders.  It has an appearance in the widgets on iOS, so like on my lock screen or on the whatever it is where you slide to the left from the home screen.  I'm able to access it there.  I'm able to put the few things that I use most there.  I'm also able to just have it copy the OTP into the clipboard, if I'm logging in on that phone.



Anyway, OTP Auth is the app that I chose there.  That's what I'm using.  And I'm not - I guess I should try this just because it's possible now, although all these limitations seem a little bit annoying.  But, you know, I guess it would be - I guess my point is it's not something that I'm doing often.  I don't find myself often having to log in to Google because my computers are secure, and Google remembers me there on a persistent basis.  But when I do, I'm just using a six-digit one-time password.  But I did want all of our listeners to know, for those who are interested in physical token security, your iOS device is now considered a physical token.



And when I was doing some of the reading into this, I learned something, Leo, I never knew, and I was impressed.  We've been talking a lot in the last year about the scourge of phishing attacks.  You know, phishing relies upon and preys upon human fallibility to perpetuate successful attacks.  Gmail blocks more than, get this, 100 million phishing email attacks per day.



LEO:  Wow.



STEVE:  A tenth of a trillion.  Wait, a tenth of a billion.  100 million phishing attacks per day are spotted and blocked by Google.  So just, you know, that's significant because human fallibility is inherently porous.  I've adopted that term "porous" to talk about security because I think it properly conceptualizes the nature of security, is if you push hard enough, if the pressure is enough, you're going to get - something is going to squeeze through a leak somewhere.



And so security is not perfect.  It's porous.  And the harder you push, the more you get.  And so, boy, being able to block all of that email that someone might click on, there's a certain percentage of people, they just, I mean, even if you're trained up and educated, you're not paying attention, someone interrupts you while your finger's hovering over the button, and you click it when you didn't mean to, and, ooh, that's all it takes.



LEO:  You know I'm constantly talking to our IT department, saying what are we doing, because we've got people in the front office who are maybe not as sophisticated as some.  And as you point out, anybody's vulnerable to this.  And I always worry we're going to get hit by ransomware.  One of the things we do, and this is because Gmail does such a good job, is we use G Suite as our corporate Gmail.  Our TWiT.tv addresses all go through Gmail.  So that's step one.



STEVE:  So you get the advantage of its filtering, yes.



LEO:  Yeah.  We do a lot of other things.  But that alone is pretty reassuring.  They catch a lot.  If you're using Gmail, you're not seeing nearly the number of phishing attacks you would be otherwise.  Maybe none.



STEVE:  And I do maintain a Gmail account.  I've had one forever.  And sometimes I'll look over in the spam side because I'll look at all the stuff that makes it through.  And I'm thinking, god, there's a lot of crap here.  Then I look over at what didn't get in.



LEO:  It's a lot more.



STEVE:  It's like, whoa.



LEO:  I think they do a really, really good job, both of spam filtering and threat filtering.



STEVE:  Yeah.  So I did want to sort of just kind of - we're going to be keeping an eye on what is happening with the U.S. versus Big Tech as regards encryption.  There was some continuing back-and-forth last week.  We started the discussion of it last week.  We know that Bill Barr, our U.S. Attorney General, wrote the letter to Apple's counsel saying we need you to unlock these two phones.  Bill Barr in a press conference said:  "We have asked Apple for their help in unlocking the shooter's iPhones.  So far," this is Barr talking, "Apple has not given us any substantive assistance."



And I suppose that the characterization as "substantive" is somewhat open to interpretation.  This is why people hate attorneys and politicians.  But based upon the level of Apple's arguably quite substantive cooperation, Barr must have meant that Apple didn't give them absolutely everything they asked for.



Apple replied to Barr's characterization by saying:  "We reject the characterization that Apple has not provided substantive assistance in the Pensacola investigation.  Our responses to their many requests since the attack have been timely, thorough, and are ongoing.  Within hours of the FBI's first request on December 6, we produced a wide variety of information associated with the investigation.



"From December 7th through the 14th" - which was last Tuesday - "we received six additional legal requests, and in response provided information including iCloud backups, account information, and transactional data for multiple accounts.  We responded to each request promptly, often within hours, sharing information with the FBI offices in Jacksonville, Pensacola, and New York.  The queries resulted in" - get this - "many gigabytes of information, which we turned over to investigators.  In every instance, we responded with all of the information we had."



And Apple added that it had received the subpoena for the second iPhone on January 8th and responded to the FBI's request within hours of receiving it.  So in Barr's somewhat unfortunate press conference he said:  "This situation perfectly illustrates why it is critical that investigators be able to get access to digital evidence once they have obtained a court order based on probable cause.  We call on Apple and other technology companies to help us find a solution so that we can better protect the lives of Americans and prevent future attacks."



So standing back from this a bit, this sounds more like Barr just continuing to bang the drum for change, and not actually expecting anything else from Apple.



LEO:  He knows they can't do anything.



STEVE:  Yes.



LEO:  And they've given him everything they can, which is more than enough.



STEVE:  Yes.  It's more.



LEO:  So this is, no, political at this point.  This is not about that investigation.  And really, as far as AG Barr goes, I seriously think that this is not about terrorism.  This guy wants to suppress dissent.  He wants to know everything that's going on.  And he's the flag bearer for the absolute most intrusive kind of surveillance.  It bothers the hell out of me.



STEVE:  We did learn a little bit more during this follow-up press conference.  Barr said:  "During the gunfight with first-responders, the shooter disengaged long enough to place one of the phones on the floor and shoot a single round into the device.  It also appears the other phone was damaged.  But our experts at the FBI crime lab" - which I find amazing - "were able to fix both damaged phones" - yes, you can shoot it, and we'll still be able to bring it back to life.  Anyway, so they are operational.  Maybe the bullet just bounced off its corner or something.  Anyway.



He said:  "However, both phones are engineered to make it virtually impossible" - and I would remove the word "virtually" because we know how they've been engineered - "to unlock them without the password.  It is very important to know with whom and about what the shooter was communicating before he died."



So I agree with you, Leo, completely.  We've previously covered extensively here that Apple has deliberately designed and implemented within their devices a robust system for encrypting the data stored within iDevices' nonvolatile storage.  And the upshot of that design is that no one, not even they, Apple, have any means or ability to decrypt it.  And as you said, I'm sure they explained this at great length in their replies.  I'm sure that, you know, we didn't see the subpoena that was issued that Apple responded to providing all the information they had, gigabytes of data.



LEO:  Yeah, they gave them the iCloud.  They probably gave them the Apple iMessages.  They know who he was talking to because the phone company knows that, as well as Apple.  By the way, there's also a point to be made.  There's some evidence that even the most recent iPhone 11 can be cracked by GrayKey.  Cellebrite and GrayKey we've talked about before, are firms that specialize in helping law enforcement break into these phones.  And there's significant evidence that GrayKey has an ability to break into the iPhone 11, the most recent.  And we know, thanks to - what was that exploit that we've been talking about on older iPhones?  We know that they can root the older iPhones.  So even - and I think Barr knows this.  This has nothing to do with this case.



STEVE:  Right, right.



LEO:  There's a good post in the Lawfare blog saying they already got everything they could ever want about these phones.



STEVE:  Well, and look at how much they got.  The other flipside is law enforcement is now buried in riches.



LEO:  Yeah, they wouldn't have had this before.  They've got everything.  They didn't need a wiretap.  They got it all.



STEVE:  Yes.



LEO:  So this is to me a misleading - this is a red herring.  What they really want is totally information awareness, Chinese government-style surveillance on all the citizens.



STEVE:  We know that if Bill Barr wishes to outlaw the commercial sales of such technology in the U.S., he'll need Congress to pass legislation to essentially outlaw the commercial sale of technology that doesn't have a backdoor.



LEO:  This is political, but I think they want to outlaw Congress, too.  So I think where they're heading is we'll just take care of this.



STEVE:  Those pesky congressmen.



LEO:  Those members of Congress.



STEVE:  So until then he's just complaining that Apple cannot do something that was deliberately designed for it to be impossible for it to do.  So, okay, yeah, fine.



LEO:  And he knows that.



STEVE:  Yeah.



LEO:  This is grandstanding.  This is all about the court of public opinion.  It has nothing to do with the actual case.



STEVE:  Yeah.  And we saw somewhere that some other congressmen, I think it might have been Lindsey Graham, who while railing against Apple he said, "But I have to say, I am happy that my iPhone is secure."



LEO:  Yeah.  For him.



STEVE:  Like, wait, Lindsey.  Hold on a second.  You're saying you want a backdoor, but not in yours.



LEO:  Right.  Of course.  That's exactly what he wants.  You nailed it.  You're surprised?



STEVE:  Yeah, I think it'll be really interesting.  It'll be - this just really seems like a hot potato because, yes, it would be nice if only terrorists' phones could be unlocked, and everybody else's were safe.  But what citizen is going to say, yeah, I want the government to be able to get into my phone whenever it wants to.  I just don't see us, I mean, we've been talking about cryptographers being in their ivory towers saying, no, there's no way to do this.  I think when you ask anybody would you like the government to be able to have access to your phone, well, certainly some people will say, yeah, I have nothing in here.  Others are going to say that doesn't seem like a good idea.  So we'll see.



LEO:  Yeah.



STEVE:  Until then.  Over the weekend, two days ago, news dropped of a new serious Internet Explorer zero-day remote code execution exploit.  Now, this is, okay, first of all, this is IE.  It's not the default browser that most people are using anymore.  But it is something that has an Internet-facing surface.  And the idea that there is a remote code execution exploit is big news.  Of biggest concern are the fact that one in four desktops running Windows 7 today, since none of them will presumably ever be patched unless their users have switched to Chrome, Firefox, or another browser, IE11 is the last IE available for Windows 7, one in four desktops still operating on the Internet today.  And even if another browser is set up on those systems by default, it's still possible to explicitly invoke IE to take over the machine.



LEO:  And I still think, I may be wrong, but doesn't Microsoft ship IE on every copy of Windows, including Windows 10?



STEVE:  Yes.



LEO:  Yeah, it's still there. 



STEVE:  It is still present.  And it is vulnerable.  We don't know whether Microsoft may issue an out-of-cycle patch.  But it is a zero-day.  It was discovered being used in targeted attacks in the wild.  So it's not a theoretical issue.  I mean, look at how much fur flew from Spectre and Meltdown, and that never resulted in a practical attack.  This was found being used.



Okay.  So here's the technical details.  There are two JavaScript libraries in Windows.  There's the original JScript.dll, and there's JScript9.dll.  JScript9 is the default JavaScript interpreter that IE9, 10, and 11 use.  It is not vulnerable.  So it's the non-default earlier JavaScript DLL which, of course, because Microsoft's good about not killing off legacy stuff, there were some sites that were depending upon some quirks and characteristics of the original JavaScript library.  They are able to explicitly invoke the older Jscript DLL.



So IE9, 10, and 11 are potentially vulnerable.  And it doesn't help us that it isn't the default DLL, the default JavaScript interpreter, because a malicious site that wanted to exploit the older JScript DLL can ask for it explicitly.  It can say this is the JavaScript DLL I want to run.  And so that's what a malicious site will do.  And note that, because we allow ads to run JavaScript, it can be any advertisement that comes up on a well-trusted site can also potentially exploit this.



LEO:  So really it shouldn't be - it's not really an IE exploit, technically.  It's an exploit of the JavaScript DLL that comes with IE and is present on all Windows machines.  Is that right?



STEVE:  Correct.  Although I'm only seeing it referred to as IE9, 10, and 11.  For example, Edge might not be willing to invoke it.



LEO:  Oh, I'm sure not, yeah.



STEVE:  Yeah.  And so...



LEO:  So you have to first have IE open, and you can only invoke it from IE.



STEVE:  Correct.



LEO:  Ah.



STEVE:  Yes.



LEO:  But you can get IE open; right?  You can say...



STEVE:  Exactly.  Exactly.  In fact, I'm sure all of us who have been using Windows sometimes see that IE starts up for some reason.  It's because it is possible for apps or sites to say, oh, no, we need this under Internet Explorer, which is why it's still around, because from time to time it ends up being needed.  So we know that Windows 8.1 and 10 will be updated, though it's unclear when.



We just received our monthly Windows booster shot, and it's unclear how serious Microsoft is taking this one.  We don't know, as I said, whether we're going to see an out-of-cycle emergency patch before next month's February patches.  It is being tracked as moderate.  It's CVE-2020-0674, called "moderate," even though it is a zero-day, in the wild, targeted attack, remote code execution vulnerability being used right now.



So it has not yet been patched.  And my concern is that 7 may never get patched, and 7 doesn't have Edge as its default.  It's stuck with IE11 as the latest browser.  So one in four Windows systems stands to never get fixed, unless Microsoft thinks better of deciding not to fix Windows 7 any longer and thinks, well, this is bad enough, I mean, it's a zero-day.  It's not theoretical.  It's being exploited in the wild.



Microsoft's advisory says:  "The vulnerability could corrupt memory in such a way that an attacker could execute arbitrary code in the context of the current user.  An attacker who successfully exploited the vulnerability could gain the same user rights as the current user."  And remember that this is the coy voice that Microsoft uses.  They found this being done in the wild, not because it could be, but because it was being.



So they continue, saying:  "If the current user is logged on with administrative user rights, an attacker who successfully exploited the vulnerability could take control of an affected system.  An attacker could then install programs; view, change, or delete data; create new accounts with full user rights. Microsoft is aware of limited targeted attacks in the wild and is working on a fix.  But until a patch is released, affected users have been provided with workarounds and mitigation to prevent their vulnerable systems from cyberattacks."  Okay, now, that's sort of an odd thing to say, "affected users have been provided."



Well, okay.  Only if you do a bunch of hocus-pocus mumbo-jumbo.  I mean, it's not like it's being delivered to you.  You have to, like, be listening to this podcast, or be checking in on Internet news things.  But that brings me to my next point, the workarounds.  It's possible to quickly and easily disable all access to this vulnerable JScript.dll.  I immediately did it since I never deliberately use IE anymore, and any IE-dependent sites barely legitimately use JScript.dll any longer.  So for me, and I expect for most of our listeners, the chance of encountering any problem is next to zero.  That is, any problem where having disabled access to JScript.dll would cause a site not to work.  Whereas the probability of it being used against us maliciously is significant.



And notice that it was used in limited targeted attacks until it was discovered.  Now whoever's using it knows it's out there, and it's a race against the clock to leverage it until, for like maybe for a month, until Patch Tuesday of February.  So the fact that it has only been seen in limited targeted attacks, now that we know it exists, the wraps are off.  So suddenly the threat, you could argue, the threat jumps hugely. 



I have here in the show notes a picture I took of my JScript.dll properties last night, showing that the Everyone user, meaning  everyone, has had everything, all rights denied.  No modification, no read and execute, no read, no write.  That is, this JScript.dll cannot be invoked by the system, by IE, by anybody.  Also in the show notes are some commands that you need to use on a 32-bit system.  There is a command to take ownership of the JScript.dll, then a second command to set the everyone user to deny all.  On 64-bit systems, since they both have a 32 and a 64-bit JScript.dll, you need to do that both in the 32-bit context and the 64-bit context so that the commands are a little bit, well, there are four lines that you need to execute.



This is all in the show notes for anybody's who interested.  I would do it.  Our listeners know I'm not running around with tinfoil.  But there's reason to believe that for a month, if Microsoft doesn't do something more quickly, that everyone will be exposed to this malicious DLL which can be evoked through IE.  And IE can be evoked if it's present in your system.  So I would think it's worth removing these rights.  It's a couple commands.  They can be reversed easily.  I've got the reversal commands also here in the show notes.  It's unlikely you will ever encounter a problem after disabling it.  And then Microsoft's presumably going to fix it by next week.



LEO:  Actually...



STEVE:  Go ahead.



LEO:  Bleeping Computer does say there are some potential issues you might experience.  Windows Media Player might have trouble with MP4 files.  The FSC Tool will not replace JScript.dll with altered permissions if you want to fix your system files.  And printing to PDF from Microsoft Print is supposedly reported to break.  So there may be some weird...



STEVE:  That's interesting.



LEO:  Well, what it shows you is that this DLL is being used.



STEVE:  Yes, interesting.  And of course I'm mostly concerned about people who are deciding they want to stay on Windows 7.  It's never going to get fixed, if Microsoft holds to their commitment. 



LEO:  This will be a good test.



STEVE:  Yeah.



LEO:  A zero-day, like, a week after they stopped updating it.  Hmm.



STEVE:  Yes, exactly.



LEO:  Hmm.



STEVE:  Uh-huh, yeah.  And, you know, and not a good thing because they've got a 25%, as we say, one in four desktops are still running Windows 7.  And that number may drop in time.  But you have to think, at this point, they're not going to be moving to Windows 10 even though it's free.



So giving Windows an additional Edge, speaking of browsers.  We were just talking about IE.  Microsoft also last week, as we were expecting, took the wraps off their new Chromium-based Edge browser.  So I guess Paul and Mary Jo can now stop referring to it as Chredge.



LEO:  Never.



STEVE:  I've got the link to - but it's not being pushed out in Windows Update so Windows 10 users did not get it automatically last week.  There is a link in the show notes to get it.  But I also created one of my little GRC shortcuts, grc.sc/edge, grc.sc for shortcut, grc.sc/edge.  That will take you to the page allowing you to download it.  And it is, I have to say, it's pretty looking.  It's available right now for Windows 7 - yes, Windows 7 - 8.1, Windows 10, and iOS and Android.  So you can download it for any of those platforms.



LEO:  And Mac.



STEVE:  Okay, I didn't know that.  And Mac, cool.  And of course, as we know, this Chromium-based Edge browser abandons Microsoft's own home-rolled Edge HTML rendering engine in favor of Google's open source Chromium.



LEO:  That's an important distinction, by the way, because people are saying, well, just get Chrome.  It isn't Chrome.  It's the Chromium renderer engine; right?



STEVE:  Right, right.  It's the guts. 



LEO:  It's the guts.



STEVE:  It's the heart of Chrome.  And a perfect example is that Microsoft has added some additional features to it.



LEO:  Right.  And taken some out, I would bet.



STEVE:  Yes, exactly, yeah.  Well, they de-Google-ized it in order to make it theirs.  I've installed it, and it's quite attractive.  But there are also some nice features, or at least there are some plans to have that.  Under the Settings screen, under Privacy and Services, there's three flavors of tracking prevention:  Basic, Balanced, and Strict.  And so mine is now set to Strict because why not?  There's also supposed to be something a little bit below that on that page.  I've seen screenshots of it, but mine didn't have it.  It's "Block potentially unwanted apps," which sounds like a setting I would like to have.  Maybe it's only available in the canary versions and not yet in the one that you download.  But it sounds like that's going to be coming eventually.



Also there's the ever-popular - oh, thank goodness - "Block media autoplay," which is not enabled by default.  You go to edge://settings/content/mediaautoplay to get you to the page that enables you to turn on the blocking to prevent pages from autoplaying any crap that you didn't ask for when the page loads.  So yay for that.  So anyway, this is Microsoft, as we've said, who just decided that it just didn't make any sense for them any longer to be constantly running in parallel, trying to keep up with the platform which more and more browsers are adopting.  You know, when we were talking recently about all of the browsers that - I don't remember now which feature it was that Chromium had.  It was like, whoa.  And what's the one with "V"?  Vivaldi.



LEO:  Oh, Vivaldi.



STEVE:  Vivaldi and Brave and, I mean, Firefox is still the holdout, doing their own thing, and it's nice because...



LEO:  You don't want a monoculture.  You want...



STEVE:  You really don't.  You do not.  Yes, exactly that.  We really don't want to have a monoculture because that you can always switch to a different browser makes sense.  And where you're not just same guts with a different surface.  And of course I'm still in love with my side tabs that Firefox is alone in offering in an integrated fashion.  There's, like, weird kind of a sidecar thing you can get for Chrome.  But no.  I just want true side tabs.  And so far, well, and I like a lot of things about Firefox, as well.  Although these newer browsers, Chrome and now Edge, they really look nice.



From our, well, that's no surprise department, Bleeping Computer reports that the FBI has sent a flash security alert that nation-state actors have breached the networks of U.S. municipal government and a U.S. financial entity which remains unnamed by exploiting the critical authentication bypass vulnerability in those Pulse Secure VPN servers.  We talked about this at length last week.  The FBI says that unidentified threat actors have used the Pulse Secure VPN authentication bypass flaw to "exploit notable U.S. entities," although without naming them, since August of 2019. 



They said:  "In August of 2019, attackers gained access to a U.S. financial entity's research network by exploiting unpatched VPN servers.  The same month, a U.S. municipal government network was breached in an attack that exploited the same vulnerability.  Based upon the sophistication of the so-called  Tactics, Techniques, and Procedures" - they actually have an acronym for that, the TTPs, the Tactics, Techniques, and Procedures - "used in the two attacks, the FBI believes unidentified nation-state actors are involved in both compromises; however, it remains unclear," they said, "if these are isolated incidents."



So according to the FBI, the attack that targeted and compromised the U.S. municipal government network took place in mid-August of 2019.  This attack, quote:  "The operators enumerated and exfiltrated user accounts, host configuration information, and session identifiers that could allow them to gain further access to the internal network."



In the other known attack, they said, quote:  "The intruders remotely exploited a Pulse Secure VPN appliance."  The flash alert said:  "The vulnerability in Pulse Secure allowed directory transversal and access to a file where login credentials were written in plain text.  In addition, the Pulse Secure appliance may have been vulnerable to a buffer overflow and command injection.  After breaching the network, the nation-state actors gained access to the Active Directory, harvesting and exfiltrating user credentials, the usernames and passwords, for the VPN client."



They said:  "Following attempts to enumerate and gain access to other network segments, the attackers were only able to infiltrate the exploited segment, which was the only one on the network using single-factor authentication."  They said:  "The intruders attempted to access several Outlook webmail accounts, but were unsuccessful due to the accounts being on separate domains requiring different credentials not obtained by the intruders.  While the intruders performed additional enumeration, there was no evidence that any data was compromised or exfiltrated, and the intruders seemingly did not install any persistence capability or foothold in the network."



So in the flash notice, the FBI did not directly connect these attacks to Iranian-based hackers.  A private industry notification detailing Iranian cyber tactics and techniques shared a day later mentions information indicating Iranian cyber actors have attempted to exploit the same vulnerability.  And the FBI assesses this targeting, which has occurred since late 2019, is broadly scoped and has affected numerous sectors in the United States and other countries.  The FBI has observed actors using information acquired from exploiting these vulnerabilities to further access targeted networks and establish other footholds, even after the victim patched the vulnerability.  So that, of course, jives with what we talked about and had seen last week.



So what's clear is that today we use the term "threat landscape."  And it's clearly not hyperbole.  We don't just face a situation where we have opportunistic hackers, as is often said, operating out of their parents' basement.  Cyberwarfare is a real thing.  And there are serious players scrutinizing every announced vulnerability, just waiting for an opportunity to gain an advantage.  So when something like a VPN authentication bypass is announced, as it was in this case in April of 2019, I mean, the guy's doing this Pulse Secure VPN, okay, well, they had a flaw.  They fixed it.  They announced it.  They offered an update.  I'm sure they tried to notify people who were their customers, who were still able to receive these notifications.



What we saw was that, despite that fact, there were still 15,000 instances of this insecure VPN operating out on the Internet.  So here we've got Iranian, apparently Iranian cyber hackers.  They're taking this stuff seriously.  They see this announcement, they go enumerate the Internet, find vulnerable endpoints, use the flaw, reverse engineer the fix, figure out a way of bypassing authentication, and then crawl inside the networks, which are behind these VPNs.  And in this case, as the FBI's flash alert indicates, there were some significant U.S. properties which were using this VPN and, to their shame, had not updated, given six months to do so.  Still had not updated.



So in one case we know that that international foreign currency exchange, Travelex, was - and we talked about it - hit by the Sodinokibi ransomware on December 3rd after not patching seven of their Pulse Secure VPN servers.  Travelex was one of the companies that the Bad Packets guys warned of having vulnerable servers four months earlier, back on September 13th of 2019.  Travelex never replied to the email warning that they received.  And they got hit four months later.



So I don't know how we solve this problem as an industry.  We have Adobe, who has for years been responsibly pushing out updates to their products every month, often on the second Tuesday.  Of course Microsoft.  There are an increasing number of examples of patches being made available and pushed responsibly.  We've said here that SoHo routers need to be doing this themselves.  It's no longer enough to require that users go and click on, log into their router and then check for an update and then update their router's firmware.  These things are unattended, unsupervised, IoT devices, essentially.  They're on the Internet.  They need to go and update themselves.



So I just, you know, as an industry we have to keep moving in this direction because it's clear that, again, it's not opportunistic hackers.  We really are now operating in an environment where any flaw found is followed up on by teams that are being paid, often by their governments.  And I'm not looking for a job, but wouldn't that be fun?  Imagine being able to do that, and it's not illegal.  It would be, you know, you're on the good guy side when you're doing that kind of serious cool hacking of other entities on the Internet.  Wow.  That would be really neat.



Okay.  So something that slipped under the radar, and I'm kind of amazed by this, I didn't see anyone talk about this.  The U.S. Department of Homeland Security, CISA, published the alert AA20-014A titled - this is last week, on Tuesday:  "Critical Vulnerabilities in Microsoft Windows Operating Systems."  And I have the alert, a link to the alert in the show notes.



Essentially what I think happened is that this more, I guess more interesting and also serious CryptoAPI vulnerability that we'll be talking about in a minute, which the NSA discovered in Windows 10 and Windows Servers, which Microsoft, okay, Microsoft rated that one as being merely "important."  And of course it's now been dubbed "CurveBall."  That obtained the lion's share of the attention last week.  We'll be looking at that next.



But this CurveBall obscured an entirely different vulnerability that Microsoft rates as critical for reasons we will see.  And believe it or not, it is another pre-authentication remote code execution vulnerability in Microsoft's Remote Desktop Protocol, in this case in something known as the Remote Desktop Gateway.  There's a link.  I also have a link to Microsoft's own security guidance.



They said, in their disclosure of this last Tuesday:  "A remote code execution vulnerability exists in Windows Remote Desktop Gateway when an unauthenticated attacker connects to the target system using RDP and sends specially crafted requests.  This vulnerability is pre-authentication and requires no user interaction.  An attacker who successfully exploited this vulnerability could execute arbitrary code on the target system.  An attacker could then install programs; view, change, or delete data; or create new accounts with full user rights."  Doesn't that sound familiar.



They said:  "To exploit this vulnerability, an attacker would need to send a specially crafted request to the target system's RD Gateway via RDP."  This affects Windows Servers 2012, 2012 R2, 2016 and 2019.  So all the servers that are being maintained.  It also turns out that this same RD Gateway is in Windows Server 2008 R2.  We don't know whether it also affects it because Microsoft stops saying that anything that they're not supporting is affected, even when it may be.  So we don't know.



Under "Mitigations" they say:  "Microsoft has not identified any mitigating factors for this vulnerability."  Under "Workarounds" they say:  "Microsoft has not identified any workarounds for this vulnerability."  Under their FAQ:  "What network ports are vulnerable to this attack?"  They say:  "The vulnerability only affects UDP transport, which by default runs on UDP port 3391."



So as our listeners will recall, it hasn't been a year, it was back on May 14th of 2019 that we first learned of the now widely known BlueKeep vulnerability.  Microsoft's disclosure back then was CVE-2019-0708, and that was titled "Remote Desktop Services Remote Code Execution Vulnerability."  And I have it here in the show notes, the language from that disclosure.  It's identical to the language that I just read.  Nothing has changed.  So, yes, it sounds familiar.  This exploit is against the server-oriented Remote Desktop Gateway, which is, we can hope, much less widely deployed.  But it turns out there are plenty of them, too.



So we're back here once again with a very serious vulnerability which will surely result in another round of exploitation.  Kryptos Logic, spelled K-R-Y-P-T-O-S Logic, has examined the DLL that was repaired last Tuesday in this update.  In their introduction, they explain the intent of the Remote Desktop Gateway.  They said:  "Remote Desktop Gateway (RDG), previously known as Terminal Services Gateway" - that's as a consequence of the fact that it's been around a long time; and, yes, it affects  Server 2008 - "is a Windows Server component that provides routing for Remote Desktop.  Rather than users connecting directly to an RDP Server, users instead connect and authenticate to the gateway.  Upon successful authentication, the gateway will forward RDP traffic to an address specified by the user, essentially acting as a proxy.  The idea," they say, "is that only the gateway needs to be exposed to the Internet, leaving all RDP Servers safely behind the firewall."  Whoops.  Except the firewall is broken, in fact introduces a new vulnerability.



"Due to the fact that RDP is a much larger attack surface, a setup properly using RDG can significantly reduce an organization's attack surface."  Except when it creates a new one.  Anyway, I have a link to the full disclosure.  They completely reverse engineer the vulnerability, show exactly what it is that's going on, find and tell us about the problem.  It turns out, remember before I mentioned that the vulnerability was only over UDP.  Well, this is yet another problem associated with UDP packet fragmentation and out-of-order reassembly after receipt.  We've talked about UDP packet fragmentation and reassembly problems many times in the past.  It has historically been a huge headache for IP stack implementers.  It's inherently prone to having mistakes made.  And in this case it has bit us again.



The idea is that with UDP, the Datagram Protocol on the Internet, a large packet can be emitted, and you can, as the packet transits the Internet, encounter a link from router to router, an inter-router link, where the protocol doesn't support such a large packet.  So the router that is unable to forward the packet that's that big is able to fragment it, is able to chop it into smaller pieces and send them in pieces.  There is a fragmented flag in the packet, and a which fragment of the larger packet this fragment is, added to the fragmented UDP packet.



The problem, the dilemma essentially, is that once packets are fragmented, they are never reassembled.  They then make their way the rest of the way over the Internet as fragments.  They are also, as is the case for UDP, UDP packets themselves are allowed to come in out of order, and they need to be reordered.  Fragments are allowed to come in out of order because they are themselves just UDP packets that carry this fragmented bit.  So you need some buffer on the receiving end in order to receive packets of a fragmented UDP packet out of order and wait for them all to arrive.  Turns out that that logic of doing that properly is inherently problematical.  It's been a constant source of problems.  And as I said, here was a new one.



The problem and its disclosure is only a week old.  It has been patched, that is, this was fixed, except for Windows Server 2008 R2, last Tuesday.  But we also know that these things take a while to get fixed.  It may be that the RDP servers on the Internet were not attacked before; only the RDP servers not behind the gateway were being attacked.  Well, now the gateways can be attacked.  And we have a complete reverse engineering of the problem is now public.



So we can expect another round of problems.  And maybe we're going to give it a cute name.  Who knows what this one will be called.  Last one was BlueKeep.  Here we have yet another problem.  And I will say again, the only way to do this safely is to put this behind a secure VPN in order to keep RDP and the Remote Desktop Protocol Gateway off of the Internet.



Leo, when you were at CES I shared with Jason and our listeners the happy news that there is now native support for Drupal for SQRL.



LEO:  Oh, nice. 



STEVE:  And I just wanted you to know that.  I just put this back in the show notes because a contributor of ours, Jurgen Haas, said:  "Happy to announce that I was finally able to finish off the SQRL integration into Drupal 8 and the forthcoming version 9.  It is feature complete and supports all of the SQRL Protocol v1."



LEO:  That's great.



STEVE:  And in fact you just go to Drupal, and you're able to add SQRL to your Drupal installation.  So that was very cool.



LEO:  Thank you.



STEVE:  I just wanted to make sure you knew that.



LEO:  Yeah.



STEVE:  I also mentioned last week that I was considering revamping GRC's certificates.  That happened since I last spoke to our listeners on the podcast.  I had a bunch of EV certificates.  Every certificate I had was Extended Validation, all from DigiCert, of course, my absolute favorite Certificate Authority.  But I had the GRC - my main GRC.com certificate was coming due and coming up for renewal.  I've been using more and more certificates because I'm creating more subdomains of GRC, you know, sqrl.grc, blog.grc.  There will be a forums.grc where we'll have web forums for supporting SpinRite 6.1.  And I just like the freedom of being able to create subdomains.



And of course, as we know, EV certs are not allowed to have wildcards in their domain names.  You can't do *.grc.com with an EV cert.  That's not allowed.  So that required individual EV certs or more and more subject alternative names in EV certs in order to stick with that.  And of course browsers no longer celebrate or even disclose without any prompting that there is an EV cert.  So you don't even get any credit for having them.



So I just decided, okay, time for a change.  So it only made sense for me to switch.  The process with DigiCert was as shockingly painless as every interaction with them has been.  Since I was already approved for all the domains that I needed to bundle into the new single certificate, I simply used their Windows Certificate Manager.  They have an app for Windows that makes it really simple to generate a Certificate Signing Request.  I put the root domains and the wildcard domains I needed, basically grc.com, *.grc.com.  I still have grctech.com, that's what brought me to talk about the cookies and cookie forensics last week, and *.grc.com.  And of course grc.sc and a couple other miscellaneous domains.  And I now have one cert that I have installed across all of my servers.



So thanks again to DigiCert making it easy.  Oh, I forgot to say, after I generated that Certificate Signing Request, I went to DigiCert, clicked "yes," and I had a cert.  It was like in minutes, like less than a minute.  It's like, okay.  Actually, I think it came up immediately in the web browser, and I was able to download it.  And then I also received it through email.  So hats off to DigiCert.



And I did want to mention that I pulled the trigger on recertifying all of my certs at GRC.  And I'm really happy with having made that change.  And I'm sad to see what has happened with Extended Validation certificates.  I liked the idea of a person being in the loop.  As I noted before, I understand the value of automated certificate issuance for domain validation, as opposed to organization validation.  This is an OV cert, not a DV cert, meaning that some higher level of authentication was applied to it.  I get it that for opportunistic encryption, using the ACME protocol, an automated issuance cert makes sense.  I wouldn't be surprised if at some point in the future browsers do start indicating whether a human was involved in the loop or not.  We'll see what happens with that.



So, okay.  We all understand the critical importance of the Internet's trust model.



LEO:  Yes.



STEVE:  Which, with the caveats we often examine, for the most part works.  How much fun have we had at the expense of the Hong Kong Post Office through the years.  Yeah, and my startled discovery of how many certificate authorities were in my old XP system a long time ago, it was like, oh, my goodness, and the fact that it's a "trust all" model.  Well, okay, so it got, you know, it's a little creaky.  But for the most part it works.



We trust that we have established a private connection to a website because of the HTTPS certificates presented by the server, which are automatically verified by our browser, and are rejected with glaring warnings if anything at all seems wrong.  Our browsers are very careful to remind us when we encounter a problem such as an invalid certificate.



And I'm sure we've all from time to time encountered a site whose certificate expired a day or two before.  And it's like, it just happened to me, I don't know, a week ago, I was going somewhere, and I got like a warning.  And I checked, I looked at the certificate, and it's like, oh, yeah, that was day before yesterday.  Well, I'm sure they know they need to get a new certificate.  But mostly we're protected.  So we enter our credit card numbers, we interact with our banks, and essentially trust these technologies to keep us safe.



So that's, really, it explains why a fundamental flaw, a deep and serious flaw in the Windows OS Cryptographic API, that is, the thing that is the component responsible for doing that work, such that certificates could be spoofed, caused quite an uproar.  It's being tracked as CVS-2020-0601.  And it has upset the easy trust that we've developed in the model that we have.  The good news is this upset is not the result of some fundamental flaw that's been found in the public key infrastructure, but this particular implementation on some Windows systems is flawed in a way that allows it to accept a spoofed certificate when it should not.



So as we know, last Tuesday Microsoft issued a security update to fix - and I don't understand still why they call this an "important" vulnerability.  It's like, important.



LEO:  Critical.  Critical.  Critical.



STEVE:  Yeah.  I don't get it.  But so this is in their Crypt32.dll.  And what has caused lots of people to discuss on the Internet is the fact that this is the first time the NSA has stepped up and privately reported a vulnerability to accompany that, where they could have used that vulnerability themselves to their own advantage.  I mean, they've done that before.  They've done that in the past.  That's what we learned that the NSA was doing, for example, with BlueKeep, is that they knew of a - was it BlueKeep?  No.  EternalBlue, sorry, EternalBlue.



They have used vulnerabilities themselves, kept them quiet, worked to not have them disclosed, in order to give themselves an advantage.  Maybe it's because they felt this was too critical to go unfixed.  I mean, because this is really bad.  And I'll explain exactly how bad next.  So the fix to this ensures that the Windows Cryptographic API library correctly completely properly validates ECC, Elliptic Curve Crypto certificates.  So not all crypto certificates, but ECC certificates.



Okay.  So this flaw, which now has been referred to or has been named CurveBall, was introduced into Windows, as I mentioned at the top of the show, in July of 2015.  It's a spoofing vulnerability in the way that certificates are accepted without a correct verification of the explicit curve parameters provided within certificates.  Essentially, the flaw allows an attacker to supply their own generated X.509 certificates, which is the class of certificates all of these are, by using an "explicit parameters" option to set its own curve parameters.



What the NSA wrote was:  "Certificates containing explicitly defined elliptic curve parameters which only partially match a standard curve are suspicious, especially if they include the public key for a trusted certificate, and may represent bona fide exploitation attempts."



Okay.  So translating that a little more into English, the way for us to think about this is that Microsoft's original code, which has been in place five years, was allowing certificates to over-specify their own elliptic curve parameters, which were then used to verify their own signatures.  So in other words, the certificate was being allowed to say both "This is how you verify who I am" and "This is who I am."



So the flaw affects all of the places where we depend upon trusting certificates, including HTTPS connections, signed files and emails, and signed executable code.  In essence, anything that is signed.  And you can understand why the NSA said holy crap, you know, this is really too awful.  If this were discovered independently, it would, well, you could imagine.  I mean, it would be a huge exploit.  So they really didn't have any choice.  It completely subverted signing.  All signatures.



So from an adversarial context, man-in-the-middle attacks, you would not have any way of assuring that you were actually connecting to any website that you believed you were because the signature of the certificate that your browser received would be trusted - unless you were using Firefox - when it should not be.  And, oh, the good news is Windows 7 and Server 2008 R2 in this case, even though they received their final updates last week, they were never vulnerable in the first place.  And unlike the famous Spectre and Meltdown vulnerabilities, which remained even two years later, essentially theoretical despite all of the uproar they caused, we never had working exploitable obvious proofs of concepts.  We had, yes, a theoretical cross-process leakage, so in some settings that could be a problem.



In this case, immediately upon the disclosure, researchers jumped on this to create and release proof-of-concept code to demonstrate that the vulnerability can indeed be exploited in the wild.  Researchers immediately predicted that it would only be a matter of days, and that prediction turned out to be spot-on.  The first researcher to prove the vulnerability was exploitable was a guy named Saleem Rashid, who developed, or maybe that's a pseudonym, don't know, he developed a proof-of-concept code that permitted the immediate faking of TLS certificates.  And of course that would allow anybody to create fake websites.  He has not shared his code, but he tweeted some visual proof.  I have a link to his tweet in the show notes.  And we could say, well, okay, but that's a tweet that could be spoofed, that's true.



Immediately on the heels of that came published code from Kudelski Security, and that was followed closely by a Danish researcher Ollypwn, O-L-L-Y-P-W-N.  Both now have proof-of-concept code up on GitHub.  Kudelski calls his the "Chain of Fools" because of course it's all about certificate chains.  That's on GitHub.  He also provides, on a blog posting that is synchronized with this, a complete explainer, in addition to a walk through his proof-of-concept code.  So the cat is completely out of the bag.  This is not theoretical.  This is not, oh, maybe someone's going to figure out how to do it.



Ollypwn has a nice explainer in his GitHub intro about this.  He wrote:  "CVE-2020-0601, commonly referred to as CurveBall, is a vulnerability in which the signature of certificates using elliptic curve crypto is not correctly verified."  He says:  "ECC relies on different parameters.  These parameters are standardized for many curves.  However, Microsoft didn't check all these parameters.  The parameter G, the generator, was not checked; and the attacker can therefore supply his own generator, such that when Microsoft tries to validate the certificate against a trusted Certificate Authority, it'll only look for matching public keys, and then use the generator of the certificate."



He finishes, saying that there's a cert:  "MicrosoftECCProductRootCertificateAuthority.cer is by default a trusted root certificate authority using ECC on Windows 10.  Anything signed with this certificate will therefore automatically be trusted."  And of course this flaw allows a spoofed signature of that certificate to be readily created, and nobody has any questions now about how to do it.  So we have publicly available code and exploitation in the wild.  Well, we have publicly available code demonstrating the vulnerability and proof of concept code.  Exploitation in the wild by malicious actors will surely follow.



Yes, it was patched on Tuesday.  Well, we know in practical terms how much good that does.  Again, Windows 7 systems that could not be patched, or, well, Windows 7 and Server 2008 also received their last patch, but they had nothing wrong with them.  They're not broken in this way.  Only subsequent Windows 10 and servers.  There will be a patch delay.  We know that will happen.  It always does.  There will be servers that will never be patched, which have received code since July of 2015 and then at some point stopped receiving it.  This is going to get exploited.  This is not good.  And that's why this was such a big deal.



I'm sure that our listeners have installed the patches and rebooted their systems.  They're probably fine.  Again, Firefox never trusted any websites using that ECC certificate problem because Firefox brings its own crypto library along with it.  Chrome briefly had the problem, but they were able to update themselves.  Chrome updated to 79.0.3945.130.  Once again, I had to go into Help>About, even though I was using Chrome yesterday and today, and it didn't update until I went to Help>About.  Then it said, oh, and then it updated and restarted, and I got .130.  And now I know that my Chrome won't accept the TLS certs.  I think, however, that's like maybe the minimum, or a minimization of the problem.  This is anything signed.  So we trust code signed by trusted signers.  Windows trusts code.



I did note, although I just saw this in passing, and I forgot to follow up on it, that apparently Windows' own Windows Update will not be spoofed by this.  Yay.  So maybe it uses a different approach.  Maybe it's not using elliptic curve signatures.  It might be using RSA, a large RSA signature, and so it's not vulnerable.  The problem is that there is a vulnerable cert that is in Windows 10, is absolutely trusted, and anything signed with it or spoofed with a spoofed signature of it will also be trusted.  So that's a problem.



So anyway, so I think that the website spoofing problem is less of a concern, although not, I mean, still, for systems not updated, for people using systems not updated, any site can now be spoofed.  That's the case.  So Windows needs to get fixed.  But signatures are pervasive.  And we're going to end up seeing some more exploits of this moving forward.  So make sure your Windows is up to date, and you're okay.



LEO:  Crypto CurveBall.



STEVE:  The Crypto CurveBall.



LEO:  Not easy to hit the curve.



STEVE:  And thank you, NSA, for helping because this was too bad to leave out in the...



LEO:  It wasn't a NOBUS.



STEVE:  For them to know and us to be hurt by.



LEO:  I told you about NOBUSes last week; didn't I?



STEVE:  No.



LEO:  There's a doctrine at the NSA - Michael Hayden, the former chief of the NSA, told folks about this - called NOBUS, N-O-B-U-S.  An exploit that can be used by Nobody But Us is okay.  He uses an example, look, if it takes four acres of supercomputers to crack it, then we don't have to tell the world about it.  That's a NOBUS.  Nobody but the NSA could use it.  This was not a NOBUS.  This was something any idiot, including a script kiddie, could take advantage of.



STEVE:  This was an omnibus.  It was an all of us.



LEO:  So don't pat them on the back too hard.  They still do keep some stuff, hold some stuff back.  But to their credit, if it's something relatively easily exploitable, they're not going to let that go.



STEVE:  No.  And this is really bad.  The ability just, you know, to get anything trusted by any Windows 10 system.  So, whew.  Yeah.



LEO:  Whew.  Whew. It's as bad a bus as you can get.  Steve is the greatest; isn't he?  We are glad you are here every Tuesday, 1:30 Pacific, 4:30 Eastern, 21:30 UTC.  That's when we record Security Now!.  If you want to watch us do it live, TWiT.tv/live.  You can watch it live.  You can also get copies of the show from Steve at his website, GRC.com.  He's got 16Kb audio, 64Kb audio, and transcripts, so you can read along as you listen at GRC.com.



You'll also find lots of great stuff there, including SpinRite, his bread and butter, the world's best hard drive maintenance and recovery utility.  You'll also find lots of freebies, including ShieldsUP!.  People still - just a couple of weeks ago somebody called the radio show and said, "My ICMP port is open."  Or actually he said, "It's not stealth, it's closed."  And I said, "You've been using ShieldsUP!, haven't you."  



STEVE:  103 million and counting. 



LEO:  Wow.  That's awesome.  That's...



STEVE:  Yeah.



LEO:  I use it any time I set up a network.  I test it with ShieldsUP!.  It's a great tool.  Highly recommended.  GRC.com.  He's on Twitter at @SGgrc.  You can follow him there, follow his tweets.  He always posts the show notes before the show there.  That's actually how I get them.  But you can also DM him there.  He's open to DMs.  And if you have a question, a comment, a leak, a tip, information, @SGgrc.



We live at TWiT.tv.  This show is TWiT.tv/sn.  You can download shows there, as well.  We have audio as well as video.  And I guess the best thing to do - we're on YouTube, as well.  The best thing to do would be subscribe in your favorite podcast client so that you get the show the minute it's available and have it just in time for your Wednesday morning commute.



Steve, thank you so much.  And I'll see you next week on Security Now!.



STEVE:  Until then.



Copyright (c) 2020 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#751

DATE:		January 28, 2020

TITLE:		SHAmbles

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-751.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we look at some surprising revelations of Apple's cloud storage encryption (or lack thereof).  We also cover a Microsoft cloud database mistake, some interesting legislation under consideration in New York, new attacks against a consumer router firmware, a rise of new attacks against our browsers, a welcome new publication from NIST on Privacy, a massive leakage of telnet usernames and passwords, a welcome micropatch for this month's IE zero-day, a bit of miscellany and SpinRite news, and then some coverage of the final nail that was recently pounded into SHA-1's coffin.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  We knew it was coming.  It's been a long time coming.  But finally it looks like SHA-1 is really deprecated.  Steve explains why and what the future holds.  We'll also talk about Apple's very confusing encryption policy.  Is your data on iCloud safe or not, and what can you do about it?  And yet another router problem from Tomato.  Are you using Tomato?  We'll talk about the Tomato router, open telnet ports, Microsoft data leaks, and a whole lot more.  It's all coming up next at Security Now!.



LEO LAPORTE:  This is Security Now! Episode 751, recorded Tuesday, January 28th, 2020:  SHAmbles.



It's time for Security Now!.  Oh, yes, he's here, ladies and gentlemen.  I feel like we should have, I don't know, a big red curtain, and you could come out from behind the curtain, and the crowd would go crazy, on their feet.  Johnny Carson style, you know?  Mr. Steve Gibson, the head of research at GRC.com.



STEVE GIBSON:  Well, we did that at the beginning of the SQRL...



LEO:  Yeah, that was fun.



STEVE:  That was very nice.  It was staged, but it was effective.



LEO:  Yeah.  Shhh, don't tell anybody.



STEVE:  So today we're finally going to get to talk about the subject that I've mentioned pushing off for the last several weeks because something more urgent kept coming up.  And this, I can't take credit for this clever word.  This is the word from the paper that was written, "SHAmbles," shambles, which is to say that our, well, I was going to say our much-beloved SHA-1 hash, but no one loves it anymore.



LEO:  Oh, nobody loves it.  Nobody.



STEVE:  Nobody loves it.



LEO:  So let me - well, we'll talk about it.  When we get to it, I have lots of questions to ask you.



STEVE:  Cool.  But we do have other news, of course.  We're going to take a look at some surprising revelations about Apple's cloud storage encryption, or lack thereof.  We also cover a Microsoft cloud database mistake, some interesting legislation under consideration in the state of New York, new attacks against a consumer router firmware, a rise of new attacks against our browsers, a welcome new publication from the U.S. National Institute of Standards and Technology (NIST)  about privacy, a massive leakage of telnet usernames and passwords and their IPs, a welcome micropatch for this previous Patch Tuesday/IE zero-day, a bit of miscellany, a little bit of SpinRite news - and of course SpinRite news, we're going to be having a lot more of that here coming up.  And then some coverage, as I mentioned, of the final nail that was recently pounded into SHA-1's coffin.  So I think another great podcast for our listeners.



LEO:  Did you get the email from Intel this week?



STEVE:  No.



LEO:  I got - actually it was from Digital Ocean.  Yet another kind of - I was thinking it's speculative execution because it affects people on shared processors.



STEVE:  No.



LEO:  But this one from Digital Ocean saying, yeah, we're going to have to fix this one, too.  It just never stops.



STEVE:  Well, and so far we've not actually seen...



LEO:  There's been no exploits.



STEVE:  ...any exploit, exactly.  So, but on the other hand, we know that, if it isn't fixed, it will be exploited.  And the bad guys will go to any extremes to make that happen.  So, yeah, as long as it, I mean, it doesn't really affect our listeners very much, and I like to mostly focus, well, accept where theoretical weirdness happens, which is fun to talk about, like what happened with speculation two years ago.



LEO:  This came yesterday.  "Hi there.  Today Intel released a statement regarding two processor data leakage security vulnerabilities, vector register sampling and L1D eviction sampling, that may allow unintended information disclosure for users of multitenant cloud environments like Digital Ocean.  Theoretically they could use a droplet to infer partial data used by previously run software or another droplet" - that's what DO calls the "instances" on this same post.



STEVE:  Boy.



LEO:  So I don't know, are you familiar with these?  Or are these new ones?



STEVE:  No, but I know what they just said.



LEO:  You're way ahead of me.



STEVE:  What they just said was that the nature of caching is now under attack.  And of course.



LEO:  Of course.



STEVE:  What is a cache except a storage of recently used and probably likely to be soon reused information.  And of course the reason we have caching is that the processors, the performance of the processors have so far exceeded the performance of main memory that you just can't afford not to cache it locally.  And so that's what the L1 eviction probing or sampling is, is that they're saying there's a way for you to tell what's in your L1 cache based on the length of time it takes.  And so, yeah, of course that's going to happen.  So, I mean, I guess the one...



LEO:  Kind of an amazing hack, isn't it, though?



STEVE:  Yeah.



LEO:  Because it's not that they get the data.  They get it from the timing.



STEVE:  Yup.



LEO:  What?



STEVE:  And so it's an inference hack.  There is on the horizon, we know, that there is that next-generation non - well, no, it is volatile.  Is it volatile?  I don't remember.  I don't think it's volatile actually, that high-speed crosswire memory technology.  There is like the promise of finally solving the DRAM problem.  This is all a fault of the fact that we need so much density in RAM now that a bit is now a tiny little capacitor.  And you've got to refresh it.  And the only way of sensing it is by dumping that capacitor into a sense amplifier which makes it a destructive read, which means you then have to write it back because, I mean, you can't just lose your memory the moment you query the memory.  There are old people for whom that happens, but that's not our computers.



LEO:  Me.



STEVE:  And so we're in this mess because nobody has figured out how to make that faster.  When we make the capacitors too small, then we become susceptible to Rowhammer attacks because the noise immunity drops such that adjacent rows of reads affect the bits that we're trying to not have affected.  So there's that problem.  So, I mean, it's a mess right now.



And I think probably in the future we're going to end up with some next-generation technology that just doesn't have this problem.  And when we can move memory onto the chip, which will probably happen, when we can move main memory onto the chip, then this whole need for L1, L2, and L3 caching potentially goes away so that we're no longer leaving a footprint of what we've done because at this point we have no choice.  We have to cache.



And so the only solution is to cache per core, and then make sure that you don't switch a core outside of a virtual machine because that would then make it susceptible to probing from the other virtual machine.  So, yeah, we're in a mess.



LEO:  So this is not a speculative execution.  This is a cache problem.



STEVE:  Right, right.



LEO:  Well, there you have it.



STEVE:  It's a microarchitectural flaw.



LEO:  It shows how hard this stuff is.



STEVE:  It's inherent in caching, yes.



LEO:  And really it's not - I want to say it again, it's not that Intel made any obvious blunders.



STEVE:  This has been there from the beginning.



LEO:  It's that the hackers have become incredibly sophisticated in the techniques.  And it's not even hackers.  It's really researchers at universities.



STEVE:  Yes.



LEO:  Who've become incredibly sophisticated.  These timing attacks like Rowhammer are crazy sophisticated.



STEVE:  Well, and it also sort of demonstrates what happens when you point academics at a whole new area.  Two years ago somebody said, ooh, look what happens with branch prediction.  And it's like, oh, well, if that's true, then this and this and this and this and this and this and this, I mean, like pretty much everything just collapsed.



So our Picture of the Week is fun.  It's one that someone tweeted me, and I said, oh, my goodness, that's just a kick.  And so I had a slot for it.  We show sort of a large office environment with a bunch of desks and two in the foreground, the rest in the background.  And one gal on the left is saying apparently to the guy on the right, she says:  "I've asked the hackers to turn the thermostat down after they're done accessing our system."  So, yes.



LEO:  Love it.



STEVE:  The world we live in today.  Speaking of the world we live in today, the question has arisen, and apparently it's been answered, actually, whether Apple is actually encrypting our iCloud storage backups.  I wanted to start out this week by discussing this since we were talking about this recently.  I was first made aware of it by a guy named Jeff Root, who is a frequent participant in our newsgroups, and I told him I'd give him a little shout-out, so there you go, Jeff.  He pointed me at a story in Reuters which has since been picked up by a bunch of the tech press because it makes a little - after a lot of interviewing of former Apple employees and some FBI people who are in the know, they end up citing six sources familiar with the fact that Apple explicitly dropped its plan for encrypting backups after the FBI complained.



So these six sources told Reuters that Apple had dropped its plans to give iPhone users the option to fully encrypt backups of their devices that are backed up to iCloud, of course, after the FBI complained that the Apple's plan would harm their investigations.  This happened about two years ago and had not been previously explicitly reported.  Reuters stated that it showed how much Apple has been willing to help U.S. law enforcement.  Of course we were talking about this back and forth over the last couple weeks as a result of this new Pensacola shooting.



But in addition to this, Reuters says that behind the scenes Apple has been providing the U.S. Federal Bureau of Investigation with rather comprehensive help, that is, as much as they've been able to, not specifically related to a couple of the more high-profile cases that we've talked about, but just sort of in general, citing one current and three former FBI officials, and one current and one former Apple employee.



Reuters reported that more than two years ago Apple told the FBI that it planned to offer users end-to-end encryption - and we take that now to mean nobody can see into it, that even they can't - when storing their phone data on iCloud.  Under that plan, primarily designed to thwart hackers, Apple themselves would no longer have a key to unlock the encrypted data, meaning it would not be able to turn over material to authorities in a readable form, even under court order.  Subsequently, during private talks with Apple, representatives of the FBI cybercrime agents and its operational technology division objected to the plan, arguing it would deny them the most effective means for gaining evidence against suspects using iPhones.



And when Apple later spoke privately to the FBI about its work on phone security about a year later, the end-to-end encryption plan had been dropped, according to those six well-placed sources.  Reuters said that it was unable to determine exactly why Apple had dropped their sweeping encryption plan, but that it had been.  Another former Apple employee said he was told, without any specific mention of why the plan was dropped, or if the FBI was a factor in the decision:  "Legal killed it, for reasons you can imagine."



LEO:  That is important.  "Imagine" is perhaps what that employee was also doing because there's a little dispute over this.  Rene Ritchie, we talked about it a couple of weeks ago.



STEVE:  Oh, okay, good.  I'm glad. 



LEO:  Rene Ritchie said he remembers that time, and Apple made a conscious decision, and this is actually applicable for this show, to instead of fail secure, to fail safe; that they were worried, if they did do Trust No One encryption, and somebody lost their password, yes, it would fail secure, but they wouldn't be able to help that person.  So they opted - and that may be what Legal said is, no, we don't want to be on the hook for losing people's data.  We'll make it fail safe.  The password can be reset.  Which means of course it's not Trust No One.



STEVE:  Well, and that is what Tim Cook has officially said was their position, that is, they decided, exactly as you said, Leo, that it would be a problem for their users if there was no recourse for the recovery of storage from phone data backed up to the iCloud.



LEO:  And that's actually an important point because, if any company can say, oh, you forgot your password, we'll reset it, then it's not Trust No One; right?



STEVE:  That's absolutely right.  And in fact what's a little bit slimy here is what Apple says.  For example, because after reading all this I went back to their privacy stuff for iCloud just to figure out what it was they were saying that was different than this because, I mean, to give our listeners a sense for this, during the first half of 2019, so the first half of last year, which is the period covered by Apple's most recent semi-annual transparency report on requests for data it received from government agencies, U.S. authorities armed with court orders asked for and obtained full device backups and other iCloud content for 1,568 cases, covering about 6,000 accounts.  And it turns out that U.S. Secret Service, or I guess secret U.S. intelligence court directives that it received over the same period of time, it responded to more than 18,000 accounts during the first half of last year.  So Apple is busy disclosing iCloud backup data.



So anyway, I went over and looked.  So they have a section for iCloud, end-to-end encrypted data.  And what they've said is that, or what Tim has said is that the most sensitive data is end-to-end encrypted.  And I thought, okay, what does that mean?  So they said:  "End-to-end encryption provides the highest level of data security.  Your data is protected with a key derived from information unique to your device, combined with your device passcode, which only you know.  No one else can access or read this data."  So that all sounds, like, really good.



And then they said:  "These features and their data are transmitted and stored in iCloud using end-to-end encryption:  home data, health data, iCloud Keychain, payment information, QuickType Keyboard learned vocabulary, Screen Time, Siri information, Wi-Fi passwords."  And then they said:  "To access your data on a new device, you might have to enter the passcode for an existing or former device."  So they enumerate carefully the things they do end-to-end encrypt, that is, which they have no visibility into, which I guess is meant to say, but everything else that we back up, we're not end-to-end encrypting.



LEO:  Right.



STEVE:  And then they said:  "Messages in iCloud uses end-to-end encryption."  They said:  "If you have iCloud Backup turned on, your backup includes a copy of the key protecting your Messages."  And this is what's a little slippery.  "This ensures you can recover Messages if you lose access to iCloud Keychain and your trusted devices."  They said:  "When you turn off iCloud Backup, a new key is generated on your device to protect future Messages and isn't stored by Apple."



So this is really slippery because they're saying Messages in iCloud also uses end-to-end encryption; but they're also saying, "and the key to decrypt them is stored there, too."  And then I looked under their privacy, Apple.com/privacy, and they said:  "Messages are only seen by who you send them to.  Apple can't read your iMessages while they're being sent..."



LEO:  In transit.



STEVE:  Uh-huh, "...between you and the person you're texting."



LEO:  Right.



STEVE:  And it's like, okay.  So, yeah, you know, our listeners know how to hear that.  And what that says is, right, we send them to the iCloud before they're encrypted and after they're decrypted.  Yes, we encrypt them in the iCloud, but we store their key alongside the encrypted Messages.  Oh, because if you lost access to that, you wouldn't want to lose your valuable Messages.  It's like, okay.



LEO:  So we talked a long time with Rene, and he's promised to make a video on this because this is extremely confusing.  There are Trust No One ways to store Messages, as well as passwords and certain other data.  The default is of course not Trust No One, it's fail safe.



STEVE:  Yup.



LEO:  But there are ways for - and this confused the hell out of me.



STEVE:  Well, I would call it "fail open."



LEO:  Fail open.



STEVE:  Fail safe is being too nice.  It doesn't fail closed, it fails open.



LEO:  So there is, on Messages, a setting to do cloud Messages.  And if you turn that on, Rene tells me, you've just turned off encryption storage.  However, if you just have it be part of iCloud backup, it is encrypted.  Now, maybe they're storing the key - remember they said the key has to be then parsed with your code, your unlock code, to decrypt. So maybe they're storing that partial key.  His sense, and I think it's extremely confusing, so I'm not going to say this is absolutely the case.



I'll tell you what is absolutely the case, according to Apple.  If you're using cloud backup for anything, you should assume, because it's not clear, even though Apple says what's on your iPhone stays in your iPhone, you should assume that Apple has access to that.  And that means, by the way, not only law enforcement has access to it, but a sufficiently sophisticated hacker taking advantage of a flaw in iCloud or social engineering might also get access to it.



If you back up your Apple device, your iDevice, iPhone or iPad, directly to a computer, you check the box that says "encrypted backup" in iTunes, or I guess it's now - I don't know what it's called now, but in that app, and you give it a password, that is Trust No One at this point.  Only you have access to that.  That is fully encrypted.  Apple cannot get it.  Soon as you put it on the cloud, some is, some isn't, it's not immediately clear what is, what isn't.  So from now on, for my backups, if I want real privacy, back it up to your computer locally, turn on encryption, use a password, and don't lose it because Apple can't fix it if you lose it.



STEVE:  Well, and I'd have to say, I mean, so many people say to me, because they think I know, oh, my iCloud backup is full.  What do I do?  And so my point is that everybody wants to have their precious little...



LEO:  It's convenient.



STEVE:  ...phone backed up to the iCloud.  Yeah, it's like, oh, look, it's magic.



LEO:  It's more than convenient.  Apple pushes it.  They push it hard because they make money on it because it's full, you have to buy more.  And they push it.  They really strongly encourage it.  And they, I think, should be blamed a little bit for giving the misimpression that it is fully secure.  What's on your iPhone stays on your iPhone.  That was the big ad last year in CES.



STEVE:  Yes.



LEO:  By the way, the hubris of that was immediately handled bad karma because I think there was a crack right after that ad was up.  So it wasn't strictly true.  It isn't true.  But there are ways to do it. I hope - I don't know if Rene has made that video yet.  He's got to make that video because it isn't clear to anybody what is safe and what isn't safe.



STEVE:  And props to Rene if he can figure this out.



LEO:  Yeah.  I can't.



STEVE:  This is a mess.



LEO:  Yeah.



STEVE:  And, you know, confusion, unfortunately, causes most people to go, well, oh, it's probably okay.



LEO:  Well, the thing that pulled me up short is he said, yeah, if you turn on messages in the cloud, you are now turning off Trust No One Encryption.  And I went, what?  What?



STEVE:  Yeah.  Well, and Android Central picked up on this.  Their title was "Apple may have ditched encrypted backups; Google hasn't."  And Android Central said:  "A bombshell report from Reuters suggests Apple ditched end-to-end encryption for iCloud backups at the behest of the FBI.  Citing several former Apple employees and FBI officials, the publication [Reuters] notes that Apple planned to switch to end-to-end encryption for iCloud  putting it on the same level as iPhones and iPads  but reversed" - meaning native local encryption - "but reversed course after consulting with the FBI."  So, yeah.  That would be good if, I mean, I think Rene would have himself a very popular piece of video if he were able to unscramble this.



LEO:  Yeah, because no one can translate this.



STEVE:  No, it's just nuts.  Microsoft, to change companies, had a little slipup.  On December 5th of last year, so late last year, Microsoft misconfigured the access controls of a database of more than a quarter billion, so 250 million, technical support service records dating back 14 years to 2005.  In some of the press coverage of this, someone said, "If you've had any contact with Microsoft Support in the last 14 years, your data was published."



So although they claimed that automated tools had removed unneeded personally identifiable information, Bob Diachenko, who we've spoken of before - he's the guy who's been making it his personal crusade to go find exposed databases on the Internet, he's a database sleuth - he spotted the unprotected database, reported it to Microsoft, and said there remained ample readable information, including email addresses, IP addresses, locations, descriptions of claims and cases, Microsoft Support agent email, case numbers, resolutions, and remarks.  Oh, and internal notes marked "confidential."



So anyway, I'm not sure when he found it, but he knows that it appeared on the 5th of December.  On New Year's Eve, I thought it was interesting, they were kept late.  They were there on December 31st, turning this off.  So it disappeared from public exposure as a consequence of this misconfigured access control on New Year's Eve.  But it was up there for three weeks or so.  Not a big deal.  But I just didn't want to skip over it completely without making note of the fact that that had happened, in case anyone cared.



Okay.  So here's an interesting piece of news.  New York state is exploring the possibility of introducing legislation to ban the use of taxpayer funds for paying ransoms, ransomware ransoms.



LEO:  Oh, interesting.



STEVE:  Yeah.  The state of New York is exploring whether outlawing the payment of ransomware ransoms might make the problem go away because the bad guys will know nobody in New York by law is able to pay.  I'm not hopeful that that strategy's going to work.  Two legislative bills have been proposed that would require government agencies to tell ransomware attackers they cannot pay.



The first bill, S7246, was proposed by Senator Phil Boyle on the 14th of January, obviously, what, exactly two weeks ago today.  If it were enacted into law, it would for small cities and towns with populations under a million people restrict the paying of attackers with taxpayer money.  If it were passed, it would set up a $5 million fund to help overhaul the IT infrastructures of such small towns.  But 5 million, that would not go very far, unfortunately, under the terms of how much IT stuff costs these days.  So, okay.



The second bill, S7289, introduced by Senator David Carlucci two days later, on the 16th of January, would prohibit government agencies from paying ransom in the event of a cyberattack against their critical infrastructures.  At this point, the bills are both under discussion in committee, and it's unclear which, if either, of the bills would make it to a vote in the state senate.  And of course we talked about last summer how in June of 2019 that U.S. Conference of Mayors passed a nonbinding resolution to sort of formally say we don't want municipal systems to be put back online as a consequence of paying ransom.  We want you to say no.  But of course their resolution was not binding.  It was just like, okay...



LEO:  They sort of said please, we beg of you.



STEVE:  Yeah, exactly.  Maybe it gave mayors some cover to say, well, we don't want to pay, and the U.S. Conference of Mayors is behind us on this.  But of course it lacked any legal teeth.  So this New York legislation would be the first state to move in that direction, to like formally say it's against the law for a state agency, a state municipality to pay a ransom.  So anyway, we'll see what happens.



That second bill, S7289, referred to the attack in Albany, New York last month - actually it was on Christmas Day - which paralyzed the Albany International Airport.  At that time the attackers demanded a ransom in exchange for the return of the data and the restoration of the airport's systems.  Because they were desperate, they paid a ransom.  They're claiming, although the amount was not disclosed, that it was less than six figures.



So anyway, in response, the legislation says, in the bill, we don't want to keep rewarding these crooks for these attacks.  They said:  "When municipal corporations and government agencies comply with these ransoms, they incentivize cyberattackers looking to make a quick buck.  Prohibiting these entities from complying with ransom requests will remove this incentive and safeguard taxpayer dollars."  Uh-huh.



LEO:  You know where they would be even more effective is if they also helped these agencies not get bit in the first place.



STEVE:  Yeah.



LEO:  Establish some standards.  Get some IT money in there.



STEVE:  Yeah.



LEO:  It's one thing if they don't pay.  Help them not have to pay; right?



STEVE:  Yeah.  And Leo, in every case the municipality that gets attacked says, well...



LEO:  We don't have a choice.



STEVE:  Yes.  Give us the money.  We don't have a budget that allows us to do - because, you know, in every case they have an IT infrastructure which is old and creaky.  It's like it's barely working.  Everybody's overworked and underpaid.  And it's like, we'd love to be ransomware proof.  But to do that, we've asked our IT people, what would it cost?  And they quote something which we can't pay.  So, yikes.  It'll be interesting to see.  I mean, I don't think this is going to work.  But we'll find out.



There is a bad new botnet, the Muhstik (M-U-H-S-T-I-K) botnet, which is attacking Tomato routers.  And we've not talked about Tomato, but it is a popular alternative router firmware.  Palo Alto Networks Unit 42 researchers observed a variant of the wormlike botnet, this Muhstik botnet, that has added scanner technology to brute force web authentication against Tomato routers - and when I read this, I'm thinking, what? - on port 8080, which bypasses the admin web authentication using the default credentials.  The defaults in the case of Tomato routers are admin and admin, or root and admin.



LEO:  I would take root and admin.  Go for that one.  



STEVE:  Yes.  The researchers wrote that they, quote, "captured Tomato router web authentication brute forcing traffic."  Okay, now, I would take exception to the characterization of using admin:admin and root:admin as "brute force."  I mean, okay, maybe wimp force.



LEO:  I pushed it, and it fell over.



STEVE:  Exactly.  Well, we gave it our first guess, and oh, look.  We're in.  We brute forced it.  No.



Okay.  So Tomato firmware, it's Linux-based, non-proprietary firmware which is known apparently not for its security, but for its stability.  So VPN passthrough capability and advanced quality of service control.  Yes, it's so advanced, even the hackers can specify what quality of service they want.  It's typically used - apparently, multiple router vendors actually ship Tomato-based routers.  I didn't know that, that there are some commercial vendors that said, oh, we're not going to bother creating our own firmware.  Let's just use Tomato.  It looks pretty.  And in fact there is, there's Advanced Tomato.



So Tomato says of themselves:  "Tomato is a small, lean, open source alternative firmware for Broadcom-based routers.  It features a new user-friendly GUI, a new bandwidth usage monitor, more advanced quality of service and access restrictions, new wireless features such as WDS and wireless client modes, a higher peer-to-peer maximum connections limit, the ability to run custom scripts, connect via" - oh, this is wonderful - "telnet and ssh" - yes, telnet - "reprogram the SES/AOSS button, perform wireless site survey, and more."  And then Advanced Tomato.  I won't go into all the details, but basically it's really pretty.  They said, you know, basically tired of those creaky old router UIs?  Well, you want Advanced Tomato.



Anyway, so the point is that this thing has telnet, or at least web admin, rather, not telnet, web admin listening on port 8080.  And as our listeners know, I'm a fan of the FreeBSD-based pfSense firewall router, so I don't have any firsthand knowledge of Tomato router configuration.  But you and I, Leo, we've all  heard of Tomato routers, and I'm sure our listeners have.  It is inconceivable to me that any modern router could or would have its admin access enabled on its WAN interface.



LEO:  Yeah.



STEVE:  I mean, it just - it can't.



LEO:  Not by default, anyway.



STEVE:  No.  Yes, exactly.  Can you say RDP?  But even assuming that the WAN interface binding must be manually enabled, in this day and age it is malpractice for firmware to allow any default login for the WAN-facing side.  I'm encountering more and more software, and I'm always happy when I see this, which refuses to do things like that.  If you are turning on WAN-side, it won't let you use a default credential.  It won't have it.  It'll say - and make you use a strong password and do a little bit of password strength metering and make you protect yourself.  I mean, what year is this?  It's crazy that you could simply click "Turn on WAN admin," and the default credentials which presumably haven't been changed facing the LAN, are now also facing the WAN.  That's the only way you get admin:admin as your user:password, or root:admin.



So I just - I'm stunned that it's even possible.  But of course that's the only way that the strategy of adding it to so-called brute forcing, although it's first guess in, of some wormlike botnet makes any sense is that, you know, it's like, oh, look, admin:admin.  What do you know?  Because, I mean, no user would choose that for themselves.  They would do something else.  Sarah or Wilbur or something.  But not admin:admin.  So it has to be that that's the default, and that people are turning it on, and then not changing the default username and password.



So, you know, if there's anyone in charge of router firmware within the sound of this podcast, please, I mean, how hard is it to refuse to not require the user to manually enter, make up some username and password when you, I mean, when you enable WAN-side admin.  Which, by the way, is really bad.  I mean, it's just bad.



LEO:  Well, and who uses telnet anymore?  I mean, SSH; right?



STEVE:  Yeah, well, actually we'll be talking about a little story about that here in a minute.  But, yeah, exactly.  SSH is what everyone should be using.



LEO:  Telnet's in the clear.  So even if you didn't have access, you could still see the traffic.  So it just seems like a bad idea all around.



STEVE:  Yup.



LEO:  Yeah.  Now, I don't know, last time I checked Tomato hadn't been updated in years.  I don't know if there's...



STEVE:  Yeah, there actually has been an update.  I did take a look, and it was current.  There have been some recent changes.



LEO:  So we usually recommend DD-WRT.  But they're related, I think.  I feel like they're...



STEVE:  Yeah, there's DD-WRT, OpenWrt.  There's something called Gargoyle because I was wondering about third-party firmware.  Tomato.  There's the LEDE project and then libreCMC are apparently the top six third-party firmwares.  But I agree, DD-WRT and OpenWrt are the main ones that we talk about.



LEO:  Apparently there are a lot of forks.  And ASUS uses Tomato as the base of its entire line of home routers.  I thought they were using DD-WRT.



STEVE:  That's interesting.  It certainly doesn't look, I mean, it looks ASUS-ized.



LEO:  Fresh Tomato is the only fork under "active."



STEVE:  Fresh Tomato.  



LEO:  It just begs to, you know...



STEVE:  You don't want to use the Rotten Tomato firmware.



LEO:  No, use the Fresh, yeah.  I wonder.  That seems so weird that that would be turned on, WAN administration would be turned on, and then telnet port would be open.



STEVE:  Well, no, no.  I  misspoke there.  We do have a story coming up.



LEO:  Oh, this is not telnet.  This is just the administrator WAN.



STEVE:  Yeah.  This is port 8080 in order for it to open a port in the high port numbers.



LEO:  All right.  Yeah, there are a lot of Tomato forks.  There's EasyTomato.  There's Toastman.  There's Shibby.  Tomato ND, Tomato VPN, Tomato USB.  There's a lot of Tomatoes.  Lot of them out there.  Wow.



STEVE:  Well, and Broadcom is a popular chipset.  So it must be any hardware that is Broadcom based.  It's like, oh, yeah, we've got your Tomato right here.



LEO:  And I would hope any manufacturer shipping Tomato or a version of Tomato as their default firmware would fix that little flaw before they ship it. 



STEVE:  I should hope.



LEO:  You'd think so.



STEVE:  You'd hope.  So our web browsers are under attack.  Get a load of this one, Leo.  The Chrome Web Store has been experiencing a wave of fraudulent transactions and has temporarily suspended publishing and updating of paid Chrome extensions due to a spike in fraudulent transactions.  The Google security team has indefinitely suspended the publishing or updating of any commercial Chrome extensions on the official Chrome Web Store following a spike in the number of paid extensions engaging in fraudulent transactions.  So let me repeat that.  "The Google security team has indefinitely suspended the publishing or updating of any commercial Chrome extensions."  So I guess only non-commercial extensions can be modified, just due to fraud.



Google said that the wave of fraudulent transactions began earlier this month. Google engineers described the fraudulent transactions as happening "at scale," meaning they couldn't deal with it.  And this ban on publishing or updating impacts all paid extensions including Chrome extensions that require paying a fee before installing, extensions that work based on monthly subscriptions, or Chrome extensions that use one-time in-app purchases to get access to various features.  Any existing commercial extensions that are already in place are still available for download via the official Chrome Web Store, but their developers are now forbidden from pushing any updates for fear that they might be fraudulent.



Simeon Vincent, who's the developer advocate for Chrome Extensions at Google, said:  "This is a temporary measure meant to stem this influx as we look for long-term solutions to address the broader pattern of abuse."  So it's apparently just  gone out of control, and they just had to terminate any changes to commercial extensions.



Extension developers who try to publish a new paid Chrome extension or push a new update of their existing commercial extensions are currently receiving an automated message that all it says is "Spam and Placement in the Store."  I saw a tweet, someone tweeted the message they got, which is terse and doesn't really explain what's going on.  And since it's a blanket ban, even big name extensions have been impacted by it, the password manager Dashlane and meeting planner app Comeet.



The decision to ban publishing or updating Chrome extensions was formally announced just late last Friday night, on January 24th.  Jeff Johnson, the creator of the StopTheMadness Chrome extension, told ZDNet that Google has been silently blocking updates for paid Chrome extensions actually before the official announcement, for a number of days beforehand.  It's unclear how long the ban will last.  Vincent said:  "We're working to resolve this as quickly as possible, but we do not have a resolution timeline at the moment."  And he said:  "Apologies for the inconvenience."



So, yikes.  Wow.  And, you know, it just sort of, when you think about it, browsers are now the way we interact with more and more of the world, the Internet, and browser-based apps are a thing.  And it would make sense that extensions would come under attack.



LEO:  But it's also part of this world where, if it can be gamed, it will be gamed.



STEVE:  Yes, yes.



LEO:  It's just frustrating.  People are so evil.



STEVE:  I know.  It really is.  I mean, it is despoiling the Internet.



LEO:  This is why we can't have nice things.



STEVE:  Yeah.  Also, and it's not just Chrome.  Over the past two weeks Mozilla has banned nearly 200 Firefox add-ons in a crackdown on Firefox browser add-on misbehavior.  Now, in this case, this wasn't a recent explosion.  This is Mozilla basically making good on their promise to crack down on misbehaving add-ons and in ways that our listeners will find familiar because we've talked about this already.



So in total, Mozilla's add-on review team banned 197 Firefox add-ons which were caught with various forms of misbehavior - executing malicious code, stealing user data, using obfuscation to hide their code.  The add-ons have been banned and removed from the Mozilla add-on portal to prevent new installs, and they've been retroactively disabled in the browsers of users who already had them installed.  So 129 out of the 197 add-ons were all developed - that's a huge number, 129 - by a single developer named 2Ring, which calls itself a provider of business-to-business software.



And get this.  In those 129 cases, the ban was triggered because 2Ring's add-ons were all found to be downloading and executing code from a remote server.  And as we've discussed in the past, for obvious security reasons, Mozilla's updated add-on rule - and updated some time ago, so everyone had plenty of time to fix this - forbid add-ons from obtaining external code.  And as we know, even if such code was benign now, there's no telling or controlling what any such code might do in the future.  And if add-ons existed that did this, they would represent a very tempting target for supply chain attacks, which we've seen before, where somebody compromises the 2Ring server in order to insert malicious code that then these 129 different add-ons, whatever the hell they are, download and run.



So Mozilla has started strictly enforcing this rule across its entire add-on ecosystem.  A different developer had six add-ons doing the same thing, and another one had three add-ons banned which were offering fake premium products.  There were also bans levied against add-ons found to be improperly collecting user data:  WeatherPool, Your Social, Pdfviewer, RoliTrade, and Rolimons Plus.  There were also 30 add-ons banned for various types of malicious behavior.  In those cases, only the add-on IDs were published so the developers could appeal the ban after curing the misbehavior.  So there was also questionable behavior spotted from something called FromDocToPDF add-on, and they found that it was loading remote content into Firefox's New Page tab.



So anyway, what we're seeing is a growing instance of browser add-on exploitation.  And exactly as you said, Leo, it's sort of like, well, of course it's going to happen because there's a possibility for it.  The idea is, of course, we have browsers.  We want to create a richer browser behavior.  We're all fans of add-ons - LastPass, famously.  UBlock Origin, of course.  Once upon a time NoScript.  It makes sense to allow users to customize their browser the way they want to. 



LEO:  Well, especially if you're using a Chromebook because that's really the primary way you add features to a Chromebook.



STEVE:  Yes.



LEO:  Is with Chrome extensions.



STEVE:  Yeah.



LEO:  Wow.



STEVE:  Yeah.  So we've got something of a mess on our hands.



LEO:  Hard to fix this, yeah.  I don't know what the solution is.



STEVE:  Well, the problem is anything that's done will create hurdles and restrictions.



LEO:  Yeah.  You could do what Apple's doing with their App Store; but people don't like that, either.



STEVE:  Right.  And it's very much like the move to HTTPS.  You could say, well, HTTP still works.  You don't have to have a certificate.  Except now the browsers are labeling any HTTP site as not secure.  Which most end users are like, oh no, you know, like what does that mean?  It's like, oh, it's just the way it used to be.  And maybe the site doesn't have to be secure.  But now you get dinged if you're not.  So you can imagine if we do something to secure add-ons, it's going to make it more difficult, it has to make it more difficult for certifying an add-on developer somehow and making them obtain credentials and sign their work and so forth.  It's like, okay.  You know?  It's just more difficult.



LEO:  Yup.  I think that's what's going to happen, though.  That's the only way out.  You have to do that.



STEVE:  Yeah.



LEO:  Apple's proven it can work.



STEVE:  Yeah.



LEO:  Apple's doubling down on that on the Mac, you know, they're going to require people to get notarized.  You have to not only be a known developer, you have to be notarized to get an app running on the Mac.



STEVE:  Wow.



LEO:  As problematic as I find that, it's also the right answer for this particular problem.



STEVE:  Yeah.



LEO:  This malware issue.



STEVE:  So as we know, the U.S. federal government's National Institute of Standards and Technology (NIST) has produced several standards and guides over the years to aid organizations.  Back in 2016 it published a set of password rules which, you know, we made some fun of from time to time.  They were updated and improved.  NIST also publishes a cybersecurity framework that has become a useful litmus test for those who need to secure their data.



NIST's latest such work is a new privacy framework laid out in a 43-page document aimed at helping to protect Internet users' personal privacy.  I think it's a good thing.  I read through it and looked at it yesterday.  So this is a - and so it's NIST.gov/privacy-framework.  The framework can be used when developing new products and services to ensure that that new work hasn't overlooked something important.  So like a valuable checklist, just to sort of step through everything and make sure that you've got all your ducks in a row.  Very much sort of like a pass, like "Is this password safe" set of rules.



This is more extensive.  It can be used with an organization of any size.  And I like the idea that you would look through, knowing what your company is doing, sort of go through this framework step by step and just sort of, I mean, even if you don't choose to make a change, at least this brought up the issue so you can be aware.  And if you were to be able to be completely compliant, you'd be able to say we are compliant with NIST's formally established privacy framework.



LEO:  This is good.  I'm going to download this and send it to my executive team.  I think we should take a look at this.  That's good.



STEVE:  Yeah, I really do think it represents a nice step forward.  In the executive summary, the framework describes itself, saying:  "For more than two decades, the Internet and associated information technologies have driven unprecedented innovation, economic value, and improvement in social services.  Many of these benefits are fueled by data about individuals that flow through a complex ecosystem.  As a result, individuals may not be able to understand the potential consequences for their privacy as they interact with systems, products, and services. At the same time, organizations may not realize the full extent of these consequences for individuals, for society, or for their enterprises, which can affect their brands, their bottom lines, and their future prospects for growth.



"Following a transparent, consensus-based process, including both private and public stakeholders, to produce this voluntary tool" - all this is is just a checklist - "the National Institute of Standards and Technology (NIST) is publishing this Privacy Framework:  'A Tool for Improving Privacy through Enterprise Risk Management' to enable better privacy engineering practices that support privacy-by-design concepts and help organizations protect individuals' privacy.



"The Privacy Framework can support organizations in" - and they have three bullet points - "building customers' trust by supporting ethical decision-making in product and service design or deployment that optimizes beneficial uses of data while minimizing adverse consequences for individuals' privacy and society as a whole; two, fulfilling current compliance obligations, as well as future-proofing products and services to meet these obligations in a changing technological and policy environment; and, three, facilitating communication about privacy practices with individuals, business partners, assessors, and regulators."



Anyway, it goes on and on and on at length.  And I looked through it.  It is very comprehensive.  It sort of takes a tree branching approach, but ends up with a comprehensive bullet-point checklist.  And I would suggest that anybody could take advantage of it.  That is, you sit down and step through it with your IT group and just say, okay, where are we on this?  Where are we on this?  Where are we on this?  And, you know, you could certainly make a conscious decision not to do that, or to say, oh, yeah, we probably need to be better about that.  And it's like, okay.  Let's write that down and talk about it next month sort of deal.



So anyway, I wanted to bring it to the attention of our listeners since I think it would be - and you obviously see this, too, Leo - an extremely useful tool, just to create a series of discussion points, rather than just sort of saying, without any focus, well, are we protecting our users' privacy?  And it's like, okay, well, here it is.  I mean, it is comprehensive.  So anyway, I commend our listeners to take a look at it.  NIST.gov/privacy-framework.



LEO:  Yeah, it's good.



STEVE:  And here's where I got confused about telnet.  Get a load of this.  A hacker has leaked more than half a million, more than 500,000 telnet credentials for IoT devices.



LEO:  Half of them were admin:admin.  He brute forced them.



STEVE:  Yeah, that took a lot of work.  So as we know, port 23 is the so-called "well-known port" for the telnet service.  And as I'm sure our listeners know, telnet is the original old-school remote console access service that's been around from the start of the Internet.  And in fact, look at its low port number.  It's port 23; right?  So you know it's been around since the beginning.



It is such a massive perennial security risk that modern systems won't even allow the telnet service to be used remotely, not even for an instant.  I've been using FreeBSD Unix as my preferred non-Windows Unix platform for years.  Back on FreeBSD v5, which was the previous system I set up 15 years ago to host GRC's newsgroups, and we run our DNS server there, back then you had to really struggle to get the system to allow telnet connections.  I mean, so 15 years ago FreeBSD was saying, oh,  no, no, no, no.  I mean, like it wasn't on by default.  If you turned it on, it would fight you.  If you tried to bind it to a public interface, it just really refused.



So I was curious over the holidays when I was setting up GRC's new replacement Unix machine, which is running FreeBSD v12.1.  Turns out there is no way to do it any longer.  The OS simply and wisely refuses.  The only way to connect externally now is over port 22 using, as you mentioned before, Leo, secure shell, SSH.  But not all operating systems are designed correctly.  And I would argue today that is correct design.  There's just no way because, I mean, it's not like there aren't secure shell clients for everything.  There are.  So why would you not use a secure shell connection which, as you mentioned, Leo, is encrypted, and which also has all kinds of very strong authentication measures.  As I said, not all operating systems are yet designed correctly.  And various IoT devices are the worst offenders.



Consequently, news was made last week when the long-time operator of a DDoS-for-hire service, a so-called "booter" service because it boots the target, the DDoS target off the Internet, published the remote login telnet - and get this Leo - username and password combinations, along with the IP address.  That is, this is not just a username and password list.  This is more than half a million, here is your telnet, the IP of the device, with its username and password.  It turns out it was 500, little more than 515,000 devices of all kinds, which are listening on the Internet.  And yeah, some of them may have IPs that float around a little bit.  But more than half a million.  So, yeah.  The brain-dead telnet service, providing what is effectively full remote console access, is still actively supported and being served by a huge number of insecure and now suddenly much less secure Internet-connected devices.



This guy was asked why he published such a massive list of essentially what are either bots or about to be bots.  And he said he had upgraded his DDoS service from working on top of IoT botnets to a new model that relies on renting high-output servers from cloud service providers.  In other words, he'd apparently moved on to commandeering bigger game, so he thought he'd just drop off a little gift.



Anyway, Leo, last week you mentioned that you occasionally use GRC's ShieldsUP! service.  We talked about how it had 103 million uses last time I looked.  So I'll note that since telnet runs over TCP, and since ShieldsUP! checks all, well, checks for and reports on all listening ports from 1 to 1023 - no, actually it goes a few above that now.  I think it goes like 15 or 18 ports above that.  It crosses the 1023 barrier.  I'll note that, anyway, our users can quickly use ShieldsUP! just to verify that no device on their internal network may have surreptitiously used UPnP to open a telnet port through their routers.



As we've said from the start, Universal Plug and Play is an extremely mixed blessing.  In order for it to work, it needs to be UI free.  And that's its whole point.  But that also allows anything on your internal LAN to silently open incoming ports without permission.  And I wouldn't put it past some poorly designed webcam to use your router's Universal Plug and Play to open an incoming telnet port.  And who knows, it might have a non-default or a default for its make and model username and password to allow remote admin from the mothership.



Anyway, you just want to make sure nothing has opened a port into your network.  And ShieldsUP! can do that very quickly.  So anyway, 515,000 devices, username, password, and IP, dropped onto the Internet.  So again, you don't have to be anything more than a so-called "script kiddie" in order to take that and literally write a script to see whether you're able to log onto any of those devices.  And once you have, well, you've got telnet access to something.  What a world.  I'm surprised that, I guess, I was going to say I'm surprised that ISPs are still allowing...



LEO:  They should just block that port.  But there are good reasons, well, there's no good reason to use it.  But there's reasons they put it in, for remote access and stuff.



STEVE:  Yeah.  I mean, it is a bona fide - although they are blocking Windows.



LEO:  SFTP and port 138 or 139, yeah.



STEVE:  Yup.



LEO:  Yeah.  I guess they could reasonably block telnet port.  But, yeah, I mean, who really uses that?



STEVE:  So we have a welcome micropatch for the Windows IE JScript.dll zero-day vulnerability.  And I was hoping that was going to happen.  We talked about this last week.  This is the somewhat worrisome zero-day vulnerability.  IE is able to invoke a vulnerable JScript.dll which contains a remote code execution vulnerability.  We've talked about this company before, 0patch, the numeral 0patch.com.  Their deal is they'll, when a Windows vulnerability is announced for which Microsoft has not yet created a patch, these guys will create what they call a "micropatch," which is typically just exactly that.  It's not replacing the whole DLL.



In this case, it's 18 bytes, an 18-byte micropatch which prevents the invocation by IE of this particular DLL.  I heard, well, we know both anecdotally - I ran across some reports in GRC's own newsgroup and I've seen coverage of this - that, for example, killing all access to this JScript.dll keeps Windows Media Player from being able to operate because Windows Media Player, of all things, has the option to allow web content to be displayed.  And so it attempts to invoke the DLL.  When it's unable to, it complains.



So anyway, their blog posting is titled:  "Micropatching a Workaround for CVE-2020-0674."  And they said, and I'm going to share this, the top of their blog posting - it was long - but to give our users a sense for this and because it might end up being useful in the long term for those one out of four systems that are still running Windows 7.



"Last Friday," they wrote, "Microsoft published an advisory about a remotely exploitable memory corruption vulnerability that was reported to them by Qihoo 360 as being exploited in the wild.  These attacks were reportedly limited, so Microsoft decided not to rush with issuing a patch, but rather will provide one as part of February's Patch Tuesday.  They did, however, provide a workaround."



They said:  "Because the provided workaround has multiple negative side effects, and because it is likely that Windows 7 and Windows Server 2008 R2 users without Extended Security Updates will not get the patch at all" - since support, as we know, ended this month, they said - "we decided to provide a micropatch that simulates the workaround without its negative side effects.  The vulnerability," they wrote, "is in JScript.dll, which is the scripting engine for legacy JScript code."  They said:  "Note that all non-legacy JScript code" - whatever that might be - "and all JavaScript code gets executed by the newer scripting engine implemented in JScript9.dll," which as we said is not vulnerable.



They wrote:  "Microsoft's workaround comprises setting permissions on JScript.dll such that nobody will be able to read it.  This workaround has an expected negative side effect that, if you're using a web application that employs legacy Jscript, and can as such only be used with IE, this application will no longer work in your browser."  They said:  "There are also several other negative side effects.  Windows Media Player is reported to break on playing MP4 files.  The sfc (Resource Checker tool) that scans the integrity of all protected system files and replaces incorrect versions with correct Microsoft versions, chokes on JScript.dll with altered permissions.  Printing to Microsoft's 'Print to PDF' is reported to break.  Proxy Automatic Configuration (PAC) scripts may not work."



So they said:  "Microsoft's advisory states that the provided workaround will have to be reverted when they issue a patch for JScript.dll.  However, note that some additional side effects may result from changing the ownership on JScript.dll."



Anyway, so then they go on to talk about a test case.  Leo, we were talking about what it took to invoke it.  And in fact they show a little bit of HTML here that I have in the show notes where it's as simple as saying <script language="Jscript.Encode">.  That's all it takes to say that's the script engine we want.  And in their little test, they enclose in the script tags alert("JScript.dll was loaded").  So that if you run that HTML, and JScript.dll is available, it'll just pop up a little alert dialog saying it was loaded.  And so they use that in order to test their micropatch.



Anyway, so they have a micropatch.  It's free.  Anybody who encountered the Windows Media Player problem could revert the permissions, apply this free patch, and you'd have the problem fixed.  And if you're a Windows 7 user, you could revert the fix, apply the patch, and you're good to go moving forward.  So they said, and this is what I thought was interesting, they explain that they've ported this patch to Windows 7 and 10 workstation and server editions.  And they're promoting their free personal and for non-profit use service.



They say:  "If you're a 0patch user, you already have this micropatch downloaded to all your online computers with the 0patch Agent and, depending on your settings, already automatically applied and protected to all processes using IE11 engine for rendering content.  This includes Internet Explorer, Microsoft Word, Microsoft Outlook, and a variety of other applications."



Then they said:  "As with all our micropatches, you can switch this micropatch on or off and have it instantly applied to, or removed, from running applications, effectively making it a kill switch" - but it's a selective kill switch because it only does it for IE11, and that doesn't affect things like Windows Media Player - "a kill switch for JScript.dll."



And so since I think this is cool, I wanted to share a couple of the Q&A.  They ask themselves:  "Why would we apply your micropatch instead of Microsoft's recommended workaround?"  They answer:  "Our micropatch is designed to avoid negative side effects of Microsoft's workaround.  It can also be easily reverted, unapplied, with a switch of a button without leaving any traces, while the workaround changes the ownership on JScript.dll."



They ask themselves:  "Will Microsoft provide a patch for CVE-2020-0674 to Windows 7 and Windows Server 2008 R2 users without Extended Security Updates?"  And they replied:  "We don't know, but these systems are now officially out of support, and Microsoft has historically only issued security patches for unsupported systems in extreme cases."  Then they quote the Shadow Brokers leak and BlueKeep.  They said:  "We at 0patch have committed to provide post" - and here's the point.  "We at 0patch have committed to provide post-end-of-service security micropatches for Windows 7 and Windows Server 2008 R2 for three additional years, which is why we're also issuing this micropatch for these platforms."



And they said:  "What will happen if I apply your micropatch, and then apply Microsoft's patch after it comes out?"  They said:  "When Microsoft issues a patch for the vulnerability, we'll inspect it and decide whether to replace our current micropatch," they said, "which resides in mshtml.dll and disables JScript.dll entirely, with a more targeted micropatch in JScript.dll, which will only fix that vulnerability but keep JScript.dll available."  They said:  "It might happen that we do so on supported Windows platforms, but keep the current micropatch on Windows 7."



Anyway, so here's the deal.  These are good guys.  I've never run across any downside or negative news about them.  What they do is, until a patch is available, they will do a selective disabling.  They've got a system on extended security updates.  When Microsoft patches this, they're going to take a look at it, in the same way that bad guys reverse engineer unknown problems.  If it's something they can fix, they're going to issue a true patch for Windows 7 and Windows Server 2008 R2, assuming that Microsoft opts not to break their own statement of not keeping those systems updated.



Which suggests to me that anybody who wants to stay with Windows 7 and Windows 7 2008 R2 could add this 0patch agent to their system and be kept secure moving forward.  As Microsoft creates patches for the Windows 7 that is receiving Extended Security Updates, these guys are going to reverse engineer them and create a flow of monthly micropatches to keep Windows 7 security current for the same three years that paying for Extended Security Updates will do so.  So we're certainly on this podcast going to keep our eye on this, and we will let our listeners know if this continues to work and how it goes.  But it's a terrific free opportunity.  And again, a free service for free and non-profit organizations or individual use.



LEO:  Why are they doing this?



STEVE:  They do have an upsell.  They have, if you go to 0patch.com, there is a paid service which is how they're making their money.  So if an organization wants to do it, if you want to deploy it to all of your systems and so forth, then I would recommend doing it that way.  But for our listeners who want to keep their home systems current, I have a feeling we're going to be recommending this moving forward.



LEO:  I'm always nervous about free stuff.  You saw what Avast has been doing with all the people using their free antivirus.



STEVE:  Uh-huh.  Yes, yes, we talked about that.



LEO:  Geez, Louise.



STEVE:  It was awful.



LEO:  There's new stuff on it.  It's much worse, yeah.



STEVE:  Oh, no kidding.  Agh.



LEO:  Yeah.  It was watching everything you did, everything.  And it wasn't that extension.  It was just they were selling it to a - I don't know if it's a third party or a branch of Avast that collects this information.



STEVE:  Yeah, we did talk about this.  They purchased an organization that was then selling this intelligence information.  And, yeah.



LEO:  Nasty, yeah.



STEVE:  And really profiting.



LEO:  Yeah.



STEVE:  So Leo, a little bit of miscellany.  I forgot to tell you last week that we made another attempt at Star Wars.  You had said, and you were right...



LEO:  Oh, yeah, because you went to that crappy...



STEVE:  Oh, my god, that 4DX.



LEO:  ...amusement park theater version.



STEVE:  The vomitorium Star Wars.  Oh, my lord.



LEO:  So absent the seats moving and the spray in your face, what did you think of the movie?



STEVE:  I loved it.



LEO:  Yeah.



STEVE:  Absolutely loved it.



LEO:  Yeah.  I mean, I understand the criticisms.  But we're completing a nine-movie cycle here; right?



STEVE:  Yeah.  I mean, we were kids when the first one came out.



LEO:  Literally, yeah, '77, yeah.



STEVE:  So, yeah.  I did like it.  And I did resign my Disney Plus subscription before - for me it was on the 24th it was coming up, and that's why Lorrie and I watched that two-hour making of the first Star Wars trilogy that was really good.  We're really glad we watched that.  



LEO:  Good.



STEVE:  And now I have to mention "Picard."



LEO:  Uh-oh.



STEVE:  Which was, I thought, wonderful also.



LEO:  Oh, good.  Oh, good.



STEVE:  It's 8.8 on IMDB, which is not easy to get.  And it's released, an episode drops every Thursday on CBS All Access.  Non-U.S. Prime users can get it a day later on Amazon Prime.  So if you're using a VPN to appear to be somewhere else, then maybe you can get it through Amazon Prime.  I have CBS All Access.  It's one of my cord-cutter subscriptions.  It really looks good.  We're going to watch this first episode a second time just because this guy can really act.  And lots of special effects, lots of action.  Looks like it's going to be an interesting series.  So I wanted to tell our listeners that I give it a thumbs-up, even though it's not as easy to do it because you've got to buy Access in order - unless you have some other way of finding it.



And at grc.sc/, you know, "sc" as in shortcut, grc.sc/roadmap is a recently created roadmap for the future of SpinRite.  So for anyone who is curious, grc.sc/roadmap.  I lay out where we are, what I'm going to do with 6.1.  And a new wrinkle was added because I'm in the process of firing this up.  It turns out all of my SpinRite tools from, what was it, 2013 were running under Windows XP, and they were 16-bit.  So I'm having to change a bunch of things around.



It turns out that finding a linker that will run under Windows is not easy.  But Watcom looks like the right solution.  The Watcom system is a beautiful piece of work that doesn't really get much attention.  It's known within the development community as a fabulous compiler, and it's got really broad support.  So I'll probably be using it for remote debugging, which I wasn't doing before.  I was using a native debugger, a 16-bit debugger.  Well, naturally, because it's on DOS, which is where SpinRite runs.



But anyway, the point is that somebody referred to AHCI, the Advanced Host Controller Interface, which is what SpinRite 6.1 will be supporting natively, as "legacy."  And I thought, whoa, wait a minute.  When did that happen?  Well, it turns out that it's legacy inasmuch as it is fixed, it is set in concrete and is not changing any longer.  But of course the newer controller is the NVME controller, the Non-Volatile Memory controller, which non-volatile memory built into the latest laptops and like my little Intel NUCs that I've been talking about, they're all NVME.  So of course that's going to need native support, too.



So anyway, I realized, okay, I need to sort of lay out the sequence of work that I'll be doing and how I see the future.  So anyway, I created a roadmap for my ongoing development work.  I have a feeling, I mean, I'm, like, I'm back in.  I can't say I've actually started, but within a couple days.  The problem is I've had to, like, really, my whole development environment has had to be reengineered in this era of 64-bit OS which is trying to reach down to a 16-bit real mode DOS and do development work there.  But anyway, I've figured out how to do it.  I just haven't finished putting all the pieces together yet.  



LEO:  Cool.



STEVE:  For those who are interested, we are getting there.  SpinRite 6.1 is on its way.



Okay.  SHAmbles.  What happened?  There's been a duo, Gatan Leurent and Thomas Peyrin.  We've spoken of them before because for years they have been methodically pounding on and essentially chipping away at the aging SHA-1 hash.  And in the course of all of that, they've come to know it extremely well.  And in a paper just published, they have finally broken SHA-1.  They've taken it from weakened to, well, I would say "weakened and worrisome" to "demonstrably broken."  Their recently released paper is titled "SHA-1 Is a Shambles.  First Chosen-Prefix Collision on SHA-1 and Application to the PGP Web of Trust."  And I'm going to share with our...



LEO:  Oh, interesting.



STEVE:  Yeah, yeah, yeah.  They created a practical attack on PGP.  Essentially broke its, well, to the degree that PGP still supports SHA-1.  So not SHA-256 signatures, but SHA-1.  So to kind of get a look inside, I'm going to share the abstract from their paper because it's just got a whole bunch of cool bits.  They said:  "The SHA-1 hash function was designed in 1995 and has been widely used during two decades."  They said:  "A theoretical collision attack was first proposed in '04, but due to its high complexity it was only implemented in practice in 2017."  So isn't that cool?  First of all, a theoretical collision attack proposed in 2004.  But due to its high complexity - that is, computational cost - was only implemented in practice, that is, we only got the computation resources 13 years later, in 2017, using a large GPU cluster.



They said:  "More recently, an almost practical chosen-prefix collision attack against SHA-1 has been proposed.  This more powerful attack allows to build colliding messages with two arbitrary prefixes, which is much more threatening to real protocols."  And I'll explain why in a second.  They said:  "In this paper, we report the first practical implementation of that attack and its impact on real-world security with a PGP/GnuPG impersonation attack.  We managed to significantly reduce the complexity of collisions attacking against SHA-1.  On an Nvidia GTX 970, identical-prefix collisions can now be computed with a complexity of" - and I'll explain what these things mean in a second - "the complexity of 2^61.2 rather than 2^64.7."



Okay, now, think about that.  So 2^62.2 versus 2^64.7.  So that's a difference of a little more than three, which is to say 2^3 is 8.  So it's a factor of 8 reduction in complexity.  And chosen-prefix collisions with a complexity of 2^63.4 versus 2^67.1.  Okay.  So there, 63 versus 67, that's a difference of 4, so 2 to that is 16.  So they made it 16 times more practical.



So they said:  "When renting cheap GPUs, this translates to a cost of $11,000 U.S. in the first case, for a collision, and $45,000 for a chosen-prefix collision within the means of academic researchers."  Okay, but also notice that what they did was, by getting that factor of 16 reduction, they were able to - I didn't do the math beforehand, but I've got a calculator right here; 45K was $720,000 for one collision.  So, yeah.



LEO:  It's not easy.  Not cheap.



STEVE:  Yeah.  $720,000 of computing.



LEO:  You've got to really want to get in.



STEVE:  For just one collision.



LEO:  That's a NOBUS, yeah.



STEVE:  Yeah.  So they dropped it from 720,000 to 45,000.  Still not cheap, but within reach.



LEO:  It shows you, I mean, that's a good measure is the cost of computing power, yeah.



STEVE:  Yes, yes.  So they said:  "Our actual attack required two months of computations using 900 Nvidia GTX 1060 GPUs."  So again, consider how high the threshold is.  I mean, when we say SHA-1 has been compromised, that is, it's broken, well, it's like it's now to the point where we can no longer trust it.  But that's how high the trust bar is, that to create an academic collision with a whole bunch of special case caveats, in this case this chosen prefix collision, it took 900 Nvidia GTX 1060 GPUs.  Oh, and they said:  "We paid $75,000 because GPU prices were higher at the time.  And," they said, "we wasted some time preparing the attack."



So they said:  "Therefore, the same attacks that have been practical on MD5" - remember that's the much smaller hash, like way older hash, MD5 - "since 2009 are now practical on SHA-1.  In particular, chosen-prefix collisions can break signature schemes and handshake security in secure channel protocols such as TLS and SSH.  We strongly advise to remove SHA-1 from those types of applications as soon as possible."  That is to say, signatures and secure handshakes.



They said:  "We exemplify our cryptanalysis by creating a pair of PGP/GnuPG keys with different identities, but the same SHA-1 certificates.  An SHA-1 certificate of the first key can therefore be transferred to the second key, leading to forgery.  This proves that SHA-1 signatures now offer virtually no security in practice.  The legacy branch of GPG still uses SHA-1 by default for identity certifications.  But after notifying the authors, the modern branch now rejects SHA-1 signatures."  And they said the issue was tracked as CVE-2019-14855.



So then I'll just finish with a little bit of background, which is fun, from their paper, since we have got time.  They said in their introduction:  "Cryptographic hash functions are present in countless security applications and protocols, used for various purposes such as building digital signature schemes, messaging authentication codes, or password hashing functions.  In the key application of digital signatures, for example, hash functions are classically applied on the message before signing it, in order to improve efficiency and provide security guarantees.  Informally, a cryptographic hash function H is a function that maps an arbitrarily long message M to a fixed-length hash value."



And they say:  "We denote n its bit size.  Collision resistance is the main security property expected from a hash function.  It should be hard for an adversary to compute a collision."  In other words, two distinct messages, M and M0, that map to the same hash value H(M) and H(M0), where by "hard," they have in quotes, one means no faster than the generic  2^n/2 computations birthday attack.



So in the case of SHA-1, it's a 160-bit hash.  So 160/2 is 80.  So it should not be possible to do this any faster than 2^80.  And as we see, they've brought it down through extensive analysis down to the region of 2 to the, what, 60s, 62, 63.  So what is that, a 2^14 reduction.  Well, 2^14 is a big number.  1024 is 10, so 16.  So it's 16,000.  So they've reduced the complexity by an order of - on the order of 16,000 making - and so we saw even now it's still $45,000 in GPU cost.  So multiply 45,000 times 16,000, and we're into some serious money.  Which means, you know, as long as there was no weakening of the hash, it was sufficiently strong.  Unfortunately, it hasn't stood the test of time.



Anyway, they said:  "A cryptanalyst will try to find a collision for the hash function at a reduced cost, but ad hoc collision attacks are hard to exploit in practice because the attacker has then usually little control over the value of the actual colliding messages, in particular where the differences are inserted, which are the interesting parts when attacking a digital signature scheme, for example."



They said:  "Thus, one can consider stronger and more relevant variants of the collision attack in practice, such as the so-called chosen-prefix collision or CP collision."  Okay, which is two message prefixes, P and P0, are first given as challenge to the adversary, and his goal is to compute two messages M and M0 such that the hash of P concatenated to M equals the hash of P0 concatenated to M0.



"With such ability, the attacker can obtain a collision even though prefixes can be chosen arbitrarily, and thus potentially contain some meaningful information.  A chosen-prefix collision can also be found generically," as they note, "with 2^n/2 computations.  And there we're back to 2^80," they said, "for a 160-bit hash function like SHA-1.  But ad hoc chosen-prefix collision attacks are much more difficult to find than plain collision attacks because of the random and completely uncontrolled internal differences created by the prefixes.  Yet a chosen-prefix collision attack was found for the MD5 hash function, eventually leading to the creation of colliding X.509 certificates, and later a rogue Certificate Authority.  CP collisions have also been shown to break important Internet protocols, including TLS, IKE, and SSH, because they allow forgeries of the handshake messages."



So anyway, I won't go on at that level.  Their paper is online.  I've got a link in the show notes if anyone's interested.  Essentially what this means is that, through a lot of work, I mean, serious academic research, these guys have conclusively demonstrated that, even though it's still expensive, it is no longer credible to trust anything which is protected by SHA-1.  Although there are still limitations.  It's still expensive.  It's not like it just, like, falls out instantly.  No cryptographic protocol or cryptographer worth their salt would recommend using SHA-1 now.  It is the case that you cannot trust it. 



LEO:  Even if it's salted?



STEVE:  Even if it's had salt sprinkled on its tail, yup.



LEO:  Aha.



STEVE:  And the good news is the cryptographic industry, as we all know, anticipated this.



LEO:  Yeah.



STEVE:  We've had SHA-2, which comes in various flavors, SHA-256, what is it, -384 and -512, they all exist.  They're there.  They're in place.  They're ready.  And we already have SHA-3 waiting in the wings, although no one thinks we need it yet because SHA-256 is very strong.



LEO:  So all of these, it's really just inevitable, as long as processing power increases exponentially, that they're going to get cracked eventually.



STEVE:  Yup.  I mean, they're doing something that is "hard," in quotes, but our definition of what is "hard" keeps changing.



LEO:  Yeah, it gets easier.



STEVE:  Now we're got facial recognition happening on the street corner of everyone who walks by.  That was inconceivable 20 years ago.



LEO:  Yeah.



STEVE:  But now it's like, oh, yeah.



LEO:  It's kind of amazing, yeah.  Ah.  Once again, another gripping, thrilling edition of Security Now!, Steverino.  Thank you.  



STEVE:  Well, the spouses of some of our listeners may disagree.



LEO:  Oh, they're all asleep by now.  Hey, I do want to remind people we do want you to participate in our survey.



STEVE:  The survey, yes.



LEO:  Want to make sure the Security Now! listeners are well represented.  The audience survey takes a few minutes.  We don't ask for email.  Because we don't track you, we don't have any way to know anything about you.  So by doing this survey once a year, we get a better idea.  It helps us prepare programming that you're going to like, but it also helps us sell ads, and that's our lifeblood.  So twit.to/survey20, if you don't mind.  You don't have to answer any question you don't want to answer.  You don't have to answer the survey at all.  But it's nice if we can get some of you to do that, anyway:  twit.to/survey20.  Thank you, as always, for supporting Security Now!.



Steve's site is GRC.com, the Gibson Research Corporation.  That's where you'll find SpinRite, the world's best hard drive maintenance and recovery tool.  Everyone needs a copy of SpinRite.  And while you're there, of course, you can pick up a copy of the show, 16 and 64Kb audio versions there.  A great transcript.  Takes about three or four days for Elaine to put that up after the show. 



STEVE:  She gets the audio, typically I compress it early in the morning on Wednesday.  I get the transcript from her early, like mid-morning on Thursday.  And then, if I'm not distracted, I get it up immediately; or it's delayed depending upon when I see her email and blah blah blah.



LEO:  I don't want to know how you get distracted.



STEVE:  Yeah.



LEO:  Oh, look.  It's a Fibonacci series.  GRC.com, that's the place to get all of that.  You can also find many other free and useful tools like ShieldsUP! and everything.  So check it out, GRC.com.  Steve's on Twitter at @SGgrc.  DM him there if you have questions, comments, suggestions.  His DMs are open.



You can also get the show from our website, TWiT.tv/sn.  Audio and video available there of every show, all 751 of them now.  Or on YouTube.  You can watch it on YouTube, of course.  Best thing to do, if you ask me, subscribe.  Find your favorite podcast application and subscribe.  Then you don't even have to think about it.  It's just on your device, ready to listen to whenever you're in the mood.  And who isn't always in the mood for Security Now!?  I've always got time for Security Now!.  Thank you, Steve.  I'll see you next week on Security Now!.



STEVE:  Just like Jell-O.  There's always room.



Copyright (c) 2020 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#752

DATE:		February 4, 2020

TITLE:		The Little Red Wagon

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-752.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we examine the most recent flaw found in Intel's processors and what it means.  We look at the continually moving target that is Windows 10.  We consider the Free Software Foundation's suggestion that Microsoft open source Windows 7 and the fact that last month's was apparently NOT the last update of Windows 7 for all non-ESU users.  We look at the evolution of exploitation of the Remote Desktop Gateway flaw, Google's record breaking vulnerability bounty payouts, the return of Roskomnadzor, the size of fines, the question of who owns our biometrics, an update on Avast/AVG spying, the future of third-party AV, a major milestone for the WireGuard VPN, and the wonderful Little Red Wagon hack of the decade which titled this podcast.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  It's one of those great potpourri episodes with lots to talk about, including the strangest Windows 7 bug.  Microsoft says this time they are going to fix it.  We'll also talk about some scary hacks, including the latest Intel problem.  And then that Little Red Wagon that could.  It's all coming up next on Security Now!. 



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 752, recorded February 4th, 2020:  The Little Red Wagon.



It is time for Security Now! with our friend and chief, Steve Gibson, the man in charge of the Gibson Research Corporation.  I've always meant to ask you.  What is it you're researching, Steve?



STEVE GIBSON:  Well, the story behind that is that I really liked Gibson Labs as the name.  And that was my previous company's name, Gibson Labs.  That's where I did the light pen.



LEO:  That must be where I got the idea for Tech Guy Labs, from that.  Because that's the website for the radio show.



STEVE:  Maybe.  I just sort of liked that.  But I sort of sold it to Atari along with the light pen.



LEO:  Boom.  Whoops.



STEVE:  And when I wanted then to do something again, my attorneys at the time said, you know, technically...



LEO:  You sort of sold that.



STEVE:  You still have it.  Well, because then Atari backed out because the home computer business collapsed.  So then I took it to Koala, and they made the light pen for a while.  And then...



LEO:  I remember them.



STEVE:  And then it kind of stayed there, but it wasn't clear whether they just got the pen or everything.  And so my attorneys said, "Yeah, you need another name."  And I said, "I don't want another name."  And they said, "You need another name."



LEO:  You've got to have one.



STEVE:  I said, "Okay, Gibson Research."



LEO:  There you go.



STEVE:  I'll research something.



LEO:  Something.  Anything. 



STEVE:  And they said, "Okay, that's what we'll do."  So that's, you know.



LEO:  That's really funny.



STEVE:  I could probably get it back now, but I don't care.  Now it's GRC.  And I mean, like, well, and it's funny, too, because I remember when we were trying to get a domain, I said to Millard Ellingsworth III...



LEO:  What?



STEVE:  Who was, yeah, he was one of my guys.  I said, "Millard."  And he was sort of that way anyway.  I said, "We need to have a domain name, apparently."  And this was, like, six months after Microsoft, Bill, had decided the same thing.  So of course they already had their name.  Anyway, so Millard came back, he said, "Well, we can't get Gibson."  And I said, "Why?"  He says, "Guitars."  I said, "Oh."



LEO:  Oh, yeah.  Oh, yeah.



STEVE:  Or maybe it was refrigerators.  I don't remember.  It was something.



LEO:  There is a guitar company called Gibson.



STEVE:  Yeah.  I do know that.  And he said, "But I got GRC."  And I said, "Oh.  I think I like that."  And so...



LEO:  It's short.



STEVE:  Yeah, exactly.  And then, like, sometime later, because back then we were using something called cc:Mail, where we had sort of a network, and we had a dialup that would phone into something and retrieve our email.



LEO:  I remember that.



STEVE:  And retrieve our email every so often.



LEO:  Yeah, yeah, yeah.



STEVE:  You know, I mean, it was the dark ages still.  And it was like a big deal when I wired the office for coax in order to have 10Base2 interconnectivity, and we could all print to a common printer.  And we had a laser printer; and that was, like, whoa, look at that, doesn't make any noise.  And so eventually it was like time to actually get on the Internet.  And so there was a local company down here, and so I called them up.  And this gal answered, and I said, "Yeah, I need to hook up to the Internet."  So she says, "Okay.  Do you have a domain?"  I said, "Yes, I do."  And she said, "What is it?"  And I said, "GRC.com."  And even back then she said, "Oh, three letters."



LEO:  Three letters, whoo.



STEVE:  And I get offers constantly.  The most recent one was an offer of $50,000 for just a blind offer for...



LEO:  What?



STEVE:  Yeah, they just want GRC.com because it's three letters.



LEO:  That's impressive.  Wow.



STEVE:  Yeah, so, you know, somehow, after I'm dead, someone, my estate will liquidate GRC.com and, you know...



LEO:  Get profit.



STEVE:  By a little bit.  Today's Episode 752 - now, because you're clued into the world, Leo, you knew what I meant by "The Little Red Wagon."



LEO:  That's the title.



STEVE:  And that's the title of today's podcast is "The Little Red Wagon," just because it is the coolest hack that - anyway, I can't wait to share it with our listeners.  That will be - we didn't have like a big earth-shaking killer topic.  Lots of interesting stuff happened, which is what today's podcast will be about.  But it will conclude with what is probably the coolest hack I have seen of the decade.  And I'm not sure whether the decade actually starts next year or this year.  There seems to be some concern about that.  But it's just so fun.  So "The Little Red Wagon" is the title for today's podcast.



LEO:  I added "Little" because it just said "The Red Wagon."  So I put "Little" in there.



STEVE:  Oh, and that's interesting because - maybe it wouldn't fit.  Because I'm looking at my show notes, and it says "The Little Red Wagon."



LEO:  Oh, yeah.  You got it right, yeah.  Somebody just left out "little," that's all.



STEVE:  Ah, okay.  And believe it or not, the Picture of the Week is the way it came.  It just happens to use my name, but that's a fluke.



LEO:  All right, Steve.  This is the customized Picture of the Week.



STEVE:  Okay.  So, yeah, it just happened to involve a character named Steve.  We've got two guys walking down the sidewalk.  And they've just passed a store whose awning says "Clothes for Steve."  And then they're crossing in front of the store "Furniture That Would Look Good in Steve's House."  And then down toward the end of the whole cartoon is "Steve's Favorite Ice Cream Flavors."



LEO:  Okay.



STEVE:  And so the caption underneath says - the other guy's apparently talking to Steve, says "Maybe you should disable your cookies, Steve."



LEO:  I love it.



STEVE:  Yes, you are being tracked.



LEO:  And really the web is like that; isn't it.  Hi, Steve.



STEVE:  It is very much like that.



LEO:  Yeah, everything's for Steve.



STEVE:  Okay.  So our first little tidbit, we've got lots of little tidbits, was the follow-up on the news that you broke at the beginning of last week's podcast, L1D eviction sampling, which the news had just happened.  And you presented this, and correctly, as a breaking news item.  Another flaw in Intel chips had just surfaced called L1D Eviction Sampling.  And we guessed correctly that L1 would refer to the Level 1 cache.  And since it had the term "eviction" in it, which is the term used by caching, in fact, I heard you use it on MacBreak Weekly.  You were talking about CacheFly and evictions, eviction of cached data.  So of course it applies there, too.  Eviction is the term used when new room needs to be made in the cache for something more recent.



And so typically LRU caching, Least Recently Used caching, knows what the oldest thing is which is just sort of statistically less likely to be reused.  And so it gets evicted from the cache.  Anyway, so we were right about our guess.  We now have the fancy name, or the popular name.  It's called CacheOut.  And of course it has a website, CacheOutAttack.com.  And the logo, it has to have a logo these days, and so it's got a slot machine as the CacheOut Attack logo.



The page's introduction explains - this is the "Leaking Data on Intel CPUs via Cache Evictions."  And they said, the researchers said:  "We present CacheOut, a new speculative execution attack that's capable of leaking data from Intel CPUs across many security boundaries.  We show that, despite Intel's attempts to address previous generations of speculative execution attacks, CPUs are still vulnerable, allowing attackers to exploit these vulnerabilities to leak sensitive data."



Then they said:  "However, unlike previous MDS" - and now we know, we've all been tuned up on this, this is the acronym for Microarchitectural Data Sampling, so MDS, Microarchitectural Data Sampling issues.  "We show in our work how an attacker can exploit the CPU's caching mechanisms to select what data to leak, as opposed to waiting for the data to be available. Finally, we empirically demonstrate" - and do they - "that CacheOut can violate nearly every hardware-based security domain, leaking data from the OS kernel, co-resident virtual machines, and even SGX enclaves."



So these are researchers at the universities of Adelaide and Michigan.  They showed in their paper - and I'll share five bullet points because these are a little bracing - the effectiveness of CacheOut in violating process isolation by recovering AES keys and plaintexts from an OpenSSL-based victim.  Second bullet, practical exploits for completely de-randomizing Linux's kernel ASLR - right, the Address Space Layout Randomization, which is like a preamble for then exploiting return-oriented programming attacks - they said, and for recovering secret stack canaries from the Linux kernel.  And that again, if you can recover the stack canary, then you're able to avoid tripping Linux's stack overflow checks as again, in order to leverage a powerful attack.



Three, how CacheOut effectively violates the isolation between two virtual machines running on the same physical core.  Four, how CacheOut could also be used to breach the confidentiality SGX guarantees by reading out the contents of a secure enclave.  And, finally, how some of the latest Meltdown-resistant Intel CPUs are still vulnerable, despite all of the most recent patches and mitigations.



And as I was pulling the notes together, I then wrote at this point:  "Intel, who really used to enjoy their original job of printing money, once again responded to this latest annoyance." Because, gee, it's just not so easy to make these checks anymore.  We used to just go out in the back, and we'd get a wheelbarrow full of sand, which is of course silicon, and melt it down and purify it and then just charge a thousand dollars for these little tiny chips of silicon.  That's not quite so easy anymore.



So basically they acknowledged, yes, they have now the security software guidance.  And it's like, once upon a time, firmware, I mean, microcode updates were like not a thing.  Now, yeah, get your latest update here.



So they said:  "A speculative execution side channel variant known as L1D Eviction Sampling may allow" - we know that everybody, you know, Microsoft phrases their things the same way, as opposed to "has been shown to allow."  No, "may allow  the data value of some modified cache lines in the L1 data cache to be inferred under a specific set of complex conditions."  Yeah, well, okay, fine.  That's not much of a mitigation because bad guys, as we know, are willing to work hard to infer the data, the value of the data that's just been evicted from the cache.



Intel said:  "On some processors under certain microarchitectural conditions" - which is to say ours - "data from the most recently evicted modified L1 data cache line may be propagated into an unused, invalid L1D fill buffer."  Okay.  Basically they're going to dazzle us with the details.  "On processors affected by Microarchitectural Data Sampling or Transactional Asynchronous Abort (TAA), data from an L1D fill buffer may be inferred using one of these data sampling side channel methods.  By combining these two behaviors together, it may be possible for a malicious actor to" - and again, has been shown to be possible - "to infer data values from modified cache lines that were previously evicted from the L1 data cache."  At this point everyone starts to snore.



They said:  "This is called L1D eviction sampling.  Malicious software may be able to use L1D eviction sampling to infer modified cache line data written by previously run software, or modified cache line data written by software running on a sibling hyperthread on the same physical core."  And note here  the term "modified" in "modified cache line data" refers to the fact that the data being evicted has been modified, and that cache is "write back" rather than "write through."  So that modified line needs to be written back out to at least a lower level cache, maybe all the way back out to external DRAM.  That's what takes the time. If the data had not been modified, then it could just be overwritten, which would take no time.



Anyway, they go on with similar, like, you know, I won't drag our listeners through it, but mumbo jumbo of this sort.  So the CacheOut page has an extensive FAQ for anyone who's interested.  But the takeaway for most of us, if not all of us, is the same as it's been for the past two years.  Yes, it's scary, and it sounds bad.  But even though it's real, it's another difficult-to-exploit fringe theoretical academic problem.  No known real-world attacks have ever been detected in the wild.



On the other hand, they would not likely be because it doesn't leave any obvious footprints.  It just allows one evil process sharing a core, because that's where the L1 cache is, so it allows - you have to have core sharing; two, if it wanted to find something out about what has recently been done on that core.  And Intel has released another round of microcode patches for this, which reduces processor performance, of course, because that's the way that you have to deal with speculation.  The reason all of these performance improvements have been put into the microarchitecture is to creep the performance forward.  Linux can incorporate these newly released microcode patches into its boot, and Windows 10 will be getting them.  In fact, has them.



So basically, interesting, and I think important for us to just sort of address for the sake of understanding the territory that we're now operating in.  And it will be nice when we start having high-performance Intel cores that don't have all of these problems, if we can get the best of both worlds, and it's not yet clear we can.  But, for example, the way to resolve this would be to turn off hyperthreading.  Except hyperthreading is an inexpensive means of keeping a core busy, inexpensive inasmuch as it allows two threads to do a very quick context switch, to jump between threads, and where you're only storing a little bit about what each thread's state is.  And so Intel doesn't want to give it up.



And users who were super concerned about this could turn off hyperthreading, and normally that's still an option in the BIOS.  Just I don't know why it's there, but it's kind of handy that it's there now.  Not that anyone's really ever had any problem with it.  So the researchers noted that it's unlikely that AV products would detect or block CacheOut because it would be really difficult to see it happening.  And it's very unlikely that anybody has exploited it.



So it is not in AMD processors.  Apparently there were a couple brands, I think it might have been - I don't remember now.  I think I saw IBM and one other processor sort of was doing the same thing.  They had not looked at those.  So we don't know.  But certainly anyone who's making processors these days is looking at what's happening with all of the incoming fire that Intel is taking and thinking, ooh, let's fix that before one of those annoying academic researchers thinks, hmm, I wonder what about these processors.  And speaking of microcode updates for Windows, which can be loaded at boot time, last Thursday Microsoft did release updated Intel microcode for Windows, pretty much all of them - 1909, 1903, 1809, 1803, 1709, 1703, and 1607.  They also had one they just labeled Windows 10.  And it's like, oh, were they calling it Windows 10 before 1607?  I don't know.



LEO:  All of the above.



STEVE:  Yeah.  So the important part is this came out on Thursday, which was like, not a Tuesday; right?  And it's all versions of Windows.  Or, well, of Windows 10.  So no Windows 7 fixes, significantly, but we didn't expect them any longer.  Except, as we'll be seeing a little bit later in the show, there will be an upcoming change to Windows 7 due to something that they broke a couple weeks ago.



So in the show notes I've got - because it turns out that each of these microcode updates is different, depending upon the edition of Windows you're using.  So if you're at 1903 or 1909, you need KB4497165.  1809 needs KB4494174.  If you have 1803, you need KB4494451.  And I'm going to explain in a minute why I'm going through this.  If you're at 1709, you need KB4494452.  1703, KB4494453.  1607, KB4494175.  And what they described as just Windows 10.  And I went to the knowledge base article, and that's what it just says, Windows 10.  It's like, oh.  That's KB4494454.



So the point is, if you were concerned, or you had a version of Windows 10, or presumably Windows 10 Server - the server versions are similarly affected, and I would argue that only the server versions, which might be running untrusted code, would need to bother with these updates - then Microsoft is not giving them to you.  You need to go to that knowledge base article and manually install the Windows Update from that article.  And, boy, you look at any of those, and they cover everything, Leo.  You ought to just google "KB4494454" and look at the list of processors.  Because now they've added coverage for Denverton, Sandy Bridge, Valley View, and Whiskey Lake on top of, like, everything else like all the way back that is being affected by this.  So it's a mess.  But I want to say I would never consider putting any of those on any of my machines, on any of my Windows 10 machines.



LEO:  Because you're not in a shared environment.



STEVE:  Exactly.  When or if Microsoft should ever decide it's necessary for me to have them, then I presume it'll be part of a monthly rollup, and I'll just get them, and I won't really be able to say no unless I try to fight back.  There's really no reason for an individual to do that.  We all cherish the performance of the machines we have.  Caching and speculative execution are all about performance.  So we don't want to turn that off.



And if you've got something in your personal machine which is trying to steal something, well, first of all, there's lots of easier ways to do it, if it's code running on your machine, than trying to leverage a very subtle timing flaw in L1 cache eviction.  So you've got bigger problems than that, if you've got something in your machine trying to do that.  So it's like, eh, again, if something were ever found to be doing it, then Microsoft would just roll that out to the rest of us.  At this point I don't think it makes any sense.



And looking over those seven different knowledge base articles, each for one - or mostly for one.  In the case of 1903 and 1909 they're sharing one.  What the hell ever happened, Leo, to there being only one Windows 10 from now on?  Wasn't that the big promise?



LEO:  Well, they said the last version of Windows.  I don't know.



STEVE:  Okay.



LEO:  If they said there'd only be one.



STEVE:  That sounded like the one good thing that this new operating system had going for it.  It was like, oh, thank god.  They're finally going to give us Windows 10, and we'll be done.  But as I've been gaining some experience with it, as far as I can see, what we now have is effectively separate and differing versions of Windows that don't change now every six years, the way they used to.  They're changing every six months.



And I was just recently, last week, trying to solve a mystery with Windows 10, and getting it to work on an old version of Server Message Blocks, SMB v1.  And what was really interesting was I encountered exactly this, is that over time, because Microsoft no longer likes SMB v1, but that's the only version that DOS understands, and I was trying to put, for the sake of my work with SpinRite, I wanted to get an MS-DOS or FreeDOS system, either in a VM or an actual physical machine, on my network so that I could easily share files among them.



Well, since DOS only knows about SMB v1, it was necessary to get Windows to speak that.  Windows 7, no problem at all.  I was wanting to run SMB v1.  So I do what, like when I'm faced with a mystery, what we all do now is you figure, okay, if I'm having this problem, lots of other have had it before me, so you google.  And what I discovered was that apparently everybody has a different version of Windows, of Windows 10.  So it's no longer the case that, like, there is a version, and the Internet's knowledge base can be used to solve it.



And so what I realized is what Windows 10 has become is sort of a smear over time.  It's just, you know, it's a moving target.  And so it's a smear.  So anyway, I ended up solving the problem, I think, as a consequence of noting the problem in the GRC newsgroup, and a couple people suggested some registry tweaks which may help.  Anyway, I ended up actually coming up with a development solution for SpinRite that no longer required me to do file sharing, which is probably just as well because, boy, I mean, sooner or later Microsoft is just going to lower the boom on SMB v1, which they argue is not secure any longer.



But I just wanted to note that it is - it's unfortunate that they won't leave it alone.  Now, I mean, for example, when I do the control panel, you used to be able to flip it into a mode where you would see all the little icons, rather than just sort of the summary of things.  In older versions of Windows, it's there.  I'm on the latest one, Windows 10, 1909, the Enterprise build.  And it's gone.



And so, and I'm trying to do it, and it's like, you know?  So I think, okay, how do I show all the little icons?  Because I'd rather have the granular view that I used to have, and I'm sure that Windows used to have.  Windows 10 used to have.  And I go online and say, you know, like ask Google a question, and up come lots of people who have different versions of Windows 10 than I do, and they have it.  But not the 1909 Enterprise version.  Microsoft in their infinite wisdom said, no, we're trying to phase this out, so we'll just take it off of the UI.  It's like, I just - okay.  Fine.



LEO:  I don't know how much I can tell you about this event yet.  Some of our - not you, Steve, because you did the LastPass event.  We're not going to make you do everything all the time.



STEVE:  Going to rotate, that's good.



LEO:  We're going to rotate.  Some of our other TWiT hosts are coming out to St. Louis.  It's going to be a lot of fun.  And Steve, LastPass does want us to do more events, and I hope we can lure you out of your Fortress of Solitude to get somewhere else out in the country because people [crosstalk].



STEVE:  I will begin to have deliverables for SpinRite stuff before long, so that'll buy me a little latitude.



LEO:  You must have been working your butt off.



STEVE:  I'm working.  Well, yeah.  So there's SpinTest is the tool that I was developing where I was able to get some early benchmarks.  That's running again.  I've got my development environment established.  I've been building stuff.  So I'm going to first produce a sort of a little more official benchmark.  I'm just curious to know whether I'm right that in the seven years since we last did this, the throughput of drives has increased as a consequence of the density going up.  So we know that density's gone crazy.  But so should the throughput because drives are still spinning at the same speed.  So what I'm going to do is I'm going to turn the work I've got into a benchmark so that anyone who's interested can run it on their drives and get a sense for how quickly SpinRite will, well, first of all, how fast their drives go.



LEO:  Right.



STEVE:  And then how quickly SpinRite will be able to run across them.



LEO:  I just bought four of the new... 



STEVE:  Crazy drives, yeah.



LEO:  16TB helium drives from Seagate.  16TB.



STEVE:  And as I remember, the reason they're using helium is that it allows the head to fly closer to the surface.



LEO:  Yeah, yeah.



STEVE:  Because it's a lighter gas than just - than air.  So it's a hermetically sealed environment, and wow.



LEO:  I can only imagine how much data loss is happening every second on these things; right?  The ECC must be just going crazy, or whatever they call it on drives, yeah.



STEVE:  Yeah, no, it is ECC, and it is no longer something you only use when a defect is involved.  It's just part of the process because it's like, I think that was a one.  Well, we'll let that get solved when we get to the end and see how everything added up right.



LEO:  Wow.



STEVE:  Okay.  Now, this is just looney tunes.  But I thought it was fun and worth mentioning, just in passing.  Since we've been talking about Windows 10, I thought it was worth mentioning that Richard Stallman's founded in 1985 Free Software Foundation has asked Microsoft, and this is a term I was not familiar with, Leo, to upcycle Windows 7 by releasing it to the public.



LEO:  To hoots and howls of laughter from Redmond.



STEVE:  Exactly.  Uh-huh.  Like that's ever going to happen.  So they created a page and a petition.  FSF, as in Free Software Foundation, fsf.org.



LEO:  I'll sign it.



STEVE:  Yeah, in fact, go check how many are there because they were looking for 7,777.



LEO:  Oh, they've got more than that.  Oh, yeah.



STEVE:  Last night they were nearly - they weren't quite double that, but they were 13,000 and some.



LEO:  13,365 right now.



STEVE:  So, okay.



LEO:  They want to open source Windows 7, is what they're saying.



STEVE:  Yes.  They want to open source Windows 7.  Despite the fact that XP hasn't been, nor has, I mean, the last thing with the - I don't think even the 16-bit OSes were.  MS-DOS finally was because...



LEO:  And actually that benefited you, though; right?  Because you used FreeDOS in your SpinRite 6 distro.



STEVE:  Yeah.  Also though there were, like, leaks of MS-DOS source...



LEO:  Yeah, but you can't use that legally.



STEVE:  No, no.



LEO:  The problem is, and we talked about this on Wednesday with Paul and Mary Jo, the real problem, the thing that stops Microsoft, I'm sure they would love to do this, is there's a lot of proprietary code in there.  It's not just their code.



STEVE:  Leo, Windows 10 is Windows 7.



LEO:  And that's the other problem.



STEVE:  I mean, that's...



LEO:  You'd be giving away the keys to the current version, yeah.



STEVE:  Yes.  It's not any different.  I mean, look at it.  You click about three layers down.



LEO:  There's a little overlap.



STEVE:  And you see the same dialog.  Nothing changed.  It just,  yes, they're trying - it's not like they wrote a whole new operating system.  It's the same.



LEO:  Yeah.



STEVE:  So, yeah.  Anyway, I got a big kick out of this because they said:  "Current signers," that is, of their petition.  "We've reached the goal, but there's still time to show your support."  I think it closes, oh, it closes tomorrow.  They said:  "Sign the petition before February 5th to stay updated on the campaign."  Like this campaign is going anywhere.  "On January 14th..."



LEO:  Here's the latest.  Microsoft still says no.



STEVE:  Yeah.  Here's the latest, Microsoft seems to be ignoring us.



LEO:  Yeah, they're not even saying anything.  What?



STEVE:  So they're telling us like we didn't know:  "Windows 7 reached its official end of life, bringing an end to its updates as well as its 10 years" - and get this - "of poisoning education, invading privacy" - this is the way to win friends, right, and convince Microsoft that they should release it - "invading privacy and threatening user security."  They said:  "The end of Windows 7's lifecycle gives Microsoft the perfect opportunity to undo past wrongs, and to upcycle..."



LEO:  Oh, lord.



STEVE:  Which, again, is a term I've never encountered, "...upcycle it instead."



LEO:  Well, they don't want to say "recycle Windows 7."



STEVE:  "We" - this is Stallman and company - "call on them to release it as free software and give it to the community."  It's like, what?



LEO:  Stallman's gone, by the way.  He was ousted.  But Stallman's spirit lives on, clearly.



STEVE:  Oh, my god.



LEO:  We call on you, Microsoft.



STEVE:  It gets better, "...give it to the community to study and improve."



LEO:  Honestly, they could; right?



STEVE:  Yes.  Well, the problem is, you know, the bad guys could study it also and find things that are obscure that could be leveraged.  Because, again, it's Windows 10.  "As there is already a precedent for releasing some core Windows utilities as free software" - and by the way, I think that was Calculator - "Microsoft has nothing to lose by liberating" - it's liberating - "a version of their operating system that they themselves say has 'reached its end.'"



So here's the best part.  So in this it says, I'm not making this up:  "To the executives at Microsoft."  We have three bullet points.  First one.  I can't even say this.  "We demand that Windows 7 be released as free software."  As you said, Leo, they fell out of their chairs there in Redmond.  "We demand."  That's why I always want to make sure that you're centered over your ball for something like this.



"We demand that Windows 7 be released as free software.  Its life doesn't have to end.  Give it to the community to study, modify, and share."  Uh-huh.  "We urge you to respect the freedom and privacy of your users, not simply strong-arm them into the newest Windows version."  So they must have their own weed that they're, like, smoking.  "We want more proof that you really respect users and user freedom, and aren't just using those concepts as marketing when convenient."



And then they finish, now speaking to those who would sign:  "We need your help to send Microsoft a strong message."  Well, okay, first of all, a strong message would number in the billions of signatures.  I mean, again, not that that would be any more effective, not we didn't quite get double the 7,777 supporters that we were looking for.  "We need your help to send Microsoft a strong message.  We want 7,777 supporters to take a stand with us for freedom."



LEO:  Oh, I agree.  I don't disagree with them.  It would be wonderful.  It's just not going to happen.



STEVE:  Oh, Leo.  I mean, it's not even, I mean, it's not even like maybe it would happen.



LEO:  They've put out some of it.  I mean, they did put out, what, they put the Calculator out in open source.



STEVE:  Yeah.



LEO:  They could do Minesweeper next.  Solidarity



STEVE:  Valuable, valuable piece of intellectual property, yeah.  Oh, my.  Please put out Candy Crush Soda Saga.



LEO:  Oh, I wish they would put that out of its misery, yes.



STEVE:  Oh, god.  Anyway, so they didn't quite double that number.  So anyway, just it was so bizarre, so far out in left field that I just want to share that with our listeners.  But it does give me a segue into an interesting piece of, well, a bemusing piece of news about Windows 7 updates.  It turns out that the "was supposed to be the last" January Patch Tuesday update for Windows 7, broke something.



LEO:  Yeah, something stupid.



STEVE:  Oh, my god, Leo.  I know.  Something that apparently...



LEO:  We don't have to test this.  It's the last one.  Let's just put it out.



STEVE:  But don't you wonder, like, what are they doing that broke it?  Okay.



LEO:  Yeah.  It's all, well, it's spaghetti code, that's why.



STEVE:  It is.  It's a disaster.  So something that Microsoft cannot let lie, believe it or not, it broke the desktop wallpaper stretch functionality, which results in a black-as-night desktop when an image is being stretched to fit the desktop.  And so the initial announcement of this said that the Windows 7 ESU, the Extended Service Updates people who are paying now, would have this fixed.  Turns out apparently that didn't go down very well somewhere.  I don't know, somewhere within Microsoft.



Now, in their announcement of this January 14th, 2020 update, Microsoft acknowledged the error under, and they called it "Known issues in this update."  And they said:  "After installing KB4534310, your desktop wallpaper might display as black when set to stretch."  And then, under workarounds they said:  "To mitigate the issue, you can do one of the following:  Set your custom image to an option other than Stretch, such as Fill, or Fit, or Tile, or Center.  Or choose a custom wallpaper that matches the resolution of your desktop."



Now, of course you can also, if you just happen to love the wallpaper you have, you could use a third-party, I mean, like any image-stretching tool, to stretch it to size, and then just use the, what, Fill or Fit or Center, anything but Stretch.



LEO:  Yeah.



STEVE:  So then they said:  "We are working on a resolution and will provide an update in an upcoming release" - and here's the change - "which will be released to all customers running Windows 7 and Windows Server 2008 R2 SP1," because you really do want your Windows Server to have wallpaper that is working properly.



LEO:  This shows what Windows users think is a showstopper.



STEVE:  Oh, my god.



LEO:  I can't make my wallpaper stretch.



STEVE:  You broke my wallpaper with the last update ever.  So what are you going to do about it?  So anyway, as I said, they initially said ESU only.  Now they're apparently saying "all."  So if they're going to be doing a rollout, what would that be?  Tomorrow, or, no, next Tuesday, the 11th, will be the second Tuesday of the month.  Maybe they'll sneak in the month's security fixes for free because why not?  If we're going to have to - actually, that probably would not require a reboot.  One hopes that fixing the black desktop wallpaper stretch function would not require you to reboot your machine.  So who knows.



In more serious and interesting news, we have the remote code execution exploit for Windows RDP Gateway has now been demoed.  We recently discussed the discovery of what was at the time only a denial of service attack on Microsoft's remote desktop protocol gateway service.  And once again we see that when a sufficiently skilled hacker carefully examines nearly any sort of software flaw, it's often possible for that researcher to discover some way to manipulate the vulnerable machine into executing code of their choosing, whether it's code they provide in a buffer, or whether it's code that's already there, and they just cause the machine to jump around and execute little bits at the tails of various existing functions in so-called "return oriented programming."



InfoGuard AG's penetration tester, Luca Marcelli, demonstrated a working remote code execution exploit.  The exploit targets what we were talking about recently, this Remote Desktop Gateway, on devices running Windows Server versions from 2012 through 2019, so all of them except the one before 2012, which was the one based on Windows 7, so that's 2008 R2.  And we now have a name.  Whereas the previous Remote Desktop Protocol, the RDP protocol, the sort of the raw protocol, that was the now-famous BlueKeep.  This one is BlueGate, of course, because it is the gateway service.  Marcelli said that a blog post detailing how to achieve remote code execution with BlueGate will be forthcoming in the next few days, but that he wanted to be responsible and "wait a bit until people had enough time to patch before releasing this to the public."



Okay.  So that's probably reasonable because it was fixed in January's Patch Tuesday, which is only a few weeks hence.  Or, wait, no, in the other direction.  A few weeks ago.  So, you know, we can hope that these systems are going to get fixed.  The problem is that at last count there are more than, I'm sorry, no, not quite 20,000 currently unpatched and vulnerable Remote Desktop Gateways accepting connections worldwide with 6,816 of them in the U.S. alone.  And since the use of this vulnerable Remote Desktop Gateway is only higher end server platforms, it's very likely that those are valuable networks.  So they're from 2012 on.



And a small company is not going to need the gateway, since remember that that's a frontend behind which Remote Desktop Protocol servers stand.  So it's probably larger installations, larger corporations, or government facilities.  And if they are not applying patches in a timely fashion, and at this point several weeks downstream from it being fixed, they're still vulnerable.  There's still a significant count of vulnerable systems.  Now and very shortly we're going to have a proof-of-concept disclosure for its exploitation.  So I have a feeling it won't be long before we actually see these exploits moved, weaponized, and occurring.



This is a happy story, finally.  Google is really making some payouts for their bounties.  Compared to 2018, Google doubled the reward payouts for bug bounties in 2019.  I have a graph in the show notes that really demonstrates what's been going on from 2015, '16, '17, '18, and '19.  And props to Google.  I think one of the things we're seeing is that, and as we know, we've been discussing bounties as a viable career for sufficiently talented hackers.  Or part-time, see if you can find one in the evening and sharpen your skills in the process.  Google gave out $3.4 million in bounties during 2018, and 6.5 million last year.



So last year the reason for this escalation is they launched their Developer Data Protection Reward Program, which was aimed at uncovering data abuse issues in Android apps, OAuth projects, and Chrome extensions.  They're looking for any apps that violate Google Play, the Google API, and Google Chrome Web Store extension privacy policies.  And privacy violators are not going to win a huge reward.  On the other hand, they're easy to find.  Depending on the impact of the bug found, researchers may be rewarded up to $50,000 per report.



Also last year, Google tripled its top reward payouts for security flaws in Chrome from 5,000 to $15,000 and doubled the maximum reward amount for high-quality reports from 15,000 to 30,000.  The Android security rewards program added additional exploit categories and raised the top prize to $1 million for a full-chain remote code execution exploit with persistence that compromises the Titan M secure element on Pixel devices.



And recall when we talked about this, like we talked about this happening last May, Google had at that time recalled the Bluetooth versions of the chip after discovering a vulnerability that would allow attackers within Bluetooth range to take control of the device.  So there's also the Google Play Security Reward Program, which paid out $650,000 total in rewards during this - just the second half of 2019, after its scope was expanded to any app, including third-party apps that had more than 100 million installs.  So it has to be a significant app.  But even if it's not theirs, they will pay.



And also recall that there is a 50% bonus bump, which we previously discussed.  Their security team added in their posting about this last Tuesday, they said, if you achieve the top reward Titan Pixel M exploit on specific developer preview versions of Android, they will add, they said, a 50% bonus.  So that raises the top prize to $1.5 million.  The discovery of major problems in pre-release Android, of course, is worth much more to Google because they would really love not to have that exploit get out into the field.  And they're willing to pay for it.  Of course Google's got plenty of cash.  So yeah.



And this is its 10th bug bounty anniversary.  They began offering bounties back in 2010.  During this past 10 years they've paid a total, a grand total of $21 million in rewards to date.  And so it's significant that 6.5 million of that 21, so nearly a third, was just in this most recent year.  As that graph shows that you've put up a couple times, Leo, it's really accelerated recently.



LEO:  Somebody in the chatroom said, well, this just means Google's got buggy software.  Let's flatten that, shall we?



STEVE:  No.  Yes.  Well, it means that people are looking.  I mean, the lesson we've learned is all sufficiently complex software is buggy.  I mean, it is so difficult to produce absolutely bug-free software that it isn't cost effective.  The space shuttle computer software had the benefit of being simple, but it was incredibly complex because it had to be accurate.  So an incredible amount of money was put into making it bug-free.  I believe, and we've talked about this before, that eventually we'll get to, and we have been talking about the changes we're beginning to see, where we have so much excess processing power now that we no longer need to be coding in C, where a string is a pointer to a region of memory.  And, I mean, that's all it is.  You are given a pointer to a region of memory in C.  That's about as dangerous as anything could be.



And so as a consequence we see disasters all over the place because somebody figures out how to cast that as a signed pointer, and now you can put a negative value in it and poke around in memory below the string that you are allocated, and stuff like that.  So we're going to be moving to languages which enforce safety where C just absolutely doesn't even try.  Now, programmers like having that power.  Unfortunately, the lesson we're learning is programmers can't be trusted with that power.  Not that they mean ill, but they just make mistakes.  And so in the future languages will catch the mistakes, or the language simply won't give you a pointer that you can do anything with.  You'll get a handle to a string, and the language will make sure that there's, like, no way possible for you to abuse that handle.



So anyway, I think what we're seeing is we're seeing that Google is incentivizing people to inspect their code and other people's code and are being willing to pay because we've seen also that everyone talks about, oh, the solution is to open source it, then everybody can look at it.  Except, yeah, everybody's busy.  And so open source code sits there, open but uninspected.  And as we've seen, you need to raise money in order to fund an audit of an open source program in order to get people to look at it and go, "Oh, look what I found over here.  This is really bad."  And then people go, "Oh, you're right," and then it gets fixed.  But the fact that it's open doesn't mean that it's going to get fixed.  It's got to be inspected in order for it to get fixed.



And speaking of inspectors, Leo, we have the return of Roskomnadzor.  Roskomnadzor is, as we know, Russia's telecommunications watchdog.  They announced last Friday that it has instituted administrative proceedings, which sounds kind of ominous, against Facebook and Twitter because of each company's continued refusal to move the data of their Russian users onto servers located inside that country's borders.  What could possibly go wrong?



Roskomnadzor said:  "These companies did not provide information on meeting the requirements for localizing the databases of Russian users of the corresponding social networks on servers located in the Russian Federation, as provided for in Part 5 of Article 18 of the Law on Personal Data No. 152-03."  Bureaucracy much?  "Administrative proceedings," they continued, "have therefore been instituted on the grounds of an administrative offense in accordance with Part 8 of Article 13.11 of Administrative Code of the Russian Federation, which provides for an administrative fine in the amount of" - uh-oh, wait for it - "1 million to 6 million rubles."  Which is somewhere between $16,000 and $94,000 U.S.



LEO:  Oh, it sounded like more.  I'm disappointed.



STEVE:  So Leo, do you think maybe Facebook and Twitter can deal with a fine not to exceed $94,000?



LEO:  I guess so.  Geez Louise.



STEVE:  I know.



LEO:  Boy, they're really coming down hard on them.



STEVE:  Whoa, boy, they lowered the sickle on Facebook and Twitter.  As we've covered before, Facebook was previously threatened with a ban in September 2017 for the same reason.  Twitter agreed to the demands of Russian officials at the time and proceeded to inform the Roskomnadzor that it was planning to move Russian users' data by mid-2018.  According to the Moscow Times, Roskomnadzor said Friday that a complaint will also be filed - oh, Leo, they're going to complain - will also be filed in Russian courts next week.  And a new law, just signed last month by Vladimir Putin, imposes higher fines of up to 18 million rubles, Leo, for repeat offenders.  That's $280,000.



LEO:  Goodness.  Terrible.



STEVE:  Meanwhile, last Wednesday, the ProtonMail Twitter account tweeted that "The Russian government has blocked ProtonMail and ProtonVPN within Russia.  We are reaching out to the appropriate authorities to get the block lifted as soon as possible."  So this ban was prompted by Proton's refusal to register their VPN services with Russian authorities, which was asked of all VPN providers operating in Russia.  ProtonMail and ProtonVPN users are advised by the company to access the two services through Tor.



So, Leo, what's going on?  So does Russia really want social media companies operating inside Russia or not?  Do they really care where their citizens' data is stored?  And if yes to either, then why are years passing with fines so low relative to the sizes of those companies?  I just, you know, doesn't make any sense.



LEO:  The GDPR fines are percentages of revenue.



STEVE:  Yes.



LEO:  And significant percentages.



STEVE:  That will get somebody's attention.  So this is just nuts.  I mean, like what are they actually doing?  Are they just wanting to look like they're caring?  Certainly they don't want Facebook and Twitter to pull out of Russia.



LEO:  They look like they care, yeah.



STEVE:  Yeah.



LEO:  You nailed it, yeah.



STEVE:  18 million rubles, ooh.



LEO:  Yikes.



STEVE:  But those rubles are itty-bitty rubles.



LEO:  I bet, though, that until they did the currency conversion, there were some people who blanched a little bit at Facebook and Twitter, going, "Oh, that's - oh."



STEVE:  Yeah.  Whoa, they're going to shape up now, baby.  And apparently you can just, you know, you get fined, and you ignore it until you get blocked.



LEO:  Right.



STEVE:  And again, are they going to block Facebook and Twitter?  Seems unlikely.



LEO:  Right, yeah.



STEVE:  But Facebook did get fined, but not by Russia.  Facebook just lost an interesting class action lawsuit it had been fighting for the past five years.  And it will pay $550 million in settlement of that suit.  So that's, okay, still Facebook.  But $550 million, more than half a billion dollars.  So that's a significant chunk of money.  The lawsuit was brought against Facebook for scanning their users' faces in photos and offering tagging suggestions.  The plaintiffs claimed that the platform, Facebook, violated the strictest biometric privacy law in the land, Illinois's Biometric Information Privacy Act (BIPA) as a consequence of its tag suggestions tool.



Facebook started using the tool in 2015 to automatically recognize people's faces in photos and suggest to their friends that they tag them.  And it does this without users' permission and without telling them how long it will hang onto their biometrics.  The lawsuit contends that Facebook squirreled away face prints in what Facebook has themselves claimed is the largest privately held database of facial recognition data in the world.  It's like, yes, let's shout that from the mountaintops.  What could possibly go wrong?



Last September Facebook said it was ending tag suggestions in favor of the multipurpose face recognition setting, which it made available to all users, along with an opt-out option.  The New York Times in its reporting of this lawsuit outcome, this $550 billion, referred to the 550 - I'm sorry, 550 million hit as "a rounding error" for Facebook.  Facebook reported that its revenue rose 25% to 21 billion in the fourth quarter, that is just the fourth quarter of last year, while profit increased by 7% to $7.3 billion.  So in the last quarter of last year, it had revenue of $21 billion and profit of one third of that, 7.3 billion.



Yeah, so Facebook also is - they are the new Intel.  They are printing money.  Although this is a lot of money, that is, the fine, it turns out it could have been worse because Illinois's BIPA requires companies to get written permission before collecting a person's biometrics, whether they are fingerprints, facial scans, or other identifying biological characteristics.  I mean, it is broad.  It also gives Illinois residents the right to sue companies for up to $5,000 per violation, which could get pretty expensive.



So, not surprisingly, Facebook fought this lawsuit tooth and nail.  In 2016 it tried and failed to wriggle its way out by saying that its user agreement stipulates that California law would govern any disputes within the company.  On the other hand, I don't know how long they're going to be safe in California.  And besides, Facebook said in its motion that BIPA didn't apply to Facebook's facial tagging suggestions for photos.  The judge's response was nope on all counts.  Going by Illinois law was just fine, the judge said, and it was always clear that BIPA would cover faceprints because it governs the use of all biometrics.



So, you know, we're sort of in a position where we're trying to figure out exactly who owns our biometric data these days.  23andMe just laid off 100 employees, which was 14% of its workforce, as consumer demand for its DNA testing kits has dropped significantly.  The company said, in announcing this, that a variety of factors, including privacy concerns, could have contributed to the slowing market.  And this of course followed a bunch of news coverage indicating that the company was responding to law enforcement queries of its DNA database.



Since I was curious, I looked up 23andMe's own disclosure titled "How 23andMe responds to law enforcement requests for consumer information."  I have the link in the show notes for anyone who's interested.  They said in that page, in their own page on their site:  "We work very hard to protect your information from unauthorized access by law enforcement.  However, under certain circumstances" - now, our listeners know how to hear this.  It's very much like Intel saying, "It is possible."  Uh-huh.



So 23andMe says:  "Under certain circumstances your information may be subject to disclosure pursuant to a judicial or other government subpoena, warrant, or order, or in coordination with regulatory authorities.  If such a situation arises, we have to comply with valid governmental requests, and we will notify the affected individual(s) unless the legal request prevents us from doing so."  In other words, they're saying we must comply with a lawful court order and its likely accompanying gag order.  So, yeah.



And apropos of this, a New York-based facial recognition startup named Clearview AI has amassed a massive database containing more than three billion images scraped from employment sites, news sites, educational sites, and social networks including Facebook, YouTube, Twitter, Instagram, and Venmo.  And since this data collection was done without the consent of the people whose images have been collected, that company is now also being sued in what will likely become a class-action lawsuit.  Since that suit is citing the BIPA Illinois regulations, the complaint against Clearview AI, this company based in New York, was filed in Illinois.



The New York Times published an expos about how Clearview has been quietly selling access to faceprints and facial recognition software to law enforcement agencies across the U.S., claiming that it can identify a person based on a single photo, revealing their real name and far more.  The New York Times said:  "The tool could identify activists at a protest or an attractive stranger on the subway, revealing not just their names but where they lived, what they did, and whom they knew."



Clearview told the Times that more than 600 law enforcement agencies have started using Clearview in the last year, and it's sold its technology to a handful of companies for security purposes.  Clearview declined to provide a list of its customers.  Eric Goldman, the co-director of the High Tech Law Institute at Santa Clara University, told the newspaper that "The weaponization possibilities of such a tool are endless."  He said:  "Imagine a rogue law enforcement officer who wants to stalk potential romantic partners, or a foreign government using this to dig up secrets about people to blackmail them or throw them in jail."



The New York Times headline about Clearview AI suggested that the company "might end privacy as we know it."  From their report they said:  "Even if Clearview doesn't make its app publicly available, a copycat company might, now that the taboo is broken.  Searching someone by face could become as easy as googling a name.  Strangers would be able to listen in on sensitive conversations, take photos of the participants, and know personal secrets.  Someone walking down the street could be immediately identifiable, and his or her home address would be only a few clicks away.  It would herald the end of public anonymity."



Welcome, Leo, to our sci-fi future.  And it is, I mean, it will change our world.  I have nothing to hide, and when I'm out in public I'm not in disguise.  But since I'm not a high-visibility celebrity, thank goodness, there's a sort of assumption that people who I don't know, don't know me, just as I don't know them.  But it would change if there was an app like "Super Shazam" or "Photo Shazam" where we could point our smartphone camera at anyone and instantly obtain their name, address, full biographical background, and links to their various social media accounts.  It would change the world.



LEO:  Yeah.



STEVE:  And I have to say it seems like it's going to happen.



LEO:  Oh, I think it happened.



STEVE:  Well, yes.



LEO:  That's the point.



STEVE:  It did happen for private purchase.  I'm sure it costs a pretty penny.  But it seems like it's just a matter of time before we end up with an app on our phone that can identify anybody that it sees unless it's regulated out of existence.



LEO:  You may remember Google has had that capability for some time and opted not to put it in the phone because they realized what a problem it would be.  I mean, I think that technology is out there.  The sad thing is, as individuals, we don't have it.  But law enforcement has it.  Everybody else has it.  Businesses probably have it.



STEVE:  Right, right.



LEO:  Our government has it.  So we're the only ones who don't have it at this point.



STEVE:  Yeah, there was some - I skipped over a little bit here.  It says the lawsuit claims that Clearview isn't just selling this technology to law enforcement, it's also allegedly sold its database to private entities...



LEO:  Of course it has.



STEVE:  ...including banks and retail loss prevention specialists.  So that says that cameras in retail locations are profiling everyone who walks in the door.



LEO:  You're in public, dude.



STEVE:  Yeah.



LEO:  I mean, the pharmacist may look at everybody who walks in the door and with his limited abilities try to identify everybody.  Delta Air Lines has been testing face recognition identification at gates.  And something like 99%, even though they're given an option to opt out, of customers choose it because it's faster.  It's a little bit faster.  People, you are in public.



STEVE:  Yeah.  And it's true you have no expectation of privacy because, again, you're in public.  You're able to be seen.  But there has always been some expectation of relative anonymity.



LEO:  That's before computers.



STEVE:  And that's what this changes.



LEO:  Yeah.  I mean, every bank has had cameras forever, and every grocery store has had cameras forever.  The difference is now they're automated, and they know who you are when you lift that package of Tang up off the shelf.  I mean, that's the whole purpose of the Amazon Go Store.  You don't pay because it just watches you pick stuff up and go out the door.  And then they do this pro forma, oh, scan your Amazon - just in case we aren't sure who you are.  But they don't need to do that, obviously.  I don't know.  I mean, I think it's happened.  I think that the horse has left the barn.



STEVE:  Yeah, well, so all of our listeners, get ready for this brave new world.



LEO:  Yeah.



STEVE:  You won't need to, like you'll be able to walk into Starbucks.



LEO:  Oh, hi, Leo.



STEVE:  And they'll - yeah.



LEO:  Good to see you.  We have your venti latte ready.  All right.  On we go.



STEVE:  So we all remember that Avast and AVG were found to be massively tracking their users behind their backs.  Their cover story for that tracking was that the users' browsing was being sent back to the mothership so that the URLs could be checked for safety, even though all other browser-based safety checkers offer exactly the same service locally and without sending the user's URL clickstream off to some central database somewhere.



What happened was that the tracking turned out to be quite extensive, essentially sending back all of the user's actions, such as changing tabs, scrolling the page to provide detailed, very detailed monitoring of their users.  The data was supposedly anonymized, but the researcher who discovered it, who was the well-known author of Adblock Plus, felt that deanonymizing the data wouldn't be difficult, and that turned out to have been shown later.



But that wasn't the point or the concern.  What was creepy was Avast had also purchased a data analytics firm that was apparently found to be reselling this data to whomever wanted to troll through it.  Mozilla reacted to the news by immediately ousting Avast and AVG's browser extensions from their browser repository, their add-on repository.



We subsequently heard that Avast and AVG had dramatically reduced the amount of data being collected.  And I wondered aloud on this podcast how their previous business model could be sustained if so much less data was being collected.  Well, the other shoe has dropped.  It turns out the answer is it cannot be.  Avast is winding down, they said, their subsidiary named Jumpshot, following this investigation into the sale of their users' data to third parties that may pose a risk to their users' privacy.  Last Thursday Avast said they will no longer have access to user information harvested from users of Avast products and services, which will be fully terminated.  And I was curious.  Go to Jumpshot.com, Leo, www.jumpshot.com.  And we will see what winding down an organization looks like.



LEO:  Oh, my god.  That's - geez, Louise.  Okay.  At least they still left the server up.  I didn't get a 404.



STEVE:  Uh-huh.



LEO:  Wow.



STEVE:  Leo is just seeing what I saw last night.  Jumpshot has ceased operations, period.  Thank you.



LEO:  I hope they're sincere about this.  They sure got a lot of heat for it.



STEVE:  Well, they did.  A joint investigation conducted by Motherboard and PCMag, which was just published, revealed that information scraped by Avast from their users was handed over to Jumpshot and linked to individuals through a unique ID in an effort to anonymize, yet track them.  It turns out it's possible to pick apart data strings and deanonymize users to reveal their identify, tracing their online footprint, browsing habits, and their purchases.  In a blog post, Avast's CEO said that the recent news about Jumpshot "has hurt the feelings of many of you" - he's writing to us collectively.



LEO:  Hurt, you know, hurt my feelings.  Really?



STEVE:  It hurt, yeah.



LEO:  Little more than that.



STEVE:  Uh-huh.



LEO:  It's not about feelings here.



STEVE:  "And raised a number of questions."  As the chief executive, he feels "personally responsible."  Well, he is.



LEO:  Uh, yeah.



STEVE:  For the turmoil.  Yeah.  And apparently, reportedly, Jumpshot was bragging they had access to information from over 100 million devices.  And I guess Avast and AVG were very popular free AV offerings.  And as we said, is there any such thing as a free lunch?  So, but speaking of third-party antivirus, to no one's surprise, every single antivirus vendor has formally announced that they will continue offering and fully supporting their existing AV solutions for Windows 7 for at least the next two years through 2022.



Now, I saw a tweet which suggested that that was only for extended service users, but I don't understand - or ESU, Extended Security Updates.  I doubt that that's the case.  So ZDNet had a very clear reporting that stated that they're going to be continuing to offer Windows 7 AV support.  And it represents a huge marketing opportunity because I, for example, I'm not sure what I'm going to do.  I love my Windows 7 setup that I've got.  It's mature.  It's not that old because it wasn't - it was only last year that XP died on me, and I thought, okay, well, fine.  I'll go to 7.  I'm not going to 10.  I have a Windows 10 setup at my other location which I'm using, and I like it also.  But that's the one where I realized that there wasn't a Windows 10 any longer, there were 10 of them.  So, yeah.



Anyway, for what it's worth, all the vendors will be continuing to offer AV.  And I wanted to ask you, Leo, do we have a recommendation for like if you can't use Windows Defender anymore?



LEO:  No.  The recommendation is don't use AV.



STEVE:  Right.



LEO:  Right?  I just - I find it hard to find a compelling reason why anybody should be - on Windows you've got AV.  Defender is as good as anything out there.



STEVE:  Yes.  And I have to say the only time mine has ever made any noise is when it has found a folder of known viruses.



LEO:  Well, that's good.



STEVE:  Yeah.  And it's like, oh, okay, good.



LEO:  It works.



STEVE:  Like it's awake, yeah.



LEO:  I think we focus on behavior, keeping your system up to date, your behaviors.  And both macOS and Windows have pretty good security features in place to keep the most egregious stuff out.



STEVE:  Well, and our browsers are going to stay supported, and our browsers are doing a very good job.  Our browsers are the way this stuff gets into our system these days.



LEO:  Right.  Well, that and email.  And in fact Gmail is really good for that.  They filter out almost everything.  We use a different - we use both Gmail and another system.  So I shouldn't say we, I mean, we use lots of perimeter protection.  But none of the machines we use have antiviruses on them because, well, we know.  There's a lot of reasons.  Not just privacy.  They open up holes.  They operate at ring 0.



STEVE:  Oh, my god, yes.



LEO:  So they open up holes.



STEVE:  They cause Windows updates to fail when Microsoft changes a hook that the AV had to hook into.  It is a privacy concern because it needs to inspect into your encrypted traffic.  And so that's not a supported function.



LEO:  I also tell people the other problem with antivirus is no antivirus is perfect, of course.  In fact, on average, at best 50 to 60%.  And it doesn't catch anything new, which is a lot.  So it gives you a false sense of confidence.  So I think people, you know, there are a lot of people out there, I've got an antivirus, I'm fine.  I don't have to do anything else.  And that's really bad.  You want to be kind of cautious.



STEVE:  That's the best thing.



LEO:  Yeah.  And people always disagree with me.  There's a guy in the chatroom now.  No, do not use - I say this on the radio to everybody.  Do not install an antivirus.  And really don't - you don't need one on a mobile device, really.  That's even more the case.



STEVE:  Right, because they're so well...



LEO:  They're so locked down.



STEVE:  They are so well locked down.



LEO:  Do you disagree?



STEVE:  No, I don't.  I mean, my Windows Defender will now have stopped being updated, and I don't care.  And whatever I had on XP, the previous iteration, it stopped being updated.  I don't care.  I just, again, I'm just not doing anything.  Again, it is a function of who you are.  If you're living in the dark underbelly of the web, then I would do my, I mean, then I wouldn't trust an AV anyway.



LEO:  Well, that's the point.



STEVE:  Because of all the reasons you just said.



LEO:  Yes, yes.



STEVE:  There I would do my work in a virtual machine.



LEO:  A sandbox, yeah, yeah.



STEVE:  Yes, in a really strong sandbox, and be very careful with the stuff I download.



LEO:  I'm completely comfortable saying on the radio, "Do not use an antivirus."  I say it again and again.  It's the wrong thing to do.



STEVE:  I think you're right.



LEO:  They're crap, frankly.



STEVE:  Yeah, they are.  An update on the - well, and they're intrusive.  I mean, how many times have we been talking about the problems created by AV recently, hooking into the kernel and causing problems.



An update on the WireGuard VPN in the Linux kernel.  The lean-coded, fast, modern, and secure WireGuard VPN protocol has made it into the Linux kernel, as Linus Torvalds merged it into his source tree for v5.6.  That's the next Linux kernel expected to be formally released in a couple of months.  And the list of kernel changes...



LEO:  I think this is fascinating.  I can't wait to see this.



STEVE:  Yeah.  In the list of kernel changes, the WireGuard VPN was the first thing on the list.  So just to remind our listeners, the WireGuard protocol and its implementation is a project from security researcher and kernel developer Jason Donenfeld.  He created it as an alternative to IPSec and OpenVPN.  In its current form, WireGuard has about 4,000 lines of code, versus the - get ready - 100,000 lines of code that OpenVPN has been dragging forward.  And WireGuard doesn't require OpenSSL, which is another blob of huge ancient code.  Four thousand lines of code versus 100,000 lines of code.



Compared to current OpenVPN's kitchen sink, "something for everyone" options, WireGuard relies upon a small set of carefully chosen modern cryptographic primitives that are stronger, perform better, and have been highly scrutinized recently by the cryptographic community.  It uses ChaCha20 for symmetric encryption, authenticated with Poly1305, using RFC 7539's AEAD construction - that's the [Authenticated Encryption with Associated Data]; Curve25519, which is what SQRL uses, for elliptic curve Diffie-Hellman key agreement; BLAKE2 for hashing and keyed hashing; SipHash24 for hash table keys; and HKDF for key derivation.  So all latest state of the art.



WireGuard provides perfect forward secrecy, protection against denial of service, brute force attack protection, key impersonation protection, replay attack protection, as well as support for an additional layer of symmetric key cryptography to offer post-quantum crypto resistance.  It's in there now.  This lean selection of crypto primitives deliberately dropped all of the unnecessary choices for encryption, key encryption, and hashing algorithms.  And that increases interoperability since there's less to mismatch, while also increasing the risks from the support of obsolete crypto.  That is, OpenVPN and OpenSSL are dragging all of this old stuff forward because, well, maybe you'll want to hook up to a server that offers that.



The good news is WireGuard never had it, which is the benefit of starting over from scratch.  It's also faster because it lives in the kernel space, meaning they're not having to do ring transitions constantly.  And because it's so small, it's much easier to audit for security vulnerabilities.  It's simple to configure and deploy.  There is a 20-page whitepaper on the protocol.  I've got a link to it in the show notes.  I have not yet made time to study it.  But as I mentioned at the beginning of the year, when we talked about this, it's on my list of things to get to.



And it's not just for Linux.  It's also fully cross-platform, with implementations for Windows, macOS, BSD Unix, iOS, and Android.  There are some VPN service providers for it.  Mullvad, AzireVPN, IVPN, VPN.ac and TorGuard are currently supporting the WireGuard protocol with servers.  And allow me to reiterate that while I was in Gothenburg, Sweden for the SQRL OWASP presentation there, I happened to meet and break bread with the founder of Mullvad VPN.



And our listeners will remember how surprised I was when I signed up for Sync.com, and the "Send me an email for password recovery" checkbox was unchecked by default.  I thought, wow, I couldn't believe that they were, like, choosing the secure option.  Similarly, that's the way I felt over dinner as I was chatting with the guys from Mullvad, one of them being the founder and owner as they described their company's philosophy and shared anecdotes about the unforeseen consequences of deliberately wanting to know nothing about their customers, even their IP addresses, which they never log.  Anyway, I have a feeling - I've not made the switch yet myself.  I'm using SSH a lot, and OpenVPN hardly anymore.  But when it's time next to fire up a VPN, I think I'll be looking at WireGuard.



LEO:  So it's both client and server?



STEVE:  Yes, yes.  It's considered a secure encrypted tunnel.  So the server is the end of the tunnel that listens, and the client is the end of the tunnel that initiates the connection.  But otherwise it's just a tunnel, so you're able to tunnel - it's a Layer 3 tunnel, so it operates over UDP.  Very cool.



And we conclude with this week's Best Hack of the New Decade.  Oh, my god.  So the question we pose, why would an ingenious German performance artist place 99 active cell phones into a little red wagon and walk them around the streets of Berlin?  Everybody, let's think about that.  Why would an ingenious German performance artist place 99 cell phones, operating cell phones, into a little red wagon and patiently walk them around the streets of Berlin?



The answer?  Because it brilliantly confused and spoofed Google Maps into believing that there must be a massive traffic congestion, since so many "cars" were all moving so slowly through a region of the city.  There's a YouTube in the show notes, and I snapped a picture of it from his website, showing him all alone.  And it's not clear what time of day it is.  But, I mean, the sun's up.  There's a big shadow being cast by him, and it's not the moon.  He is alone on the street because Google Maps shows an absolute red in both directions for a good distance.



And so maybe everybody thought, oh, shoot, I can't go the way I normally do.  Apparently this bridge is, like, gone.  So I'm going to have to go somewhere else and drive around.  So he's walking these streets, during the day, and there's no other cars.  There's no traffic.  Because apparently everyone's using Google Maps now, and it's like, oh, shoot, got to find some other way to go.  Incredible.  Anyway, I just thought that was such an amazing hack.  And the video shows his little red wagon with this, like, just filled with 99 cell phones which he's all by himself just walking through the streets of Berlin, and Google Maps is basically not functional.



LEO:  They each had their own SIM, so they were unique.  And he tried stopping, but he said Google Maps wasn't affected unless I move.  You have to have some movement.



STEVE:  That makes sense, that they would - well, because if you're in a restaurant or stopped on the side of the road, no movement doesn't indicate, like, there's still traffic.



LEO:  Right, you're just stopped, yeah.



STEVE:  Yeah.



LEO:  He also said - and, see, I think we're getting a little more detail in the video.  He had to bring the cart up and down the same street many times.  He says also when another vehicle drove past using Maps, Google's system said, oh, there's no traffic jam, and set the street back to green.



STEVE:  Oh, nice.



LEO:  So Google was kind of actively doing this.



STEVE:  Doing everything it could to filter out any extraneous reporting.



LEO:  9to5Google talked to a spokesperson from Google and got this statement:  "Whether via car or cart or camel, we love seeing creative uses of Google Maps, as it helps us make Maps work better over time.  Traffic data and Google Maps is refreshed continuously, thanks to information from a variety of sources, including aggregated, anonymized data from people who have location services turned on."  By the way, he got the idea for this after seeing the Hong Kong protests and seeing Google Maps affected by that, even though probably most of those protesters weren't running Maps at the time.



Google goes on to say:  "And contributions from Google Maps community.  We've launched" - here we go - "we've launched the ability to distinguish between cars and motorcycles in several countries including India, Indonesia, and Egypt."  That's because motorcycles traffic split and cut through and drive around, so they go a lot faster.



STEVE:  Yeah.  And so that was generating false non-traffic reports.



LEO:  Yes.  They say, they go on to say:  "Even though we  haven't quite tracked traveling by wagon, we appreciate seeing creative uses of Google Maps like this."  So they took it in the spirit in which it was intended, instead of saying how dare you.



STEVE:  Very cool.



LEO:  It's interesting.  I mean, you learn a little bit about what Google Maps is doing.  And yeah, somebody said, well, how do they know it's not 100 people?  I don't know.  I don't know.  There's a lot of heuristics going on in all of Google's stuff.  There's gate and so forth.  They're looking at a lot of things.



STEVE:  And we know that heuristics false positive and false negative.  I mean, heuristics are like rules of thumb by nature.  So it's doing the best job it can.  I mean, and really, when you think about it, it's entirely inferential.



LEO:  Of course, yeah.



STEVE:  It's inferring something about the real world from the collection of data that it's getting.



LEO:  A subset of the real world.



STEVE:  And it's not always right.



LEO:  Yeah.  Although I think it's very telling that as soon as a car goes by, it goes, oh, that's not a traffic jam.  So no doubt as soon as this guy stopped going up and down with his little red wagon, those streets went back to green.



STEVE:  Blink.



LEO:  It's fun, though, isn't it.  And it's a great video to watch him pulling his little wagon.  Steve, always a pleasure.  Thank you so much for doing this show.  I look forward to it all week long.  I know our listeners do, as well.



Security Now! reaches the air every Tuesday.  We actually let you watch live, if you'd like to, just as long as you understand that the time is a little soft.  We try to get in here around 1:30 p.m. Pacific on Tuesdays, 4:30 Eastern, 21:30 UTC.  But "try" is the operative word here.  We're usually a little bit later than that because it's the third show of the day.  So, but, you know, watch all the shows.  Why not?  The live audio and video streams are always available, 24/7, at TWiT.tv/live.  Most of the time it's recorded content, but during a live broadcast you'll see the live show.  You could also get on-demand versions of the show.



Steve's got his own copies, 16Kb audio.  It's the only place you can get that.  It sounds like Thomas Edison on his Victrola.  "Mary had a little lamb."  There's also a 64Kb audio, sounds normal.  And there are - actually, it's the only place you can the transcripts which Steve commissions.  And I think that's a really nice service.  Thank you, Steve, for doing that, because for a lot of people reading along is the best way to participate.



If you're watching live, it's nice because you can be in the chartroom, IRC.  You hear me talk about them all the time, irc.twit.tv.  But we also have forums; Steve has forums.  Oh, I didn't mention where you can get all these things Steve has, at his website, GRC.com.  GRC, the Gibson Research Corporation, as we were talking about at the beginning of the show, GRC.com, where he is hard at work on SpinRite 6.1.  Lots of other free stuff there, including ShieldsUP! and, I mean, just go and check it out.  It's a wonderful site, full of goodness.



Our show is available for download on our site, too, TWiT.tv/sn for Security Now!.  There's a YouTube channel.  I think it's, I don't know, youtube.com/securitynow or securitynowshow, something like that.  Actually, just go to YouTube.com/twit, and on the right side all the shows are listed.  Each show has its own YouTube channel.  Let's see.  What else?  We have a forum now at TWiT, as well, twit.community, and a Mastodon instance, which is much like Twitter, at twit.social.  You're more than welcome to join us in there.  Many of our hosts - I'm in there all the time - participate.  Thank you, Steve, and I'll see you next week.



STEVE:  My pleasure.  And for the second Tuesday of February, we will see what Microsoft has in store for Windows 7 users.



LEO:  Next week.



STEVE:  Yeah.



LEO:  We will also be coming on a little late next week because of the Samsung announcement.  We might catch up by then because Samsung usually only does an hour.  But Samsung's going to be talking about the new Samsung S20 phone on Tuesday morning.  So we're going to push iOS Today forward, push MacBreak Weekly and Security Now! backward, and it should all work out.



STEVE:  Cool.



LEO:  In case you tune in and we're a little late, that's why.  Thank you, Steve.  We'll see you next time.



STEVE:  Bye.



Copyright (c) 2020 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#753

DATE:		February 11, 2020

TITLE:		Promiscuous Cookies

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-753.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we offer some welcome news about Microsoft AV under Windows 7, we follow even more blow-by-blow consequences of January's final updates for Windows 7, we look at a worrisome exploitable Bluetooth bug Google just fixed in Android and what it means for those not fixed, we update on the Clearview AI face scanning saga, we take a peek into data recovery from physically destroyed phones, we entertain yet another wacky data exfiltration channel, and we conclude by looking at the consequences of the recent changes to make cookies less promiscuous.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson is here.  We've got lots to talk about, including the return of Security Essentials to Windows 7, plus two new bugs.  Hey, no big deal.  We'll also talk about the surprising revelation that the CIA has been spying on everybody, every customer of Crypto AG for 40, 50 years.  That, and we'll talk a lot about Google's plan to eliminate third-party cookies.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 753 recorded Tuesday, February 11th, 2020:  Promiscuous Cookies.



It's time for Security Now!, the show where we cover the latest insecurity news, cover your privacy, cover your privates, and we give you all the information you need to protect yourself with this guy right here doing the Vulcan salute.  Ladies and gentlemen, I give you Mr. Steve Gibson.  Hello, Steve.



STEVE GIBSON:  You've got to watch your privates this episode, Leo, because we're going to be talking about promiscuous cookies. 



LEO:  Oh, my goodness.



STEVE:  And so you want to make sure you don't get any cookie crumbs.



LEO:  Yes.  Well, it is a privacy issue; isn't it.



STEVE:  Yeah, yeah.



LEO:  All right.



STEVE:  So we have, we're going to start the week with some welcome news and an apology that I have to give to my Twitter followers, since they must have been tweeting at me about this.  But I've been so busy and focused on other things that I've fallen behind in keeping up with Twitter because it was Elaine who first provided some good news, which we will discuss, that I'm sure other people were trying to tell me about.  And I'll hold us in suspense for a moment about that.



Then we're going to follow even more blow-by-blow consequences of January's final updates to Windows 7.  They're just really having a problem with this last one.  We look at a worrisome exploitable Bluetooth bug Google just fixed in Android.  But unfortunately, it's potentially bad, and we know that a lot of older Android smartphones aren't going to get fixed.  So we're going to take a look at that.  We're also going to update on a subject from last week, this Clearview AI company, and the ongoing saga with them as more major companies have awoken to the fact that they've had their sites scraped and have not taken it well.



We're going to take a peek into data recovery from physically destroyed phones.  Our NIST revealed some interesting work on the problem of getting data off of phones which bad guys have attempted to physically destroy in order to prevent that recovery.  We look at yet another wacky data exfiltration channel.  And then we're going to conclude, as I mentioned, by looking at the consequences of the recent changes to make cookies less promiscuous for this Episode 753 for February 11th.  So I think another great podcast for our listeners.



LEO:  Nice.



STEVE:  And a very techie, geeky Picture of the Week, but something that I've had in my pile, waiting to share, that will be fun.  So, yeah.



LEO:  I have a Video of the Week to share for you.



STEVE:  Okay.



LEO:  So when you do the picture, I'll do the video.  How about that?  It's a little surprise for you, Steve.  But first - well, no.  It's a pleasant surprise.



STEVE:  Yeah, I knew you weren't going...



LEO:  I wouldn't do anything mean.  All right, Steve.  Who first?  You want me first or you first?  You want the picture or the video?



STEVE:  Let's see the video.  What have you got for us?



LEO:  We had our engineering dinner last night.  And Patrick Delahanty, I don't know if you know, he's our guy, he does all our programming, he manages the API.  He's really our coder, our security guy.  He moved back East with his wife Svet, who is a children's author, very successful.  And they have a beautiful boy, [Kaden], who is - how old is [Kaden] now, maybe a year and a half?  Maybe just a year.  He sent us this video of [Kaden].  Apparently [Kaden] has excellent taste in podcasts.



[BEGIN VIDEO]



ADULT:  Are you watching Steve Gibson?  Steve Gibson.



CHILD:  [Laughing]



ADULT:  Steve Gibson.



CHILD:  [Laughing]



ADULT:  Steve Gibson.



CHILD:  [Babbling]



ADULT:  Yeah.



CHILD:  Ha.



[END VIDEO]



LEO:  He loves Steve Gibson.  He asks for Steve Gibson.  This guy's going far.  I think he's a red teamer.  I think.  I don't know.  I may be wrong.  But this kid [Kaden] is adorable.  And he loves, for some reason...



STEVE:  Yes, the Mr. Rogers of security.



LEO:  Even children love Steve Gibson.  Isn't that awesome?  All right.  Now your picture.  Now your turn.



STEVE:  Well, so all of us, I'm sure, you and I have discussed this particular breed...



LEO:  Oh, I hate CAPTCHAs.  



STEVE:  Yes, this particular breed of CAPTCHA is so annoying.  They take a photo and then chop it up into a grid.  And, you know, typically it'll be like an intersection with various things going on, and your job as the human is to select all the squares of the grid that have an auto, or that have a crosswalk.



LEO:  I hate this.  Ugh.



STEVE:  Or that have a crossing sign or something.



LEO:  I hate helping Google's self-driving cars.  Because that's what we're doing.  You know that.  For free.  We're giving them free human input.



STEVE:  Funny you should say that because we may be helping Google with their code in this case, debug their code.  This CAPTCHA says:  "Select all squares with bugs.  If there are none, click Skip."



LEO:  This is a joke now.  Come on.



STEVE:  It's like, well, I don't know, Leo, that second one, that second line down, third over...



LEO:  There's a lot of hex in coding here.  I don't know.



STEVE:  We've got unbalanced parentheses, I think, in that "if" clause.



LEO:  Oh, my god.



STEVE:  So, yeah, anyway, I thought - someone sent me this, and I thought, oh, that's kind of...



LEO:  That's hysterical.



STEVE:  That's fun.  That's definitely way out there in upper geek land gets the humor of that.



Okay.  So this came from Elaine yesterday when she sent back the transcript.  "Hi Steve.  I've been using MS Security Essentials since it came out, through XP and 7.  Wikipedia says:  'Although support for Windows 7 ended on January 14, 2020 and MSE is no longer available to download, Microsoft will continue to update virus definitions for existing users until 2023.'"  And so then she said:  "Guess I'm good for a while.  I still get new definitions every night."  And I can confirm that I, too, am still getting nightly updates on my Windows 7 machine, and my MSE is continuing to scan and protect my Win7 machine.



So all of that, that we've been saying and grumbling about the last few weeks about, oh, the end of AV, and what are we going to switch to, and should we just go with nothing, just go commando.  Turns out, no.  Yes, our Windows 7 machines no longer get patches.  But Microsoft is going to continue keeping the virus scanner, Windows Security Essentials or presumably Defender, depending on what you have in your Win7, current.  So yay for that.  And I just wanted to correct the record, that this is not going to be a problem.  We all get to keep our virus things updated.



And, frankly, that's the biggest issue.  Most of us are using non-Microsoft browsers, so Microsoft may choose not to update their browser.  That's fine.  Google's going to keep Chrome updated, they've said.  Mozilla will be keeping Firefox updated.  We have the commitment from them for that.  And if we also have our in-built native AV still getting definitions for another three years, we're good.  I mean, yes, if there's some horrible problem with Internet connectivity, I mean, you know, who knows what.  Of course we'll be keeping our eye on that and see whether anything that happens in Windows 10 could affect 7.  But for what it's worth, we don't need to go searching for some other AV tool.  So thank you, Elaine, for the news.



We did get a fix from Microsoft for the problem we discussed last week on Friday.  That was the 7th of February.  They dropped an out-of-cycle update to fix the desktop wallpaper stretch black screen of death problem which got introduced in the January Patch Tuesday.  There's really nothing to their update.  What's interesting is I was curious to know whether they were going to drop a formal Patch Tuesday on Windows 7 customers, but so far I checked this morning, and there were no updates for my Windows 7 machine.  I meant to check again just before the podcast, but I got...



LEO:  Have you had the problem that other people are reporting now where you can't shut it down?



STEVE:  Aha, that's what we're coming to.



LEO:  Okay.



STEVE:  We're going to get to that in a second.



LEO:  Okay, yes, because they've got more to fix.



STEVE:  They sure do.  And I'll be surprised if that one doesn't get itself pushed out.  So they said, of the wallpaper stretching problem, they said:  "This update resolves the following issue."  So just so people know, you've got to go get it.  It is - and I don't have the number.  I must have it here somewhere.  Oh, yeah.  KB4539602.  So if you just want to be able in the future, even if you're not stretching a bitmap today, you may want to do that some day.  So KB4539602 will allow you to do that, fixes that problem.



And they said, "This update resolves the following issue:  Addresses an issue that might cause your wallpaper that is set to Stretch to display as black."  Then they said:  "Important."  And, boy, is it.  "Before you apply this update, see the prerequisite section."  Well, now, it's not such a - I don't think it's that big an update in this instance.  But we'll get there.  So the prerequisites are that:  "You must have the following updates installed before you apply this update."



And there are two.  There's the one that we talked about way back earlier last year, the SHA-2 update.  Remember that until I think it was June or July of last summer, the updates were double-signed.  All the updates from Microsoft carried both an SHA-1 and an SHA-2 signature.  And of course we were just talking about how SHA-1 has finally pretty much collapsed under continual academic pressure.



So Microsoft realized, well, it's not good to co-sign an update where one of the co-signatures is now known to be weak.  So anticipating that, they removed SHA-1 signatures from their updates.  The problem was that Windows 7 needed to be informed about SHA-2 updates.  Unless you received this particular knowledge base update, which is 4474419, you would not be - your Windows 7 would reject the updates as being invalid because they weren't signed with SHA-1, and it hadn't been taught yet about SHA-2.  So that's the first of the two.



The second one is kind of mysterious.  It's from March of last year, and all they're calling it is a Servicing Stack Update, an SSU, Servicing Stack Update.  And that has to be in place also.  And I should mention that when I did, over the holidays, at the beginning of the year, I did a big update catch-up for GRC's servers.  And I'm slow in installing updates for reasons we're about to get to here in a minute, relative to server.  And so I'm kind of glad.  And of course I also have lots of other rings of security surrounding GRC's servers.  So they're protected in ways that depend much less on Microsoft than a server that's just sitting there with remote desktop protocol exposed to the Internet, for example.



But the point was my ability to get current was crippled by a mystery.  I was unable to update one of my two servers.  And it was finally, when I went back in my logs and saw that this Servicing Stack Update had not installed, that I thought, oh, well, okay.  The Internet is telling me I need that.  So I manually installed it, and then I was able to bring my machine current.  So what's interesting is that Microsoft is saying that you will have automatically received this in Windows Update.  But at the same time, they're saying, but if not, make sure you do.  Well, I don't know what the story is, but I didn't get it automatically, and apparently people don't.  So you need to have those both in order to fix the wallpaper stretch problem.



However, it turns out that after installing the fix, the 4539602 update, and I was supposed to know what that one was, that was not - oh, yeah.  The wallpaper, the black wallpaper stretching fix.  So, okay, first of all I should just say, I don't know who is going to have a stretched bitmap on a server.  I guess people just think of it as Windows.  I just sort of think of my servers as special.  I don't, like, install all kinds of junk on them.  And I don't care what the desktop looks like.  I'm rarely seeing it.



But anyway, people applied the fix on Friday and then began getting a notice that you referred to, Leo.  Oh, wait, no.  You referred to the "can't shut it down."  This is way worse.  After installing the fix for - and this is just, like, beyond comprehension.  After installing the fix for your bitmap stretch being black, that renders Windows Server 2008 R2, which is, as we know, the server version of Windows 7, unbootable.  It will no longer boot.



LEO:  But at least your wallpaper's not black.



STEVE:  Yeah, exactly.  Now you're no longer being bothered by a black desktop.



LEO:  You've got to really wonder.  Good lord.  



STEVE:  Unbelievable.  On any instance of Windows Server 2008 R2 which is lacking those prerequisite updates I noted above, the consequence of attempting to install 4539602 to fix your wallpaper isn't a notice of an update failure or some nice mention about prerequisite updates missing.  No, the result is a fully bricked server.  For reasons only Microsoft knows, attempting to fix the desktop wallpaper stretching issue results in the deletion of two critical boot files that live in...



LEO:  Oh, my god.  This is so awful.



STEVE:  It's unbelievable.  Winload.efi, think that might be important, and winload.exe are deleted from your Windows system.



LEO:  This should in no way be coupled to anything with wallpaper.



STEVE:  No.  No.  It's like, what?  So anyway, so as a consequence, since Friday, people were installing 602, and their system wouldn't restart.  So the community has come up with some solutions.  If you boot into the system recovery mode, then that'll kind of get your system basically going.  You can get to a command prompt.  If you have other Windows 2008 R2 servers that have not been bricked, they'll still have those two files.  So if you can copy those files back into the Windows system32 directory, you're recovered.  That's all you need to do.  Or you can use the Windows imaging command line, the system imaging command "dism."  And I have the command in the show notes here if anyone wants to take that approach and has these problems and hasn't already figured out how to fix it:  dism.exe /image:C:\ /cleanup-image / revertpendingactions.  And so that's sort of a manual way of undoing what it was that Windows inadvertently did that will bring your system current again.



And again, in my own experience, I mean, I could have easily fallen into this, too, because one of those prerequisites, that Servicing Stack Update, was missing.  And it wasn't being given to me.  I mean, I had other updates subsequent to March when that thing was released, but it just kind of got forgotten.  And in this case that's not good.  And the hits keep coming because, Leo, as you said, another problem has been afflicting people since the "final," maybe not so final Patch Tuesday update for Windows 7, which broke the wallpaper.  Some people - and you've got to love the irony of this one, since Microsoft has been frantically working to push everyone off of Windows 7 and over to Windows 10.  But now some Windows 10 users are being told:  "You don't have permission to shut down this computer."  And I have a picture of... 



LEO:  You bad man, you. 



STEVE:  Oh, my lord.  



LEO:  That's crazy.



STEVE:  That is just unbelievable.



LEO:  It really underscores how it must be the most horribly written program of all.  I mean, ridiculous.



STEVE:  Yes.  I had a friend, a long since ex-coworker who was at Microsoft.  He was at Berkeley with me.  Super smart guy.  Oh, no, wait.  No, I think [Loren] was definitely an MIT person.  So he went to MIT.  Anyway, we were talking about the state of Windows many years ago.  And he just said, oh, it's just oh, oh.



LEO:  If you only knew, Steve.  If you only knew.  Had he seen it?  Had he seen the source code?  Did he work at Microsoft?



STEVE:  Oh, yeah, he was deep, deep in.  And I had another friend I mentioned who was the author, the originator of the whole .NET concept, the idea of a common language runtime was the brainchild of - actually he was an ex-employee of mine.  And similarly, just like, oh, we're just trying to leave that behind as quickly as we can.  And think about it.  I didn't have the update for today, that is, ready for today's podcast.  But I did notice 99 fixes in today's Patch Tuesday.  This is the second Tuesday in February for all Windows 8 and Windows 10 users, 99 things fixed including an IE zero-day, another problem in Internet Explorer actively being exploited in the wild.



And as I said, grumbling, last week, about how they're constantly changing it?  Well, we know, if they won't just leave it alone, get their hands off it, it is never going to get fixed.  They're going to just - this is life now is this moving, as I called it, a "smear," a Windows 10 version smear, because they're just constantly changing it.



So anyway, a number of workarounds have been found for people who are being told they no longer have permission to turn off their own computer.  If you log off, that'll bring you back to the logon screen.  And as we know, down in the lower right there is the option to shut the computer off.  You can do that.  Apparently you can do CTRL-ALT-DELETE a few times to get to a similar screen where powering off is an option.  That'll avoid this.  And there is a group policy edit tweak that you can apply, basically giving yourself permission - imagine that - to turn off the computer.



Again, are we going to see an out-of-cycle update?  Is Microsoft going to say, okay, maybe we ought to just push an update out to people?  I don't know.  I don't know what their policy is.



LEO:  It's almost guaranteed to break something.



STEVE:  Maybe they unplugged all of those Windows 7 update servers, and they're just gone now.  Or they said, let's send them to the Azure cloud.  Send them off to a better place.  I don't know.



LEO:  Unbelievable.



STEVE:  Wow.  And then just, you know, because this just bugs me, Windows 10 Firefox users are being reminded about Edge.  So I figured, while I'm on the topic of Microsoft and Windows 10, I suppose anyone who hasn't deliberately turned off the "suggestions" option, I mean, that's like - it's not Candy Crush Soda Saga, but it's up there.  I mean, you know, when presented with a switch from Microsoft about would I like to have some suggestions, like, no.  You have nothing to suggest that I want to know about.  So of course mine's off.



Anyway, so this is on the Start menu.  And maybe you didn't know you could turn it off, or maybe you left it on because you are interested in what Microsoft may have to suggest.  But, you know, as we know, Windows 10 is now free and intended to be a source of, in Microsoft words, "significant marketing and profit opportunities moving forward."  So this is what we get with this new approach towards an operating system.



And I'll just say it's not for the sake of running Windows that we run Windows, exciting and harrowing thought it can sometimes be.  Windows is a means to an end.  It exists to host and launch other programs.  It's an operating system.  So it seems a bit unseemly for people with their Start menu suggestions still enabled to receive the following selective notice when Microsoft is their deliberately chosen browser.  What Firefox users are now getting on their Start menu...



LEO:  Oh, this pisses me off. 



STEVE:  Yes.



LEO:  I'm sorry.  Did I say that out loud?



STEVE:  Yes, you did.  Thank you, Leo, I appreciate your report.



LEO:  It irritates me no end.



STEVE:  Isn't that wrong?  Doesn't that feel wrong?  So up at the top of your Start menu, under "Suggested," it says, with the new surf wave Edge logo, "Still using Firefox?"



LEO:  What's wrong with you?



STEVE:  Microsoft Edge is here.



LEO:  Oh.  Ugh.



STEVE:  You know?



LEO:  Now, does it say it just once?



STEVE:  Well, let's hope.  I don't know.  Once is enough.  That just...



LEO:  Yeah.



STEVE:  Okay, so, you know, every week these notes that I publish, that you're looking at, that I'm reading from, that our listeners can download, they're authored in Google Docs on Chrome because that's the best way I've found of doing something like this.  That's alongside a Firefox browser, with a long vertical column of tabs, where all of the news of the week I've pulled together and sorted and arranged and put them in, you know, so like pulled what I want to talk about this week together.  And there's an instance of the ThoughtManager Desktop outliner app.



You know, I'm working toward a tentative peace with Windows 10 because Microsoft really hasn't left me or anyone else, any of us, with any practical choice unless we want to just leave the Windows universe.  But I don't need their help choosing the best tool for the job.  I'm delighted Edge has incorporated Chromium.  But to answer your question, yes, Microsoft, I'm still using Firefox.



LEO:  Yeah.  Yes, I am.  You got a problem with that, buddy?



STEVE:  I know.  Oh, boy.  That just, you know, well, it's part and parcel of Candy Crush Soda Saga.  It's not there because they want it there, it's there because they're getting paid.  And in this case they're just annoyed that, I mean, I guess I could see the logic.  Maybe somebody used Firefox because the previous Edge didn't work for them somehow.  So they're saying, oh, like, we fixed it.  Now it actually is a good browser.  I don't know.



LEO:  Yeah.  I don't know.  I don't know.  You know, they're, well, we talked about this before.  They're making some enterprise users change their search from Google to Bing.



STEVE:  Yes, yes.



LEO:  I mean, this is - there's somebody at Microsoft doesn't really understand user freedom or something.  I don't know.  I don't get it.



STEVE:  Well, I mean, it is the direction they're going in.  And, you know...



LEO:  I don't know.  I wouldn't say unilaterally.  The company also supports open source and, I mean, I don't know.



STEVE:  And they do have Edge based on Chromium, which is, you know, a good thing.



LEO:  Right.  There's somebody in marketing that's just annoying.  I don't know.  I don't get it.



STEVE:  So last week Google closed an Android remote code execution flaw which was discovered in the Bluetooth daemon running in Android.  It has been patched.  It was patched last week in Android's February security update.  And, you know, we've been encountering Bluetooth flaws recently.  And while they're not good because they are potentially hands-off and at a distance, at least the deliberately lower power short-range operation of Bluetooth tends - and of course we know that's not an absolute, either, because you can get directional antennas and Pringles cans and things - the short-rangeness tends to limit the vulnerability's exportability and severity.  Certainly WiFi vulnerabilities are worse.  And Internet TCP flaws are worse still because they work on a global scale.



But in this case a critical vulnerability was found and fixed in the Bluetooth implementation on Android devices which could allow, which is to say does allow, attackers to launch remote code execution attacks without any user interaction.  So it's one of those bad ones where the user who had been compromised would not know it.



Last Thursday, after the patch had been pushed out, the researchers who found it revealed additional, but not all, because they're trying to be responsible, details behind this flaw.  It's tracked as CVE-2020-0022.  It poses a potential critical severity threat to Android versions Pie, so that's 9, and Oreo, 8.0 and 8.1, which account currently for almost two thirds of Android devices today, assuming that they've got Bluetooth enabled, as most Android devices probably do for the various, I mean, I even find it turned on on my things when I've explicitly turned it off.



We've talked before about how Apple seems to think they know better, and they keep turning Bluetooth back on for me.  It's like, okay, well, I don't need that power drain, and I don't have any Bluetooth things hooked up, and I'd rather not have the risk of one additional radio thing which could have a problem, exactly as this does.  And that's been our advice on this podcast for years.  If you're not using a radio, whatever it is, turn it off because it's not helping you.  It's consuming some power, and it creates an inherent vulnerability at a distance.



LEO:  It's analogous to don't turn any services on on a computer unless you know you're going to use them.



STEVE:  Exactly.



LEO:  You're just opening it up for - yup.



STEVE:  So against 9.0 Pie and Oreo, Pie and Oreo 8.0, 8.1, and 9.0.  The researcher said that a remote attacker within Bluetooth range can silently execute arbitrary code with the privilege of the Bluetooth Daemon, and it runs in the kernel.  The flaw is worrisome because no additional interaction is required, and only the Bluetooth MAC address of the target device needs to be known to launch an attack.



Okay.  So, well, there are a couple reasons that's not comforting, because it turns out that for many devices the Bluetooth MAC address can be deduced from the WiFi MAC address.  They're often sequential.  And so WiFi is easily known.  It's being broadcast by the smartphone's WiFi.  So obtaining the Bluetooth MAC address is probably a matter of adding or subtracting one, depending upon which phone you're using, and maybe they're all the same.  I haven't looked.



The same vulnerability does impact Google's most recent Android v10.  However, with Android 10, the severity rating is dropped to moderate rather than critical because the impact is not a remote code execution as a consequence of other changes made in Android 10.  It will crash the Bluetooth daemon, but it won't give you remote code execution access.  And they did not test any Android versions older than 8.  So we don't know either way whether those may be affected.  The flaw's discoverers said they are confident all patches - they said, sorry, once they are "confident" - and I put "confident" in quotes in the show notes because you'll see where I'm going - all patches have reached the end users, they will publish a technical report on the flaw that includes a description of the exploit as well as proof of concept code.



The trouble is all of us here know that a great many Android devices running Oreo and Pie are never going to receive an update.  So, I mean, even Windows systems that have automatic updates universally applied somehow manage not to receive them.  So we know that many suppliers of lower Android devices aren't being responsible with pushing updates out to their customers.  So those people who are not receiving updates for Oreo and Pie, two thirds of the current Android install base, we don't know what percentage are receiving updates, they will now probably forever be vulnerable to the possibility of an engineered proximity takeover and malware installation.  And we know that completely descriptive documentation including a working proof of concept, will be made available shortly.  Maybe they'll wait a week.  Maybe they'll wait two weeks.  But maybe a month, doesn't matter, a huge percentage of devices are not going to get fixed.



And this is precisely the sort of powerful and persistent vulnerability that the powers that be, hostile powers, border shenanigans, crossing into China, wherever, where people say, "Oh, yeah, I got some stuff installed on my phone."  Well, this is going to - they're like, here's a new way for that to happen for anybody whose phone is sufficiently vulnerable.  They didn't even have to take your phone behind a screen somewhere.  They could just, you know, figure out that this is you, or maybe just try everybody who's moving through and see how many phones they're able to install some backdoor spyware onto.



So this is really bad.  We've said it before, and it bears repeating.  Today's smartphones are seen by bad guys as a huge target of opportunity.  And just as no one today wants to use an operating system that's no longer receiving security updates, people should be reluctant in the extreme to use any smartphone whose manufacturer does not have a solid track record of providing updates.  It's true that such after-sale support comes at a cost.  The cheapest phones won't have it.  But in this case, you really are getting something valuable for the money.



So I just can't imagine, Leo, using a phone that is not from a major manufacturer that is known to be responsibly putting out updates.  And to be doing it for the service life of the phone.  It's not good enough to say, oh, we're going to do it for three years, but you keep using the phone afterwards.  Or as an end user, you have to have the self-control to say, okay, updates have stopped.  I'm going to update to a newer phone.  You just have to.



Jonathan Knudsen, who is the senior security strategist at Synopsis, said of CVE-2020-0022, he said:  "It can be exploited by anyone within range of your vulnerable phone who can determine your Bluetooth MAC address, which is not difficult."  He said:  "As a user, keeping current with updates and applying them in a timely manner is important.  Unfortunately, many vulnerable, slightly older phones will not have continuing software update support from the manufacturer, which means users are faced with two unattractive options:  either disable Bluetooth entirely" - and certainly that's a good way to do it, and make sure it stays off - "or get a newer phone."



So the February patch roundup for Android included patches for 25 bugs, with 19 of those vulnerabilities rated high.  There were four others that were high, but they were specifically tied to Qualcomm chipsets used inside Android devices.  So this was the most worrisome of those.  If you happen, if you're listening to this, and you've got an Android phone that didn't get updated last week, you need it to be updated or turn Bluetooth off, if you're concerned that, you know, if you're a target of opportunity, if you're a little unnerved by the idea that in a couple of weeks full disclosure will be provided; all the bad guys in the world will know how to do this to any Android devices.  Not that old, either.  Pie and Oreo that haven't had updates.  It's just you can't use a smartphone that isn't on the update flow.



So last week we talked about the Clearview AI company, who were doing the facial recognition and bragging that they scraped the web for three billion faceprints and made them available to 600 police departments so they could identify people within seconds.  Since then, Clearview has increased their collection of cease-and-desist letters, which was not exactly what they were hoping to be collecting, from major U.S. social media players.  The first one they received was from Twitter a couple weeks ago, when Twitter told Clearview to stop collecting its data and to delete whatever it had.  In addition, Facebook has similarly demanded that Clearview stop scraping photos because that action violates Facebook's policies.  And now Google and YouTube are also both telling Clearview to stop violating their policies against data scraping.



Clearview's take on this is defiance.  The CEO, Hoan Ton-That, was interviewed last Wednesday morning on CBS's "This Morning" news show.  He told listeners to trust him.  He said the technology is only to be used by law enforcement, and only to identify potential criminals.  Ton-That claims that the results, which is not encouraging, are 99.6% accurate.  I guess, though, you wouldn't want a false positive to misidentify you as a bad guy.  So I guess accuracy is a better thing.  And he also claimed that it's his right to collect public photos to feed into his facial recognition archive.  He said:  "There's also a First Amendment right to public information.  So the way we have built our system is to only take publicly available information and index it that way."



LEO:  And, by the way, there was a recent Supreme Court decision having to do - was it the Supreme Court?  Maybe Ninth Circuit Court - having to do with scraping of LinkedIn in which they ruled, yup, you can't stop scraping.  If it's public information, you can't stop it.



STEVE:  In fact, I have that, I have a mention of that here.  So we know from last week when we talked about this that in Illinois, at least, with their BIPA, the Biometric Information Privacy Act, it's illegal there.  And YouTube's statement read:  "YouTube's Terms of Service explicitly forbid collecting data that could be used to identify a person.  Clearview has publicly admitted to doing exactly that.  And in response, we sent them a cease-and-desist letter."



As for Facebook, Facebook said last Tuesday that it has demanded that Clearview stop scraping photos because the action violates its policies.  Facebook said:  "We have serious concerns with Clearview's practices, which is why we've requested information as part of our ongoing review.  How they respond will determine the next steps we take."  Which I'm sure Facebook intended to sort of sound ominous.  And Ton-That defended Clearview as being a Google-like search engine.  He said:  "Google can pull in information from all different websites.  If it's public, and it can be inside Google's search engine, it can be in ours, as well."



Google disagreed, saying that Clearview isn't at all like their search engine.  Google said:  "There's a big difference between what we do and the way you're shanghaiing everyone's face images without their consent.  Most websites want to be included in Google search, and we give webmasters control over what information from their site is included in our search results, including the option to opt out entirely."  Google said:  "Clearview secretly collected image data of individuals without their consent, and in violation of rules explicitly forbidding them from doing so."  So the question is, when is public information not public?



Which brings me to the point you raised, Leo.  Back in 2016 a company called hiQ, which I recall we talked about at the time, a San Francisco startup was marketing two products which depended upon whatever data LinkedIn's 500 million members had chosen to make public.  There was the first product they called "Keeper," which identified employees who might be ripe for being recruited, and "Skills Mapper" summarized a LinkedIn member's skills.  In that instance, hiQ was amassing public information, grabbing the same material that anyone could get from Linked In, without having to log in.  So any browser would display the same information hiQ was vacuuming up, organizing, and reselling.  And when sufficiently analyzed, inferences could be made to alert companies, for example, when their pivotal employees might be interviewing for another position.  And you can do much more, as we know, with this kind of advanced informatics.



LEO:  Oh, geez.



STEVE:  Yeah, isn't that interesting.  You put a flag to be notified when any of your employees' LinkedIn profiles indicate maybe they're, you know, that longer lunch break was a little more than a lunch break.



Okay.  So in the case of hiQ, LinkedIn sent a cease-and-desist letter alleging that it was violating serious anti-hacking and anti-copyright violation laws.  And LinkedIn cited the Computer Fraud and Abuse Act, the CFAA; the Digital Millennium Copyright Act, the DMCA that we've talked about so much; and California Penal Code section 502(c), whatever that is.  LinkedIn, and this is a little interesting aside, had been exploring how to do the same thing with its own data that hiQ had achieved, also noted that it had blocked hiQ from accessing its data.  And, as you mentioned, it was just this past September, in 2019, an appeals court told LinkedIn to back off, and that it had no legal right to interfere with hiQ's profiting from its users' publicly available data.  The court protected data scraping of public data in what at first looks like a major legal precedent, but it's actually a lot less clear.



Our friends at the Electronic Frontier Foundation wrote:  "While this decision represents an important step to putting limits on using the CFAA" - the Computer Fraud and Abuse Act, because the concern has been that it's being abused - "to intimidate researchers with the legalese of cease-and-desist letters, the Ninth Circuit sadly left the door open to other claims, such as trespass to chattels or even copyright infringement, that might allow actors like LinkedIn to limit competition with its products."  So essentially the Ninth Circuit didn't go as far as our EFF folks wished it had.  But in this case at least it said, hey, you, LinkedIn, are not lawfully allowed to prevent somebody from visiting your site with automated scrapers and obtain whatever information has been made public by your users.



And of course the problem is the CFAA, the Computer Fraud and Abuse Act, is broadly written and subject to multiple conflicting interpretations across different federal circuits.  This makes it likely that the Supreme Court will ultimately be forced to resolve the meaning of terms which are not really clear.  The CFAA says, for example, "without authorization."  Well, people want to take without authorization the way that they want to.  And this of course is bad.  This occurs with broadly written legislation that just ends up having to - it's a problem for everyone and ends up having to get resolved in the courts.



The EFF's Surveillance Litigation Director - there is such a person.  Their Surveillance Litigation Director Jennifer Lynch said that Clearview is the latest example - so now we're talking about Clearview, the facial scraping people.  That doesn't sound right, facial - well, anyway.



LEO:  Yes, it's a laser procedure.



STEVE:  Yes, you can get that done.



LEO:  Makes your skin soft.



STEVE:  Yeah, it's covered by insurance - is the latest example of why we need laws that ban or at least pause, pending more clarification, law enforcement's secret abuse of facial biometric recognition.  She cited many cases of what she called law enforcement's, and Clearview's specifically, abuse of facial recognition, stating:  "Police abuse of facial recognition technology is not theoretical.  It's happening today.  Law enforcement has already used live face recognition on public streets and at political protests."



And of course as we've observed before, this is all being enabled by the recent incredible reductions in cost.  The cost of processing power has crashed.  The cost of mass storage has collapsed.  The cost and presence of ubiquitous networking communications, it just doesn't cost anything anymore to send data, massive data all over the place.  We didn't have this 10 years ago.  And Leo, although the podcast will have run out by 10 years from now, we'll still be around, and we'll see what we have 10 years from now.



LEO:  We'll see if anybody recognizes us.



STEVE:  The NIST is testing methods of recovering data from smashed smartphones.  And, you know, it makes sense when you think about it.  There's been a lot of discussion through the years about how to best irreversibly kill a hard drive.  And we talked recently, not too long ago, about this.  One of my favorites, since many modern hard drive platters are now being made of glass, you can often take a hard drive, kind of like a - what was it we used to smash?  Bit-O-Honey?  No, no, not Bit-O-Honey.



LEO:  With a little silver hammer?



STEVE:  Yes.  You could take a...



LEO:  That was Bit-O-Honey.



STEVE:  There was that big white thing that was chewy.



LEO:  Laffy Taffy?



STEVE:  I remember, yeah, you'd smash it.



LEO:  Bonomo.  It was Bonomo.  And, yeah, you'd smash it, and it'd be little pieces, and you'd eat it.



STEVE:  Yes.  And then you peeled the paper off.  And it's like all got little itty-bitty chewy...



LEO:  Yeah.  It was good.  Loved that.



STEVE:  Yeah.  Anyway, turns out you can do that with many hard drives.  You take a hard drive, just smash it face down onto the concrete or asphalt or something, a hard surface.  And then, if you shake it, put it up next to your ear and shake it, if you hear the sound of lots of little bitty fragments, then you know...



LEO:  How do you know you got all the platters?



STEVE:  That's true.  Chances are...



LEO:  I know this is true because 20 years ago Patrick Norton, we were showing people how to destroy hard drives, and he opened up a hard drive.  See, because usually it's pretty easy to unscrew a hard drive.



STEVE:  You just need a torque spring, yeah.



LEO:  Yeah.  Pull out the thing.  There's three, four, five platters.  In the case of the 16TBs, I don't know, 28 platters.  And then he said, and watch, I can just - and he hit it with a hammer.  He didn't know it was glass.  He thought it was going to be metal.  He was going to bend it.  And shards of glass flew everywhere.  We weren't wearing protective eyewear.  This is live television.  We're very lucky.  Talk about face scraping, I mean, it was - anyway, yes.  So this is nothing new.  They've been made out of glass, in some cases anyway, for a while.



STEVE:  Yeah.  It just turns out glass is a fluid, and it is an easier substance to work with to get the level of smoothness that you need...



LEO:  Oh, yeah, of course, yeah.



STEVE:  ...in order to fly heads as close to these things as they are.  But what about an entirely solid-state smartphone?  We talked about it just a couple weeks ago.  One of those guys had shot one of his two smartphones, and the FBI claimed to have brought it back to life.



LEO:  Yeah.



STEVE:  Which I think is a miracle.



LEO:  By the way, you got any Bit-O-Honey?  Because I've got a hammer.



STEVE:  The bad guys, it turns out, this is a thing.  Bad guys are now smashing their phones, drowning them in water, shooting them with a gun. 



LEO:  What did they do - doesn't he throw it in the microwave on "Mr. Robot"?  Oh, those were the little SIM cards he'd throw in the microwave.



STEVE:  Yeah.  You don't want to do that.  That would be rough.  You used to be able to put a CD, an audio CD in the microwave.



LEO:  Yeah, it would spark.



STEVE:  And it would make all kind of sparkliness and things, yeah.



LEO:  Don't do that at home, kids.  It's not good. 



STEVE:  So the question is, how effective is physically destroying a smartphone?  It turns out that many criminals have discovered to their chagrin that reducing their devices to smashed plastic and glass means nothing, if the device's little black epoxy memory chips have managed to survive.  Forensic engineers who work with police to gather evidence have become quite adept at performing, like, amazing feats of posthumous data extraction.  With more and more evidence now sitting on smartphones, a better understanding of what works and what doesn't has been turned into a growing issue of some urgency.



So our U.S. National Institute of Standards and Technology (NIST) recently conducted a series of tests using 10 popular Android smartphones which had accumulated a mix of data during their simulated use.  The NIST engineers and their forensic partners then attempted to extract the data from the surviving memory chips using various methods to compare with the original dataset.  In some cases the chips could be left attached to the original motherboard and accessed via the JTAG serial interface, which all systems have.  JTAG is an industry standard serial communications protocol for testing and programming electronics.  In other cases, the chips were physically removed from their original motherboards and then interconnected to directly, sort of an in vitro data extraction.



So the NIST wrote in their report:  "The comparison showed that both JTAG and chip-off extracted" - that's what they call it where they have to remove the chip from the board - "were able to extract the data without altering it, but that some of the software tools were better at interpreting the data than others, especially for data from social media apps."



And as I was reading this and thinking about it, I thought, that's a good point.  It's one thing to have access to the raw - presumably unencrypted, I think that's why they chose Android phones - data.  But you still need to be able to make heads or tails of what you have.  I mean, it's a chip, you know, so it's like, okay, here's the contents of this grid of bits.  Now you've got to make sense of it.  Either way, it's a big ask, to tell some guy, okay, here's a destroyed phone.  We need to know what data is in here.



It turns out that there are trained forensics people whose days are spent doing this.  They have an expert at NIST, Rick Ayers, who said:  "Many labs have an overwhelming workload, and some of these tools are very expensive.  To be able to look at a report and say this tool will work better than another for a particular case can be a big advantage."  So essentially they're sort of trying to create some decision framework for forensic data recovery.



What really piqued my interest was that Cellebrite, the company and the technology that we've spoken of often here, was one of the two systems that was used.  I've got a link to a PDF in the show notes.  The PDF is titled "Test Results for Binary Image JTAG and Chip-Off Decoding and Analysis Tool:  Cellebrite Universal Forensic Extraction Device (UFED)."  And that's an acronym that we've seen before and talked about.  So they call it the Cellebrite Universal Forensic Extraction Device Physical Analyzer, and it's now at v7.20.0.123.  And it is interesting to scroll through this.  They're located in Parsippany, New Jersey, at 7 Campus Drive.  We know that from the report.



And the results summary said:  "Cellebrite's Physical Analyzer is a versatile mobile forensic solution that runs on existing hardware.  It comes with a suite of applications, peripherals, and accessories.  Physical Analyzer was tested for its ability to decode and analyze binary images created by performing Chip-Off and JTAG data extractions from supported mobile devices.  Except for the following anomalies, the tool was able to decode and report all supported data objects completely and accurately for all mobile devices tested."



And so what we have is a list of a few exceptions for - there were some standalone files for an HTC One Mini in the chip-off that I guess it had a problem with.  There were some social media-related data that an LG K7 chip-off extraction had a problem with, the ZTE 970 chip-off had a problem with, and that related to some Twitter data that they could not recover.  The HTC One XL where the chip had been removed, the HTC Desire S where its chip had been removed, and then two HTC phones where JTAG, the JTAG serial interface was used had some problem reconstructing some Facebook data.



But by and large, we're talking about a contact, well, it says in the report here they were able to recover and perfectly reconstruct deleted contacts, calendar, memo note entries recovered from the HTC Desire 626, ZTE 970, the Moto-E, the Samsung S2, the HTC One XL, and the Samsung S4.  They were able to pull deleted contacts and calendar entries from the LG K7 and HTC Desire S.  Deleted contacts and memo entries were recovered from the HTC One Mini.  Deleted call logs were recovered from the LG K7, the Moto-E, Samsung S2, Samsung S4, and HTC Desire S.  They were able to pull deleted SMS entries recovered from the HTC Desire 626 and a bunch of others, and bookmark entries recovered from the HTC Desire 626 and others.



So I thought this would be interesting to our listeners because, I mean, this demonstrates that you can't crack your phone in half or apparently even shoot it with a bullet.  You need to, if you were someone, I mean, even for benign purposes, you know, these are reconstructed deleted data from the memory is being completely recovered by this Cellebrite forensic analysis tool.  So this stuff is real.  And essentially what it means is you need to take your phone apart and get your drill and drill a hole through all of the little black chips that you see on your phone, if you really and truly want to keep solid-state memory from being recoverable.  These are not forensically wiped.  If you were able to do a really good forensic overwrite of the data, then that would have rendered them unrecoverable.



But failing that, you really need to reduce solid-state storage to a state where the individual components of it are clearly destroyed.  Otherwise, if anybody had sufficient motivation to reconstruct the data, apparently this is something that is now just, I mean, there are people who spend their days doing this.  And the NIST has, oh, the other issue that I didn't bring up that was mentioned was the issue of the chain of evidence.  So in order for a defense attorney not to be able to poke holes in this, it's necessary for this to be done in conditions where the chain of evidence is not broken.  So labs need to be certified, and the phones need to be handled appropriately and so forth.



But the point is it is really and truly a thing to be able to recover data from a phone, even if it really looks very sad.  It may still have some vital components that are intact, and that's all it takes.  Which, wow, you know, it's just like it really does happen.



Okay.  And on the brighter side, actually, that's a pun because the paper was titled "Brightness."  This is from our Yet Another Data Exfiltration Technique of the Week department.  The title of the paper:  "Brightness: Leaking Sensitive Data from Air-Gapped Workstations via [yes] Screen Brightness."



LEO:  Oh, my god.



STEVE:  I know.  But these are the guys at the Ben-Gurion University and the Department of Electrical and Electronics Engineering at the Shamoon College of Engineering, both in Israel.  These are the guys that have done some amazing stuff before.  We'll talk about that in a second.  Their abstract from their paper, I have a link to the PDF here in the show notes, they said:  "Air-gapped computers are systems that are kept isolated from the Internet since they store or process sensitive information.  In this paper we introduce an optical covert channel in which an attacker can leak, or exfiltrate, sensitive information from air-gapped computers through manipulations of the screen brightness."



LEO:  It makes sense, though.  It's like a semaphore.  Yeah.



STEVE:  Yeah, yeah.  "This covert channel is invisible, and it works even while the user is working on the computer.  Malware on a compromised computer can obtain sensitive data - files, images, encryption keys, passwords, whatever - and modulate it within the screen brightness, invisible to users.  The small changes in the brightness are invisible to humans, but can be recovered from video streams taken by cameras such as a local security camera, a smartphone camera, or a webcam.  We present related work and discuss the technical and scientific background of this covert channel.  We examined the channel's boundaries under various parameters, with different types of computer and TV screens, and at several distances.  We also tested different types of camera receivers to demonstrate the covert channel.  Lastly, we present relevant countermeasures to this type of attack."



Okay.  So first of all, what's fun about these guys is they take strange things and, like, really wrestle them all the way to the ground.  We've talked about the topic before.  And this appears to be a particular hobby horse for these guys.  In previous years we've covered their serious research into all manner of air-gapped computer data exfiltration.  They're the guys, they talked about ways of getting data out through PC speakers, blinking LEDs, infrared lights in surveillance cameras, and - remember this one, Leo? - even modulating the rotation rate of a computer's cooling fans.



That was their famous, they called it "Fansmitter" research.  And it demonstrates that, yes, where there's a will, there's a way.  They were actually changing, slightly changing the fan speed and using the fact that that could be audibly detected as bits.  And I don't remember now how many different levels of speed, maybe it was three bits, so they were like eight different speed levels.  And so they were able to pull eight bits at a time.  It wasn't fast, but they were able to do it.



So my first reaction to this was to wonder what computer containing data worthy of exfiltration through such measures would tolerate having its screen within the view of a camera of any kind.  So that seemed a little bit farfetched.  You needed to have an environment where that could happen.  But they have a seven-page research paper, and they tackled the problem with their usual thoroughness, just as they tackled the question of how many bits per second can we transmit with the sound of a fan's speed being varied.



Anyway, they conclude, undeterred after seven pages:  "In this paper we present an optical covert channel in which data is concealed on the LCD screen brightness, invisibly to users."  They talk about, I think it was a 3% change in brightness where that was enough to electronically detect it from a video recording of a surveillance camera while the user just sat there looking at it, but there was no obvious change to the user.  They say:  "We exploit the limitations of bare human vision concerning brightness perception, using sufficiently low values of contrast between the brightness levels.  Consequently, the current results demonstrate the feasibility of our covert channel, while outlining its boundaries.  Notably, this kind of covert channel is not monitored by existing data leakage prevention systems."



So, yup, you could slightly change the intensity of a screen in order to send a one or a zero.  You'd need some fancy coding in order to do that.  But self-clocking technologies for data exist.  That's what hard drives use.  So it's possible to do it, and these guys figured out what the maximum baud rate was.  It would still take a long time.  But remember that we do have many high-value secrets that are not very long, like an elliptic key.  We like elliptic keys because they're short.  They're much easier to handle.  And they get processed more quickly.  Unfortunately, they also get exfiltrated more quickly.  So anyway, just another little wacky bit of data exfiltration research.  So Leo...



LEO:  Now let's talk about cookies, yes.



STEVE:  In a minute.  I wanted you to tell our listeners about some news that just broke in the Washington Post.



LEO:  I saw this.



STEVE:  I did not have a chance to come up to speed fully.



LEO:  Big story, yeah.



STEVE:  But, yeah.



LEO:  Okay.  I'll let you go ahead, and then I'll get the story out.



STEVE:  Oh, okay.  So, well, I guess I just wanted to mention...



LEO:  Oh, you want to talk about it now.



STEVE:  Well, I don't - I didn't know how much you knew about it, if you knew anything more than just the headline that the CIA was behind basically a fraudulent crypto company.



LEO:  We talked about it on MacBreak Weekly.



STEVE:  Right.



LEO:  Since the '40s.  Since the '40s, Crypto AG was, everybody thought, a Swiss company.  But in fact it was owned outright by the CIA and West German Intelligence.  They were making coding hardware, like Enigma machine-style things.  And according to the Post, they rigged, the CIA and West German Intelligence rigged the company's devices so they could break the codes that countries used to send encrypted messages.  The decades-long arrangement among the most closely guarded secrets of the Cold War is laid bare in a classified, comprehensive, CIA history of the operation obtained by the Washington Post and ZDF, a German public broadcaster.  It was codenamed "Thesaurus," later "Rubicon."



The CIA report concludes it was the intelligence coup of the century.  Foreign governments were paying good money to the U.S. and West Germany for the privilege of having their most secret communications read by at least two and possibly as many as five or six foreign countries.  The 1979 hostage crisis they were monitoring the Iran mullahs.  They fed intelligence about Argentina's military to the British during the Falklands War.  It's stunning.



But it underscores something I've said for a long time.  The only kind of crypto you should use is open source.  If it's a binary blob, if it's a black box, you don't know who's on the other side.  You've got to use open source.  And then at least Matthew Green can look at it and tell you, okay; right?



STEVE:  Yeah.  Well, and of course that's the way we have to go with voting systems in this country.



LEO:  Same thing.



STEVE:  It would be fine for Diebold to manufacture the hardware and to sell it.  But they've got to be using something that has been heavily scrutinized.  So it can still be a profit center.  It just, you know, we just have to not have the software be part of what's proprietary.



LEO:  Right.  Exactly.  Yeah, isn't that an amazing story?  I thought you were going to - I thought you wanted to talk about the fact that Microsoft has apparently backed down on "Bing jacking," according to our good friends at Bleeping Computer.



STEVE:  Yay.



LEO:  Yay.  Microsoft - I knew they would do this at some point, backpedaling on forcing Bing search for Office 365 users.  Mary Jo Foley and Paul will be doing a little dance.  Microsoft says it heard customers' concerns by the way the company planned to roll this value out.  So they're not going to do it.



STEVE:  Yeah, good.  Good.  And stop telling Firefox users to change browsers.



LEO:  Yes.  Criminently.



STEVE:  That's just wrong.  Okay.  So promiscuous cookies.  This is relevant to us at the moment because Chrome 80 appeared last week with its implementation of the updated handling of an optional cookie property called SameSite.  We first noted that this was happening last May.  We talked about it briefly at the time.  There was an IETF draft from Google which proposed a change to the default behavior for when cookie behavior was non-specified.  This all revolves around third-party cookies.  We've talked about them a lot.  A third-party cookie is a cookie that the browser returns to a domain other than the one that provided the page that you're looking at.



So, famously, this is the way advertisers track us is that an advertiser presents their bit of content in a window on a web page.  And even though it was never intended for this reason, browsers have always honored by default third-party cookies.  Notably, Safari never has.  I've always thought Apple was just on the ball for this.  Of course it does demonstrate there are other ways to track people than cookies.  But cookies is like the official means for maintaining state.  But the ad is not the first party, which is the site that you're visiting.  It's coming from a third-party server.



And then, of course, if you go to some other location, some other website that is also being served an ad from the same ad server, well, your browser returns the same cookie at this other site as it returned from the previous site.  That links you.  That's the way tracking happens.  So the problem is there are other abuses, cross-site request forgeries, which are a real problem for web applications, which also involve sort of a different flavor of abuse of third-party cookies, not relative to tracking, but spoofing session state in a way that can be used to steal credentials.



So last May, in the abstract from a guy from Google, and this is what I remember sharing last summer, he says:  "This document proposes two changes to cookies, inspired by the properties of the HTTP State Tokens mechanism proposed in" - and then there's another document reference.  Now, we should mention that HTTP State Tokens is a "maybe we're going to someday get this" replacement for cookies.  So I guess the point is that the engineers who are moving the web technology forward, they fully recognize that cookies, well, the world has changed dramatically since cookies were first created.  Cookies have been overtaxed with the things that they're being asked to do.



So it would be wonderful if we could design a proper HTTP State Token mechanism to correctly do what it was that cookies were originally created by Netscape back in the beginning of all this to handle.  Problem is change is difficult.  I mean, even this change, the change we're talking about today, about cookie promiscuity, is turning out to be difficult, as we'll see.  But so this guy is saying the document proposes two changes to cookies inspired by the properties of the HTTP State Tokens mechanism.  What happened was, out of that discussion came an awareness that cookies could be fixed a little bit without throwing them out completely.



So he says:  "First, cookies should be treated as" - and there's an expression - "SameSite=Lax by default."  There is a, I can't think of the name, an attribute, that's the word I wanted, an attribute which cookies can be given.  Cookies can, for example, be given the attribute of "secure," in which case no browser will send a cookie that it has for a site over a non-secure connection.  In other words, HTTPS has to be present if the cookie has the attribute tag "secure."



Similarly, and there are a number of different types of attribute tags.  For example, "expires" is another one, how long should the browser keep this before letting it go.  And if there is no expires date or time, then it's automatically a session cookie, so that it will not save it in permanent memory.  It will, as soon as you close the browser, the browser forgets that cookie, which can be useful in some instances.  Another attribute is "SameSite."  And that can have the values of, first of all, it could not be specified at all, or it can explicitly have the value of None, Lax, or Strict.  And we should think of it as same-site enforcement.  In other words, you could have no enforcement of same-site handling, you could have lax enforcement, or strict enforcement.



So this guy is saying that, first of all, he's proposing that the default, which has been None, that is, no same-site enforcement, should be elevated to Lax by default, which is a big change.  That is, so if there's no specification, rather than the no specification meaning, okay, no change in same-site enforcement, then we're going to change this so that there will be a change.  And he said, secondly, cookies that explicitly assert SameSite=None in order to enable cross-site delivery should also be marked as Secure.



So this guy is proposing these two changes.  We're going to change the default to tighten the cross-site handling of cookies in two ways.  And I'll explain the first one here in a second.  The second of those is that, if somebody had explicitly said we want no change, SameSite=None, that from now on that will only be done over a secure connection.  Even if secure isn't explicitly stated, that is, that becomes now part of the spec.



So I have a lot of detail here that it's difficult for me to - I've pretty much covered this already just verbally.  If you have a cookie marked as Strict, then it will never be sent in a third-party context.  And the way that could happen is you can receive a cookie in a first-party context.  And then, if a query comes in, or if you then make a query to that domain for which you received a cookie in the first-party context, but that's a third-party domain, then if the cookie was marked as Strict for same-site enforcement, the browser pretends not to have a cookie, just doesn't return it to the server as part of the response.



So "The HTTP State Tokens proposal," this IETF document explains, "aims to replace cookies with a state management mechanism that has better security and privacy properties.  That proposal is somewhat aspirational," he recognizes, says "it's going to take a long time to come to agreement on the exact contours of a cookie replacement, and an even longer time to actually do so."



He says:  "While we're debating the details of a new state management primitive, it seems quite reasonable to reevaluate some aspects of the existing primitive," which is cookies.  He says:  "When we can find consensus on some aspect of HTTP State Tokens, we can apply those aspirations to cookies now, driving incremental improvements to state management in the status quo."



And so essentially what has happened is Google is the first browser with Chrome 80 to bite this bullet and make this change, to essentially change the way in some instances cross-site cookies are handled and returned.  And I've scrolled through a whole bunch of stuff in the show notes that anyone who's really interested in the nitty-gritty can read.



Troy Hunt blogged about this pending change last month.  On January 3rd he blogged a posting titled "Promiscuous Cookies and Their Impending Death via the SameSite Policy."  The top of his blog, before he gets into a lot of details, he said:  "Cookies like to get around.  They have no scruples about where they go, save for some basic constraints relating to the origin from which they were set."  He says:  "I mean, have a think about it."



He said:  "If a website sets a cookie, then you click a link to another page on that same site, will the cookie be automatically sent with the request?  Yes.  What if an attacker sends you a link to that same website in a malicious email and you click that link.  Will the cookie be sent?  Also yes."  Then he says, finally:  "What if an attacker directs you to a malicious website and upon visiting it your browser makes a post request to the original website that set the cookie?  Will that cookie still be sent with the request?"  And he says, "Yes!"  So there are ways that this can be abused.



He says:  "Cookies just don't care about how the request was initiated, nor from which origin.  All they care about is that they are valid for the requested resource."  He says:  "'Origin' is a key word here, too.  Those last two examples above are cross-origin requests in that they were initiated from origins other than the original website that set the cookie.  The problem is that opens up a rather nasty attack vector we know of as Cross-Site Request Forgery, or CSRF."



He says:  "Way back in 2010 I was writing about this as part of the OWASP Top 10 for ASP.NET series, and a near decade later, it is still a problem."  So in his posting he gives some examples of how a POST providing both the old and a new password, for example, a POST which is in the response to a password change action, dialog and submission, carries a promiscuous cookie which can be abused.  I don't have it here because I can't describe it in detail in the podcast.  But it's in the link that I provided, if anyone wants to see the details.



He explains that in a secure response, which is the second of the examples he offers, there are two anti-forgery tokens passed along with the request.  One is in a cookie, and one is in the body, both of them called RequestVerificationToken.  This is a familiar approach being used to deal with cross-site request forgery prevention, which is familiar to anybody who's been writing secure web applications.  Many frameworks now just build this into the framework.  It's a mess, but it's the best thing we have right now currently.



Anyway, so he talks about how both the cookie and the body carry a request verification token.  He says they're not identical, but they're paired such that, when the server receives the request, it checks to see if both values exist and if they have been previously paired together.  They belong together.  If not, the request is rejected.  He says:  "This works because, while the one in the cookie will be automatically sent with the request regardless of its origin, in a forged request scenario the one in the body would need to be provided by the attacker, and they have no idea what the value should be.  The browser's security model ensures there's no way for the attacker causing the victim's browser to visit the target site, generate the token in the HTML, then pull it out of the browser in a way that the malicious actor can access."



He says:  "At least not without a cross-site scripting vulnerability, as well; and then that's a whole," he says, "that's a whole different class of vulnerability with different defenses."  Anyway, the point is that we're living in a world where the use of cookies as our state management is vulnerable to exploits that clever hackers have come up with over time.  So the change that Google has made in Chrome 80 to change the default to a way that blocks this class of cross-site scripting forgeries is expected to have some consequences.



So I salute Google for making the change.  Microsoft plans to follow, although I think they're probably going to stand back a little bit and wait to see what happens.  And Mozilla has indicated that it supports the idea also.  In some reporting that I saw of this, it turns out that OpenID apps may be breaking.  Microsoft warned very early on that these SameSite changes would break sites and applications that rely on OpenID-based federation.



Erik Anderson wrote, in a July 23rd Chromium Forum post on the use of cookies with SameSite, he said:  "We love the intent and spirit of this change, but we fairly quickly determined that this breaks a large number of our sites leveraging Azure Active Directory (AAD) and Microsoft Account (MSA) authentication using the contract as defined here.  We suspect that other OpenID-based federated auth providers may have similar scenarios and be broken."



Google warned IT professionals in its October 23rd Chromium blog post that there could be problems with internal applications and single sign-on implementations.  They said:  "Enterprise IT administrators may need to implement special policies to temporarily revert Chrome Browser to legacy behavior if some services such as single sign-on or internal applications are not ready for the February launch."  And that was last week.



Microsoft has issued a specific warning about the coming SameSite changes.  They said effects could be felt when using Microsoft Teams client applications.  They wrote:  "There are considerations for sites that use ASP.NET.  Exchange Server, SharePoint Server, and Skype for Business client will all need to have the latest updates installed."  So this is not something that has taken the industry by surprise.  However, I wouldn't be surprised if it takes some people who haven't been paying attention or who haven't been keeping these applications updated.



So essentially people like Microsoft with ASP.NET, Exchange Server, SharePoint Server and so forth, they recognized this was happening, and they changed their technology to be compatible.  For example, it may have been nothing more than saying explicitly SameSite=None, adding that attribute to cookies where they know that will not introduce a vulnerability.  That allows the legacy third-party behavior to still continue, and it overrides the default if they didn't have SameSite equal something, which changed last week in Chrome.



Microsoft warned in a Microsoft Teams and SameSite cookies document that:  "Applications running in the Teams desktop client are incompatible with the SameSite=None attribute, and they will not work as expected."  They said the document offered a couple of workaround options.  It also explained that the Secure attribute needs to be used when the SameSite attribute's value is set to None - that's that second thing that got changed here - to assure that third-party cookies won't get rejected.



For sites using ASP.NET or ASP.NET core, Microsoft warned in an October 18th ASP.NET blog post that the new SameSite changes will be in effect with .NET 4.7.2 and .NET Core 2.1 and above, and they could break OpenID Connect logins.  Updates to .NET that were released back in December and November added support for the new SameSite behavior.  So again, as long as everybody's platforms have been kept current and are current, this should have gone smoothly.  I imagine people may already have discovered that things like authentication, among other things, are broken because, as we've talked about in the case of OpenID, you're bouncing the user around and deliberately using some of these browser features in order to maintain the connectivity of all these pieces.  Unfortunately, that breaks unless it has been updated in the months preceding Google's change.



So we see another example here of security is hard, and change is even harder.  But tightening up the default behavior of cross-site cookies will clearly be a good thing moving forward.  And if we learned anything, it's that until there is some actual pain, some actual breakage, no one will change anything.  So this is why I salute Google.  They're biting the bullet.  They're being willing to break a few eggs in order to force the changes that will in the end significantly improve the security of web applications for everyone.



So we end up with Chrome, you know, going first; applications, some things, some corner things, some edge cases probably getting broken.  It's like, what?  Why can't I authenticate anymore?  Or why is my web app no longer working?  I imagine the person will find out rather quickly what's going on.  And then it's just, I mean, it's not like making the change is hard.  If you're sure you're not going to introduce or perpetuate a security vulnerability, you just add SameSite=None in order to revert to what was the default behavior.  But at the same time the reason that's been changed is to begin making some meaningful security improvements to cookies.  This does that.  And again, it makes cookies less promiscuous.  And I salute Google for leading the way.



LEO:  Yeah.  I mean, I'm skeptical.  I feel like they've got an alternative that'll work just as well for them and their advertisers.  And so in a way this is just pulling, you know, we've got our method.  You guys shouldn't use that other method.  Sorry.  We won't use that.  Well, but it's better than nothing; right?



STEVE:  Yeah.



LEO:  They have their own fingerprinting techniques.



STEVE:  Well, and the other browsers will be following suit, too.  So Firefox recognizes that, you know, this is going to be a good...



LEO:  The difference is Google's an ad company.  Firefox is not an ad company.



STEVE:  Right.



LEO:  So what I'm saying is Google's got its own means, and it doesn't really need third-party cookies.  And, yeah, I'm skeptical.



Hey, good show.  Thank you.  Lots to talk about.  You could find the show, along with SpinRite, the world's best hard drive and recovery utility.  Hard drive maintenance, I left a word out, maintenance and recovery utility.  It's GRC.com.  That's Steve's home on the Internet.  He has 16Kb audio for the bandwidth impaired, he has 64Kb audio for people with two ears, and he has a beautifully written transcript by Elaine, all at GRC.com.  While you're there, not only SpinRite, but lots of free stuff like ShieldsUP! and all sorts of wonderful useful tools, Perfect Paper Passwords and on and on and on.  GRC.com.  He's @SGgrc on Twitter, if you want to follow him, and you can leave him messages there.



We have a copy of the show, too.  We have audio and video at TWiT.tv/sn.  And there's a YouTube channel.  But, you know, honestly, the best thing to do is subscribe.  Get your podcaster pointed toward our feed.  Actually, if you search for TWiT on your podcast application, just press subscribe, subscribe, subscribe, subscribe, subscribe.  Get all the shows, automatically downloaded.  The minute you need them, you've got them on your stuff.



Steve, we will be back here next Tuesday, 1:30 Pacific.  We should be on time next week.  We were a little delayed by the Samsung event.  1:30 Pacific, 4:30 Eastern, 21:30 UTC.  Live stream's at TWiT.tv/live.  Chatroom at irc.twit.tv.  Join the kids in the back of the class as they throw spitwads at us.  And I will see you next week, right here.



STEVE:  Yup.  We have President's Day holiday observation on Monday.  And so you and I will be here the following day, on Tuesday.



LEO:  See you then.  Thanks, Steve.



STEVE:  Right-o, buddy.  Bye.



Copyright (c) 2020 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




GIBSON RESEARCH CORPORATION		https://www.GRC.com/



SERIES:		Security Now!

EPISODE:	#754

DATE:		February 18, 2020

TITLE:		The Internet of Troubles

HOSTS:	Steve Gibson & Leo Laporte

SOURCE:	https://media.grc.com/sn/sn-754.mp3

ARCHIVE:	https://www.grc.com/securitynow.htm



DESCRIPTION:  This week we continue following the continuing agony surrounding this month's increasingly troubled Window Update.  We examine several significant failures which have befallen Windows 10 users after applying the month's "fixes," which have had the tendency of breaking things that weren't broken in the first place.  We look at the danger presented by a very popular GDPR-compliance add-in for WordPress sites.  We look at an eye-opening report about the stresses that CISOs are being subjected to, and also today's pilot test of Microsoft's new ElectionGuard voting system.  We then touch on some SQRL and SpinRite news before taking a close look at two newly revealed IoT - Internet of Troubles - security worries.



SHOW TEASE:  It's time for Security Now!.  Steve Gibson's here.  Yes, more woes.  And this time, not just Windows 7; Windows 10, too.  We will talk about the troubling life of a CISO.  If you're one, we have the deepest sympathy for you.  And then the trouble with the Internet of Things, including a new Bluetooth flaw named after the son of Harald Bluetooth.  It's all coming up next on Security Now!.



LEO LAPORTE:  This is Security Now! with Steve Gibson, Episode 754, recorded Tuesday, February 18th, 2020:  The Internet of Troubles.



It's time for Security Now!, the show where we cover the latest security and privacy updates with this guy right here.  He's the man in charge at the GRC, the Gibson Research Corporation.  I give you Steve Gibson.  Hello, Steve.



STEVE GIBSON:  Leo.  You have said that 754 times.



LEO:  Holy cow.  I don't know, I think that intro isn't quite as old as the show is. 



STEVE:  That is actually true.  I was thinking the same thing.  So this was one of those weeks where initially, as I pulled everything together, nothing really stood out badly until I did some digging into a couple of the stories, and I thought, okay. So we've got two very worrisome IoT things.  So this podcast this week is the Internet of Troubles.



LEO:  Oy.



STEVE:  We're going to continue following the continuing agony surrounding this month's increasingly troubled Windows Update, in this case affecting Windows 10 people because Windows 7 people, they had their problems in January.  Now it's Windows 10's turn in February.  The monthly fixes have had a tendency to break things that were not broken in the first place.  So we're going to take a look at that.



We're going to look at the danger presented by a very popular GDPR compliance add-on for WordPress.  Also we're going to take a look, as I was talking to you before we began recording, at an eye-opening report about the stresses that are being subjected or that CISOs are being subjected to.  It turns out the average employment duration is, I think, well, the report says it's 26 months because, I mean, it's just so bad being a CISO.



LEO:  The time to breach.



STEVE:  Yeah.  And then it's your fault.



LEO:  It's your fault.  Goodbye.



STEVE:  Yeah.  Also today it's Tuesday, as you know.  As is often said, if it's Tuesday, somewhere there's an election.  Well, it turns out that during today there is a pilot test of Microsoft's new ElectionGuard voting system.  I had no expectation it was going to happen this quickly.  And in fact, it's as a consequence of the concerns over voting security that this thing has just, like it fast-tracked itself into existence. So we've got some fun things to talk about there.  We're also going to briefly touch on a little bit of SQRL news; some SpinRite news; and then, as I said, take a close look at two newly revealed Internet of Troubles security worries.  So I think I can promise our listeners another great episode.



LEO:  And a completely incomprehensible Picture of the Week.



STEVE:  Yes.



LEO:  You're going to explain it to us.



STEVE:  Yes.



LEO:  It'll all make sense when Steve is done.  And that's actually kind of the motto of the show.  It'll all make sense.



STEVE:  So I cannot really describe our Picture of the Week.



LEO:  I can't, either.



STEVE:  And when it was sent to me, I thought, okay, wait a minute.  You know how like sometimes photos are rotated 90 degrees.  I thought, okay, wait.  Is that the floor?  Or is that the ceiling?



LEO:  What the hell is this?



STEVE:  And then I saw like the drop ceiling with the bars, so I figured, okay, that's got to be the ceiling.  So that means that's the wall that meets the ceiling.  The clincher was down in the distance there's like an emergency battery backup lighting thing on the wall.



LEO:  Yeah.  So this is the upright wall.  And coming out of the ceiling is a cable bundle and a track, a fairly hefty cable bundle.  It looks like a monitor mount or something on the wall.  But then what's this thing sticking out that has its own power supply and a plug just dropping down to the floor?  Whatever it is, it's not pretty.



STEVE:  No, it's a catastrophe.  So that black bracket that has been attached to the wall...



LEO:  Yeah.



STEVE:  ...is a small, it looks like maybe, what, maybe like a 4U high 19-inch rack thing.



LEO:  Oh, I get it.  There was no room to put the rack unit in the rack.



STEVE:  Correct, because the rack that they had...



LEO:  Was too shallow.



STEVE:  ...was too shallow.  It looks like maybe eight inches deep or something.



LEO:  Oh, I'm getting it now, yeah.



STEVE:  So some, dare I say "enterprising" techie thought, well, that's not a problem.  And this must be a switch or, you know, it's some sort of an appliance, a network appliance.



LEO:  It could be a UDP.  There's all sorts of things it could be.



STEVE:  Well, it's got all of that networking plugged into it.



LEO:  Right, right.  Could be a switch, right, yeah.



STEVE:  And so, you know, yeah, it's got to be like a big switch.  So whoever did this screwed it to the rack so that it's sticking out with no support out into the air.  And then of course the power plugs into the back of it.  So the power cord is hanging straight down in the walkway path.  And I think this thing, this other beige thing must be a power outlet box because it looks like there's maybe something plugged into it.  I mean, it just is like - anyway.  So our topic, one of the things we're going to talk about is the dilemma that CISOs are in.  And if a CISO did this, well, I guess they deserve whatever stress they're feeling because this looks like it's going to just...



LEO:  This is just jerry-rigged.  They put it in backwards.  It's just crazy.



STEVE:  Really is bad.



LEO:  That's hysterical.



STEVE:  So don't try this, not only at home, but at work.



LEO:  Anywhere, especially at work.



STEVE:  Anywhere.  Never do this.



LEO:  I could see that in my house, but not at work, no.



STEVE:  Yeah.  So, and we have had some pictures of wiring closets from hell in the past.  So where, like, the network cables are used to hang socks that are drying and similar things.



So anyway, it turns out that the Windows 7 "You don't have permission to shut down Windows" is not restricted to Windows 7 after all.  Windows 10 users have also been receiving the same permission denial.  We have a picture of a Windows 10 dialog saying "You don't have permission to shut down and restart this computer."



LEO:  Crazy.



STEVE:  So as are many weird things that only affect some subset of Windows users when they're triggered by a Windows Update, this also, this whole problem, this whole "You don't have permission to shut down Windows," whether it's hitting a Windows 7 user or a Windows 10 user, turns out to be a subtle interaction with some third-party software, which of course explains why it only affects a percentage of the population.  In this case, the culprits are background services installed by Adobe's Creative Cloud.



LEO:  Oh.



STEVE:  Yup, so it's only people who have Adobe's Creative Cloud that in some weird way collided with a Windows Update at the beginning of the year to cause a denial of permission to shut down and restart the computer.



Bleeping Computer provided some comprehensive coverage of the situation.  They wrote:  "Windows 10 users are reporting being affected by a bug that prevents them from shutting down their devices without logging out first, an issue that we previously" - we the industry - "thought only Windows 7 customers were experiencing.  On February 6th, Windows 7 users started reporting encountering 'You don't have permission to shut down this computer.'  Since then," they wrote, "this error has been reported by several Windows 10 users, too, one of them saying that he saw the error pop up on a recently installed device running Adobe Creative Cloud," as initially reported by some guy named Gnter Born.



Others also confirmed that the issue was impacting their Windows 10 Home edition devices, as well as multiple Windows 10 installations in an environment where Windows 7 devices were also experiencing shutdown issues, naturally because in that environment Adobe Creative Cloud had been installed both on Windows 7 and Windows 10 systems.



They wrote:  "There are currently hundreds of user comments in Reddit threads, as well as on the Microsoft Answers forums and Twitter.  While the shutdown issues aren't as widespread on Windows 10 as they are on 7, all reports point at the same error and the same underlying bug being behind the problems."



Then last Thursday, on February 13th, a Microsoft employee posted:  "We've identified and resolved the issue, which was related to a recent Adobe Genuine update that impacted a small number of Windows 7 users.  Adobe has fully rolled back the update automatically for all impacted customers.  No action is needed by customers.  If you're experiencing the issue, it will be resolved shortly via an automatic update from Adobe."



LEO:  This kind of makes sense because it felt like something, some background process was blocking; right?



STEVE:  Yup, right, right.  And then, finally, Bleeping Computer added:  "While Adobe has already rolled back the update for Windows 7 customers, Windows 10 users are out of luck until the bug is also acknowledged for their platform and a fix is provided by either Adobe or Microsoft."



So it turns out that what Adobe - and I guess at some point Adobe will do that, but they hadn't as of when this most recent report was rolled out.  However, Bleeping Computer said until then, you can disable - well, first of all, you can log out and then shut down, although that's kind of a pain.  You can disable the Adobe services which are triggering the bug which, you know, go into Manage Windows into Services.  You'll then find Adobe Genuine Monitor Service, Adobe Genuine Software Integrity Service, and Adobe Update.  You could just halt those, stop them, disable them, whatever, and then later turn them back on after Adobe fixes the problem, or after you learn that it does.



In yet another problem, bizarrely, Windows 10 One Button PC Reset turns out to fail after this month's Patch Tuesday.  The regular monthly rollup is KB4524244.  And so it turns out - and, you know, this is not likely to affect lots of people.  The Windows 10 One Button PC Reset, for those who have never encountered it, is a means, is something that's offered in Windows 10 which, if for some reason you want to just return your Windows 10 machine to factory settings, meaning a complete reset of the system, you know, there is like a Windows recovery partition compressed and hidden on a Windows 10 machine.  And by jumping through some hoops, and you can get to it through menus, I've had occasion to do it myself, you can get to this option that allows you to completely return your system to sort of like new condition.



It turns out that Microsoft introduced this problem by resolving a possible security vulnerability that might be introduced by third-party UEFI boot managers which as a side effect turned out to kill Windows 10 One Button PC Reset.  Yes, Leo, I can hear you, and I feel the same way.  It's like, this whole thing is just beginning to collapse under its own weight.  So Microsoft has pulled that update and has no plans to reissue it.



As Microsoft explained it:  "You might restart into recovery with 'Choose an option' at the top of the screen with various options, or you might restart to your desktop and receive the error 'There was a problem resetting your PC.'  Anyone experiencing this problem is advised to simply remove the update from their affected Windows 10 machine."  So again, they're like, you know, they rescinded it, but it's unlikely to affect that many people.  If it does, you can remove that update.



But get this one.  This one is causing some serious concern.  Ever since this Patch Tuesday, some unlucky Windows 10 users, and I've not seen any forensics on this yet, they're reporting that another of the updates in the rollup is causing an additional problem that is very worrisome.  According to reports, a bug in, in this case it's KB4532693, hides their user profile and all of their user data.



LEO:  This is such a nasty one.



STEVE:  Yes.  Because, I mean, you know, Microsoft is selling Windows 10, well, or pushing it, to people who are just, you know...



LEO:  To normals, yeah.



STEVE:  Normals, yes, exactly.  And so it's being reported on Microsoft Forums, Twitter, Reddit, tech sites including AskWoody, Bleeping Computer, and BornCity.  Users are reporting that, after installing the standard monthly update rollup, they're just doing their normal, you know, Microsoft makes you do it now, so okay, fine, and it doesn't take that long for a regular monthly update.  However, after doing this, they can no longer view or access their original Windows 10 profile.  In other words, according to these reports, after the update users are left logged into a blank default Windows 10 profile where all of their previous data is missing.



LEO:  That's scary.  Now, it's not actually  missing.  You're just logged into the wrong profile.



STEVE:  Right.  Well, it's not deleted, essentially.



LEO:  Right.



STEVE:  Yeah.  So all installed apps, desktop wallpapers, desktop files, downloads, you know, everything that, you know, for those who don't know, your whole profile is what makes you different from someone else logging into the same machine under their own profile.  The profile is your per-user account.  So as you can imagine, it's quite unsettling to just update Windows, as you're told you should, and then all your stuff is gone.



Last Wednesday Woody Leonhard tweeted:  "Multiple reports that the Feb Cumulative Update for Win10 resets the desktop - custom icons missing, background set to Windows logo - and would not recognize the established logon account."  He asks, "Are you seeing the same?"  And then sent a link where presumably people could respond.  So as you said, Leo, the good news is nothing was permanently deleted.  The data's not gone.  It's only been hidden.



According to a report on Bleeping Computer, the bug is caused by a faulty, as I said, KB4532693 installation procedure.  The bug occurs when the Windows Update service creates a temporary profile for use by the installation procedure, but then - whoops - forgets to remove it after finishing the update installation.  When the update finishes, this temporary profile remains the one that users are logged into, and all their stuff is gone.



Reports indicate that the original user profile folders are still available on disk, but that they've been renamed with a .000 or a .bak extension.  And while it's technically possible to recover these sequestered profiles by renaming and relinking them, the steps required are error prone.  And as you said, Leo, this is, you know, many people are just normals.  They're like, where did all my stuff go?



LEO:  Oh, yeah.  I've had calls like this, and it's terrifying for them.



STEVE:  Yes.



LEO:  Because it's not clear it's not gone.



STEVE:  Right, exactly.  Well, it looks gone.  And it's like any normal person would go, all my stuff is gone.



LEO:  Right.



STEVE:  So it turns out that the best solution is to uninstall the faulty update that's part of the rollup, KB4532693.  Just uninstall it.  Multiple users have reported that removing the faulty update restores their old profiles.  So anyway, all that said, it's obvious that not all Windows 10 users are impacted by this bug.  So, yes, most users will have no problems at all.  Who knows what's going on?  All of my Win10 machines are current with the February rollup, and I didn't see that happen to me.  But it clearly is happening to many people.



So as we know, given the problems that Microsoft is now continually having every month, the advice would be don't immediately jump on one of these rollups.  What that means is you may be planning to.  The good news is, if this does happen to any of our listeners, everybody who hears this will know, okay, all is not lost.  Nothing is lost.  You can just uninstall this 4532693, and you'll get your stuff back.  But, boy, Microsoft, I don't, I mean, yeah, I don't know how we explain the continuing problems that Windows is having.



Leaving them alone for a while, there is a very popular GDPR Cookie Consent WordPress plug-in which has a critical flaw.  It's of some concern because more than 700,000 active installations exist of this problem.  So anybody who's listening, and if you know anybody who has a WordPress site, and you may know that they've got this GDPR Cookie Consent plugin installed, it needs to be updated to version 1.83 or later with high priority, due to a serious vulnerability.



As its name implies, it serves as an aid to making websites compliant with the EU's GDPR regulation.  But since its February 10th update, it has also been critically, I'm sorry, until it was just updated on the 10th, so a little over a week ago, last Monday, a week ago Monday, until a week ago Monday, February 10th, it has been critically flawed in a fashion that, if exploited, could enable attackers to modify the site's content and inject malicious JavaScript into any of these 700,000 active installations.



I don't use it myself on my WordPress blog, so I wasn't able to verify whether this has automatic update technology as part of it.  So what I would recommend, any of our listeners, if your WordPress site uses this GDPR Cookie Consent plugin, make sure that it is at 1.83 or later.  After the developer was notified of the critical flaw by the people who found it, the plugin was removed from the WordPress.org plugin directory, saying "pending a full review," according to the plugins directory page.  And now the new version, 1.83, was released by the plugin's developer Cookie Law Info, as I said, Monday before last.



The vulnerability stems from improper access controls in an endpoint used by the WordPress plugin's AJAX API.  AJAX, for those who don't know, is the acronym for a technology which allows JavaScript running on a page to independently initiate its own outbound HTTP and other connections for retrieving data for use by that JavaScript.  For example, I use it myself on the SQRL login page.  That's what creates that magic where, without even touching the login page, after you authenticate with your smartphone, the login page that is there, that is present, spontaneously updates to show that you're logged in.



What's happening is that there's a so-called AJAX query running on that page in the background which is periodically pinging the site's server that you're logging into for a new URL for the page to jump to.  AJAX scripting cannot ask for anything from anywhere, thank goodness, or we'd be in real trouble.  It's constrained by same-origin rules, so it can only request additional information from a site that meets the same-origin restrictions, typically just the site that provided the page.  So it's just sort of like it allows sophisticated actions.  It's behind the scenes of many of the web applications that we've now grown to take for granted.



But in this case this plugin has a method "_construct" which is used by initializing code in the plugin for creating new objects.  It constructs the objects.  And unfortunately, this fails to implement required security checks.  Because of this, that AJAX endpoint, which is only intended to be accessible to administrators, is allowing visitors to the blog to perform a number of actions that can compromise the site's security.  It accepts three different values from the AJAX API.



Two of them, one of them is "save_contentdata" and the other is "autosave_contant_data," turns out can be leveraged for exploitation by an attacker.  The mistakenly accessible save_contentdata allows admins to save the GDPR cookie notices to the database.  However, since the method is not checked for authentication, any visitor to the site can also modify any existing page or post, meaning most of the website.  And as is so often the case with HTTP, it's possible to delete or change the page's content.  Injecting content then allows reformatting of text, local or remote images to be obtained, as well as hyperlinks, shortcodes.  You could embed a frame that then pulls content from somewhere else and get up to all kinds of high jinks.



And then that second problem, the autosave content data, is used to save the GDPR cookie info page in the background while the admin is editing it.  And it turns out it saves it into a sensitive database field, skipping any validation checks.  And again, this opens it for abuse.  So not surprisingly, the researchers who discovered it urge WordPress plugin users to update immediately.  And it was that language that concerned me that this might not have an auto update mechanism.  I may have some additional news on that by next week from people who do have it and have some experience with it.  But 700,000 WordPress sites is not nothing.  And this allows lots of at least defacement of WordPress blogs and probably much more malicious modification.  So make sure if you're using that that you get it updated.



So it turns out the average tenure of a CISO, a C-level Information Security Officer, like a CEO and CFO and so forth, is just 26 months.



LEO:  Wow.



STEVE:  Due to high stress and burnout.



LEO:  Yup.



STEVE:  It turns out that the vast majority of interviewed CISO executives, 88% of the 800 that were interviewed report high levels of stress.  One third report stress-caused physical health issues, and half report mental health issues.  We've touched on this in the past.  Information security, cybersecurity, is still a relatively new thing.  It's still seen as more of a necessary evil by corporations than as an obvious profit center, like sales, marketing, or R&D.  Not to mention the fact that the high priests of information security appear to speak in a strange language that makes no sense to any of the other C-suite executives who are the ones, after all, who establish the budgets and the schedules.



So as a consequence, most companies are still not ready to truly embed CISOs into their company culture and into their day-to-day operations.  They're sort of like, you know, it's like that guy who hangs out in the wire closet.  So today CISO jobs come with low budgets, long working hours, a lack of power on executive boards, a diminishing pool of trained professionals that they're able to hire, and a constant stress of not having done enough to secure the company's infrastructure against cyberattacks.  There's also continuous pressure due to new arising threats.  I mean, look at what we talk about every week.  This stuff is real, you know, like oh my god, you'd better not have Remote Desktop Protocol exposed.  And if you do, it's real trouble.



It's funny, as I was thinking about this and looking at and reading over this report, it really struck me that this is like the problem with asymmetric warfare.  You know?  It's not like the British lining up in a row in very brightly colored uniforms with their muskets and like, okay, now we're going to - when we say "go," both sides are going to charge each other.  You know?  This is asymmetric.  This is bad guys all over the world trying to pry their way in through a nook and cranny.



And I'm glad I don't have responsibility for a large organization, as I've, you know, back when we were talking about the Sony hack, Leo, I said very clearly I don't think it's possible to secure an organization like that.  So imagine having the responsibility for the security of a major firm on one's shoulders.  I just - I can't imagine it.  So anyway, it turns out that through the years CISOs have often pointed out the problems with their jobs and the stress and damage that those jobs inflict upon them and their families.  But there's been no conclusive study up until now to support what were essentially these broad assertions.



Last November Nominet, they're an Internet and DNS security firm that we've talked about before, the name's familiar to me, I forgot exactly what their product is, but N-O-M-I-N-E-T, they independently surveyed 800 CISOs and executives from companies in the U.S. and the U.K. to explore and examine this topic and to determine how much of a role stress plays for CISOs across the industry.  The survey's results painted a gloomy picture about one of today's most in-demand jobs.  I mean, it's no longer the case that a company can go without somebody whose job is to focus on the security of the company's Internet and networking infrastructure.



So what the report found, I have a link to the entire report in the show notes.  I think I do.  I don't see it here.  Oh, yeah, back at the top of this topic is there.  So what the report showed, 88% of CISOs reported being between moderately to tremendously stressed.  Eighty-eight percent.  Nearly half, 48%, said that work stress has had a detrimental impact on their mental health.  Forty percent of CISOs said that their stress levels had affected their relationships with their partners or their children.  Thirty-two percent, essentially one in three, said that their job stress levels had repercussions on their marriage or romantic relationships.  The same, 32%, said that their stress levels had affected their personal friendships.  Twenty-three percent of CISOs said that they had turned to medication or alcohol as a means of dealing with the stress from their job.



Nominet, who prepared the report, said that even when they are not at work, many CISOs feel unable to switch off.  As a result, CISOs reported missing family birthdays, holidays, weddings, and even funerals.  The report said they are also not taking their annual leave, their sick days, or time for doctors' appointments, which contributes to physical and mental health problems because of course, if you never feel like you have enough time in the day to get the work done that you need to get done, you're going to keep pushing those things off.  Nominet said that while investigating the cause of CISO stress, they found that almost all CISOs were working well beyond their contracted hours by an average of 10 hours of extra time per week, for which they were not compensated.



Furthermore, many were under pressure from their boards.  Almost a quarter of those interviewed said that boards didn't accept or understand that breaches are inevitable.  So what we know of that is, because we've often talked about this, that there's a culture clash.  There's those who don't get it, and the CISO who does get it.  He's not making excuses.  He or she is trying to explain this.  So the board said they don't understand that breaches are inevitable, and the CISOs reported that the boards would be holding them personally accountable for any security incidents.  So you can imagine how that affects stress levels.



Nominet said that 29% of CISOs who answered the survey said they'd be fired in the event of a breach, while 20% said they'd be fired anyway, even if they were not responsible.  So the answers explain why most CISOs don't last in their jobs more than an average of 26 months, and why 90% of those surveyed were willing to take pay cuts if they could reduce stress levels.  Nominet said CISOs were willing to give up, on average - and the reason this is a weird number is that it's an average of the responses - on average, $9,642 per year, so almost 10K per year, on average, just to reduce stress levels and improve their work-life balance, which many CISOs said they had problems with.



Nominet's numbers may seem staggering to someone looking in from the outside, but of course they come as no surprise to anyone working in the field who realizes what a mess this is right now.  Although the Nominet study only surveyed high-ranking CISO executive jobs, the problem is widespread across the industry.  Infosec, or cybersecurity, has a habit of grinding through employees due to the rigors of the job.  They said low-level infosec positions like threat analysts or pen testers, penetration testers, are just as bad in terms of stress, if not worse, primarily for the same reasons - a constant fear of new incoming attacks, long work hours, low pay, and very little job satisfaction.



So I realize I'm not doing a lot to sell everyone here on the glories of being in charge of Internet security for your organization.



Within the infosec community, signs of the growing problem of stress and burnout leading to mental health issues have been mounting.  There are some efforts underway to raise awareness about infosec job stress levels - like this report - burnout, along with the mental health issues arising from ignoring this problem.  There has also been a rise of so-called "Mental Health Hackers," an online community that's been attending cybersecurity conferences on a regular basis now in order to help raise awareness of the topic, that it's a real thing.



And I don't see any obvious solution to the dilemma, other than time, frankly.  The problem ultimately is one of respect.  It's impossible for other C-level executives to respect what they do not understand.  Traditionally, as we know, nerds and geeks have enjoyed sort of keeping their dark arts a secret.  But being understood is vastly more valuable than being mysterious.  So part of the job should be to explain and train other C-level execs so they can better understand what the job is all about.  And of course, fortunately, time and additional experience with the realities of cybercrime are going, regardless, to slowly bring about a cultural attitude change.



We know that that's in process now.  When you have, like, last year's massive, widespread, in the popular press coverage of ransomware attacks, there's no way that C-level board executives are not perforce becoming more aware that this is something happening.  And so, yeah, when it happens to them, they realize it's not an isolated incident.  There is an aggressive pressure for this to happen.  So unfortunately, as we also know, this sort of change takes time.



So I guess my advice to CISOs would be to as much as possible try not to carry the entire organization's cybersecurity responsibility on your own shoulders.  I'm sure that's easier said than done, but try.  And also try to retain a sense of perspective as much as possible.  In the end, it is just a job, and your life is yours.  Try not to give it away.  So yikes.  I mean, it's nice to see a report like that, that helps to put some numbers to a problem that we sort of say, oh, yeah, boy, being in charge of security is stressful.  Well, yeah.  It's really stressful.



One thing that's helping to take some stress out is Microsoft's ElectionGuard, which as I mentioned at the top of the show, this being a Tuesday, is being used for the first time today.  As the saying goes, if it's Tuesday in the U.S., there's an election somewhere.  And in this case that somewhere is the small town of Fulton, Wisconsin.  What's making history there today is that the residents of Fulton, Wisconsin will be electing representatives for the Wisconsin Supreme Court using voting machines for the first time powered by Microsoft's ElectionGuard software.  These are the first voting machines deployed in any U.S. election that will be running Microsoft's new voting software, which we've been keeping an eye on on the podcast since the summer.



Recall that ElectionGuard is a fully open SDK that Microsoft has made available at no charge on GitHub:  github.com/microsoft/electionguard.  The project's goal is to create voting software that uses strong encryption, actually massively cool encryption.  It's built by some of the world's top cryptographers.  And it allows it to be, thanks to being open source, extensively audited for bugs.  I was very surprised when I saw this news because it was just in May of last year, 2019, that Microsoft announced the existence of this for the first time.  They then first demonstrated their prototype voting machines to a small audience of the Aspen Security Forum last July.  Then they released the first ElectionGuard code to GitHub in September, and opened a bug bounty program the following month, last October.



Today's pilot test is deliberately small, expected to have only a few hundred voters casting ballots.  But this will provide voting machine vendors, as well as quite anxious U.S. election officials, with a real-world test of the software to see whether it's worth a shot and ready for wider deployment.  Before today's event, Tom Burt, who's Microsoft's VP for Customer Security and Trust, thus in charge of this, said that using ElectionGuard won't be complicated, since Microsoft designed the software from the ground up for ease of use, accessibility, and with a user-friendly interface.  He explained that the voting experience is a three-step process.



First, a voter will select candidates on a touchscreen and verify their choices.  Then the voter will print and review for accuracy a paper ballot and simultaneously receive a separate tracking code.  Finally, the voter deposits their ballot into a ballot box for counting.  And presumably this is an electronically scannable paper ballot.  But as we've described, there's a lot of wonderful, quite advanced crypto technology happening behind the scenes.  After casting their ballot, each voter receives that tracking code.  They are able to use that tracking code on an election website to verify that their vote has been counted and that the vote has not been altered.  In other words, that tracking code lets them see their votes.  The tracking code, however, does not reveal the vote, so it won't allow third parties to see who voted for whom.



ElectionGuard employs a homomorphic encryption scheme which was developed in-house at Microsoft under the watchful eye of senior cryptographer Josh Benaloh.  Counterintuitive though it is, this homomorphic - I have a hard time pronouncing that - homomorphic form of encryption allows the counting of individual votes while never decrypting them.  They stay encrypted, yet they can still be counted.  What?  Yeah.  The ElectionGuard SDK also supports third-party verifier apps which are able to independently check that the encrypted votes have been counted properly and have not been altered.  The verifier apps were created for use by voting officials, the media, or any third party interested in the voting process and in adding their own verification to it.  And ElectionGuard machines can also produce, as in today's case, paper ballots as a printed record of their vote, which voters can then place inside traditional voting boxes, just like old-fashioned ballots.  And, finally, ElectionGuard supports voting through open accessibility hardware.  Apparently Microsoft has some Xbox-based controllers that are able to be used.



LEO:  Are you joking, or are you serious?



STEVE:  No, I'm serious.  They're Xbox-based controllers, yeah.



LEO:  It's not running on Windows; is it?



STEVE:  I don't know what it...



LEO:  It can't be open source.  It can't be.



STEVE:  Well, it could be open source running on...



LEO:  On top of Windows.  Yeah.



STEVE:  Yeah, exactly.  I mean, given Microsoft.  So the voting machines being deployed today in Fulton were built by VotingWorks at Voting.Works.  And Leo, if you go there, you will like what you see.  Their home page is exactly what we would want to see.  It states:  "Democracy is a choice.  VotingWorks is a nonpartisan nonprofit, building a secure, affordable, and," they said, "delightful voting system."  It's delightful.  "Our voting machine creates paper ballots that voters can directly verify.  Our risk-limiting audit software" - which of course is based on what Microsoft has done - "ensures votes cast on any paper-based system will be correctly tabulated.  Our source code is available on GitHub.  You can help by making a tax-deductible donation or joining our team."



And VotingWorks is not alone.  Other voting machine vendors including Smartmatic and Clear Ballot have also announced partnerships with Microsoft to build ElectionGuard-based voting machines; and a fourth group, Dominion Voting Systems, is also exploring the use of Microsoft's SDK.  I think this is a perfect storm outcome since, once officials see how this works, what it means for the systems to be open and auditable, and all of the new features that this system offers, no one who isn't doing this will continue being viable.  This makes the welcome and long-overdue end to proprietary closed voting machines, I think, just a given.



And good riddance to, you know, I want to say Diebold or Diebold, and Diebold is welcome to produce ElectionGuard-based machines of their own.  But they are going to have a hard time, I think, in the future selling anything that doesn't use this software.  We need this, I mean, this has to be the way it's being done moving forward.  So, you know, big, big bravo to Microsoft for doing this, putting it out there, giving it away, making it open.  And to all those companies that have jumped on it and said, hey, we see the writing on the wall.  We need to support this, or we're not going to be able to sell our stuff in the future.  So yay.  Isn't that cool?



LEO:  Yeah.  It uses TypeScript.  I'm guessing it might not require Windows, though.  Might be able to run it on something else because it looks like it's all web technologies.



STEVE:  Nice.



LEO:  Yeah, [crosstalk] Python.



STEVE:  If it's secure, nice.



LEO:  Yeah.



STEVE:  Very cool.



LEO:  Yeah.



STEVE:  I did want to note that we've added an implementation for SQRL.  To the growing list of SQRL implementations we now add a general purpose pure PHP implementation for, I guess it's Laravel, the PHP framework, L-A-R-A...



LEO:  Yeah, Laravel, yeah.



STEVE:  Laravel.  So there is now a pure PHP SQRL implementation for Laravel.  It's up on GitHub, and I have in the show notes a link to it, and also a thread over in the GRC forums.



I am at work on SpinRite.  I have no, of course, it's going to be a while before I have any deliverables.  I've just rebuilt the FreeDOS kernel.  Back when I stopped working on SpinRite, I had had to tweak the FreeDOS kernel a little bit because FreeDOS just assumes that when it boots up, all of the drives that it sees, it's going to be able to understand.  Well, it doesn't understand the GUI, you know, the GUID partition format.  It only understands the older MBR format.



And of course one of SpinRite's next features will be the ability to operate on any drives.  What happens then is that FreeDOS tries to essentially log onto all the drives it sees, and it freaks out in a very ungraceful manner.  So I had previously implemented a new config.sys option, skip init.  You just put skipinit=1 in the config.sys for DOS, and it no longer tries to initialize all the drives.  I found out that it was a little tricky to rebuild FreeDOS because one of the things I lost is compatibility with 16 bits when I went to a 64-bit Windows, unlike my old XP machine that was still able to run 16-bit code.  I've overcome that hurdle and just actually before the podcast I rebuilt the FreeDOS kernel.



Anyway, we're moving forward, and I will keep our listeners updated.  There is a page which the people who are participating over in GRC's newsgroups know about.  I'm not ready to make it widely public because it'll just generate too much traffic and too many questions at this point.  And I need to keep running everything through GRC's newsgroups and not have  Greg and Sue submerged in how do I do this and how do I do that questions anyway.  But if anyone is interested in participating, there is an active discussion now underway over at the grc.spinrite.dev newsgroup.  And you can figure out how to get there by going to GRC.com/discussions.  That's the page that explains about our newsgroups.



I actually revved my favorite newsreader for Windows.  It turns out it broke at the beginning of 2020.  Gravity was first written in the late 1990s.  But it is my absolute favorite newsreader.  Back then they added a sanity check for any headers in incoming postings, deciding that if they appeared to be older than 2020, then they must be bogus.  So of course, well, sort of a variation on the Y2K problem; right?  So starting January 1st, date sorting of threading broke.  And so the good news is  Gravity was released to the open source community, and I'm now maintaining a fork of Gravity that adds that and a number of other features that I have felt were missing for years.  And so there's news of that over at GRC.com/discussions for anyone who's interested.  So lots of fun stuff happening.



So it turns out that IoT light bulb vulnerabilities are not such a joke after all.  Our listeners know that I often joke about having our internal networks hacked and attacked by something as ridiculous-seeming as a light bulb.  I chose light bulbs, I guess, to receive that abuse over the general lack of attention to IoT security because they're pretty much the dumbest, lowest rung of the ladder and the least fancy IoT device we have.  Well, it turns out that the extremely popular Philips IoT light bulbs, the Philips Hue IoT light bulbs, or in this case actually the bridge that's part of the Philips Hue system, rather than the light bulbs themselves, are able to, in combination, expose our internal WiFi networks to bad guys.



The Hacker News had some good coverage of this.  They explained, well, they began by saying:  "There are over a hundred potential ways hackers could ruin your life by having access to your WiFi network that's also connected to your computers, smartphones, and other smart devices."  Right, we don't want bad guys on our WiFi network.



They said:  "Whether it's about exploiting operating system and software vulnerabilities or manipulating network traffic, every attack relies on the reachability between an attacker and the targeted devices.  In recent years," they wrote, "we've seen how hundreds of widely used, smart-but-insecure devices made it easier for remote attackers to sneak into connected networks without breaking WiFi passwords.



"In the latest research shared with The Hacker News, Check Point revealed a new, high-severity vulnerability affecting Philips Hue Smart Light Bulbs that can be exploited over the air from over 100 meters away to gain entry into a targeted WiFi network.  The underlying high-severity vulnerability, which is tracked as CVE-2020-6007, resides in the way Philips implemented the Zigbee communication protocol in its smart light bulb.  This leads to a heap-based buffer overflow."  Whoops.



So, okay.  As we know, Zigbee is the widely used mesh wireless technology that allows each device in a Zigbee group to communicate with any other device on that network.  And it's widely used.  It's the protocol built into tens of millions of devices worldwide, the Amazon device, the home hub device.  I'm reluctant to say the name because I don't want to set them off.  Samsung's SmartThings, the Belkin Wemo, and many more.  Lots of devices use Zigbee.



The Check Point researchers said:  "Through this exploitation, a threat actor can infiltrate a home or office's computer network over the air, spreading ransomware or spyware, by using nothing but a laptop and an antenna from over 100 meters away."  They also confirmed that the buffer overflow happens on a component called the "bridge" which is the module that receives remote commands sent to the bulb over the Zigbee protocol from other devices like a mobile app or the Amazon home assistant.  Due to its severity, that is, the severity of this problem that Check Point found, they have not revealed any technical details, nor are they providing any proof of concept for the flaw, in order to give users some time to apply patches.



But we have a blog from Check Point.  Of course they couldn't resist saying "The Dark Side of Smart Lighting" as the title for this.  They wrote:  "With the help of the Checkpoint Institute for Information Security (CPIIS) at Tel Aviv University, the researchers were able to take control of a Hue light bulb on a target network and install malicious firmware on it.  From that point, they used the light bulb as a platform to take over the bulbs' control bridge, and attacked the target network."



So they said, first, the attacker controls the bulb's color or brightness to trick users into thinking the bulb has a glitch.  The bulb appears as "Unreachable" in the user's control app, so they try to reset it.  The only way to reset the bulb is to delete it from the app, then instruct the control bridge to rediscover the bulb.  Apparently it's that action of the rediscovery that allows the bulb to then attack the bridge which has just rediscovered it.



They wrote:  "The bridge discovers the compromised bulb, and the user adds it back onto their network.  The hacker-controlled bulb with updated firmware then uses the Zigbee protocol to trigger a heap-based buffer overflow on the control bridge by sending a large amount of data to it.  This data enables the hacker to install their own malware onto the bridge, which is in turn connected to the target business or home network."  Malware then connects back to the hacker and, using a known exploit like EternalBlue or whatever, they're able to infiltrate the target IP network from the bridge to spread ransomware or spyware.



So this is a classic get in and then pivot attack.  So the bad guys take control of the light bulb, install their malign firmware onto it, make the light bulb go crazy so that its owner thinks, what the hell, it's gone nuts.  The owner deletes the bulb, re-adds the bulb to the system, and in that repairing process the bulb is able to infiltrate the shared hub, get its malware then onto the hub.  And the hub is now in a privileged position, being part of the WiFi network, in order to exploit the internal network from its vantage point.  And as I'm reading this, I'm thinking, wow, you know, all this from somebody who wants to have Philips Hue smart light bulbs and control them using wireless technology.



This research was first disclosed to Philips and Signify, who is the owner of the Philips Hue brand, last November, so a few months ago.  Signify confirmed the existence of the vulnerability in their product, and after about two months they issued a patched firmware version.  So anybody who has Philips Hue bulbs are going to want to take note of this.  You want to make sure this is the firmware you're using.  It ends in 4040, that's the easy way to - it's Firmware 1935144040.



LEO:  And they do over-the-air updates.  So the chances are you already do have it.  You don't have to do anything.



STEVE:  Yes, exactly.  I was curious about that, so I wanted to  make sure that that was done.  And there is meethue.com.  Check Point said:  "In a joint decision with Signify, we decided to postpone the release of the full technical details of our research in order to allow Philips Hue clients to have enough time to safely update their products to the latest version.  The full technical details of this research will only be published in our research blog in the upcoming weeks.  Stay tuned."



So anyway, I was curious to know, as you said, Leo, if there is auto update, to verify that.  There apparently are two bridges.  There's a Version 1 bridge, which is a rounded shaped bridge, well, they said round-shaped bridge, and a Version 2 which is a square-shaped bridge.  And that's the one that supports the Apple HomeKit.  And they wrote on their page, if you don't want to miss any improvements on quality, security, or performance, and you want your Hue system fully compatible with the upcoming new Hue products, please be sure that you enable automatic updates for your Hue system in the Hue app.  That's under Settings > Software Update > Automatic Update.  And I presume that's the default setting, right, Leo? 



LEO:  Actually, I should check.  That's what we've been saying, but I should check just to make sure.



STEVE:  Yeah.  And on January 13th of 2020, so last month, almost a month ago, they said Firmware 1935144040 for the Version 2 bridge, they said:  "We regularly update your Hue bridge to improve the performance and reliability of the system."



LEO:  My bridge is up to date, and I didn't do anything.  So yes.



STEVE:  Good.  This update includes a patch for a security vulnerability in the Hue Bridge v2.



LEO:  Make sure, it says, all your lights are powered on to get them up to date.  It can take up to one hour per light or accessory to download, and lights may briefly turn off while updating.



STEVE:  Wow, well, that's cool.



LEO:  What world we live in, huh?



STEVE:  I know.  Can you believe it, Leo?  It's like, okay, turn on the lights and wait an hour.  And I'm not surprised because it's going to be a slow - it's a low-power, bandwidth-constrained protocol.  And they're having to receive a firmware update through Zigbee.  So it's going to take a while.  



LEO:  Yeah.



STEVE:  But I'm glad to know that the light bulb itself, not just the bridge, can be updated.  That's a question I had, and I'm glad you answered it for us.



LEO:  And the weird thing is this was a problem earlier.  They didn't patch it correctly, apparently, and so it came back.



STEVE:  Whoopsie.



LEO:  Yeah.  But, yeah, you should automatically be updated.  And if you for some reason turned off auto updates, well, you might want to turn those back on.



STEVE:  Yeah.  And so the big takeaway here, and this perfectly factors into our next topic, our final discussion, the so-called "Sweyntooth vulnerabilities" which have just been disclosed, the utter importance that anything you have that which is IoT has an upgrade path.  I mean, I know that I always say this.  There's typically some reason to bring it up every single week.  But it is crucial that we have that.



So Sweyntooth.  The Sweyntooth vulnerabilities are a set of more than 12 newly discovered and disclosed, and notice I said "more than 12," across a wide range of Bluetooth devices.  Unfortunately, many of which will never be updated.



LEO:  [Sigh]



STEVE:  I know.  More than 480 individual products have been identified that are affected by this.  These allow for, among other things, full device compromise.  Only 12 of these vulnerabilities have been disclosed so far since some Bluetooth vendors - as you might guess from Sweyntooth, this is about Bluetooth - some Bluetooth vendors have not yet released updated SDKs, so more disclosures will be forthcoming.



But let's back up a bit.  First of all, I know everyone is thinking, Sweyntooth?  The etymology of Sweyntooth is not as immediately obvious as are many other named vulnerabilities.  In this case, Sweyntooth was formed from the names of Sweyn Forkbeard and his father, King Harald Bluetooth.



LEO:  Yeah.  King Harald was the Bluetooth, yeah.  So it's Bluetooth's son.



STEVE:  Turns out that King Harald Bluetooth, of course, being the namesake of our widely used Bluetooth technology, he was exiled by his upstart son, Sweyn.



LEO:  Don't you know.



STEVE:  Who revolted against his father, exiled King Harald, and shortly thereafter the king died.



LEO:  Oh, no.



STEVE:  So, yes.  Sweyntooth.  The discoverers of these vulnerabilities wrote that they "envision that if Sweyntooth-style vulnerabilities are not appropriately handled by BLE vendors, then the technology can become a breeding ground for attackers, which may in turn lead the Bluetooth technology to become obsolete."  I doubt that Bluetooth is going to become obsolete, but this is going to be a serious problem.  Their paper, "Sweyntooth:  Unleashing Mayhem Over Bluetooth..."



LEO:  Oh, geez.  Oh, no.  Mayhem?  Oh, man.



STEVE:  I love that.  I love the word.  "Mayhem" is one of my favorite words.  Mayhem.  That's just such a great word.  "Unleashing Mayhem Over Bluetooth Low Energy."  I've got a link to the disclosures that are posted on GitHub, a link to their 11-page PDF, which is a partial disclosure because they still have not heard from some of the Bluetooth vendors.  They start off their paper explaining.



"Sweyntooth captures a family of 12 vulnerabilities - more under nondisclosure - across different BLE software development kits of seven major system-on-a-chip (SoC) vendors.  The vulnerabilities expose flaws in specific BLE SoC implementations that allow an attacker within radio range to trigger deadlocks, crashes, and buffer overflows, or completely bypass security, Bluetooth security, depending upon the circumstances.  Sweyntooth potentially affects IoT products in appliances such as smart homes, wearables, and environmental tracking or sensing."



They said:  "We have also identified several medical and logistics products that could be affected."  There was one from Medtronics that I saw.  "As of today," they wrote, "Sweyntooth vulnerabilities are found in the BLE SDKs sold by major SoC vendors."  So when I was reading this, I was hoping that these were going to be kind of obscure, never really heard about them, don't know who they are so not a big deal.  Unh-unh.  TI. 



LEO:  Oh.



STEVE:  NXP that of course used to be Philips, Cypress, Dialog Semi, Microchip, STMicroelectronics, and Telink Semiconductor, all major suppliers of Bluetooth SoC system-on-a-chip devices.  "By no means is this list of SoC vendors exhaustive," they wrote, "in terms of being affected by Sweyntooth.  We have followed responsive disclosure during our discovery, which allowed almost all SoC vendors to publicly release their respective patches already.  However, a substantial number of IoT products relying on the affected SoCs for BLE connectivity will still need to independently receive patches from their respective vendors, as long as a firmware update mechanism is supported by the vendor.



"Sweyntooth highlights concrete flaws in the Bluetooth Low Energy stack certification process.  We envision substantial amendments to the BLE stack certification to avoid Sweyntooth-style security flaws in the future.  We also urge SoC vendors and IoT product manufacturers to be aware of such security issues and to initiate focused effort in security testing.  A proper classification of the vulnerability set is presented in the next section."



And I'll mention briefly they've got Crash, where vulnerabilities in this category can remotely crash a device by triggering hard faults.  They said:  "This happens due to some incorrect code behavior or memory corruption, for example, when a buffer overflow or Bluetooth Low Energy reception buffer occurs.  When a device crash occurs, they usually restart.  However, such a restart capability depends on whether a correct hard fault handling mechanism was implemented in the product that uses the vulnerable BLE SoC."



Second classification, Deadlock.  Deadlocks are vulnerabilities that affect the availability of the Bluetooth Low Energy connection without causing a hard fault or memory corruption.  Usually they occur due to some improper synchronization between user code and the SDK firmware distributed by the system-on-a-chip vendor, leaving the user code being stuck at some point. Crashes originated from hard faults, if not properly handled, can become a deadlock if the device is not automatically restarted.  In most cases when a deadlock occurs, the user is required to manually power off and power on the device to reestablish proper communication.  In other words, a deadlock kills the Bluetooth Low Energy device without the user interceding and shutting it down and powering it back up again, which of course still leaves it prone to re-attack.



And, finally, Security Bypass:  This vulnerability is the most critical one, they wrote.  This is because the vulnerability allows attackers in radio range to bypass the latest secure pairing mode of Bluetooth Low Energy, i.e., the Secure Connections pairing mode.  After the bypass is completed, an attacker within radio range has arbitrary read-and-write access to the device's functions, functions which are only intended to be accessed by authorized users.  So yes.  A complete bypass of BLE Bluetooth security, essentially by forcing a pairing of a malicious device with the Bluetooth device.



This led to a raft of CVEs - yup, all 12 of them are there - affecting different of the devices in different ways.  They have in their report a table of vulnerabilities and SDK versions of the affected system-on-a-chip devices.  And they're listed, I'm looking at Cypress, at TI, at STMicro, at NXP, at Texas Instruments, Microchip, I mean, they're all there.



So the problem we face here is the nature of the Bluetooth consumer device chain.  We're talking about the suppliers of the original silicon way at the start of the chain.  They have the silicon.  They provide a software development kit where the engineers who are then customizing the system on a chip, integrating it into their solution, are using the Bluetooth software stack that was provided by the system on a chip vendor, the BLE vendor, which ended up being - it gets burned into the firmware of the product.  Which is to say that every single Bluetooth device from these companies has these vulnerabilities.  And the sad truth of today's supply chain is most of these will never be updated.  By far and away, most Bluetooth-enabled devices are not our mainstream smartphones.



And, for example, Home Hubs or the Philips Hue Hub and bulbs, which I'm glad to find out were updated, they're not - most Bluetooth-enabled devices are not part of an active ecosystem.  And notice that you have to have some way of talking to the device.  I mean, our smartphones and our home hubs and things like the Bluetooth, well, those are tied into WiFi, which are tied into the Internet, that gives them a means of getting themselves updated.  But we've got Bluetooth stuff all over the place which doesn't have a means of getting connected to the Internet.  Most of them will never see a firmware update.  And even if one was available on some, what, Chinese website vendor or retailer somewhere, how would we the user ever know that it was even there?



So imagine some random Bluetooth Low Energy-based corporate or residential alarm system that was purchased through Amazon from typically these days a Chinese vendor that used a Bluetooth Low Energy chip from Cypress, TI, NXP, Microchip or apparently any of the other still not even named vendors.  We know that original manufacturer doesn't care about after-sales support.  You bought it.  It was cheap.  Good luck.  No after-sales support is even offered.  So that alarm system is now highly unlikely to ever receive a firmware update.  It works, yes.  But it will also be forever vulnerable - "forever" is the key word - forever vulnerable to wireless proximity attacks that will eventually be made fully public.



And in the specific case of an alarm system, it doesn't need to be vulnerable to the least common of those attacks, which was the full security bypass.  It might be sufficient for an attacking burglar's purpose to create a deadlock so that it is unable to sound the alarm.  But if a security bypass can be found, even more damage could probably be done.  And we've all seen science fiction where, for example, as with Neo at the start of "The Matrix," some lesser skilled individuals are purchasing some advanced hacking technology from him, a more highly skilled hacker.  I've always regarded this sort of world as more fanciful than real, that is, the idea that anything, anything can be hacked for a price.



But it's becoming increasingly clear that the way things are going, the fundamentally insecure way that we are now cavalierly and casually purchasing, deploying, using, and relying upon technology can be hacked and, in cases like this, will be forever known to be hackable.  This really does suggest that a brave new world where anything could be hacked for a price.



You know, imagine the phone rings, and the hacker says, "Hey, what's up?  Oh.  You want to bypass a Chimera 412 home alarm system?  Sure.  Piece of cake.  Those use the old Cypress 2313 BLE chip that its manufacturer never updated.  The hack for that's been around forever.  But mine has a few extra touches.  Transfer one-tenth of a bitcoin to my wallet, and once it's there I'll shoot you a script that you can run on any rooted Android smartphone that'll completely and silently shut down any Chimera 412."



LEO:  You don't think that's happening?



STEVE:  Probably is in some places.  Probably already is.  You're right.



LEO:  I don't think it's that farfetched, really.



STEVE:  You know?



LEO:  There's money out there.



STEVE:  That's the world we are in, yes.  The knowledge is there.  Hackers are there.  We know that there's an underground world.  We now have a means of transferring funds in a way that is essentially untraceable.



LEO:  I mean, I think most hackers are honorable, believe it or not.  But I think that we've heard that organized crime has started using hackers for hire.  I wouldn't be surprised.  I mean, all it would take is a small percentage of the total hacking community to be a problem.  



STEVE:  Well, and my point is it's not just big things like RDP servers that we could even, you know, we're able to count those.  You can't even count the number of Bluetooth Low Energy devices that are now floating around the world being used, and which are now known to be vulnerable and are never going to be updated.



LEO:  That's right.



STEVE:  Wow.  This podcast is going to get more interesting.



LEO:  I don't see how it can.  We are doing a little party.  I can make it more interesting if you're going to be at the RSA Conference in a couple of weeks.  Our sponsors, the namers of our beautiful studio, LastPass, are having, as they have in years past, they're having an event at this great place called Bourbon & Branch in San Francisco.  It's a speakeasy, and you have to know the password to get in, you know.  And it's not "swordfish."  And you get in, and there's secret rooms and stuff, and great drinks.  And we would love to see you there.  I'll be there, Lisa'll be there, a lot of our staff will be at Bourbon & Branch at the speakeasy.  It's February 26th, so eight days from today, a week from tomorrow, at 7:00 p.m.  I can't tell you what the secret password is.  I don't know it.  But if you RSVP, it will be sent to you under secret cover.



STEVE:  Ah.



LEO:  Go to twit.to/rsalastpass20.  That's the URL shortener we use, twit.to.  Twit.to/rsalastpass20.  But only if you can be there.  You know, some number of people are going to just say, I just want to know what the password is.  It's not that interesting, trust me.  It's, you know, it's going to be simple.  But if you can be at the party, we'd love to have you.  Open bar and everything.  February 26th, 7:00 p.m., Bourbon & Branch in San Francisco, if you're going to be at RSA.  I wonder what's going on in the long run with RSA because, you know, IBM Platinum Sponsor dropped out.  I'm just curious.  I guess everybody's watching COVID-19 to see, well, is it going to get worse, or is it going to get better?



STEVE:  Yeah.  And you guys were talking about that recently.  I think you're right about this really putting a chill on...



LEO:  Conferences.



STEVE:  ...on physical attendance at conferences.



LEO:  Shut down Mobile World Congress pretty good.



STEVE:  And we know that Apple just announced that they've got a profitability problem, said they can't get their stuff out of China now.



LEO:  That's right.  But so far RSA is still going strong.  And if you're going, I would love to see you on the 26th.  Steve, thank you so much.  Steve's home on the 'Net, GRC.com, the Gibson Research Corporation.  That's where you'll find all the stuff that he does, including SpinRite, the world's best hard drive and maintenance utility, now being updated as we speak.  You can also find lots of free stuff, including this show, 16Kb audio for the bandwidth-impaired, 64Kb audio for those of you who like the rich stereo experience.  We also have, he has carefully written transcripts so you can read along as you listen.  That's GRC.com.



Steve's on the Twitter at @SGgrc, if you have a question or a comment.  He accepts DMs.  And of course you should follow him to see what he's up to.  We have copies of the show, audio and video, too, at the website, TWiT.tv/sn.  And of course you can get it on demand anytime, just go to that website.  We do the show Tuesdays, 1:30 Pacific, 4:30 Eastern, 21:30 UTC.  So you can also watch us stream it live for the unexpurgated version, that's at TWiT.tv/live, both audio and video there.



And of course the best thing to do, it would help us a lot if you would subscribe in your favorite podcast application.  That just sends them a signal that people like this show.  Maybe they'll feature it.  It helps us.  So subscribe, and that way it'll help you, too.  You'll get a copy of it the minute it's available of a Tuesday evening.



Steve, that concludes this thrilling, gripping edition of Security Now!.  We'll see you next time.



STEVE:  Ciao.



LEO:  Ciao.



Copyright (c) 2020 by Steve Gibson and Leo Laporte.  SOME RIGHTS RESERVED.  This work is licensed for the good of the Internet Community under the Creative Commons License v2.5.  See the following Web page for details:  https://creativecommons.org/licenses/by-nc-sa/2.5/.




